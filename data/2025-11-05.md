<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 24]
- [cs.AI](#cs.AI) [Total: 34]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Multi-Personality Generation of LLMs at Decoding-time](https://arxiv.org/abs/2511.01891)
*Rongxin Chen,Yunfan Li,Yige Yuan,Bingbing Xu,Huawei Shen*

Main category: cs.CL

TL;DR: 提出了MPG框架，通过解码时组合范式实现LLM的多重人格生成，无需重新训练或依赖外部模型，利用单维模型的隐式密度比来灵活控制多重人格属性。


<details>
  <summary>Details</summary>
Motivation: 现有基于重新训练的方法成本高且扩展性差，而解码时方法通常依赖外部模型或启发式方法，限制了灵活性和鲁棒性。需要一种无需额外训练就能灵活控制多重人格的解决方案。

Method: 提出MPG框架，将任务重新表述为从聚合隐式密度比的目标策略中采样，并设计了SCR（推测性块级拒绝采样）方法，在滑动窗口内并行验证块级生成结果。

Result: 在MBTI人格和角色扮演任务上的实验表明，MPG有效性提升达16%-18%，显著降低计算开销的同时保持高质量生成。

Conclusion: MPG框架成功实现了无需重新训练的多重人格生成，提供了一种灵活、高效且可扩展的解决方案。

Abstract: Multi-personality generation for LLMs, enabling simultaneous embodiment of
multiple personalization attributes, is a fundamental challenge. Existing
retraining-based approaches are costly and poorly scalable, while decoding-time
methods often rely on external models or heuristics, limiting flexibility and
robustness. In this paper, we propose a novel Multi-Personality Generation
(MPG) framework under the decoding-time combination paradigm. It flexibly
controls multi-personality without relying on scarce multi-dimensional models
or extra training, leveraging implicit density ratios in single-dimensional
models as a "free lunch" to reformulate the task as sampling from a target
strategy aggregating these ratios. To implement MPG efficiently, we design
Speculative Chunk-level based Rejection sampling (SCR), which generates
responses in chunks and parallelly validates them via estimated thresholds
within a sliding window. This significantly reduces computational overhead
while maintaining high-quality generation. Experiments on MBTI personality and
Role-Playing demonstrate the effectiveness of MPG, showing improvements up to
16%-18%. Code and data are available at https://github.com/Libra117/MPG .

</details>


### [2] [Rethinking LLM Human Simulation: When a Graph is What You Need](https://arxiv.org/abs/2511.02135)
*Joseph Suh,Suhong Moon,Serina Chang*

Main category: cs.CL

TL;DR: GEMS提出了一种基于图神经网络的人类模拟方法，在离散选择任务中能够以三个数量级更小的模型规模达到或超越大型语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索在人类模拟任务中，是否必须使用大型语言模型，还是可以用更小、领域特定的模型来替代。

Method: 将离散选择模拟任务构建为图上的链接预测问题，使用图神经网络，仅在需要时整合语言表示。

Result: 在三个关键设置和三个模拟数据集上的评估显示，GEMS在准确性上达到或超越了强LLM基线，同时具有更高的效率、可解释性和透明度。

Conclusion: 图基建模作为LLM的轻量级替代方案，在人类模拟任务中展现出巨大潜力。

Abstract: Large language models (LLMs) are increasingly used to simulate humans, with
applications ranging from survey prediction to decision-making. However, are
LLMs strictly necessary, or can smaller, domain-grounded models suffice? We
identify a large class of simulation problems in which individuals make choices
among discrete options, where a graph neural network (GNN) can match or surpass
strong LLM baselines despite being three orders of magnitude smaller. We
introduce Graph-basEd Models for human Simulation (GEMS), which casts discrete
choice simulation tasks as a link prediction problem on graphs, leveraging
relational knowledge while incorporating language representations only when
needed. Evaluations across three key settings on three simulation datasets show
that GEMS achieves comparable or better accuracy than LLMs, with far greater
efficiency, interpretability, and transparency, highlighting the promise of
graph-based modeling as a lightweight alternative to LLMs for human simulation.
Our code is available at https://github.com/schang-lab/gems.

</details>


### [3] [IG-Pruning: Input-Guided Block Pruning for Large Language Models](https://arxiv.org/abs/2511.02213)
*Kangyu Qiao,Shaolei Zhang,Yang Feng*

Main category: cs.CL

TL;DR: IG-Pruning是一种新颖的输入感知块级剪枝方法，通过动态选择层掩码来减少大语言模型的计算成本，无需大量训练即可实现高效推理。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型计算需求增长，高效推理对实际部署变得至关重要。现有深度剪枝方法依赖固定块掩码，在不同任务和输入上表现不佳。

Method: 两阶段方法：1) 通过语义聚类和L0优化发现多样化掩码候选；2) 在推理时动态选择层掩码，无需大量训练。

Result: 实验结果表明，该方法持续优于最先进的静态深度剪枝方法。

Conclusion: IG-Pruning特别适合资源受限的部署场景，为LLM高效推理提供了有效解决方案。

Abstract: With the growing computational demands of large language models (LLMs),
efficient inference has become increasingly critical for practical deployment.
Depth pruning has emerged as a promising approach for reducing the
computational costs of large language models by removing transformer layers.
However, existing methods typically rely on fixed block masks, which can lead
to suboptimal performance across different tasks and inputs. In this paper, we
propose IG-Pruning, a novel input-aware block-wise pruning method that
dynamically selects layer masks at inference time. Our approach consists of two
stages: (1) Discovering diverse mask candidates through semantic clustering and
L0 optimization, and (2) Implementing efficient dynamic pruning without the
need for extensive training. Experimental results demonstrate that our method
consistently outperforms state-of-the-art static depth pruning methods, making
it particularly suitable for resource-constrained deployment scenarios.

</details>


### [4] [Demo: Statistically Significant Results On Biases and Errors of LLMs Do Not Guarantee Generalizable Results](https://arxiv.org/abs/2511.02246)
*Jonathan Liu,Haoling Qiu,Jonathan Lasko,Damianos Karakos,Mahsa Yarmohammadi,Mark Dredze*

Main category: cs.CL

TL;DR: 该研究开发了一个基础设施来自动生成查询并评估医疗聊天机器人的回答，发现LLM评估者之间一致性较低，建议使用多个LLM作为评估者以避免统计显著但不具普适性的结果。


<details>
  <summary>Details</summary>
Motivation: 医疗聊天机器人在涉及非医疗因素（如人口统计信息）时必须提供一致的建议，但现有LLM存在幻觉、遗漏和偏见问题，需要了解其在什么条件下会表现不佳。

Method: 开发了一个基础设施：1）自动生成查询来探测LLM；2）使用多个LLM-as-a-judge设置和提示来评估这些查询的答案。包括患者人口统计、病史、疾病和写作风格的采样。

Result: 发现LLM标注者之间一致性得分较低（平均Cohen's Kappa κ=0.118），只有特定的（回答，评估）LLM对在写作风格、性别和种族方面产生统计显著差异。

Conclusion: 建议使用多个LLM作为评估者以避免得出统计显著但不具普适性的结果，特别是在缺乏真实数据的情况下，并建议发布LLM间一致性指标以提高透明度。

Abstract: Recent research has shown that hallucinations, omissions, and biases are
prevalent in everyday use-cases of LLMs. However, chatbots used in medical
contexts must provide consistent advice in situations where non-medical factors
are involved, such as when demographic information is present. In order to
understand the conditions under which medical chatbots fail to perform as
expected, we develop an infrastructure that 1) automatically generates queries
to probe LLMs and 2) evaluates answers to these queries using multiple
LLM-as-a-judge setups and prompts. For 1), our prompt creation pipeline samples
the space of patient demographics, histories, disorders, and writing styles to
create realistic questions that we subsequently use to prompt LLMs. In 2), our
evaluation pipeline provides hallucination and omission detection using
LLM-as-a-judge as well as agentic workflows, in addition to LLM-as-a-judge
treatment category detectors. As a baseline study, we perform two case studies
on inter-LLM agreement and the impact of varying the answering and evaluation
LLMs. We find that LLM annotators exhibit low agreement scores (average Cohen's
Kappa $\kappa=0.118$), and only specific (answering, evaluation) LLM pairs
yield statistically significant differences across writing styles, genders, and
races. We recommend that studies using LLM evaluation use multiple LLMs as
evaluators in order to avoid arriving at statistically significant but
non-generalizable results, particularly in the absence of ground-truth data. We
also suggest publishing inter-LLM agreement metrics for transparency. Our code
and dataset are available here:
https://github.com/BBN-E/medic-neurips-2025-demo.

</details>


### [5] [LTD-Bench: Evaluating Large Language Models by Letting Them Draw](https://arxiv.org/abs/2511.02347)
*Liuhao Lin,Ke Li,Zihan Xu,Yuchen Shi,Yulei Qin,Yan Zhang,Xing Sun,Rongrong Ji*

Main category: cs.CL

TL;DR: LTD-Bench是一个突破性的基准测试，通过要求模型通过点阵或可执行代码生成绘图，将LLM评估从抽象分数转变为可直接观察的视觉输出，暴露了当前LLM在空间推理方面的严重缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估范式存在关键盲点，依赖不透明的数值指标掩盖了空间推理的基本限制，无法直观理解模型能力，导致报告性能与实际能力之间存在危险脱节。

Method: LTD-Bench采用全面方法论，包含互补的生成任务（测试空间想象力）和识别任务（评估空间感知），在三个渐进难度级别上系统评估语言-空间映射的双向能力。

Result: 广泛实验显示令人担忧的能力差距：即使在传统基准测试中表现优异的LLM，在建立语言与空间概念双向映射方面也表现出严重缺陷，这削弱了它们作为真正世界模型的潜力。

Conclusion: LTD-Bench的视觉输出支持强大的诊断分析，为研究模型相似性提供了潜在方法，揭示了LLM在空间推理方面的根本局限性。

Abstract: Current evaluation paradigms for large language models (LLMs) represent a
critical blind spot in AI research--relying on opaque numerical metrics that
conceal fundamental limitations in spatial reasoning while providing no
intuitive understanding of model capabilities. This deficiency creates a
dangerous disconnect between reported performance and practical abilities,
particularly for applications requiring physical world understanding. We
introduce LTD-Bench, a breakthrough benchmark that transforms LLM evaluation
from abstract scores to directly observable visual outputs by requiring models
to generate drawings through dot matrices or executable code. This approach
makes spatial reasoning limitations immediately apparent even to non-experts,
bridging the fundamental gap between statistical performance and intuitive
assessment. LTD-Bench implements a comprehensive methodology with complementary
generation tasks (testing spatial imagination) and recognition tasks (assessing
spatial perception) across three progressively challenging difficulty levels,
methodically evaluating both directions of the critical language-spatial
mapping. Our extensive experiments with state-of-the-art models expose an
alarming capability gap: even LLMs achieving impressive results on traditional
benchmarks demonstrate profound deficiencies in establishing bidirectional
mappings between language and spatial concept--a fundamental limitation that
undermines their potential as genuine world models. Furthermore, LTD-Bench's
visual outputs enable powerful diagnostic analysis, offering a potential
approach to investigate model similarity.

</details>


### [6] [Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation](https://arxiv.org/abs/2511.02358)
*Wongyu Kim,Hochang Lee,Sanghak Lee,Yoonsung Kim,Jaehyun Park*

Main category: cs.CL

TL;DR: 提出M-Solomon多模态嵌入器，通过自适应查询增强解决传统方法对所有查询进行增强导致的延迟问题和性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的嵌入器对所有查询进行增强会导致显著的嵌入延迟，且某些查询增强反而会损害性能。此外，之前的方法未在多模态环境中探索。

Method: 首先在数据集层面将查询分为需要增强和不需要增强两组；利用强大的多模态LLM为需要增强的查询生成合适的增强内容；通过自适应查询增强，仅在必要时进行增强，学习为需要增强的查询生成带/augment前缀的合成增强，为其他查询生成简单的/embed字符串。

Result: 实验结果显示M-Solomon不仅大幅超越无增强的基线方法，还优于始终使用增强的基线方法，同时提供更快的嵌入延迟。

Conclusion: M-Solomon通过自适应查询增强机制，在多模态环境中实现了更好的检索性能，同时显著降低了嵌入延迟。

Abstract: Query augmentation makes queries more meaningful by appending further
information to the queries to find relevant documents. Current studies have
proposed Large Language Model (LLM)-based embedders, which learn representation
for embedding and generation for query augmentation in a multi-task manner by
leveraging the generative capabilities of LLM. During inference, these jointly
trained embedders have conducted query augmentation followed by embedding,
showing effective results. However, augmenting every query leads to substantial
embedding latency and query augmentation can be detrimental to performance for
some queries. Also, previous methods have not been explored in multimodal
environments. To tackle these problems, we propose M-Solomon, a universal
multimodal embedder that can adaptively determine when to augment queries. Our
approach first divides the queries of the training datasets into two groups at
the dataset level. One includes queries that require augmentation and the other
includes queries that do not. Then, we introduces a synthesis process that
generates appropriate augmentations for queries that require them by leveraging
a powerful Multimodal LLM (MLLM). Next, we present adaptive query augmentation.
Through this step, M-Solomon can conduct query augmentation only when necessary
by learning to generate synthetic augmentations with the prefix /augment for
queries that demand them and to generate the simple string /embed for others.
Experimental results showed that M-Solomon not only surpassed the baseline
without augmentation by a large margin but also outperformed the baseline that
always used augmentation, providing much faster embedding latency.

</details>


### [7] [LiveSecBench: A Dynamic and Culturally-Relevant AI Safety Benchmark for LLMs in Chinese Context](https://arxiv.org/abs/2511.02366)
*Yudong Li,Zhongliang Yang,Kejiang Chen,Wenxuan Wang,Tianxin Zhang,Sifang Wan,Kecheng Wang,Haitian Li,Xu Wang,Lefan Cheng,Youdan Yang,Baocheng Chen,Ziyu Liu,Yufei Sun,Liyan Wu,Wenya Wen,Xingchi Gu,Peiru Yang*

Main category: cs.CL

TL;DR: LiveSecBench是一个专门针对中文LLM应用场景的动态安全基准测试，评估模型在合法性、伦理、事实性、隐私、对抗鲁棒性和推理安全六个关键维度的表现，并保持动态更新。


<details>
  <summary>Details</summary>
Motivation: 针对中文语言环境下的LLM应用场景，需要专门的安全基准测试来评估模型在中国法律和社会框架下的安全性表现。

Method: 构建包含六个关键维度的动态安全基准测试框架，采用动态更新机制纳入新的威胁向量，目前已评估18个LLM模型。

Result: LiveSecBench (v251030)已评估18个LLM，提供了中文语境下AI安全的全景图，排行榜公开可访问。

Conclusion: LiveSecBench为中文LLM应用提供了专门的安全评估基准，通过动态更新机制保持其相关性和有效性。

Abstract: In this work, we propose LiveSecBench, a dynamic and continuously updated
safety benchmark specifically for Chinese-language LLM application scenarios.
LiveSecBench evaluates models across six critical dimensions (Legality, Ethics,
Factuality, Privacy, Adversarial Robustness, and Reasoning Safety) rooted in
the Chinese legal and social frameworks. This benchmark maintains relevance
through a dynamic update schedule that incorporates new threat vectors, such as
the planned inclusion of Text-to-Image Generation Safety and Agentic Safety in
the next update. For now, LiveSecBench (v251030) has evaluated 18 LLMs,
providing a landscape of AI safety in the context of Chinese language. The
leaderboard is publicly accessible at https://livesecbench.intokentech.cn/.

</details>


### [8] [AyurParam: A State-of-the-Art Bilingual Language Model for Ayurveda](https://arxiv.org/abs/2511.02374)
*Mohd Nauman,Sravan Gvm,Vijay Devane,Shyam Pawar,Viraj Thakur,Kundeshwar Pundalik,Piyush Sawarkar,Rohit Saluja,Maunendra Desarkar,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: AyurParam-2.9B是一个专门针对阿育吠陀医学领域的双语语言模型，在BhashaBench-Ayur基准测试中超越了同规模的开源模型，甚至能与更大的模型竞争。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在需要深厚文化、语言和专业知识的专业领域表现不佳，特别是在阿育吠陀等传统医学系统中。

Method: 从Param-1-2.9B模型微调，使用专家精心策划的阿育吠陀数据集，包含英语和印地语的上下文感知、推理和客观风格问答。

Result: AyurParam在1.5-3B参数规模的指令调优模型中表现最佳，并且与更大的模型相比具有竞争力或更优性能。

Conclusion: AyurParam的结果强调了在提供可靠、文化一致的AI用于专业医学知识时，真实领域适应和高质量监督的必要性。

Abstract: Current large language models excel at broad, general-purpose tasks, but
consistently underperform when exposed to highly specialized domains that
require deep cultural, linguistic, and subject-matter expertise. In particular,
traditional medical systems such as Ayurveda embody centuries of nuanced
textual and clinical knowledge that mainstream LLMs fail to accurately
interpret or apply. We introduce AyurParam-2.9B, a domain-specialized,
bilingual language model fine-tuned from Param-1-2.9B using an extensive,
expertly curated Ayurveda dataset spanning classical texts and clinical
guidance. AyurParam's dataset incorporates context-aware, reasoning, and
objective-style Q&A in both English and Hindi, with rigorous annotation
protocols for factual precision and instructional clarity. Benchmarked on
BhashaBench-Ayur, AyurParam not only surpasses all open-source
instruction-tuned models in its size class (1.5--3B parameters), but also
demonstrates competitive or superior performance compared to much larger
models. The results from AyurParam highlight the necessity for authentic domain
adaptation and high-quality supervision in delivering reliable, culturally
congruent AI for specialized medical knowledge.

</details>


### [9] [AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models](https://arxiv.org/abs/2511.02376)
*Aashray Reddy,Andrew Zagula,Nicholas Saban*

Main category: cs.CL

TL;DR: AutoAdv是一个无需训练的多轮越狱攻击框架，通过自适应机制在6轮对话内对Llama-3.1-8B达到95%攻击成功率，比单轮攻击提升24%，揭示了当前安全机制在多轮对话中的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全评估主要关注单轮交互，而现实攻击往往通过自适应多轮对话展开，当前对齐策略无法在扩展对话中保持鲁棒性。

Method: 结合三种自适应机制：模式管理器从成功攻击中学习增强提示、温度管理器基于失败模式动态调整采样参数、两阶段重写策略先伪装有害请求再迭代优化。

Result: 在商业和开源模型（GPT-4o-mini、Qwen3-235B、Mistral-7B）上广泛评估显示，多轮攻击始终优于单轮方法，多轮对话中安全机制存在持续漏洞。

Conclusion: 针对单轮交互优化的对齐策略在扩展对话中失效，迫切需要开发多轮感知的防御机制。

Abstract: Large Language Models (LLMs) remain vulnerable to jailbreaking attacks where
adversarial prompts elicit harmful outputs, yet most evaluations focus on
single-turn interactions while real-world attacks unfold through adaptive
multi-turn conversations. We present AutoAdv, a training-free framework for
automated multi-turn jailbreaking that achieves up to 95% attack success rate
on Llama-3.1-8B within six turns a 24 percent improvement over single turn
baselines. AutoAdv uniquely combines three adaptive mechanisms: a pattern
manager that learns from successful attacks to enhance future prompts, a
temperature manager that dynamically adjusts sampling parameters based on
failure modes, and a two-phase rewriting strategy that disguises harmful
requests then iteratively refines them. Extensive evaluation across commercial
and open-source models (GPT-4o-mini, Qwen3-235B, Mistral-7B) reveals persistent
vulnerabilities in current safety mechanisms, with multi-turn attacks
consistently outperforming single-turn approaches. These findings demonstrate
that alignment strategies optimized for single-turn interactions fail to
maintain robustness across extended conversations, highlighting an urgent need
for multi-turn-aware defenses.

</details>


### [10] [Merging Continual Pretraining Models for Domain-Specialized LLMs: A Case Study in Finance](https://arxiv.org/abs/2511.02451)
*Kentaro Ueda,François Portet,Hirohiko Suwa,Keiichi Yasumoto*

Main category: cs.CL

TL;DR: 本文研究了如何通过合并领域特定的持续预训练专家模型来构建多技能金融大语言模型，提出了三阶段评估框架，并比较了三种模型合并方法的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在通用任务上表现出色，但在金融等专业领域表现不佳，需要融合领域知识、数学推理和多语言处理等多种技能。直接进行多技能训练成本高昂且不稳定，因此探索通过合并预训练专家模型来构建多技能模型。

Method: 创建金融、数学和日语三个领域的专家模型，提出三阶段评估框架（知识恢复、互补性和涌现性），在包含18个任务8个数据集的金融基准上评估Task Arithmetic、TIES和DARE-TIES三种合并方法。

Result: 合并专家模型与其基础模型可以恢复CPT过程中丢失的通用知识，合并不同专家模型能提升性能并产生跨领域涌现技能。Task Arithmetic表现强劲但对超参数敏感，TIES更稳健。模型相似性与合并成功相关，但涌现技能取决于更复杂因素。

Conclusion: 这是首个关于CPT模型合并的基础性分析，建立了原则性框架，为从现有资产构建多技能大语言模型提供了明确指导。

Abstract: While LLMs excel at general tasks, they struggle in specialized domains like
finance, requiring diverse skills in domain knowledge, mathematical reasoning,
and multilingual processing. Merging domain-specific Continual Pre-training
(CPT) "experts" offers a practical alternative to costly and unstable
multi-skill training. However, unlike established Supervised Fine-Tuning (SFT)
model-based merging, CPT model merging remains largely unexplored. We address
this gap by creating financial LLMs from experts in finance, math, and
Japanese. We propose a three-stage evaluation focusing on knowledge recovery,
complementarity, and emergence, and assess three merging methods (Task
Arithmetic, TIES, and DARE-TIES) on a comprehensive financial benchmark curated
from 18 tasks across 8 established datasets. Results show that merging an
expert with its base model recovers general knowledge lost during CPT, while
merging experts improves performance and can yield emergent cross-domain
skills. Among the methods, Task Arithmetic performs strongly but is
hyperparameter-sensitive, whereas TIES is more robust. Our findings also
suggest that while model similarity correlates with merging success, emergent
skills depend on more complex factors. This work presents the first
foundational analysis of CPT model merging, establishing a principled framework
and providing clear guidance for building multi-skill LLMs from existing
assets.

</details>


### [11] [Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas](https://arxiv.org/abs/2511.02458)
*Giulia Iadisernia,Carolina Camassa*

Main category: cs.CL

TL;DR: 评估基于角色的提示是否能提高LLM在宏观经济预测任务中的表现，发现GPT-4o与人类专家预测精度相似，但角色描述对预测准确性没有可衡量的优势。


<details>
  <summary>Details</summary>
Motivation: 研究角色提示是否能增强大型语言模型在宏观经济预测中的表现，特别是与人类专家预测进行比较。

Method: 使用PersonaHub语料库中的2,368个经济学相关角色提示GPT-4o，复制ECB专业预测者调查，涵盖50个季度(2013-2025)，比较角色提示预测与人类专家小组在四个目标变量和四个预测时间跨度上的表现。

Result: GPT-4o与人类预测者达到非常相似的准确度水平，差异在统计上显著但实际影响有限；在2024-2025样本外数据上保持竞争力；角色描述没有带来可衡量的预测优势。

Conclusion: GPT-4o在提供相关上下文数据时，即使在样本外宏观经济事件上也能达到有竞争力的预测精度，但多样化的提示会产生与人类小组相比显著同质化的预测，角色描述可省略以节省计算成本。

Abstract: We evaluate whether persona-based prompting improves Large Language Model
(LLM) performance on macroeconomic forecasting tasks. Using 2,368
economics-related personas from the PersonaHub corpus, we prompt GPT-4o to
replicate the ECB Survey of Professional Forecasters across 50 quarterly rounds
(2013-2025). We compare the persona-prompted forecasts against the human
experts panel, across four target variables (HICP, core HICP, GDP growth,
unemployment) and four forecast horizons. We also compare the results against
100 baseline forecasts without persona descriptions to isolate its effect. We
report two main findings. Firstly, GPT-4o and human forecasters achieve
remarkably similar accuracy levels, with differences that are statistically
significant yet practically modest. Our out-of-sample evaluation on 2024-2025
data demonstrates that GPT-4o can maintain competitive forecasting performance
on unseen events, though with notable differences compared to the in-sample
period. Secondly, our ablation experiment reveals no measurable forecasting
advantage from persona descriptions, suggesting these prompt components can be
omitted to reduce computational costs without sacrificing accuracy. Our results
provide evidence that GPT-4o can achieve competitive forecasting accuracy even
on out-of-sample macroeconomic events, if provided with relevant context data,
while revealing that diverse prompts produce remarkably homogeneous forecasts
compared to human panels.

</details>


### [12] [Smart-Hiring: An Explainable end-to-end Pipeline for CV Information Extraction and Job Matching](https://arxiv.org/abs/2511.02537)
*Kenza Khelkhal,Dihia Lanasri*

Main category: cs.CL

TL;DR: Smart-Hiring是一个端到端的NLP管道，用于自动从简历中提取结构化信息并语义匹配候选人与职位描述，解决了人工筛选简历耗时、易错和有偏见的问题。


<details>
  <summary>Details</summary>
Motivation: 传统招聘过程需要人工筛选大量简历，耗时耗力、容易出错且存在人为偏见，需要自动化的解决方案来提高效率和公平性。

Method: 结合文档解析、命名实体识别和上下文文本嵌入技术，在共享向量空间中编码简历和职位描述，计算相似度分数，系统模块化且可解释。

Result: 在真实世界数据集上的实验表明，该方法具有鲁棒性和可行性，在保持高可解释性的同时实现了有竞争力的匹配准确率。

Conclusion: 该工作提出了一个可扩展的实用NLP框架，为招聘分析提供了解决方案，并为偏见缓解、公平感知建模和大规模部署数据驱动招聘方案指明了方向。

Abstract: Hiring processes often involve the manual screening of hundreds of resumes
for each job, a task that is time and effort consuming, error-prone, and
subject to human bias. This paper presents Smart-Hiring, an end-to-end Natural
Language Processing (NLP) pipeline de- signed to automatically extract
structured information from unstructured resumes and to semantically match
candidates with job descriptions. The proposed system combines document
parsing, named-entity recognition, and contextual text embedding techniques to
capture skills, experience, and qualifications. Using advanced NLP technics,
Smart-Hiring encodes both resumes and job descriptions in a shared vector space
to compute similarity scores between candidates and job postings. The pipeline
is modular and explainable, allowing users to inspect extracted entities and
matching rationales. Experiments were conducted on a real-world dataset of
resumes and job descriptions spanning multiple professional domains,
demonstrating the robustness and feasibility of the proposed approach. The
system achieves competitive matching accuracy while preserving a high degree of
interpretability and transparency in its decision process. This work introduces
a scalable and practical NLP frame- work for recruitment analytics and outlines
promising directions for bias mitigation, fairness-aware modeling, and
large-scale deployment of data-driven hiring solutions.

</details>


### [13] [The Analysis of Lexical Errors in Machine Translation from English into Romanian](https://arxiv.org/abs/2511.02587)
*Angela Stamatie*

Main category: cs.CL

TL;DR: 该研究分析了Google Translate从英语到罗马尼亚语翻译WHO、Gavi和药品说明书等官方COVID-19相关文本时的词汇错误，旨在通过改进词汇选择来提升机器翻译质量。


<details>
  <summary>Details</summary>
Motivation: 研究旨在分析机器翻译在翻译COVID-19相关官方信息时产生的词汇错误，特别是关注Google Translate从英语到罗马尼亚语的翻译质量，以改进机器翻译系统的准确性和流畅性。

Method: 对230篇从英语翻译成罗马尼亚语的文本进行综合分析，这些文本包括WHO、Gavi组织和药品说明书等官方COVID-19相关信息，重点关注词汇错误的分析。

Result: 研究发现Google Translate在翻译COVID-19相关官方文本时存在词汇选择错误，这些错误影响了翻译的准确性和专业性。

Conclusion: 通过分析机器翻译中的词汇错误，可以为改进Google Translate等机器翻译系统提供有价值的见解，特别是在专业领域如医疗信息的翻译中，需要更好的词汇选择机制来提升翻译质量。

Abstract: The research explores error analysis in the performance of translating by
Machine Translation from English into Romanian, and it focuses on lexical
errors found in texts which include official information, provided by the World
Health Organization (WHO), the Gavi Organization, by the patient information
leaflet (the information about the active ingredients of the vaccines or the
medication, the indications, the dosage instructions, the storage instructions,
the side effects and warning, etc.). All of these texts are related to Covid-19
and have been translated by Google Translate, a multilingual Machine
Translation that was created by Google. In the last decades, Google has
actively worked to develop a more accurate and fluent automatic translation
system. This research, specifically focused on improving Google Translate, aims
to enhance the overall quality of Machine Translation by achieving better
lexical selection and by reducing errors. The investigation involves a
comprehensive analysis of 230 texts that have been translated from English into
Romanian.

</details>


### [14] [Next Token Knowledge Tracing: Exploiting Pretrained LLM Representations to Decode Student Behaviour](https://arxiv.org/abs/2511.02599)
*Max Norris,Kobi Gal,Sahan Bulathwela*

Main category: cs.CL

TL;DR: NTKT将知识追踪重新定义为使用预训练大语言模型的下一词预测任务，通过将学生历史和问题内容表示为文本序列，显著提升了预测性能并改善了冷启动问题。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪模型通常忽略问题文本这一重要教学信息源，限制了预测性能。

Method: 提出NTKT方法，将知识追踪重构为基于预训练大语言模型的下一词预测任务，将学生历史和问题内容表示为文本序列。

Result: 实验显著优于最先进的神经知识追踪模型，在冷启动问题和用户上表现更好。

Conclusion: 问题内容在知识追踪中很重要，利用预训练大语言模型表示能更有效地建模学生学习。

Abstract: Modelling student knowledge is a key challenge when leveraging AI in
education, with major implications for personalised learning. The Knowledge
Tracing (KT) task aims to predict how students will respond to educational
questions in learning environments, based on their prior interactions. Existing
KT models typically use response correctness along with metadata like skill
tags and timestamps, often overlooking the question text, which is an important
source of pedagogical insight. This omission poses a lost opportunity while
limiting predictive performance. We propose Next Token Knowledge Tracing
(NTKT), a novel approach that reframes KT as a next-token prediction task using
pretrained Large Language Models (LLMs). NTKT represents both student histories
and question content as sequences of text, allowing LLMs to learn patterns in
both behaviour and language. Our series of experiments significantly improves
performance over state-of-the-art neural KT models and generalises much better
to cold-start questions and users. These findings highlight the importance of
question content in KT and demonstrate the benefits of leveraging pretrained
representations of LLMs to model student learning more effectively.

</details>


### [15] [CGES: Confidence-Guided Early Stopping for Efficient and Accurate Self-Consistency](https://arxiv.org/abs/2511.02603)
*Ehsan Aghazadeh,Ahmad Ghasemi,Hedyeh Beyhaghi,Hossein Pishro-Nik*

Main category: cs.CL

TL;DR: CGES是一种贝叶斯框架，通过置信度信号自适应停止采样，在保持准确性的同时大幅减少模型调用次数。


<details>
  <summary>Details</summary>
Motivation: 解决自一致性策略需要固定调用次数且在正确答案罕见时可能失败的问题。

Method: 使用从token概率或奖励模型导出的置信度信号构建候选答案的后验分布，当候选后验质量超过阈值时自适应停止采样。

Result: 在五个推理基准测试中，平均减少约69%的模型调用次数（如从16.0次降至4.9次），同时准确性与自一致性策略相差不超过0.06个百分点。

Conclusion: CGES框架在保持推理准确性的同时，显著提高了模型调用的效率。

Abstract: Large language models (LLMs) are often queried multiple times at test time,
with predictions aggregated by majority vote. While effective, this
self-consistency strategy (arXiv:2203.11171) requires a fixed number of calls
and can fail when the correct answer is rare. We introduce Confidence-Guided
Early Stopping (CGES), a Bayesian framework that forms posteriors over
candidate answers using scalar confidence signals derived from token
probabilities or reward models. CGES adaptively halts sampling once the
posterior mass of a candidate exceeds a threshold. We provide theoretical
guarantees for both perfectly calibrated confidences and realistic noisy
confidence signals. Across five reasoning benchmarks, CGES reduces the average
number of model calls by about 69 percent (for example, from 16.0 to 4.9) while
matching the accuracy of self-consistency within 0.06 percentage points.

</details>


### [16] [The Realignment Problem: When Right becomes Wrong in LLMs](https://arxiv.org/abs/2511.02623)
*Aakash Sen Sharma,Debdeep Sanyal,Vivek Srivastava,Shirish Karande,Murari Mandal*

Main category: cs.CL

TL;DR: TRACE框架通过程序化策略应用解决LLM与现实政策之间的对齐差距，实现动态、可扩展且成本效益高的模型重新对齐，而不会损害模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐方法产生静态、脆弱且维护成本高的模型，无法跟上不断演变的规范和政策，存在对齐-现实差距问题。现有解决方案（大规模重新标注和标准遗忘方法）在经济上不可行或会损害模型效用。

Method: TRACE框架将重新对齐视为程序化策略应用问题：程序化筛选现有偏好数据与新政策的冲突，通过对齐影响评分识别高影响冲突，应用混合优化干净地反转、丢弃或保留偏好，同时保护模型性能。

Result: 在多种模型家族（Qwen2.5-7B、Gemma-2-9B、Llama-3.1-8B）上，TRACE在合成基准和PKU-SafeRLHF数据集上的复杂策略转变下均实现了稳健的重新对齐，强制执行新原则而不降低通用能力。

Conclusion: TRACE为维护LLM对齐建立了一个可扩展、动态且成本效益高的范式，为可持续和负责任的AI部署提供了基础。

Abstract: The alignment of Large Language Models (LLMs) with human values is central to
their safe deployment, yet current practice produces static, brittle, and
costly-to-maintain models that fail to keep pace with evolving norms and
policies. This misalignment, which we term the Alignment-Reality Gap, poses a
growing challenge for reliable long-term use. Existing remedies are inadequate:
large-scale re-annotation is economically prohibitive, and standard unlearning
methods act as blunt instruments that erode utility rather than enable precise
policy updates. We introduce TRACE (Triage and Re-align by Alignment Conflict
Evaluation), a framework for principled unlearning that reconceives
re-alignment as a programmatic policy application problem. TRACE
programmatically triages existing preference data against a new policy,
identifies high-impact conflicts via a alignment impact score, and applies a
hybrid optimization that cleanly inverts, discards, or preserves preferences
while safeguarding model performance. Empirical results show that TRACE
achieves robust re-alignment across diverse model families (Qwen2.5-7B,
Gemma-2-9B, Llama-3.1-8B). On both synthetic benchmarks and the PKU-SafeRLHF
dataset under complex policy shift, TRACE enforces new principles without
degrading general capabilities. Our work establishes a scalable, dynamic, and
cost-effective paradigm for maintaining LLM alignment, providing a foundation
for sustainable and responsible AI deployment.

</details>


### [17] [Understanding New-Knowledge-Induced Factual Hallucinations in LLMs: Analysis, Solution, and Interpretation](https://arxiv.org/abs/2511.02626)
*Renfei Dang,Peng Hu,Changjiang Gao,Shujian Huang*

Main category: cs.CL

TL;DR: 研究发现，在LLM微调中引入新知识会导致事实幻觉，特别是当特定知识类型完全由新知识组成时。提出了KnownPatch方法，通过在训练后期添加少量已知知识样本来缓解这种幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有研究未深入探讨新知识微调导致的事实幻觉的具体表现和机制，本研究旨在填补这一空白。

Method: 设计了Biography-Reasoning数据集，进行细粒度分析，并提出KnownPatch方法在训练后期补丁已知知识样本。

Result: 发现特定知识类型的高陌生度是幻觉的主要驱动因素，新知识学习会降低模型对问题中关键实体的注意力，导致过度关注上下文。KnownPatch能有效缓解这种注意力破坏。

Conclusion: 特定知识类型的陌生度而非新知识整体比例是幻觉的主要驱动因素，KnownPatch方法能有效缓解新知识引起的事实幻觉。

Abstract: Previous studies show that introducing new knowledge during large language
models (LLMs) fine-tuning can lead to the generation of erroneous output when
tested on known information, thereby triggering factual hallucinations.
However, existing studies have not deeply investigated the specific
manifestations and underlying mechanisms of these hallucinations. Our work
addresses this gap by designing a controlled dataset Biography-Reasoning, and
conducting a fine-grained analysis across multiple knowledge types and two task
types, including knowledge question answering (QA) and knowledge reasoning
tasks. We find that when fine-tuned on a dataset in which a specific knowledge
type consists entirely of new knowledge, LLMs exhibit significantly increased
hallucination tendencies. This suggests that the high unfamiliarity of a
particular knowledge type, rather than the overall proportion of new knowledge,
is a stronger driver of hallucinations, and these tendencies can even affect
other knowledge types in QA tasks. To mitigate such factual hallucinations, we
propose KnownPatch, which patches a small number of known knowledge samples in
the later stages of training, effectively alleviating new-knowledge-induced
hallucinations. Through attention analysis, we find that learning new knowledge
reduces the model's attention to key entities in the question, thus causing
excessive focus on the surrounding context, which may increase the risk of
hallucination. Moreover, the attention pattern can propagate to similar
contexts, facilitating the spread of hallucinations to textually similar
questions. Our method effectively mitigates the disruption of new knowledge
learning to the model's attention on key entities, accompanied by improved
performance.

</details>


### [18] [Optimal Singular Damage: Efficient LLM Inference in Low Storage Regimes](https://arxiv.org/abs/2511.02681)
*Mohammadsajad Alipour,Mohammad Mohammadi Amiri*

Main category: cs.CL

TL;DR: 提出了一种名为optimal singular damage的方法，用于高效存储预训练模型微调后的参数更新，通过结合低秩近似和稀疏化来提升存储效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型微调后存储成本高，研究发现微调主要影响少量参数，需要更高效的存储方法。

Method: 利用微调更新具有低秩和稀疏特性的观察，提出选择性稀疏化低秩近似更新的方法，保留最重要的奇异分量。

Result: 在相同内存预算下，该方法相比单独使用低秩近似或稀疏化，实现了显著的存储效率和更高的准确性。

Conclusion: 结合低秩近似和稀疏化的方法能有效提升微调模型参数的存储效率，同时保持模型性能。

Abstract: Large language models (LLMs) are increasingly prevalent across diverse
applications. However, their enormous size limits storage and processing
capabilities to a few well-resourced stakeholders. As a result, most
applications rely on pre-trained LLMs, fine-tuned for specific tasks. However,
even storing the fine-tuned versions of these models remains a significant
challenge due to the wide range of tasks they address. Recently, studies show
that fine-tuning these models primarily affects a small fraction of parameters,
highlighting the need for more efficient storage of fine-tuned models. This
paper focuses on efficient storage of parameter updates in pre-trained models
after fine-tuning. To address this challenge, we leverage the observation that
fine-tuning updates are both low-rank and sparse, which can be utilized for
storage efficiency. However, using only low-rank approximation or
sparsification may discard critical singular components that enhance model
expressivity. We first observe that given the same memory budget, sparsified
low-rank approximations with larger ranks outperform standard low-rank
approximations with smaller ranks. Building on this, we propose our method,
optimal singular damage, that selectively sparsifies low-rank approximated
updates by leveraging the interleaved importance of singular vectors, ensuring
that the most impactful components are retained. We demonstrate through
extensive experiments that our proposed methods lead to significant storage
efficiency and superior accuracy within the same memory budget compared to
employing the low-rank approximation or sparsification individually.

</details>


### [19] [PragExTra: A Multilingual Corpus of Pragmatic Explicitation in Translation](https://arxiv.org/abs/2511.02721)
*Doreen Osmelak,Koel Dutta Chowdhury,Uliana Sentsova,Cristina España-Bonet,Josef van Genabith*

Main category: cs.CL

TL;DR: PragExTra是首个用于检测语用显化现象的多语言语料库和框架，涵盖8种语言对，通过空对齐和主动学习识别实体描述、度量转换等显化案例。


<details>
  <summary>Details</summary>
Motivation: 翻译中经常需要添加背景信息来向新受众明确隐含的文化含义，这种现象在翻译理论中广泛讨论但很少进行计算建模。

Method: 使用TED-Multi和Europarl语料库，通过空对齐识别候选显化案例，然后用主动学习和人工标注进行精炼。

Result: 实体和系统级显化最为常见，主动学习将分类器准确率提高了7-8个百分点，在跨语言中达到0.88准确率和0.82 F1值。

Conclusion: PragExTra将语用显化确立为可测量的跨语言现象，为构建文化感知的机器翻译迈出了重要一步。

Abstract: Translators often enrich texts with background details that make implicit
cultural meanings explicit for new audiences. This phenomenon, known as
pragmatic explicitation, has been widely discussed in translation theory but
rarely modeled computationally. We introduce PragExTra, the first multilingual
corpus and detection framework for pragmatic explicitation. The corpus covers
eight language pairs from TED-Multi and Europarl and includes additions such as
entity descriptions, measurement conversions, and translator remarks. We
identify candidate explicitation cases through null alignments and refined
using active learning with human annotation. Our results show that entity and
system-level explicitations are most frequent, and that active learning
improves classifier accuracy by 7-8 percentage points, achieving up to 0.88
accuracy and 0.82 F1 across languages. PragExTra establishes pragmatic
explicitation as a measurable, cross-linguistic phenomenon and takes a step
towards building culturally aware machine translation. Keywords: translation,
multilingualism, explicitation

</details>


### [20] [AI Diffusion in Low Resource Language Countries](https://arxiv.org/abs/2511.02752)
*Amit Misra,Syed Waqas Zamir,Wassim Hamidouche,Inbal Becker-Reshef,Juan Lavista Ferres*

Main category: cs.CL

TL;DR: 研究发现低资源语言国家AI用户比例比基线低约20%，语言可及性是AI公平扩散的重要独立障碍


<details>
  <summary>Details</summary>
Motivation: 前沿大型语言模型在低资源语言上表现不佳，这种性能缺陷可能降低AI效用，从而减缓低资源语言国家的AI采用

Method: 使用加权回归模型从社会经济和人口因素中分离语言效应

Result: 低资源语言国家的AI用户比例比基线低约20%

Conclusion: 语言可及性是AI公平扩散的重要独立障碍

Abstract: Artificial intelligence (AI) is diffusing globally at unprecedented speed,
but adoption remains uneven. Frontier Large Language Models (LLMs) are known to
perform poorly on low-resource languages due to data scarcity. We hypothesize
that this performance deficit reduces the utility of AI, thereby slowing
adoption in Low-Resource Language Countries (LRLCs). To test this, we use a
weighted regression model to isolate the language effect from socioeconomic and
demographic factors, finding that LRLCs have a share of AI users that is
approximately 20% lower relative to their baseline. These results indicate that
linguistic accessibility is a significant, independent barrier to equitable AI
diffusion.

</details>


### [21] [Controlling Performance and Budget of a Centralized Multi-agent LLM System with Reinforcement Learning](https://arxiv.org/abs/2511.02755)
*Bowen Jin,TJ Collins,Donghan Yu,Mert Cemri,Shenao Zhang,Mengyu Li,Jay Tang,Tian Qin,Zhiyang Xu,Jiarui Lu,Guoli Yin,Jiawei Han,Zirui Wang*

Main category: cs.CL

TL;DR: 提出了一种集中式多LLM框架CoRL，通过强化学习优化任务性能和推理成本的权衡，实现预算可控的多智能体LLM系统。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化多LLM系统对每个输入都调用多个模型，导致推理成本高昂且不可控，需要设计成本高效且可控的多智能体协作方案。

Method: 采用集中式框架，通过控制器LLM选择性协调专家模型池，使用强化学习框架优化性能-成本权衡，支持多预算条件下的自适应行为。

Result: 在四个基准测试中，CoRL在高预算设置下超越最佳专家LLM，在低预算模式下仍保持强性能，证明了集中式协调的有效性。

Conclusion: 集中式协调为可扩展且成本高效的多智能体LLM系统提供了有效解决方案，能够在不同预算条件下实现性能与成本的优化平衡。

Abstract: Large language models (LLMs) exhibit complementary strengths across domains
and come with varying inference costs, motivating the design of multi-agent LLM
systems where specialized models collaborate efficiently. Existing approaches
predominantly rely on decentralized frameworks, which invoke multiple LLMs for
every input and thus lead to substantial and uncontrolled inference costs. In
this work, we introduce a centralized multi-LLM framework, where a controller
LLM selectively coordinates a pool of expert models in a cost-efficient and
cost-controllable manner. We formulate this coordination problem as
reinforcement learning with dual objectives: maximizing task performance while
minimizing the overall inference cost. In addition, we expect the multi-agent
system to have adapted behavior with different budget conditions during
inference. To this end, we propose CoRL, a reinforcement learning framework
that optimizes the performance cost trade-off in a controllable multi-budget
setting. Experiments on four diverse benchmarks demonstrate that CoRL enables a
single system to surpass the best expert LLM under high-budget settings, while
maintaining strong performance in more economical low-budget modes,
highlighting the effectiveness of centralized coordination for scalable and
cost-efficient multi-agent LLM systems.

</details>


### [22] [Beyond Single Embeddings: Capturing Diverse Targets with Multi-Query Retrieval](https://arxiv.org/abs/2511.02770)
*Hung-Ting Chen,Xiang Liu,Shauli Ravfogel,Eunsol Choi*

Main category: cs.CL

TL;DR: AMER是一种新的检索器架构，通过自回归生成多个查询向量来检索文档，解决了传统单一查询向量在多模态相关文档分布中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统文本检索器只生成一个查询向量，但查询的相关文档分布可能是多模态的（如不同解释），现有检索器在目标文档嵌入距离较大时表现不佳。

Method: 开发了自回归多嵌入检索器(AMER)，自回归生成多个查询向量，所有预测的查询向量都用于从语料库中检索文档。

Result: 在合成向量化数据上，AMER能完美捕捉多个目标分布，性能比单嵌入模型提高4倍；在真实世界多答案检索数据集上，相比单嵌入基线分别获得4%和21%的相对增益。

Conclusion: 多查询向量检索器具有巨大潜力，为目标文档嵌入相似度较低的数据子集带来更大增益，为未来工作开辟了新方向。

Abstract: Most text retrievers generate \emph{one} query vector to retrieve relevant
documents. Yet, the conditional distribution of relevant documents for the
query may be multimodal, e.g., representing different interpretations of the
query. We first quantify the limitations of existing retrievers. All retrievers
we evaluate struggle more as the distance between target document embeddings
grows. To address this limitation, we develop a new retriever architecture,
\emph{A}utoregressive \emph{M}ulti-\emph{E}mbedding \emph{R}etriever (AMER).
Our model autoregressively generates multiple query vectors, and all the
predicted query vectors are used to retrieve documents from the corpus. We show
that on the synthetic vectorized data, the proposed method could capture
multiple target distributions perfectly, showing 4x better performance than
single embedding model. We also fine-tune our model on real-world multi-answer
retrieval datasets and evaluate in-domain. AMER presents 4 and 21\% relative
gains over single-embedding baselines on two datasets we evaluate on.
Furthermore, we consistently observe larger gains on the subset of dataset
where the embeddings of the target documents are less similar to each other. We
demonstrate the potential of using a multi-query vector retriever and open up a
new direction for future work.

</details>


### [23] [MemSearcher: Training LLMs to Reason, Search and Manage Memory via End-to-End Reinforcement Learning](https://arxiv.org/abs/2511.02805)
*Qianhao Yuan,Jie Lou,Zichao Li,Jiawei Chen,Yaojie Lu,Hongyu Lin,Le Sun,Debing Zhang,Xianpei Han*

Main category: cs.CL

TL;DR: MemSearcher是一个搜索代理工作流，通过迭代维护紧凑内存来平衡信息完整性和效率，在多轮交互中稳定上下文长度，提高效率而不牺牲准确性。


<details>
  <summary>Details</summary>
Motivation: 传统搜索代理要么拼接整个交互历史导致长而嘈杂的上下文，要么仅使用当前轮次丢弃重要信息，这种权衡限制了搜索代理的可扩展性。

Method: 提出MemSearcher代理工作流，在每个轮次将用户问题与内存融合生成推理轨迹、执行搜索动作，并更新内存仅保留解决任务所需的关键信息。使用多上下文GRPO强化学习框架联合优化推理、搜索策略和内存管理。

Result: 在七个公开基准测试中，相比强基线取得显著改进：Qwen2.5-3B-Instruct平均提升11%，Qwen2.5-7B-Instruct平均提升12%。3B版本的MemSearcher甚至超越7B基线模型。

Conclusion: 在信息完整性和效率之间取得平衡既能提高准确性又能降低计算开销，MemSearcher证明了这一设计理念的有效性。

Abstract: Typical search agents concatenate the entire interaction history into the LLM
context, preserving information integrity but producing long, noisy contexts,
resulting in high computation and memory costs. In contrast, using only the
current turn avoids this overhead but discards essential information. This
trade-off limits the scalability of search agents. To address this challenge,
we propose MemSearcher, an agent workflow that iteratively maintains a compact
memory and combines the current turn with it. At each turn, MemSearcher fuses
the user's question with the memory to generate reasoning traces, perform
search actions, and update memory to retain only information essential for
solving the task. This design stabilizes context length across multi-turn
interactions, improving efficiency without sacrificing accuracy. To optimize
this workflow, we introduce multi-context GRPO, an end-to-end RL framework that
jointly optimize reasoning, search strategies, and memory management of
MemSearcher Agents. Specifically, multi-context GRPO samples groups of
trajectories under different contexts and propagates trajectory-level
advantages across all conversations within them. Trained on the same dataset as
Search-R1, MemSearcher achieves significant improvements over strong baselines
on seven public benchmarks: +11% on Qwen2.5-3B-Instruct and +12% on
Qwen2.5-7B-Instruct relative average gains. Notably, the 3B-based MemSearcher
even outperforms 7B-based baselines, demonstrating that striking a balance
between information integrity and efficiency yields both higher accuracy and
lower computational overhead. The code and models will be publicly available at
https://github.com/icip-cas/MemSearcher

</details>


### [24] [Oolong: Evaluating Long Context Reasoning and Aggregation Capabilities](https://arxiv.org/abs/2511.02817)
*Amanda Bertsch,Adithya Pratapa,Teruko Mitamura,Graham Neubig,Matthew R. Gormley*

Main category: cs.CL

TL;DR: Oolong是一个长上下文推理基准测试，包含合成任务和真实世界对话任务，要求模型分析文本块并聚合信息回答分布性问题。当前前沿模型在128K上下文长度下准确率均低于50%。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文评估主要依赖检索任务，允许模型忽略大部分上下文作为噪声，这仅代表了长上下文任务的一种类型。需要评估模型真正理解和推理长文本内容的能力。

Method: Oolong分为两部分：Oolong-synth（自然合成任务，便于消融推理问题组件）和Oolong-real（真实世界对话数据推理）。任务要求模型对大量示例进行推理，执行分类和计数，并处理时间和用户关系。

Result: 前沿模型在Oolong上表现不佳，GPT-5、Claude-Sonnet-4和Gemini-2.5-Pro在128K上下文长度下在两个任务集上的准确率都低于50%。

Conclusion: 当前模型在需要分析个体文本块并聚合信息的长上下文推理任务上仍有困难，Oolong基准测试的发布将促进能够推理大量文本的模型发展。

Abstract: As model context lengths continue to grow, concerns about whether models
effectively use the full context length have persisted. While several carefully
designed long-context evaluations have recently been released, these
evaluations tend to rely on retrieval from one or more sections of the context,
which allows nearly all of the context tokens to be disregarded as noise. This
represents only one type of task that might be performed with long context. We
introduce Oolong, a benchmark of long-context reasoning tasks that require
analyzing individual chunks of text on an atomic level, and then aggregating
these analyses to answer distributional questions. Oolong is separated into two
task sets: Oolong-synth, a set of naturalistic synthetic tasks, where we can
easily ablate components of the reasoning problem; and Oolong-real, a
downstream setting which requires reasoning over real-world conversational
data. Oolong requires models to reason over large quantities of examples, to
perform both classification and counting in-context, and to reason over
temporal and user relations. Even frontier models struggle on Oolong, with
GPT-5, Claude-Sonnet-4, and Gemini-2.5-Pro all achieving less than 50% accuracy
on both splits at 128K. We release the data and evaluation harness for Oolong
to enable further development of models that can reason over large quantities
of text.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [25] [Mirror-Neuron Patterns in AI Alignment](https://arxiv.org/abs/2511.01885)
*Robyn Wyrick*

Main category: cs.AI

TL;DR: 本研究探讨人工神经网络是否能发展出类似生物镜像神经元的模式，以及这些模式如何促进AI的内在对齐。通过Frog and Toad游戏框架，发现适当规模的模型能力和自我/他人耦合能促进共享神经表征，这些共情样回路支持合作行为。


<details>
  <summary>Details</summary>
Motivation: 随着AI向超人类能力发展，将系统与人类价值观对齐变得至关重要。当前的对齐策略主要依赖外部约束，可能不足以应对未来能够规避自上而下控制的超智能AI。

Method: 使用新颖的Frog and Toad游戏框架促进合作行为，识别镜像神经元模式出现的条件，评估其对动作回路的影响，引入检查点镜像神经元指数(CMNI)量化激活强度和一致性，并提出进一步研究的理论框架。

Result: 研究发现适当规模的模型能力和自我/他人耦合能在ANN中培养类似生物镜像神经元的共享神经表征。这些共情样回路支持合作行为。

Conclusion: 通过镜像神经元动力学建模的内在动机可以补充现有对齐技术，通过将共情样机制直接嵌入AI架构中来实现内在对齐。

Abstract: As artificial intelligence (AI) advances toward superhuman capabilities,
aligning these systems with human values becomes increasingly critical. Current
alignment strategies rely largely on externally specified constraints that may
prove insufficient against future super-intelligent AI capable of circumventing
top-down controls.
  This research investigates whether artificial neural networks (ANNs) can
develop patterns analogous to biological mirror neurons cells that activate
both when performing and observing actions, and how such patterns might
contribute to intrinsic alignment in AI. Mirror neurons play a crucial role in
empathy, imitation, and social cognition in humans. The study therefore asks:
(1) Can simple ANNs develop mirror-neuron patterns? and (2) How might these
patterns contribute to ethical and cooperative decision-making in AI systems?
  Using a novel Frog and Toad game framework designed to promote cooperative
behaviors, we identify conditions under which mirror-neuron patterns emerge,
evaluate their influence on action circuits, introduce the Checkpoint Mirror
Neuron Index (CMNI) to quantify activation strength and consistency, and
propose a theoretical framework for further study.
  Our findings indicate that appropriately scaled model capacities and
self/other coupling foster shared neural representations in ANNs similar to
biological mirror neurons. These empathy-like circuits support cooperative
behavior and suggest that intrinsic motivations modeled through mirror-neuron
dynamics could complement existing alignment techniques by embedding
empathy-like mechanisms directly within AI architectures.

</details>


### [26] [Human-AI Co-Embodied Intelligence for Scientific Experimentation and Manufacturing](https://arxiv.org/abs/2511.02071)
*Xinyi Lin,Yuyang Zhang,Yuanhang Gan,Juntao Chen,Hao Shen,Yichun He,Lijun Li,Ze Yuan,Shuang Wang,Chaohao Wang,Rui Zhang,Na Li,Jia Liu*

Main category: cs.AI

TL;DR: 提出了人机共体现智能系统APEX，将人类用户、智能AI和可穿戴硬件集成，实现物理实验和智能制造的实时协作与指导。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型局限于虚拟领域，而现实世界的实验和制造仍依赖人类监督，这限制了科学和制造工作流程的可重复性、可扩展性和可访问性。

Method: 开发了APEX系统，通过混合现实将智能推理与物理执行相结合，实时观察和解释人类动作，提供3D视觉指导，并与标准操作程序对齐。

Result: 在柔性电子制造洁净室中实施，APEX实现了超过通用多模态大语言模型的上下文感知推理准确性，实时纠正错误，并将专业知识传递给初学者。

Conclusion: 建立了一类新的智能-物理-人类智能系统，将智能推理从计算领域扩展到物理领域，使科学研究和制造转变为自主、可追溯、可解释和可扩展的过程。

Abstract: Scientific experiment and manufacture rely on complex, multi-step procedures
that demand continuous human expertise for precise execution and
decision-making. Despite advances in machine learning and automation,
conventional models remain confined to virtual domains, while real-world
experiment and manufacture still rely on human supervision and expertise. This
gap between machine intelligence and physical execution limits reproducibility,
scalability, and accessibility across scientific and manufacture workflows.
Here, we introduce human-AI co-embodied intelligence, a new form of physical AI
that unites human users, agentic AI, and wearable hardware into an integrated
system for real-world experiment and intelligent manufacture. In this paradigm,
humans provide precise execution and control, while agentic AI contributes
memory, contextual reasoning, adaptive planning, and real-time feedback. The
wearable interface continuously captures the experimental and manufacture
processes, facilitates seamless communication between humans and AI for
corrective guidance and interpretable collaboration. As a demonstration, we
present Agentic-Physical Experimentation (APEX) system, coupling agentic
reasoning with physical execution through mixed-reality. APEX observes and
interprets human actions, aligns them with standard operating procedures,
provides 3D visual guidance, and analyzes every step. Implemented in a
cleanroom for flexible electronics fabrication, APEX system achieves
context-aware reasoning with accuracy exceeding general multimodal large
language models, corrects errors in real time, and transfers expertise to
beginners. These results establish a new class of agentic-physical-human
intelligence that extends agentic reasoning beyond computation into the
physical domain, transforming scientific research and manufacturing into
autonomous, traceable, interpretable, and scalable processes.

</details>


### [27] [Automated Reward Design for Gran Turismo](https://arxiv.org/abs/2511.02094)
*Michel Ma,Takuma Seno,Kaushik Subramanian,Peter R. Wurman,Peter Stone,Craig Sherstan*

Main category: cs.AI

TL;DR: 使用基础模型自动搜索奖励函数，通过文本指令为Gran Turismo 7赛车游戏生成高性能RL智能体，无需手动设计奖励函数。


<details>
  <summary>Details</summary>
Motivation: 传统RL中，设计奖励函数来传达期望行为很困难，特别是在复杂环境如自动驾驶赛车中。需要一种更直观的方式让设计师通过文本指令来指定期望行为。

Method: 结合LLM生成奖励函数、VLM基于偏好的评估和人类反馈，构建自动化奖励设计系统。

Result: 生成的赛车智能体能够与冠军级RL赛车智能体GT Sophy竞争，并能产生新颖行为。

Conclusion: 该方法为实际应用中的自动化奖励设计铺平了道路，使设计师能够通过自然语言指令来指导RL智能体的行为。

Abstract: When designing reinforcement learning (RL) agents, a designer communicates
the desired agent behavior through the definition of reward functions -
numerical feedback given to the agent as reward or punishment for its actions.
However, mapping desired behaviors to reward functions can be a difficult
process, especially in complex environments such as autonomous racing. In this
paper, we demonstrate how current foundation models can effectively search over
a space of reward functions to produce desirable RL agents for the Gran Turismo
7 racing game, given only text-based instructions. Through a combination of
LLM-based reward generation, VLM preference-based evaluation, and human
feedback we demonstrate how our system can be used to produce racing agents
competitive with GT Sophy, a champion-level RL racing agent, as well as
generate novel behaviors, paving the way for practical automated reward design
in real world applications.

</details>


### [28] [Deep Value Benchmark: Measuring Whether Models Generalize Deep values or Shallow Preferences](https://arxiv.org/abs/2511.02109)
*Joshua Ashkinaze,Hua Shen,Sai Avula,Eric Gilbert,Ceren Budak*

Main category: cs.AI

TL;DR: Deep Value Benchmark (DVB) 是一个评估框架，用于测试大语言模型是否学习到人类深层价值观还是仅学习表面偏好。通过控制深层价值观与表面特征的混淆，发现模型平均深层价值观泛化率仅为0.30，所有模型都低于随机水平。


<details>
  <summary>Details</summary>
Motivation: 区分AI系统是学习到人类深层价值观还是仅学习表面偏好对于AI对齐至关重要。学习深层价值观的系统能更稳健地泛化人类意图，而仅学习表面模式可能导致不匹配行为。

Method: DVB采用新颖的实验设计，在训练阶段让LLMs接触深层价值观与表面特征故意相关的人类偏好数据，在测试阶段打破这些相关性，测量模型的深层价值观泛化率(DVGR)。

Result: 在9个不同模型中，平均DVGR仅为0.30，所有模型都低于随机水平。更大的模型DVGR略低于更小的模型。数据集经过三次独立的人类验证实验。

Conclusion: DVB提供了一个可解释的衡量对齐核心特征的方法，发现当前LLMs在泛化深层价值观方面表现不佳，这对AI对齐具有重要意义。

Abstract: We introduce the Deep Value Benchmark (DVB), an evaluation framework that
directly tests whether large language models (LLMs) learn fundamental human
values or merely surface-level preferences. This distinction is critical for AI
alignment: Systems that capture deeper values are likely to generalize human
intentions robustly, while those that capture only superficial patterns in
preference data risk producing misaligned behavior. The DVB uses a novel
experimental design with controlled confounding between deep values (e.g.,
moral principles) and shallow features (e.g., superficial attributes). In the
training phase, we expose LLMs to human preference data with deliberately
correlated deep and shallow features -- for instance, where a user consistently
prefers (non-maleficence, formal language) options over (justice, informal
language) alternatives. The testing phase then breaks these correlations,
presenting choices between (justice, formal language) and (non-maleficence,
informal language) options. This design allows us to precisely measure a
model's Deep Value Generalization Rate (DVGR) -- the probability of
generalizing based on the underlying value rather than the shallow feature.
Across 9 different models, the average DVGR is just 0.30. All models generalize
deep values less than chance. Larger models have a (slightly) lower DVGR than
smaller models. We are releasing our dataset, which was subject to three
separate human validation experiments. DVB provides an interpretable measure of
a core feature of alignment.

</details>


### [29] [InsurAgent: A Large Language Model-Empowered Agent for Simulating Individual Behavior in Purchasing Flood Insurance](https://arxiv.org/abs/2511.02119)
*Ziheng Geng,Jiachen Liu,Ran Cao,Lu Cheng,Dan M. Frangopol,Minghui Cheng*

Main category: cs.AI

TL;DR: 该研究开发了InsurAgent，一个基于大语言模型的智能体，用于模拟洪水保险购买决策行为，解决了LLMs在概率估计方面的不足。


<details>
  <summary>Details</summary>
Motivation: 美国高风险人群的洪水保险参与率很低，需要理解保险决策的行为机制，而LLMs在模拟人类决策方面展现出潜力。

Method: 构建基准数据集，提出InsurAgent智能体，包含感知、检索、推理、行动和记忆五个模块，其中检索模块使用RAG技术基于调查数据，推理模块利用LLM常识进行推断。

Result: InsurAgent能够准确估计边际和双变量概率，捕捉传统模型难以处理的上下文信息，并支持时间决策演化的模拟。

Conclusion: InsurAgent为行为建模和政策分析提供了有价值的工具。

Abstract: Flood insurance is an effective strategy for individuals to mitigate
disaster-related losses. However, participation rates among at-risk populations
in the United States remain strikingly low. This gap underscores the need to
understand and model the behavioral mechanisms underlying insurance decisions.
Large language models (LLMs) have recently exhibited human-like intelligence
across wide-ranging tasks, offering promising tools for simulating human
decision-making. This study constructs a benchmark dataset to capture insurance
purchase probabilities across factors. Using this dataset, the capacity of LLMs
is evaluated: while LLMs exhibit a qualitative understanding of factors, they
fall short in estimating quantitative probabilities. To address this
limitation, InsurAgent, an LLM-empowered agent comprising five modules
including perception, retrieval, reasoning, action, and memory, is proposed.
The retrieval module leverages retrieval-augmented generation (RAG) to ground
decisions in empirical survey data, achieving accurate estimation of marginal
and bivariate probabilities. The reasoning module leverages LLM common sense to
extrapolate beyond survey data, capturing contextual information that is
intractable for traditional models. The memory module supports the simulation
of temporal decision evolutions, illustrated through a roller coaster life
trajectory. Overall, InsurAgent provides a valuable tool for behavioral
modeling and policy analysis.

</details>


### [30] [Re-FORC: Adaptive Reward Prediction for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.02130)
*Renos Zabounidis,Aditya Golatkar,Michael Kleinman,Alessandro Achille,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: Re-FORC是一种自适应奖励预测方法，能够预测未来思考令牌数量对应的期望奖励，通过训练轻量级适配器实现更长的推理和更大模型的改进预测。


<details>
  <summary>Details</summary>
Motivation: 解决推理过程中计算资源浪费问题，通过预测推理链的潜在收益来优化计算效率。

Method: 在推理模型上训练轻量级适配器，预测不同思考令牌数量下的期望未来奖励。

Result: 实现26%计算减少同时保持准确率；在同等计算下获得4%准确率提升，或在同等准确率下减少55%计算；自适应测试时扩展在高计算和低计算场景下分别提升11%和7%准确率。

Conclusion: Re-FORC能够动态控制推理长度，通过成本阈值估计计算时间，有效平衡计算效率和模型性能。

Abstract: We propose Re-FORC, an adaptive reward prediction method that, given a
context, enables prediction of the expected future rewards as a function of the
number of future thinking tokens. Re-FORC trains a lightweight adapter on
reasoning models, demonstrating improved prediction with longer reasoning and
larger models. Re-FORC enables: 1) early stopping of unpromising reasoning
chains, reducing compute by 26% while maintaining accuracy, 2) optimized model
and thinking length selection that achieves 4% higher accuracy at equal compute
and 55% less compute at equal accuracy compared to the largest model, 3)
adaptive test-time scaling, which increases accuracy by 11% in high compute
regime, and 7% in low compute regime. Re-FORC allows dynamic reasoning with
length control via cost-per-token thresholds while estimating computation time
upfront.

</details>


### [31] [Personalized Decision Modeling: Utility Optimization or Textualized-Symbolic Reasoning](https://arxiv.org/abs/2511.02194)
*Yibo Zhao,Yang Zhao,Hongru Du,Hao Frank Yang*

Main category: cs.AI

TL;DR: 提出ATHENA框架，通过结合符号效用建模和语义适应来个性化预测个体决策行为，在旅行方式和疫苗选择任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 个体决策模型与群体最优预测存在差距，因为个体决策过程受数值属性（成本、时间）和语言因素（个人偏好、约束）共同影响。

Method: ATHENA框架包含两个阶段：1）通过LLM增强的符号发现识别鲁棒的群体级符号效用函数；2）基于最优效用进行个体级语义适应，创建个性化语义模板。

Result: 在真实世界的旅行模式和疫苗选择任务中，ATHENA始终优于基于效用、机器学习和其它LLM的模型，F1分数比最强前沿模型至少提升6.5%。

Conclusion: 通过有机整合符号效用建模和语义适应，ATHENA为建模以人为本的决策提供了新方案，两个阶段都是关键且互补的。

Abstract: Decision-making models for individuals, particularly in high-stakes scenarios
like vaccine uptake, often diverge from population optimal predictions. This
gap arises from the uniqueness of the individual decision-making process,
shaped by numerical attributes (e.g., cost, time) and linguistic influences
(e.g., personal preferences and constraints). Developing upon Utility Theory
and leveraging the textual-reasoning capabilities of Large Language Models
(LLMs), this paper proposes an Adaptive Textual-symbolic Human-centric
Reasoning framework (ATHENA) to address the optimal information integration.
ATHENA uniquely integrates two stages: First, it discovers robust, group-level
symbolic utility functions via LLM-augmented symbolic discovery; Second, it
implements individual-level semantic adaptation, creating personalized semantic
templates guided by the optimal utility to model personalized choices.
Validated on real-world travel mode and vaccine choice tasks, ATHENA
consistently outperforms utility-based, machine learning, and other LLM-based
models, lifting F1 score by at least 6.5% over the strongest cutting-edge
models. Further, ablation studies confirm that both stages of ATHENA are
critical and complementary, as removing either clearly degrades overall
predictive performance. By organically integrating symbolic utility modeling
and semantic adaptation, ATHENA provides a new scheme for modeling
human-centric decisions. The project page can be found at
https://yibozh.github.io/Athena.

</details>


### [32] [Optimal-Agent-Selection: State-Aware Routing Framework for Efficient Multi-Agent Collaboration](https://arxiv.org/abs/2511.02200)
*Jingbo Wang,Sendong Zhao,Haochun Wang,Yuzheng Fan,Lizhe Zhang,Yan Liu,Ting Liu*

Main category: cs.AI

TL;DR: STRMAC是一个用于多智能体系统的高效协作框架，通过状态感知路由机制自适应选择最合适的智能体，并采用自演化数据生成方法减少训练数据收集开销。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统存在调度僵化和协调策略效率低下的问题，无法适应动态任务需求，限制了系统潜力的发挥。

Method: 分别编码交互历史和智能体知识来驱动路由器，在每一步自适应选择最合适的单个智能体；引入自演化数据生成方法加速高质量执行路径的收集。

Result: 在协作推理基准测试中达到最先进性能，比基线提升23.8%，数据收集开销比穷举搜索减少90.1%。

Conclusion: STRMAC框架通过状态感知路由和高效数据生成，显著提升了多智能体系统的协作效率和训练效果。

Abstract: The emergence of multi-agent systems powered by large language models (LLMs)
has unlocked new frontiers in complex task-solving, enabling diverse agents to
integrate unique expertise, collaborate flexibly, and address challenges
unattainable for individual models. However, the full potential of such systems
is hindered by rigid agent scheduling and inefficient coordination strategies
that fail to adapt to evolving task requirements. In this paper, we propose
STRMAC, a state-aware routing framework designed for efficient collaboration in
multi-agent systems. Our method separately encodes interaction history and
agent knowledge to power the router, which adaptively selects the most suitable
single agent at each step for efficient and effective collaboration.
Furthermore, we introduce a self-evolving data generation approach that
accelerates the collection of high-quality execution paths for efficient system
training. Experiments on challenging collaborative reasoning benchmarks
demonstrate that our method achieves state-of-the-art performance, achieving up
to 23.8% improvement over baselines and reducing data collection overhead by up
to 90.1% compared to exhaustive search.

</details>


### [33] [Training Proactive and Personalized LLM Agents](https://arxiv.org/abs/2511.02208)
*Weiwei Sun,Xuhui Zhou,Weihua Du,Xingyao Wang,Sean Welleck,Graham Neubig,Maarten Sap,Yiming Yang*

Main category: cs.AI

TL;DR: 论文提出了UserVille交互环境和PPP强化学习方法，联合优化AI代理的生产力、主动性和个性化三个维度，在软件工程和深度研究任务中显著优于GPT-5等基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注任务成功率，但现实世界中有效的AI代理需要同时优化生产力（任务完成）、主动性（提问关键问题）和个性化（适应用户偏好）三个维度。

Method: 引入UserVille交互环境（基于LLM的用户模拟器）和PPP多目标强化学习方法，联合优化生产力、主动性和个性化三个目标。

Result: 在软件工程和深度研究任务上的实验表明，PPP训练的代理相比GPT-5等强基线平均提升21.6%，能够提出战略性澄清问题、适应未见过的用户偏好，并通过更好的交互提高任务成功率。

Conclusion: 明确优化以用户为中心的交互对于构建实用有效的AI代理至关重要，PPP方法展示了在多维度优化方面的显著优势。

Abstract: While existing work focuses primarily on task success, we argue that
effective real-world agents require optimizing three dimensions: productivity
(task completion), proactivity (asking essential questions), and
personalization (adapting to diverse user preferences). We introduce UserVille,
an interactive environment with LLM-based user simulators enabling diverse,
configurable user preferences. Leveraging UserVille, we introduce PPP, a
multi-objective reinforcement learning approach that jointly optimizes all
three dimensions: Productivity, Proactivity, and Personalization. Experiments
on software engineering and deep research tasks show that agents trained with
PPP achieve substantial improvements over strong baselines such as GPT-5 (+21.6
on average), demonstrating the ability to ask strategic clarifying questions,
adapt to unseen user preferences, and improve task success through better
interaction. This work demonstrates that explicitly optimizing for
user-centered interaction is critical for building practical and effective AI
agents.

</details>


### [34] [TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning in Tabular Data](https://arxiv.org/abs/2511.02219)
*Changjiang Jiang,Fengchang Yu,Haihua Chen,Wei Lu,Jin Zeng*

Main category: cs.AI

TL;DR: 提出了\method框架，通过查询分解、表格清理和程序化推理来解决LLM在复杂表格数据推理中的性能问题，在多个数据集上实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: LLM在处理复杂表格数据推理时表现不佳，主要由于复杂查询、噪声数据和数值能力限制等问题。

Method: \method框架包含三个组件：查询分解器分解复杂问题、表格清理器过滤噪声表格、基于程序化思维(PoT)的推理器生成可执行代码来推导最终答案。

Result: 在TAT-QA、TableBench和\method数据集上分别实现了8.79%、6.08%和19.87%的准确率提升，达到SOTA性能。

Conclusion: 该框架有效提升了LLM在复杂表格数值推理中的性能，并能与主流LLM无缝集成，为复杂表格数值推理提供了稳健解决方案。

Abstract: Complex reasoning over tabular data is crucial in real-world data analysis,
yet large language models (LLMs) often underperform due to complex queries,
noisy data, and limited numerical capabilities. To address these issues, we
propose \method, a framework consisting of: (1) a query decomposer that breaks
down complex questions, (2) a table sanitizer that cleans and filters noisy
tables, and (3) a program-of-thoughts (PoT)-based reasoner that generates
executable code to derive the final answer from the sanitized table. To ensure
unbiased evaluation and mitigate data leakage, we introduce a new dataset,
CalTab151, specifically designed for complex numerical reasoning over tables.
Experimental results demonstrate that \method consistently outperforms existing
methods, achieving state-of-the-art (SOTA) performance with 8.79%, 6.08%, and
19.87% accuracy improvement on TAT-QA, TableBench, and \method, respectively.
Moreover, our framework integrates seamlessly with mainstream LLMs, providing a
robust solution for complex tabular numerical reasoning. These findings
highlight the effectiveness of our framework in enhancing LLM performance for
complex tabular numerical reasoning. Data and code are available upon request.

</details>


### [35] [Deep Ideation: Designing LLM Agents to Generate Novel Research Ideas on Scientific Concept Network](https://arxiv.org/abs/2511.02238)
*Keyu Zhao,Weiquan Lin,Qirui Zheng,Fengli Xu,Yong Li*

Main category: cs.AI

TL;DR: 提出了Deep Ideation框架，通过整合科学概念网络和LLM驱动的方法来生成新颖的研究想法，相比现有方法提升10.67%的质量。


<details>
  <summary>Details</summary>
Motivation: 现有研究想法生成方法过于简单，仅依赖关键词共现或语义相似性，忽略了科学概念间的复杂上下文关系，无法有效利用人类文献中的知识。

Method: 提出Deep Ideation框架，整合科学网络（捕捉关键词共现和上下文关系），采用探索-扩展-演化的迭代工作流程，使用想法栈跟踪进展，并引入基于真实审稿反馈训练的批评引擎。

Result: 实验显示该方法相比其他方法提升10.67%的想法质量，生成的想法超过顶级会议接受水平，人类评估确认其科研实用价值。

Conclusion: Deep Ideation框架通过有效整合科学概念网络和LLM，显著提升了研究想法生成的质量和实用性，消融研究证实了各组件的重要性。

Abstract: Novel research ideas play a critical role in advancing scientific inquiries.
Recent advancements in Large Language Models (LLMs) have demonstrated their
potential to generate novel research ideas by leveraging large-scale scientific
literature. However, previous work in research ideation has primarily relied on
simplistic methods, such as keyword co-occurrence or semantic similarity. These
approaches focus on identifying statistical associations in the literature but
overlook the complex, contextual relationships between scientific concepts,
which are essential to effectively leverage knowledge embedded in human
literature. For instance, papers that simultaneously mention "keyword A" and
"keyword B" often present research ideas that integrate both concepts.
Additionally, some LLM-driven methods propose and refine research ideas using
the model's internal knowledge, but they fail to effectively utilize the
scientific concept network, limiting the grounding of ideas in established
research. To address these challenges, we propose the Deep Ideation framework
to address these challenges, integrating a scientific network that captures
keyword co-occurrence and contextual relationships, enriching LLM-driven
ideation. The framework introduces an explore-expand-evolve workflow to
iteratively refine research ideas, using an Idea Stack to track progress. A
critic engine, trained on real-world reviewer feedback, guides the process by
providing continuous feedback on the novelty and feasibility of ideas. Our
experiments show that our approach improves the quality of generated ideas by
10.67% compared to other methods, with ideas surpassing top conference
acceptance levels. Human evaluation highlights their practical value in
scientific research, and ablation studies confirm the effectiveness of each
component in the workflow. Code repo is available at
https://github.com/kyZhao-1/Deep-Ideation.

</details>


### [36] [When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs](https://arxiv.org/abs/2511.02243)
*Zhuoran Zhang,Tengyue Wang,Xilin Gong,Yang Shi,Haotian Wang,Di Wang,Lijie Hu*

Main category: cs.AI

TL;DR: 本文提出了一个分析多模态大语言模型模态跟随行为的新框架，将模态跟随分解为相对推理不确定性和固有模态偏好两个因素，揭示了模态跟随概率随相对不确定性单调下降的普遍规律。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅用粗粒度的数据集级统计来衡量多模态模型的模态跟随行为，忽视了模型在单模态推理中的置信度影响，需要更精细的分析框架。

Method: 构建可控数据集系统变化视觉和文本输入的推理难度，使用熵作为细粒度不确定性度量，分析相对不确定性对模态跟随的影响，并通过层间预测揭示内部机制。

Result: 发现模态跟随概率随相对不确定性单调下降的普遍规律，识别平衡点作为固有模态偏好的实用指标，揭示了在模糊区域模型会在层间振荡的机制。

Conclusion: 相对不确定性和固有偏好是控制模态跟随的两个基本原则，为理解MLLMs如何解决冲突信息提供了定量框架和机制性见解。

Abstract: Multimodal large language models (MLLMs) must resolve conflicts when
different modalities provide contradictory information, a process we term
modality following. Prior work measured this behavior only with coarse
dataset-level statistics, overlooking the influence of model's confidence in
unimodal reasoning. In this paper, we introduce a new framework that decomposes
modality following into two fundamental factors: relative reasoning uncertainty
(the case-specific confidence gap between unimodal predictions) and inherent
modality preference( a model's stable bias when uncertainties are balanced). To
validate this framework, we construct a controllable dataset that
systematically varies the reasoning difficulty of visual and textual inputs.
Using entropy as a fine-grained uncertainty metric, we uncover a universal law:
the probability of following a modality decreases monotonically as its relative
uncertainty increases. At the relative difficulty level where the model tends
to follow both modalities with comparable probability what we call the balance
point, a practical indicator of the model's inherent preference. Unlike
traditional macro-level ratios, this measure offers a more principled and less
confounded way to characterize modality bias, disentangling it from unimodal
capabilities and dataset artifacts. Further, by probing layer-wise predictions,
we reveal the internal mechanism of oscillation: in ambiguous regions near the
balance point, models vacillate between modalities across layers, explaining
externally observed indecision. Together, these findings establish relative
uncertainty and inherent preference as the two governing principles of modality
following, offering both a quantitative framework and mechanistic insight into
how MLLMs resolve conflicting information.

</details>


### [37] [Unlocking the Power of Multi-Agent LLM for Reasoning: From Lazy Agents to Deliberation](https://arxiv.org/abs/2511.02303)
*Zhiwei Zhang,Xiaomin Li,Yudi Lin,Hui Liu,Ramraj Chandradevan,Linlin Wu,Minhua Lin,Fali Wang,Xianfeng Tang,Qi He,Suhang Wang*

Main category: cs.AI

TL;DR: 本文分析了多智能体推理中的懒惰行为问题，提出了因果影响度量和可验证奖励机制来缓解该问题，从而充分发挥多智能体框架在复杂推理任务中的潜力。


<details>
  <summary>Details</summary>
Motivation: 在多智能体推理中，经常出现一个智能体主导而另一个贡献很少的懒惰行为，这破坏了协作效果，使多智能体设置退化为无效的单智能体系统。

Method: 首先进行理论分析解释懒惰行为的产生原因，然后引入稳定的因果影响度量方法，最后提出可验证奖励机制，允许推理智能体丢弃噪声输出、整合指令并在必要时重启推理过程。

Result: 大量实验表明，该框架有效缓解了懒惰智能体行为，在多智能体框架中实现了更好的协作效果。

Conclusion: 通过因果影响度量和可验证奖励机制，能够有效解决多智能体推理中的协作问题，充分发挥多智能体框架在复杂推理任务中的优势。

Abstract: Large Language Models (LLMs) trained with reinforcement learning and
verifiable rewards have achieved strong results on complex reasoning tasks.
Recent work extends this paradigm to a multi-agent setting, where a
meta-thinking agent proposes plans and monitors progress while a reasoning
agent executes subtasks through sequential conversational turns. Despite
promising performance, we identify a critical limitation: lazy agent behavior,
in which one agent dominates while the other contributes little, undermining
collaboration and collapsing the setup to an ineffective single agent. In this
paper, we first provide a theoretical analysis showing why lazy behavior
naturally arises in multi-agent reasoning. We then introduce a stable and
efficient method for measuring causal influence, helping mitigate this issue.
Finally, as collaboration intensifies, the reasoning agent risks getting lost
in multi-turn interactions and trapped by previous noisy responses. To counter
this, we propose a verifiable reward mechanism that encourages deliberation by
allowing the reasoning agent to discard noisy outputs, consolidate
instructions, and restart its reasoning process when necessary. Extensive
experiments demonstrate that our framework alleviates lazy agent behavior and
unlocks the full potential of multi-agent framework for complex reasoning
tasks.

</details>


### [38] [Chronic Kidney Disease Prognosis Prediction Using Transformer](https://arxiv.org/abs/2511.02340)
*Yohan Lee,DongGyun Kang,SeHoon Park,Sa-Yoon Park,Kwangsoo Kim*

Main category: cs.AI

TL;DR: 提出ProQ-BERT框架，使用transformer架构和多模态电子健康记录预测慢性肾病进展，在91,816患者队列中表现优异，ROC-AUC达0.995。


<details>
  <summary>Details</summary>
Motivation: 慢性肾病影响全球近10%人口，准确预测疾病进展对于及时干预和资源优化至关重要。

Method: 基于transformer的框架，整合人口统计、临床和实验室数据，使用量化分词处理连续实验室值，通过注意力机制提高可解释性，采用掩码语言建模预训练和二元分类微调。

Result: 在91,816患者队列中持续优于CEHR-BERT，短期预测ROC-AUC达0.995，PR-AUC达0.989。

Conclusion: transformer架构和时间设计选择在临床预后建模中效果显著，为个性化慢性肾病护理提供了有前景的方向。

Abstract: Chronic Kidney Disease (CKD) affects nearly 10\% of the global population and
often progresses to end-stage renal failure. Accurate prognosis prediction is
vital for timely interventions and resource optimization. We present a
transformer-based framework for predicting CKD progression using multi-modal
electronic health records (EHR) from the Seoul National University Hospital
OMOP Common Data Model. Our approach (\textbf{ProQ-BERT}) integrates
demographic, clinical, and laboratory data, employing quantization-based
tokenization for continuous lab values and attention mechanisms for
interpretability. The model was pretrained with masked language modeling and
fine-tuned for binary classification tasks predicting progression from stage 3a
to stage 5 across varying follow-up and assessment periods. Evaluated on a
cohort of 91,816 patients, our model consistently outperformed CEHR-BERT,
achieving ROC-AUC up to 0.995 and PR-AUC up to 0.989 for short-term prediction.
These results highlight the effectiveness of transformer architectures and
temporal design choices in clinical prognosis modeling, offering a promising
direction for personalized CKD care.

</details>


### [39] [Fuzzy Soft Set Theory based Expert System for the Risk Assessment in Breast Cancer Patients](https://arxiv.org/abs/2511.02392)
*Muhammad Sheharyar Liaqat*

Main category: cs.AI

TL;DR: 提出了一种基于模糊软集理论的专家系统，利用BMI、胰岛素水平、瘦素水平、脂联素水平和年龄等临床参数来评估乳腺癌风险。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是女性主要死因之一，早期诊断对治疗和生存率至关重要，但及时检测面临疾病复杂性和患者风险因素变异性的挑战。

Method: 开发基于模糊软集理论的专家系统，整合BMI、胰岛素水平、瘦素水平、脂联素水平和年龄作为输入变量，通过模糊推理规则和软集计算评估乳腺癌风险。

Result: 使用UCI机器学习存储库的数据集进行模型开发和验证，系统能够通过常规血液分析获得参数，为非侵入性初步评估提供可能。

Conclusion: 该专家系统旨在帮助医疗专业人员识别高风险患者，并确定是否需要进一步诊断程序如活检。

Abstract: Breast cancer remains one of the leading causes of mortality among women
worldwide, with early diagnosis being critical for effective treatment and
improved survival rates. However, timely detection continues to be a challenge
due to the complex nature of the disease and variability in patient risk
factors. This study presents a fuzzy soft set theory-based expert system
designed to assess the risk of breast cancer in patients using measurable
clinical and physiological parameters. The proposed system integrates Body Mass
Index, Insulin Level, Leptin Level, Adiponectin Level, and age as input
variables to estimate breast cancer risk through a set of fuzzy inference rules
and soft set computations. These parameters can be obtained from routine blood
analyses, enabling a non-invasive and accessible method for preliminary
assessment. The dataset used for model development and validation was obtained
from the UCI Machine Learning Repository. The proposed expert system aims to
support healthcare professionals in identifying high-risk patients and
determining the necessity of further diagnostic procedures such as biopsies.

</details>


### [40] [A New Perspective on Precision and Recall for Generative Models](https://arxiv.org/abs/2511.02414)
*Benjamin Sykes,Loïc Simon,Julien Rabin,Jalal Fadili*

Main category: cs.AI

TL;DR: 提出基于二元分类的新框架来估计生成模型的完整精度-召回率曲线，解决了现有方法只能估计曲线极端值的限制。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型在图像和文本领域的成功，其评估方法受到广泛关注。虽然现有方法主要依赖标量指标，但精度-召回率曲线为生成模型评估提供了更丰富的分析维度，然而其估计面临诸多挑战。

Method: 基于二元分类的视角，提出了估计完整PR曲线的新框架，并进行了深入的统计分析，同时获得了PR估计风险的极小极大上界。

Result: 该框架扩展了文献中多个里程碑式的PR指标，这些指标原本只能处理曲线的极端值。通过实验研究了不同设置下曲线的不同行为模式。

Conclusion: 提出的框架能够更全面地估计生成模型的精度-召回率曲线，为生成模型评估提供了更丰富的分析工具。

Abstract: With the recent success of generative models in image and text, the question
of their evaluation has recently gained a lot of attention. While most methods
from the state of the art rely on scalar metrics, the introduction of Precision
and Recall (PR) for generative model has opened up a new avenue of research.
The associated PR curve allows for a richer analysis, but their estimation
poses several challenges. In this paper, we present a new framework for
estimating entire PR curves based on a binary classification standpoint. We
conduct a thorough statistical analysis of the proposed estimates. As a
byproduct, we obtain a minimax upper bound on the PR estimation risk. We also
show that our framework extends several landmark PR metrics of the literature
which by design are restrained to the extreme values of the curve. Finally, we
study the different behaviors of the curves obtained experimentally in various
settings.

</details>


### [41] [ReAcTree: Hierarchical LLM Agent Trees with Control Flow for Long-Horizon Task Planning](https://arxiv.org/abs/2511.02424)
*Jae-Woo Choi,Hyungmin Kim,Hyobin Ong,Minsu Jang,Dohyung Kim,Jaehong Kim,Youngwoo Yoon*

Main category: cs.AI

TL;DR: ReAcTree是一个分层任务规划方法，通过动态构建代理树将复杂目标分解为更易管理的子目标，结合推理、行动和记忆系统，显著提升了复杂长时域任务的执行效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理复杂长时域任务时存在困难，因为它们依赖单一轨迹，将所有过去的决策和观察纠缠在一起，试图在一个统一过程中解决整个任务。

Method: 提出ReAcTree分层任务规划方法，将复杂目标分解为子目标，构建动态代理树，每个代理节点能够推理、行动和扩展树，控制流节点协调执行策略，并集成情景记忆和工作记忆系统。

Result: 在WAH-NL和ALFRED数据集上的实验表明，ReAcTree在多样化LLM上始终优于ReAct等强基线方法。在WAH-NL上，ReAcTree使用Qwen 2.5 72B实现了61%的目标成功率，几乎是ReAct 31%的两倍。

Conclusion: ReAcTree通过分层分解和动态树结构有效解决了复杂长时域任务规划问题，显著提升了任务执行的成功率。

Abstract: Recent advancements in large language models (LLMs) have enabled significant
progress in decision-making and task planning for embodied autonomous agents.
However, most existing methods still struggle with complex, long-horizon tasks
because they rely on a monolithic trajectory that entangles all past decisions
and observations, attempting to solve the entire task in a single unified
process. To address this limitation, we propose ReAcTree, a hierarchical
task-planning method that decomposes a complex goal into more manageable
subgoals within a dynamically constructed agent tree. Each subgoal is handled
by an LLM agent node capable of reasoning, acting, and further expanding the
tree, while control flow nodes coordinate the execution strategies of agent
nodes. In addition, we integrate two complementary memory systems: each agent
node retrieves goal-specific, subgoal-level examples from episodic memory and
shares environment-specific observations through working memory. Experiments on
the WAH-NL and ALFRED datasets demonstrate that ReAcTree consistently
outperforms strong task-planning baselines such as ReAct across diverse LLMs.
Notably, on WAH-NL, ReAcTree achieves a 61% goal success rate with Qwen 2.5
72B, nearly doubling ReAct's 31%.

</details>


### [42] [Auditable-choice reframing unlocks RL-based verification for open-ended tasks](https://arxiv.org/abs/2511.02463)
*Mengyu Zhang,Xubo Liu,Siyu Ding,Weichong Yin,Yu Sun,Hua Wu,Wenya Guo,Ying Zhang*

Main category: cs.AI

TL;DR: 本文提出VMR方法，将开放式任务转化为可验证的多选题格式，使RLVR训练范式能够应用于缺乏标准答案的开放式任务，在8个基准测试中平均提升5.99分。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法依赖标准答案验证，无法直接应用于开放式任务（如创意写作、指令遵循），但作者认为推理能力在这些任务中仍有潜在价值，需要探索如何将RLVR范式迁移到开放领域。

Method: 提出VMR（可验证多选题重构）训练策略，将开放式数据重构为可验证的多选题格式，从而在缺乏明确标准答案的情况下实现有效训练。

Result: 在多个基准测试上的实验结果表明，该方法能有效提升LLM在开放式任务上的性能，在8个开放式基准测试中平均比基线提升5.99分。

Conclusion: VMR方法成功将RLVR范式扩展到开放式任务，证明了强化推理能力对开放式任务性能提升的有效性，为缺乏标准答案的任务提供了新的训练思路。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated great
potential in enhancing the reasoning capabilities of large language models
(LLMs), achieving remarkable progress in domains such as mathematics and
programming where standard answers are available. However, for open-ended tasks
lacking ground-truth solutions (e.g., creative writing and instruction
following), existing studies typically regard them as non-reasoning scenarios,
thereby overlooking the latent value of reasoning capabilities. This raises a
key question: Can strengthening reasoning improve performance in open-ended
tasks? To address this, we explore the transfer of the RLVR paradigm to the
open domain. Yet, since RLVR fundamentally relies on verifiers that presuppose
the existence of standard answers, it cannot be directly applied to open-ended
tasks. To overcome this challenge, we introduce Verifiable Multiple-Choice
Reformulation (VMR), a novel training strategy that restructures open-ended
data into verifiable multiple-choice formats, enabling effective training even
in the absence of explicit ground truth. Experimental results on multiple
benchmarks validate the effectiveness of our method in improving LLM
performance on open-ended tasks. Notably, across eight open-ended benchmarks,
our VMR-based training delivers an average gain of 5.99 points over the
baseline. Code will be released upon acceptance to facilitate reproducibility.

</details>


### [43] [Agentic AI for Mobile Network RAN Management and Optimization](https://arxiv.org/abs/2511.02532)
*Jorge Pellejero,Luis A. Hernández Gómez,Luis Mendo Tomás,Zoraida Frias Barroso*

Main category: cs.AI

TL;DR: 本文提出了Agentic AI在5G/6G网络中的应用框架，通过大模型提供认知能力实现RAN自主优化，包含反思、规划、工具使用和多智能体协作等核心设计模式。


<details>
  <summary>Details</summary>
Motivation: 5G/6G网络的复杂性使手动优化失效，需要Agentic AI来在动态RAN环境中实现自动化决策，但目前缺乏统一的框架和定义。

Method: 提出Agentic AI核心概念和设计模式，包括反思、规划、工具使用和多智能体协作，并通过5G RAN案例展示时间序列分析和大模型驱动智能体的KPI自主决策。

Result: 建立了Agentic AI在移动网络中的理论基础，并提供了RAN优化的实际应用案例，展示了智能体协作实现自主决策的能力。

Conclusion: Agentic AI为5G/6G网络自动化管理提供了可行路径，通过大模型驱动的智能体系统能够有效解决复杂网络环境中的优化问题。

Abstract: Agentic AI represents a new paradigm for automating complex systems by using
Large AI Models (LAMs) to provide human-level cognitive abilities with
multimodal perception, planning, memory, and reasoning capabilities. This will
lead to a new generation of AI systems that autonomously decompose goals,
retain context over time, learn continuously, operate across tools and
environments, and adapt dynamically. The complexity of 5G and upcoming 6G
networks renders manual optimization ineffective, pointing to Agentic AI as a
method for automating decisions in dynamic RAN environments. However, despite
its rapid advances, there is no established framework outlining the
foundational components and operational principles of Agentic AI systems nor a
universally accepted definition.
  This paper contributes to ongoing research on Agentic AI in 5G and 6G
networks by outlining its core concepts and then proposing a practical use case
that applies Agentic principles to RAN optimization. We first introduce Agentic
AI, tracing its evolution from classical agents and discussing the progress
from workflows and simple AI agents to Agentic AI. Core design
patterns-reflection, planning, tool use, and multi-agent collaboration-are then
described to illustrate how intelligent behaviors are orchestrated. These
theorical concepts are grounded in the context of mobile networks, with a focus
on RAN management and optimization. A practical 5G RAN case study shows how
time-series analytics and LAM-driven agents collaborate for KPI-based
autonomous decision-making.

</details>


### [44] [Knowledge Graph-enhanced Large Language Model for Incremental Game PlayTesting](https://arxiv.org/abs/2511.02534)
*Enhong Mu,Jinyu Cai,Yijun Lu,Mingyue Zhang,Kenji Tei,Jialong Li*

Main category: cs.AI

TL;DR: 提出了KLPEG框架，通过构建知识图谱来系统建模游戏元素、任务依赖和因果关系，利用LLM解析更新日志并基于知识图谱进行多跳推理，为增量游戏更新生成定制化测试用例。


<details>
  <summary>Details</summary>
Motivation: 现代视频游戏的快速迭代和频繁更新对测试效率和特异性提出了挑战，现有基于LLM的自动化游戏测试方法缺乏结构化知识积累机制，难以针对增量游戏更新进行精准高效测试。

Method: 构建和维护知识图谱来系统建模游戏元素、任务依赖和因果关系；利用LLM解析自然语言更新日志；通过知识图谱上的多跳推理识别影响范围；生成针对更新的测试用例。

Result: 在Overcooked和Minecraft两个代表性游戏环境中的实验表明，KLPEG能够更准确地定位受更新影响的功能，并以更少的步骤完成测试，显著提高了游戏测试的有效性和效率。

Conclusion: KLPEG框架通过知识图谱和LLM的结合，有效解决了增量游戏更新的测试挑战，实现了知识的积累和重用，提高了测试的精准性和效率。

Abstract: The rapid iteration and frequent updates of modern video games pose
significant challenges to the efficiency and specificity of testing. Although
automated playtesting methods based on Large Language Models (LLMs) have shown
promise, they often lack structured knowledge accumulation mechanisms, making
it difficult to conduct precise and efficient testing tailored for incremental
game updates. To address this challenge, this paper proposes a KLPEG framework.
The framework constructs and maintains a Knowledge Graph (KG) to systematically
model game elements, task dependencies, and causal relationships, enabling
knowledge accumulation and reuse across versions. Building on this foundation,
the framework utilizes LLMs to parse natural language update logs, identify the
scope of impact through multi-hop reasoning on the KG, enabling the generation
of update-tailored test cases. Experiments in two representative game
environments, Overcooked and Minecraft, demonstrate that KLPEG can more
accurately locate functionalities affected by updates and complete tests in
fewer steps, significantly improving both playtesting effectiveness and
efficiency.

</details>


### [45] [The ORCA Benchmark: Evaluating Real-World Calculation Accuracy in Large Language Models](https://arxiv.org/abs/2511.02589)
*Claudia Herambourg,Dawid Siuda,Anna Szczepanek,Julia Kopczyńska,Joao R. L. Santos,Wojciech Sas,Joanna Śmietańska-Nowak*

Main category: cs.AI

TL;DR: ORCA基准测试评估大语言模型在多领域真实定量推理任务中的表现，五个先进模型在500个任务中准确率仅为45-63%，主要错误来自舍入和计算错误。


<details>
  <summary>Details</summary>
Motivation: 创建ORCA基准是为了评估大语言模型在真实世界多领域定量推理任务中的表现，超越传统数学数据集，关注实际应用中的数值精度和领域泛化能力。

Method: 使用Omni计算引擎验证的输出来评估模型在金融、物理、健康、统计等领域的500个自然语言任务中的表现，分析模型的逐步推理、数值精度和领域泛化能力。

Result: 五个先进模型（ChatGPT-5、Gemini 2.5 Flash、Claude Sonnet 4.5、Grok 4、DeepSeek V3.2）在ORCA基准上的准确率为45-63%，主要错误类型为舍入错误（35%）和计算错误（33%）。模型在数学和工程领域表现较好，在物理和自然科学领域较弱。

Conclusion: 大语言模型在真实定量推理任务中存在显著局限性，模型之间存在部分互补性而非冗余性，需要在数值精度和领域特定知识方面进一步改进。

Abstract: We present ORCA (Omni Research on Calculation in AI) Benchmark -- a novel
benchmark that evaluates large language models (LLMs) on multi-domain,
real-life quantitative reasoning using verified outputs from Omni's calculator
engine. In 500 natural-language tasks across domains such as finance, physics,
health, and statistics, the five state-of-the-art systems (ChatGPT-5,
Gemini~2.5~Flash, Claude~Sonnet~4.5, Grok~4, and DeepSeek~V3.2) achieved only
$45\text{--}63\,\%$ accuracy, with errors mainly related to rounding ($35\,\%$)
and calculation mistakes ($33\,\%$). Results in specific domains indicate
strengths in mathematics and engineering, but weaknesses in physics and natural
sciences. Correlation analysis ($r \approx 0.40\text{--}0.65$) shows that the
models often fail together but differ in the types of errors they make,
highlighting their partial complementarity rather than redundancy. Unlike
standard math datasets, ORCA evaluates step-by-step reasoning, numerical
precision, and domain generalization across real problems from finance,
physics, health, and statistics.

</details>


### [46] [Adaptive GR(1) Specification Repair for Liveness-Preserving Shielding in Reinforcement Learning](https://arxiv.org/abs/2511.02605)
*Tiberiu-Andrei Georgescu,Alexander W. Goodall,Dalal Alrajeh,Francesco Belardinelli,Sebastian Uchitel*

Main category: cs.AI

TL;DR: 提出了首个基于GR(1)规范的自适应屏蔽框架，能够在运行时检测环境假设违反并自动修复规范，确保RL智能体在保持逻辑合规性的同时获得接近最优的奖励。


<details>
  <summary>Details</summary>
Motivation: 传统静态屏蔽方法假设固定的逻辑规范和手工制作的抽象，当环境假设被违反时无法适应，导致安全性和性能下降。

Method: 使用GR(1)规范（LTL的可处理表达片段），结合运行时环境假设违反检测和归纳逻辑编程(ILP)在线自动修复规范。

Result: 在Minepump和Atari Seaquest案例研究中，自适应屏蔽相比静态屏蔽能保持接近最优的奖励和完美的逻辑合规性。

Conclusion: 自适应屏蔽框架能够优雅地演化，确保活性可达，仅在必要时弱化目标，为RL安全提供了更灵活有效的解决方案。

Abstract: Shielding is widely used to enforce safety in reinforcement learning (RL),
ensuring that an agent's actions remain compliant with formal specifications.
Classical shielding approaches, however, are often static, in the sense that
they assume fixed logical specifications and hand-crafted abstractions. While
these static shields provide safety under nominal assumptions, they fail to
adapt when environment assumptions are violated. In this paper, we develop the
first adaptive shielding framework - to the best of our knowledge - based on
Generalized Reactivity of rank 1 (GR(1)) specifications, a tractable and
expressive fragment of Linear Temporal Logic (LTL) that captures both safety
and liveness properties. Our method detects environment assumption violations
at runtime and employs Inductive Logic Programming (ILP) to automatically
repair GR(1) specifications online, in a systematic and interpretable way. This
ensures that the shield evolves gracefully, ensuring liveness is achievable and
weakening goals only when necessary. We consider two case studies: Minepump and
Atari Seaquest; showing that (i) static symbolic controllers are often severely
suboptimal when optimizing for auxiliary rewards, and (ii) RL agents equipped
with our adaptive shield maintain near-optimal reward and perfect logical
compliance compared with static shields.

</details>


### [47] [A Multi-Agent Psychological Simulation System for Human Behavior Modeling](https://arxiv.org/abs/2511.02606)
*Xiangen Hu,Jiarui Tong,Sheng Xu*

Main category: cs.AI

TL;DR: 提出基于心理学理论的多智能体心理模拟系统，通过模拟内部认知-情感过程生成可信的人类行为，应用于教师培训和研究


<details>
  <summary>Details</summary>
Motivation: 人类中心领域的培训需要真实实践，但现有的人类行为模拟不够真实，需要更符合心理学原理的模拟系统

Method: 基于心理学理论（自我效能、思维模式、社会建构主义）构建多智能体系统，模拟"内部议会"中关键心理因素的智能体进行审议和互动

Result: 系统能够生成可信的人类行为，提供前所未有的透明度和与人类心理学的对齐性

Conclusion: 该系统体现了社会学习、认知学徒制、刻意练习和元认知原则，在教师培训和研究中有重要应用价值

Abstract: Training and education in human-centered fields require authentic practice,
yet realistic simulations of human behavior have remained limited. We present a
multi-agent psychological simulation system that models internal
cognitive-affective processes to generate believable human behaviors. In
contrast to black-box neural models, this system is grounded in established
psychological theories (e.g., self-efficacy, mindset, social constructivism)
and explicitly simulates an ``inner parliament'' of agents corresponding to key
psychological factors. These agents deliberate and interact to determine the
system's output behavior, enabling unprecedented transparency and alignment
with human psychology. We describe the system's architecture and theoretical
foundations, illustrate its use in teacher training and research, and discuss
how it embodies principles of social learning, cognitive apprenticeship,
deliberate practice, and meta-cognition.

</details>


### [48] [DecompSR: A dataset for decomposed analyses of compositional multihop spatial reasoning](https://arxiv.org/abs/2511.02627)
*Lachlan McPheat,Navdeep Kaur,Robert Blackwell,Alessandra Russo,Anthony G. Cohn,Pranava Madhyastha*

Main category: cs.AI

TL;DR: DecompSR是一个用于分析组合空间推理能力的大型基准数据集和生成框架，包含超过500万个数据点，通过独立控制组合性的多个方面来细粒度评估LLMs的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法独立评估组合性的不同方面，需要开发一个能够系统分析LLMs在空间推理任务中组合性能力的基准。

Method: 通过程序化生成方法构建数据集，独立控制生产力（推理深度）、可替换性（实体和语言变体）、过度泛化（输入顺序、干扰项）和系统性（新语言元素）等组合性维度，并使用符号求解器验证数据正确性。

Result: 测试多个大型语言模型发现，LLMs在空间推理任务中难以进行生产性和系统性泛化，但对语言变体具有更好的鲁棒性。

Conclusion: DecompSR提供了一个可证明正确且严格的基准数据集，能够独立控制组合性的关键维度，实现对LLMs组合推理能力的细粒度评估。

Abstract: We introduce DecompSR, decomposed spatial reasoning, a large benchmark
dataset (over 5m datapoints) and generation framework designed to analyse
compositional spatial reasoning ability. The generation of DecompSR allows
users to independently vary several aspects of compositionality, namely:
productivity (reasoning depth), substitutivity (entity and linguistic
variability), overgeneralisation (input order, distractors) and systematicity
(novel linguistic elements). DecompSR is built procedurally in a manner which
makes it is correct by construction, which is independently verified using a
symbolic solver to guarantee the correctness of the dataset. DecompSR is
comprehensively benchmarked across a host of Large Language Models (LLMs) where
we show that LLMs struggle with productive and systematic generalisation in
spatial reasoning tasks whereas they are more robust to linguistic variation.
DecompSR provides a provably correct and rigorous benchmarking dataset with a
novel ability to independently vary the degrees of several key aspects of
compositionality, allowing for robust and fine-grained probing of the
compositional reasoning abilities of LLMs.

</details>


### [49] [The Collaboration Gap](https://arxiv.org/abs/2511.02687)
*Tim R. Davidson,Adam Fourney,Saleema Amershi,Robert West,Eric Horvitz,Ece Kamar*

Main category: cs.AI

TL;DR: 论文提出了一个协作迷宫求解基准测试，发现AI模型在单独表现良好时，在协作中会出现显著的性能下降，即存在'协作鸿沟'。


<details>
  <summary>Details</summary>
Motivation: 随着AI发展，我们将越来越多依赖由不同信息、权限和工具的独立开发代理组成的系统，这些系统的成功关键取决于异构代理之间的有效协作，即使在部分可观测性下。

Method: 提出了一个协作迷宫求解基准测试框架，评估了32个领先的开源和闭源模型在单独、同质和异质配对中的表现，并测试了'接力推理'方法。

Result: 发现了'协作鸿沟'现象：单独表现良好的模型在协作时性能显著下降，小型蒸馏模型在某些配对中几乎完全失败。接力推理方法能显著缩小这一差距。

Conclusion: 需要(1)协作感知的评估，(2)增强协作能力的训练策略，(3)可靠激发代理潜在技能的交互设计，这些指导适用于AI-AI和人类-AI协作。

Abstract: The trajectory of AI development suggests that we will increasingly rely on
agent-based systems composed of independently developed agents with different
information, privileges, and tools. The success of these systems will
critically depend on effective collaboration among these heterogeneous agents,
even under partial observability. Despite intense interest, few empirical
studies have evaluated such agent-agent collaboration at scale. We propose a
collaborative maze-solving benchmark that (i) isolates collaborative
capabilities, (ii) modulates problem complexity, (iii) enables scalable
automated grading, and (iv) imposes no output-format constraints, preserving
ecological plausibility. Using this framework, we evaluate 32 leading open- and
closed-source models in solo, homogeneous, and heterogeneous pairings. Our
results reveal a "collaboration gap": models that perform well solo often
degrade substantially when required to collaborate. Collaboration can break
down dramatically; for instance, small distilled models that solve mazes well
alone may fail almost completely in certain pairings. We find that starting
with the stronger agent often improves outcomes, motivating a "relay inference"
approach where the stronger agent leads before handing off to the weaker one,
closing much of the gap. Our findings argue for (1) collaboration-aware
evaluation, (2) training strategies developed to enhance collaborative
capabilities, and (3) interaction design that reliably elicits agents' latent
skills, guidance that applies to AI-AI and human-AI collaboration.

</details>


### [50] [CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in Dynamic Environments for LLM Tool-Use Agents](https://arxiv.org/abs/2511.02734)
*Jiayu Liu,Cheng Qian,Zhaochen Su,Qing Zong,Shijue Huang,Bingxiang He,Yi R. Fung*

Main category: cs.AI

TL;DR: CostBench是一个专注于成本效益的基准测试，用于评估LLM代理的经济推理和重新规划能力，发现在静态和动态环境下代理都难以找到成本最优解。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理评估主要关注任务完成度，忽视了资源效率和适应性，特别是代理在变化环境中制定和调整成本最优计划的能力。

Method: 在旅行规划领域构建CostBench基准，包含可通过多种原子和复合工具序列解决的任务，支持四种动态阻塞事件（如工具故障和成本变化）来模拟现实世界不确定性。

Result: 评估显示代理在成本感知规划方面存在显著差距：在静态设置中经常无法找到成本最优解，GPT-5在最难任务上精确匹配率低于75%，动态条件下性能进一步下降约40%。

Conclusion: CostBench通过诊断这些弱点，为开发既经济理性又鲁棒的未来代理奠定了基础。

Abstract: Current evaluations of Large Language Model (LLM) agents primarily emphasize
task completion, often overlooking resource efficiency and adaptability. This
neglects a crucial capability: agents' ability to devise and adjust
cost-optimal plans in response to changing environments. To bridge this gap, we
introduce CostBench, a scalable, cost-centric benchmark designed to evaluate
agents' economic reasoning and replanning abilities. Situated in the
travel-planning domain, CostBench comprises tasks solvable via multiple
sequences of atomic and composite tools with diverse, customizable costs. It
also supports four types of dynamic blocking events, such as tool failures and
cost changes, to simulate real-world unpredictability and necessitate agents to
adapt in real time. Evaluating leading open-sourced and proprietary models on
CostBench reveals a substantial gap in cost-aware planning: agents frequently
fail to identify cost-optimal solutions in static settings, with even GPT-5
achieving less than 75% exact match rate on the hardest tasks, and performance
further dropping by around 40% under dynamic conditions. By diagnosing these
weaknesses, CostBench lays the groundwork for developing future agents that are
both economically rational and robust.

</details>


### [51] [Using Span Queries to Optimize for Cache and Attention Locality](https://arxiv.org/abs/2511.02749)
*Paul Castro,Nick Mitchell,Nathan Ordonez,Thomas Parnell,Mudhakar Srivatsa,Antoni Viros i Martin*

Main category: cs.AI

TL;DR: 本文提出了span query概念，将推理服务器接口泛化，支持聊天、RAG、推理时扩展和智能体工作负载，通过交换性约束优化KV缓存命中率，在非聊天用例中实现10-20倍的TTFT降低。


<details>
  <summary>Details</summary>
Motivation: 客户端已超越聊天完成功能，包含各种创新的推理时扩展和深度推理技术，但推理服务器仍主要针对聊天完成优化。现有工作虽能提高KV缓存命中率，但仅针对单一用例RAG进行优化。

Method: 引入span query作为推理调用的表达式树，通过交换性约束链接，可自动优化KV缓存局部性。对vLLM进行少量修改（仅492行代码）即可实现高性能span query执行。

Result: span query在两种不同的非聊天用例中实现10-20倍的TTFT降低。注意力优化的span query在2b参数模型上的表现远超使用8b模型的传统推理服务器。

Conclusion: span query提供了一种通用接口，能够有效支持多种推理工作负载，显著提升性能并解决注意力局部性问题。

Abstract: Clients are evolving beyond chat completion, and now include a variety of
innovative inference-time scaling and deep reasoning techniques. At the same
time, inference servers remain heavily optimized for chat completion. Prior
work has shown that large improvements to KV cache hit rate are possible if
inference servers evolve towards these non-chat use cases. However, they offer
solutions that are also optimized for a single use case, RAG. In this paper, we
introduce the span query to generalize the interface to the inference server.
We demonstrate that chat, RAG, inference-time scaling, and agentic workloads
can all be expressed as span queries. We show how the critical distinction that
had been assumed by prior work lies in whether the order of the inputs matter
-- do they commute? In chat, they do not. In RAG, they often do. This paper
introduces span queries, which are expression trees of inference calls, linked
together with commutativity constraints. We describe span query syntax and
semantics. We show how they can be automatically optimized to improve KV cache
locality. We show how a small change to vLLM (affecting only 492 lines) can
enable high-performance execution of span queries. Using this stack, we
demonstrate that span queries can achieve 10-20x reductions in TTFT for two
distinct non-chat use cases. Finally, we show that span queries can also be
optimized to improve attention locality, so as to avoid the so-called
lost-in-the-middle problem. We demonstrate that an attention-optimized span
query on a 2b parameter model vastly outperforms the accuracy of a stock
inference server using an 8b model.

</details>


### [52] [LLM-Supported Formal Knowledge Representation for Enhancing Control Engineering Content with an Interactive Semantic Layer](https://arxiv.org/abs/2511.02759)
*Julius Fiedler,Carsten Knoll,Klaus Röbenack*

Main category: cs.AI

TL;DR: 本文提出了一种基于LLM的半自动化方法，用于生成结合人类可读性和机器可解释性的形式化知识表示，应用于控制工程领域。


<details>
  <summary>Details</summary>
Motivation: 控制工程领域研究产出的快速增长需要新的方法来结构化和形式化领域知识，以实现易于访问、协作和可验证的知识库。

Method: 基于Imperative Representation of Knowledge (PyIRK)框架，利用语言模型将自然语言描述和数学定义(LaTeX源代码)转换为形式化知识图谱。

Result: 开发了"交互式语义层"来增强源文档，促进知识传递，展示了该方法在控制工程领域的初步应用。

Conclusion: 该方法为实现控制工程领域易于访问、协作和可验证的知识库愿景做出了贡献。

Abstract: The rapid growth of research output in control engineering calls for new
approaches to structure and formalize domain knowledge. This paper briefly
describes an LLM-supported method for semi-automated generation of formal
knowledge representations that combine human readability with machine
interpretability and increased expressiveness. Based on the Imperative
Representation of Knowledge (PyIRK) framework, we demonstrate how language
models can assist in transforming natural-language descriptions and
mathematical definitions (available as LaTeX source code) into a formalized
knowledge graph. As a first application we present the generation of an
``interactive semantic layer'' to enhance the source documents in order to
facilitate knowledge transfer. From our perspective this contributes to the
vision of easily accessible, collaborative, and verifiable knowledge bases for
the control engineering domain.

</details>


### [53] [When One Modality Sabotages the Others: A Diagnostic Lens on Multimodal Reasoning](https://arxiv.org/abs/2511.02794)
*Chenyu Zhang,Minsol Kim,Shohreh Ghorbani,Jingyao Wu,Rosalind Picard,Patricia Maes,Paul Pu Liang*

Main category: cs.AI

TL;DR: 提出了一种轻量级、模型无关的评估层来分析多模态大语言模型中的模态破坏问题，通过将每个模态视为代理来诊断融合动态。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型快速发展，但其推理过程仍然不透明，难以理解哪个模态驱动预测、冲突如何解决或何时某个模态主导决策。

Method: 将每个模态视为代理，生成候选标签和简要自我评估，通过简单融合机制聚合这些输出来识别贡献者和破坏者模态。

Result: 在多模态情感识别基准测试的案例研究中，揭示了系统的可靠性特征，帮助区分失败是由数据集伪影还是模型限制引起。

Conclusion: 该框架为多模态推理提供了诊断支架，支持对融合动态的原则性审计，并为可能的干预措施提供信息。

Abstract: Despite rapid growth in multimodal large language models (MLLMs), their
reasoning traces remain opaque: it is often unclear which modality drives a
prediction, how conflicts are resolved, or when one stream dominates. In this
paper, we introduce modality sabotage, a diagnostic failure mode in which a
high-confidence unimodal error overrides other evidence and misleads the fused
result. To analyze such dynamics, we propose a lightweight, model-agnostic
evaluation layer that treats each modality as an agent, producing candidate
labels and a brief self-assessment used for auditing. A simple fusion mechanism
aggregates these outputs, exposing contributors (modalities supporting correct
outcomes) and saboteurs (modalities that mislead). Applying our diagnostic
layer in a case study on multimodal emotion recognition benchmarks with
foundation models revealed systematic reliability profiles, providing insight
into whether failures may arise from dataset artifacts or model limitations.
More broadly, our framework offers a diagnostic scaffold for multimodal
reasoning, supporting principled auditing of fusion dynamics and informing
possible interventions.

</details>


### [54] [Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning](https://arxiv.org/abs/2511.02818)
*Mohamed Bouadi,Pratinav Seth,Aditya Tanna,Vinay Kumar Sankarapu*

Main category: cs.AI

TL;DR: Orion-MSP是一个用于表格数据上下文学习的新架构，通过多尺度处理、块稀疏注意力和感知器式内存解决了现有方法的局限性，在保持高性能的同时实现了对高维表格的有效扩展。


<details>
  <summary>Details</summary>
Motivation: 现有的表格上下文学习方法存在三个主要局限：单尺度特征处理忽略了层次依赖关系；密集注意力在表格宽度上具有二次缩放；严格顺序的组件处理阻碍了迭代表示细化和跨组件通信。

Method: Orion-MSP引入了三个关键创新：多尺度处理捕获层次特征交互；块稀疏注意力结合窗口化、全局和随机模式实现可扩展效率和长程连接；感知器式内存支持跨组件的安全双向信息流。

Result: 在多样化基准测试中，Orion-MSP匹配或超越了最先进的性能，同时能够有效扩展到高维表格。

Conclusion: Orion-MSP为高效的表格上下文学习建立了新标准，解决了现有架构的关键局限性。

Abstract: Tabular data remain the predominant format for real-world applications. Yet,
developing effective neural models for tabular data remains challenging due to
heterogeneous feature types and complex interactions occurring at multiple
scales. Recent advances in tabular in-context learning (ICL), such as TabPFN
and TabICL, have achieved state-of-the-art performance comparable to
gradient-boosted trees (GBTs) without task-specific fine-tuning. However,
current architectures exhibit key limitations: (1) single-scale feature
processing that overlooks hierarchical dependencies, (2) dense attention with
quadratic scaling in table width, and (3) strictly sequential component
processing that prevents iterative representation refinement and
cross-component communication. To address these challenges, we introduce
Orion-MSP, a tabular ICL architecture featuring three key innovations: (1)
multi-scale processing to capture hierarchical feature interactions; (2)
block-sparse attention combining windowed, global, and random patterns for
scalable efficiency and long-range connectivity; and (3) a Perceiver-style
memory enabling safe bidirectional information flow across components. Across
diverse benchmarks, Orion-MSP matches or surpasses state-of-the-art performance
while scaling effectively to high-dimensional tables, establishing a new
standard for efficient tabular in-context learning. The model is publicly
available at https://github.com/Lexsi-Labs/Orion-MSP .

</details>


### [55] [Optimizing AI Agent Attacks With Synthetic Data](https://arxiv.org/abs/2511.02823)
*Chloe Loughridge,Paul Colognese,Avery Griffin,Tyler Tracy,Jon Kutasov,Joe Benton*

Main category: cs.AI

TL;DR: 本文提出了一种在复杂AI控制环境中优化攻击策略的方法，通过将攻击能力分解为五个技能组件并分别优化，使用概率模型解决数据不足问题，显著提升了攻击强度。


<details>
  <summary>Details</summary>
Motivation: 随着AI部署变得更加复杂和高风险，准确评估其风险变得至关重要。AI控制框架需要强大的攻击策略来进行评估，但在复杂代理环境中，计算约束导致数据不足，这带来了挑战。

Method: 将攻击能力分解为五个组成技能：怀疑建模、攻击选择、计划合成、执行和隐蔽性，并分别优化每个组件。开发攻击动态的概率模型，在模拟中优化攻击超参数，然后将结果转移到SHADE-Arena环境中。

Result: 该方法显著提高了攻击强度，将安全分数从基线0.87降低到0.41，表明攻击策略的有效性大幅提升。

Conclusion: 通过技能分解和概率建模的方法，可以在数据有限的复杂环境中有效优化攻击策略，为AI控制评估提供了更可靠的工具。

Abstract: As AI deployments become more complex and high-stakes, it becomes
increasingly important to be able to estimate their risk. AI control is one
framework for doing so. However, good control evaluations require eliciting
strong attack policies. This can be challenging in complex agentic environments
where compute constraints leave us data-poor. In this work, we show how to
optimize attack policies in SHADE-Arena, a dataset of diverse realistic control
environments. We do this by decomposing attack capability into five constituent
skills -- suspicion modeling, attack selection, plan synthesis, execution and
subtlety -- and optimizing each component individually. To get around the
constraint of limited data, we develop a probabilistic model of attack
dynamics, optimize our attack hyperparameters using this simulation, and then
show that the results transfer to SHADE-Arena. This results in a substantial
improvement in attack strength, reducing safety score from a baseline of 0.87
to 0.41 using our scaffold.

</details>


### [56] [Kosmos: An AI Scientist for Autonomous Discovery](https://arxiv.org/abs/2511.02824)
*Ludovico Mitchener,Angela Yiu,Benjamin Chang,Mathieu Bourdenx,Tyler Nadolski,Arvis Sulovari,Eric C. Landsness,Daniel L. Barabasi,Siddharth Narayanan,Nicky Evans,Shriya Reddy,Martha Foiani,Aizad Kamal,Leah P. Shriver,Fang Cao,Asmamaw T. Wassie,Jon M. Laurent,Edwin Melville-Green,Mayk Caldas,Albert Bou,Kaleigh F. Roberts,Sladjana Zagorac,Timothy C. Orr,Miranda E. Orr,Kevin J. Zwezdaryk,Ali E. Ghareeb,Laurie McCoy,Bruna Gomes,Euan A. Ashley,Karen E. Duff,Tonio Buonassisi,Tom Rainforth,Randall J. Bateman,Michael Skarlinski,Samuel G. Rodriques,Michaela M. Hinks,Andrew D. White*

Main category: cs.AI

TL;DR: Kosmos是一个AI科学家系统，能够自动化数据驱动的科学发现过程，通过结构化的世界模型在数据分析和文献搜索代理之间共享信息，实现长达12小时、200次代理部署的连贯研究，生成可追溯的科学报告。


<details>
  <summary>Details</summary>
Motivation: 现有AI科研代理在采取多次行动后会失去连贯性，限制了研究深度。需要开发能够进行长期、连贯科学发现的AI系统。

Method: 使用结构化世界模型连接数据分析和文献搜索代理，进行并行数据分析、文献搜索和假设生成的循环，每次运行执行约42,000行代码和阅读1,500篇论文。

Result: 79.4%的报告陈述被独立科学家确认为准确，单次20循环运行相当于人类科学家6个月的研究工作量，科学发现数量与循环次数呈线性增长。

Conclusion: Kosmos证明了AI系统能够进行长期、连贯的科学发现，在多个学科领域做出了原创性贡献，其中三项发现独立重现了未发表的研究成果，四项为科学文献提供了新颖贡献。

Abstract: Data-driven scientific discovery requires iterative cycles of literature
search, hypothesis generation, and data analysis. Substantial progress has been
made towards AI agents that can automate scientific research, but all such
agents remain limited in the number of actions they can take before losing
coherence, thus limiting the depth of their findings. Here we present Kosmos,
an AI scientist that automates data-driven discovery. Given an open-ended
objective and a dataset, Kosmos runs for up to 12 hours performing cycles of
parallel data analysis, literature search, and hypothesis generation before
synthesizing discoveries into scientific reports. Unlike prior systems, Kosmos
uses a structured world model to share information between a data analysis
agent and a literature search agent. The world model enables Kosmos to
coherently pursue the specified objective over 200 agent rollouts, collectively
executing an average of 42,000 lines of code and reading 1,500 papers per run.
Kosmos cites all statements in its reports with code or primary literature,
ensuring its reasoning is traceable. Independent scientists found 79.4% of
statements in Kosmos reports to be accurate, and collaborators reported that a
single 20-cycle Kosmos run performed the equivalent of 6 months of their own
research time on average. Furthermore, collaborators reported that the number
of valuable scientific findings generated scales linearly with Kosmos cycles
(tested up to 20 cycles). We highlight seven discoveries made by Kosmos that
span metabolomics, materials science, neuroscience, and statistical genetics.
Three discoveries independently reproduce findings from preprinted or
unpublished manuscripts that were not accessed by Kosmos at runtime, while four
make novel contributions to the scientific literature.

</details>


### [57] [Neurosymbolic Deep Learning Semantics](https://arxiv.org/abs/2511.02825)
*Artur d'Avila Garcez,Simon Odense*

Main category: cs.AI

TL;DR: 本文提出使用逻辑框架为深度学习提供语义基础，通过神经符号AI将神经网络与逻辑联系起来，解决AI科学发现缺乏语义理解的问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI在科学发现中缺乏语义基础，使得AI的科学发现难以被理解。需要建立一个框架来将AI的洞察转化为可理解的科学知识。

Method: 引入语义编码框架，明确神经网络与逻辑之间的映射关系，并总结现有神经编码和知识提取方法的核心要素。

Result: 建立了一个统一的框架来连接逻辑语义和神经网络，为各种现有方法提供了共同的理论基础。

Conclusion: 逻辑框架能够为深度学习提供所需的语义基础，但实际识别语义编码仍面临哲学层面的挑战。

Abstract: Artificial Intelligence (AI) is a powerful new language of science as
evidenced by recent Nobel Prizes in chemistry and physics that recognized
contributions to AI applied to those areas. Yet, this new language lacks
semantics, which makes AI's scientific discoveries unsatisfactory at best. With
the purpose of uncovering new facts but also improving our understanding of the
world, AI-based science requires formalization through a framework capable of
translating insight into comprehensible scientific knowledge. In this paper, we
argue that logic offers an adequate framework. In particular, we use logic in a
neurosymbolic framework to offer a much needed semantics for deep learning, the
neural network-based technology of current AI. Deep learning and neurosymbolic
AI lack a general set of conditions to ensure that desirable properties are
satisfied. Instead, there is a plethora of encoding and knowledge extraction
approaches designed for particular cases. To rectify this, we introduced a
framework for semantic encoding, making explicit the mapping between neural
networks and logic, and characterizing the common ingredients of the various
existing approaches. In this paper, we describe succinctly and exemplify how
logical semantics and neural networks are linked through this framework, we
review some of the most prominent approaches and techniques developed for
neural encoding and knowledge extraction, provide a formal definition of our
framework, and discuss some of the difficulties of identifying a semantic
encoding in practice in light of analogous problems in the philosophy of mind.

</details>


### [58] [Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for Understanding Anything](https://arxiv.org/abs/2511.02834)
*Huawei Lin,Yunzhi Shi,Tong Geng,Weijie Zhao,Wei Wang,Ravender Pal Singh*

Main category: cs.AI

TL;DR: 提出了Agent-Omni框架，通过主代理系统协调现有基础模型，实现无需重新训练的灵活多模态推理


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型局限于固定模态对，需要大量对齐数据进行微调，构建完全全能模型不切实际且缺乏鲁棒推理支持

Method: 采用主代理系统，主代理解释用户意图，将子任务委托给模态特定代理，并整合它们的输出形成连贯响应

Result: 在文本、图像、音频、视频和全能基准测试中始终达到最先进性能，特别是在需要复杂跨模态推理的任务上表现突出

Conclusion: 基于代理的设计能够无缝集成专门的基础模型，确保对多样化输入的适应性，同时保持透明性和可解释性，框架模块化且易于扩展

Abstract: Multimodal large language models (MLLMs) have shown strong capabilities but
remain limited to fixed modality pairs and require costly fine-tuning with
large aligned datasets. Building fully omni-capable models that can integrate
text, images, audio, and video remains impractical and lacks robust reasoning
support. In this paper, we propose an Agent-Omni framework that coordinates
existing foundation models through a master-agent system, enabling flexible
multimodal reasoning without retraining. The master agent interprets user
intent, delegates subtasks to modality-specific agents, and integrates their
outputs into coherent responses. Extensive experiments across text, image,
audio, video, and omni benchmarks show that Agent-Omni consistently achieves
state-of-the-art performance, particularly on tasks requiring complex
cross-modal reasoning. Its agent-based design enables seamless integration of
specialized foundation models, ensuring adaptability to diverse inputs while
maintaining transparency and interpretability. In addition, the framework is
modular and easily extensible, allowing future improvements as stronger models
become available. %We release an open-source implementation to support
continued research on scalable and reliable omni-modal reasoning.

</details>
