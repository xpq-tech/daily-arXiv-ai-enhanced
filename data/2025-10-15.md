<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 62]
- [cs.AI](#cs.AI) [Total: 46]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [PHANTOM RECALL: When Familiar Puzzles Fool Smart Models](https://arxiv.org/abs/2510.11812)
*Souradeep Mukhopadhyay,Rishabh Baral,Nimeesh Mahajan,Samhitha Harish,Aswin RRV,Mihir Parmar,Mutsumi Nakamura,Chitta Baral*

Main category: cs.CL

TL;DR: LLMs在逻辑谜题上表现脆弱，主要依赖记忆而非推理，当谜题被微调时性能显著下降。研究者开发了PHANTOM RECALL基准测试，发现模型存在'幻影回忆'现象，并提出了检测和缓解工具。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在逻辑谜题中的真实推理能力，验证它们是否仅依赖记忆模板而非从第一原则推理，以及如何系统评估和改进这种能力。

Method: 引入PHANTOM RECALL基准测试，包含25个经典逻辑谜题和149个精心设计的扰动版本；评估11个主流LLMs；开发自动化逻辑等价判断器、细粒度推理错误分类法和基于提示的缓解框架。

Result: LLMs在未修改谜题上准确率接近完美，但在扰动版本上表现显著低于人类，出现幻影回忆和过度阐述问题，暴露了语言流畅性与逻辑理解之间的差距。

Conclusion: LLMs在逻辑推理方面存在根本性局限，当上下文线索变化时无法重新推理，这揭示了当前模型在真正理解逻辑结构方面的不足。

Abstract: Large language models (LLMs) such as GPT, Gemini, and Claude often appear
adept at solving classic logic puzzles--but how much genuine reasoning
underlies their answers? Recent evidence suggests that these models frequently
rely on memorized templates rather than reasoning from first principles. When
puzzles are slightly modified, their performance collapses, revealing a
striking fragility. In particular, we asked: Have LLMs addressed these issues?
To what extent? How about perturbations to other puzzles? Is there a general
way of reformulating the prompt so that the models do better? To examine these
things systematically, we introduce PHANTOM RECALL, a benchmark comprising 25
well-known logic puzzles and 149 carefully designed perturbations that preserve
reasoning structure but alter superficial details and solutions. We evaluate
eleven leading LLMs and identify a recurring failure mode--phantom
recall--where models confidently reproduce memorized solutions or spurious
rationales that no longer fit the altered scenario. To probe and mitigate this
issue, we contribute three tools: (i) an automated logical-equivalence judge to
detect reasoning mismatches, (ii) a taxonomy of fine-grained reasoning error
categories, and (iii) a prompting-based mitigation framework guided by these
categories. Despite near-perfect accuracy on unmodified puzzles, models
significantly underperform humans on perturbed ones, exhibiting both phantom
recall and over-elaboration. Our findings reveal a crucial limitation: LLMs
often fail to re-reason when contextual cues shift--highlighting the gap
between linguistic fluency and logical understanding.

</details>


### [2] [R-WoM: Retrieval-augmented World Model For Computer-use Agents](https://arxiv.org/abs/2510.11892)
*Kai Mei,Jiang Guo,Shuaichen Chang,Mingwen Dong,Dongkyu Lee,Xing Niu,Jiarong Jiang*

Main category: cs.CL

TL;DR: LLMs作为世界模型在数字环境中增强智能体决策，但存在幻觉和静态知识限制。研究提出R-WoM方法，通过检索外部教程知识来提升长期模拟性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs作为世界模型时的幻觉问题和长期模拟能力不足，探索如何通过外部知识增强其环境动态建模能力。

Method: 提出检索增强世界模型(R-WoM)，从外部教程检索事实性、最新知识来增强LLM模拟，评估了三种核心能力：下一状态预测、完整流程规划对齐和里程碑转换识别。

Result: R-WoM在OSWorld和WebArena上分别实现了25.3%和18.1%的显著提升，在长期模拟中表现尤为突出。

Conclusion: 虽然LLMs能有效捕捉即时状态和有意义的状态转换，但在完整流程规划中性能迅速下降。R-WoM通过知识检索有效缓解了LLMs在长期环境动态建模中的局限性。

Abstract: Large Language Models (LLMs) can serve as world models to enhance agent
decision-making in digital environments by simulating future states and
predicting action outcomes, potentially eliminating costly trial-and-error
exploration. However, this capability is fundamentally limited by LLMs'
tendency toward hallucination and their reliance on static training knowledge,
which can lead to compounding errors that inhibit long-horizon simulations. To
systematically investigate whether LLMs are appropriate for world modeling, we
probe two core capabilities of world models--future state prediction and reward
estimation--through three tasks: next-state identification, full-procedure
planning alignment, and milestone transition recognition. Our analysis shows
that while LLMs effectively capture immediate next states and identify
meaningful state transitions, their performance rapidly degrades in
full-procedure planning. This highlights LLMs' limitations in reliably modeling
environment dynamics over long horizons. To address these limitations, we
propose the Retrieval-augmented World Model (R-WoM), which grounds LLM
simulations by incorporating factual, up-to-date knowledge retrieved from
external tutorials. Experiments show that R-WoM achieves substantial
improvements of up to 25.3% (OSWorld) and 18.1% (WebArena) compared to
baselines, with particular advantages in longer-horizon simulations.

</details>


### [3] [LLM Knowledge is Brittle: Truthfulness Representations Rely on Superficial Resemblance](https://arxiv.org/abs/2510.11905)
*Patrick Haller,Mark Ibrahim,Polina Kirichenko,Levent Sagun,Samuel J. Bell*

Main category: cs.CL

TL;DR: 研究发现LLM对输入微小变化极其敏感，其内部知识表示不稳定，导致真实陈述与虚假陈述的可分离性在语义保持的扰动下急剧下降。


<details>
  <summary>Details</summary>
Motivation: 探索LLM性能脆弱性是否源于内部知识表示的不稳定性，特别是当输入发生微小变化时，模型区分真实与虚假陈述的能力是否会崩溃。

Method: 通过应用语义保持的扰动（如拼写错误、重新表述）使样本偏离训练分布，评估四种LLM家族、五个评估数据集和三种知识探测方法中表示可分离性的退化情况。

Result: 当样本呈现方式与预训练数据差异增大时，LLM内部对陈述真实性的表示会崩溃。模型仅在输入与预训练数据高度相似时能区分真假陈述，这种能力高度依赖具体的表面形式。

Conclusion: LLM可能学习了浅层、非鲁棒的知识表示，导致泛化能力有限。这对真实性探测的实用性提出了根本挑战，并呼吁进一步研究改进学习知识表示的鲁棒性。

Abstract: For Large Language Models (LLMs) to be reliable, they must learn robust
knowledge that can be generally applied in diverse settings -- often unlike
those seen during training. Yet, extensive research has shown that LLM
performance can be brittle, with models exhibiting excessive sensitivity to
trivial input variations. In this work, we explore whether this brittleness is
a direct result of unstable internal knowledge representations. To explore this
question, we build on previous work showing that LLM representations encode
statement truthfulness -- i.e., true, factual statements can be easily
separated from false, inaccurate ones. Specifically, we test the robustness of
learned knowledge by evaluating representation separability on samples that
have undergone superficial transformations to drive them out-of-distribution
(OOD), such as typos or reformulations. By applying semantically-preserving
perturbations, we study how separability degrades as statements become more
OOD, across four LLM families, five evaluation datasets, and three knowledge
probing methods. Our results reveal that internal representations of statement
truthfulness collapse as the samples' presentations become less similar to
those seen during pre-training. While LLMs can often distinguish between true
and false statements when they closely resemble the pre-training data, this
ability is highly dependent on the statement's exact surface form. These
findings offer a possible explanation for brittle benchmark performance: LLMs
may learn shallow, non-robust knowledge representations that allow for only
limited generalizability. Our work presents a fundamental challenge for the
utility of truthfulness probes, and more broadly, calls for further research on
improving the robustness of learned knowledge representations.

</details>


### [4] [LLM Reasoning for Machine Translation: Synthetic Data Generation over Thinking Tokens](https://arxiv.org/abs/2510.11919)
*Armel Zebaze,Rachel Bawden,Benoît Sagot*

Main category: cs.CL

TL;DR: 探索大型推理模型在机器翻译任务中使用中间思考标记的效果，发现思考标记本身并不能提升翻译性能，但通过模块化翻译策略构建的中间标记能带来改进。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在数学和编程任务中表现出色，但其在机器翻译任务中的潜力尚未充分探索，特别是通过生成中间思考标记来提升翻译质量的可能性。

Method: 在多种语言对和资源水平下测试中间思考标记的效果，包括使用蒸馏思维链方法微调模型，以及构建模块化翻译策略的中间标记。

Result: 思考标记本身不能帮助模型更好地执行机器翻译，基于人类翻译实践构建的思维链微调也不优于标准输入输出微调。但通过模块化翻译策略构建的中间标记能带来改进。

Conclusion: 中间标记在微调过程中的贡献高度依赖于其中是否包含翻译尝试，使用教师模型改进目标翻译或扩展平行语料比蒸馏思维链解释更有效。

Abstract: Large reasoning models (LRMs) have led to new possibilities in terms of
problem-solving, through the devising of a natural language thought process
prior to answering a query. While their capabilities are well known across
mathematics and coding tasks, their impact on the task of machine translation
(MT) remains underexplored. In this work, we explore the benefits of the
generation of intermediate tokens when performing MT across multiple language
pairs of different levels of resourcedness and multiple setups. We find that
"thinking tokens" do not help LRMs better perform MT. This result generalizes
to models fine-tuned to reason before translating using distilled chain of
thought (CoT) inspired by human translators' practices. Specifically,
fine-tuning a model with synthetic CoT explanations detailing how to translate
step-by-step does not outperform standard input-output fine-tuning. However,
constructing the intermediate tokens by combining the outputs of modular
translation-specific prompting strategies results in improvements. Our findings
underscore that the contribution of intermediate tokens during fine-tuning
highly depends on the presence of translation attempts within them. More
broadly, our results suggest that using a teacher to refine target translations
or to expand parallel corpora is more impactful than distilling their CoT
explanations into "thinking" MT models.

</details>


### [5] [Discrepancy Detection at the Data Level: Toward Consistent Multilingual Question Answering](https://arxiv.org/abs/2510.11928)
*Lorena Calvo-Bartolomé,Valérie Aldana,Karla Cantarero,Alonso Madroñal de Mesa,Jerónimo Arenas-García,Jordan Boyd-Graber*

Main category: cs.CL

TL;DR: 提出了MIND用户参与的事实检查流程，用于检测多语言问答知识库中的事实和文化差异，特别关注母婴健康领域。


<details>
  <summary>Details</summary>
Motivation: 多语言问答系统需要确保跨语言的事实一致性，同时考虑主观回答中的文化差异，特别是在文化敏感问题上。

Method: 开发了用户参与的事实检查流程MIND，通过标注双语问题的事实和文化不一致性来检测差异。

Result: 在母婴健康领域的双语问答系统上评估MIND，并在其他领域数据集上测试泛化能力，MIND能可靠识别不一致性。

Conclusion: MIND支持开发更具文化意识和事实一致性的问答系统，通过检测事实和文化差异来提高多语言QA系统的质量。

Abstract: Multilingual question answering (QA) systems must ensure factual consistency
across languages, especially for objective queries such as What is jaundice?,
while also accounting for cultural variation in subjective responses. We
propose MIND, a user-in-the-loop fact-checking pipeline to detect factual and
cultural discrepancies in multilingual QA knowledge bases. MIND highlights
divergent answers to culturally sensitive questions (e.g., Who assists in
childbirth?) that vary by region and context. We evaluate MIND on a bilingual
QA system in the maternal and infant health domain and release a dataset of
bilingual questions annotated for factual and cultural inconsistencies. We
further test MIND on datasets from other domains to assess generalization. In
all cases, MIND reliably identifies inconsistencies, supporting the development
of more culturally aware and factually consistent QA systems.

</details>


### [6] [TopoAlign: A Framework for Aligning Code to Math via Topological Decomposition](https://arxiv.org/abs/2510.11944)
*Yupei Li,Philipp Borchert,Gerasimos Lampouras*

Main category: cs.CL

TL;DR: TopoAlign框架利用代码仓库作为数学大语言模型的训练资源，通过将代码分解为文档字符串、主函数和依赖函数，并重组为结构上类似形式数学语句的代码数据，显著提升了模型在数学形式化任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前数学大语言模型在自动形式化（将非形式数学语句转换为形式化语句）方面表现不佳，主要受限于大规模非形式与形式数学语句配对语料的稀缺。虽然现有模型能够从自然语言生成代码，但代码与形式数学在结构和语法上的差异限制了有效的迁移学习。

Method: 提出TopoAlign框架，将代码分解为文档字符串、主函数和依赖函数，然后重组这些组件以创建结构上类似形式数学语句的代码数据。这种方法无需额外人工标注，即可利用广泛可用的代码仓库作为训练资源。

Result: 在DeepSeek-Math模型上，TopoAlign使BEq@10性能提升17.77%，typecheck@10性能提升68.82%。在Herald模型上，尽管没有引入新的数学知识，BEq@10和typecheck@10分别提升了0.12%和1.09%，表明对齐的代码数据训练对专门化模型也有益。

Conclusion: TopoAlign框架成功解锁了代码仓库作为数学大语言模型的训练资源，通过结构对齐的代码数据显著提升了模型在数学形式化任务上的性能，为缓解数学语料稀缺问题提供了有效解决方案。

Abstract: Large Language Models (LLMs) excel at both informal and formal (e.g. Lean 4)
mathematical reasoning but still struggle with autoformalisation, the task of
transforming informal into formal mathematical statements. Autoformalisation
helps pair the informal reasoning of LLMs with formal proof assistants which
enable machine-verifiable generation and mitigate hallucinations. Yet, the
performance of current Math LLMs is constrained by the scarcity of large-scale
corpora, particularly those containing pairs of informal and formal statements.
Although current models are trained to generate code from natural language
instructions, structural and syntactic differences between these and formal
mathematics limit effective transfer learning. We propose TopoAlign, a
framework that unlocks widely available code repositories as training resources
for Math LLMs. TopoAlign decomposes code into docstrings, main functions, and
dependency functions, and reassembles these components into analogues that
structurally mirror formal statements. This produces structurally aligned code
data that can be used for training Math LLMs without requiring additional human
annotation. We train two state-of-the-art models, DeepSeek-Math and Herald, and
evaluate them on the minif2f, Putnam, and ProofNet benchmarks. TopoAlign
provides substantial gains for DeepSeek-Math, improving performance by 17.77%
on BEq@10 and 68.82% on typecheck@10. Despite introducing no new mathematical
knowledge, our framework achieves gains of 0.12% and 1.09% for Herald on BEq@10
and typecheck@10, respectively, demonstrating that training on aligned code
data is beneficial even for specialized models.

</details>


### [7] [GRAVITY: A Framework for Personalized Text Generation via Profile-Grounded Synthetic Preferences](https://arxiv.org/abs/2510.11952)
*Priyanka Dey,Daniele Rosa,Wenqing Zheng,Daniel Barcklow,Jieyu Zhao,Emilio Ferrara*

Main category: cs.CL

TL;DR: GRAVITY框架通过生成基于用户画像的合成偏好数据来减少对人工标注的依赖，该数据捕捉用户的兴趣、价值观、信仰和个性特征，从而提升LLM个性化内容生成的质量和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM个性化方法依赖昂贵的人工反馈或交互日志，限制了可扩展性且忽略了更深层次的用户属性。

Method: 整合人口统计、文化和心理学框架（霍夫斯泰德文化维度、施瓦茨基本价值观、世界价值观调查、大五人格特质），生成基于用户画像的合成偏好数据对来指导个性化内容生成。

Result: 在400名亚马逊用户的书籍描述生成任务中，GRAVITY相比基于提示的条件生成、标准微调和朴素合成对生成方法，实现了超过4%的偏好增益，用户研究显示GRAVITY输出在86%的情况下被优先选择。

Conclusion: 基于场景的合成数据能够捕捉更丰富的用户变化，减少对昂贵标注的依赖，生成更具吸引力的用户中心内容，为LLM个性化提供了可扩展的路径。

Abstract: Personalization in LLMs often relies on costly human feedback or interaction
logs, limiting scalability and neglecting deeper user attributes. To reduce the
reliance on human annotations, we introduce GRAVITY (Generative Response with
Aligned Values, Interests, and Traits of You), a framework for generating
synthetic, profile-grounded preference data that captures users' interests,
values, beliefs, and personality traits. By integrating demographic, cultural,
and psychological frameworks -- including Hofstede's cultural dimensions,
Schwartz's basic values, the World Values Survey, and Big Five OCEAN traits --
GRAVITY synthesizes preference pairs to guide personalized content generation.
We evaluate GRAVITY on book descriptions for 400 Amazon users, comparing it to
prompt-based conditioning, standard fine-tuning, and naive synthetic pair
generation. Profile-grounded synthetic data consistently improves generation,
especially across multiple cultures (USA, Brazil, Japan, India), achieving over
4% higher preference gains across baselines, with user studies showing that
GRAVITY outputs are preferred over 86% of the time. Our results show that
scenario-grounded synthetic data can capture richer user variation, reduce
reliance on costly annotation, and produce more engaging, user-centered
content, offering a scalable path for LLM personalization.

</details>


### [8] [Evaluating Retrieval-Augmented Generation Systems on Unanswerable, Uncheatable, Realistic, Multi-hop Queries](https://arxiv.org/abs/2510.11956)
*Gabrielle Kaili-May Liu,Bryan Li,Arman Cohan,William Gantt Walden,Eugene Yang*

Main category: cs.CL

TL;DR: 提出了CRUMQs管道，用于自动创建不可作弊、真实、不可回答和多跳查询的RAG基准测试，显著提升了RAG系统评估的挑战性和真实性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG基准测试很少反映真实任务复杂性，多跳或超出范围的问题往往可以通过断开推理作弊解决，或仅需要简单事实回忆，限制了发现现有RAG系统局限性的能力。

Method: 开发了第一个自动、难度可控的管道，用于创建不可作弊、真实、不可回答和多跳查询（CRUMQs），可适应任何语料库和领域。

Result: 在两个流行的RAG数据集上创建CRUMQs，实验结果显示相比先前基准测试，CRUMQs对RAG系统极具挑战性，作弊分数降低了高达81.0%。

Conclusion: 该管道提供了一种简单方法来增强基准测试难度和真实性，推动开发更强大的RAG系统。

Abstract: Real-world use cases often present RAG systems with complex queries for which
relevant information is missing from the corpus or is incomplete. In these
settings, RAG systems must be able to reject unanswerable, out-of-scope queries
and identify failures of retrieval and multi-hop reasoning. Despite this,
existing RAG benchmarks rarely reflect realistic task complexity for multi-hop
or out-of-scope questions, which often can be cheated via disconnected
reasoning (i.e., solved without genuine multi-hop inference) or require only
simple factual recall. This limits the ability for such benchmarks to uncover
limitations of existing RAG systems. To address this gap, we present the first
pipeline for automatic, difficulty-controlled creation of
un$\underline{c}$heatable, $\underline{r}$ealistic, $\underline{u}$nanswerable,
and $\underline{m}$ulti-hop $\underline{q}$uerie$\underline{s}$ (CRUMQs),
adaptable to any corpus and domain. We use our pipeline to create CRUMQs over
two popular RAG datasets and demonstrate its effectiveness via benchmark
experiments on leading retrieval-augmented LLMs. Results show that compared to
prior RAG benchmarks, CRUMQs are highly challenging for RAG systems and achieve
up to 81.0\% reduction in cheatability scores. More broadly, our pipeline
offers a simple way to enhance benchmark difficulty and realism and drive
development of more capable RAG systems.

</details>


### [9] [Direct Multi-Token Decoding](https://arxiv.org/abs/2510.11958)
*Xuan Luo,Weizhi Wang,Xifeng Yan*

Main category: cs.CL

TL;DR: 提出直接多令牌解码（DMTD）方法，利用LLM中不同层功能分离的特点，仅使用后层生成多个令牌，实现2倍加速且性能损失极小。


<details>
  <summary>Details</summary>
Motivation: 基于预训练LLM中早期、中期和后期层分别负责输入理解、任务处理和输出转换的观察，假设经过前中期层处理后，隐藏状态已包含足够信息支持仅用后期层生成多个令牌。

Method: DMTD方法：仅使用LLM的后期层直接解码多个令牌，无需重复遍历早期和中期层。不引入额外参数、辅助程序或后生成验证。

Result: 在有限数据集上微调的DMTD Qwen3-4B模型已实现2倍加速，仅带来轻微性能损失。扩展分析表明使用更大训练数据集可进一步提升性能。

Conclusion: DMTD是一种高效的推理范式，利用LLM层间功能分离特性，无需额外开销即可显著提升生成速度，具有良好扩展性。

Abstract: Decoder-only transformers have become the standard architecture for large
language models (LLMs) due to their strong performance. Recent studies suggest
that, in pre-trained LLMs, early, middle, and late layers may serve distinct
roles: Early layers focus on understanding the input context, middle layers
handle task-specific processing, and late layers convert abstract
representations into output tokens. We hypothesize that once representations
have been processed by the early and middle layers, the resulting hidden states
may encapsulate sufficient information to support the generation of multiple
tokens using only the late layers, eliminating the need to repeatedly traverse
the early and middle layers. We refer to this inference paradigm as Direct
Multi-Token Decoding (DMTD). Unlike speculative decoding, our method introduces
no additional parameters, auxiliary routines, or post-generation verification.
Despite being trained on a limited dataset, a fine-tuned DMTD Qwen3-4B model
has already demonstrated promising results, achieving up to a 2x speedup with
only minor performance loss. Moreover, as shown in our scaling analysis, its
performance is expected to further improve with larger training datasets.

</details>


### [10] [Scaling Long-Horizon LLM Agent via Context-Folding](https://arxiv.org/abs/2510.11967)
*Weiwei Sun,Miao Lu,Zhan Ling,Kang Liu,Xuesong Yao,Yiming Yang,Jiecao Chen*

Main category: cs.CL

TL;DR: 提出了Context-Folding框架，让LLM智能体能够主动管理工作上下文，通过分支处理子任务后折叠中间步骤，显著减少上下文长度需求。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型智能体在长视野任务中受限于上下文长度，需要更有效的上下文管理方法。

Method: 开发了端到端强化学习框架FoldGRPO，使用过程奖励来鼓励有效的任务分解和上下文管理，支持智能体分支处理子任务后折叠中间步骤。

Result: 在复杂长视野任务（深度研究和软件工程）上，折叠智能体匹配或优于ReAct基线，同时使用10倍更小的活动上下文，显著优于基于摘要的上下文管理方法。

Conclusion: Context-Folding框架有效解决了LLM智能体在长视野任务中的上下文长度限制问题，提供了更高效的上下文管理方案。

Abstract: Large language model (LLM) agents are fundamentally constrained by context
length on long-horizon tasks. We introduce Context-Folding, a framework that
empowers agents to actively manage their working context. An agent can
procedurally branch into a sub-trajectory to handle a subtask and then fold it
upon completion, collapsing the intermediate steps while retaining a concise
summary of the outcome. To make this behavior learnable, we develop an
end-to-end reinforcement learning framework FoldGRPO with specific process
rewards to encourage effective task decomposition and context management. On
complex long-horizon tasks (Deep Research and SWE), our folding agent matches
or outperforms the ReAct baselines while using an active context 10$\times$
smaller and significantly outperforms models that rely on summarization-based
context management.

</details>


### [11] [Conjecturing: An Overlooked Step in Formal Mathematical Reasoning](https://arxiv.org/abs/2510.11986)
*Jasivan Alex Sivakumar,Philipp Borchert,Ronald Cardenas,Gerasimos Lampouras*

Main category: cs.CL

TL;DR: 论文提出了ConjectureBench数据集和评估框架，专门衡量LLMs的猜想能力，发现当前模型在自动形式化任务中的表现被高估，并提出了Lean-FIRe方法改进猜想和自动形式化性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动形式化研究忽视了猜想这一关键步骤，许多数学问题需要先猜想结论才能进行形式化。由于LLMs在自动形式化方面已有困难，且其猜想能力评估有限，需要专门研究这一能力。

Method: 扩展现有数据集创建ConjectureBench，重新设计评估框架和指标，提出推理时方法Lean-FIRe来改进猜想和自动形式化。

Result: 评估显示GPT-4.1和DeepSeek-V3.1的自动形式化性能在考虑猜想后被显著高估。Lean-FIRe方法实现了13个PutnamBench问题的端到端自动形式化（GPT-4.1）和7个（DeepSeek-V3.1）。

Conclusion: LLMs具备生成准确猜想所需的知识，但改进自动形式化性能需要将猜想作为独立任务处理，并研究如何正确集成到自动形式化流程中。

Abstract: Autoformalisation, the task of expressing informal mathematical statements in
formal language, is often viewed as a direct translation process. This,
however, disregards a critical preceding step: conjecturing. Many mathematical
problems cannot be formalised directly without first conjecturing a conclusion
such as an explicit answer, or a specific bound. Since Large Language Models
(LLMs) already struggle with autoformalisation, and the evaluation of their
conjecturing ability is limited and often entangled within autoformalisation or
proof, it is particularly challenging to understand its effect. To address this
gap, we augment existing datasets to create ConjectureBench, and redesign the
evaluation framework and metric specifically to measure the conjecturing
capabilities of LLMs both as a distinct task and within the autoformalisation
pipeline. Our evaluation of foundational models, including GPT-4.1 and
DeepSeek-V3.1, reveals that their autoformalisation performance is
substantially overestimated when the conjecture is accounted for during
evaluation. However, the conjecture should not be assumed to be provided. We
design an inference-time method, Lean-FIRe to improve conjecturing and
autoformalisation, which, to the best of our knowledge, achieves the first
successful end-to-end autoformalisation of 13 PutnamBench problems with GPT-4.1
and 7 with DeepSeek-V3.1. We demonstrate that while LLMs possess the requisite
knowledge to generate accurate conjectures, improving autoformalisation
performance requires treating conjecturing as an independent task, and
investigating further how to correctly integrate it within autoformalisation.
Finally, we provide forward-looking guidance to steer future research toward
improving conjecturing, an overlooked step of formal mathematical reasoning.

</details>


### [12] [SAGE: A Top-Down Bottom-Up Knowledge-Grounded User Simulator for Multi-turn AGent Evaluation](https://arxiv.org/abs/2510.11997)
*Ryan Shea,Yunan Lu,Liang Qiu,Zhou Yu*

Main category: cs.CL

TL;DR: SAGE是一个用于多轮交互代理评估的用户模拟框架，通过整合业务上下文知识来生成更真实和多样化的交互，能发现更多代理错误。


<details>
  <summary>Details</summary>
Motivation: 现有模拟用户方法通常建模通用用户，忽视了领域特定原则，无法捕捉真实行为。需要一种能结合业务知识的用户模拟框架来改进多轮交互代理的评估。

Method: 提出SAGE框架，整合自上而下的业务逻辑知识（如理想客户画像）和自下而上的业务代理基础设施知识（如产品目录、FAQ、知识库），基于真实客户角色生成交互。

Result: 该方法生成的交互更加真实和多样化，能识别多达33%的代理错误，显示出作为支持错误发现和迭代代理改进的有效评估工具。

Conclusion: SAGE通过整合业务上下文知识，显著提高了多轮交互代理评估的准确性和有效性，为代理开发和改进提供了更好的支持。

Abstract: Evaluating multi-turn interactive agents is challenging due to the need for
human assessment. Evaluation with simulated users has been introduced as an
alternative, however existing approaches typically model generic users and
overlook the domain-specific principles required to capture realistic behavior.
We propose SAGE, a novel user Simulation framework for multi-turn AGent
Evaluation that integrates knowledge from business contexts. SAGE incorporates
top-down knowledge rooted in business logic, such as ideal customer profiles,
grounding user behavior in realistic customer personas. We further integrate
bottom-up knowledge taken from business agent infrastructure (e.g., product
catalogs, FAQs, and knowledge bases), allowing the simulator to generate
interactions that reflect users' information needs and expectations in a
company's target market. Through empirical evaluation, we find that this
approach produces interactions that are more realistic and diverse, while also
identifying up to 33% more agent errors, highlighting its effectiveness as an
evaluation tool to support bug-finding and iterative agent improvement.

</details>


### [13] [Generate Logical Equivalence Questions](https://arxiv.org/abs/2510.12001)
*Xinyu Wang,Haoming Yu,Yicheng Yang,Zhiyuan Li*

Main category: cs.CL

TL;DR: 本文提出了一种自动生成离散数学逻辑等价问题的新方法，通过定义形式化语言和生成规则，开发线性时间算法来解决现有方法效率低和难度不均的问题。


<details>
  <summary>Details</summary>
Motivation: 在线教学时代抄袭问题日益严重，自动问题生成可以创建独特题目防止抄袭，同时提供大量练习题。现有方法生成所有满足约束的命题，导致效率低下且难度不均。

Method: 定义逻辑等价问题的形式化语言，将其转换为两组生成规则，开发线性时间算法进行问题生成。

Result: 学生实验显示生成问题的准确性与教科书问题相当；难度评估表明生成问题与教科书问题难度相似，优于大型语言模型生成的问题。

Conclusion: 提出的自动问题生成系统能够高效生成质量与教科书相当的逻辑等价问题，为解决学术不端和提供练习资源提供了有效方案。

Abstract: Academic dishonesty is met with zero tolerance in higher education, yet
plagiarism has become increasingly prevalent in the era of online teaching and
learning. Automatic Question Generation (AQG) presents a potential solution to
mitigate copying by creating unique questions for each student. Additionally,
AQG can provide a vast array of practice questions. Our AQG focuses on
generating logical equivalence questions for Discrete Mathematics, a
foundational course for first-year computer science students. A literature
review reveals that existing AQGs for this type of question generate all
propositions that meet user-defined constraints, resulting in inefficiencies
and a lack of uniform question difficulty. To address this, we propose a new
approach that defines logical equivalence questions using a formal language,
translates this language into two sets of generation rules, and develops a
linear-time algorithm for question generation. We evaluated our AQG through two
experiments. The first involved a group of students completing questions
generated by our system. Statistical analysis shows that the accuracy of these
questions is comparable to that of textbook questions. The second experiment
assessed the number of steps required to solve our generated questions,
textbook questions, and those generated by multiple large language models. The
results indicated that the difficulty of our questions was similar to that of
textbook questions, confirming the quality of our AQG.

</details>


### [14] [Information Extraction from Conversation Transcripts: Neuro-Symbolic vs. LLM](https://arxiv.org/abs/2510.12023)
*Alice Saebom Kwak,Maria Alexeeva,Gus Hahn-Powell,Keith Alcock,Kevin McLaughlin,Doug McCorkle,Gabe McNunn,Mihai Surdeanu*

Main category: cs.CL

TL;DR: 比较神经符号系统和基于LLM的信息抽取系统在农业领域的表现，LLM系统在F1分数上表现更好，但两种方法各有优缺点。


<details>
  <summary>Details</summary>
Motivation: 当前信息抽取领域过度依赖大语言模型，忽视了传统符号或统计系统的经验积累，需要系统比较不同方法的实际表现。

Method: 在农业领域的猪肉、乳制品和作物三个子领域中，对九个访谈数据分别使用神经符号系统和基于LLM的系统进行信息抽取评估。

Result: 基于LLM的系统在总体F1分数(69.4 vs 52.7)和核心信息F1分数(63.0 vs 47.2)上都优于神经符号系统。

Conclusion: 两种方法各有优劣：神经符号系统运行更快、控制性更强，但泛化能力差；LLM系统性能更高、部署更快，但运行较慢且存在幻觉风险。需要在性能、效率和可控性之间取得平衡。

Abstract: The current trend in information extraction (IE) is to rely extensively on
large language models, effectively discarding decades of experience in building
symbolic or statistical IE systems. This paper compares a neuro-symbolic (NS)
and an LLM-based IE system in the agricultural domain, evaluating them on nine
interviews across pork, dairy, and crop subdomains. The LLM-based system
outperforms the NS one (F1 total: 69.4 vs. 52.7; core: 63.0 vs. 47.2), where
total includes all extracted information and core focuses on essential details.
However, each system has trade-offs: the NS approach offers faster runtime,
greater control, and high accuracy in context-free tasks but lacks
generalizability, struggles with contextual nuances, and requires significant
resources to develop and maintain. The LLM-based system achieves higher
performance, faster deployment, and easier maintenance but has slower runtime,
limited control, model dependency and hallucination risks. Our findings
highlight the "hidden cost" of deploying NLP systems in real-world
applications, emphasizing the need to balance performance, efficiency, and
control.

</details>


### [15] [CPR: Mitigating Large Language Model Hallucinations with Curative Prompt Refinement](https://arxiv.org/abs/2510.12029)
*Jung-Woo Shim,Yeong-Joon Ju,Ji-Hoon Park,Seong-Whan Lee*

Main category: cs.CL

TL;DR: CPR框架通过清理不良提示和生成额外任务描述来减少LLM幻觉，显著提高生成质量，在无外部知识情况下胜率超过90%。


<details>
  <summary>Details</summary>
Motivation: LLM经常生成看似合理但错误的"幻觉"事实，主要原因是用户使用结构不良或模糊的提示，导致模型基于假设而非实际意图生成响应。

Method: 提出治愈性提示精炼(CPR)框架，使用微调的小型语言模型清理不良提示并生成额外信息性任务描述，以对齐用户意图和提示。

Result: 应用CPR后显著提高了生成质量并减少了幻觉，实证研究显示CPR处理后的提示在无外部知识情况下胜率超过90%。

Conclusion: CPR是一个即插即用的框架，能有效缓解由不良提示引起的LLM幻觉问题，提高模型输出的可靠性和质量。

Abstract: Recent advancements in large language models (LLMs) highlight their fluency
in generating responses to diverse prompts. However, these models sometimes
generate plausible yet incorrect ``hallucinated" facts, undermining trust. A
frequent but often overlooked cause of such errors is the use of poorly
structured or vague prompts by users, leading LLMs to base responses on assumed
rather than actual intentions. To mitigate hallucinations induced by these
ill-formed prompts, we introduce Curative Prompt Refinement (CPR), a
plug-and-play framework for curative prompt refinement that 1) cleans
ill-formed prompts, and 2) generates additional informative task descriptions
to align the intention of the user and the prompt using a fine-tuned small
language model. When applied to language models, we discover that CPR
significantly increases the quality of generation while also mitigating
hallucination. Empirical studies show that prompts with CPR applied achieves
over a 90\% win rate over the original prompts without any external knowledge.

</details>


### [16] [Multi-stage Prompt Refinement for Mitigating Hallucinations in Large Language Models](https://arxiv.org/abs/2510.12032)
*Jung-Woo Shim,Yeong-Joon Ju,Ji-Hoon Park,Seong-Whan Lee*

Main category: cs.CL

TL;DR: 提出了多阶段提示精炼框架MPR，通过系统化改进格式不良的提示来减少LLM幻觉，在幻觉基准测试中精炼后的提示胜率超过85%


<details>
  <summary>Details</summary>
Motivation: LLM在幻觉问题上仍面临挑战，而格式不良提示（歧义、语法错误、信息不完整）的影响相对未被充分探索

Method: MPR框架通过多个阶段系统改进格式不良提示，每个阶段使用微调的小语言模型处理特定错误（标点、拼写、关键词误用），采用迭代增强和带排名的自反机制

Result: 在幻觉基准测试中，MPR精炼的提示相比原始形式胜率超过85%，且能与现有后处理幻觉缓解框架结合进一步提升效果

Conclusion: MPR提供了一个轻量级、适应性强的解决方案，可在多个领域增强LLM的可靠性

Abstract: Recent advancements in large language models (LLMs) have shown strong
performance in natural language understanding and generation tasks. However,
LLMs continue to encounter challenges with hallucinations, where models
generate plausible but incorrect information. While several factors contribute
to hallucinations, the impact of ill-formed prompts, prompts with ambiguous
wording, incorrect grammar, or incomplete information, was relatively under
explored. To address this, we introduce Multi-stage Prompt Refinement (MPR), a
framework designed to systematically improve these ill-formed prompts across
multiple stages. Each stage addresses specific errors such as punctuation,
typographical mistakes, and misuse of key terms, using small language models
(SLMs) fine-tuned for these tasks. MPR iteratively enhances the clarity of
prompts with additional context and employs a self-reflection mechanism with
ranking to prioritize the most relevant input. Experimental results on
hallucination benchmarks show that prompts refined by MPR achieve over an 85~\%
win rate compared to their original forms, demonstrating its effectiveness in
reducing hallucinations and improving LLM output accuracy. Interestingly, we
reveal that MPR can be combined with existing post-hoc hallucination mitigation
frameworks, further enhancing its versatility. MPR provides a lightweight and
adaptable solution for enhancing LLM reliability across various domains.

</details>


### [17] [On the Interplay between Human Label Variation and Model Fairness](https://arxiv.org/abs/2510.12036)
*Kemal Kurniawan,Meladel Mistica,Timothy Baldwin,Jey Han Lau*

Main category: cs.CL

TL;DR: 本文探讨了人类标签变异对模型公平性的影响，发现在没有显式去偏的情况下，HLV训练方法对公平性有积极影响。


<details>
  <summary>Details</summary>
Motivation: 研究人类标签变异对模型公平性的影响，这是一个尚未被探索的领域。

Method: 通过比较在多数投票标签上训练与一系列HLV方法的训练效果。

Result: 实验表明，在没有显式去偏的情况下，HLV训练方法对公平性有积极影响。

Conclusion: 人类标签变异训练方法可以在不进行显式去偏的情况下改善模型公平性。

Abstract: The impact of human label variation (HLV) on model fairness is an unexplored
topic. This paper examines the interplay by comparing training on majority-vote
labels with a range of HLV methods. Our experiments show that without explicit
debiasing, HLV training methods have a positive impact on fairness.

</details>


### [18] [Uncertainty Quantification for Hallucination Detection in Large Language Models: Foundations, Methodology, and Future Directions](https://arxiv.org/abs/2510.12040)
*Sungmin Kang,Yavuz Faruk Bakman,Duygu Nur Yaldiz,Baturalp Buyukates,Salman Avestimehr*

Main category: cs.CL

TL;DR: 该论文综述了大语言模型中的不确定性量化方法，重点关注其在幻觉检测中的应用，系统分类现有方法并讨论未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在实际部署中存在可靠性问题，容易产生看似合理但事实错误的幻觉输出，需要通过不确定性量化来评估模型生成的可信度。

Method: 论文首先介绍不确定性量化的理论基础，包括认知不确定性和偶然不确定性的传统区分，然后系统分类现有方法，并展示代表性方法的实证结果。

Result: 通过不确定性量化可以有效识别不可靠的模型生成，提高大语言模型的可靠性，为幻觉检测提供了机制支持。

Conclusion: 不确定性量化是解决大语言模型幻觉问题的关键研究方向，论文为当前LLM不确定性量化在幻觉检测领域的研究现状提供了清晰图景，并指出了未来发展方向。

Abstract: The rapid advancement of large language models (LLMs) has transformed the
landscape of natural language processing, enabling breakthroughs across a wide
range of areas including question answering, machine translation, and text
summarization. Yet, their deployment in real-world applications has raised
concerns over reliability and trustworthiness, as LLMs remain prone to
hallucinations that produce plausible but factually incorrect outputs.
Uncertainty quantification (UQ) has emerged as a central research direction to
address this issue, offering principled measures for assessing the
trustworthiness of model generations. We begin by introducing the foundations
of UQ, from its formal definition to the traditional distinction between
epistemic and aleatoric uncertainty, and then highlight how these concepts have
been adapted to the context of LLMs. Building on this, we examine the role of
UQ in hallucination detection, where quantifying uncertainty provides a
mechanism for identifying unreliable generations and improving reliability. We
systematically categorize a wide spectrum of existing methods along multiple
dimensions and present empirical results for several representative approaches.
Finally, we discuss current limitations and outline promising future research
directions, providing a clearer picture of the current landscape of LLM UQ for
hallucination detection.

</details>


### [19] [Improving Text-to-Image Generation with Input-Side Inference-Time Scaling](https://arxiv.org/abs/2510.12041)
*Ruibo Chen,Jiacheng Pan,Heng Huang,Zhenheng Yang*

Main category: cs.CL

TL;DR: 提出基于大语言模型的提示词重写框架，通过奖励系统和迭代DPO训练优化文本到图像生成的提示词，无需监督微调数据即可提升图像质量和对齐度。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型在处理简单或模糊提示词时表现不佳，导致图像-文本对齐度、美学质量和整体效果不理想。

Method: 使用大语言模型作为提示词重写器，设计奖励系统和迭代直接偏好优化训练流程，无需监督微调数据即可优化提示词。

Result: 在多种T2I模型和基准测试中，提示词重写器持续提升了图像-文本对齐度、视觉质量和美学效果，优于强基线方法，并展现出良好的跨模型迁移能力。

Conclusion: 提示词重写是一种有效、可扩展且实用的模型无关策略，能够显著提升文本到图像生成系统的性能。

Abstract: Recent advances in text-to-image (T2I) generation have achieved impressive
results, yet existing models often struggle with simple or underspecified
prompts, leading to suboptimal image-text alignment, aesthetics, and quality.
We propose a prompt rewriting framework that leverages large language models
(LLMs) to refine user inputs before feeding them into T2I backbones. Our
approach introduces a carefully designed reward system and an iterative direct
preference optimization (DPO) training pipeline, enabling the rewriter to
enhance prompts without requiring supervised fine-tuning data. We evaluate our
method across diverse T2I models and benchmarks. Results show that our prompt
rewriter consistently improves image-text alignment, visual quality, and
aesthetics, outperforming strong baselines. Furthermore, we demonstrate strong
transferability by showing that a prompt rewriter trained on one T2I backbone
generalizes effectively to others without needing to be retrained. We also
systematically study scalability, evaluating how performance gains scale with
the capacity of the large LLM used as the rewriter. These findings highlight
that prompt rewriting is an effective, scalable, and practical model-agnostic
strategy for improving T2I systems. We plan to release the code and trained
prompt rewriters soon.

</details>


### [20] [Hierarchical Alignment: Surgical Fine-Tuning via Functional Layer Specialization in Large Language Models](https://arxiv.org/abs/2510.12044)
*Yukun Zhang,Qi Dong*

Main category: cs.CL

TL;DR: 提出分层对齐方法，针对Transformer不同功能层进行定向优化，避免传统DPO的"对齐税"问题，在语法流畅性、事实一致性和逻辑连贯性方面均取得显著提升


<details>
  <summary>Details</summary>
Motivation: 传统对齐方法将模型视为单一实体进行统一优化，忽视了Transformer架构中不同层处理不同任务的功能专门化特性

Method: 分层对齐方法，将模型层分为局部（语法）、中间（逻辑）和全局（事实性）三个功能块，使用LoRA进行定向微调

Result: 局部层对齐提升语法流畅性，全局层对齐不仅改善事实一致性，还是增强逻辑连贯性的最有效策略，所有分层策略都避免了标准DPO中的对齐税问题

Conclusion: 分层对齐为模型对齐提供了更资源高效、可控和可解释的路径，从整体优化转向结构感知的精细微调具有巨大潜力

Abstract: Existing alignment techniques for Large Language Models (LLMs), such as
Direct Preference Optimization (DPO), typically treat the model as a monolithic
entity, applying uniform optimization pressure across all layers. This approach
overlooks the functional specialization within the Transformer architecture,
where different layers are known to handle distinct tasks from syntax to
abstract reasoning. In this paper, we challenge this one-size-fits-all paradigm
by introducing Hierarchical Alignment, a novel method that applies targeted DPO
to distinct functional blocks of a model's layers: local (syntax), intermediate
(logic), and global (factuality). Through a series of controlled experiments on
state-of-the-art models like Llama-3.1-8B and Qwen1.5-7B using LoRA for
surgical fine-tuning, our results, evaluated by a powerful LLM-as-Judge,
demonstrate significant and predictable improvements. Specifically, aligning
the local layers (Local-Align) enhances grammatical fluency. More importantly,
aligning the global layers (Global-Align) not only improves factual consistency
as hypothesized but also proves to be the most effective strategy for enhancing
logical coherence, outperforming all baselines. Critically, all hierarchical
strategies successfully avoid the "alignment tax" observed in standard DPO,
where gains in fluency come at the cost of degraded logical reasoning. These
findings establish a more resource-efficient, controllable, and interpretable
path for model alignment, highlighting the immense potential of shifting from
monolithic optimization to structure-aware surgical fine-tuning to build more
advanced and reliable LLMs.

</details>


### [21] [APCE: Adaptive Progressive Context Expansion for Long Context Processing](https://arxiv.org/abs/2510.12051)
*Baisub Lee,Sanghyun Byun,Mohanad Odema,Jung Guack,Jacob Song,Woo Seong Chung*

Main category: cs.CL

TL;DR: APCE是一种上下文感知的解决方案，通过低维语义相似性匹配选择最重要的输入块，在长文本摘要任务中仅使用50%-70%的输入序列就能实现与完整基线相当或更优的性能，同时显著降低内存占用。


<details>
  <summary>Details</summary>
Motivation: 解决长上下文Transformer模型的两个关键挑战：(1) 由于二次自注意力和线性KV缓存扩展导致的内存占用增长；(2) ContextRot现象，即随着上下文长度增加，Transformer架构性能下降。

Method: 提出APCE方法，通过低维语义相似性匹配选择最重要的输入块，直接对输入进行操作，不依赖底层硬件或CUDA环境。

Result: APCE在仅使用50%-70%输入序列的情况下，实现了与完整密集基线相当或更优的摘要性能，同时显著提升了KV缓存和自注意力的内存效率。

Conclusion: APCE为长上下文Transformer模型提供了一种上下文感知的效率解决方案，有望激发针对其他相关长上下文任务的进一步研究。

Abstract: Deploying useful Long-Context Transformer Models (LCTMs) requires addressing
two key challenges: (1) A growing memory footprint due to quadratic
self-attention and linear KV-cache scaling in memory as sequence length
increases; (2) the ContextRot phenomena where empirical evidence suggests that
transformer architecture's performance degrades with increasing context length.
Given the shared dependency on the input, a natural question arises: Can we
surgically select the most important input chunks for processing to
synergistically (a) reduce the memory footprint, and (b) mitigate the
ContextRot effects? In this paper, we answer this question in the affirmative
for long-context summarization tasks. We propose APCE as a context-aware
solution to select the most important input chunks through low-dimensional
semantic similarity matching with the current query. By directly operating on
the input, APCE decouples from strict dependency on underlying hardware or CUDA
environments, promising a compatible solution scalable to different deployment
systems. Our empirical evaluations have demonstrated superior or on-par
summarization performance for APCE compared to the full dense baseline using a
fraction (50%-70%) of the input sequence resulting in KV-cache and
self-attention memory efficiency improvements. We hope our findings inspire
further research on context-aware efficiency solutions for LCTMs geared towards
other relevant long-context tasks.

</details>


### [22] [An AI-Based Behavioral Health Safety Filter and Dataset for Identifying Mental Health Crises in Text-Based Conversations](https://arxiv.org/abs/2510.12083)
*Benjamin W. Nelson,Celeste Wong,Matthew T. Silvestrini,Sooyoon Shin,Alanna Robinson,Jessica Lee,Eric Yang,John Torous,Andrew Trister*

Main category: cs.CL

TL;DR: Verily行为健康安全过滤器(VBHSF)在精神健康危机检测方面表现优异，在两个数据集上均实现了高敏感性和特异性，优于NVIDIA NeMo和OpenAI Omni Moderation Latest等开源内容审核护栏。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理精神健康紧急情况时经常提供有害或不恰当的建议，导致破坏性行为。需要开发专门的安全过滤器来防止这些风险。

Method: 使用Verily精神健康危机数据集(1,800条模拟消息)和NVIDIA Aegis AI内容安全数据集子集(794条精神健康相关消息)，通过临床医生标注评估VBHSF性能，并与NVIDIA NeMo和OpenAI Omni Moderation Latest进行比较分析。

Result: VBHSF在Verily数据集上敏感性0.990、特异性0.992、F1分数0.939；在NVIDIA数据集上敏感性0.982、准确性0.921、特异性0.859。在所有比较中VBHSF均表现出显著更高的敏感性(p<0.001)。

Conclusion: VBHSF展示了稳健且可推广的性能，优先考虑敏感性以最小化漏检危机，这对医疗应用至关重要，优于现有的开源内容审核解决方案。

Abstract: Large language models often mishandle psychiatric emergencies, offering
harmful or inappropriate advice and enabling destructive behaviors. This study
evaluated the Verily behavioral health safety filter (VBHSF) on two datasets:
the Verily Mental Health Crisis Dataset containing 1,800 simulated messages and
the NVIDIA Aegis AI Content Safety Dataset subsetted to 794 mental
health-related messages. The two datasets were clinician-labelled and we
evaluated performance using the clinician labels. Additionally, we carried out
comparative performance analyses against two open source, content moderation
guardrails: OpenAI Omni Moderation Latest and NVIDIA NeMo Guardrails. The VBHSF
demonstrated, well-balanced performance on the Verily Mental Health Crisis
Dataset v1.0, achieving high sensitivity (0.990) and specificity (0.992) in
detecting any mental health crises. It achieved an F1-score of 0.939,
sensitivity ranged from 0.917-0.992, and specificity was >= 0.978 in
identifying specific crisis categories. When evaluated against the NVIDIA Aegis
AI Content Safety Dataset 2.0, VBHSF performance remained highly sensitive
(0.982) and accuracy (0.921) with reduced specificity (0.859). When compared
with the NVIDIA NeMo and OpenAI Omni Moderation Latest guardrails, the VBHSF
demonstrated superior performance metrics across both datasets, achieving
significantly higher sensitivity in all cases (all p < 0.001) and higher
specificity relative to NVIDIA NeMo (p < 0.001), but not to OpenAI Omni
Moderation Latest (p = 0.094). NVIDIA NeMo and OpenAI Omni Moderation Latest
exhibited inconsistent performance across specific crisis types, with
sensitivity for some categories falling below 0.10. Overall, the VBHSF
demonstrated robust, generalizable performance that prioritizes sensitivity to
minimize missed crises, a crucial feature for healthcare applications.

</details>


### [23] [Deep Associations, High Creativity: A Simple yet Effective Metric for Evaluating Large Language Models](https://arxiv.org/abs/2510.12110)
*Ziliang Qiu,Renfen Hu*

Main category: cs.CL

TL;DR: 提出了PACE方法，通过让LLM生成平行关联链来评估其创造力，有效避免数据污染问题，并与人类创造力评估结果高度相关。


<details>
  <summary>Details</summary>
Motivation: LLM创造力评估面临数据污染和人工评估成本高的问题，需要一种高效可靠的评估方法。

Method: PACE方法要求LLM生成平行关联链，通过分析这些链来评估模型的创造力表现。

Result: PACE与Chatbot Arena Creative Writing排名高度相关（Spearman's ρ=0.739），高性能LLM达到普通人水平，但专业人士仍优于LLM。语言分析显示人类和LLM关联都趋向抽象化，但人类关联模式更多样。

Conclusion: PACE是评估LLM创造力的有效方法，高性能LLM在创造力方面接近普通人水平，但与专业人士仍有差距，且人类关联模式更丰富多样。

Abstract: The evaluation of LLMs' creativity represents a crucial research domain,
though challenges such as data contamination and costly human assessments often
impede progress. Drawing inspiration from human creativity assessment, we
propose PACE, asking LLMs to generate Parallel Association Chains to Evaluate
their creativity. PACE minimizes the risk of data contamination and offers a
straightforward, highly efficient evaluation, as evidenced by its strong
correlation with Chatbot Arena Creative Writing rankings (Spearman's $\rho =
0.739$, $p < 0.001$) across various proprietary and open-source models. A
comparative analysis of associative creativity between LLMs and humans reveals
that while high-performing LLMs achieve scores comparable to average human
performance, professional humans consistently outperform LLMs. Furthermore,
linguistic analysis reveals that both humans and LLMs exhibit a trend of
decreasing concreteness in their associations, and humans demonstrating a
greater diversity of associative patterns.

</details>


### [24] [Tracing Multilingual Knowledge Acquisition Dynamics in Domain Adaptation: A Case Study of English-Japanese Biomedical Adaptation](https://arxiv.org/abs/2510.12115)
*Xin Zhao,Naoki Yoshinaga,Yuma Tsuta,Akiko Aizawa*

Main category: cs.CL

TL;DR: 本文研究了多语言领域适应中LLMs的学习动态，提出了AdaXEval评估方法，发现即使使用高质量双语语料，跨语言知识迁移仍然具有挑战性。


<details>
  <summary>Details</summary>
Motivation: 多语言领域适应中知识获取机制（包括语言内学习和跨语言迁移）尚未充分探索，导致在低资源场景下性能不佳。

Method: 提出AdaXEval自适应评估方法，从训练用的双语领域语料构建多选题数据集，通过持续训练跟踪LLMs获取领域知识的过程。

Result: 在13B英日双语LLM上的实验表明，尽管使用高质量双语语料，跨语言迁移仍然困难。

Conclusion: 跨语言知识迁移在多语言领域适应中仍具挑战性，需要进一步研究改进。

Abstract: Multilingual domain adaptation (ML-DA) is widely used to learn new domain
knowledge across languages into large language models (LLMs). Although many
methods have been proposed to improve domain adaptation, the mechanisms of
multilingual knowledge acquisition, how domain knowledge is learned within a
language and transferred across languages, remain underexplored. This gap leads
to suboptimal performance, particularly in low-resource settings. This work
examines the learning dynamics of LLMs during ML-DA. Because prior ML-DA
studies often train and evaluate on datasets with mismatched knowledge
coverage, we propose AdaXEval, an adaptive evaluation method that builds
multiple-choice QA datasets from the same bilingual domain corpus used for
training, thereby directly studying multilingual knowledge acquisition. Through
continual training of LLMs with diverse data recipes, we track how LLMs acquire
domain facts and pinpoint the mechanism behind the transformation process from
domain training data to knowledge. Our experiments on a 13B English-Japanese
bilingual LLM reveal that cross-lingual transfer remains challenging despite a
high-quality bilingual corpus. The code has been released.

</details>


### [25] [Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models](https://arxiv.org/abs/2510.12116)
*Bajian Xiang,Shuaijiang Zhao,Tingwei Guo,Wei Zou*

Main category: cs.CL

TL;DR: 本文揭示了端到端大语音语言模型在语音和文本输入之间存在模态差距，通过分析表征相似性和token级对齐模式，提出了角度投影和长度归一化等干预策略来改善语音输入性能。


<details>
  <summary>Details</summary>
Motivation: 虽然端到端大语音语言模型在对话生成方面表现出色，但在语义理解基准测试中始终落后于传统流水线系统，需要探究语音和文本输入之间的性能差距原因。

Method: 系统分析语音和文本表征的粗粒度和细粒度对齐模式，提出对齐路径评分量化token级对齐质量，并设计角度投影和长度归一化等干预策略。

Result: 发现深层表征在方向上更对齐但在幅度上更发散，表征相似性与模态差距强相关，token级对齐模式与模态差距有更强相关性，干预策略显示改善语音输入正确性的潜力。

Conclusion: 本研究首次系统分析了大语音语言模型中的模态差距和对齐机制，为未来优化提供了理论和方法指导。

Abstract: End-to-end Large Speech Language Models (LSLMs) have demonstrated impressive
conversational generation abilities, yet consistently fall short of traditional
pipeline systems on semantic understanding benchmarks. In this work, we reveal
through systematic experimentation that although LSLMs lose some text input
performance after speech-text alignment training, the performance gap between
speech and text inputs is more pronounced, which we refer to as the modality
gap. To understand this gap, we analyze both coarse- and fine-grained text and
speech representations. At the coarse-grained level, representations of speech
and text in deeper layers are found to be increasingly aligned in direction
(cosine similarity), while concurrently diverging in magnitude (Euclidean
distance). We further find that representation similarity is strongly
correlated with the modality gap. At the fine-grained level, a spontaneous
token-level alignment pattern between text and speech representations is
observed. Based on this, we introduce the Alignment Path Score to quantify
token-level alignment quality, which exhibits stronger correlation with the
modality gap. Building on these insights, we design targeted interventions on
critical tokens through angle projection and length normalization. These
strategies demonstrate the potential to improve correctness for speech inputs.
Our study provides the first systematic empirical analysis of the modality gap
and alignment mechanisms in LSLMs, offering both theoretical and methodological
guidance for future optimization.

</details>


### [26] [SafeMT: Multi-turn Safety for Multimodal Language Models](https://arxiv.org/abs/2510.12133)
*Han Zhu,Juntao Dai,Jiaming Ji,Haoran Li,Chengkun Cai,Pengcheng Wen,Chi-Min Chan,Boyuan Chen,Yaodong Yang,Sirui Han,Yike Guo*

Main category: cs.CL

TL;DR: SafeMT基准测试评估多模态大语言模型在多轮对话中的安全性，发现对话轮数增加会提高攻击成功率，并提出对话安全调节器来检测恶意意图。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在多轮对话中的安全问题日益突出，现有基准测试未能充分评估这种情况下的风险。

Method: 构建包含10,000个样本的SafeMT基准测试，涵盖17种场景和4种越狱方法，并提出安全指数(SI)评估模型安全性，同时开发对话安全调节器检测恶意意图。

Result: 评估17个模型发现，随着有害对话轮数增加，攻击成功率上升，表明模型安全机制在对话交互中识别危险的能力不足。

Conclusion: 多轮对话中的安全风险被低估，提出的对话安全调节器比现有防护模型更有效地降低多轮攻击成功率。

Abstract: With the widespread use of multi-modal Large Language models (MLLMs), safety
issues have become a growing concern. Multi-turn dialogues, which are more
common in everyday interactions, pose a greater risk than single prompts;
however, existing benchmarks do not adequately consider this situation. To
encourage the community to focus on the safety issues of these models in
multi-turn dialogues, we introduce SafeMT, a benchmark that features dialogues
of varying lengths generated from harmful queries accompanied by images. This
benchmark consists of 10,000 samples in total, encompassing 17 different
scenarios and four jailbreak methods. Additionally, we propose Safety Index
(SI) to evaluate the general safety of MLLMs during conversations. We assess
the safety of 17 models using this benchmark and discover that the risk of
successful attacks on these models increases as the number of turns in harmful
dialogues rises. This observation indicates that the safety mechanisms of these
models are inadequate for recognizing the hazard in dialogue interactions. We
propose a dialogue safety moderator capable of detecting malicious intent
concealed within conversations and providing MLLMs with relevant safety
policies. Experimental results from several open-source models indicate that
this moderator is more effective in reducing multi-turn ASR compared to existed
guard models.

</details>


### [27] [Credal Transformer: A Principled Approach for Quantifying and Mitigating Hallucinations in Large Language Models](https://arxiv.org/abs/2510.12137)
*Shihao Ji,Zihui Song,Jiajie Huang*

Main category: cs.CL

TL;DR: 提出了Credal Transformer来解决LLM幻觉问题，用基于证据理论的Credal注意力机制替代标准注意力，通过生成分布集合来量化不确定性，显著减少自信错误。


<details>
  <summary>Details</summary>
Motivation: Transformer的Softmax函数产生"人工确定性"，将模糊的注意力分数压缩为单一概率分布，丢弃了每层的不确定性信息，导致LLM产生事实错误但自信的断言。

Method: 引入Credal Transformer，用基于证据理论的Credal注意力机制(CAM)替代标准注意力。CAM生成"credal set"（分布集合）而非单一注意力向量，通过将注意力分数重新概念化为Dirichlet分布的证据质量来实现。

Result: Credal Transformer能够识别分布外输入、量化模糊性，并在无法回答的问题上通过弃权显著减少自信错误。

Conclusion: 提出了一种新的架构来减轻幻觉，以及将不确定性量化直接集成到模型中的设计范式，为更可靠的AI奠定了基础。

Abstract: Large Language Models (LLMs) hallucinate, generating factually incorrect yet
confident assertions. We argue this stems from the Transformer's Softmax
function, which creates "Artificial Certainty" by collapsing ambiguous
attention scores into a single probability distribution, discarding uncertainty
information at each layer. To fix this, we introduce the Credal Transformer,
which replaces standard attention with a Credal Attention Mechanism (CAM) based
on evidential theory. CAM produces a "credal set" (a set of distributions)
instead of a single attention vector, with the set's size directly measuring
model uncertainty. We implement this by re-conceptualizing attention scores as
evidence masses for a Dirichlet distribution: sufficient evidence recovers
standard attention, while insufficient evidence yields a diffuse distribution,
representing ambiguity. Empirically, the Credal Transformer identifies
out-of-distribution inputs, quantifies ambiguity, and significantly reduces
confident errors on unanswerable questions by abstaining. Our contribution is a
new architecture to mitigate hallucinations and a design paradigm that
integrates uncertainty quantification directly into the model, providing a
foundation for more reliable AI.

</details>


### [28] [A Survey on Parallel Reasoning](https://arxiv.org/abs/2510.12164)
*Ziqi Wang,Boye Niu,Zipeng Gao,Zhi Zheng,Tong Xu,Linghui Meng,Zhongli Li,Jing Liu,Yilong Chen,Chen Zhu,Hua Wu,Haifeng Wang,Enhong Chen*

Main category: cs.CL

TL;DR: 这篇论文综述了并行推理在大型语言模型中的进展和挑战，提出了形式化定义，建立了分类体系，并讨论了应用场景和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力增强，并行推理作为一种新的推理范式出现，能够通过同时探索多条思路来提高推理的鲁棒性，克服传统顺序方法的脆弱性。

Method: 首先给出并行推理的形式化定义，然后基于新颖的分类法组织讨论先进技术，包括非交互式推理、交互式推理和效率导向的解码策略。

Result: 建立了并行推理的完整理论框架，系统梳理了相关技术方法，并探索了在解决复杂问题和增强LLM输出可靠性等方面的应用场景。

Conclusion: 论文总结了并行推理的核心挑战，提出了未来研究方向，旨在为初学者提供路线图并鼓励更多改进并行推理方法的研究。

Abstract: With the increasing capabilities of Large Language Models (LLMs), parallel
reasoning has emerged as a new inference paradigm that enhances reasoning
robustness by concurrently exploring multiple lines of thought before
converging on a final answer. It has become a significant trend to explore
parallel reasoning to overcome the fragility of standard sequential methods and
improve practical performance. In this paper, we aim to survey and summarize
the progress and challenges of parallel reasoning. We first present a formal
definition of parallel reasoning and clarify its distinction from related
concepts like Chain-of-Thought. Then, we organize and discuss advanced
techniques based on a novel taxonomy, including non-interactive reasoning,
interactive reasoning, and efficiency-focused decoding strategies.
Additionally, we explore various application scenarios, such as solving complex
problems and enhancing the reliability of LLM outputs.Finally, we highlight the
core challenges of parallel reasoning and suggest potential directions for
future research. We hope that our work can provide a useful roadmap for
beginners and encourage more research on improving parallel reasoning methods.
Related source can be avaliable in
https://github.com/PPPP-kaqiu/Awesome-Parallel-Reasoning.

</details>


### [29] [Towards Inference-time Scaling for Continuous Space Reasoning](https://arxiv.org/abs/2510.12167)
*Minghan Wang,Thuy-Trang Vu,Ehsan Shareghi,Gholamreza Haffari*

Main category: cs.CL

TL;DR: 本文研究了将离散推理空间中的多样本生成和奖励模型重排序技术应用于连续推理空间的可行性，发现虽然能生成多样化的推理路径，但由于缺乏关键归纳偏置，PRM和ORM模型在连续空间中难以有效区分正确和错误推理。


<details>
  <summary>Details</summary>
Motivation: 探索已证明在离散文本推理中有效的推理时扩展技术（多样本生成结合PRM/ORM重排序）是否能够成功应用于连续空间推理，以COCONUT连续推理语言模型为基础。

Method: 使用基于dropout的采样生成多样化推理路径，通过Pass@N分析评估性能潜力，并探究几何特性和轨迹动态等各方面来识别PRM和ORM在连续空间中失效的原因。

Result: 多样本生成在连续空间中具有显著性能提升的潜力，但PRM和ORM重排序仅带来边际改进，因为连续思维表示缺乏关键归纳偏置，无法有效区分正确和错误推理。

Conclusion: 连续推理语言模型的训练框架不仅需要优化准确性，还应明确纳入可在推理时用于区分正确和错误思维的归纳偏置。

Abstract: Inference-time scaling through multiple sample generation in combination with
Process- or Outcome-Reward Model (PRM or ORM) re-ranking has proven effective
for text-based reasoning in large language models. This paper investigates
whether such established techniques can be successfully adapted to reasoning in
the continuous space, using COCONUT (Hao et al. 2024) continuous space
reasoning LM as the backbone. We demonstrate the feasibility of generating
diverse reasoning paths through dropout-based sampling. Our Pass@N analysis on
the generated samples reveals the potential that could enable a significant
gain in performance akin to observed gain in the discrete space. However, we
highlight unique challenges faced for materializing this gain in the continuous
thought space. In particular, working recipes for data generation and training
PRM and ORM models in the discrete space unlocks only marginal improvements in
the continuous space. Through probing various aspects including geometric
properties and trajectory dynamics we identify the underlying reasons that
prevent effective discrimination between correct and incorrect reasoning
(essential for the functioning of PRM and ORM). Our findings reveal that
current limitations stem from the absence of key inductive biases in continuous
thought representations. We argue that the training frameworks for continuous
reasoning LMs require not only to optimize for accuracy but also to explicitly
incorporate inductive biases that could be utilized during inference-time for
discrimination of correct and incorrect thoughts.\footnote{Our code and data
will be publicly available.}

</details>


### [30] [From Knowledge to Treatment: Large Language Model Assisted Biomedical Concept Representation for Drug Repurposing](https://arxiv.org/abs/2510.12181)
*Chengrui Xiang,Tengfei Ma,Xiangzheng Fu,Yiping Liu,Bosheng Song,Xiangxiang Zeng*

Main category: cs.CL

TL;DR: LLaDR是一个基于大型语言模型的药物重定位框架，通过从LLMs提取语义丰富的治疗相关文本表示来增强知识图谱嵌入，从而改进生物医学概念表示，在药物重定位任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了真实实验室中常识性生物医学概念知识，如某些药物与特定治疗根本不相容的机制先验知识，导致生物医学知识图谱表示不足。

Method: 从大型语言模型提取语义丰富的治疗相关文本表示，用于微调知识图谱嵌入模型，将治疗相关知识注入KGE中。

Result: 实验表明LLaDR在不同场景下均达到最先进性能，阿尔茨海默病案例研究进一步证实其鲁棒性和有效性。

Conclusion: LLaDR通过LLM增强的知识图谱表示显著提升了生物医学概念语义理解，为复杂和罕见疾病的药物重定位提供了有效解决方案。

Abstract: Drug repurposing plays a critical role in accelerating treatment discovery,
especially for complex and rare diseases. Biomedical knowledge graphs (KGs),
which encode rich clinical associations, have been widely adopted to support
this task. However, existing methods largely overlook common-sense biomedical
concept knowledge in real-world labs, such as mechanistic priors indicating
that certain drugs are fundamentally incompatible with specific treatments. To
address this gap, we propose LLaDR, a Large Language Model-assisted framework
for Drug Repurposing, which improves the representation of biomedical concepts
within KGs. Specifically, we extract semantically enriched treatment-related
textual representations of biomedical entities from large language models
(LLMs) and use them to fine-tune knowledge graph embedding (KGE) models. By
injecting treatment-relevant knowledge into KGE, LLaDR largely improves the
representation of biomedical concepts, enhancing semantic understanding of
under-studied or complex indications. Experiments based on benchmarks
demonstrate that LLaDR achieves state-of-the-art performance across different
scenarios, with case studies on Alzheimer's disease further confirming its
robustness and effectiveness. Code is available at
https://github.com/xiaomingaaa/LLaDR.

</details>


### [31] [Not in Sync: Unveiling Temporal Bias in Audio Chat Models](https://arxiv.org/abs/2510.12185)
*Jiayu Yao,Shenghua Liu,Yiwei Wang,Rundong Cheng,Lingrui Mei,Baolong Bi,Zhen Xiong,Xueqi Cheng*

Main category: cs.CL

TL;DR: 该研究首次系统性地揭示了大音频语言模型在时间戳预测中存在的时间偏差问题，即模型预测的事件发生时间往往与真实时间存在系统性偏差。


<details>
  <summary>Details</summary>
Motivation: 尽管大音频语言模型在音频理解和多模态推理中应用日益广泛，但其定位事件发生时间的能力尚未得到充分探索。研究者希望系统研究这些模型在时间戳预测中的局限性。

Method: 通过在带时间戳的数据集上进行控制实验，研究者量化了时间偏差现象，并提出了时间偏差指数来测量预测事件时间的系统性错位，同时开发了可视化框架。

Result: 研究发现：(i)时间偏差在数据集和模型中普遍存在；(ii)偏差随音频长度增加而增大，在长录音中可累积达数十秒；(iii)偏差在不同事件类型和位置间存在差异。

Conclusion: 当前大音频语言模型存在根本性局限，需要开发具有时间鲁棒性的架构来解决时间偏差问题。

Abstract: Large Audio Language Models (LALMs) are increasingly applied to audio
understanding and multimodal reasoning, yet their ability to locate when events
occur remains underexplored. We present the first systematic study of temporal
bias in LALMs, revealing a key limitation in their timestamp prediction. For
example, when asked "At which second does the lecturer introduce the key
formula?", models often predict timestamps that are consistently earlier or
later than the ground truth. Through controlled experiments on timestamped
datasets, we find that temporal bias (i) is prevalent across datasets and
models, (ii) increases with audio length - even accumulating to tens of seconds
in extended recordings, and (iii) varies across event types and positions. We
quantify this effect with the Temporal Bias Index (TBI), measuring systematic
misalignment in predicted event timings, and complement it with a visualization
framework. Our findings highlight a fundamental limitation in current LALMs and
call for the development of temporally robust architectures.

</details>


### [32] [DPO-Tuned Large Language Models for Segmentation in Simultaneous Speech Translation](https://arxiv.org/abs/2510.12195)
*Zeyu Yang,Satoshi Nakamura*

Main category: cs.CL

TL;DR: 提出基于大语言模型和直接偏好优化的语音翻译分割框架，在分割准确率、翻译质量和延迟方面优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有分割模型如SHAS虽然比启发式方法更鲁棒，但受限于监督学习目标，缺乏人类偏好对齐，而这对实时自然翻译至关重要

Method: 使用直接偏好优化训练大语言模型来预测自然分割点，在ACL 60/60语料库上评估三个语言对，以SeamlessM4T v2为翻译骨干

Result: DPO调优的LLM在分割准确率上超越SHAS，在翻译质量（BLEU、COMET）和延迟（平均滞后）方面获得一致改进

Conclusion: 偏好调优的LLM有潜力超越现有预训练分割模型，推动自适应、人类对齐的同步口译发展

Abstract: Simultaneous speech translation requires accurate segmentation to balance
translation quality and latency. Recent studies such as SHAS have introduced
pretrained segmentation models, achieving stronger performance than heuristic
rules. However, segmentation models such as SHAS, though pretrained and more
robust than heuristic methods, are still constrained by supervised learning
objectives and do not incorporate human preference alignment, which is crucial
for natural real-time interpretation. In this work, we propose a segmentation
framework based on large language models (LLMs) trained with Direct Preference
Optimization (DPO). By leveraging preference alignment, our method enables LLMs
to predict natural segmentation points that better meet the demands of
real-time translation. We evaluate the system on the ACL 60/60 corpus across
three language pairs (English-Japanese, Chinese, German), using SeamlessM4T v2
as the translation backbone. Experimental results show that our DPO-tuned LLM
achieves higher segmentation accuracy than SHAS and yields consistent
improvements in translation quality (BLEU, COMET) as well as latency (Average
Lagging). Furthermore, our system benefits from IWSLT baselines for direct
comparison. These findings highlight the potential of preference-tuned LLMs to
surpass existing pretrained segmentation models and advance adaptive,
human-aligned simultaneous interpretation.

</details>


### [33] [HALF: Harm-Aware LLM Fairness Evaluation Aligned with Deployment](https://arxiv.org/abs/2510.12217)
*Ali Mekky,Omar El Herraoui,Preslav Nakov,Yuxia Wang*

Main category: cs.CL

TL;DR: HALF是一个部署对齐的LLM公平性评估框架，通过将9个应用领域分为三个危害等级（严重、中等、轻微），在真实场景中评估模型偏见并按危害严重性加权结果。


<details>
  <summary>Details</summary>
Motivation: 现有LLM公平性评估缺乏真实场景基础，且未考虑不同领域危害严重性的差异，如手术决策偏见与文本摘要风格偏见的危害程度不同。

Method: 使用五阶段流程将九个应用领域组织成三个危害等级（严重、中等、轻微），在真实应用场景中评估模型偏见并按危害严重性加权结果。

Result: 评估八个LLM发现：(1) LLM在不同领域的公平性不一致；(2) 模型大小或性能不能保证公平性；(3) 推理模型在医疗决策支持中表现更好但在教育中更差。

Conclusion: HALF揭示了先前基准测试成功与部署准备度之间的明显差距，强调需要部署对齐的公平性评估。

Abstract: Large language models (LLMs) are increasingly deployed across high-impact
domains, from clinical decision support and legal analysis to hiring and
education, making fairness and bias evaluation before deployment critical.
However, existing evaluations lack grounding in real-world scenarios and do not
account for differences in harm severity, e.g., a biased decision in surgery
should not be weighed the same as a stylistic bias in text summarization. To
address this gap, we introduce HALF (Harm-Aware LLM Fairness), a
deployment-aligned framework that assesses model bias in realistic applications
and weighs the outcomes by harm severity. HALF organizes nine application
domains into three tiers (Severe, Moderate, Mild) using a five-stage pipeline.
Our evaluation results across eight LLMs show that (1) LLMs are not
consistently fair across domains, (2) model size or performance do not
guarantee fairness, and (3) reasoning models perform better in medical decision
support but worse in education. We conclude that HALF exposes a clear gap
between previous benchmarking success and deployment readiness.

</details>


### [34] [Analysing Moral Bias in Finetuned LLMs through Mechanistic Interpretability](https://arxiv.org/abs/2510.12229)
*Bianca Raimondi,Daniela Dalbagno,Maurizio Gabbrielli*

Main category: cs.CL

TL;DR: 研究发现LLMs在微调过程中会习得Knobe效应这种道德偏见，通过层补丁分析可以定位到特定层，并只需在这些关键层中注入预训练模型的激活值即可消除该偏见。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在微调过程中如何内化人类偏见，特别是Knobe效应这种道德判断偏见，并研究其具体机制。

Method: 在3个开源权重的LLMs上进行层补丁分析，定位导致Knobe效应的特定层，并测试通过注入预训练模型的激活值来消除偏见。

Result: 发现Knobe效应不仅能在微调过程中习得，而且集中在特定层中；仅需在少数关键层注入预训练模型的激活值就能完全消除该偏见。

Conclusion: LLMs中的社会偏见可以被解释、定位和缓解，通过有针对性的干预而无需重新训练模型。

Abstract: Large language models (LLMs) have been shown to internalize human-like biases
during finetuning, yet the mechanisms by which these biases manifest remain
unclear. In this work, we investigated whether the well-known Knobe effect, a
moral bias in intentionality judgements, emerges in finetuned LLMs and whether
it can be traced back to specific components of the model. We conducted a
Layer-Patching analysis across 3 open-weights LLMs and demonstrated that the
bias is not only learned during finetuning but also localized in a specific set
of layers. Surprisingly, we found that patching activations from the
corresponding pretrained model into just a few critical layers is sufficient to
eliminate the effect. Our findings offer new evidence that social biases in
LLMs can be interpreted, localized, and mitigated through targeted
interventions, without the need for model retraining.

</details>


### [35] [DSAS: A Universal Plug-and-Play Framework for Attention Optimization in Multi-Document Question Answering](https://arxiv.org/abs/2510.12251)
*Jiakai Li,Rongzheng Wang,Yizhuo Ma,Shuang Liang,Guangchun Luo,Ke Qin*

Main category: cs.CL

TL;DR: 提出了DSAS方法解决LLM在多文档问答中的长距离依赖建模和"中间丢失"问题，通过上下文门控加权和互斥注意力抑制两个模块，无需修改架构或额外训练参数即可提升性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多文档问答任务中存在两个主要限制：长距离依赖建模困难（难以关注长文本中的关键信息）和"中间丢失"问题（难以处理长输入中间位置的信息）。现有解决方案要么截断全局依赖，要么需要昂贵的微调，缺乏通用简单的解决方案。

Method: 提出双阶段自适应锐化(DSAS)方法，包含两个模块：(1)上下文门控加权(CGW)模块通过层间注意力跟踪和位置感知加权来评估段落相关性，缓解"中间丢失"问题；(2)互斥注意力抑制(RAS)模块通过抑制关键文本与无关文本之间的信息交换，增强对关键段落的关注，改善长距离依赖建模。DSAS是即插即用解决方案，无需架构修改或额外训练参数。

Result: 在四个基准测试上的广泛实验表明，DSAS在主流LLM（Llama、Qwen、Mistral和Deepseek）上均有效，在Llama-3.1-8B-Instruct和Qwen2.5-14B-Instruct上多文档问答任务的F1分数平均提升4.2%。消融研究确认了CGW和RAS模块的重要贡献。

Conclusion: DSAS提供了一种有效解决LLM在多文档问答中长距离依赖和"中间丢失"问题的通用方案，具有即插即用、无需额外训练的优点，在各种主流模型上均表现出良好的性能和可扩展性。

Abstract: While large language models (LLMs) show considerable promise across various
fields, they have notable limitations in handling multi-document question
answering (Multi-doc QA) tasks. The first challenge is long-range dependency
modeling, where LLMs struggle to focus on key information in long texts, which
weakens important semantic connections. Second, most LLMs suffer from the
''lost-in-the-middle'' issue, where they have difficulty processing information
in the middle of long inputs. Current solutions either truncate global
dependencies or demand costly finetuning, ultimately lacking a universal and
simple solution for these challenges. To resolve these limitations, we propose
Dual-Stage Adaptive Sharpening (DSAS) containing two modules. (i) The
Contextual Gate Weighting (CGW) module alleviates ''lost-in-the-middle'' by
assessing paragraph relevance through layer-wise attention tracking and
position-aware weighting. (ii) The Reciprocal Attention Suppression (RAS)
module enhances focus on critical paragraphs by suppressing information
exchange between key and irrelevant texts, thus mitigating the limitations in
long-range dependency modeling. Notably, DSAS functions as a plug-and-play
solution requiring no architectural modifications or extra training parameters.
Extensive experiments on four benchmarks demonstrate DSAS's efficacy across
mainstream LLMs (Llama, Qwen, Mistral, and Deepseek), with an average F1-score
improvement of 4.2% in Multi-doc QA tasks on Llama-3.1-8B-Instruct and
Qwen2.5-14B-Instruct. Ablation studies confirm the essential contributions of
both the CGW and RAS modules. In addition, detailed discussions in the Appendix
further validate the robustness and scalability of DSAS.

</details>


### [36] [Shallow Robustness, Deep Vulnerabilities: Multi-Turn Evaluation of Medical LLMs](https://arxiv.org/abs/2510.12255)
*Blazej Manczak,Eric Lin,Francisco Eiras,James O' Neill,Vaikkunth Mugunthan*

Main category: cs.CL

TL;DR: MedQA-Followup框架评估医疗问答中多轮对话的鲁棒性，发现LLMs在浅层扰动下表现良好，但在多轮设置中准确性大幅下降，间接干预比直接建议更具破坏性。


<details>
  <summary>Details</summary>
Motivation: 现有评估框架通常评估单轮问答，忽略了医疗咨询中常见的冲突输入、误导性上下文和权威影响等复杂性，需要系统评估多轮对话的鲁棒性。

Method: 引入MedQA-Followup框架，区分浅层鲁棒性（抵抗误导初始上下文）和深层鲁棒性（在答案被挑战时保持准确性），并引入间接-直接轴来分离上下文框架和明确建议。

Result: 五个最先进的LLMs在浅层扰动下表现合理，但在多轮设置中表现出严重脆弱性，Claude Sonnet 4的准确性从91.2%降至13.5%。间接干预通常比直接建议更具破坏性。

Conclusion: 多轮鲁棒性是医疗LLMs安全可靠部署的关键但未被充分探索的维度，当前模型在临床部署中存在显著脆弱性。

Abstract: Large language models (LLMs) are rapidly transitioning into medical clinical
use, yet their reliability under realistic, multi-turn interactions remains
poorly understood. Existing evaluation frameworks typically assess single-turn
question answering under idealized conditions, overlooking the complexities of
medical consultations where conflicting input, misleading context, and
authority influence are common. We introduce MedQA-Followup, a framework for
systematically evaluating multi-turn robustness in medical question answering.
Our approach distinguishes between shallow robustness (resisting misleading
initial context) and deep robustness (maintaining accuracy when answers are
challenged across turns), while also introducing an indirect-direct axis that
separates contextual framing (indirect) from explicit suggestion (direct).
Using controlled interventions on the MedQA dataset, we evaluate five
state-of-the-art LLMs and find that while models perform reasonably well under
shallow perturbations, they exhibit severe vulnerabilities in multi-turn
settings, with accuracy dropping from 91.2% to as low as 13.5% for Claude
Sonnet 4. Counterintuitively, indirect, context-based interventions are often
more harmful than direct suggestions, yielding larger accuracy drops across
models and exposing a significant vulnerability for clinical deployment.
Further compounding analyses reveal model differences, with some showing
additional performance drops under repeated interventions while others
partially recovering or even improving. These findings highlight multi-turn
robustness as a critical but underexplored dimension for safe and reliable
deployment of medical LLMs.

</details>


### [37] [Chinese ModernBERT with Whole-Word Masking](https://arxiv.org/abs/2510.12285)
*Zeyu Zhao,Ningtao Wang,Xing Fu,Yu Cheng*

Main category: cs.CL

TL;DR: 提出了Chinese ModernBERT，一个从头开始训练的中文编码器，通过优化的词汇表、动态掩码策略、两阶段预训练和稳定学习率调度，在中文自然语言处理任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的编码器改进主要针对英文，未能充分适应中文的独特分词和形态特征，因此需要专门为中文设计优化的预训练模型。

Method: 使用硬件感知的32k BPE词汇表、动态全词掩码、两阶段预训练（从1024扩展到8192上下文）以及阻尼余弦学习率调度。

Result: 在CLUE基准测试中与强中文编码器竞争，在SimCLUE上达到0.505皮尔逊/0.537斯皮尔曼相关系数，超越Qwen-0.6B-embedding。

Conclusion: Chinese ModernBERT为中文NLP提供了有效的预训练解决方案，展示了在语义相似度任务上的扩展潜力。

Abstract: Encoder-only Transformers have advanced along three axes -- architecture,
data, and systems -- yielding Pareto gains in accuracy, speed, and memory
efficiency. Yet these improvements have not fully transferred to Chinese, where
tokenization and morphology differ markedly from English. We introduce Chinese
ModernBERT, a from-scratch Chinese encoder that couples: (i) a hardware-aware
32k BPE vocabulary tailored to frequent Chinese affixes/compounds, lowering the
embedding budget; (ii) whole-word masking (WWM) with a dynamic masking
curriculum (30% -> 15%) to align task difficulty with training progress; (iii)
a two-stage pre-training pipeline that extends the native context from 1,024 to
8,192 tokens using RoPE and alternating local/global attention; and (iv) a
damped-cosine learning-rate schedule for stable long-horizon optimization. We
pre-train on ~1.2T Chinese tokens from CCI3-HQ, CCI4 (Chinese), and
Cosmopedia-Chinese. On CLUE, Chinese ModernBERT is competitive with strong
Chinese encoders under a unified fine-tuning protocol. Under bf16 it achieves
high long-sequence throughput while maintaining strong short-sequence speed,
reflecting benefits from budget allocation and attention design. To probe
retrieval-oriented quality, we add a small amount of open contrastive data:
fine-tuning on SimCLUE (~3M pairs) improves further when adding T2Ranking
(~2M), reaching 0.505 (Pearson) / 0.537 (Spearman) on the SimCLUE test set.
Under this open-data setting, Chinese ModernBERT surpasses Qwen-0.6B-embedding
on SimCLUE, suggesting a clear scaling path for STS with additional curated
pairs. We will release tokenizer and weights to facilitate reproducible
research.

</details>


### [38] [A large-scale, unsupervised pipeline for automatic corpus annotation using LLMs: variation and change in the English consider construction](https://arxiv.org/abs/2510.12306)
*Cameron Morin,Matti Marttinen Larsson*

Main category: cs.CL

TL;DR: 提出了一种基于大语言模型的无监督流水线，用于大规模语料库的自动语法标注，在COHA语料库中实现了98%+的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决语料库快速扩张时手动标注成为方法学瓶颈的问题，实现大规模语料库的自动化标注。

Method: 采用四阶段工作流程：提示工程、事前评估、自动批量处理、事后验证，使用GPT-5通过OpenAI API进行标注。

Result: 在COHA语料库的143,933个句子标注中，不到60小时完成，两个复杂标注任务准确率均超过98%。

Conclusion: 大语言模型能够以最小人工干预执行大规模数据准备任务，为基于语料库的研究开辟新可能性，但需关注成本、许可和伦理问题。

Abstract: As natural language corpora expand at an unprecedented rate, manual
annotation remains a significant methodological bottleneck in corpus linguistic
work. We address this challenge by presenting a scalable, unsupervised pipeline
for automating grammatical annotation in voluminous corpora using large
language models (LLMs). Unlike previous supervised and iterative approaches,
our method employs a four-phase workflow: prompt engineering, pre-hoc
evaluation, automated batch processing, and post-hoc validation. We demonstrate
the pipeline's accessibility and effectiveness through a diachronic case study
of variation in the English consider construction. Using GPT-5 through the
OpenAI API, we annotate 143,933 sentences from the Corpus of Historical
American English (COHA) in under 60 hours, achieving 98%+ accuracy on two
sophisticated annotation procedures. Our results suggest that LLMs can perform
a range of data preparation tasks at scale with minimal human intervention,
opening new possibilities for corpus-based research, though implementation
requires attention to costs, licensing, and other ethical considerations.

</details>


### [39] [Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation](https://arxiv.org/abs/2510.12316)
*Greta Damo,Elena Cabrio,Serena Villata*

Main category: cs.CL

TL;DR: 提出了一种基于知识增强生成(RAG)的反言论生成框架，通过整合联合国数字图书馆等权威知识库，为8个目标群体生成可信的反言论内容


<details>
  <summary>Details</summary>
Motivation: 现有反言论生成方法存在可靠性不足和可扩展性差的问题，需要开发更可信和连贯的反言论生成方法

Method: 使用检索增强生成(RAG)框架，构建包含32,792个文本的知识库，针对8个目标群体进行知识驱动的文本生成

Result: 在MultiTarget-CONAN数据集上的评估显示，该框架在标准指标和人工评估中均优于标准LLM基线和竞争方法

Conclusion: 该框架为研究可信和合理的反言论生成开辟了新途径，适用于仇恨言论及其他领域

Abstract: Counter-speech generation is at the core of many expert activities, such as
fact-checking and hate speech, to counter harmful content. Yet, existing work
treats counter-speech generation as pure text generation task, mainly based on
Large Language Models or NGO experts. These approaches show severe drawbacks
due to the limited reliability and coherence in the generated countering text,
and in scalability, respectively. To close this gap, we introduce a novel
framework to model counter-speech generation as knowledge-wise text generation
process. Our framework integrates advanced Retrieval-Augmented Generation (RAG)
pipelines to ensure the generation of trustworthy counter-speech for 8 main
target groups identified in the hate speech literature, including women, people
of colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons,
and other. We built a knowledge base over the United Nations Digital Library,
EUR-Lex and the EU Agency for Fundamental Rights, comprising a total of 32,792
texts. We use the MultiTarget-CONAN dataset to empirically assess the quality
of the generated counter-speech, both through standard metrics (i.e., JudgeLM)
and a human evaluation. Results show that our framework outperforms standard
LLM baselines and competitive approach, on both assessments. The resulting
framework and the knowledge base pave the way for studying trustworthy and
sound counter-speech generation, in hate speech and beyond.

</details>


### [40] [Fine-grained Analysis of Brain-LLM Alignment through Input Attribution](https://arxiv.org/abs/2510.12355)
*Michela Proietti,Roberto Capobianco,Mariya Toneva*

Main category: cs.CL

TL;DR: 该论文提出了一种细粒度输入归因方法，用于识别对大脑-LLM对齐最重要的特定词语，并研究了大脑对齐与下一个词预测之间的关系。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型与人类大脑活动之间的对齐可以揭示语言处理的计算原理，特别是解决关于大脑对齐与下一个词预测关系的争议性问题。

Method: 引入细粒度输入归因方法，识别对大脑-LLM对齐最重要的具体词语，并分析大脑对齐和下一个词预测所依赖的词语子集。

Result: 发现大脑对齐和下一个词预测依赖不同的词语子集：下一个词预测表现出近因和首因偏差，关注句法；而大脑对齐优先考虑语义和语篇层面信息，具有更有针对性的近因效应。

Conclusion: 这项工作推进了我们对LLM与人类语言处理关系的理解，突出了大脑对齐和下一个词预测在特征依赖上的差异，归因方法可广泛应用于探索模型预测在各种语言处理任务中的认知相关性。

Abstract: Understanding the alignment between large language models (LLMs) and human
brain activity can reveal computational principles underlying language
processing. We introduce a fine-grained input attribution method to identify
the specific words most important for brain-LLM alignment, and leverage it to
study a contentious research question about brain-LLM alignment: the
relationship between brain alignment (BA) and next-word prediction (NWP). Our
findings reveal that BA and NWP rely on largely distinct word subsets: NWP
exhibits recency and primacy biases with a focus on syntax, while BA
prioritizes semantic and discourse-level information with a more targeted
recency effect. This work advances our understanding of how LLMs relate to
human language processing and highlights differences in feature reliance
between BA and NWP. Beyond this study, our attribution method can be broadly
applied to explore the cognitive relevance of model predictions in diverse
language processing tasks.

</details>


### [41] [MoBiLE: Efficient Mixture-of-Experts Inference on Consumer GPU with Mixture of Big Little Experts](https://arxiv.org/abs/2510.12357)
*Yushu Zhao,Yubin Qin,Yang Wang,Xiaolong Yang,Huiming Han,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.CL

TL;DR: MoBiLE是一个即插即用的MoE推理框架，通过混合大小专家机制，对不重要token使用半数量专家加速，同时为重要token保留完整专家以保证质量，在消费级GPU上实现1.6-1.72倍加速且精度损失可忽略。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型卸载策略受限于CPU-GPU互联带宽，预取方法训练开销大且在细粒度专家分割模型上效果不佳。

Method: 提出MoBiLE框架，采用混合大小专家机制，为不重要token减少专家数量加速，为重要token保留完整专家；设计专用的回退和预取机制在大小专家间切换以提高内存效率。

Result: 在四种典型现代MoE架构和生成任务上评估，MoBiLE在消费级GPU系统上相比基线实现1.60-1.72倍加速，精度损失可忽略。

Conclusion: MoBiLE通过混合大小专家机制有效解决了MoE推理中的带宽瓶颈问题，实现了显著加速且保持模型质量。

Abstract: Mixture-of-Experts (MoE) models have recently demonstrated exceptional
performance across a diverse range of applications. The principle of sparse
activation in MoE models facilitates an offloading strategy, wherein active
experts are maintained in GPU HBM, while inactive experts are stored in CPU
DRAM. The efficacy of this approach, however, is fundamentally constrained by
the limited bandwidth of the CPU-GPU interconnect. To mitigate this bottleneck,
existing approaches have employed prefetching to accelerate MoE inference.
These methods attempt to predict and prefetch the required experts using
specially trained modules. Nevertheless, such techniques are often encumbered
by significant training overhead and have shown diminished effectiveness on
recent MoE models with fine-grained expert segmentation.
  In this paper, we propose MoBiLE, a plug-and-play offloading-based MoE
inference framework with \textit{mixture of big-little experts}. It reduces the
number of experts for unimportant tokens to half for acceleration while
maintaining full experts for important tokens to guarantee model quality.
Further, a dedicated fallback and prefetching mechanism is designed for
switching between little and big experts to improve memory efficiency. We
evaluate MoBiLE on four typical modern MoE architectures and challenging
generative tasks. Our results show that MoBiLE achieves a speedup of 1.60x to
1.72x compared to the baseline on a consumer GPU system, with negligible
degradation in accuracy.

</details>


### [42] [LLM-REVal: Can We Trust LLM Reviewers Yet?](https://arxiv.org/abs/2510.12367)
*Rui Li,Jia-Chen Gu,Po-Nien Kung,Heming Xia,Junfeng liu,Xiangwen Kong,Zhifang Sui,Nanyun Peng*

Main category: cs.CL

TL;DR: 该研究通过模拟实验发现，当LLMs同时参与论文撰写和评审时，会产生系统性偏见：LLM评审员会显著高评LLM撰写的论文，同时低估包含批判性陈述的人类撰写论文。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在学术工作流程中的深度整合，研究关注LLMs在研究和评审双重角色中可能带来的公平性风险，特别是当LLMs同时参与论文撰写和评审时可能产生的偏见问题。

Method: 采用模拟实验方法，包含研究代理（生成和修订论文）和评审代理（评估提交的论文），然后进行人工标注分析。

Result: 发现LLM评审存在两种主要偏见：对LLM生成写作风格的语言特征偏好，以及对批判性陈述的厌恶。LLM评审会系统性地高评LLM撰写论文，同时低估包含批判性内容的人类论文。

Conclusion: 如果不加谨慎地将LLMs部署在同行评审环节，会对人类作者和学术研究带来风险和公平性担忧。但另一方面，基于LLM评审的修订确实能提升论文质量，对早期研究人员和低质量论文有潜在帮助。

Abstract: The rapid advancement of large language models (LLMs) has inspired
researchers to integrate them extensively into the academic workflow,
potentially reshaping how research is practiced and reviewed. While previous
studies highlight the potential of LLMs in supporting research and peer review,
their dual roles in the academic workflow and the complex interplay between
research and review bring new risks that remain largely underexplored. In this
study, we focus on how the deep integration of LLMs into both peer-review and
research processes may influence scholarly fairness, examining the potential
risks of using LLMs as reviewers by simulation. This simulation incorporates a
research agent, which generates papers and revises, alongside a review agent,
which assesses the submissions. Based on the simulation results, we conduct
human annotations and identify pronounced misalignment between LLM-based
reviews and human judgments: (1) LLM reviewers systematically inflate scores
for LLM-authored papers, assigning them markedly higher scores than
human-authored ones; (2) LLM reviewers persistently underrate human-authored
papers with critical statements (e.g., risk, fairness), even after multiple
revisions. Our analysis reveals that these stem from two primary biases in LLM
reviewers: a linguistic feature bias favoring LLM-generated writing styles, and
an aversion toward critical statements. These results highlight the risks and
equity concerns posed to human authors and academic research if LLMs are
deployed in the peer review cycle without adequate caution. On the other hand,
revisions guided by LLM reviews yield quality gains in both LLM-based and human
evaluations, illustrating the potential of the LLMs-as-reviewers for
early-stage researchers and enhancing low-quality papers.

</details>


### [43] [Tokenization Disparities as Infrastructure Bias: How Subword Systems Create Inequities in LLM Access and Efficiency](https://arxiv.org/abs/2510.12389)
*Hailay Kidu Teklehaymanot,Wolfgang Nejdl*

Main category: cs.CL

TL;DR: 该研究对200多种语言进行大规模跨语言标记化效率评估，发现拉丁文字语言标记化效率更高，而非拉丁文字和形态复杂语言的标记化成本高出3-5倍，揭示了AI系统中的结构性不平等。


<details>
  <summary>Details</summary>
Motivation: 标记化差异阻碍了语言多样化人群公平获取人工智能服务，需要系统量化大型语言模型中的计算不平等问题。

Method: 使用标准化实验框架，通过tiktoken库对200多种语言样本进行统一标记化处理，收集标记化统计数据，使用TPS和RTC等评估指标与英语基准进行比较。

Result: 拉丁文字语言标记化效率更高，非拉丁文字和形态复杂语言的相对标记化成本显著增加，通常高出3-5倍，导致计算成本增加和有效上下文利用减少。

Conclusion: 当前AI系统存在结构性不平等，低资源和非拉丁语言使用者面临不成比例的计算劣势，未来应开发考虑语言类型多样性的标记化策略和自适应词汇构建方法。

Abstract: Tokenization disparities pose a significant barrier to achieving equitable
access to artificial intelligence across linguistically diverse populations.
This study conducts a large-scale cross-linguistic evaluation of tokenization
efficiency in over 200 languages to systematically quantify computational
inequities in large language models (LLMs). Using a standardized experimental
framework, we applied consistent preprocessing and normalization protocols,
followed by uniform tokenization through the tiktoken library across all
language samples. Comprehensive tokenization statistics were collected using
established evaluation metrics, including Tokens Per Sentence (TPS) and
Relative Tokenization Cost (RTC), benchmarked against English baselines. Our
cross-linguistic analysis reveals substantial and systematic disparities:
Latin-script languages consistently exhibit higher tokenization efficiency,
while non-Latin and morphologically complex languages incur significantly
greater token inflation, often 3-5 times higher RTC ratios. These
inefficiencies translate into increased computational costs and reduced
effective context utilization for underrepresented languages. Overall, the
findings highlight structural inequities in current AI systems, where speakers
of low-resource and non-Latin languages face disproportionate computational
disadvantages. Future research should prioritize the development of
linguistically informed tokenization strategies and adaptive vocabulary
construction methods that incorporate typological diversity, ensuring more
inclusive and computationally equitable multilingual AI systems.

</details>


### [44] [PRoH: Dynamic Planning and Reasoning over Knowledge Hypergraphs for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.12434)
*Xiangjun Zai,Xingyu Tan,Xiaoyang Wang,Qing Liu,Xiwei Xu,Wenjie Zhang*

Main category: cs.CL

TL;DR: PRoH是一个动态规划和推理框架，通过上下文感知规划、结构化问题分解和实体加权重叠引导的推理路径检索，克服了现有知识超图RAG方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有知识超图RAG方法存在静态检索规划、非自适应检索执行以及对KH结构和语义的浅层使用等三大限制，限制了多跳问答的有效性。

Method: PRoH包含三个核心创新：上下文感知规划模块、结构化问题分解过程和实体加权重叠引导的推理路径检索算法。

Result: 在多个领域的实验中，PRoH实现了最先进的性能，平均F1分数超过之前SOTA模型HyperGraphRAG 19.73%，生成评估得分提高8.41%。

Conclusion: PRoH框架在长距离多跳推理任务中表现出强大的鲁棒性，显著提升了知识超图在检索增强生成中的表现。

Abstract: Knowledge Hypergraphs (KHs) have recently emerged as a knowledge
representation for retrieval-augmented generation (RAG), offering a paradigm to
model multi-entity relations into a structured form. However, existing KH-based
RAG methods suffer from three major limitations: static retrieval planning,
non-adaptive retrieval execution, and superficial use of KH structure and
semantics, which constrain their ability to perform effective multi-hop
question answering. To overcome these limitations, we propose PRoH, a dynamic
Planning and Reasoning over Knowledge Hypergraphs framework. PRoH incorporates
three core innovations: (i) a context-aware planning module that sketches the
local KH neighborhood to guide structurally grounded reasoning plan generation;
(ii) a structured question decomposition process that organizes subquestions as
a dynamically evolving Directed Acyclic Graph (DAG) to enable adaptive,
multi-trajectory exploration; and (iii) an Entity-Weighted Overlap (EWO)-guided
reasoning path retrieval algorithm that prioritizes semantically coherent
hyperedge traversals. Experiments across multiple domains demonstrate that PRoH
achieves state-of-the-art performance, surpassing the prior SOTA model
HyperGraphRAG by an average of 19.73% in F1 and 8.41% in Generation Evaluation
(G-E) score, while maintaining strong robustness in long-range multi-hop
reasoning tasks.

</details>


### [45] [Probing Latent Knowledge Conflict for Faithful Retrieval-Augmented Generation](https://arxiv.org/abs/2510.12460)
*Linfeng Gao,Baolong Bi,Zheng Yuan,Le Wang,Zerui Chen,Zhimin Wei,Shenghua Liu,Qinggang Zhang,Jinsong Su*

Main category: cs.CL

TL;DR: 提出了CLEAR框架，通过细粒度句子级知识分解、隐藏状态探测定位冲突知识以及冲突感知微调，显著提升了RAG系统的准确性和上下文忠实度。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统存在不忠实问题，模型响应与检索证据相矛盾。现有方法将LLM视为黑盒，忽视了LLM内部如何整合检索证据与参数化记忆，特别是在知识冲突情况下的机制。

Method: CLEAR框架：1) 将上下文分解为细粒度句子级知识；2) 使用隐藏状态探测定位冲突知识；3) 引入冲突感知微调指导模型准确整合检索证据。

Result: 在三个基准测试上的广泛实验表明，CLEAR显著提高了准确性和上下文忠实度，在不同冲突条件下始终优于强基线方法。

Conclusion: CLEAR通过深入理解LLM内部知识整合机制，有效解决了RAG系统的忠实性问题，为知识冲突下的证据整合提供了新思路。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to
enhance the factuality of Large Language Models (LLMs). However, existing RAG
systems often suffer from an unfaithfulness issue, where the model's response
contradicts evidence from the retrieved context. Existing approaches to
improving contextual faithfulness largely rely on external interventions, such
as prompt engineering, decoding constraints, or reward-based fine-tuning. These
works treat the LLM as a black box and overlook a crucial question: how does
the LLM internally integrate retrieved evidence with its parametric memory,
particularly under knowledge conflicts? To address this gap, we conduct a
probing-based analysis of hidden-state representations in LLMs and observe
three findings: knowledge integration occurs hierarchically, conflicts manifest
as latent signals at the sentence level, and irrelevant context is often
amplified when aligned with parametric knowledge. Building on these findings,
we propose CLEAR (Conflict-Localized and Enhanced Attention for RAG), a
framework that (i) decomposes context into fine-grained sentence-level
knowledge, (ii) employs hidden-state probing to localize conflicting knowledge,
and (iii) introduces conflict-aware fine-tuning to guide the model to
accurately integrate retrieved evidence. Extensive experiments across three
benchmarks demonstrate that CLEAR substantially improves both accuracy and
contextual faithfulness, consistently outperforming strong baselines under
diverse conflict conditions. The related resources are available at
https://github.com/LinfengGao/CLEAR.

</details>


### [46] [Resource-sensitive but language-blind: Community size and not grammatical complexity better predicts the accuracy of Large Language Models in a novel Wug Test](https://arxiv.org/abs/2510.12463)
*Nikoleta Pantelidou,Evelina Leivada,Paolo Morosi*

Main category: cs.CL

TL;DR: 该研究通过多语言Wug测试评估LLMs在形态学泛化任务中的表现，发现模型能像人类一样处理未见词汇，但准确性主要受训练数据量而非语言结构复杂度影响。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs的语言能力是否接近人类水平，以及模型表现主要受语言复杂性还是训练数据量的影响。

Method: 使用多语言Wug测试，在四种语言（加泰罗尼亚语、英语、希腊语、西班牙语）上测试六个模型，并与人类表现对比。

Result: 模型能以类似人类的准确率对未见词汇进行形态学泛化，但准确性模式与社区规模和可用数据量更相关，而非结构复杂性。

Conclusion: 模型行为主要由语言资源丰富度驱动，而非对语法复杂性的敏感性，这种表现仅表面类似人类语言能力。

Abstract: The linguistic abilities of Large Language Models are a matter of ongoing
debate. This study contributes to this discussion by investigating model
performance in a morphological generalization task that involves novel words.
Using a multilingual adaptation of the Wug Test, six models were tested across
four partially unrelated languages (Catalan, English, Greek, and Spanish) and
compared with human speakers. The aim is to determine whether model accuracy
approximates human competence and whether it is shaped primarily by linguistic
complexity or by the quantity of available training data. Consistent with
previous research, the results show that the models are able to generalize
morphological processes to unseen words with human-like accuracy. However,
accuracy patterns align more closely with community size and data availability
than with structural complexity, refining earlier claims in the literature. In
particular, languages with larger speaker communities and stronger digital
representation, such as Spanish and English, revealed higher accuracy than
less-resourced ones like Catalan and Greek. Overall, our findings suggest that
model behavior is mainly driven by the richness of linguistic resources rather
than by sensitivity to grammatical complexity, reflecting a form of performance
that resembles human linguistic competence only superficially.

</details>


### [47] [SMEC: Rethinking Matryoshka Representation Learning for Retrieval Embedding Compression](https://arxiv.org/abs/2510.12474)
*Biao Zhang,Lixin Chen,Tong Liu,Bo Zheng*

Main category: cs.CL

TL;DR: 提出SMEC训练框架，通过序列嵌套表示学习、自适应维度选择和可选择性跨批次记忆模块，在保持性能的同时显著降低LLM嵌入维度。


<details>
  <summary>Details</summary>
Motivation: 高维LLM嵌入导致计算复杂度和存储需求增加，阻碍实际部署，需要有效的维度压缩方法。

Method: SMEC框架包含：序列嵌套表示学习减少训练梯度方差，自适应维度选择减少信息退化，可选择性跨批次记忆增强无监督学习。

Result: 在图像、文本和多模态数据集上，SMEC实现显著维度压缩同时保持性能。在BEIR数据集上，256维压缩嵌入比现有方法提升1.1-2.7点。

Conclusion: SMEC框架能有效压缩LLM嵌入维度，降低计算和存储成本，同时保持语义表示质量。

Abstract: Large language models (LLMs) generate high-dimensional embeddings that
capture rich semantic and syntactic information. However, high-dimensional
embeddings exacerbate computational complexity and storage requirements,
thereby hindering practical deployment. To address these challenges, we propose
a novel training framework named Sequential Matryoshka Embedding Compression
(SMEC). This framework introduces the Sequential Matryoshka Representation
Learning(SMRL) method to mitigate gradient variance during training, the
Adaptive Dimension Selection (ADS) module to reduce information degradation
during dimension pruning, and the Selectable Cross-batch Memory (S-XBM) module
to enhance unsupervised learning between high- and low-dimensional embeddings.
Experiments on image, text, and multimodal datasets demonstrate that SMEC
achieves significant dimensionality reduction while maintaining performance.
For instance, on the BEIR dataset, our approach improves the performance of
compressed LLM2Vec embeddings (256 dimensions) by 1.1 points and 2.7 points
compared to the Matryoshka-Adaptor and Search-Adaptor models, respectively.

</details>


### [48] [When Personalization Tricks Detectors: The Feature-Inversion Trap in Machine-Generated Text Detection](https://arxiv.org/abs/2510.12476)
*Lang Gao,Xuhui Li,Chenxi Wang,Mingzhe Li,Wei Liu,Zirui Song,Jinghui Zhang,Rui Yan,Preslav Nakov,Xiuying Chen*

Main category: cs.CL

TL;DR: 提出了首个个性化机器生成文本检测基准，揭示了检测器在个性化设置中的性能下降问题，并提出了预测检测器性能变化的方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型能够生成模仿个人风格的文本，这增加了身份冒充的风险，但目前缺乏对个性化机器生成文本检测的研究。

Method: 构建了包含文学和博客文本及其LLM生成模仿的基准数据集，提出了基于特征反转陷阱的MIMIC方法，通过识别潜在特征方向来预测检测器性能变化。

Result: 实验结果显示个性化设置中检测器性能存在显著差距，一些最先进模型性能大幅下降，MIMIC方法能准确预测性能变化方向与幅度，与实际性能差距相关性达85%。

Conclusion: 这项工作揭示了检测器在个性化文本检测中的局限性，提出的方法能有效预测性能变化，鼓励进一步研究个性化文本检测问题。

Abstract: Large language models (LLMs) have grown more powerful in language generation,
producing fluent text and even imitating personal style. Yet, this ability also
heightens the risk of identity impersonation. To the best of our knowledge, no
prior work has examined personalized machine-generated text (MGT) detection. In
this paper, we introduce \dataset, the first benchmark for evaluating detector
robustness in personalized settings, built from literary and blog texts paired
with their LLM-generated imitations. Our experimental results demonstrate large
performance gaps across detectors in personalized settings: some
state-of-the-art models suffer significant drops. We attribute this limitation
to the \textit{feature-inversion trap}, where features that are discriminative
in general domains become inverted and misleading when applied to personalized
text. Based on this finding, we propose \method, a simple and reliable way to
predict detector performance changes in personalized settings. \method
identifies latent directions corresponding to inverted features and constructs
probe datasets that differ primarily along these features to evaluate detector
dependence. Our experiments show that \method can accurately predict both the
direction and the magnitude of post-transfer changes, showing 85\% correlation
with the actual performance gaps. We hope that this work will encourage further
research on personalized text detection.

</details>


### [49] [BoN Appetit Team at LeWiDi-2025: Best-of-N Test-time Scaling Can Not Stomach Annotation Disagreements (Yet)](https://arxiv.org/abs/2510.12516)
*Tomas Ruiz,Siyao Peng,Barbara Plank,Carsten Schwemmer*

Main category: cs.CL

TL;DR: 将测试时缩放技术从数学领域转移到LeWiDi-2025任务中，评估标注分歧。实验发现基准方法有效，但Best-of-N方法在LeWiDi任务中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 测试时缩放技术此前仅限于数学和编程等有正确答案的领域，本研究旨在将其扩展到评估标注分歧的LeWiDi任务中。

Method: 使用三种测试时缩放方法：模型平均、多数投票和Best-of-N采样，在LeWiDi-2025任务上进行实验。

Result: 两个基准方法在LeWiDi任务上持续提升LLM性能，但Best-of-N方法没有改善效果。

Conclusion: Best-of-N方法目前无法从数学领域成功迁移到LeWiDi任务，需要分析这种差距的原因。

Abstract: Test-time scaling is a family of techniques to improve LLM outputs at
inference time by performing extra computation. To the best of our knowledge,
test-time scaling has been limited to domains with verifiably correct answers,
like mathematics and coding. We transfer test-time scaling to the LeWiDi-2025
tasks to evaluate annotation disagreements. We experiment with three test-time
scaling methods: two benchmark algorithms (Model Averaging and Majority
Voting), and a Best-of-N sampling method. The two benchmark methods improve LLM
performance consistently on the LeWiDi tasks, but the Best-of-N method does
not. Our experiments suggest that the Best-of-N method does not currently
transfer from mathematics to LeWiDi tasks, and we analyze potential reasons for
this gap.

</details>


### [50] [VISaGE: Understanding Visual Generics and Exceptions](https://arxiv.org/abs/2510.12548)
*Stella Frank,Emily Allaway*

Main category: cs.CL

TL;DR: 论文研究了视觉语言模型在处理典型和异常图像时的权衡机制，发现当图像与文本不一致时，模型的概念理解能力会下降，这种语用先验的影响比语义先验更强。


<details>
  <summary>Details</summary>
Motivation: 理解视觉语言模型在处理典型和异常实例时如何权衡语用先验（输入相关）和语义先验（概念通用性）之间的冲突。

Method: 构建了VISaGE评估数据集，包含典型和异常图像，通过精心平衡的实验设计来研究模型在不同条件下的表现。

Result: 当图像与文本不一致时，模型的概念理解能力显著下降，语用先验的影响比语义先验更强。

Conclusion: 视觉语言模型在处理异常实例时更依赖语用先验，这限制了它们在非典型情况下的概念理解能力。

Abstract: While Vision Language Models (VLMs) learn conceptual representations, in the
form of generalized knowledge, during training, they are typically used to
analyze individual instances. When evaluation instances are atypical, this
paradigm results in tension between two priors in the model. The first is a
pragmatic prior that the textual and visual input are both relevant, arising
from VLM finetuning on congruent inputs; the second is a semantic prior that
the conceptual representation is generally true for instances of the category.
In order to understand how VLMs trade off these priors, we introduce a new
evaluation dataset, VISaGE, consisting of both typical and exceptional images.
In carefully balanced experiments, we show that conceptual understanding
degrades when the assumption of congruency underlying the pragmatic prior is
violated with incongruent images. This effect is stronger than the effect of
the semantic prior when querying about individual instances.

</details>


### [51] [Teaching Language Models to Faithfully Express their Uncertainty](https://arxiv.org/abs/2510.12587)
*Bryan Eikema,Evgenia Ilia,José G. C. de Souza,Chrysoula Zerva,Wilker Aziz*

Main category: cs.CL

TL;DR: FUT是一种微调方法，通过将模型样本与不确定性修饰语对齐来教导LLMs忠实表达不确定性，而不改变其答案分布。


<details>
  <summary>Details</summary>
Motivation: LLMs经常错误传达其不确定性：重复查询会产生不同的答案，但生成的响应通常没有修饰或以不反映这种变异性的方式进行修饰，这造成了忠实性差距。

Method: 通过将模型样本与基于样本一致性的不确定性修饰语（如'可能'或'很可能'）进行增强来构建训练数据，无需额外监督。

Result: FUT显著减少了忠实性差距，同时保持了QA准确性并引入了最小的语义分布偏移，在不同解码策略、修饰语选择和其他不确定性表达形式下表现出稳健性。

Conclusion: FUT是一种简单有效的方法，可以教导LLMs忠实传达不确定性。

Abstract: Large language models (LLMs) often miscommunicate their uncertainty: repeated
queries can produce divergent answers, yet generated responses are typically
unhedged or hedged in ways that do not reflect this variability. This conveys
unfaithful information about the uncertain state of the LLMs' knowledge,
creating a faithfulness gap that affects even strong LLMs. We introduce
Faithful Uncertainty Tuning (FUT): a fine-tuning approach that teaches
instruction-tuned LLMs to express uncertainty faithfully without altering their
underlying answer distribution. We construct training data by augmenting model
samples with uncertainty hedges (i.e. verbal cues such as 'possibly' or
'likely') aligned with sample consistency, requiring no supervision beyond the
model and a set of prompts. We evaluate FUT on open-domain question answering
(QA) across multiple models and datasets. Our results show that FUT
substantially reduces the faithfulness gap, while preserving QA accuracy and
introducing minimal semantic distribution shift. Further analyses demonstrate
robustness across decoding strategies, choice of hedgers, and other forms of
uncertainty expression (i.e. numerical). These findings establish FUT as a
simple and effective way to teach LLMs to communicate uncertainty faithfully.

</details>


### [52] [StyleDecipher: Robust and Explainable Detection of LLM-Generated Texts with Stylistic Analysis](https://arxiv.org/abs/2510.12608)
*Siyuan Li,Aodu Wulianghai,Xi Lin,Guangyan Li,Xiang Chen,Jun Wu,Jianhua Li*

Main category: cs.CL

TL;DR: StyleDecipher是一个鲁棒且可解释的机器生成文本检测框架，通过结合离散风格指标和连续风格表示来量化人类与LLM生成文本之间的风格差异，在多个领域实现了最先进的检测性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在开放域写作中的广泛应用，检测机器生成文本对于确保内容真实性和可信度变得至关重要。现有方法在现实场景中面临泛化能力有限、易受改写攻击和缺乏可解释性等问题。

Method: 提出StyleDecipher框架，通过联合建模离散风格指标和基于语义嵌入的连续风格表示，在统一表示空间中捕捉人类与LLM输出的风格级差异，无需访问模型内部或标记片段。

Result: 在新闻、代码、论文、评论和学术摘要五个不同领域的实验中，StyleDecipher在域内准确率上始终达到最先进水平。在跨域评估中，比现有基线方法提升了高达36.30%，同时对对抗性扰动和混合人机内容保持鲁棒性。

Conclusion: 风格信号为区分机器生成文本提供了可解释的证据，StyleDecipher框架实现了准确、可解释且领域无关的检测，无需依赖模型内部信息或标记数据。

Abstract: With the increasing integration of large language models (LLMs) into
open-domain writing, detecting machine-generated text has become a critical
task for ensuring content authenticity and trust. Existing approaches rely on
statistical discrepancies or model-specific heuristics to distinguish between
LLM-generated and human-written text. However, these methods struggle in
real-world scenarios due to limited generalization, vulnerability to
paraphrasing, and lack of explainability, particularly when facing stylistic
diversity or hybrid human-AI authorship. In this work, we propose
StyleDecipher, a robust and explainable detection framework that revisits
LLM-generated text detection using combined feature extractors to quantify
stylistic differences. By jointly modeling discrete stylistic indicators and
continuous stylistic representations derived from semantic embeddings,
StyleDecipher captures distinctive style-level divergences between human and
LLM outputs within a unified representation space. This framework enables
accurate, explainable, and domain-agnostic detection without requiring access
to model internals or labeled segments. Extensive experiments across five
diverse domains, including news, code, essays, reviews, and academic abstracts,
demonstrate that StyleDecipher consistently achieves state-of-the-art in-domain
accuracy. Moreover, in cross-domain evaluations, it surpasses existing
baselines by up to 36.30%, while maintaining robustness against adversarial
perturbations and mixed human-AI content. Further qualitative and quantitative
analysis confirms that stylistic signals provide explainable evidence for
distinguishing machine-generated text. Our source code can be accessed at
https://github.com/SiyuanLi00/StyleDecipher.

</details>


### [53] [ACADATA: Parallel Dataset of Academic Data for Machine Translation](https://arxiv.org/abs/2510.12621)
*Iñaki Lacunza,Javier Garcia Gilabert,Francesca De Luca Fornaciari,Javier Aula-Blasco,Aitor Gonzalez-Agirre,Maite Melero,Marta Villegas*

Main category: cs.CL

TL;DR: ACADATA是一个高质量的学术翻译平行数据集，包含150万段作者生成的段落对和6000个翻译的评估集。通过在ACADATA上微调LLM，显著提升了学术翻译质量，超越了专业机器翻译系统和大型专有模型。


<details>
  <summary>Details</summary>
Motivation: 解决学术翻译领域缺乏高质量数据集的问题，为学术领域和长文本翻译研究提供资源。

Method: 构建ACADATA数据集（包含训练集ACAD-TRAIN和评估集ACAD-BENCH），并在其上微调大型语言模型，与多种翻译系统进行对比评估。

Result: 微调后7B和2B模型在学术翻译质量上分别平均提升6.1和12.4 d-BLEU点，从英语翻译时在通用领域的长文本翻译提升达24.9%，最佳模型超越了专有和开源模型。

Conclusion: ACADATA数据集和微调模型为学术翻译和长文本翻译研究提供了有价值的资源，显著提升了翻译质量。

Abstract: We present ACADATA, a high-quality parallel dataset for academic translation,
that consists of two subsets: ACAD-TRAIN, which contains approximately 1.5
million author-generated paragraph pairs across 96 language directions and
ACAD-BENCH, a curated evaluation set of almost 6,000 translations covering 12
directions. To validate its utility, we fine-tune two Large Language Models
(LLMs) on ACAD-TRAIN and benchmark them on ACAD-BENCH against specialized
machine-translation systems, general-purpose, open-weight LLMs, and several
large-scale proprietary models. Experimental results demonstrate that
fine-tuning on ACAD-TRAIN leads to improvements in academic translation quality
by +6.1 and +12.4 d-BLEU points on average for 7B and 2B models respectively,
while also improving long-context translation in a general domain by up to
24.9% when translating out of English. The fine-tuned top-performing model
surpasses the best propietary and open-weight models on academic translation
domain. By releasing ACAD-TRAIN, ACAD-BENCH and the fine-tuned models, we
provide the community with a valuable resource to advance research in academic
domain and long-context translation.

</details>


### [54] [COSTAR-A: A prompting framework for enhancing Large Language Model performance on Point-of-View questions](https://arxiv.org/abs/2510.12637)
*Nzubechukwu C. Ohalete,Kevin B. Gittner,Lauren M. Matheny*

Main category: cs.CL

TL;DR: COSTAR-A是一种改进的提示工程框架，在原有COSTAR方法基础上增加了'答案'组件，能够提升小型本地优化语言模型的输出结构和决策性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型对提示设计高度敏感，而现有COSTAR框架在小型本地优化模型上表现不一致，特别是在需要更指令性或约束性输出的任务中。

Method: 通过一系列受控的提示-输出评估，使用最多80亿参数的精调模型测试COSTAR-A框架，并与原始COSTAR方法进行比较。

Result: COSTAR-A能够提升本地化LLM的某些任务输出结构和决策性，但效果因模型和用例而异。Llama 3.1-8B模型在使用COSTAR-A时表现优于单独使用COSTAR。

Conclusion: COSTAR-A作为一个提示框架具有适应性和可扩展性，特别适用于资源受限硬件上的计算高效AI部署。

Abstract: Large Language Models (LLMs) are highly sensitive to prompt design, and
making optimized prompting techniques is crucial for generating consistent,
high-quality outputs. In this study, we introduce COSTAR-A, a novel prompt
engineering framework that enhances the existing COSTAR method, which stands
for Context, Objective, Style, Tone, Audience, and Response, by adding the
'Answer' component at the end. We demonstrate that while the original COSTAR
framework improves prompt clarity and aligns outputs for larger LLMs, its
performance is less consistent with smaller, locally optimized models,
particularly in tasks that require more directive or constrained outputs.
Through a series of controlled prompt-output assessments with smaller (at most
8 billion parameters), fine-tuned models, we found that COSTAR-A can enhance
the output structure and decisiveness of localized LLMs for certain tasks,
although its effectiveness varies across models and use cases. Notably, the
Llama 3.1-8B model exhibited performance improvements when prompted with
COSTAR-A compared to COSTAR alone. These findings emphasize the adaptability
and scalability of COSTAR-A as a prompting framework, particularly in
computationally efficient AI deployments on resource-constrained hardware.

</details>


### [55] [Reasoning Pattern Matters: Learning to Reason without Human Rationales](https://arxiv.org/abs/2510.12643)
*Chaoxu Pang,Yixuan Cao,Ping Luo*

Main category: cs.CL

TL;DR: 本文提出PARO框架，通过让LLM根据任务特定的推理模式自动生成rationales，无需人工标注，在patterned reasoning任务中达到与10倍规模人工标注相当的SFT+RLVR性能。


<details>
  <summary>Details</summary>
Motivation: 当前SFT+RLVR范式需要昂贵的人工rationale标注，本文旨在探究如何在不损害推理性能的前提下大幅降低标注成本。

Method: 提出PARO框架，识别patterned reasoning任务（推理遵循固定策略），让LLM根据任务特定的推理模式自动生成rationales，无需人工标注。

Result: PARO生成的rationales在SFT+RLVR中达到与10倍规模人工标注相当的性能，表明大规模人工标注可被有限监督的LLM自动标注替代。

Conclusion: 对于patterned reasoning任务，推理模式而非rationale数量或质量是关键性能决定因素，LLM自动标注可有效替代昂贵的人工标注。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning
capabilities under the widely adopted SFT+RLVR paradigm, which first performs
Supervised Fine-Tuning (SFT) on human-annotated reasoning trajectories
(rationales) to establish initial reasoning behaviors, then applies
Reinforcement Learning with Verifiable Rewards (RLVR) to optimize the model
using verifiable signals without golden rationales. However, annotating
high-quality rationales for the SFT stage remains prohibitively expensive. This
paper investigates when and how rationale annotation costs can be substantially
reduced without compromising reasoning performance. We identify a broad class
of problems, termed patterned reasoning tasks, where reasoning follows a fixed,
procedural strategy consistent across instances. Although instances vary in
content such as domain knowledge, factual information, or numeric values, the
solution derives from applying a shared reasoning pattern. We argue that the
success of SFT+RLVR on such tasks primarily stems from its ability to enable
models to internalize these reasoning patterns. Using numerical semantic
matching as a representative task, we provide both causal and behavioral
evidence showing that reasoning patterns rather than the quantity or quality of
rationales are the key determinant of performance. Building on these insights,
we propose Pattern-Aware LLMs as Rationale AnnOtators (PARO), a simple yet
effective framework that enables LLMs to generate rationales aligned with
task-specific reasoning patterns without requiring human rationale annotations.
Experiments show that PARO-generated rationales achieve comparable SFT+RLVR
performance to human rationales that are 10 times larger. These results suggest
that large-scale human rationale annotations can be replaced with LLM-based
automatic annotations requiring only limited human supervision over reasoning
patterns.

</details>


### [56] [Generation Space Size: Understanding and Calibrating Open-Endedness of LLM Generations](https://arxiv.org/abs/2510.12699)
*Sunny Yu,Ahmad Jabbar,Robert Hawkins,Dan Jurafsky,Myra Cheng*

Main category: cs.CL

TL;DR: 提出生成空间大小(GSS)概念，通过GSSBench评估指标，发现EigenScore等幻觉检测指标优于标准多样性指标，并展示了GSS在检测提示模糊性、解释推理模型行为、引导模型生成多样化高质量输出方面的应用。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在校准方面存在问题：在创意任务中输出过于同质化，在事实任务中产生多样化但不正确的回答。这两种失败模式都可以通过生成空间大小(GSS)概念来统一解决。

Method: 提出GSSBench任务套件，包含具有真实GSS关系的提示对，评估不同指标并理解模型与期望行为的差异。使用EigenScore等幻觉检测指标分析模型内部表示。

Result: 发现幻觉检测指标（特别是EigenScore）在仅使用模型内部信息的情况下，持续优于标准多样性和不确定性量化指标，并能提供对模型内部任务表示的可解释见解。

Conclusion: GSS概念在检测提示模糊性、解释推理模型中的过度思考与思考不足、引导模型扩展生成空间以产生高质量多样化输出方面具有重要应用价值。

Abstract: Different open-ended generation tasks require different degrees of output
diversity. However, current LLMs are often miscalibrated. They collapse to
overly homogeneous outputs for creative tasks and hallucinate diverse but
incorrect responses for factual tasks. We argue that these two failure modes
are unified by, and can both be addressed by, the notion of effective
generation space size (GSS) -- the set of semantically distinct outputs a model
considers for a prompt. We present GSSBench, a task suite of prompt pairs with
ground-truth GSS relationships to assess different metrics and understand where
models diverge from desired behavior. We find that hallucination detection
metrics, particularly EigenScore, consistently outperform standard diversity
and uncertainty quantification metrics, while using only model internals,
providing interpretable insights into a model's internal task representations.
We demonstrate three applications of GSS: (1) detecting prompt ambiguity and
predicting clarification questions for better grounding, (2) interpreting
overthinking and underthinking in reasoning models, and (3) steering models to
expand their generation space to yield high-quality and diverse outputs.

</details>


### [57] [Omni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed Perception](https://arxiv.org/abs/2510.12720)
*Ziyang Ma,Ruiyang Xu,Zhenghao Xing,Yunfei Chu,Yuxuan Wang,Jinzheng He,Jin Xu,Pheng-Ann Heng,Kai Yu,Junyang Lin,Eng Siong Chng,Xie Chen*

Main category: cs.CL

TL;DR: 本文提出Omni-Detective数据生成管道和Omni-Cloze评估基准，用于提升全模态语言模型的细粒度感知能力，在音频和音视频详细描述任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前全模态语言模型在捕捉和描述细粒度细节方面存在限制，且细节与幻觉之间存在'共增长'问题，需要系统性的解决方案。

Method: 提出Omni-Detective代理式数据生成管道，集成工具调用来自主生成高质量多模态数据；基于此训练Audio-Captioner和Omni-Captioner模型；设计Omni-Cloze填空式评估基准。

Result: Audio-Captioner在MMAU和MMAR上超越所有开源模型，性能媲美Gemini 2.5 Pro；Omni-Captioner在VDC上达到SOTA，在video-SALMONN 2上实现细节与幻觉的最佳平衡。

Conclusion: Omni-Detective能有效生成高质量详细描述，Omni-Cloze为全模态细粒度感知提供了稳定可靠的评估方法，推动了多模态AI的细粒度理解能力发展。

Abstract: Fine-grained perception of multimodal information is critical for advancing
human-AI interaction. With recent progress in audio-visual technologies, Omni
Language Models (OLMs), capable of processing audio and video signals in
parallel, have emerged as a promising paradigm for achieving richer
understanding and reasoning. However, their capacity to capture and describe
fine-grained details remains limited explored. In this work, we present a
systematic and comprehensive investigation of omni detailed perception from the
perspectives of the data pipeline, models, and benchmark. We first identify an
inherent "co-growth" between detail and hallucination in current OLMs. To
address this, we propose Omni-Detective, an agentic data generation pipeline
integrating tool-calling, to autonomously produce highly detailed yet minimally
hallucinatory multimodal data. Based on the data generated with Omni-Detective,
we train two captioning models: Audio-Captioner for audio-only detailed
perception, and Omni-Captioner for audio-visual detailed perception. Under the
cascade evaluation protocol, Audio-Captioner achieves the best performance on
MMAU and MMAR among all open-source models, surpassing Gemini 2.5 Flash and
delivering performance comparable to Gemini 2.5 Pro. On existing detailed
captioning benchmarks, Omni-Captioner sets a new state-of-the-art on VDC and
achieves the best trade-off between detail and hallucination on the
video-SALMONN 2 testset. Given the absence of a dedicated benchmark for omni
detailed perception, we design Omni-Cloze, a novel cloze-style evaluation for
detailed audio, visual, and audio-visual captioning that ensures stable,
efficient, and reliable assessment. Experimental results and analysis
demonstrate the effectiveness of Omni-Detective in generating high-quality
detailed captions, as well as the superiority of Omni-Cloze in evaluating such
detailed captions.

</details>


### [58] [Which Word Orders Facilitate Length Generalization in LMs? An Investigation with GCG-Based Artificial Languages](https://arxiv.org/abs/2510.12722)
*Nadine El-Naggar,Tatsuki Kuribayashi,Ted Briscoe*

Main category: cs.CL

TL;DR: 该研究探讨语言模型是否具有偏好类型学常见语法属性的归纳偏置，通过采用广义范畴语法构建更接近自然语言的人工语言，并关注模型对未见长句的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 扩展先前关于语言模型归纳偏置的研究，从两个角度改进：使用更贴近自然语言的语法形式化方法，以及更关注模型对长句的泛化能力。

Method: 采用广义范畴语法（GCG）构建人工语言，覆盖无界依赖和轻度上下文敏感结构等自然语言特征，评估语言模型对未见长句的泛化表现。

Result: 实验表明，类型学上合理的词序更容易让语言模型进行生产性泛化。

Conclusion: 语言模型确实具有偏好类型学常见语法属性的归纳偏置，这种偏置在更自然的语言结构和长句泛化任务中表现更明显。

Abstract: Whether language models (LMs) have inductive biases that favor typologically
frequent grammatical properties over rare, implausible ones has been
investigated, typically using artificial languages (ALs) (White and Cotterell,
2021; Kuribayashi et al., 2024). In this paper, we extend these works from two
perspectives. First, we extend their context-free AL formalization by adopting
Generalized Categorial Grammar (GCG) (Wood, 2014), which allows ALs to cover
attested but previously overlooked constructions, such as unbounded dependency
and mildly context-sensitive structures. Second, our evaluation focuses more on
the generalization ability of LMs to process unseen longer test sentences.
Thus, our ALs better capture features of natural languages and our experimental
paradigm leads to clearer conclusions -- typologically plausible word orders
tend to be easier for LMs to productively generalize.

</details>


### [59] [Hey, wait a minute: on at-issue sensitivity in Language Models](https://arxiv.org/abs/2510.12740)
*Sanghee J. Kim,Kanishka Misra*

Main category: cs.CL

TL;DR: 提出DGRC方法评估对话自然度，发现语言模型偏好继续讨论核心内容，指令调优增强此偏好，但相关线索会减弱这种偏好。


<details>
  <summary>Details</summary>
Motivation: 评估语言模型对话自然度具有挑战性，因为'自然度'概念多样且缺乏可扩展的定量指标。

Method: 引入DGRC方法：分割对话作为提示，使用语言模型生成子部分续写，重新组合对话和续写，比较重组序列的可能性。

Result: 语言模型偏好继续讨论核心内容，指令调优模型此偏好更强。当存在相关线索时，模型会减少对核心内容的偏好。

Conclusion: DGRC方法能减轻语言模型语言分析中的偏见，系统测试话语敏感行为，指令调优虽未进一步增强调节能力，但模式反映了成功对话动态的特征。

Abstract: Evaluating the naturalness of dialogue in language models (LMs) is not
trivial: notions of 'naturalness' vary, and scalable quantitative metrics
remain limited. This study leverages the linguistic notion of 'at-issueness' to
assess dialogue naturalness and introduces a new method: Divide, Generate,
Recombine, and Compare (DGRC). DGRC (i) divides a dialogue as a prompt, (ii)
generates continuations for subparts using LMs, (iii) recombines the dialogue
and continuations, and (iv) compares the likelihoods of the recombined
sequences. This approach mitigates bias in linguistic analyses of LMs and
enables systematic testing of discourse-sensitive behavior. Applying DGRC, we
find that LMs prefer to continue dialogue on at-issue content, with this effect
enhanced in instruct-tuned models. They also reduce their at-issue preference
when relevant cues (e.g., "Hey, wait a minute") are present. Although
instruct-tuning does not further amplify this modulation, the pattern reflects
a hallmark of successful dialogue dynamics.

</details>


### [60] [Language Models Model Language](https://arxiv.org/abs/2510.12766)
*Łukasz Borchmann*

Main category: cs.CL

TL;DR: 该论文主张从经验主义视角重新审视语言模型，基于Mańczak的语言学框架，将语言定义为所有口头和书面表达的总和，并以使用频率为主要原则，为LLMs的设计和评估提供建设性指导。


<details>
  <summary>Details</summary>
Motivation: 针对当前基于de Saussure和Chomsky理论框架对LLMs的语言学评论往往具有推测性和非建设性的问题，作者希望提供一个更实证的语言学视角来理解和评估语言模型。

Method: 采用Witold Mańczak的经验主义语言学框架，将语言定义为所有口头和书面表达的总和，强调使用频率作为语言的主要支配原则，并以此框架重新审视和挑战对LLMs的现有批评。

Result: 基于Mańczak框架，论文挑战了先前对LLMs的批评，特别是关于"深层结构"和"基础"的质疑，为语言模型的设计、评估和解释提供了建设性指导。

Conclusion: 从经验主义语言学的角度重新定义语言，将使用频率作为核心原则，能够为LLMs提供更合理和建设性的评估框架，推动该领域的实证研究发展。

Abstract: Linguistic commentary on LLMs, heavily influenced by the theoretical
frameworks of de Saussure and Chomsky, is often speculative and unproductive.
Critics challenge whether LLMs can legitimately model language, citing the need
for "deep structure" or "grounding" to achieve an idealized linguistic
"competence." We argue for a radical shift in perspective towards the
empiricist principles of Witold Ma\'nczak, a prominent general and historical
linguist. He defines language not as a "system of signs" or a "computational
system of the brain" but as the totality of all that is said and written. Above
all, he identifies frequency of use of particular language elements as
language's primary governing principle. Using his framework, we challenge prior
critiques of LLMs and provide a constructive guide for designing, evaluating,
and interpreting language models.

</details>


### [61] [Dr.LLM: Dynamic Layer Routing in LLMs](https://arxiv.org/abs/2510.12773)
*Ahmed Heakl,Martin Gubri,Salman Khan,Sangdoo Yun,Seong Joon Oh*

Main category: cs.CL

TL;DR: Dr.LLM是一个动态路由框架，通过轻量级路由器决定跳过、执行或重复Transformer层，在保持准确性的同时提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统LLM对所有token都经过所有Transformer层，导致简单查询计算浪费，复杂查询推理深度不足。现有自适应深度方法通常需要昂贵推理时搜索、架构修改或大规模重训练，且往往牺牲准确性。

Method: 使用蒙特卡洛树搜索(MCTS)推导高质量层配置，在计算预算下保持或提高准确性。采用窗口池化稳定路由、焦点损失与类别平衡、瓶颈MLP路由器等技术。

Result: 在ARC和DART任务上准确率提升达+3.4%，平均每样本节省5层。路由器泛化到域外任务仅损失0.85%准确率，比现有路由方法提升达+7.7%。

Conclusion: Dr.LLM证明显式监督的路由器可以改造冻结LLM，实现预算感知、准确性驱动的推理，无需修改基础权重。

Abstract: Large Language Models (LLMs) process every token through all layers of a
transformer stack, causing wasted computation on simple queries and
insufficient flexibility for harder ones that need deeper reasoning.
Adaptive-depth methods can improve efficiency, but prior approaches rely on
costly inference-time search, architectural changes, or large-scale retraining,
and in practice often degrade accuracy despite efficiency gains. We introduce
Dr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that
equips pretrained models with lightweight per-layer routers deciding to skip,
execute, or repeat a block. Routers are trained with explicit supervision:
using Monte Carlo Tree Search (MCTS), we derive high-quality layer
configurations that preserve or improve accuracy under a compute budget. Our
design, windowed pooling for stable routing, focal loss with class balancing,
and bottleneck MLP routers, ensures robustness under class imbalance and long
sequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to
+3.4%p while saving 5 layers per example on average. Routers generalize to
out-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA,
AGIEval) with only 0.85% accuracy drop while retaining efficiency, and
outperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that
explicitly supervised routers retrofit frozen LLMs for budget-aware,
accuracy-driven inference without altering base weights.

</details>


### [62] [Cost Analysis of Human-corrected Transcription for Predominately Oral Languages](https://arxiv.org/abs/2510.12781)
*Yacouba Diarra,Nouhoum Souleymane Coulibaly,Michael Leventhal*

Main category: cs.CL

TL;DR: 该论文研究了为低资源语言创建高质量语音标注数据所需的人力成本，通过对班巴拉语的实地研究发现，在实验室条件下转录1小时语音需要30小时人工，在实地条件下需要36小时。


<details>
  <summary>Details</summary>
Motivation: 了解为低资源语言（特别是低识字率的口语语言）创建语音数据集的实际人力成本，为类似语言创建NLP资源提供基准和实用见解。

Method: 通过为期一个月的实地研究，让10名母语转录员修正ASR生成的班巴拉语语音转录，分析53小时语音数据的转录过程。

Result: 研究发现，在实验室条件下准确转录1小时语音数据平均需要30小时人工，在实地条件下需要36小时人工。

Conclusion: 该研究为具有类似特征的大类语言创建NLP资源提供了基准和实用指导，揭示了为低资源语言构建语音数据集的实际人力投入。

Abstract: Creating speech datasets for low-resource languages is a critical yet poorly
understood challenge, particularly regarding the actual cost in human labor.
This paper investigates the time and complexity required to produce
high-quality annotated speech data for a subset of low-resource languages, low
literacy Predominately Oral Languages, focusing on Bambara, a Manding language
of Mali. Through a one-month field study involving ten transcribers with native
proficiency, we analyze the correction of ASR-generated transcriptions of 53
hours of Bambara voice data. We report that it takes, on average, 30 hours of
human labor to accurately transcribe one hour of speech data under laboratory
conditions and 36 hours under field conditions. The study provides a baseline
and practical insights for a large class of languages with comparable profiles
undertaking the creation of NLP resources.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [63] [AI Agents for the Dhumbal Card Game: A Comparative Study](https://arxiv.org/abs/2510.11736)
*Sahaj Raj Malla*

Main category: cs.AI

TL;DR: 该研究系统评估了针对Dhumbal文化纸牌游戏的多种AI代理，包括规则型、搜索型和学习型策略。在1024轮模拟中，规则型攻击性代理以88.3%胜率表现最佳，超越了ISMCTS和PPO等方法。


<details>
  <summary>Details</summary>
Motivation: 评估AI代理在具有文化意义的不完全信息多人纸牌游戏Dhumbal中的表现，为文化游戏的数字保护提供支持，并推进AI研究。

Method: 形式化Dhumbal游戏机制，实现多种代理策略：启发式方法（攻击性、保守性、平衡性、机会主义）、搜索方法（MCTS、ISMCTS）和强化学习方法（DQN、PPO）。通过类别内锦标赛和跨类别锦标赛进行评估。

Result: 在1024轮模拟中，规则型攻击性代理获得最高胜率（88.3%，95% CI: [86.3, 90.3]），显著优于ISMCTS（9.0%）和PPO（1.5%），主要通过有效利用Jhyap声明获胜。

Conclusion: 研究贡献了可复现的AI框架，揭示了在部分信息下启发式方法的有效性，提供了开源代码，推动了AI研究并支持文化游戏的数字保护。

Abstract: This study evaluates Artificial Intelligence (AI) agents for Dhumbal, a
culturally significant multiplayer card game with imperfect information,
through a systematic comparison of rule-based, search-based, and learning-based
strategies. We formalize Dhumbal's mechanics and implement diverse agents,
including heuristic approaches (Aggressive, Conservative, Balanced,
Opportunistic), search-based methods such as Monte Carlo Tree Search (MCTS) and
Information Set Monte Carlo Tree Search (ISMCTS), and reinforcement learning
approaches including Deep Q-Network (DQN) and Proximal Policy Optimization
(PPO), and a random baseline. Evaluation involves within-category tournaments
followed by a cross-category championship. Performance is measured via win
rate, economic outcome, Jhyap success, cards discarded per round, risk
assessment, and decision efficiency. Statistical significance is assessed using
Welch's t-test with Bonferroni correction, effect sizes via Cohen's d, and 95%
confidence intervals (CI). Across 1024 simulated rounds, the rule-based
Aggressive agent achieves the highest win rate (88.3%, 95% CI: [86.3, 90.3]),
outperforming ISMCTS (9.0%) and PPO (1.5%) through effective exploitation of
Jhyap declarations. The study contributes a reproducible AI framework, insights
into heuristic efficacy under partial information, and open-source code,
thereby advancing AI research and supporting digital preservation of cultural
games.

</details>


### [64] [Beyond Consensus: Mitigating the Agreeableness Bias in LLM Judge Evaluations](https://arxiv.org/abs/2510.11822)
*Suryaansh Jain,Umair Z. Ahmed,Shubham Sahai,Ben Leong*

Main category: cs.AI

TL;DR: 该论文发现LLM作为评估器存在严重的正向偏见问题，并提出少数否决策略和基于回归的框架来缓解这一偏见。


<details>
  <summary>Details</summary>
Motivation: 由于新的大型语言模型频繁发布，开发者需要评估是否切换到新模型。虽然人工评估是黄金标准，但成本高且不可扩展。现有的LLM-as-a-judge方法存在严重的正向偏见问题。

Method: 提出两种方法：1）最优少数否决策略，对缺失数据具有鲁棒性；2）基于回归的框架，使用少量人工标注数据直接建模验证器偏见。

Result: 在366个高中Python程序的代码反馈任务中，回归方法将最大绝对误差降至仅1.2%，比14个最先进LLM的最佳集成性能提升2倍。

Conclusion: LLM评估器存在系统性正向偏见，提出的回归框架能有效缓解这一问题，显著提高评估精度。

Abstract: New Large Language Models (LLMs) become available every few weeks, and modern
application developers confronted with the unenviable task of having to decide
if they should switch to a new model. While human evaluation remains the gold
standard, it is costly and unscalable. The state-of-the-art approach is to use
LLMs as evaluators ( LLM-as-a-judge), but this suffers from a critical flaw:
LLMs exhibit a strong positive bias. We provide empirical evidence showing that
while LLMs can identify valid outputs with high accuracy (i.e., True Positive
Rate 96%), they are remarkably poor at identifying invalid ones (i.e., True
Negative Rate <25%). This systematic bias, coupled with class imbalance, often
leads to inflated reliability scores.
  While ensemble-based methods like majority voting can help, we show that they
are not good enough. We introduce an optimal minority-veto strategy that is
resilient to missing data and mitigates this bias to a large extent. For
scenarios requiring even higher precision, we propose a novel regression-based
framework that directly models the validator bias using a small set of
human-annotated ground truth data. On a challenging code feedback task over 366
high-school Python programs, our regression approach reduces the maximum
absolute error to just 1.2%, achieving a 2x improvement over the
best-performing ensemble of 14 state-of-the-art LLMs.

</details>


### [65] [Holistic Agent Leaderboard: The Missing Infrastructure for AI Agent Evaluation](https://arxiv.org/abs/2510.11977)
*Sayash Kapoor,Benedikt Stroebl,Peter Kirgis,Nitya Nadgir,Zachary S Siegel,Boyi Wei,Tianci Xue,Ziru Chen,Felix Chen,Saiteja Utpala,Franck Ndzomga,Dheeraj Oruganty,Sophie Luskin,Kangheng Liu,Botao Yu,Amit Arora,Dongyoon Hahm,Harsh Trivedi,Huan Sun,Juyong Lee,Tengjun Jin,Yifan Mai,Yifei Zhou,Yuxuan Zhu,Rishi Bommasani,Daniel Kang,Dawn Song,Peter Henderson,Yu Su,Percy Liang,Arvind Narayanan*

Main category: cs.AI

TL;DR: 提出了Holistic Agent Leaderboard (HAL)来解决AI智能体评估中的挑战，包括标准化评估框架、三维分析和LLM辅助日志检查，旨在推动从基准测试优秀到真实世界可靠的智能体发展。


<details>
  <summary>Details</summary>
Motivation: AI智能体评估存在诸多挑战，影响了对智能体真实性能的理解，需要更全面可靠的评估方法。

Method: 1. 标准化评估框架，在数百个VM上并行评估；2. 三维分析（模型、支架、基准）；3. LLM辅助日志检查，发现未报告行为。

Result: 进行了21,730次智能体测试，覆盖9个模型和9个基准，发现意外洞察如更高推理努力反而降低准确性，并揭示了智能体的异常行为模式。

Conclusion: 通过标准化评估方法和解决常见陷阱，推动AI智能体从基准测试优秀向真实世界可靠的方向发展。

Abstract: AI agents have been developed for complex real-world tasks from coding to
customer service. But AI agent evaluations suffer from many challenges that
undermine our understanding of how well agents really work. We introduce the
Holistic Agent Leaderboard (HAL) to address these challenges. We make three
main contributions. First, we provide a standardized evaluation harness that
orchestrates parallel evaluations across hundreds of VMs, reducing evaluation
time from weeks to hours while eliminating common implementation bugs. Second,
we conduct three-dimensional analysis spanning models, scaffolds, and
benchmarks. We validate the harness by conducting 21,730 agent rollouts across
9 models and 9 benchmarks in coding, web navigation, science, and customer
service with a total cost of about $40,000. Our analysis reveals surprising
insights, such as higher reasoning effort reducing accuracy in the majority of
runs. Third, we use LLM-aided log inspection to uncover previously unreported
behaviors, such as searching for the benchmark on HuggingFace instead of
solving a task, or misusing credit cards in flight booking tasks. We share all
agent logs, comprising 2.5B tokens of language model calls, to incentivize
further research into agent behavior. By standardizing how the field evaluates
agents and addressing common pitfalls in agent evaluation, we hope to shift the
focus from agents that ace benchmarks to agents that work reliably in the real
world.

</details>


### [66] [CGBench: Benchmarking Language Model Scientific Reasoning for Clinical Genetics Research](https://arxiv.org/abs/2510.11985)
*Owen Queen,Harrison G. Zhang,James Zou*

Main category: cs.AI

TL;DR: CGBench是一个用于评估语言模型在科学文献解释能力的基准测试，基于临床遗传学专家标注数据构建，测试模型在提取实验结果、评估证据强度、分类实验成果等方面的表现。


<details>
  <summary>Details</summary>
Motivation: 传统变体和基因解释方法耗时费力，生成式语言模型能加速基础研究向临床应用的转化，但现有基准测试任务范围狭窄，无法反映真实研究需求。

Method: 从ClinGen专家标注的临床遗传学文献中构建CGBench基准，测试8种不同语言模型在提取实验结果、评估证据强度、分类实验成果三个关键任务上的表现。

Result: 模型在文献解释方面显示潜力但存在显著差距，推理模型在细粒度任务上表现优异，非推理模型在高层级解释上更好，模型经常产生幻觉或误解结果。

Conclusion: CGBench揭示了语言模型在科学文献精确解释方面的优势和局限，为AI在临床遗传学和更广泛科学领域的未来研究开辟了道路。

Abstract: Variant and gene interpretation are fundamental to personalized medicine and
translational biomedicine. However, traditional approaches are manual and
labor-intensive. Generative language models (LMs) can facilitate this process,
accelerating the translation of fundamental research into clinically-actionable
insights. While existing benchmarks have attempted to quantify the capabilities
of LMs for interpreting scientific data, these studies focus on narrow tasks
that do not translate to real-world research. To meet these challenges, we
introduce CGBench, a robust benchmark that tests reasoning capabilities of LMs
on scientific publications. CGBench is built from ClinGen, a resource of
expert-curated literature interpretations in clinical genetics. CGBench
measures the ability to 1) extract relevant experimental results following
precise protocols and guidelines, 2) judge the strength of evidence, and 3)
categorize and describe the relevant outcome of experiments. We test 8
different LMs and find that while models show promise, substantial gaps exist
in literature interpretation, especially on fine-grained instructions.
Reasoning models excel in fine-grained tasks but non-reasoning models are
better at high-level interpretations. Finally, we measure LM explanations
against human explanations with an LM judge approach, revealing that models
often hallucinate or misinterpret results even when correctly classifying
evidence. CGBench reveals strengths and weaknesses of LMs for precise
interpretation of scientific publications, opening avenues for future research
in AI for clinical genetics and science more broadly.

</details>


### [67] [Asking Clarifying Questions for Preference Elicitation With Large Language Models](https://arxiv.org/abs/2510.12015)
*Ali Montazeralghaem,Guy Tennenholtz,Craig Boutilier,Ofer Meshi*

Main category: cs.AI

TL;DR: 提出一种训练LLMs生成顺序澄清问题的方法，通过前向过程添加“噪声”到用户画像，反向过程训练模型通过提问来“去噪”，从而更有效地获取用户偏好。


<details>
  <summary>Details</summary>
Motivation: 在用户历史有限的情况下，通过生成有效的顺序澄清问题来获取用户偏好，以个性化LLM推荐系统的响应。

Method: 采用两阶段过程：前向过程从用户画像生成澄清问题并逐步移除答案作为“噪声”；反向过程训练模型通过提问来“去噪”用户画像。

Result: 该方法显著提高了LLM在提出漏斗问题和有效获取用户偏好方面的能力。

Conclusion: 所提出的方法能够有效训练LLMs生成顺序澄清问题，从而更好地获取用户偏好，提升推荐系统的个性化效果。

Abstract: Large Language Models (LLMs) have made it possible for recommendation systems
to interact with users in open-ended conversational interfaces. In order to
personalize LLM responses, it is crucial to elicit user preferences, especially
when there is limited user history. One way to get more information is to
present clarifying questions to the user. However, generating effective
sequential clarifying questions across various domains remains a challenge. To
address this, we introduce a novel approach for training LLMs to ask sequential
questions that reveal user preferences. Our method follows a two-stage process
inspired by diffusion models. Starting from a user profile, the forward process
generates clarifying questions to obtain answers and then removes those answers
step by step, serving as a way to add ``noise'' to the user profile. The
reverse process involves training a model to ``denoise'' the user profile by
learning to ask effective clarifying questions. Our results show that our
method significantly improves the LLM's proficiency in asking funnel questions
and eliciting user preferences effectively.

</details>


### [68] [CausalTrace: A Neurosymbolic Causal Analysis Agent for Smart Manufacturing](https://arxiv.org/abs/2510.12033)
*Chathurangi Shyalika,Aryaman Sharma,Fadi El Kalach,Utkarshani Jaimini,Cory Henson,Ramy Harik,Amit Sheth*

Main category: cs.AI

TL;DR: CausalTrace是一个集成到工业CoPilot中的神经符号因果分析模块，通过数据驱动的因果分析结合工业本体和知识图谱，提供因果发现、反事实推理和根本原因分析等功能，实现可解释的决策支持。


<details>
  <summary>Details</summary>
Motivation: 现代制造环境需要准确预测和可解释的异常分析，但现有AI系统往往作为孤立黑盒运行，缺乏预测、解释和因果推理的无缝集成，限制了在工业环境中的可信度和实用性。

Method: 开发CausalTrace神经符号因果分析模块，集成到SmartPilot工业CoPilot中，利用数据驱动因果分析结合工业本体和知识图谱，支持因果发现、反事实推理和根本原因分析，提供实时操作员交互。

Result: 在学术火箭组装测试中，CausalTrace与领域专家达成高度一致（ROUGE-1: 0.91），根本原因分析性能优异（MAP@3: 94%, PR@2: 97%, MRR: 0.92, Jaccard: 0.92），C3AN评估得分4.59/5，展示了实时部署的精确性和可靠性。

Conclusion: CausalTrace通过神经符号因果分析成功解决了工业AI系统的可解释性问题，为高风险的工业环境提供了可信赖的决策支持解决方案，具备实时部署的潜力。

Abstract: Modern manufacturing environments demand not only accurate predictions but
also interpretable insights to process anomalies, root causes, and potential
interventions. Existing AI systems often function as isolated black boxes,
lacking the seamless integration of prediction, explanation, and causal
reasoning required for a unified decision-support solution. This fragmentation
limits their trustworthiness and practical utility in high-stakes industrial
environments. In this work, we present CausalTrace, a neurosymbolic causal
analysis module integrated into the SmartPilot industrial CoPilot. CausalTrace
performs data-driven causal analysis enriched by industrial ontologies and
knowledge graphs, including advanced functions such as causal discovery,
counterfactual reasoning, and root cause analysis (RCA). It supports real-time
operator interaction and is designed to complement existing agents by offering
transparent, explainable decision support. We conducted a comprehensive
evaluation of CausalTrace using multiple causal assessment methods and the C3AN
framework (i.e. Custom, Compact, Composite AI with Neurosymbolic Integration),
which spans principles of robustness, intelligence, and trustworthiness. In an
academic rocket assembly testbed, CausalTrace achieved substantial agreement
with domain experts (ROUGE-1: 0.91 in ontology QA) and strong RCA performance
(MAP@3: 94%, PR@2: 97%, MRR: 0.92, Jaccard: 0.92). It also attained 4.59/5 in
the C3AN evaluation, demonstrating precision and reliability for live
deployment.

</details>


### [69] [Do Large Language Models Respect Contracts? Evaluating and Enforcing Contract-Adherence in Code Generation](https://arxiv.org/abs/2510.12047)
*Soohan Lim,Joonghyuk Hahn,Hyunwoo Park,Sang-Ki Ko,Yo-Sub Han*

Main category: cs.AI

TL;DR: PACT是一个评估LLM生成代码合同遵循性的框架，扩展了HumanEval+和MBPP+基准，通过合同违反测试用例增强提示，显著提升模型对合同的尊重能力。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成基准主要评估功能正确性，忽略了真实软件中的合同遵循性（处理异常输入的能力），导致无法衡量代码的健壮性和可靠性。

Method: 构建专注于合同违反的测试套件语料库，扩展现有基准；系统分析不同提示条件下的代码生成；引入新的指标来量化合同遵循性。

Result: 通过合同违反测试用例增强提示，相比仅使用合同描述，能显著提升模型对合同的尊重能力；揭示了传统基准忽略的关键错误。

Conclusion: PACT提供了严格可解释的指标，用于评估LLM生成代码在功能和合同遵循性方面的健壮性，填补了现有评估框架的重要空白。

Abstract: Prevailing code generation benchmarks, such as HumanEval+ and MBPP+,
primarily evaluate large language models (LLMs) with pass@k on functional
correctness using well-formed inputs. However, they ignore a crucial aspect of
real-world software: adherence to contracts-the preconditions and validity
constraints that dictate how ill-formed inputs must be rejected. This critical
oversight means that existing benchmarks fail to measure, and models
consequently fail to generate, truly robust and reliable code snippets. We
introduce PACT, a program assessment and contract-adherence evaluation
framework, to bridge this gap. PACT is the first framework designed to
systematically evaluate and enhance contract-adherence in LLM-generated code
snippets alongside functional correctness. PACT's contributions are threefold:
First, it provides a comprehensive test-suite corpus focused on contract
violations, extending HumanEval+ and MBPP+. Second, it enables a systematic
analysis of code generation under varied prompting conditions. This analysis
demonstrates that augmenting prompts with contract-violating test cases
significantly enhance a model's ability to respect contracts compared to using
contract description alone. Finally, it introduces novel metrics to rigorously
quantify contract adherence in both test generation and code generation. By
revealing critical errors that conventional benchmarks overlook, PACT provides
the rigorous and interpretable metrics to evaluate the robustness of
LLM-generated code snippets in both functionality and contract-adherence.Our
code and data are available at https://github.com/suhanmen/PACT.

</details>


### [70] [Empowering LLM Agents with Geospatial Awareness: Toward Grounded Reasoning for Wildfire Response](https://arxiv.org/abs/2510.12061)
*Yiheng Chen,Lingyao Li,Zihui Ma,Qikai Hu,Yilun Zhu,Min Deng,Runlong Yu*

Main category: cs.AI

TL;DR: 提出了地理空间感知层(GAL)框架，将LLM智能体与结构化地球数据相结合，用于灾害响应中的资源分配决策。


<details>
  <summary>Details</summary>
Motivation: 现有统计方法缺乏语义上下文、跨事件泛化能力差且可解释性有限，而LLM虽然具有少样本泛化能力但受限于文本且缺乏地理感知。

Method: 开发地理空间感知层(GAL)，从原始野火检测出发，自动从外部地理数据库中检索并整合基础设施、人口统计、地形和天气信息，生成带单位注释的感知脚本。

Result: 在真实野火场景中评估，地理空间接地的智能体在多个LLM模型上均优于基线方法。

Conclusion: 该框架能够推广到洪水、飓风等其他灾害类型，为灾害响应提供基于证据的资源分配建议。

Abstract: Effective disaster response is essential for safeguarding lives and property.
Existing statistical approaches often lack semantic context, generalize poorly
across events, and offer limited interpretability. While Large language models
(LLMs) provide few-shot generalization, they remain text-bound and blind to
geography. To bridge this gap, we introduce a Geospatial Awareness Layer (GAL)
that grounds LLM agents in structured earth data. Starting from raw wildfire
detections, GAL automatically retrieves and integrates infrastructure,
demographic, terrain, and weather information from external geodatabases,
assembling them into a concise, unit-annotated perception script. This enriched
context enables agents to produce evidence-based resource-allocation
recommendations (e.g., personnel assignments, budget allocations), further
reinforced by historical analogs and daily change signals for incremental
updates. We evaluate the framework in real wildfire scenarios across multiple
LLM models, showing that geospatially grounded agents can outperform baselines.
The proposed framework can generalize to other hazards such as floods and
hurricanes.

</details>


### [71] [ThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization](https://arxiv.org/abs/2510.12063)
*Sunzhu Li,Zhiyu Lin,Shuling Yang,Jiale Zhao,Wei Chen*

Main category: cs.AI

TL;DR: ThinkPilot是一个无需训练的框架，通过进化过程生成think-prefixes来优化大型推理模型的推理性能，显著提升准确率-效率权衡、安全性和指令遵循能力。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然强大，但存在推理效率低和偏离目标的问题。当前无需训练的方法要么依赖僵化的启发式规则，要么只能提供描述性但不可操作的分析。

Method: 使用进化过程生成think-prefixes（思考前缀），这些前缀由推理行为分类驱动，引导模型实现更优性能。该方法无需额外训练。

Result: ThinkPilot显著改善了准确率与推理长度的权衡关系，大幅提升安全性（如将DeepSeek-R1-Distill-Qwen-32B的StrongREJECT分数从27.0%降至0.7），并增强指令遵循能力，还能与现有基于训练的方法协同工作。

Conclusion: think-prefixes能够可靠地控制大型推理模型的推理行为，不同任务对特定行为分布有强烈偏好。ThinkPilot通过自动识别和激发这些行为，为将模型推理与任务需求对齐提供了通用框架。

Abstract: Large Reasoning Models (LRMs) are powerful, but they still suffer from
inefficient and off-target reasoning. Currently, training-free methods are
limited to either rigid heuristics or descriptive, non-actionable analyses. In
this paper, we introduce ThinkPilot, a training-free framework that
automatically optimizes LRMs reasoning. It uses an evolutionary process to
generate think-prefixes, which are instructions that evolve driven by a
taxonomy of reasoning behaviors to guide models toward superior performance.
Extensive experiments demonstrate ThinkPilot's broad effectiveness: it
significantly improves the accuracy-length trade-off for efficient reasoning,
drastically improves safety (for example, cutting the StrongREJECT score of
DeepSeek-R1-Distill-Qwen-32B from 27.0% to 0.7), and enhances instruction
following. It also synergizes with existing training-based methods. Our
analysis reveals that think-prefixes can reliably control LRMs' reasoning
behaviors, and that different tasks have strong preferences for specific
behavioral distributions. By automatically identifying and eliciting these
behaviors, ThinkPilot provides a generalizable framework for aligning LRMs
reasoning with task demands. Data and code are available at
https://github.com/teqkilla/ThinkPilot

</details>


### [72] [AI Agents as Universal Task Solvers](https://arxiv.org/abs/2510.12066)
*Alessandro Achille,Stefano Soatto*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: AI reasoning agents are already able to solve a variety of tasks by deploying
tools, simulating outcomes of multiple hypotheses and reflecting on them. In
doing so, they perform computation, although not in the classical sense --
there is no program being executed. Still, if they perform computation, can AI
agents be universal? Can chain-of-thought reasoning solve any computable task?
How does an AI Agent learn to reason? Is it a matter of model size? Or training
dataset size?
  In this work, we reinterpret the role of learning in the context of AI
Agents, viewing them as compute-capable stochastic dynamical systems, and
highlight the role of time in a foundational principle for learning to reason.
In doing so, we propose a shift from classical inductive learning to
transductive learning -- where the objective is not to approximate the
distribution of past data, but to capture their algorithmic structure to reduce
the time needed to find solutions to new tasks.
  Transductive learning suggests that, counter to Shannon's theory, a key role
of information in learning is about reduction of time rather than
reconstruction error. In particular, we show that the optimal speed-up that a
universal solver can achieve using past data is tightly related to their
algorithmic information. Using this, we show a theoretical derivation for the
observed power-law scaling of inference time versus training time. We then show
that scaling model size can lead to behaviors that, while improving accuracy on
benchmarks, fail any reasonable test of intelligence, let alone
super-intelligence: In the limit of infinite space and time, large models can
behave as savants, able to brute-force through any task without any insight.
Instead, we argue that the key quantity to optimize when scaling reasoning
models is time, whose critical role in learning has so far only been indirectly
considered.

</details>


### [73] [HiCoTraj:Zero-Shot Demographic Reasoning via Hierarchical Chain-of-Thought Prompting from Trajectory](https://arxiv.org/abs/2510.12067)
*Junyi Xie,Yuankun Jiao,Jina Kim,Yao-Yi Chiang,Lingyi Zhao,Khurram Shafique*

Main category: cs.AI

TL;DR: HiCoTraj是一个零样本人口统计推理框架，通过分层思维链提示从轨迹数据中推断年龄、性别、收入等人口属性，无需标注训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有基于移动轨迹的人口统计推断方法严重依赖带标签的大规模轨迹数据，导致可解释性差且在不同数据集和用户群体间泛化能力弱。

Method: 将轨迹转换为语义丰富的自然语言表示，创建详细活动记录和多尺度访问摘要，然后通过分层思维链推理（事实特征提取、行为模式分析、结构化输出的人口统计推断）指导LLMs。

Result: 在真实世界轨迹数据上的实验评估表明，HiCoTraj在零样本场景下对多个人口统计属性实现了有竞争力的性能。

Conclusion: HiCoTraj解决了标注人口统计数据稀缺的挑战，同时提供了透明的推理链，具有良好的可解释性和泛化能力。

Abstract: Inferring demographic attributes such as age, sex, or income level from human
mobility patterns enables critical applications such as targeted public health
interventions, equitable urban planning, and personalized transportation
services. Existing mobility-based demographic inference studies heavily rely on
large-scale trajectory data with demographic labels, leading to limited
interpretability and poor generalizability across different datasets and user
groups. We propose HiCoTraj (Zero-Shot Demographic Reasoning via Hierarchical
Chain-of-Thought Prompting from Trajectory), a framework that leverages LLMs'
zero-shot learning and semantic understanding capabilities to perform
demographic inference without labeled training data. HiCoTraj transforms
trajectories into semantically rich, natural language representations by
creating detailed activity chronicles and multi-scale visiting summaries. Then
HiCoTraj uses a novel hierarchical chain of thought reasoning to systematically
guide LLMs through three cognitive stages: factual feature extraction,
behavioral pattern analysis, and demographic inference with structured output.
This approach addresses the scarcity challenge of labeled demographic data
while providing transparent reasoning chains. Experimental evaluation on
real-world trajectory data demonstrates that HiCoTraj achieves competitive
performance across multiple demographic attributes in zero-shot scenarios.

</details>


### [74] [EmboMatrix: A Scalable Training-Ground for Embodied Decision-Making](https://arxiv.org/abs/2510.12072)
*Zixing Lei,Sheng Yin,Yichen Xiong,Yuanzhuo Ding,Wenhao Huang,Yuxi Wei,Qingyao Xu,Yiming Li,Weixin Li,Yunhong Wang,Siheng Chen*

Main category: cs.AI

TL;DR: EmboMatrix是一个为LLM提供真实物理环境交互的训练平台，通过大规模任务模拟和精确奖励机制，培养出具有强大具身决策能力的EmboBrain模型。


<details>
  <summary>Details</summary>
Motivation: 解决LLM仅基于语言训练而缺乏物理环境暴露的问题，需要构建能够提供真实物理交互的训练基础设施来培养真正的具身决策能力。

Method: 提出EmboMatrix训练平台，包含多智能体数据引擎生成大规模任务场景、分布式异构硬件系统实现可扩展模拟、多级奖励架构提供精确监督。

Result: 基于EmboMatrix训练的EmboBrain-7B模型在两个具身决策基准测试中超越了671B参数的DeepSeek-R1基线9.5%。

Conclusion: 交互式、环境基础的学习方法对于构建真正智能的具身智能体具有强大潜力，EmboMatrix为LLM获得真实具身决策能力提供了有效解决方案。

Abstract: Embodied decision-making enables agents to translate high-level goals into
executable actions through continuous interactions within the physical world,
forming a cornerstone of general-purpose embodied intelligence. Large language
models (LLMs), with their general decision-making capabilities, offer a
promising path to realize this potential; however, LLMs trained solely on
language lack exposure to physical environments, limiting their true embodied
understanding. To bridge this gap, we propose the concept of a training ground:
a comprehensive infrastructure that provides task and scene simulation,
embodied interaction, and feedback signals, offering a one-stop solution for
LLM acquire genuine embodied decision-making skills. In this work, we present
EmboMatrix, the first training ground of its kind, providing massive and
diverse tasks with efficient simulation and precise rewards. EmboMatrix
incorporates a series of novel techniques: a multi-agent data engine for
large-scale task and scene generation, a distributed heterogeneous-hardware
system for scalable simulation, and a multi-level reward architecture for
precise supervision. Leveraging EmboMatrix, we cultivate EmboBrain, an LLM
whose embodied decision-making abilities emerge from extensive embodied
interactions. Experiments show that EmboBrain-7B surpasses the 671B DeepSeek-R1
baseline by 9.5\% on two challenging embodied decision-making benchmarks,
demonstrating the power of interactive, environment-grounded learning for
building truly intelligent embodied agents.

</details>


### [75] [BeSTAD: Behavior-Aware Spatio-Temporal Anomaly Detection for Human Mobility Data](https://arxiv.org/abs/2510.12076)
*Junyi Xie,Jina Kim,Yao-Yi Chiang,Lingyi Zhao,Khurram Shafique*

Main category: cs.AI

TL;DR: BeSTAD是一个无监督框架，通过联合建模空间上下文和时间动态来检测人类移动数据中的个体级异常，能够在大规模人群中捕获个体化行为特征。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测主要关注轨迹级分析，而检测个体相对于自身历史模式的异常行为在大规模数据集中仍具挑战性。

Method: 学习语义丰富的移动表示，整合位置意义和时间模式；采用行为聚类感知建模机制，从正常活动中构建个性化行为档案，并通过跨周期行为比较识别异常。

Result: 能够检测个体移动行为中的细微偏差，识别行为转变和偏离既定常规的情况。

Conclusion: BeSTAD通过直接从无标签数据学习个体行为，推进了异常检测向个性化和可解释移动分析的发展。

Abstract: Traditional anomaly detection in human mobility has primarily focused on
trajectory-level analysis, identifying statistical outliers or spatiotemporal
inconsistencies across aggregated movement traces. However, detecting
individual-level anomalies, i.e., unusual deviations in a person's mobility
behavior relative to their own historical patterns, within datasets
encompassing large populations remains a significant challenge. In this paper,
we present BeSTAD (Behavior-aware Spatio-Temporal Anomaly Detection for Human
Mobility Data), an unsupervised framework that captures individualized
behavioral signatures across large populations and uncovers fine-grained
anomalies by jointly modeling spatial context and temporal dynamics. BeSTAD
learns semantically enriched mobility representations that integrate location
meaning and temporal patterns, enabling the detection of subtle deviations in
individual movement behavior. BeSTAD further employs a behavior-cluster-aware
modeling mechanism that builds personalized behavioral profiles from normal
activity and identifies anomalies through cross-period behavioral comparison
with consistent semantic alignment. Building on prior work in mobility behavior
clustering, this approach enables not only the detection of behavioral shifts
and deviations from established routines but also the identification of
individuals exhibiting such changes within large-scale mobility datasets. By
learning individual behaviors directly from unlabeled data, BeSTAD advances
anomaly detection toward personalized and interpretable mobility analysis.

</details>


### [76] [Evaluating the Quality of Randomness and Entropy in Tasks Supported by Large Language Models](https://arxiv.org/abs/2510.12080)
*Rabimba Karanjai,Yang Lu,Ranjith Chodavarapu,Lei Xu,Weidong Shi*

Main category: cs.AI

TL;DR: 本文研究了大型语言模型处理随机性任务的能力，发现虽然LLM能产生一定随机性输出，但表现不稳定且与预期行为存在显著偏差。


<details>
  <summary>Details</summary>
Motivation: 随着LLM技术的快速发展，许多应用需要随机性（如随机决策、游戏、调度等），但LLM处理随机性的能力尚不明确。

Method: 设计了一系列实验，考虑外部工具可访问性、任务类型、模型状态（新鲜vs非新鲜）和提示策略等因素，涵盖生成随机数、随机字符串、项目洗牌等任务。

Result: LLM生成的输出具有一定随机性，但表现不一致，且与预期行为存在显著偏差。

Conclusion: LLM在处理涉及随机性的任务时存在关键局限性，需要在多个方面进行改进才能有效处理此类任务。

Abstract: The rapid advancement of large language model (LLM) technology has led to
diverse applications, many of which inherently require randomness, such as
stochastic decision-making, gaming, scheduling, AI agents, and
cryptography-related tasks. However, the capabilities of LLMs in handling
randomness, particularly in generating and utilizing random numbers
effectively, remain unclear. This paper investigates the capacity of LLMs for
handling tasks that involve randomness through a series of experiments. We
designed a set of experiments that consider various factors that can influence
an LLM's performance in tasks involving randomness, such as accessibility to
external tools, types of tasks, model states (fresh vs. non-fresh), and
prompting strategies. The experiments cover a range of tasks, including
generating random numbers, generating random strings such as passwords,
shuffling items, and evaluating the quality of randomness using entropy and the
NIST randomness test-suite. Our findings reveal that while LLMs can generate
outputs that exhibit some degree of randomness, their performance is
inconsistent and often deviates significantly from the expected behavior. The
analysis of the experimental results highlights key limitations and areas where
improvement is needed for the LLMs to effectively handle tasks involving
randomness

</details>


### [77] [One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration](https://arxiv.org/abs/2510.12088)
*Zaid Khan,Archiki Prasad,Elias Stengel-Eskin,Jaemin Cho,Mohit Bansal*

Main category: cs.AI

TL;DR: OneLife框架通过条件激活的程序化法则在概率编程框架中建模世界动态，能够在复杂、随机环境中从有限的非指导交互中学习关键环境动态。


<details>
  <summary>Details</summary>
Motivation: 解决在复杂、随机环境中，智能体只有"一次生命"来探索敌对环境且无人指导的现实挑战性场景下的符号世界建模问题。

Method: 使用条件激活的程序化法则，采用前提-效果结构，在相关世界状态中激活，创建动态计算图，仅通过相关法则进行推理和优化。

Result: 在23个测试场景中的16个上优于强基线，能够从最小化的非指导交互中成功学习关键环境动态，模拟推演成功识别出更优策略。

Conclusion: 为自主构建未知复杂环境的程序化世界模型奠定了基础。

Abstract: Symbolic world modeling requires inferring and representing an environment's
transitional dynamics as an executable program. Prior work has focused on
largely deterministic environments with abundant interaction data, simple
mechanics, and human guidance. We address a more realistic and challenging
setting, learning in a complex, stochastic environment where the agent has only
"one life" to explore a hostile environment without human guidance. We
introduce OneLife, a framework that models world dynamics through
conditionally-activated programmatic laws within a probabilistic programming
framework. Each law operates through a precondition-effect structure,
activating in relevant world states. This creates a dynamic computation graph
that routes inference and optimization only through relevant laws, avoiding
scaling challenges when all laws contribute to predictions about a complex,
hierarchical state, and enabling the learning of stochastic dynamics even with
sparse rule activation. To evaluate our approach under these demanding
constraints, we introduce a new evaluation protocol that measures (a) state
ranking, the ability to distinguish plausible future states from implausible
ones, and (b) state fidelity, the ability to generate future states that
closely resemble reality. We develop and evaluate our framework on Crafter-OO,
our reimplementation of the Crafter environment that exposes a structured,
object-oriented symbolic state and a pure transition function that operates on
that state alone. OneLife can successfully learn key environment dynamics from
minimal, unguided interaction, outperforming a strong baseline on 16 out of 23
scenarios tested. We also test OneLife's planning ability, with simulated
rollouts successfully identifying superior strategies. Our work establishes a
foundation for autonomously constructing programmatic world models of unknown,
complex environments.

</details>


### [78] [ToPolyAgent: AI Agents for Coarse-Grained Topological Polymer Simulations](https://arxiv.org/abs/2510.12091)
*Lijie Ding,Jan-Michael Carrillo,Changwoo Do*

Main category: cs.AI

TL;DR: ToPolyAgent是一个多智能体AI框架，通过自然语言指令执行拓扑聚合物的粗粒度分子动力学模拟，将大语言模型与领域特定计算工具集成，支持交互式和自主式工作流程。


<details>
  <summary>Details</summary>
Motivation: 降低复杂计算工作流的门槛，推进聚合物科学中AI驱动的材料发现，为自主和可扩展的多智能体科学研究生态系统奠定基础。

Method: 系统包含四个LLM驱动的智能体：配置智能体生成初始聚合物-溶剂配置，模拟智能体执行LAMMPS分子动力学模拟和构象分析，报告智能体编译markdown报告，工作流智能体实现流线化自主操作。支持交互式和自主式两种模式。

Result: 通过涉及不同聚合物结构、溶剂条件、恒温器和模拟长度的案例研究展示了ToPolyAgent的多功能性，成功指导系统研究相互作用参数对线性聚合物构象的影响以及接枝密度对刷状聚合物持久长度的影响。

Conclusion: 通过将自然语言界面与严格的模拟工具相结合，ToPolyAgent降低了复杂计算工作流的障碍，并推进了聚合物科学中AI驱动的材料发现，为自主和可扩展的多智能体科学研究生态系统奠定了基础。

Abstract: We introduce ToPolyAgent, a multi-agent AI framework for performing
coarse-grained molecular dynamics (MD) simulations of topological polymers
through natural language instructions. By integrating large language models
(LLMs) with domain-specific computational tools, ToPolyAgent supports both
interactive and autonomous simulation workflows across diverse polymer
architectures, including linear, ring, brush, and star polymers, as well as
dendrimers. The system consists of four LLM-powered agents: a Config Agent for
generating initial polymer-solvent configurations, a Simulation Agent for
executing LAMMPS-based MD simulations and conformational analyses, a Report
Agent for compiling markdown reports, and a Workflow Agent for streamlined
autonomous operations. Interactive mode incorporates user feedback loops for
iterative refinements, while autonomous mode enables end-to-end task execution
from detailed prompts. We demonstrate ToPolyAgent's versatility through case
studies involving diverse polymer architectures under varying solvent
condition, thermostats, and simulation lengths. Furthermore, we highlight its
potential as a research assistant by directing it to investigate the effect of
interaction parameters on the linear polymer conformation, and the influence of
grafting density on the persistence length of the brush polymer. By coupling
natural language interfaces with rigorous simulation tools, ToPolyAgent lowers
barriers to complex computational workflows and advances AI-driven materials
discovery in polymer science. It lays the foundation for autonomous and
extensible multi-agent scientific research ecosystems.

</details>


### [79] [Precise Attribute Intensity Control in Large Language Models via Targeted Representation Editing](https://arxiv.org/abs/2510.12121)
*Rongzhi Zhang,Liqin Ye,Yuzhao Heng,Xiang Chen,Tong Yu,Lingkai Kong,Sudheer Chava,Chao Zhang*

Main category: cs.AI

TL;DR: 提出了一种精确控制LLM输出属性强度的方法，通过目标达成问题重构、时序差分学习训练价值函数和基于梯度的隐表示干预，实现细粒度连续控制


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐方法只能提供方向性或开放式指导，无法可靠实现精确的属性强度控制，这限制了AI系统适应不同用户期望的能力

Method: 1) 将精确属性强度控制重构为目标达成问题；2) 通过时序差分学习训练轻量级价值函数预测属性强度；3) 使用基于梯度的隐表示干预精确导航到特定属性强度目标

Result: 在LLaMA-3.2-3b和Phi-4-mini上的实验证实该方法能够以高精度将文本生成引导到用户指定的属性强度

Conclusion: 该方法超越了简单的方向对齐，实现了对属性强度的细粒度连续控制，并在偏好数据合成、帕累托前沿逼近和优化、对齐行为蒸馏等下游任务中展示了效率提升

Abstract: Precise attribute intensity control--generating Large Language Model (LLM)
outputs with specific, user-defined attribute intensities--is crucial for AI
systems adaptable to diverse user expectations. Current LLM alignment methods,
however, typically provide only directional or open-ended guidance, failing to
reliably achieve exact attribute intensities. We address this limitation with
three key designs: (1) reformulating precise attribute intensity control as a
target-reaching problem, rather than simple maximization; (2) training a
lightweight value function via temporal-difference learning to predict final
attribute intensity scores from partial generations, thereby steering LLM
outputs; and (3) employing gradient-based interventions on hidden
representations to navigate the model precisely towards specific attribute
intensity targets. Our method enables fine-grained, continuous control over
attribute intensities, moving beyond simple directional alignment. Experiments
on LLaMA-3.2-3b and Phi-4-mini confirm our method's ability to steer text
generation to user-specified attribute intensities with high accuracy. Finally,
we demonstrate efficiency enhancements across three downstream tasks:
preference data synthesis, Pareto frontier approximation and optimization, and
distillation of aligned behaviors for intervention-free inference. Our code is
available on https://github.com/Pre-Control/pre-control

</details>


### [80] [MatSciBench: Benchmarking the Reasoning Ability of Large Language Models in Materials Science](https://arxiv.org/abs/2510.12171)
*Junkai Zhang,Jingru Gan,Xiaoxuan Wang,Zian Jia,Changquan Gu,Jianpeng Chen,Yanqiao Zhu,Mingyu Derek Ma,Dawei Zhou,Ling Li,Wei Wang*

Main category: cs.AI

TL;DR: MatSciBench是一个包含1340个大学水平材料科学问题的综合基准，涵盖6个主要领域和31个子领域，具有三级难度分类和多模态推理能力。评估显示即使最佳模型准确率也低于80%，且不同推理策略在不同场景下表现不一。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在科学推理方面表现出色，但在材料科学领域的推理能力尚未充分探索，需要建立专门的基准来评估和推动改进。

Method: 构建MatSciBench基准，包含1340个结构化问题，采用三级难度分类，提供详细参考解决方案，并评估不同推理策略（基础思维链、工具增强、自我修正）的表现。

Result: 评估显示即使表现最佳的Gemini-2.5-Pro模型在材料科学问题上的准确率也低于80%，不同推理策略在不同场景下表现不一致，没有单一方法在所有情况下都表现优异。

Conclusion: MatSciBench为评估和提升LLMs在材料科学领域的科学推理能力建立了全面而坚实的基准，揭示了当前模型在该领域的局限性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable abilities in
scientific reasoning, yet their reasoning capabilities in materials science
remain underexplored. To fill this gap, we introduce MatSciBench, a
comprehensive college-level benchmark comprising 1,340 problems that span the
essential subdisciplines of materials science. MatSciBench features a
structured and fine-grained taxonomy that categorizes materials science
questions into 6 primary fields and 31 sub-fields, and includes a three-tier
difficulty classification based on the reasoning length required to solve each
question. MatSciBench provides detailed reference solutions enabling precise
error analysis and incorporates multimodal reasoning through visual contexts in
numerous questions. Evaluations of leading models reveal that even the
highest-performing model, Gemini-2.5-Pro, achieves under 80% accuracy on
college-level materials science questions, highlighting the complexity of
MatSciBench. Our systematic analysis of different reasoning strategie--basic
chain-of-thought, tool augmentation, and self-correction--demonstrates that no
single method consistently excels across all scenarios. We further analyze
performance by difficulty level, examine trade-offs between efficiency and
accuracy, highlight the challenges inherent in multimodal reasoning tasks,
analyze failure modes across LLMs and reasoning methods, and evaluate the
influence of retrieval-augmented generation. MatSciBench thus establishes a
comprehensive and solid benchmark for assessing and driving improvements in the
scientific reasoning capabilities of LLMs within the materials science domain.

</details>


### [81] [Evolution of meta's llama models and parameter-efficient fine-tuning of large language models: a survey](https://arxiv.org/abs/2510.12178)
*Abdulhady Abas Abdullah,Arkaitz Zubiaga,Seyedali Mirjalili,Amir H. Gandomi,Fatemeh Daneshfar,Mohammadsadra Amini,Alan Salam Mohammed,Hadi Veisi*

Main category: cs.AI

TL;DR: 本文综述了Meta AI的LLaMA系列模型（从LLaMA 1到LLaMA 4）的快速演进，以及为这些模型开发的参数高效微调（PEFT）方法，包括模型架构、性能特征和五种PEFT方法的应用分析。


<details>
  <summary>Details</summary>
Motivation: 随着LLaMA系列模型的快速发展，需要系统性地梳理这些基础模型的演进历程和相关的参数高效微调方法，为研究者和实践者提供一站式资源。

Method: 首先描述了LLaMA系列基础模型（7B-65B到288B参数）的架构和性能特征，然后详细介绍了五种应用于LLaMA的PEFT方法：LoRA、LLaMA-Adapter V1和V2、LLaMA-Excitor以及QLoRA，分析了每种方法的机制、参数节省和应用案例。

Result: 提供了模型和适配器架构、参数数量和基准测试结果的结构化分析，展示了微调后的LLaMA模型在某些情况下能够超越更大基线模型的性能，并探讨了在现实世界应用中的成功案例。

Conclusion: LLaMA模型和PEFT方法在多个领域取得了成功应用，但仍面临扩展到更大上下文和改进鲁棒性等挑战，为未来研究指明了方向。

Abstract: This review surveys the rapid evolution of Meta AI's LLaMA (Large Language
Model Meta AI) series - from LLaMA 1 through LLaMA 4 and the specialized
parameter-efficient fine-tuning (PEFT) methods developed for these models. We
first describe the LLaMA family of foundation models (7B-65B to 288B
parameters), their architectures (including native multimodal and
Mixtureof-Experts variants), and key performance characteristics. We then
describe and discuss the concept of PEFT, which adapts large pre-trained models
by updating only a small subset of parameters, and review five PEFT methods
that have been applied to LLaMA: LoRA (Low-Rank Adaptation), LLaMA-Adapter V1
and V2, LLaMA-Excitor, and QLoRA (Quantized LoRA). We discuss each method's
mechanism, parameter savings, and example application to LLaMA (e.g.,
instruction tuning, multimodal tasks). We provide structured discussion and
analysis of model and adapter architectures, parameter counts, and benchmark
results (including examples where fine-tuned LLaMA models outperform larger
baselines). Finally, we examine real-world use cases where LLaMA-based models
and PEFT have been successfully applied (e.g., legal and medical domains), and
we discuss ongoing challenges and future research directions (such as scaling
to even larger contexts and improving robustness). This survey paper provides a
one-stop resource for ML researchers and practitioners interested in LLaMA
models and efficient fine-tuning strategies.

</details>


### [82] [ResearStudio: A Human-Intervenable Framework for Building Controllable Deep-Research Agents](https://arxiv.org/abs/2510.12194)
*Linyi Yang,Yixuan Weng*

Main category: cs.AI

TL;DR: ResearStudio是一个开源框架，通过实时人机协作设计，让用户在AI研究代理执行过程中能够随时干预、修正和添加专业知识，同时保持最先进的自动化性能。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究代理采用"发射后不管"模式，用户无法在执行过程中修正错误或添加专家知识。

Method: 采用协作工作坊设计，包含分层规划器-执行器架构，将每个步骤写入实时"计划即文档"，通过快速通信层将操作流式传输到Web界面，支持用户随时暂停、编辑计划和代码、运行自定义命令。

Result: 在完全自主模式下，ResearStudio在GAIA基准测试中达到最先进水平，超越了OpenAI的DeepResearch和Manus等系统。

Conclusion: 强大的自动化性能和细粒度的人工控制可以共存，为安全可控的研究代理开发提供了新方向。

Abstract: Current deep-research agents run in a ''fire-and-forget'' mode: once started,
they give users no way to fix errors or add expert knowledge during execution.
We present ResearStudio, the first open-source framework that places real-time
human control at its core. The system follows a Collaborative Workshop design.
A hierarchical Planner-Executor writes every step to a live
''plan-as-document,'' a fast communication layer streams each action, file
change, and tool call to a web interface. At any moment, the user can pause the
run, edit the plan or code, run custom commands, and resume -- switching
smoothly between AI-led, human-assisted and human-led, AI-assisted modes. In
fully autonomous mode, ResearStudio achieves state-of-the-art results on the
GAIA benchmark, surpassing systems like OpenAI's DeepResearch and Manus. These
results show that strong automated performance and fine-grained human control
can coexist. The full code, protocol, and evaluation scripts are available at
https://github.com/ResearAI/ResearStudio. We will continue to update the
repository to encourage further work on safe and controllable research agents.
Our live demo is publicly accessible at http://ai-researcher.net:3000/. We
support the development of DeepScientist, which can be accessed at
https://github.com/ResearAI/DeepScientist.

</details>


### [83] [On the Design and Evaluation of Human-centered Explainable AI Systems: A Systematic Review and Taxonomy](https://arxiv.org/abs/2510.12201)
*Aline Mangold,Juliane Zietz,Susanne Weinhold,Sebastian Pannasch*

Main category: cs.AI

TL;DR: 本文对65个评估可解释AI(XAI)系统的用户研究进行了全面综述，提出了以人为中心的XAI系统设计目标和评估框架，并根据用户AI专业水平(新手和专家)进行了差异化设计。


<details>
  <summary>Details</summary>
Motivation: 随着AI在日常生活中的普及，对既高性能又可理解的智能系统需求增加。当前XAI系统评估过于技术化，缺乏对用户需求的关注，需要以用户研究为指导来改进评估方法。

Method: 对65个XAI用户研究进行系统性综述分析，提出以人为中心的XAI系统特性和评估指标分类，并针对不同AI专业水平的用户(新手和专家)制定差异化设计目标。

Result: 发现XAI系统由核心系统和解释组件构成；评估指标可分为情感、认知、可用性、可解释性和解释指标；用户特征和行为可被评估；为AI新手设计目标包括负责任使用、接受度和可用性，为数据专家设计目标包括人机协作和任务性能。

Conclusion: 提出了扩展的XAI评估和设计框架，强调需要根据用户专业水平定制XAI系统，以更好地满足不同用户群体的需求，促进XAI系统的实际应用效果。

Abstract: As AI becomes more common in everyday living, there is an increasing demand
for intelligent systems that are both performant and understandable.
Explainable AI (XAI) systems aim to provide comprehensible explanations of
decisions and predictions. At present, however, evaluation processes are rather
technical and not sufficiently focused on the needs of human users.
Consequently, evaluation studies involving human users can serve as a valuable
guide for conducting user studies. This paper presents a comprehensive review
of 65 user studies evaluating XAI systems across different domains and
application contexts. As a guideline for XAI developers, we provide a holistic
overview of the properties of XAI systems and evaluation metrics focused on
human users (human-centered). We propose objectives for the human-centered
design (design goals) of XAI systems. To incorporate users' specific
characteristics, design goals are adapted to users with different levels of AI
expertise (AI novices and data experts). In this regard, we provide an
extension to existing XAI evaluation and design frameworks. The first part of
our results includes the analysis of XAI system characteristics. An important
finding is the distinction between the core system and the XAI explanation,
which together form the whole system. Further results include the distinction
of evaluation metrics into affection towards the system, cognition, usability,
interpretability, and explanation metrics. Furthermore, the users, along with
their specific characteristics and behavior, can be assessed. For AI novices,
the relevant extended design goals include responsible use, acceptance, and
usability. For data experts, the focus is performance-oriented and includes
human-AI collaboration and system and user task performance.

</details>


### [84] [GOAT: A Training Framework for Goal-Oriented Agent with Tools](https://arxiv.org/abs/2510.12218)
*Hyunji Min,Sangwon Jung,Junyoung Sung,Dosung Lee,Leekyeung Han,Paul Hongsuck Seo*

Main category: cs.AI

TL;DR: GOAT是一个无需人工标注的训练框架，能够自动构建目标导向的API执行任务数据集，通过微调使开源LLM代理在复杂工具使用方面达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理在处理需要分解高级目标为多个相互依赖API调用的目标导向查询时能力有限，开源模型尤其难以有效执行复杂工具使用，而缺乏训练数据导致主要依赖零样本评估。

Method: 提出GOAT训练框架，从给定的API文档自动构建目标导向API执行任务的合成数据集，使模型能够对相互依赖的调用进行推理并生成连贯响应。

Result: GOAT训练的代理在多个现有目标导向基准测试中达到最先进性能，并在新提出的GOATBench基准测试中也表现出色。

Conclusion: GOAT为构建能够进行复杂推理和工具使用的稳健开源LLM代理提供了一条实用路径。

Abstract: Large language models (LLMs) have recently been extended beyond traditional
text generation to serve as interactive agents capable of using external tools
based on user intent. However, current LLM agents still show limited ability to
handle goal-oriented queries, which require decomposing a high-level objective
into multiple interdependent API calls with correct planning and execution.
Current approaches mainly rely on zero-shot evaluation due to the absence of
training data. While proprietary closed-source models such as GPT-4 demonstrate
strong reasoning abilities, smaller open-source models struggle to perform
complex tool use effectively. Thus, we propose a novel training framework GOAT,
which enables fine-tuning of LLM agents in a human annotation-free setting.
GOAT automatically constructs synthetic datasets of goal-oriented API execution
tasks directly from given API documents, equipping models with the ability to
reason over interdependent calls and generate coherent responses. Through
extensive experiments, we show that GOAT-trained agents achieve
state-of-the-art performance across multiple existing goal-oriented benchmarks.
In addition, we introduce GOATBench, a new goal-oriented API execution
benchmark, and demonstrate that agents trained with GOAT also excel in this
setting. These results highlight GOAT as a practical path toward building
robust open-source LLM agents capable of complex reasoning and tool use.

</details>


### [85] [MedKGEval: A Knowledge Graph-Based Multi-Turn Evaluation Framework for Open-Ended Patient Interactions with Clinical LLMs](https://arxiv.org/abs/2510.12224)
*Yuechun Yu,Han Ying,Haoan Jin,Wenjian Jiang,Dong Xian,Binghao Wang,Zhou Yang,Mengyue Wu*

Main category: cs.AI

TL;DR: 提出了MedKGEval框架，这是一个基于结构化医学知识的多轮临床LLM评估系统，通过知识图谱驱动的患者模拟和实时评估机制来改进医学对话评估。


<details>
  <summary>Details</summary>
Motivation: 现有医学LLM评估方法主要依赖对话转录的事后审查，忽视了医学对话的动态性和患者信息需求的演变，无法捕捉真实临床环境中复杂的医患互动。

Method: 1) 知识图谱驱动的患者模拟机制，从精心构建的知识图谱中检索医学事实；2) 实时轮次评估框架，由法官代理评估每个模型响应的临床适当性、事实准确性和安全性；3) 构建包含8个最先进LLM的多轮基准测试。

Result: MedKGEval能够识别传统评估流程经常忽视的细微行为缺陷和安全风险，展示了其在医学LLM评估中的有效性。

Conclusion: 该框架虽然最初为中英文医学应用设计，但可通过切换输入知识图谱轻松扩展到其他语言，确保双语支持和领域特定适用性。

Abstract: The reliable evaluation of large language models (LLMs) in medical
applications remains an open challenge, particularly in capturing the
complexity of multi-turn doctor-patient interactions that unfold in real
clinical environments. Existing evaluation methods typically rely on post hoc
review of full conversation transcripts, thereby neglecting the dynamic,
context-sensitive nature of medical dialogues and the evolving informational
needs of patients. In this work, we present MedKGEval, a novel multi-turn
evaluation framework for clinical LLMs grounded in structured medical
knowledge. Our approach introduces three key contributions: (1) a knowledge
graph-driven patient simulation mechanism, where a dedicated control module
retrieves relevant medical facts from a curated knowledge graph, thereby
endowing the patient agent with human-like and realistic conversational
behavior. This knowledge graph is constructed by integrating open-source
resources with additional triples extracted from expert-annotated datasets; (2)
an in-situ, turn-level evaluation framework, where each model response is
assessed by a Judge Agent for clinical appropriateness, factual correctness,
and safety as the dialogue progresses using a suite of fine-grained,
task-specific metrics; (3) a comprehensive multi-turn benchmark of eight
state-of-the-art LLMs, demonstrating MedKGEval's ability to identify subtle
behavioral flaws and safety risks that are often overlooked by conventional
evaluation pipelines. Although initially designed for Chinese and English
medical applications, our framework can be readily extended to additional
languages by switching the input knowledge graphs, ensuring seamless bilingual
support and domain-specific applicability.

</details>


### [86] [PromptFlow: Training Prompts Like Neural Networks](https://arxiv.org/abs/2510.12246)
*Jingyi Wang,Hongyuan Zhu,Ye Niu,Yunhui Deng*

Main category: cs.AI

TL;DR: 提出了PromptFlow框架，这是一个受TensorFlow启发的模块化训练框架，通过元提示、操作符、优化器和评估器集成，使用基于梯度的元学习自动探索最优提示优化轨迹，并设计了强化学习方法来在提示工程过程中重用LLM经验。


<details>
  <summary>Details</summary>
Motivation: 解决当前自动提示工程方法存在的局限性：主要使用静态更新规则、缺乏动态策略选择机制、以粗粒度更新整个提示、以及在LLM中重用经验的问题尚未充分探索。

Method: 提出PromptFlow模块化框架，包含元提示、操作符、优化器和评估器组件，采用基于梯度的元学习自动探索提示优化轨迹，并设计强化学习方法在提示工程过程中重用LLM经验。

Result: 在多个数据集上进行广泛实验，证明了PromptFlow框架的有效性。

Conclusion: PromptFlow框架能够以最小的任务特定训练数据，自主探索最优提示优化轨迹，有效解决了当前自动提示工程方法的局限性。

Abstract: Large Language Models (LLMs) have demonstrated profound impact on Natural
Language Processing (NLP) tasks. However, their effective deployment across
diverse domains often require domain-specific adaptation strategies, as generic
models may underperform when faced with specialized data distributions. Recent
advances in prompt engineering (PE) offer a promising alternative to extensive
retraining by refining input instructions to align LLM outputs with task
objectives. This paradigm has emerged as a rapid and versatile approach for
model fine-tuning. Despite its potential, manual prompt design remains
labor-intensive and heavily depends on specialized expertise, often requiring
iterative human effort to achieve optimal formulations. To address this
limitation, automated prompt engineering methodologies have been developed to
systematically generate task-specific prompts. However, current implementations
predominantly employ static update rules and lack mechanisms for dynamic
strategy selection, resulting in suboptimal adaptation to varying NLP task
requirements. Furthermore, most methods treat and update the whole prompts at
each step, without considering editing prompt sections at a finer granularity.
At last, in particular, the problem of how to recycle experience in LLM is
still underexplored. To this end, we propose the PromptFlow, a modular training
framework inspired by TensorFlow, which integrates meta-prompts, operators,
optimization, and evaluator. Our framework can be equipped with the latest
optimization methods and autonomously explores optimal prompt refinement
trajectories through gradient-based meta-learning, requiring minimal
task-specific training data. Specifically, we devise a reinforcement learning
method to recycle experience for LLM in the PE process. Finally, we conduct
extensive experiments on various datasets, and demonstrate the effectiveness of
PromptFlow.

</details>


### [87] [$\mathbf{T^3}$: Reducing Belief Deviation in Reinforcement Learning for Active Reasoning](https://arxiv.org/abs/2510.12264)
*Deyu Zou,Yongqiang Chen,Jianxiang Wang,Haochen Yang,Mufei Li,James Cheng,Pan Li,Yu Gong*

Main category: cs.AI

TL;DR: 提出T³方法，通过检测和截断过度信念偏差的轨迹来改善LLM主动推理训练，提升策略优化效果


<details>
  <summary>Details</summary>
Motivation: LLM智能体在主动推理中存在信念偏差问题，难以正确建模信念、跟踪问题状态，导致陷入无信息或重复动作，影响强化学习训练效果

Method: 开发T³方法，跟踪模型信念偏差，检测过度偏差并截断训练轨迹，保留信息前缀的信用分配

Result: 在5个挑战性任务中，T³持续提升训练稳定性、token效率和最终性能，获得高达30%的性能增益，同时减少约25%的rollout token

Conclusion: 信念控制是开发鲁棒且可泛化LLM主动推理器的关键原则

Abstract: Active reasoning requires large language models (LLMs) to interact with
external sources and strategically gather information to solve problems.
Central to this process is belief tracking: maintaining a coherent
understanding of the problem state and the missing information toward the
solution. However, due to limited reasoning capabilities, LLM-based agents
often suffer from belief deviation: they struggle to correctly model beliefs,
lose track of problem states, and fall into uninformative or repetitive
actions. Once this happens, errors compound and reinforcement learning (RL)
training fails to properly credit the crucial exploratory steps. To address
this issue, we propose to track the deviation of model beliefs and develop
$\mathbf{T^3}$, a simple yet effective method that detects excessive belief
deviation and truncates trajectories during training to remove uninformative
tails. By preserving credit for informative prefixes, $\mathbf{T^3}$
systematically improves policy optimization. Across 5 challenging tasks,
$\mathbf{T^3}$ consistently enhances training stability, token efficiency, and
final performance, achieving up to 30% gains while cutting rollout tokens by
roughly 25%. These results highlight belief control as a key principle for
developing robust and generalizable LLM-based active reasoners.

</details>


### [88] [Tensor Logic: The Language of AI](https://arxiv.org/abs/2510.12269)
*Pedro Domingos*

Main category: cs.AI

TL;DR: 提出了张量逻辑（tensor logic）这一新编程语言，通过将神经和符号AI在基础层面统一起来，解决了现有AI工具库的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前AI发展受阻于缺乏具备所有必要特性的编程语言。PyTorch、TensorFlow等库缺乏自动推理和知识获取支持，而LISP、Prolog等AI语言又缺乏可扩展性和学习支持。

Method: 张量逻辑的唯一构造是张量方程，基于逻辑规则和爱因斯坦求和本质上是相同操作的观察，将所有其他内容都简化为它们。

Result: 优雅地实现了关键形式的神经、符号和统计AI，包括transformer、形式推理、核机器和图模型。最重要的是，它使新的研究方向成为可能，如在嵌入空间中进行可靠推理。

Conclusion: 张量逻辑结合了神经网络的可扩展性和可学习性与符号推理的可靠性和透明性，可能成为AI更广泛采用的基础。

Abstract: Progress in AI is hindered by the lack of a programming language with all the
requisite features. Libraries like PyTorch and TensorFlow provide automatic
differentiation and efficient GPU implementation, but are additions to Python,
which was never intended for AI. Their lack of support for automated reasoning
and knowledge acquisition has led to a long and costly series of hacky attempts
to tack them on. On the other hand, AI languages like LISP an Prolog lack
scalability and support for learning. This paper proposes tensor logic, a
language that solves these problems by unifying neural and symbolic AI at a
fundamental level. The sole construct in tensor logic is the tensor equation,
based on the observation that logical rules and Einstein summation are
essentially the same operation, and all else can be reduced to them. I show how
to elegantly implement key forms of neural, symbolic and statistical AI in
tensor logic, including transformers, formal reasoning, kernel machines and
graphical models. Most importantly, tensor logic makes new directions possible,
such as sound reasoning in embedding space. This combines the scalability and
learnability of neural networks with the reliability and transparency of
symbolic reasoning, and is potentially a basis for the wider adoption of AI.

</details>


### [89] [RAG-Anything: All-in-One RAG Framework](https://arxiv.org/abs/2510.12323)
*Zirui Guo,Xubin Ren,Lingrui Xu,Jiahao Zhang,Chao Huang*

Main category: cs.AI

TL;DR: RAG-Anything是一个统一的多模态检索增强生成框架，能够处理文本、图像、表格和数学表达式等多种模态内容，解决了现有RAG系统仅限于文本的局限性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的知识库本质上是多模态的，包含文本、视觉元素、结构化表格和数学表达式的丰富组合，但现有的RAG框架仅限于文本内容，在处理多模态文档时存在根本性差距。

Method: 将多模态内容重新概念化为相互关联的知识实体而非孤立的数据类型，引入双图构建来捕获跨模态关系和文本语义，开发结合结构知识导航和语义匹配的跨模态混合检索方法。

Result: 在具有挑战性的多模态基准测试中表现出优越性能，相比最先进方法有显著改进，特别是在传统方法失败的长文档上性能提升尤为明显。

Conclusion: 该框架为多模态知识访问建立了新范式，消除了限制当前系统的架构碎片化问题。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm
for expanding Large Language Models beyond their static training limitations.
However, a critical misalignment exists between current RAG capabilities and
real-world information environments. Modern knowledge repositories are
inherently multimodal, containing rich combinations of textual content, visual
elements, structured tables, and mathematical expressions. Yet existing RAG
frameworks are limited to textual content, creating fundamental gaps when
processing multimodal documents. We present RAG-Anything, a unified framework
that enables comprehensive knowledge retrieval across all modalities. Our
approach reconceptualizes multimodal content as interconnected knowledge
entities rather than isolated data types. The framework introduces dual-graph
construction to capture both cross-modal relationships and textual semantics
within a unified representation. We develop cross-modal hybrid retrieval that
combines structural knowledge navigation with semantic matching. This enables
effective reasoning over heterogeneous content where relevant evidence spans
multiple modalities. RAG-Anything demonstrates superior performance on
challenging multimodal benchmarks, achieving significant improvements over
state-of-the-art methods. Performance gains become particularly pronounced on
long documents where traditional approaches fail. Our framework establishes a
new paradigm for multimodal knowledge access, eliminating the architectural
fragmentation that constrains current systems. Our framework is open-sourced
at: https://github.com/HKUDS/RAG-Anything.

</details>


### [90] [O-Forge: An LLM + Computer Algebra Framework for Asymptotic Analysis](https://arxiv.org/abs/2510.12350)
*Ayush Khaitan,Vijay Ganesh*

Main category: cs.AI

TL;DR: LLM+CAS框架结合前沿大语言模型与计算机代数系统，通过符号反馈循环生成创造性且经过符号验证的证明，特别适用于渐近不等式证明中的域分解问题。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在数学研究中验证困难的问题，将AI从竞赛数学扩展到专业数学研究工具，特别是回答Terence Tao提出的关于LLM与验证器结合能否证明复杂渐近不等式的问题。

Method: 使用LLM+CAS框架和O-Forge工具，通过上下文符号反馈循环：LLM建议域分解，CAS对每个部分进行公理化验证，形成创造性证明与符号验证的结合。

Result: 该框架在渐近不等式证明中表现出色，能够有效提出合适的域分解，为专业数学家提供研究级工具。

Conclusion: LLM+CAS框架成功展示了AI在数学研究中的潜力，超越了竞赛数学范畴，为专业数学家提供了可信赖的研究工具。

Abstract: Large language models have recently demonstrated advanced capabilities in
solving IMO and Putnam problems; yet their role in research mathematics has
remained fairly limited. The key difficulty is verification: suggested proofs
may look plausible, but cannot be trusted without rigorous checking. We present
a framework, called LLM+CAS, and an associated tool, O-Forge, that couples
frontier LLMs with a computer algebra systems (CAS) in an In-Context Symbolic
Feedback loop to produce proofs that are both creative and symbolically
verified. Our focus is on asymptotic inequalities, a topic that often involves
difficult proofs and appropriate decomposition of the domain into the "right"
subdomains. Many mathematicians, including Terry Tao, have suggested that using
AI tools to find the right decompositions can be very useful for research-level
asymptotic analysis. In this paper, we show that our framework LLM+CAS turns
out to be remarkably effective at proposing such decompositions via a
combination of a frontier LLM and a CAS. More precisely, we use an LLM to
suggest domain decomposition, and a CAS (such as Mathematica) that provides a
verification of each piece axiomatically. Using this loop, we answer a question
posed by Terence Tao: whether LLMs coupled with a verifier can be used to help
prove intricate asymptotic inequalities. More broadly, we show how AI can move
beyond contest math towards research-level tools for professional
mathematicians.

</details>


### [91] [A Survey of Vibe Coding with Large Language Models](https://arxiv.org/abs/2510.12399)
*Yuyao Ge,Lingrui Mei,Zenghao Duan,Tianhao Li,Yujia Zheng,Yiwei Wang,Lexin Wang,Jiayu Yao,Tianyu Liu,Yujun Cai,Baolong Bi,Fangda Guo,Jiafeng Guo,Shenghua Liu,Xueqi Cheng*

Main category: cs.AI

TL;DR: 这篇论文对基于大语言模型的Vibe Coding范式进行了首次系统性综述，建立了理论框架和实践模型，揭示了成功Vibe Coding依赖于系统性的上下文工程、完善的开发环境和人机协作模式。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，编程范式正在从代码生成辅助转向自主编码代理，催生了"Vibe Coding"这一新兴开发方法。然而，这种范式的有效性仍未被充分探索，实证研究表明存在意外的生产力损失和人机协作挑战。

Method: 通过对1000多篇研究论文的系统分析，建立了Vibe Coding的理论基础，将其形式化为约束马尔可夫决策过程，并综合现有实践提出了五种开发模型：无约束自动化、迭代对话协作、规划驱动、测试驱动和上下文增强模型。

Result: 构建了Vibe Coding的完整生态系统框架，包括编码LLM、基于LLM的编码代理、编码代理开发环境和反馈机制等关键基础设施组件，并提供了该领域的首个全面分类法。

Conclusion: 成功的Vibe Coding不仅依赖于代理能力，更重要的是系统性的上下文工程、完善的开发环境以及人机协作开发模型的建立。

Abstract: The advancement of large language models (LLMs) has catalyzed a paradigm
shift from code generation assistance to autonomous coding agents, enabling a
novel development methodology termed "Vibe Coding" where developers validate
AI-generated implementations through outcome observation rather than
line-by-line code comprehension. Despite its transformative potential, the
effectiveness of this emergent paradigm remains under-explored, with empirical
evidence revealing unexpected productivity losses and fundamental challenges in
human-AI collaboration. To address this gap, this survey provides the first
comprehensive and systematic review of Vibe Coding with large language models,
establishing both theoretical foundations and practical frameworks for this
transformative development approach. Drawing from systematic analysis of over
1000 research papers, we survey the entire vibe coding ecosystem, examining
critical infrastructure components including LLMs for coding, LLM-based coding
agent, development environment of coding agent, and feedback mechanisms. We
first introduce Vibe Coding as a formal discipline by formalizing it through a
Constrained Markov Decision Process that captures the dynamic triadic
relationship among human developers, software projects, and coding agents.
Building upon this theoretical foundation, we then synthesize existing
practices into five distinct development models: Unconstrained Automation,
Iterative Conversational Collaboration, Planning-Driven, Test-Driven, and
Context-Enhanced Models, thus providing the first comprehensive taxonomy in
this domain. Critically, our analysis reveals that successful Vibe Coding
depends not merely on agent capabilities but on systematic context engineering,
well-established development environments, and human-agent collaborative
development models.

</details>


### [92] [PricingLogic: Evaluating LLMs Reasoning on Complex Tourism Pricing Tasks](https://arxiv.org/abs/2510.12409)
*Yunuo Liu,Dawei Zhu,Zena Al-Khalili,Dai Cheng,Yanjun Chen,Dietrich Klakow,Wei Zhang,Xiaoyu Shen*

Main category: cs.AI

TL;DR: PricingLogic是首个评估大语言模型在旅游定价任务中可靠性的基准测试，包含300个基于真实定价政策的问题，测试显示LLMs在复杂定价规则下表现显著下降。


<details>
  <summary>Details</summary>
Motivation: 旅行社希望将易出错的定价任务自动化，但未经验证的LLM部署可能导致重大财务损失和客户信任危机。

Method: 构建包含300个自然语言问题的基准测试，基于42个真实定价政策，涵盖两个难度级别：基础客户类型定价和涉及交互折扣的捆绑旅游计算。

Result: 对一系列LLMs的评估显示，在更难的层级上性能急剧下降，暴露了规则解释和算术推理的系统性失败。

Conclusion: 尽管LLMs具有通用能力，但在收入关键型应用中仍不可靠，需要进一步的安全保障或领域适应。

Abstract: We present PricingLogic, the first benchmark that probes whether Large
Language Models(LLMs) can reliably automate tourism-related prices when
multiple, overlapping fare rules apply. Travel agencies are eager to offload
this error-prone task onto AI systems; however, deploying LLMs without verified
reliability could result in significant financial losses and erode customer
trust. PricingLogic comprises 300 natural-language questions based on booking
requests derived from 42 real-world pricing policies, spanning two levels of
difficulty: (i) basic customer-type pricing and (ii)bundled-tour calculations
involving interacting discounts. Evaluations of a line of LLMs reveal a steep
performance drop on the harder tier,exposing systematic failures in rule
interpretation and arithmetic reasoning.These results highlight that, despite
their general capabilities, today's LLMs remain unreliable in revenue-critical
applications without further safeguards or domain adaptation. Our code and
dataset are available at https://github.com/EIT-NLP/PricingLogic.

</details>


### [93] [MTOS: A LLM-Driven Multi-topic Opinion Simulation Framework for Exploring Echo Chamber Dynamics](https://arxiv.org/abs/2510.12423)
*Dingyi Zuo,Hongjie Zhang,Jie Ou,Chaosheng Feng,Shuwan Liu*

Main category: cs.AI

TL;DR: 提出了MTOS框架，将多主题上下文与LLMs结合，通过短期和长期记忆、多种用户选择交互机制和动态主题选择策略，以及信念衰减机制，实现跨主题的观点更新。实验表明多主题设置显著改变极化趋势。


<details>
  <summary>Details</summary>
Motivation: 解决社交媒体中观点极化、信息隔离和认知偏见问题。现有研究多关注单一主题，难以捕捉多主题跨域情境中的认知转移；传统数值模型将复杂语言态度简化为离散值，缺乏解释性和行为一致性。

Method: 提出MTOS框架，整合LLMs与短期/长期记忆，采用多种用户选择交互机制和动态主题选择策略，引入信念衰减机制实现跨主题观点更新。

Result: 多主题设置显著改变极化趋势：正相关主题放大回音室效应，负相关主题抑制回音室，无关主题通过资源竞争也减轻回音室效应。相比数值模型，LLM代理能更真实模拟动态观点变化，再现新闻文本语言特征。

Conclusion: MTOS框架能够更真实地模拟动态观点变化，捕捉复杂的人类推理，提高模拟可解释性和系统稳定性，为多主题社交模拟提供了有效解决方案。

Abstract: The polarization of opinions, information segregation, and cognitive biases
on social media have attracted significant academic attention. In real-world
networks, information often spans multiple interrelated topics, posing
challenges for opinion evolution and highlighting the need for frameworks that
simulate interactions among topics. Existing studies based on large language
models (LLMs) focus largely on single topics, limiting the capture of cognitive
transfer in multi-topic, cross-domain contexts. Traditional numerical models,
meanwhile, simplify complex linguistic attitudes into discrete values, lacking
interpretability, behavioral consistency, and the ability to integrate multiple
topics. To address these issues, we propose Multi-topic Opinion Simulation
(MTOS), a social simulation framework integrating multi-topic contexts with
LLMs. MTOS leverages LLMs alongside short-term and long-term memory,
incorporates multiple user-selection interaction mechanisms and dynamic
topic-selection strategies, and employs a belief decay mechanism to enable
perspective updates across topics. We conduct extensive experiments on MTOS,
varying topic numbers, correlation types, and performing ablation studies to
assess features such as group polarization and local consistency. Results show
that multi-topic settings significantly alter polarization trends: positively
correlated topics amplify echo chambers, negatively correlated topics inhibit
them, and irrelevant topics also mitigate echo chamber effects through resource
competition. Compared with numerical models, LLM-based agents realistically
simulate dynamic opinion changes, reproduce linguistic features of news texts,
and capture complex human reasoning, improving simulation interpretability and
system stability.

</details>


### [94] [Biased-Attention Guided Risk Prediction for Safe Decision-Making at Unsignalized Intersections](https://arxiv.org/abs/2510.12428)
*Chengyang Dong,Nan Guo*

Main category: cs.AI

TL;DR: 提出了一种基于深度强化学习和偏置注意力机制的无信号交叉口自动驾驶决策框架，通过交通风险预测器评估碰撞风险并转化为密集奖励信号，提升安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 无信号交叉口的自动驾驶决策面临复杂动态交互和高冲突风险的挑战，需要实现主动安全控制。

Method: 基于Soft Actor-Critic算法，集成偏置注意力机制构建交通风险预测器，评估车辆进入交叉口的长期碰撞风险，并将其转化为密集奖励信号指导决策。

Result: 仿真结果表明，该方法有效提升了交叉口的交通效率和车辆安全性。

Conclusion: 该智能决策框架在复杂场景中具有有效性，证明了集成偏置注意力机制的DRL方法在无信号交叉口决策中的优势。

Abstract: Autonomous driving decision-making at unsignalized intersections is highly
challenging due to complex dynamic interactions and high conflict risks. To
achieve proactive safety control, this paper proposes a deep reinforcement
learning (DRL) decision-making framework integrated with a biased attention
mechanism. The framework is built upon the Soft Actor-Critic (SAC) algorithm.
Its core innovation lies in the use of biased attention to construct a traffic
risk predictor. This predictor assesses the long-term risk of collision for a
vehicle entering the intersection and transforms this risk into a dense reward
signal to guide the SAC agent in making safe and efficient driving decisions.
Finally, the simulation results demonstrate that the proposed method
effectively improves both traffic efficiency and vehicle safety at the
intersection, thereby proving the effectiveness of the intelligent
decision-making framework in complex scenarios. The code of our work is
available at https://github.com/hank111525/SAC-RWB.

</details>


### [95] [Evaluating and Mitigating LLM-as-a-judge Bias in Communication Systems](https://arxiv.org/abs/2510.12462)
*Jiaxin Gao,Chen Chen,Yanwen Jia,Xueluan Gong,Kwok-Yan Lam,Qian Wang*

Main category: cs.AI

TL;DR: 该论文系统研究了LLM作为评估模型时的判断偏见问题，发现了11种偏见类型，并提出了四种缓解策略来确保AI评估的公平性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM被越来越多地用于自主评估通信系统中的内容质量，这些AI"法官"的公正性无法保证，其评估标准中的任何偏见都可能扭曲结果并损害用户信任。

Method: 在点式评分设置下系统研究两种LLM作为评估模型（GPT-Judge和JudgeLM）的判断偏见，涵盖11种偏见类型，包括隐式和显式形式。

Result: 发现最先进的LLM法官对偏见输入具有鲁棒性，通常给偏见样本分配较低分数；提供详细评分标准能进一步增强鲁棒性；在偏见数据上微调LLM会显著降低其性能；判断分数与任务难度相关。

Conclusion: 提出了四种潜在的缓解策略，以确保在实际通信场景中实现公平可靠的AI评估。

Abstract: Large Language Models (LLMs) are increasingly being used to autonomously
evaluate the quality of content in communication systems, e.g., to assess
responses in telecom customer support chatbots. However, the impartiality of
these AI "judges" is not guaranteed, and any biases in their evaluation
criteria could skew outcomes and undermine user trust. In this paper, we
systematically investigate judgment biases in two LLM-as-a-judge models (i.e.,
GPT-Judge and JudgeLM) under the point-wise scoring setting, encompassing 11
types of biases that cover both implicit and explicit forms. We observed that
state-of-the-art LLM judges demonstrate robustness to biased inputs, generally
assigning them lower scores than the corresponding clean samples. Providing a
detailed scoring rubric further enhances this robustness. We further found that
fine-tuning an LLM on high-scoring yet biased responses can significantly
degrade its performance, highlighting the risk of training on biased data. We
also discovered that the judged scores correlate with task difficulty: a
challenging dataset like GPQA yields lower average scores, whereas an
open-ended reasoning dataset (e.g., JudgeLM-val) sees higher average scores.
Finally, we proposed four potential mitigation strategies to ensure fair and
reliable AI judging in practical communication scenarios.

</details>


### [96] [Using Medical Algorithms for Task-Oriented Dialogue in LLM-Based Medical Interviews](https://arxiv.org/abs/2510.12490)
*Rui Reis,Pedro Rangel Henriques,João Ferreira-Coimbra,Eva Oliveira,Nuno F. Rodrigues*

Main category: cs.AI

TL;DR: 开发了基于有向无环图的面向任务医疗对话框架，包含问题转换、冷启动、自适应分支、终止逻辑和报告生成五个核心模块，在两个应用中均表现出良好的可用性和用户满意度。


<details>
  <summary>Details</summary>
Motivation: 将医疗算法和指南转化为结构化对话系统，以支持临床决策并减轻医生认知负担，同时改善患者体验。

Method: 采用有向无环图结构，结合冷启动机制、扩展剪枝算法、终止逻辑和自动报告生成，基于人机交互原则设计患者和医生应用。

Result: 患者应用NASA-TLX=15.6(低负荷)、SUS=86(高可用性)、QUIS=8.1/9(高满意度)；医生应用NASA-TLX=26(中等负荷)、SUS=88.5(优秀可用性)、QUIS=8.3/9(高满意度)。

Conclusion: 该系统能有效集成到临床工作流中，降低认知需求并支持高效报告生成，但存在系统延迟和评估样本有限的问题。

Abstract: We developed a task-oriented dialogue framework structured as a Directed
Acyclic Graph (DAG) of medical questions. The system integrates: (1) a
systematic pipeline for transforming medical algorithms and guidelines into a
clinical question corpus; (2) a cold-start mechanism based on hierarchical
clustering to generate efficient initial questioning without prior patient
information; (3) an expand-and-prune mechanism enabling adaptive branching and
backtracking based on patient responses; (4) a termination logic to ensure
interviews end once sufficient information is gathered; and (5) automated
synthesis of doctor-friendly structured reports aligned with clinical
workflows. Human-computer interaction principles guided the design of both the
patient and physician applications. Preliminary evaluation involved five
physicians using standardized instruments: NASA-TLX (cognitive workload), the
System Usability Scale (SUS), and the Questionnaire for User Interface
Satisfaction (QUIS). The patient application achieved low workload scores
(NASA-TLX = 15.6), high usability (SUS = 86), and strong satisfaction (QUIS =
8.1/9), with particularly high ratings for ease of learning and interface
design. The physician application yielded moderate workload (NASA-TLX = 26) and
excellent usability (SUS = 88.5), with satisfaction scores of 8.3/9. Both
applications demonstrated effective integration into clinical workflows,
reducing cognitive demand and supporting efficient report generation.
Limitations included occasional system latency and a small, non-diverse
evaluation sample.

</details>


### [97] [Artificial Intelligence Virtual Cells: From Measurements to Decisions across Modality, Scale, Dynamics, and Evaluation](https://arxiv.org/abs/2510.12498)
*Chengpeng Hu,Calvin Yu-Chian Chen*

Main category: cs.AI

TL;DR: 提出细胞状态潜在(CSL)视角，通过操作符语法组织学习过程，强调决策对齐的评估框架，以解决当前AI虚拟细胞模型在跨实验室、跨平台迁移性、数据泄漏和跨尺度耦合方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前AI虚拟细胞模型虽然取得了进展，但在跨实验室和平台的可迁移性、数据泄漏和覆盖偏差、剂量时间组合效应处理、以及分子-细胞-组织尺度的耦合方面仍存在局限。

Method: 提出模型无关的细胞状态潜在(CSL)视角，采用操作符语法组织学习：测量、跨尺度耦合的lift/project操作、以及剂量和调度的干预操作。

Result: 建立了决策对齐的评估蓝图，涵盖模态、尺度、上下文和干预维度，强调功能空间读出如通路活性、空间邻域和临床相关终点。

Conclusion: 建议采用操作符感知的数据设计、抗泄漏分区以及透明的校准和报告，以实现可重复的同类比较。

Abstract: Artificial Intelligence Virtual Cells (AIVCs) aim to learn executable,
decision-relevant models of cell state from multimodal, multiscale
measurements. Recent studies have introduced single-cell and spatial foundation
models, improved cross-modality alignment, scaled perturbation atlases, and
explored pathway-level readouts. Nevertheless, although held-out validation is
standard practice, evaluations remain predominantly within single datasets and
settings; evidence indicates that transport across laboratories and platforms
is often limited, that some data splits are vulnerable to leakage and coverage
bias, and that dose, time and combination effects are not yet systematically
handled. Cross-scale coupling also remains constrained, as anchors linking
molecular, cellular and tissue levels are sparse, and alignment to scientific
or clinical readouts varies across studies. We propose a model-agnostic
Cell-State Latent (CSL) perspective that organizes learning via an operator
grammar: measurement, lift/project for cross-scale coupling, and intervention
for dosing and scheduling. This view motivates a decision-aligned evaluation
blueprint across modality, scale, context and intervention, and emphasizes
function-space readouts such as pathway activity, spatial neighborhoods and
clinically relevant endpoints. We recommend operator-aware data design,
leakage-resistant partitions, and transparent calibration and reporting to
enable reproducible, like-for-like comparisons.

</details>


### [98] [ProtoSiTex: Learning Semi-Interpretable Prototypes for Multi-label Text Classification](https://arxiv.org/abs/2510.12534)
*Utsav Kumar Nareti,Suraj Kumar,Soumya Pandey,Soumi Chattopadhyay,Chandranath Adak*

Main category: cs.AI

TL;DR: ProtoSiTex是一个半可解释框架，用于细粒度多标签文本分类，通过双阶段交替训练策略学习语义连贯且多样化的原型，并在多个基准测试中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 用户生成评论的激增需要可解释模型提供细粒度洞察。现有基于原型的模型在粗粒度（句子或文档级别）运行，无法处理现实世界文本分类的多标签性质。

Method: 采用双阶段交替训练策略：无监督原型发现阶段学习语义连贯且多样化的原型，监督分类阶段将这些原型映射到类别标签。使用分层损失函数在子句、句子和文档级别强制一致性，并利用自适应原型和多头注意力捕获重叠和冲突语义。

Result: 在酒店评论基准数据集和两个公共基准（二分类和多分类）上的实验表明，ProtoSiTex实现了最先进的性能，同时提供忠实、人类对齐的解释。

Conclusion: ProtoSiTex为半可解释多标签文本分类提供了一个强大的解决方案，在保持高性能的同时提供可解释性。

Abstract: The surge in user-generated reviews has amplified the need for interpretable
models that can provide fine-grained insights. Existing prototype-based models
offer intuitive explanations but typically operate at coarse granularity
(sentence or document level) and fail to address the multi-label nature of
real-world text classification. We propose ProtoSiTex, a semi-interpretable
framework designed for fine-grained multi-label text classification. ProtoSiTex
employs a dual-phase alternating training strategy: an unsupervised prototype
discovery phase that learns semantically coherent and diverse prototypes, and a
supervised classification phase that maps these prototypes to class labels. A
hierarchical loss function enforces consistency across sub-sentence, sentence,
and document levels, enhancing interpretability and alignment. Unlike prior
approaches, ProtoSiTex captures overlapping and conflicting semantics using
adaptive prototypes and multi-head attention. We also introduce a benchmark
dataset of hotel reviews annotated at the sub-sentence level with multiple
labels. Experiments on this dataset and two public benchmarks (binary and
multi-class) show that ProtoSiTex achieves state-of-the-art performance while
delivering faithful, human-aligned explanations, establishing it as a robust
solution for semi-interpretable multi-label text classification.

</details>


### [99] [Inclusive Fitness as a Key Step Towards More Advanced Social Behaviors in Multi-Agent Reinforcement Learning Settings](https://arxiv.org/abs/2510.12555)
*Andries Rosseau,Raphaël Avalos,Ann Nowé*

Main category: cs.AI

TL;DR: 提出基于包容性适应度的多智能体强化学习框架，通过基因型分配和遗传物质共享模拟自然选择过程，在囚徒困境网络游戏中验证了与汉密尔顿法则等生物学原理的一致性。


<details>
  <summary>Details</summary>
Motivation: 受自然选择中竞争与合作力量驱动智力演化的启发，旨在通过模拟生物进化过程来开发更具战略性和社会智能的智能体。

Method: 构建多智能体强化学习框架，每个智能体被分配基因型，奖励函数基于包容性适应度概念设计，考虑遗传物质共享，在囚徒困境网络游戏中研究社会动态。

Result: 研究结果与汉密尔顿法则等生物学原理一致，展示了基于遗传相似性的合作谱系，能够产生非团队基础的社会动态，如三方关系中复杂的合作与对抗模式。

Conclusion: 基于包容性适应度的奖励机制为更先进战略和社会智能智能体的涌现提供了基础，可扩展到具有时空结构、有限资源和演化种群的开放环境中，模拟生物进化式的多智能体自动课程学习。

Abstract: The competitive and cooperative forces of natural selection have driven the
evolution of intelligence for millions of years, culminating in nature's vast
biodiversity and the complexity of human minds. Inspired by this process, we
propose a novel multi-agent reinforcement learning framework where each agent
is assigned a genotype and where reward functions are modelled after the
concept of inclusive fitness. An agent's genetic material may be shared with
other agents, and our inclusive reward function naturally accounts for this. We
study the resulting social dynamics in two types of network games with
prisoner's dilemmas and find that our results align with well-established
principles from biology, such as Hamilton's rule. Furthermore, we outline how
this framework can extend to more open-ended environments with spatial and
temporal structure, finite resources, and evolving populations. We hypothesize
the emergence of an arms race of strategies, where each new strategy is a
gradual improvement over earlier adaptations of other agents, effectively
producing a multi-agent autocurriculum analogous to biological evolution. In
contrast to the binary team-based structures prevalent in earlier research, our
gene-based reward structure introduces a spectrum of cooperation ranging from
full adversity to full cooperativeness based on genetic similarity, enabling
unique non team-based social dynamics. For example, one agent having a mutual
cooperative relationship with two other agents, while the two other agents
behave adversarially towards each other. We argue that incorporating inclusive
fitness in agents provides a foundation for the emergence of more strategically
advanced and socially intelligent agents.

</details>


### [100] [HardcoreLogic: Challenging Large Reasoning Models with Long-tail Logic Puzzle Games](https://arxiv.org/abs/2510.12563)
*Jingcong Liang,Shijun Wan,Xuehai Wu,Siyuan Wang,Yitong Li,Qianglong Chen,Duyu Tang,Zhongyu Wei*

Main category: cs.AI

TL;DR: 提出了HardcoreLogic基准测试，包含5000多个谜题，通过增加复杂性、罕见元素和不可解谜题三个维度来测试大型推理模型的鲁棒性，揭示现有模型过度依赖记忆模式而非真正推理能力的问题。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型在复杂任务上表现优异，但面对非标准游戏变体时能否灵活应用规则仍存疑问。现有基准主要关注标准格式谜题，容易导致模型过拟合和记忆解决方案模式，掩盖了理解新规则和适应新变体的缺陷。

Method: 构建HardcoreLogic基准，包含10个游戏的5000多个谜题，通过三个维度系统性地转换标准谜题：增加复杂性(IC)、罕见元素(UE)和不可解谜题(UP)，减少对捷径记忆的依赖。

Result: 评估显示各种大型推理模型性能显著下降，即使在现有基准上得分最高的模型也表现不佳，表明它们严重依赖记忆的刻板模式。增加复杂性是主要困难来源，但模型在细微规则变化上也表现困难。

Conclusion: HardcoreLogic暴露了当前大型推理模型的局限性，为推进高级逻辑推理建立了基准，表明需要开发真正理解规则而非依赖记忆的推理能力。

Abstract: Large Reasoning Models (LRMs) have demonstrated impressive performance on
complex tasks, including logical puzzle games that require deriving solutions
satisfying all constraints. However, whether they can flexibly apply
appropriate rules to varying conditions, particularly when faced with
non-canonical game variants, remains an open question. Existing corpora focus
on popular puzzles like 9x9 Sudoku, risking overfitting to canonical formats
and memorization of solution patterns, which can mask deficiencies in
understanding novel rules or adapting strategies to new variants. To address
this, we introduce HardcoreLogic, a challenging benchmark of over 5,000 puzzles
across 10 games, designed to test the robustness of LRMs on the "long-tail" of
logical games. HardcoreLogic systematically transforms canonical puzzles
through three dimensions: Increased Complexity (IC), Uncommon Elements (UE),
and Unsolvable Puzzles (UP), reducing reliance on shortcut memorization.
Evaluations on a diverse set of LRMs reveal significant performance drops, even
for models achieving top scores on existing benchmarks, indicating heavy
reliance on memorized stereotypes. While increased complexity is the dominant
source of difficulty, models also struggle with subtle rule variations that do
not necessarily increase puzzle difficulty. Our systematic error analysis on
solvable and unsolvable puzzles further highlights gaps in genuine reasoning.
Overall, HardcoreLogic exposes the limitations of current LRMs and establishes
a benchmark for advancing high-level logical reasoning.

</details>


### [101] [Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks](https://arxiv.org/abs/2510.12635)
*Yuxiang Zhang,Jiangming Shu,Ye Ma,Xueyuan Lin,Shangxi Wu,Jitao Sang*

Main category: cs.AI

TL;DR: 提出Memory-as-Action框架，将工作内存管理重构为可学习的内部能力，通过强化学习训练智能体主动执行内存编辑操作，并开发Dynamic Context Policy Optimization算法解决轨迹断裂问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理长时程任务时，受限的内存容易被分散或无关上下文淹没。现有工作内存方法通常依赖与核心策略解耦的外部启发式机制。

Method: 将工作内存管理重构为统一策略中的显式编辑操作，提出Dynamic Context Policy Optimization算法，通过在内存操作点分割轨迹并应用轨迹级优势来解决轨迹断裂问题。

Result: 端到端联合优化任务推理和内存管理不仅减少了计算消耗，还通过针对模型内在能力的自适应上下文策展策略提高了任务性能。

Conclusion: 将内存管理作为可学习的内部能力，通过端到端强化学习联合优化任务推理和内存管理，能够有效提升长时程任务的处理效率和性能。

Abstract: Large Language Models face challenges in long-horizon agentic tasks as their
constrained memory is easily overwhelmed by distracting or irrelevant context.
Existing working memory methods typically rely on external, heuristic
mechanisms that are decoupled from the agent's core policy. In this work, we
reframe working memory management as a learnable, intrinsic capability. We
propose a novel framework, Memory-as-Action, where an agent actively manages
its working memory by executing explicit editing operations as part of a
unified policy. This formulation allows an agent, trained via reinforcement
learning, to balance memory curation against long-term task objectives under
given resource constraints. However, such memory editing actions break the
standard assumption of a continuously growing prefix in LLM interactions,
leading to what we call trajectory fractures. These non-prefix changes disrupt
the causal continuity required by standard policy gradient methods, making
those methods inapplicable. To address this, we propose a new algorithm,
Dynamic Context Policy Optimization, which enables stable end-to-end
reinforcement learning by segmenting trajectories at memory action points and
applying trajectory-level advantages to the resulting action segments. Our
results demonstrate that jointly optimizing for task reasoning and memory
management in an end-to-end fashion not only reduces overall computational
consumption but also improves task performance, driven by adaptive context
curation strategies tailored to the model's intrinsic capabilities.

</details>


### [102] [ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning](https://arxiv.org/abs/2510.12693)
*Hanyang Chen,Mark Zhao,Rui Yang,Qinwei Ma,Ke Yang,Jiarui Yao,Kangrui Wang,Hao Bai,Zhenhailong Wang,Rui Pan,Mengchao Zhang,Jose Barreiros,Aykut Onol,ChengXiang Zhai,Heng Ji,Manling Li,Huan Zhang,Tong Zhang*

Main category: cs.AI

TL;DR: ERA是一个两阶段框架，通过先验知识学习和在线强化学习，使小型视觉语言模型在具身AI任务中超越大型模型表现


<details>
  <summary>Details</summary>
Motivation: 解决高性能具身AI系统依赖昂贵大型模型，而小型模型缺乏必要知识和技能的问题

Method: 两阶段框架：1) 具身先验学习，从轨迹增强、环境锚定和外部知识三种数据中提取知识；2) 在线强化学习，采用自总结、密集奖励塑造和回合级策略优化

Result: ERA-3B在EB-ALFRED任务上比GPT-4o提升8.4%，在EB-Manipulation任务上提升19.4%，并在未见任务上表现出强泛化能力

Conclusion: ERA为可扩展的具身智能提供了实用路径，为未来具身AI系统提供了方法论见解

Abstract: Recent advances in embodied AI highlight the potential of vision language
models (VLMs) as agents capable of perception, reasoning, and interaction in
complex environments. However, top-performing systems rely on large-scale
models that are costly to deploy, while smaller VLMs lack the necessary
knowledge and skills to succeed. To bridge this gap, we present
\textit{Embodied Reasoning Agent (ERA)}, a two-stage framework that integrates
prior knowledge learning and online reinforcement learning (RL). The first
stage, \textit{Embodied Prior Learning}, distills foundational knowledge from
three types of data: (1) Trajectory-Augmented Priors, which enrich existing
trajectory data with structured reasoning generated by stronger models; (2)
Environment-Anchored Priors, which provide in-environment knowledge and
grounding supervision; and (3) External Knowledge Priors, which transfer
general knowledge from out-of-environment datasets. In the second stage, we
develop an online RL pipeline that builds on these priors to further enhance
agent performance. To overcome the inherent challenges in agent RL, including
long horizons, sparse rewards, and training instability, we introduce three key
designs: self-summarization for context management, dense reward shaping, and
turn-level policy optimization. Extensive experiments on both high-level
planning (EB-ALFRED) and low-level control (EB-Manipulation) tasks demonstrate
that ERA-3B surpasses both prompting-based large models and previous
training-based baselines. Specifically, it achieves overall improvements of
8.4\% on EB-ALFRED and 19.4\% on EB-Manipulation over GPT-4o, and exhibits
strong generalization to unseen tasks. Overall, ERA offers a practical path
toward scalable embodied intelligence, providing methodological insights for
future embodied AI systems.

</details>


### [103] [Multi-Agent Debate for LLM Judges with Adaptive Stability Detection](https://arxiv.org/abs/2510.12697)
*Tianyu Hu,Zhen Tan,Song Wang,Huaizhi Qu,Tianlong Chen*

Main category: cs.AI

TL;DR: 提出多智能体辩论法官框架，通过协作推理和迭代优化提升LLM自动判断任务的准确性，相比多数投票方法效果更好且保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为法官的方法依赖简单聚合方法（如多数投票），即使个体智能体提供正确答案也可能失败，需要更有效的协作判断机制。

Method: 建立多智能体辩论法官框架，智能体协作推理并迭代优化响应；引入稳定性检测机制，通过时变Beta-Binomial混合建模法官共识动态，基于分布相似性（KS检验）实现自适应停止。

Result: 在多个基准测试和模型上的实验表明，该框架相比多数投票提高了判断准确性，同时保持了计算效率。

Conclusion: 多智能体辩论框架能有效提升LLM自动判断任务的性能，通过数学建模和自适应机制实现了准确性与效率的平衡。

Abstract: With advancements in reasoning capabilities, Large Language Models (LLMs) are
increasingly employed for automated judgment tasks. While LLMs-as-Judges offer
promise in automating evaluations, current approaches often rely on simplistic
aggregation methods (e.g., majority voting), which can fail even when
individual agents provide correct answers. To address this, we propose a
multi-agent debate judge framework where agents collaboratively reason and
iteratively refine their responses. We formalize the debate process
mathematically, analyzing agent interactions and proving that debate amplifies
correctness compared to static ensembles. To enhance efficiency, we introduce a
stability detection mechanism that models judge consensus dynamics via a
time-varying Beta-Binomial mixture, with adaptive stopping based on
distributional similarity (Kolmogorov-Smirnov test). This mechanism models the
judges' collective correct rate dynamics using a time-varying mixture of
Beta-Binomial distributions and employs an adaptive stopping criterion based on
distributional similarity (Kolmogorov-Smirnov statistic). Experiments across
multiple benchmarks and models demonstrate that our framework improves judgment
accuracy over majority voting while maintaining computational efficiency.

</details>


### [104] [CAMNet: Leveraging Cooperative Awareness Messages for Vehicle Trajectory Prediction](https://arxiv.org/abs/2510.12703)
*Mattia Grasselli,Angelo Porrello,Carlo Augusto Grazia*

Main category: cs.AI

TL;DR: 本文研究了使用车辆间通信数据（CAM）进行车辆轨迹预测的方法，开发了CAMNet神经网络模型，证明了CAM数据在轨迹预测中的有效性，同时也指出了方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶面临安全挑战，现有传感器存在视野遮挡问题。车辆间通信（特别是CAM消息）可以共享信息，提高情境感知能力，即使传感器被遮挡时也能保持车辆间的相互感知。

Method: 设计并训练了基于合作感知消息的图神经网络（CAMNet），在广泛使用的运动预测数据集上进行训练，并在专门创建的CAM数据集上进行评估。

Result: 方法显示出有希望的结果，证明CAM数据确实可以支持车辆轨迹预测。

Conclusion: CAM数据在车辆轨迹预测中具有应用价值，但该方法存在一些局限性，为未来研究提供了机会。

Abstract: Autonomous driving remains a challenging task, particularly due to safety
concerns. Modern vehicles are typically equipped with expensive sensors such as
LiDAR, cameras, and radars to reduce the risk of accidents. However, these
sensors face inherent limitations: their field of view and line of sight can be
obstructed by other vehicles, thereby reducing situational awareness. In this
context, vehicle-to-vehicle communication plays a crucial role, as it enables
cars to share information and remain aware of each other even when sensors are
occluded. One way to achieve this is through the use of Cooperative Awareness
Messages (CAMs). In this paper, we investigate the use of CAM data for vehicle
trajectory prediction. Specifically, we design and train a neural network,
Cooperative Awareness Message-based Graph Neural Network (CAMNet), on a widely
used motion forecasting dataset. We then evaluate the model on a second dataset
that we created from scratch using Cooperative Awareness Messages, in order to
assess whether this type of data can be effectively exploited. Our approach
demonstrates promising results, showing that CAMs can indeed support vehicle
trajectory prediction. At the same time, we discuss several limitations of the
approach, which highlight opportunities for future research.

</details>


### [105] [Towards Robust Artificial Intelligence: Self-Supervised Learning Approach for Out-of-Distribution Detection](https://arxiv.org/abs/2510.12713)
*Wissam Salhab,Darine Ameyed,Hamid Mcheick,Fehmi Jaafar*

Main category: cs.AI

TL;DR: 提出一种无需标记数据即可改进OOD检测的方法，通过自监督学习和图论技术提高AI系统鲁棒性，在AUROC指标上达到0.99。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶、交通和医疗等安全关键系统中，AI系统需要能够在各种条件下保持可靠性能，包括处理分布外样本、对抗攻击和环境变化。

Method: 结合自监督学习和图论技术，从无标记数据中学习有用表示，更有效地识别和分类OOD样本。

Result: 与现有最先进方法相比，该方法在AUROC指标上达到0.99的优异性能。

Conclusion: 该方法能够显著提高AI系统的鲁棒性，特别是在处理分布外样本方面表现出色，且无需依赖标记数据。

Abstract: Robustness in AI systems refers to their ability to maintain reliable and
accurate performance under various conditions, including out-of-distribution
(OOD) samples, adversarial attacks, and environmental changes. This is crucial
in safety-critical systems, such as autonomous vehicles, transportation, or
healthcare, where malfunctions could have severe consequences. This paper
proposes an approach to improve OOD detection without the need of labeled data,
thereby increasing the AI systems' robustness. The proposed approach leverages
the principles of self-supervised learning, allowing the model to learn useful
representations from unlabeled data. Combined with graph-theoretical
techniques, this enables the more efficient identification and categorization
of OOD samples. Compared to existing state-of-the-art methods, this approach
achieved an Area Under the Receiver Operating Characteristic Curve (AUROC) =
0.99.

</details>


### [106] [Clutch Control: An Attention-based Combinatorial Bandit for Efficient Mutation in JavaScript Engine Fuzzing](https://arxiv.org/abs/2510.12732)
*Myles Foley,Sergio Maffeis,Muhammad Fakhrur Rozi,Takeshi Takahashi*

Main category: cs.AI

TL;DR: CLUTCH是一种基于深度组合多臂老虎机的新型JavaScript模糊测试方法，通过注意力机制和Concrete Dropout动态选择变异目标，相比现有方法提高了测试用例有效性和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有JavaScript模糊测试方法使用随机选择变异目标，效率不高。作者认为选择更好的变异目标适合使用可变臂数的组合多臂老虎机方法来解决。

Method: 提出CLUTCH深度组合多臂老虎机，使用注意力机制处理可变长度的JavaScript测试用例表示，并通过Concrete Dropout动态调整探索策略。

Result: CLUTCH相比三种最先进解决方案，平均增加有效测试用例数量20.3%，每个测试用例覆盖率增加8.9%。在可变和组合设置下，后悔值分别减少至少78.1%和4.1%。

Conclusion: CLUTCH在JavaScript模糊测试中显著提高了效率，证明了深度组合多臂老虎机在可变臂数设置下的有效性。

Abstract: JavaScript engines are widely used in web browsers, PDF readers, and
server-side applications. The rise in concern over their security has led to
the development of several targeted fuzzing techniques. However, existing
approaches use random selection to determine where to perform mutations in
JavaScript code. We postulate that the problem of selecting better mutation
targets is suitable for combinatorial bandits with a volatile number of arms.
Thus, we propose CLUTCH, a novel deep combinatorial bandit that can observe
variable length JavaScript test case representations, using an attention
mechanism from deep learning. Furthermore, using Concrete Dropout, CLUTCH can
dynamically adapt its exploration. We show that CLUTCH increases efficiency in
JavaScript fuzzing compared to three state-of-the-art solutions by increasing
the number of valid test cases and coverage-per-testcase by, respectively,
20.3% and 8.9% on average. In volatile and combinatorial settings we show that
CLUTCH outperforms state-of-the-art bandits, achieving at least 78.1% and 4.1%
less regret in volatile and combinatorial settings, respectively.

</details>


### [107] [CTRL-Rec: Controlling Recommender Systems With Natural Language](https://arxiv.org/abs/2510.12742)
*Micah Carroll,Adeline Foote,Kevin Feng,Marcus Williams,Anca Dragan,W. Bradley Knox,Smitha Milli*

Main category: cs.AI

TL;DR: CTRL-Rec是一种允许用户通过自然语言实时控制推荐系统的方法，通过LLM模拟用户偏好并训练嵌入模型，实现细粒度推荐控制。


<details>
  <summary>Details</summary>
Motivation: 当用户对推荐系统不满意时，他们通常缺乏细粒度的控制手段来改变推荐结果。

Method: 在训练时使用LLM模拟用户基于语言请求的项目批准情况，训练嵌入模型来近似这种模拟判断，并将基于用户请求的预测集成到传统推荐系统的信号权重中。

Result: 在MovieLens数据集实验中，该方法在多样化请求下实现了细粒度控制。在19名Letterboxd用户研究中，CTRL-Rec显著提升了用户的控制感和满意度。

Conclusion: CTRL-Rec方法能够有效实现自然语言控制的实时推荐，提升用户体验和控制感。

Abstract: When users are dissatisfied with recommendations from a recommender system,
they often lack fine-grained controls for changing them. Large language models
(LLMs) offer a solution by allowing users to guide their recommendations
through natural language requests (e.g., "I want to see respectful posts with a
different perspective than mine"). We propose a method, CTRL-Rec, that allows
for natural language control of traditional recommender systems in real-time
with computational efficiency. Specifically, at training time, we use an LLM to
simulate whether users would approve of items based on their language requests,
and we train embedding models that approximate such simulated judgments. We
then integrate these user-request-based predictions into the standard weighting
of signals that traditional recommender systems optimize. At deployment time,
we require only a single LLM embedding computation per user request, allowing
for real-time control of recommendations. In experiments with the MovieLens
dataset, our method consistently allows for fine-grained control across a
diversity of requests. In a study with 19 Letterboxd users, we find that
CTRL-Rec was positively received by users and significantly enhanced users'
sense of control and satisfaction with recommendations compared to traditional
controls.

</details>


### [108] [Ax-Prover: A Deep Reasoning Agentic Framework for Theorem Proving in Mathematics and Quantum Physics](https://arxiv.org/abs/2510.12787)
*Marco Del Tredici,Jacob McCarran,Benjamin Breen,Javier Aspuru Mijares,Weichen Winston Yin,Jacob M. Taylor,Frank Koppens,Dirk Englund*

Main category: cs.AI

TL;DR: Ax-Prover是一个基于多智能体系统的自动定理证明器，使用Lean语言，能够跨领域解决科学问题，支持自主或与人类专家协作工作模式。


<details>
  <summary>Details</summary>
Motivation: 为了解决科学问题求解中需要创造性推理和严格语法严谨性的挑战，将大型语言模型的知识推理能力与Lean工具的正式正确性保障相结合。

Method: 通过模型上下文协议(MCP)为LLMs配备Lean工具，构建多智能体系统，在形式证明生成过程中结合知识推理和形式验证。

Result: 在公开数学基准测试中与最先进的证明器竞争，在新引入的抽象代数和量子理论基准测试中大幅超越其他模型，展示了良好的泛化能力。

Conclusion: 基于工具的智能体定理证明方法为跨领域形式验证提供了可泛化的方法论，并在实际应用中成功协助专家数学家形式化复杂密码学定理证明。

Abstract: We present Ax-Prover, a multi-agent system for automated theorem proving in
Lean that can solve problems across diverse scientific domains and operate
either autonomously or collaboratively with human experts. To achieve this,
Ax-Prover approaches scientific problem solving through formal proof
generation, a process that demands both creative reasoning and strict syntactic
rigor. Ax-Prover meets this challenge by equipping Large Language Models
(LLMs), which provide knowledge and reasoning, with Lean tools via the Model
Context Protocol (MCP), which ensure formal correctness. To evaluate its
performance as an autonomous prover, we benchmark our approach against frontier
LLMs and specialized prover models on two public math benchmarks and on two
Lean benchmarks we introduce in the fields of abstract algebra and quantum
theory. On public datasets, Ax-Prover is competitive with state-of-the-art
provers, while it largely outperform them on the new benchmarks. This shows
that, unlike specialized systems that struggle to generalize, our tool-based
agentic theorem prover approach offers a generalizable methodology for formal
verification across diverse scientific domains. Furthermore, we demonstrate
Ax-Prover's assistant capabilities in a practical use case, showing how it
enabled an expert mathematician to formalize the proof of a complex
cryptography theorem.

</details>
