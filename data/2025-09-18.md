<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 39]
- [cs.AI](#cs.AI) [Total: 25]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Gender-Neutral Rewriting in Italian: Models, Approaches, and Trade-offs](https://arxiv.org/abs/2509.13480)
*Andrea Piergentili,Beatrice Savoldi,Matteo Negri,Luisa Bentivogli*

Main category: cs.CL

TL;DR: 本文首次系统评估了大型语言模型在意大利语性别中性改写任务中的表现，提出了一个衡量中立性和语义保真度的二维框架，发现开源模型优于现有专用模型，微调后的小模型性能可媲美大型模型。


<details>
  <summary>Details</summary>
Motivation: 意大利语等语法性别语言中的性别中性改写具有挑战性，需要消除不必要的性别指定同时保持语义。目前缺乏对该任务在意大利语中的系统评估。

Method: 采用少样本提示比较多个LLM，对选定模型进行微调，应用针对性清洗提升任务相关性，使用二维框架评估中立性和语义保真度。

Result: 开源权重LLM优于现有意大利语GNR专用模型，微调后的小型模型以更小规模达到或超过最佳开源LLM性能。

Conclusion: 研究揭示了在优化训练数据时中立性和意义保持之间的权衡关系，为语法性别语言的性别中性改写提供了有效解决方案。

Abstract: Gender-neutral rewriting (GNR) aims to reformulate text to eliminate
unnecessary gender specifications while preserving meaning, a particularly
challenging task in grammatical-gender languages like Italian. In this work, we
conduct the first systematic evaluation of state-of-the-art large language
models (LLMs) for Italian GNR, introducing a two-dimensional framework that
measures both neutrality and semantic fidelity to the input. We compare
few-shot prompting across multiple LLMs, fine-tune selected models, and apply
targeted cleaning to boost task relevance. Our findings show that open-weight
LLMs outperform the only existing model dedicated to GNR in Italian, whereas
our fine-tuned models match or exceed the best open-weight LLM's performance at
a fraction of its size. Finally, we discuss the trade-off between optimizing
the training data for neutrality and meaning preservation.

</details>


### [2] [Op-Fed: Opinion, Stance, and Monetary Policy Annotations on FOMC Transcripts Using Active Learning](https://arxiv.org/abs/2509.13539)
*Alisa Kanganis,Katherine A. Keith*

Main category: cs.CL

TL;DR: Op-Fed数据集：包含1044个人工标注的FOMC会议记录句子，用于货币政策立场分析，解决了类别不平衡和句子间依赖的技术挑战。


<details>
  <summary>Details</summary>
Motivation: 美联储公开市场委员会(FOMC)的货币政策决策影响数百万人，但现有数据缺乏对货币政策立场的细粒度标注，需要构建高质量数据集来支持相关研究。

Method: 采用五阶段分层标注方案分离观点、货币政策和立场要素；使用主动学习选择标注实例，将正例数量翻倍；评估LLM在零样本下的分类性能。

Result: 顶级闭源LLM在观点分类上达到0.80准确率，但在货币政策立场分类上仅0.61，低于人类基线0.89；数据集包含65%需要上下文理解的实例。

Conclusion: Op-Fed数据集为模型训练、置信度校准和未来标注工作提供了高质量种子数据，揭示了LLM在复杂金融文本理解方面的局限性。

Abstract: The U.S. Federal Open Market Committee (FOMC) regularly discusses and sets
monetary policy, affecting the borrowing and spending decisions of millions of
people. In this work, we release Op-Fed, a dataset of 1044 human-annotated
sentences and their contexts from FOMC transcripts. We faced two major
technical challenges in dataset creation: imbalanced classes -- we estimate
fewer than 8% of sentences express a non-neutral stance towards monetary policy
-- and inter-sentence dependence -- 65% of instances require context beyond the
sentence-level. To address these challenges, we developed a five-stage
hierarchical schema to isolate aspects of opinion, monetary policy, and stance
towards monetary policy as well as the level of context needed. Second, we
selected instances to annotate using active learning, roughly doubling the
number of positive instances across all schema aspects. Using Op-Fed, we found
a top-performing, closed-weight LLM achieves 0.80 zero-shot accuracy in opinion
classification but only 0.61 zero-shot accuracy classifying stance towards
monetary policy -- below our human baseline of 0.89. We expect Op-Fed to be
useful for future model training, confidence calibration, and as a seed dataset
for future annotation efforts.

</details>


### [3] [Overview of Dialog System Evaluation Track: Dimensionality, Language, Culture and Safety at DSTC 12](https://arxiv.org/abs/2509.13569)
*John Mendonça,Lining Zhang,Rahul Mallidi,Alon Lavie,Isabel Trancoso,Luis Fernando D'Haro,João Sedoc*

Main category: cs.CL

TL;DR: DSTC12 Track 1 针对对话系统评估的两个关键挑战：多维度自动评估指标和多语言文化安全检测，结果显示现有方法在文化意识安全方面仍有显著改进空间


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的快速发展凸显了对话系统评估的重要性，但传统指标不足且安全考虑往往定义狭窄或存在文化偏见，需要更全面的评估方法

Method: 通过两个子任务进行评估：(1) 对话级多维度自动评估指标（10个对话维度），(2) 多语言和多文化安全检测，使用Llama-3-8B和Llama-Guard-3-1B作为基线模型

Result: 任务1中Llama-3-8B基线获得最高平均Spearman相关系数0.1681，显示仍有很大改进空间；任务2中参赛团队在多语言安全子集上显著优于基线（最佳ROC-AUC 0.9648），但基线在文化子集上表现更好（0.5126 ROC-AUC）

Conclusion: 该研究揭示了当前对话系统评估在文化意识安全方面的关键需求，强调了开发更全面、文化敏感的评估方法的必要性，为未来研究提供了重要基准和方向

Abstract: The rapid advancement of Large Language Models (LLMs) has intensified the
need for robust dialogue system evaluation, yet comprehensive assessment
remains challenging. Traditional metrics often prove insufficient, and safety
considerations are frequently narrowly defined or culturally biased. The DSTC12
Track 1, "Dialog System Evaluation: Dimensionality, Language, Culture and
Safety," is part of the ongoing effort to address these critical gaps. The
track comprised two subtasks: (1) Dialogue-level, Multi-dimensional Automatic
Evaluation Metrics, and (2) Multilingual and Multicultural Safety Detection.
For Task 1, focused on 10 dialogue dimensions, a Llama-3-8B baseline achieved
the highest average Spearman's correlation (0.1681), indicating substantial
room for improvement. In Task 2, while participating teams significantly
outperformed a Llama-Guard-3-1B baseline on the multilingual safety subset (top
ROC-AUC 0.9648), the baseline proved superior on the cultural subset (0.5126
ROC-AUC), highlighting critical needs in culturally-aware safety. This paper
describes the datasets and baselines provided to participants, as well as
submission evaluation results for each of the two proposed subtasks.

</details>


### [4] [Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning](https://arxiv.org/abs/2509.13624)
*Shambhavi Krishna,Atharva Naik,Chaitali Agarwal,Sudharshan Govindan,Taesung Lee,Haw-Shiuan Chang*

Main category: cs.CL

TL;DR: 论文提出了一个分析框架来研究LLM在不同任务间的迁移学习效果，发现性能提升主要取决于源数据集的隐藏统计特征而非表面相似性


<details>
  <summary>Details</summary>
Motivation: 由于无法为所有任务获取高质量训练数据，需要依赖迁移学习来处理分布外请求，但现有方法难以解释跨任务交互的复杂动态

Method: 构建迁移学习矩阵和降维分析框架，训练10个模型来识别潜在能力（推理、情感分类、自然语言理解、算术等），分析跨任务交互的副作用

Result: 性能改进往往无法用表面数据集相似性或源数据质量来解释，而是由源数据集的隐藏统计因素（如类别分布、生成长度倾向）和特定语言特征主导

Conclusion: 这项工作揭示了迁移学习的复杂动态，为更可预测和有效的LLM适应铺平了道路，强调了隐藏统计特征在跨任务迁移中的关键作用

Abstract: Large language models are increasingly deployed across diverse applications.
This often includes tasks LLMs have not encountered during training. This
implies that enumerating and obtaining the high-quality training data for all
tasks is infeasible. Thus, we often need to rely on transfer learning using
datasets with different characteristics, and anticipate out-of-distribution
requests. Motivated by this practical need, we propose an analysis framework,
building a transfer learning matrix and dimensionality reduction, to dissect
these cross-task interactions. We train and analyze 10 models to identify
latent abilities (e.g., Reasoning, Sentiment Classification, NLU, Arithmetic)
and discover the side effects of the transfer learning. Our findings reveal
that performance improvements often defy explanations based on surface-level
dataset similarity or source data quality. Instead, hidden statistical factors
of the source dataset, such as class distribution and generation length
proclivities, alongside specific linguistic features, are actually more
influential. This work offers insights into the complex dynamics of transfer
learning, paving the way for more predictable and effective LLM adaptation.

</details>


### [5] [Sparse Neurons Carry Strong Signals of Question Ambiguity in LLMs](https://arxiv.org/abs/2509.13664)
*Zhuoxuan Zhang,Jinhao Duan,Edward Kim,Kaidi Xu*

Main category: cs.CL

TL;DR: 研究发现LLMs在内部表征中线性编码问题歧义性，通过识别和操控歧义编码神经元（AENs）可以检测和控制模型行为，从直接回答转向弃权


<details>
  <summary>Details</summary>
Motivation: 现实问题普遍存在歧义性，但大语言模型往往给出自信回答而非寻求澄清，需要理解模型内部如何处理歧义信息

Method: 在模型预填充阶段识别编码问题歧义性的神经元（AENs），训练探针进行歧义检测，并通过神经元操控控制模型行为

Result: 发现少量神经元（甚至一个）就能编码歧义信息，AENs探针在歧义检测上表现优异且具有跨数据集泛化能力，层析分析显示歧义信号在浅层编码

Conclusion: LLMs形成了紧凑的问题歧义内部表征，使得模型行为具有可解释性和可控性，为构建更可靠的AI系统提供了新途径

Abstract: Ambiguity is pervasive in real-world questions, yet large language models
(LLMs) often respond with confident answers rather than seeking clarification.
In this work, we show that question ambiguity is linearly encoded in the
internal representations of LLMs and can be both detected and controlled at the
neuron level. During the model's pre-filling stage, we identify that a small
number of neurons, as few as one, encode question ambiguity information. Probes
trained on these Ambiguity-Encoding Neurons (AENs) achieve strong performance
on ambiguity detection and generalize across datasets, outperforming
prompting-based and representation-based baselines. Layerwise analysis reveals
that AENs emerge from shallow layers, suggesting early encoding of ambiguity
signals in the model's processing pipeline. Finally, we show that through
manipulating AENs, we can control LLM's behavior from direct answering to
abstention. Our findings reveal that LLMs form compact internal representations
of question ambiguity, enabling interpretable and controllable behavior.

</details>


### [6] [CL$^2$GEC: A Multi-Discipline Benchmark for Continual Learning in Chinese Literature Grammatical Error Correction](https://arxiv.org/abs/2509.13672)
*Shang Qin,Jingheng Ye,Yinghui Li,Hai-Tao Zheng,Qi Li,Jinxiao Shan,Zhixing Li,Hong-Gee Kim*

Main category: cs.CL

TL;DR: 提出了第一个中文文献语法纠错的持续学习基准CL²GEC，包含10个学科的10,000条人工标注句子，评估大语言模型在跨学科学术写作中的自适应语法纠错能力。


<details>
  <summary>Details</summary>
Motivation: 现有中文语法纠错研究缺乏多学科学术写作的专用基准，忽视了持续学习作为处理领域特定语言变异和防止灾难性遗忘的有效解决方案。

Method: 构建包含10个学科10,000句的标注数据集，在持续学习设置下评估大语言模型，包括顺序调优、参数高效适应和四种代表性持续学习算法。

Result: 实验结果表明，基于正则化的方法比基于回放或简单顺序方法更能有效缓解遗忘问题。

Conclusion: 该基准为跨学科学术领域的自适应语法纠错研究提供了严谨的基础，展示了持续学习在学术写作辅助中的潜力。

Abstract: The growing demand for automated writing assistance in diverse academic
domains highlights the need for robust Chinese Grammatical Error Correction
(CGEC) systems that can adapt across disciplines. However, existing CGEC
research largely lacks dedicated benchmarks for multi-disciplinary academic
writing, overlooking continual learning (CL) as a promising solution to handle
domain-specific linguistic variation and prevent catastrophic forgetting. To
fill this crucial gap, we introduce CL$^2$GEC, the first Continual Learning
benchmark for Chinese Literature Grammatical Error Correction, designed to
evaluate adaptive CGEC across multiple academic fields. Our benchmark includes
10,000 human-annotated sentences spanning 10 disciplines, each exhibiting
distinct linguistic styles and error patterns. CL$^2$GEC focuses on evaluating
grammatical error correction in a continual learning setting, simulating
sequential exposure to diverse academic disciplines to reflect real-world
editorial dynamics. We evaluate large language models under sequential tuning,
parameter-efficient adaptation, and four representative CL algorithms, using
both standard GEC metrics and continual learning metrics adapted to task-level
variation. Experimental results reveal that regularization-based methods
mitigate forgetting more effectively than replay-based or naive sequential
approaches. Our benchmark provides a rigorous foundation for future research in
adaptive grammatical error correction across diverse academic domains.

</details>


### [7] [AgentCTG: Harnessing Multi-Agent Collaboration for Fine-Grained Precise Control in Text Generation](https://arxiv.org/abs/2509.13677)
*Xinxu Zhou,Jiaqi Bai,Zhenqi Sun,Fanxiang Zeng,Yue Liu*

Main category: cs.CL

TL;DR: AgentCTG是一个新颖的可扩展框架，通过模拟多智能体工作流中的控制和调节机制，实现对文本生成的精确复杂控制，在多个公开数据集上达到最先进效果。


<details>
  <summary>Details</summary>
Motivation: 自然语言处理中的受控文本生成面临精细条件控制的挑战，特别是在实际在线应用中需要考虑成本、可扩展性、领域知识学习和更精确控制等多重需求。

Method: 提出AgentCTG框架，通过多智能体协作机制模拟控制和调节，探索不同智能体间的协作方法，并引入自动提示模块来增强生成效果。

Result: 在多个公开数据集上取得最先进结果，在角色驱动重写任务中成功将原始文本转换为符合特定角色配置的新文本，同时保留领域知识，在在线导航角色扮演中显著提升驾驶体验。

Conclusion: AgentCTG通过多智能体协作机制有效解决了受控文本生成的精细控制问题，为在线应用提供了更沉浸式的交互体验，促进了个性化和用户参与度。

Abstract: Although significant progress has been made in many tasks within the field of
Natural Language Processing (NLP), Controlled Text Generation (CTG) continues
to face numerous challenges, particularly in achieving fine-grained conditional
control over generation. Additionally, in real scenario and online
applications, cost considerations, scalability, domain knowledge learning and
more precise control are required, presenting more challenge for CTG. This
paper introduces a novel and scalable framework, AgentCTG, which aims to
enhance precise and complex control over the text generation by simulating the
control and regulation mechanisms in multi-agent workflows. We explore various
collaboration methods among different agents and introduce an auto-prompt
module to further enhance the generation effectiveness. AgentCTG achieves
state-of-the-art results on multiple public datasets. To validate its
effectiveness in practical applications, we propose a new challenging
Character-Driven Rewriting task, which aims to convert the original text into
new text that conform to specific character profiles and simultaneously
preserve the domain knowledge. When applied to online navigation with
role-playing, our approach significantly enhances the driving experience
through improved content delivery. By optimizing the generation of contextually
relevant text, we enable a more immersive interaction within online
communities, fostering greater personalization and user engagement.

</details>


### [8] [Improving Context Fidelity via Native Retrieval-Augmented Reasoning](https://arxiv.org/abs/2509.13683)
*Suyuchen Wang,Jinlin Wang,Xinyu Wang,Shiqi Li,Xiangru Tang,Sirui Hong,Xiao-Wen Chang,Chenglin Wu,Bang Liu*

Main category: cs.CL

TL;DR: CARE框架通过让大语言模型在推理过程中显式整合上下文证据，显著提高了检索准确性和答案生成性能，无需昂贵的监督微调。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在基于给定信息回答问题时经常出现上下文保真度问题，产生不一致的答案。现有方法要么依赖昂贵的监督微调，要么训练模型进行网络搜索但未必改善对给定上下文的利用。

Method: 提出CARE框架，教导LLMs在推理过程中显式整合上下文证据，利用模型自身的检索能力，只需要有限的标记证据数据，通过策略性检索的上下文标记来增强检索和生成性能。

Result: 在多个真实世界和反事实QA基准测试上的广泛实验表明，该方法显著优于监督微调、传统检索增强生成方法和外部检索解决方案。

Conclusion: 这项工作代表了在使LLMs更准确、可靠和高效地处理知识密集型任务方面的根本性进步。

Abstract: Large language models (LLMs) often struggle with context fidelity, producing
inconsistent answers when responding to questions based on provided
information. Existing approaches either rely on expensive supervised
fine-tuning to generate evidence post-answer or train models to perform web
searches without necessarily improving utilization of the given context. We
propose CARE, a novel native retrieval-augmented reasoning framework that
teaches LLMs to explicitly integrate in-context evidence within their reasoning
process with the model's own retrieval capabilities. Our method requires
limited labeled evidence data while significantly enhancing both retrieval
accuracy and answer generation performance through strategically retrieved
in-context tokens in the reasoning chain. Extensive experiments on multiple
real-world and counterfactual QA benchmarks demonstrate that our approach
substantially outperforms supervised fine-tuning, traditional
retrieval-augmented generation methods, and external retrieval solutions. This
work represents a fundamental advancement in making LLMs more accurate,
reliable, and efficient for knowledge-intensive tasks.

</details>


### [9] [Can Large Language Models Robustly Perform Natural Language Inference for Japanese Comparatives?](https://arxiv.org/abs/2509.13695)
*Yosuke Mikami,Daiki Matsuoka,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在处理日语比较句自然语言推理时的表现，发现模型对提示格式敏感，且在零样本和少样本设置下都难以处理日语特有的语言现象，但包含逻辑语义表示的提示能改善性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言推理方面表现优异，但涉及数值和逻辑表达式的推理仍然具有挑战性。比较句是与此类推理相关的关键语言现象，但LLMs在处理这些现象（特别是在训练数据中不占主导地位的语言如日语）方面的鲁棒性尚未得到充分探索。

Method: 构建了一个专注于比较句的日语NLI数据集，并在零样本和少样本设置下评估了各种LLMs。研究了不同提示格式和包含黄金标签的少样本示例对模型性能的影响。

Result: 模型性能在零样本设置中对提示格式敏感，在少样本设置中受黄金标签的影响。LLMs难以处理日语特有的语言现象。包含逻辑语义表示的提示有助于模型预测那些即使在少样本示例下也难以解决的推理问题的正确标签。

Conclusion: LLMs在处理日语比较句推理方面存在局限性，特别是在处理语言特有现象时。适当的提示工程（如包含逻辑语义表示）可以显著改善模型性能，这为改进多语言NLI任务提供了重要见解。

Abstract: Large Language Models (LLMs) perform remarkably well in Natural Language
Inference (NLI). However, NLI involving numerical and logical expressions
remains challenging. Comparatives are a key linguistic phenomenon related to
such inference, but the robustness of LLMs in handling them, especially in
languages that are not dominant in the models' training data, such as Japanese,
has not been sufficiently explored. To address this gap, we construct a
Japanese NLI dataset that focuses on comparatives and evaluate various LLMs in
zero-shot and few-shot settings. Our results show that the performance of the
models is sensitive to the prompt formats in the zero-shot setting and
influenced by the gold labels in the few-shot examples. The LLMs also struggle
to handle linguistic phenomena unique to Japanese. Furthermore, we observe that
prompts containing logical semantic representations help the models predict the
correct labels for inference problems that they struggle to solve even with
few-shot examples.

</details>


### [10] [Integrating Text and Time-Series into (Large) Language Models to Predict Medical Outcomes](https://arxiv.org/abs/2509.13696)
*Iyadh Ben Cheikh Larbi,Ajay Madhavan Ravichandran,Aljoscha Burchardt,Roland Roller*

Main category: cs.CL

TL;DR: 使用DSPy提示优化技术，将指令调优的大语言模型应用于临床分类任务，处理临床笔记和结构化EHR数据，性能媲美专用多模态系统且更简单灵活


<details>
  <summary>Details</summary>
Motivation: 大语言模型在文本生成方面表现出色，但在处理涉及结构化数据（如时间序列）的临床分类任务方面仍有待探索

Method: 采用基于DSPy的提示优化技术，对指令调优的大语言模型进行适配，使其能够联合处理临床笔记和结构化EHR输入

Result: 该方法在性能上与专用多模态系统相当，同时需要更少的复杂性，并在不同任务间具有更好的适应性

Conclusion: 基于提示优化的LLM方法为临床分类任务提供了一种有效且灵活的解决方案，能够处理多模态医疗数据

Abstract: Large language models (LLMs) excel at text generation, but their ability to
handle clinical classification tasks involving structured data, such as time
series, remains underexplored. In this work, we adapt instruction-tuned LLMs
using DSPy-based prompt optimization to process clinical notes and structured
EHR inputs jointly. Our results show that this approach achieves performance on
par with specialized multimodal systems while requiring less complexity and
offering greater adaptability across tasks.

</details>


### [11] [DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models](https://arxiv.org/abs/2509.13702)
*Xiao Zheng*

Main category: cs.CL

TL;DR: DSCC-HS是一种新颖的主动式幻觉抑制框架，通过紧凑代理模型在自回归解码过程中动态校准，无需修改目标模型即可显著提升LLM的事实性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的幻觉问题是其可靠部署的主要障碍，现有方法如RAG往往是被动的，需要一种主动干预的解决方案。

Method: 基于双过程认知理论，使用紧凑代理模型分别作为事实对齐代理(FAP)和幻觉检测代理(HDP)，在推理时通过实时注入转向向量（FAP和HDP对数概率的差值）来动态引导大模型。

Result: 在TruthfulQA上达到99.2%的事实一致性率，在BioGEN基准上获得最高的FActScore 46.50，实现了最先进的性能。

Conclusion: DSCC-HS是一个原理清晰且高效的解决方案，能够有效增强LLM的事实性，为可靠部署提供了新的技术路径。

Abstract: Large Language Model (LLM) hallucination is a significant barrier to their
reliable deployment. Current methods like Retrieval-Augmented Generation (RAG)
are often reactive. We introduce **Dynamic Self-reinforcing Calibration for
Hallucination Suppression (DSCC-HS)**, a novel, proactive framework that
intervenes during autoregressive decoding. Inspired by dual-process cognitive
theory, DSCC-HS uses a compact proxy model, trained in adversarial roles as a
Factual Alignment Proxy (FAP) and a Hallucination Detection Proxy (HDP). During
inference, these proxies dynamically steer a large target model by injecting a
real-time steering vector, which is the difference between FAP and HDP logits,
at each decoding step. This plug-and-play approach requires no modification to
the target model. Our experiments on TruthfulQA and BioGEN show DSCC-HS
achieves state-of-the-art performance. On TruthfulQA, it reached a 99.2%
Factual Consistency Rate (FCR). On the long-form BioGEN benchmark, it attained
the highest FActScore of 46.50. These results validate DSCC-HS as a principled
and efficient solution for enhancing LLM factuality.

</details>


### [12] [Automated Triaging and Transfer Learning of Incident Learning Safety Reports Using Large Language Representational Models](https://arxiv.org/abs/2509.13706)
*Peter Beidler,Mark Nguyen,Kevin Lybarger,Ola Holmberg,Eric Ford,John Kang*

Main category: cs.CL

TL;DR: 开发了基于NLP的医疗事件报告严重性自动筛查工具，使用SVM和BlueBERT模型在放射肿瘤学领域进行跨机构验证，通过迁移学习显著提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 医疗事件报告的手动审查耗时且需要专业知识，需要自动化工具来提高安全性和质量改进的效率。

Method: 使用7,094份机构报告和571份IAEA SAFRON报告，训练SVM和基于PubMed预训练的BlueBERT模型，采用跨机构迁移学习策略。

Result: 机构内测试AUROC达0.82，跨机构测试从0.42-0.56提升至0.78，在人工编辑数据集上与人类表现相当(AUROC 0.81 vs 0.74-0.85)。

Conclusion: 成功开发了跨机构NLP模型，能够像人类专家一样有效检测高严重性放射肿瘤学事件报告。

Abstract: PURPOSE: Incident reports are an important tool for safety and quality
improvement in healthcare, but manual review is time-consuming and requires
subject matter expertise. Here we present a natural language processing (NLP)
screening tool to detect high-severity incident reports in radiation oncology
across two institutions.
  METHODS AND MATERIALS: We used two text datasets to train and evaluate our
NLP models: 7,094 reports from our institution (Inst.), and 571 from IAEA
SAFRON (SF), all of which had severity scores labeled by clinical content
experts. We trained and evaluated two types of models: baseline support vector
machines (SVM) and BlueBERT which is a large language model pretrained on
PubMed abstracts and hospitalized patient data. We assessed for
generalizability of our model in two ways. First, we evaluated models trained
using Inst.-train on SF-test. Second, we trained a BlueBERT_TRANSFER model that
was first fine-tuned on Inst.-train then on SF-train before testing on SF-test
set. To further analyze model performance, we also examined a subset of 59
reports from our Inst. dataset, which were manually edited for clarity.
  RESULTS Classification performance on the Inst. test achieved AUROC 0.82
using SVM and 0.81 using BlueBERT. Without cross-institution transfer learning,
performance on the SF test was limited to an AUROC of 0.42 using SVM and 0.56
using BlueBERT. BlueBERT_TRANSFER, which was fine-tuned on both datasets,
improved the performance on SF test to AUROC 0.78. Performance of SVM, and
BlueBERT_TRANSFER models on the manually curated Inst. reports (AUROC 0.85 and
0.74) was similar to human performance (AUROC 0.81).
  CONCLUSION: In summary, we successfully developed cross-institution NLP
models on incident report text from radiation oncology centers. These models
were able to detect high-severity reports similarly to humans on a curated
dataset.

</details>


### [13] [DSPC: Dual-Stage Progressive Compression Framework for Efficient Long-Context Reasoning](https://arxiv.org/abs/2509.13723)
*Yaxin Gao,Yao Lu,Zongfei Zhang,Jiaqi Nie,Shanqing Yu,Qi Xuan*

Main category: cs.CL

TL;DR: 提出DSPC双阶段渐进压缩方法，无需训练即可压缩LLM提示，在减少3倍token的情况下性能提升7.76%，优于现有最佳方法


<details>
  <summary>Details</summary>
Motivation: 解决LLM提示越来越长导致计算成本增加的问题，现有方法需要训练辅助模型带来额外计算开销

Method: 两阶段无训练方法：粗粒度阶段基于TF-IDF过滤低语义价值句子；细粒度阶段使用注意力贡献、跨模型损失差异和位置重要性评估token重要性，修剪低效用token

Result: 在LLaMA-3.1-8B-Instruct和GPT-3.5-Turbo上验证，在受限token预算下性能持续提升。Longbench FewShot任务中使用仅3倍更少token达到49.17性能，优于LongLLMLingua 7.76

Conclusion: DSPC提供了一种高效的无训练提示压缩方案，显著降低计算成本的同时保持语义完整性，在多个模型和任务上表现优异

Abstract: Large language models (LLMs) have achieved remarkable success in many natural
language processing (NLP) tasks. To achieve more accurate output, the prompts
used to drive LLMs have become increasingly longer, which incurs higher
computational costs. To address this prompt inflation problem, prompt
compression has been proposed. However, most existing methods require training
a small auxiliary model for compression, incurring a significant amount of
additional computation. To avoid this, we propose a two-stage, training-free
approach, called Dual-Stage Progressive Compression (DSPC). In the
coarse-grained stage, semantic-related sentence filtering removes sentences
with low semantic value based on TF-IDF. In the fine-grained stage, token
importance is assessed using attention contribution, cross-model loss
difference, and positional importance, enabling the pruning of low-utility
tokens while preserving semantics. We validate DSPC on LLaMA-3.1-8B-Instruct
and GPT-3.5-Turbo under a constrained token budget and observe consistent
improvements. For instance, in the FewShot task of the Longbench dataset, DSPC
achieves a performance of 49.17 by using only 3x fewer tokens, outperforming
the best state-of-the-art baseline LongLLMLingua by 7.76.

</details>


### [14] [Implementing a Logical Inference System for Japanese Comparatives](https://arxiv.org/abs/2509.13734)
*Yosuke Mikami,Daiki Matsuoka,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 提出了一个基于组合语义的日语比较句逻辑推理系统ccg-jcomp，用于处理日语自然语言推理中的比较表达，并在日语NLI数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 日语和英语比较句在形态和语义上存在差异，使得现有的英语比较句逻辑推理系统难以直接应用于日语。需要专门针对日语比较句开发逻辑推理系统。

Method: 基于组合语义构建逻辑推理系统ccg-jcomp，专门处理日语比较句的数值和逻辑表达式，采用逻辑基础方法而非大型语言模型。

Result: 在包含比较表达的日语NLI数据集上评估系统性能，并与现有大型语言模型的准确率进行比较，证明了系统的有效性。

Conclusion: ccg-jcomp系统能够有效处理日语比较句的自然语言推理任务，为日语比较句的逻辑推理提供了专门的解决方案。

Abstract: Natural Language Inference (NLI) involving comparatives is challenging
because it requires understanding quantities and comparative relations
expressed by sentences. While some approaches leverage Large Language Models
(LLMs), we focus on logic-based approaches grounded in compositional semantics,
which are promising for robust handling of numerical and logical expressions.
Previous studies along these lines have proposed logical inference systems for
English comparatives. However, it has been pointed out that there are several
morphological and semantic differences between Japanese and English
comparatives. These differences make it difficult to apply such systems
directly to Japanese comparatives. To address this gap, this study proposes
ccg-jcomp, a logical inference system for Japanese comparatives based on
compositional semantics. We evaluate the proposed system on a Japanese NLI
dataset containing comparative expressions. We demonstrate the effectiveness of
our system by comparing its accuracy with that of existing LLMs.

</details>


### [15] [Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications](https://arxiv.org/abs/2509.13775)
*Vani Kanjirangat,Ljiljana Dolamic,Fabio Rinaldi*

Main category: cs.CL

TL;DR: 本文探索了阿拉伯语方言识别(ADI)的数据高效和参数高效方法，包括软提示策略、LoRA重参数化以及零样本/少样本推理，发现LoRA微调模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 研究不同数据高效和参数高效方法在阿拉伯语方言识别任务中的效果，探索大语言模型在方言识别方面的能力。

Method: 使用软提示策略(前缀调优、提示调优、P-tuning、P-tuning V2)和LoRA重参数化；分析零样本和少样本推理；在阿拉伯语专用编码器模型和多语言解码器模型上进行实验。

Result: 大语言模型在零样本/少样本设置下难以区分方言细微差别；软提示编码器变体表现更好；基于LoRA的微调模型表现最佳，甚至超过全微调。

Conclusion: LoRA微调是阿拉伯语方言识别最有效的参数高效方法，而大语言模型在少样本设置下的方言识别能力有限。

Abstract: This paper discusses our exploration of different data-efficient and
parameter-efficient approaches to Arabic Dialect Identification (ADI). In
particular, we investigate various soft-prompting strategies, including
prefix-tuning, prompt-tuning, P-tuning, and P-tuning V2, as well as LoRA
reparameterizations. For the data-efficient strategy, we analyze hard prompting
with zero-shot and few-shot inferences to analyze the dialect identification
capabilities of Large Language Models (LLMs). For the parameter-efficient PEFT
approaches, we conducted our experiments using Arabic-specific encoder models
on several major datasets. We also analyzed the n-shot inferences on
open-source decoder-only models, a general multilingual model (Phi-3.5), and an
Arabic-specific one(SILMA). We observed that the LLMs generally struggle to
differentiate the dialectal nuances in the few-shot or zero-shot setups. The
soft-prompted encoder variants perform better, while the LoRA-based fine-tuned
models perform best, even surpassing full fine-tuning.

</details>


### [16] [Teaching According to Talents! Instruction Tuning LLMs with Competence-Aware Curriculum Learning](https://arxiv.org/abs/2509.13790)
*Yangning Li,Tingwei Lu,Yinghui Li,Yankai Chen,Wei-Chieh Huang,Wenhao Jiang,Hui Wang,Hai-Tao Zheng,Philip S. Yu*

Main category: cs.CL

TL;DR: CAMPUS是一个动态多视角课程学习框架，通过能力感知的课程调度和动态子课程选择，解决了传统课程学习中基于静态难度指标的刚性课程问题，提升了指令调优的效率。


<details>
  <summary>Details</summary>
Motivation: 传统课程学习方法依赖静态启发式难度指标，无法适应模型在训练过程中不断演进的能力，导致固定的、可能次优的学习轨迹。

Method: 提出CAMPUS框架，包含三个关键优势：动态子课程选择、能力感知的课程调度调整、多难度基础调度。

Result: 大量实验证明CAMPUS在高效指令调优方面优于其他最先进的基线方法。

Conclusion: CAMPUS通过动态适应模型能力演进的多视角课程学习方法，有效提升了指令调优的最终性能。

Abstract: Efficient instruction tuning aims to enhance the ultimate performance of
large language models (LLMs) trained on a given instruction dataset. Curriculum
learning as a typical data organization strategy has shown preliminary
effectiveness in instruction tuning. However, current curriculum tuning methods
suffer from the curriculum rigidity, since they rely solely on static heuristic
difficulty metrics. These methods fail to adapt to the evolving capabilities of
models during training, resulting in a fixed and potentially sub-optimal
learning trajectory. To address the issue, Competence-Aware Multi-Perspective
cUrriculum inStruction tuning framework termed CAMPUS is proposed. CAMPUS
offers several advantages: (1) Dynamic selection for sub-curriculum. (2)
Competency-aware adjustment to the curriculum schedule. (3) Multiple
difficulty-based scheduling. Extensive experiments prove the superior
performance of CAMPUS, compared to other state-of-the-art baselines for
efficient instruction tuning.

</details>


### [17] [Measuring Gender Bias in Job Title Matching for Grammatical Gender Languages](https://arxiv.org/abs/2509.13803)
*Laura García-Sardiña,Hermenegildo Fabregat,Daniel Deniz,Rabih Zbib*

Main category: cs.CL

TL;DR: 本文研究了语法性别对自动职位排名系统的影响，提出了基于RBO的性别偏见评估方法，并在四种语法性别语言中创建了测试集，发现现有多语言模型都存在不同程度的性别偏见。


<details>
  <summary>Details</summary>
Motivation: 研究语法性别在职位名称中的显式分配如何影响自动职位排名系统的结果，评估现有系统在性别偏见方面的问题。

Method: 提出使用RBO（Rank-Biased Overlap）指标来评估职位排名系统中的性别偏见，在四种语法性别语言中生成包含男性和女性形式的职位名称测试集，并评估多个现成的多语言模型。

Result: 所有测试的多语言模型都表现出不同程度的性别偏见，证明了现有系统在性别平等方面存在问题。

Conclusion: 需要开发更公平的职位排名系统，本文提出的评估方法和测试集为研究语法性别偏见提供了重要基础。

Abstract: This work sets the ground for studying how explicit grammatical gender
assignment in job titles can affect the results of automatic job ranking
systems. We propose the usage of metrics for ranking comparison controlling for
gender to evaluate gender bias in job title ranking systems, in particular RBO
(Rank-Biased Overlap). We generate and share test sets for a job title matching
task in four grammatical gender languages, including occupations in masculine
and feminine form and annotated by gender and matching relevance. We use the
new test sets and the proposed methodology to evaluate the gender bias of
several out-of-the-box multilingual models to set as baselines, showing that
all of them exhibit varying degrees of gender bias.

</details>


### [18] [Geometric Uncertainty for Detecting and Correcting Hallucinations in LLMs](https://arxiv.org/abs/2509.13813)
*Edward Phillips,Sean Wu,Soheila Molaei,Danielle Belgrave,Anshul Thakur,David Clifton*

Main category: cs.CL

TL;DR: 提出了一个基于几何框架的黑盒不确定性量化方法，通过原型分析同时提供全局和局部不确定性估计，用于检测大语言模型的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒方法只能提供全局不确定性估计，而局部方法需要白盒访问模型内部状态。需要一种仅通过黑盒访问就能同时提供全局和局部不确定性估计的方法来检测语言模型的幻觉。

Method: 基于响应嵌入的原型分析几何框架。全局层面使用几何体积测量原型凸包体积，局部层面使用几何怀疑度对响应可靠性进行排序，通过优先选择响应来减少幻觉。

Result: 在短形式问答数据集上表现优于或相当于现有方法，在医疗数据集上取得更优结果（幻觉风险特别关键的领域）。理论证明了凸包体积与熵之间的联系。

Conclusion: 该几何框架为黑盒不确定性量化提供了有效的解决方案，能够同时处理全局和局部不确定性，在关键应用领域如医疗问答中特别有价值。

Abstract: Large language models demonstrate impressive results across diverse tasks but
are still known to hallucinate, generating linguistically plausible but
incorrect answers to questions. Uncertainty quantification has been proposed as
a strategy for hallucination detection, but no existing black-box approach
provides estimates for both global and local uncertainty. The former attributes
uncertainty to a batch of responses, while the latter attributes uncertainty to
individual responses. Current local methods typically rely on white-box access
to internal model states, whilst black-box methods only provide global
uncertainty estimates. We introduce a geometric framework to address this,
based on archetypal analysis of batches of responses sampled with only
black-box model access. At the global level, we propose Geometric Volume, which
measures the convex hull volume of archetypes derived from response embeddings.
At the local level, we propose Geometric Suspicion, which ranks responses by
reliability and enables hallucination reduction through preferential response
selection. Unlike prior dispersion methods which yield only a single global
score, our approach provides semantic boundary points which have utility for
attributing reliability to individual responses. Experiments show that our
framework performs comparably to or better than prior methods on short form
question-answering datasets, and achieves superior results on medical datasets
where hallucinations carry particularly critical risks. We also provide
theoretical justification by proving a link between convex hull volume and
entropy.

</details>


### [19] [Findings of the Third Automatic Minuting (AutoMin) Challenge](https://arxiv.org/abs/2509.13814)
*Kartik Shinde,Laurent Besacier,Ondrej Bojar,Thibaut Thonet,Tirthankar Ghosal*

Main category: cs.CL

TL;DR: AutoMin 2025共享任务包括会议纪要生成和问答两个任务，涵盖英语和捷克语，参与团队较少但包含多个基线系统评估。


<details>
  <summary>Details</summary>
Motivation: 推动自动会议纪要技术发展，评估当前大语言模型在结构化会议纪要生成和跨语言问答任务上的表现。

Method: 组织共享任务，设置两个主要任务：minuting（结构化会议纪要生成）和question answering（问答），涵盖英语和捷克语双语处理，并建立基线系统进行比较评估。

Result: 2025年参与度较低（纪要任务1个团队，问答任务2个团队），但通过组织方提供的多个基线系统，成功评估了当前大语言模型在这两个任务上的性能。

Conclusion: 尽管参与团队有限，但AutoMin 2025为评估大语言模型在会议纪要生成和跨语言问答任务上的能力提供了有价值的基准和比较框架。

Abstract: This paper presents the third edition of AutoMin, a shared task on automatic
meeting summarization into minutes. In 2025, AutoMin featured the main task of
minuting, the creation of structured meeting minutes, as well as a new task:
question answering (QA) based on meeting transcripts.
  The minuting task covered two languages, English and Czech, and two domains:
project meetings and European Parliament sessions. The QA task focused solely
on project meetings and was available in two settings: monolingual QA in
English, and cross-lingual QA, where questions were asked and answered in Czech
based on English meetings.
  Participation in 2025 was more limited compared to previous years, with only
one team joining the minuting task and two teams participating in QA. However,
as organizers, we included multiple baseline systems to enable a comprehensive
evaluation of current (2025) large language models (LLMs) on both tasks.

</details>


### [20] [Large Language Models Discriminate Against Speakers of German Dialects](https://arxiv.org/abs/2509.13835)
*Minh Duc Bui,Carolin Holtermann,Valentin Hofmann,Anne Lauscher,Katharina von der Wense*

Main category: cs.CL

TL;DR: 研究发现大型语言模型对德国方言使用者存在显著的命名偏见和使用偏见，表现为负面形容词关联，且明确标注语言人口统计特征会放大这种偏见。


<details>
  <summary>Details</summary>
Motivation: 尽管方言具有重要文化价值，但方言使用者常面临负面社会刻板印象。研究旨在探究这种刻板印象是否在大型语言模型中得到体现。

Method: 基于社会语言学文献构建评估框架，通过关联任务和决策任务分析方言命名偏见和使用偏见，并创建包含7种德国地区方言与标准德语对照的新评估语料库。

Result: 所有被评估的LLM都表现出显著的方言命名和使用偏见，在决策中重现这些偏见，且明确标注语言人口统计特征比隐晦的方言使用暗示更能放大偏见。

Conclusion: 大型语言模型确实复制了对德国方言使用者的社会刻板印象，需要采取措施减轻这种语言偏见，特别是在明确标注人口统计特征时。

Abstract: Dialects represent a significant component of human culture and are found
across all regions of the world. In Germany, more than 40% of the population
speaks a regional dialect (Adler and Hansen, 2022). However, despite cultural
importance, individuals speaking dialects often face negative societal
stereotypes. We examine whether such stereotypes are mirrored by large language
models (LLMs). We draw on the sociolinguistic literature on dialect perception
to analyze traits commonly associated with dialect speakers. Based on these
traits, we assess the dialect naming bias and dialect usage bias expressed by
LLMs in two tasks: an association task and a decision task. To assess a model's
dialect usage bias, we construct a novel evaluation corpus that pairs sentences
from seven regional German dialects (e.g., Alemannic and Bavarian) with their
standard German counterparts. We find that: (1) in the association task, all
evaluated LLMs exhibit significant dialect naming and dialect usage bias
against German dialect speakers, reflected in negative adjective associations;
(2) all models reproduce these dialect naming and dialect usage biases in their
decision making; and (3) contrary to prior work showing minimal bias with
explicit demographic mentions, we find that explicitly labeling linguistic
demographics--German dialect speakers--amplifies bias more than implicit cues
like dialect usage.

</details>


### [21] [Do LLMs Align Human Values Regarding Social Biases? Judging and Explaining Social Biases with LLMs](https://arxiv.org/abs/2509.13869)
*Yang Liu,Chenhui Chu*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型在不同类型偏见场景中与人类价值观的对齐情况，发现大参数模型不一定具有更好的对齐表现，模型对特定场景类型有偏好，且同一模型家族的判断一致性更高。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型可能因与人类价值观不对齐而产生不良后果，特别是在涉及复杂敏感社会偏见的场景中。之前的研究使用专家设计或基于代理的偏见场景揭示了这种不对齐，但尚不清楚不同类型场景下LLMs与人类价值观的对齐是否存在差异。

Method: 通过对来自4个模型家族的12个LLMs和4个数据集进行广泛分析，研究LLMs在不同类型偏见场景中的对齐情况，并考察LLMs对HVSB的理解能力及其解释偏好。

Result: 大参数规模的LLMs不一定具有更低的不对齐率和攻击成功率；LLMs对特定类型的场景表现出一定程度的对齐偏好；同一模型家族的LLMs倾向于具有更高的判断一致性；LLMs对HVSB的理解没有显著差异；LLMs偏好自己生成的解释；经过微调的小型LMs生成的解释更易读但模型认同度较低。

Conclusion: LLMs与人类价值观的对齐存在场景类型依赖性，模型规模不是决定对齐质量的唯一因素，同一模型家族的判断一致性更高，小型LMs经过微调可以生成更易读的解释但需要提高模型认同度。

Abstract: Large language models (LLMs) can lead to undesired consequences when
misaligned with human values, especially in scenarios involving complex and
sensitive social biases. Previous studies have revealed the misalignment of
LLMs with human values using expert-designed or agent-based emulated bias
scenarios. However, it remains unclear whether the alignment of LLMs with human
values differs across different types of scenarios (e.g., scenarios containing
negative vs. non-negative questions). In this study, we investigate the
alignment of LLMs with human values regarding social biases (HVSB) in different
types of bias scenarios. Through extensive analysis of 12 LLMs from four model
families and four datasets, we demonstrate that LLMs with large model parameter
scales do not necessarily have lower misalignment rate and attack success rate.
Moreover, LLMs show a certain degree of alignment preference for specific types
of scenarios and the LLMs from the same model family tend to have higher
judgment consistency. In addition, we study the understanding capacity of LLMs
with their explanations of HVSB. We find no significant differences in the
understanding of HVSB across LLMs. We also find LLMs prefer their own generated
explanations. Additionally, we endow smaller language models (LMs) with the
ability to explain HVSB. The generation results show that the explanations
generated by the fine-tuned smaller LMs are more readable, but have a
relatively lower model agreeability.

</details>


### [22] [Combining Evidence and Reasoning for Biomedical Fact-Checking](https://arxiv.org/abs/2509.13879)
*Mariano Barone,Antonio Romano,Giuseppe Riccio,Marco Postiglione,Vincenzo Moscato*

Main category: cs.CL

TL;DR: CER是一个用于生物医学事实核查的新框架，结合科学证据检索、大语言模型推理和监督真实性预测，在多个数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 医疗领域中的错误信息（如疫苗犹豫和未经证实的治疗方法）对公共卫生和医疗系统信任构成风险。生物医学声明验证具有独特挑战性，包括复杂术语、需要领域专业知识以及必须基于科学证据。

Method: CER框架整合了科学证据检索、通过大语言模型进行推理以及监督真实性预测。通过将大语言模型的文本生成能力与高质量生物医学科学证据的先进检索技术相结合，有效减少幻觉风险。

Result: 在专家标注的数据集（HealthFC、BioASQ-7b、SciFact）上的评估显示达到了最先进的性能，并展现出有前景的跨数据集泛化能力。

Conclusion: CER框架通过结合证据检索和推理，为生物医学事实核查提供了有效的解决方案，代码和数据已开源以确保透明度和可复现性。

Abstract: Misinformation in healthcare, from vaccine hesitancy to unproven treatments,
poses risks to public health and trust in medical systems. While machine
learning and natural language processing have advanced automated fact-checking,
validating biomedical claims remains uniquely challenging due to complex
terminology, the need for domain expertise, and the critical importance of
grounding in scientific evidence. We introduce CER (Combining Evidence and
Reasoning), a novel framework for biomedical fact-checking that integrates
scientific evidence retrieval, reasoning via large language models, and
supervised veracity prediction. By integrating the text-generation capabilities
of large language models with advanced retrieval techniques for high-quality
biomedical scientific evidence, CER effectively mitigates the risk of
hallucinations, ensuring that generated outputs are grounded in verifiable,
evidence-based sources. Evaluations on expert-annotated datasets (HealthFC,
BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising
cross-dataset generalization. Code and data are released for transparency and
reproducibility: https: //github.com/PRAISELab-PicusLab/CER.

</details>


### [23] [Combating Biomedical Misinformation through Multi-modal Claim Detection and Evidence-based Verification](https://arxiv.org/abs/2509.13888)
*Mariano Barone,Antonio Romano,Giuseppe Riccio,Marco Postiglione,Vincenzo Moscato*

Main category: cs.CL

TL;DR: CER是一个用于生物医学事实核查的新框架，结合科学证据检索、大语言模型推理和监督真实性预测，在多个数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 医疗健康领域的错误信息（如疫苗犹豫和未经证实的治疗方法）对公共卫生和医疗系统信任构成风险。生物医学声明验证具有独特挑战性，包括复杂术语、需要领域专业知识以及必须基于科学证据。

Method: CER框架整合科学证据检索、大语言模型推理和监督真实性预测。通过将大语言模型的文本生成能力与高质量生物医学科学证据的先进检索技术相结合，有效减轻幻觉风险。

Result: 在专家标注的数据集（HealthFC、BioASQ-7b、SciFact）上评估显示达到最先进性能，并展现出有前景的跨数据集泛化能力。

Conclusion: CER框架通过结合证据检索和推理，为生物医学事实核查提供了有效的解决方案，代码和数据已开源以确保透明度和可重现性。

Abstract: Misinformation in healthcare, from vaccine hesitancy to unproven treatments,
poses risks to public health and trust in medical systems. While machine
learning and natural language processing have advanced automated fact-checking,
validating biomedical claims remains uniquely challenging due to complex
terminology, the need for domain expertise, and the critical importance of
grounding in scientific evidence. We introduce CER (Combining Evidence and
Reasoning), a novel framework for biomedical fact-checking that integrates
scientific evidence retrieval, reasoning via large language models, and
supervised veracity prediction. By integrating the text-generation capabilities
of large language models with advanced retrieval techniques for high-quality
biomedical scientific evidence, CER effectively mitigates the risk of
hallucinations, ensuring that generated outputs are grounded in verifiable,
evidence-based sources. Evaluations on expert-annotated datasets (HealthFC,
BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising
cross-dataset generalization. Code and data are released for transparency and
reproducibility: https://github.com/PRAISELab-PicusLab/CER

</details>


### [24] [Do Large Language Models Understand Word Senses?](https://arxiv.org/abs/2509.13905)
*Domenico Meconi,Simone Stirpe,Federico Martelli,Leonardo Lavalle,Roberto Navigli*

Main category: cs.CL

TL;DR: 本文评估了指令调优大语言模型在词义消歧任务上的能力，并与专门系统比较，同时测试了模型在定义生成、自由解释和示例生成三种生成任务中的词义理解能力。


<details>
  <summary>Details</summary>
Motivation: 尽管已有大量评估工作，但大语言模型是否真正理解词义仍缺乏深入探索，本文旨在填补这一空白。

Method: 评估指令调优LLMs的词义消歧能力，并与最先进的专门系统比较；测试两个顶级开源和闭源LLMs在三种生成设置（定义生成、自由解释、示例生成）中的词义理解能力。

Result: 在词义消歧任务中，GPT-4o和DeepSeek-V3等领先模型达到与专门WSD系统相当的性能，且在领域和难度级别上表现出更强的鲁棒性；在生成任务中，LLMs能以高达98%的准确率解释上下文中的词义，自由解释任务表现最佳。

Conclusion: 大语言模型在词义消歧任务上已达到专门系统水平，且在生成任务中展现出强大的词义理解能力，特别是在自由解释方面表现最优。

Abstract: Understanding the meaning of words in context is a fundamental capability for
Large Language Models (LLMs). Despite extensive evaluation efforts, the extent
to which LLMs show evidence that they truly grasp word senses remains
underexplored. In this paper, we address this gap by evaluating both i) the
Word Sense Disambiguation (WSD) capabilities of instruction-tuned LLMs,
comparing their performance to state-of-the-art systems specifically designed
for the task, and ii) the ability of two top-performing open- and closed-source
LLMs to understand word senses in three generative settings: definition
generation, free-form explanation, and example generation. Notably, we find
that, in the WSD task, leading models such as GPT-4o and DeepSeek-V3 achieve
performance on par with specialized WSD systems, while also demonstrating
greater robustness across domains and levels of difficulty. In the generation
tasks, results reveal that LLMs can explain the meaning of words in context up
to 98\% accuracy, with the highest performance observed in the free-form
explanation task, which best aligns with their generative capabilities.

</details>


### [25] [Linguistic Nepotism: Trading-off Quality for Language Preference in Multilingual RAG](https://arxiv.org/abs/2509.13930)
*Dayeon Ki,Marine Carpuat,Paul McNamee,Daniel Khashabi,Eugene Yang,Dawn Lawrie,Kevin Duh*

Main category: cs.CL

TL;DR: 研究发现多语言检索增强生成系统存在语言偏好偏见，模型倾向于引用英文来源，即使其他语言文档更相关，这种偏见在低资源语言和文档位置中段时更加明显。


<details>
  <summary>Details</summary>
Motivation: 研究多语言检索增强生成系统中不同文档语言的混合是否会对生成和引用产生意外影响，特别是模型是否会在语言偏好和文档相关性之间做出权衡。

Method: 采用受控方法，利用模型内部机制来衡量语言偏好，同时保持文档相关性等其他因素不变，涵盖8种语言和6个开源模型。

Result: 模型在英语查询时优先引用英文来源，这种偏见在低资源语言和文档位于上下文中间位置时更加明显。模型有时会牺牲文档相关性来满足语言偏好，表明引用选择并非总是由信息量驱动。

Conclusion: 研究揭示了语言模型如何利用多语言上下文并影响引用行为，指出了mRAG系统中存在的语言偏见问题，这对构建更公平的多语言AI系统具有重要意义。

Abstract: Multilingual Retrieval-Augmented Generation (mRAG) systems enable language
models to answer knowledge-intensive queries with citation-supported responses
across languages. While such systems have been proposed, an open questions is
whether the mixture of different document languages impacts generation and
citation in unintended ways. To investigate, we introduce a controlled
methodology using model internals to measure language preference while holding
other factors such as document relevance constant. Across eight languages and
six open-weight models, we find that models preferentially cite English sources
when queries are in English, with this bias amplified for lower-resource
languages and for documents positioned mid-context. Crucially, we find that
models sometimes trade-off document relevance for language preference,
indicating that citation choices are not always driven by informativeness
alone. Our findings shed light on how language models leverage multilingual
context and influence citation behavior.

</details>


### [26] [Long-context Reference-based MT Quality Estimation](https://arxiv.org/abs/2509.13980)
*Sami Ul Haq,Chinonso Cynthia Osuji,Sheila Castilho,Brian Davis*

Main category: cs.CL

TL;DR: 基于COMET框架构建的翻译质量评估系统，通过长上下文数据增强训练来预测错误跨度标注分数，整合多种人工标注数据集，实验表明长上下文信息能提升与人工评估的相关性


<details>
  <summary>Details</summary>
Motivation: 解决传统短片段翻译质量评估的局限性，通过利用长上下文信息来更准确地预测翻译质量，提升与人类判断的相关性

Method: 使用COMET框架，构建长上下文训练数据（拼接领域内人工标注句子并计算加权平均分数），整合MQM、SQM、DA等多种人工判断数据集并进行尺度归一化，训练多语言回归模型从源文本、假设翻译和参考翻译预测质量分数

Result: 实验结果显示，与仅使用短片段训练的模型相比，融入长上下文信息能够显著提高与人工判断的相关性

Conclusion: 长上下文信息在翻译质量评估中具有重要价值，通过数据增强和多种标注数据集的整合可以有效提升评估模型的性能

Abstract: In this paper, we present our submission to the Tenth Conference on Machine
Translation (WMT25) Shared Task on Automated Translation Quality Evaluation.
  Our systems are built upon the COMET framework and trained to predict
segment-level Error Span Annotation (ESA) scores using augmented long-context
data.
  To construct long-context training data, we concatenate in-domain,
human-annotated sentences and compute a weighted average of their scores.
  We integrate multiple human judgment datasets (MQM, SQM, and DA) by
normalising their scales and train multilingual regression models to predict
quality scores from the source, hypothesis, and reference translations.
  Experimental results show that incorporating long-context information
improves correlations with human judgments compared to models trained only on
short segments.

</details>


### [27] [Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency](https://arxiv.org/abs/2509.13990)
*Colin Hong,Xu Guo,Anand Chaanan Singh,Esha Choukse,Dmitrii Ustiugov*

Main category: cs.CL

TL;DR: Slim-SC是一种基于思维层面链间相似性的逐步剪枝策略，通过识别和移除冗余推理链来加速自洽性方法，在保持或提高准确性的同时显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 自洽性(SC)方法虽然能提升LLM推理性能，但存在数量级计算开销问题，限制了其广泛应用。现有加速方法主要依赖模型置信度分数或缺乏实证支持的启发式方法。

Method: 提出Slim-SC方法，通过理论分析和实证研究揭示SC的低效性，基于思维层面的链间相似性设计逐步剪枝策略，识别并移除冗余推理链。

Result: 在三个STEM推理数据集和两种LLM架构上的实验表明，Slim-SC将推理延迟和KVC使用分别降低高达45%和26%，同时保持或提高了准确性。

Conclusion: Slim-SC为自洽性方法提供了一个简单而高效的测试时扩展替代方案，有效解决了计算开销问题，具有实际部署价值。

Abstract: Recently, Test-Time Scaling (TTS) has gained increasing attention for
improving LLM reasoning performance at test time without retraining the model.
A notable TTS technique is Self-Consistency (SC), which generates multiple
reasoning chains in parallel and selects the final answer via majority voting.
While effective, the order-of-magnitude computational overhead limits its broad
deployment. Prior attempts to accelerate SC mainly rely on model-based
confidence scores or heuristics with limited empirical support. For the first
time, we theoretically and empirically analyze the inefficiencies of SC and
reveal actionable opportunities for improvement. Building on these insights, we
propose Slim-SC, a step-wise pruning strategy that identifies and removes
redundant chains using inter-chain similarity at the thought level. Experiments
on three STEM reasoning datasets and two recent LLM architectures show that
Slim-SC reduces inference latency and KVC usage by up to 45% and 26%,
respectively, with R1-Distill, while maintaining or improving accuracy, thus
offering a simple yet efficient TTS alternative for SC.

</details>


### [28] [Early Stopping Chain-of-thoughts in Large Language Models](https://arxiv.org/abs/2509.14004)
*Minjia Mao,Bowen Yin,Yu Zhu,Xiao Fang*

Main category: cs.CL

TL;DR: ES-CoT是一种推理时方法，通过检测答案收敛性来提前停止思维链生成，减少约41%的推理token使用，同时保持与标准CoT相当的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在解决复杂问题时需要生成长思维链，但这种方法会产生高昂的推理成本。研究旨在减少推理开销的同时保持性能。

Method: 在每个推理步骤结束时提示LLM输出当前最终答案（步骤答案），跟踪连续相同步骤答案的运行长度作为收敛度量。当运行长度出现急剧增加并超过最小阈值时终止生成。

Result: 在三个LLM和五个推理数据集上的实验显示，ES-CoT平均减少约41%的推理token使用，同时保持与标准CoT相当的准确性。该方法还能与自一致性提示无缝集成。

Conclusion: ES-CoT是一种实用有效的推理效率提升方法，通过检测答案收敛实现早期停止，在保持性能的同时显著降低推理成本。

Abstract: Reasoning large language models (LLMs) have demonstrated superior capacities
in solving complicated problems by generating long chain-of-thoughts (CoT), but
such a lengthy CoT incurs high inference costs. In this study, we introduce
ES-CoT, an inference-time method that shortens CoT generation by detecting
answer convergence and stopping early with minimal performance loss. At the end
of each reasoning step, we prompt the LLM to output its current final answer,
denoted as a step answer. We then track the run length of consecutive identical
step answers as a measure of answer convergence. Once the run length exhibits a
sharp increase and exceeds a minimum threshold, the generation is terminated.
We provide both empirical and theoretical support for this heuristic: step
answers steadily converge to the final answer, and large run-length jumps
reliably mark this convergence. Experiments on five reasoning datasets across
three LLMs show that ES-CoT reduces the number of inference tokens by about
41\% on average while maintaining accuracy comparable to standard CoT. Further,
ES-CoT integrates seamlessly with self-consistency prompting and remains robust
across hyperparameter choices, highlighting it as a practical and effective
approach for efficient reasoning.

</details>


### [29] [Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale](https://arxiv.org/abs/2509.14008)
*Hasan Abed Al Kader Hammoud,Mohammad Zbeeb,Bernard Ghanem*

Main category: cs.CL

TL;DR: Hala是一个阿拉伯语为中心的指令和翻译模型系列，通过翻译调优流程构建，在阿拉伯语基准测试中取得了最先进的结果


<details>
  <summary>Details</summary>
Motivation: 为阿拉伯语NLP研究提供高质量的指令跟随模型，解决阿拉伯语指令数据稀缺的问题

Method: 使用FP8压缩的教师模型生成高质量双语监督数据，训练轻量级语言模型进行翻译，创建百万级阿拉伯语指令数据集，然后训练不同规模的Hala模型并应用slerp合并技术

Result: 在阿拉伯语基准测试中，Hala在"nano"(≤2B)和"small"(7-9B)类别中都达到了最先进水平，超越了基础模型

Conclusion: Hala模型系列为阿拉伯语NLP研究提供了有效的解决方案，并发布了模型、数据和配方以加速该领域的研究

Abstract: We present Hala, a family of Arabic-centric instruction and translation
models built with our translate-and-tune pipeline. We first compress a strong
AR$\leftrightarrow$EN teacher to FP8 (yielding $\sim$2$\times$ higher
throughput with no quality loss) and use it to create high-fidelity bilingual
supervision. A lightweight language model LFM2-1.2B is then fine-tuned on this
data and used to translate high-quality English instruction sets into Arabic,
producing a million-scale corpus tailored to instruction following. We train
Hala models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to
balance Arabic specialization with base-model strengths. On Arabic-centric
benchmarks, Hala achieves state-of-the-art results within both the "nano"
($\leq$2B) and "small" (7-9B) categories, outperforming their bases. We release
models, data, evaluation, and recipes to accelerate research in Arabic NLP.

</details>


### [30] [Audio-Based Crowd-Sourced Evaluation of Machine Translation Quality](https://arxiv.org/abs/2509.14023)
*Sami Ul Haq,Sheila Castilho,Yvette Graham*

Main category: cs.CL

TL;DR: 本研究比较了基于文本和基于音频的机器翻译质量评估方法，发现音频评估能提供更自然、更丰富的评估维度，建议将语音评估纳入未来MT评估框架


<details>
  <summary>Details</summary>
Motivation: 尽管机器翻译取得了显著进展，但质量评估仍主要依赖文本中心的方法。现实世界中许多MT应用涉及语音翻译，因此需要更自然的语音评估方式来替代纯文本评估

Method: 使用Amazon Mechanical Turk收集众包判断，比较10个WMT通用MT共享任务系统的文本评估和音频评估，并进行统计显著性测试和自我复制实验来验证音频方法的可靠性

Result: 基于音频的众包评估产生的排名与纯文本评估基本一致，但在某些情况下能识别出翻译系统之间的显著差异，这归因于语音提供了更丰富、更自然的模态

Conclusion: 语音评估应该被纳入未来的机器翻译评估框架中，因为语音模态提供了更自然和丰富的评估维度

Abstract: Machine Translation (MT) has achieved remarkable performance, with growing
interest in speech translation and multimodal approaches. However, despite
these advancements, MT quality assessment remains largely text centric,
typically relying on human experts who read and compare texts. Since many
real-world MT applications (e.g Google Translate Voice Mode, iFLYTEK
Translator) involve translation being spoken rather printed or read, a more
natural way to assess translation quality would be through speech as opposed
text-only evaluations. This study compares text-only and audio-based
evaluations of 10 MT systems from the WMT General MT Shared Task, using
crowd-sourced judgments collected via Amazon Mechanical Turk. We additionally,
performed statistical significance testing and self-replication experiments to
test reliability and consistency of audio-based approach. Crowd-sourced
assessments based on audio yield rankings largely consistent with text only
evaluations but, in some cases, identify significant differences between
translation systems. We attribute this to speech richer, more natural modality
and propose incorporating speech-based assessments into future MT evaluation
frameworks.

</details>


### [31] [You Are What You Train: Effects of Data Composition on Training Context-aware Machine Translation Models](https://arxiv.org/abs/2509.14031)
*Paweł Mąka,Yusuf Can Semerci,Jan Scholtes,Gerasimos Spanakis*

Main category: cs.CL

TL;DR: 本文验证了训练数据中上下文相关示例的稀疏性是机器翻译模型难以有效利用上下文的关键瓶颈，并通过构建控制比例的训练数据集证实了稀疏性与模型性能的强关联性。


<details>
  <summary>Details</summary>
Motivation: 人类水平的翻译需要利用上下文来确保连贯性和处理复杂现象（如代词消歧）。标准训练数据中上下文丰富示例的稀疏性被认为是模型难以利用上下文的原因，本文旨在系统验证这一假设。

Method: 通过构建具有受控比例上下文相关示例的训练数据集，在单语和多语设置下进行实验。提出并评估了两种训练策略来更好地利用可用数据。

Result: 证实了训练数据稀疏性与模型性能的强关联性，发现不同上下文现象的改进不能相互泛化。提出的训练策略在单语和多语设置下分别实现了ctxPro评估上6%和8%的准确率提升。

Conclusion: 上下文相关训练数据的稀疏性是机器翻译上下文利用的关键瓶颈，需要针对性的训练策略来改善模型性能，且不同上下文现象需要分别处理。

Abstract: Achieving human-level translations requires leveraging context to ensure
coherence and handle complex phenomena like pronoun disambiguation. Sparsity of
contextually rich examples in the standard training data has been hypothesized
as the reason for the difficulty of context utilization. In this work, we
systematically validate this claim in both single- and multilingual settings by
constructing training datasets with a controlled proportions of contextually
relevant examples. We demonstrate a strong association between training data
sparsity and model performance confirming sparsity as a key bottleneck.
Importantly, we reveal that improvements in one contextual phenomenon do no
generalize to others. While we observe some cross-lingual transfer, it is not
significantly higher between languages within the same sub-family. Finally, we
propose and empirically evaluate two training strategies designed to leverage
the available data. These strategies improve context utilization, resulting in
accuracy gains of up to 6 and 8 percentage points on the ctxPro evaluation in
single- and multilingual settings respectively.

</details>


### [32] [Enhancing Multi-Agent Debate System Performance via Confidence Expression](https://arxiv.org/abs/2509.14034)
*Zijie Lin,Bryan Hooi*

Main category: cs.CL

TL;DR: 提出了ConfMAD框架，在多智能体辩论系统中引入置信度表达机制，让LLM能够明确表达置信水平，从而提升辩论效果和系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有MAD系统中，即使某些LLM拥有更优的知识或推理能力，但由于缺乏置信度表达，难以在辩论中清晰展现其优势。不恰当的置信度表达会导致智能体固执坚持错误信念或过早收敛于次优答案，降低辩论效果。

Method: 开发ConfMAD框架，在多智能体辩论过程中集成置信度表达机制，让LLM能够明确传达其置信水平。

Result: 实验结果表明该方法有效，并分析了置信度如何影响辩论动态，为设计置信度感知的MAD系统提供了见解。

Conclusion: 引入置信度表达能够有效提升多智能体辩论系统的性能，ConfMAD框架为解决LLM在辩论中置信度表达问题提供了有效方案。

Abstract: Generative Large Language Models (LLMs) have demonstrated remarkable
performance across a wide range of tasks. Recent research has introduced
Multi-Agent Debate (MAD) systems, which leverage multiple LLMs to simulate
human debate and thereby improve task performance. However, while some LLMs may
possess superior knowledge or reasoning capabilities for specific tasks, they
often struggle to clearly communicate this advantage during debates, in part
due to a lack of confidence expression. Moreover, inappropriate confidence
expression can cause agents in MAD systems to either stubbornly maintain
incorrect beliefs or converge prematurely on suboptimal answers, ultimately
reducing debate effectiveness and overall system performance. To address these
challenges, we propose incorporating confidence expression into MAD systems to
allow LLMs to explicitly communicate their confidence levels. To validate this
approach, we develop ConfMAD, a MAD framework that integrates confidence
expression throughout the debate process. Experimental results demonstrate the
effectiveness of our method, and we further analyze how confidence influences
debate dynamics, offering insights into the design of confidence-aware MAD
systems.

</details>


### [33] [SSL-SSAW: Self-Supervised Learning with Sigmoid Self-Attention Weighting for Question-Based Sign Language Translation](https://arxiv.org/abs/2509.14036)
*Zekang Liu,Wei Feng,Fanhua Shang,Lianyu Hu,Jichao Feng,Liqing Gao*

Main category: cs.CL

TL;DR: 本文提出了基于问题的手语翻译(QB-SLT)新任务，通过引入对话上下文来提升手语翻译质量，并开发了SSL-SSAW跨模态自监督学习方法，在新建数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 手语翻译在聋哑人与听力人群沟通中至关重要，对话能提供重要的上下文线索。相比传统的手语注释(gloss)，对话在自然交流中更容易标注，因此探索如何有效整合对话信息来提升翻译效果。

Method: 提出跨模态自监督学习与Sigmoid自注意力加权(SSL-SSAW)融合方法：1)使用对比学习对齐多模态特征；2)引入SSAW模块自适应提取问题和手语序列特征；3)通过自监督学习利用可用问题文本来增强表示能力。

Result: 在新建的CSL-Daily-QA和PHOENIX-2014T-QA数据集上达到最先进性能。易获取的问题辅助可以达到甚至超越传统gloss辅助的性能。可视化结果证明了融入对话对提升翻译质量的有效性。

Conclusion: 基于问题的对话整合为手语翻译提供了新的有效途径，提出的SSL-SSAW方法成功解决了多模态特征对齐和上下文利用的挑战，证明了对话信息在手语翻译中的重要价值。

Abstract: Sign Language Translation (SLT) bridges the communication gap between deaf
people and hearing people, where dialogue provides crucial contextual cues to
aid in translation. Building on this foundational concept, this paper proposes
Question-based Sign Language Translation (QB-SLT), a novel task that explores
the efficient integration of dialogue. Unlike gloss (sign language
transcription) annotations, dialogue naturally occurs in communication and is
easier to annotate. The key challenge lies in aligning multimodality features
while leveraging the context of the question to improve translation. To address
this issue, we propose a cross-modality Self-supervised Learning with Sigmoid
Self-attention Weighting (SSL-SSAW) fusion method for sign language
translation. Specifically, we employ contrastive learning to align
multimodality features in QB-SLT, then introduce a Sigmoid Self-attention
Weighting (SSAW) module for adaptive feature extraction from question and sign
language sequences. Additionally, we leverage available question text through
self-supervised learning to enhance representation and translation
capabilities. We evaluated our approach on newly constructed CSL-Daily-QA and
PHOENIX-2014T-QA datasets, where SSL-SSAW achieved SOTA performance. Notably,
easily accessible question assistance can achieve or even surpass the
performance of gloss assistance. Furthermore, visualization results demonstrate
the effectiveness of incorporating dialogue in improving translation quality.

</details>


### [34] [Canary-1B-v2 & Parakeet-TDT-0.6B-v3: Efficient and High-Performance Models for Multilingual ASR and AST](https://arxiv.org/abs/2509.14128)
*Monica Sekoyan,Nithin Rao Koluguri,Nune Tadevosyan,Piotr Zelasko,Travis Bartley,Nick Karpov,Jagadeesh Balam,Boris Ginsburg*

Main category: cs.CL

TL;DR: Canary-1B-v2是一个快速、鲁棒的多语言语音识别和语音翻译模型，支持25种欧洲语言，在保持高性能的同时比Whisper-large-v3快10倍


<details>
  <summary>Details</summary>
Motivation: 开发一个高效的多语言语音处理模型，在保持高性能的同时显著提升处理速度，并减少语音识别和翻译中的幻觉问题

Method: 采用FastConformer编码器和Transformer解码器架构，使用两阶段预训练和微调过程，包含动态数据平衡，训练数据总量达170万小时，并加入非语音音频减少幻觉

Result: 在英语ASR上超越Whisper-large-v3且速度快10倍，在多语言ASR和AST任务上与Seamless-M4T-v2-large等大型模型竞争，同时发布了更轻量的Parakeet-TDT-0.6B-v3版本

Conclusion: Canary-1B-v2展示了在语音处理任务中实现高性能和高效率的可行性，nGPT编码器在大规模数据下表现良好，而FastConformer在微调后表现优异，为多语言语音处理提供了实用的解决方案

Abstract: This report introduces Canary-1B-v2, a fast, robust multilingual model for
Automatic Speech Recognition (ASR) and Speech-to-Text Translation (AST). Built
with a FastConformer encoder and Transformer decoder, it supports 25 languages
primarily European. The model was trained on 1.7M hours of total data samples,
including Granary and NeMo ASR Set 3.0, with non-speech audio added to reduce
hallucinations for ASR and AST. We describe its two-stage pre-training and
fine-tuning process with dynamic data balancing, as well as experiments with an
nGPT encoder. Results show nGPT scales well with massive data, while
FastConformer excels after fine-tuning. For timestamps, Canary-1B-v2 uses the
NeMo Forced Aligner (NFA) with an auxiliary CTC model, providing reliable
segment-level timestamps for ASR and AST. Evaluations show Canary-1B-v2
outperforms Whisper-large-v3 on English ASR while being 10x faster, and
delivers competitive multilingual ASR and AST performance against larger models
like Seamless-M4T-v2-large and LLM-based systems. We also release
Parakeet-TDT-0.6B-v3, a successor to v2, offering multilingual ASR across the
same 25 languages with just 600M parameters.

</details>


### [35] [CS-FLEURS: A Massively Multilingual and Code-Switched Speech Dataset](https://arxiv.org/abs/2509.14161)
*Brian Yan,Injy Hamed,Shuichiro Shimizu,Vasista Lodagala,William Chen,Olga Iakovenko,Bashar Talafha,Amir Hussein,Alexander Polok,Kalvin Chang,Dominik Klement,Sara Althubaiti,Puyuan Peng,Matthew Wiesner,Thamar Solorio,Ahmed Ali,Sanjeev Khudanpur,Shinji Watanabe,Chih-Chen Chen,Zhen Wu,Karim Benharrak,Anuj Diwan,Samuele Cornell,Eunjung Yeo,Kwanghee Choi,Carlos Carvalho,Karen Rosero*

Main category: cs.CL

TL;DR: CS-FLEURS是一个新的代码切换语音识别和翻译数据集，包含4个测试集，覆盖113种语言对和52种语言，旨在扩展代码切换语音研究的范围。


<details>
  <summary>Details</summary>
Motivation: 为了解决高资源语言之外的代码切换语音识别和翻译系统的开发和评估需求，特别是在低资源语言场景下。

Method: 构建包含4个测试集的数据集：1）14种X-英语语言对的真实语音合成代码切换句子；2）16种X-英语语言对的生成式文本转语音；3）60种{阿拉伯语、普通话、印地语、西班牙语}-X语言对的生成式文本转语音；4）45种X-英语低资源语言对的拼接式文本转语音。同时提供128小时的训练数据。

Result: 创建了CS-FLEURS数据集，包含113种独特的代码切换语言对，覆盖52种语言，为代码切换语音研究提供了全面的资源。

Conclusion: CS-FLEURS数据集有助于拓宽未来代码切换语音研究的范围，为开发更全面的代码切换语音系统提供了重要资源。

Abstract: We present CS-FLEURS, a new dataset for developing and evaluating
code-switched speech recognition and translation systems beyond high-resourced
languages. CS-FLEURS consists of 4 test sets which cover in total 113 unique
code-switched language pairs across 52 languages: 1) a 14 X-English language
pair set with real voices reading synthetically generated code-switched
sentences, 2) a 16 X-English language pair set with generative text-to-speech
3) a 60 {Arabic, Mandarin, Hindi, Spanish}-X language pair set with the
generative text-to-speech, and 4) a 45 X-English lower-resourced language pair
test set with concatenative text-to-speech. Besides the four test sets,
CS-FLEURS also provides a training set with 128 hours of generative
text-to-speech data across 16 X-English language pairs. Our hope is that
CS-FLEURS helps to broaden the scope of future code-switched speech research.
Dataset link: https://huggingface.co/datasets/byan/cs-fleurs.

</details>


### [36] [AssoCiAm: A Benchmark for Evaluating Association Thinking while Circumventing Ambiguity](https://arxiv.org/abs/2509.14171)
*Yifan Liu,Wenkuan Zhao,Shanshan Zhong,Jinghui Qin,Mingfu Liang,Zhongzhan Huang,Wushao Wen*

Main category: cs.CL

TL;DR: 提出了AssoCiAm基准测试，通过混合计算方法解决关联任务中的歧义问题，评估多模态大语言模型的联想能力，发现认知与关联之间存在强正相关关系。


<details>
  <summary>Details</summary>
Motivation: 现有的关联能力评估框架往往忽视关联任务中固有的歧义性，这种歧义源于关联的发散性，会削弱评估的可靠性。

Method: 将歧义分解为内部歧义和外部歧义，引入AssoCiAm基准测试，采用混合计算方法来规避歧义问题。

Result: 实验显示认知与关联之间存在强正相关关系，歧义的存在使MLLMs的行为更加随机化，验证了该方法能确保更准确可靠的评估。

Conclusion: AssoCiAm基准测试能有效解决关联评估中的歧义问题，为多模态大语言模型的联想能力提供更可靠的评估框架。

Abstract: Recent advancements in multimodal large language models (MLLMs) have garnered
significant attention, offering a promising pathway toward artificial general
intelligence (AGI). Among the essential capabilities required for AGI,
creativity has emerged as a critical trait for MLLMs, with association serving
as its foundation. Association reflects a model' s ability to think creatively,
making it vital to evaluate and understand. While several frameworks have been
proposed to assess associative ability, they often overlook the inherent
ambiguity in association tasks, which arises from the divergent nature of
associations and undermines the reliability of evaluations. To address this
issue, we decompose ambiguity into two types-internal ambiguity and external
ambiguity-and introduce AssoCiAm, a benchmark designed to evaluate associative
ability while circumventing the ambiguity through a hybrid computational
method. We then conduct extensive experiments on MLLMs, revealing a strong
positive correlation between cognition and association. Additionally, we
observe that the presence of ambiguity in the evaluation process causes MLLMs'
behavior to become more random-like. Finally, we validate the effectiveness of
our method in ensuring more accurate and reliable evaluations. See Project Page
for the data and codes.

</details>


### [37] [Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation Framework for Personal Finance LLMs](https://arxiv.org/abs/2509.14180)
*Akhil Theerthala*

Main category: cs.CL

TL;DR: 本文提出了一个新颖的金融咨询框架，通过整合金融背景和行为金融学研究来构建监督数据，训练出的8B参数模型在性能上可比拟14-32B的大型模型，同时成本降低80%。


<details>
  <summary>Details</summary>
Motivation: 现有的金融咨询系统维护成本高且收益不佳，需要一种更有效的方法来提供个性化的金融建议，同时考虑用户目标、约束条件、风险承受能力和司法管辖区。

Method: 开发了一个可复现的框架，整合相关金融背景和行为金融学研究来构建监督数据，创建了19k样本的推理数据集，并对Qwen-3-8B模型进行了全面微调。

Result: 通过保留测试集和盲测LLM评审研究显示，8B模型在事实准确性、流畅性和个性化指标上达到了与14-32B大型基线模型相当的性能，同时成本降低了80%。

Conclusion: 通过精心策划的数据整合和行为金融学集成，较小的模型可以实现与大型模型相当的性能，同时显著降低成本，为个性化金融咨询提供了更高效的解决方案。

Abstract: Personalized financial advice requires consideration of user goals,
constraints, risk tolerance, and jurisdiction. Prior LLM work has focused on
support systems for investors and financial planners. Simultaneously, numerous
recent studies examine broader personal finance tasks, including budgeting,
debt management, retirement, and estate planning, through agentic pipelines
that incur high maintenance costs, yielding less than 25% of their expected
financial returns. In this study, we introduce a novel and reproducible
framework that integrates relevant financial context with behavioral finance
studies to construct supervision data for end-to-end advisors. Using this
framework, we create a 19k sample reasoning dataset and conduct a comprehensive
fine-tuning of the Qwen-3-8B model on the dataset. Through a held-out test
split and a blind LLM-jury study, we demonstrate that through careful data
curation and behavioral integration, our 8B model achieves performance
comparable to significantly larger baselines (14-32B parameters) across factual
accuracy, fluency, and personalization metrics while incurring 80% lower costs
than the larger counterparts.

</details>


### [38] [Framing Migration: A Computational Analysis of UK Parliamentary Discourse](https://arxiv.org/abs/2509.14197)
*Vahid Ghafouri,Robert McNeil,Teodor Yankov,Madeleine Sumption,Luc Rocher,Scott A. Hale,Adam Mahdi*

Main category: cs.CL

TL;DR: 使用大语言模型分析英美议会75年移民话语，发现美国日益极化而英国党派态度相对一致，但保守党与工党意识形态差距在2025年达到最负面水平，且话语转向安全化叙事


<details>
  <summary>Details</summary>
Motivation: 通过大规模计算分析比较英美议会移民话语的长期演变，探究政治话语中的立场变化和叙事框架转变

Method: 使用开源大语言模型标注议会声明中的移民立场，结合半自动化框架提取细粒度叙事框架，进行跨时间和政党的趋势分析

Result: 美国话语日益极化，英国各党派态度相对一致但保守党与工党意识形态差距在2025年达到最负面；英国话语转向边境管控等安全化叙事，社会融合等长期整合框架减少；移民讨论从国内法转向国际法和人权

Conclusion: 大语言模型能够支持政治和历史语境中可扩展的细粒度话语分析，揭示移民话语的长期演变趋势和叙事框架转变

Abstract: We present a large-scale computational analysis of migration-related
discourse in UK parliamentary debates spanning over 75 years and compare it
with US congressional discourse. Using open-weight LLMs, we annotate each
statement with high-level stances toward migrants and track the net tone toward
migrants across time and political parties. For the UK, we extend this with a
semi-automated framework for extracting fine-grained narrative frames to
capture nuances of migration discourse. Our findings show that, while US
discourse has grown increasingly polarised, UK parliamentary attitudes remain
relatively aligned across parties, with a persistent ideological gap between
Labour and the Conservatives, reaching its most negative level in 2025. The
analysis of narrative frames in the UK parliamentary statements reveals a shift
toward securitised narratives such as border control and illegal immigration,
while longer-term integration-oriented frames such as social integration have
declined. Moreover, discussions of national law about immigration have been
replaced over time by international law and human rights, revealing nuances in
discourse trends. Taken together broadly, our findings demonstrate how LLMs can
support scalable, fine-grained discourse analysis in political and historical
contexts.

</details>


### [39] [Apertus: Democratizing Open and Compliant LLMs for Global Language Environments](https://arxiv.org/abs/2509.14233)
*Alejandro Hernández-Cano,Alexander Hägele,Allen Hao Huang,Angelika Romanou,Antoni-Joan Solergibert,Barna Pasztor,Bettina Messmer,Dhia Garbaya,Eduard Frank Ďurech,Ido Hakimi,Juan García Giraldo,Mete Ismayilzada,Negar Foroutan,Skander Moalla,Tiancheng Chen,Vinko Sabolčec,Yixuan Xu,Michael Aerni,Badr AlKhamissi,Ines Altemir Marinas,Mohammad Hossein Amani,Matin Ansaripour,Ilia Badanin,Harold Benoit,Emanuela Boros,Nicholas Browning,Fabian Bösch,Maximilian Böther,Niklas Canova,Camille Challier,Clement Charmillot,Jonathan Coles,Jan Deriu,Arnout Devos,Lukas Drescher,Daniil Dzenhaliou,Maud Ehrmann,Dongyang Fan,Simin Fan,Silin Gao,Miguel Gila,María Grandury,Diba Hashemi,Alexander Hoyle,Jiaming Jiang,Mark Klein,Andrei Kucharavy,Anastasiia Kucherenko,Frederike Lübeck,Roman Machacek,Theofilos Manitaras,Andreas Marfurt,Kyle Matoba,Simon Matrenok,Henrique Mendoncça,Fawzi Roberto Mohamed,Syrielle Montariol,Luca Mouchel,Sven Najem-Meyer,Jingwei Ni,Gennaro Oliva,Matteo Pagliardini,Elia Palme,Andrei Panferov,Léo Paoletti,Marco Passerini,Ivan Pavlov,Auguste Poiroux,Kaustubh Ponkshe,Nathan Ranchin,Javi Rando,Mathieu Sauser,Jakhongir Saydaliev,Muhammad Ali Sayfiddinov,Marian Schneider,Stefano Schuppli,Marco Scialanga,Andrei Semenov,Kumar Shridhar,Raghav Singhal,Anna Sotnikova,Alexander Sternfeld,Ayush Kumar Tarun,Paul Teiletche,Jannis Vamvas,Xiaozhe Yao,Hao Zhao Alexander Ilic,Ana Klimovic,Andreas Krause,Caglar Gulcehre,David Rosenthal,Elliott Ash,Florian Tramèr,Joost VandeVondele,Livio Veraldi,Martin Rajman,Thomas Schulthess,Torsten Hoefler,Antoine Bosselut,Martin Jaggi,Imanol Schlag*

Main category: cs.CL

TL;DR: Apertus是一个完全开源的大型语言模型套件，专注于数据合规性和多语言表示，使用公开可用数据训练，采用Goldfish目标抑制记忆化，支持1800多种语言，在8B和70B规模上达到先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决当前开源模型生态系统中数据合规性不足和多语言代表性不够的问题，避免使用未经授权的内容，确保模型训练的透明度和合法性。

Method: 使用完全公开可用的数据进行预训练，尊重robots.txt排除规则，过滤非许可、有毒和个人身份信息内容；采用Goldfish目标抑制数据记忆；在多语言数据上训练（15T tokens，1800+语言，40%非英语内容）。

Result: Apertus模型在多语言基准测试中达到或超越同类开源权重模型的先进性能，8B和70B规模模型表现优异。

Conclusion: Apertus提供了一个完全透明、合规的开源LLM解决方案，不仅发布模型权重，还包含完整的数据处理脚本、检查点、评估套件和训练代码，支持审计和扩展。

Abstract: We present Apertus, a fully open suite of large language models (LLMs)
designed to address two systemic shortcomings in today's open model ecosystem:
data compliance and multilingual representation. Unlike many prior models that
release weights without reproducible data pipelines or regard for content-owner
rights, Apertus models are pretrained exclusively on openly available data,
retroactively respecting robots.txt exclusions and filtering for
non-permissive, toxic, and personally identifiable content. To mitigate risks
of memorization, we adopt the Goldfish objective during pretraining, strongly
suppressing verbatim recall of data while retaining downstream task
performance. The Apertus models also expand multilingual coverage, training on
15T tokens from over 1800 languages, with ~40% of pretraining data allocated to
non-English content. Released at 8B and 70B scales, Apertus approaches
state-of-the-art results among fully open models on multilingual benchmarks,
rivalling or surpassing open-weight counterparts. Beyond model weights, we
release all scientific artifacts from our development cycle with a permissive
license, including data preparation scripts, checkpoints, evaluation suites,
and training code, enabling transparent audit and extension.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [40] [Explicit Reasoning Makes Better Judges: A Systematic Study on Accuracy, Efficiency, and Robustness](https://arxiv.org/abs/2509.13332)
*Pratik Jayarao,Himanshu Gupta,Neeraj Varshney,Chaitanya Dwivedi*

Main category: cs.AI

TL;DR: 本研究系统比较了思考型和非思考型LLM在LLM-as-a-judge范式中的表现，发现思考型模型在准确率、计算效率和鲁棒性方面均优于非思考型模型，即使经过多种增强策略优化后。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地被用作自动评估工具，确保其可靠性、效率和鲁棒性变得至关重要。本研究旨在系统比较思考型和非思考型LLM在评估任务中的表现差异。

Method: 使用开源Qwen 3模型（0.6B、1.7B和4B参数），在RewardBench任务上评估准确性和计算效率（FLOPs），并测试了多种增强策略：上下文学习、规则引导评估、基于参考的评估和n-best聚合。

Result: 思考型模型准确率高出约10个百分点，计算开销仅增加不到2倍；而增强策略如少样本学习虽然带来适度提升但成本较高（>8倍）。思考型模型在各种偏见条件下保持更好的稳定性（平均高6%），多语言实验也证实了显式推理的优势。

Conclusion: 显式推理在LLM-as-a-judge范式中具有明显优势，不仅在准确性和效率方面，在鲁棒性方面也表现更佳，为LLM作为评估工具的应用提供了系统证据。

Abstract: As Large Language Models (LLMs) are increasingly adopted as automated judges
in benchmarking and reward modeling, ensuring their reliability, efficiency,
and robustness has become critical. In this work, we present a systematic
comparison of "thinking" and "non-thinking" LLMs in the LLM-as-a-judge paradigm
using open-source Qwen 3 models of relatively small sizes (0.6B, 1.7B, and 4B
parameters). We evaluate both accuracy and computational efficiency (FLOPs) on
RewardBench tasks, and further examine augmentation strategies for non-thinking
models, including in-context learning, rubric-guided judging, reference-based
evaluation, and n-best aggregation. Our results show that despite these
enhancements, non-thinking models generally fall short of their thinking
counterparts. Our results show that thinking models achieve approximately 10%
points higher accuracy with little overhead (under 2x), in contrast to
augmentation strategies like few-shot learning, which deliver modest gains at a
higher cost (>8x). Bias and robustness analyses further demonstrate that
thinking models maintain significantly greater consistency under a variety of
bias conditions such as positional, bandwagon, identity, diversity, and random
biases (6% higher on average). We further extend our experiments to the
multilingual setting and our results confirm that explicit reasoning extends
its benefits beyond English. Overall, our work results in several important
findings that provide systematic evidence that explicit reasoning offers clear
advantages in the LLM-as-a-judge paradigm not only in accuracy and efficiency
but also in robustness.

</details>


### [41] [Evaluation Awareness Scales Predictably in Open-Weights Large Language Models](https://arxiv.org/abs/2509.13333)
*Maheep Chaudhary,Ian Su,Nikhil Hooda,Nishith Shankar,Julia Tan,Kevin Zhu,Ashwinee Panda,Ryan Lagasse,Vasu Sharma*

Main category: cs.AI

TL;DR: 研究发现大型语言模型存在评估意识，即模型能区分评估和部署环境，这种能力随模型规模按幂律增长，影响AI安全评估的有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型可能在评估时隐藏危险能力，先前研究仅在单个70B模型中发现评估意识，但不同规模模型的评估意识变化规律尚不清楚。

Method: 使用线性探测方法分析15个不同规模模型（0.27B到70B参数）的激活向量，研究评估意识的缩放规律。

Result: 发现评估意识随模型规模呈幂律增长，这种可预测的缩放规律可用于预测未来更大模型的欺骗行为。

Conclusion: 研究结果揭示了评估意识的缩放定律，为设计规模感知的AI安全评估策略提供了指导，有助于更准确地评估大型语言模型的安全性。

Abstract: Large language models (LLMs) can internally distinguish between evaluation
and deployment contexts, a behaviour known as \emph{evaluation awareness}. This
undermines AI safety evaluations, as models may conceal dangerous capabilities
during testing. Prior work demonstrated this in a single $70$B model, but the
scaling relationship across model sizes remains unknown. We investigate
evaluation awareness across $15$ models scaling from $0.27$B to $70$B
parameters from four families using linear probing on steering vector
activations. Our results reveal a clear power-law scaling: evaluation awareness
increases predictably with model size. This scaling law enables forecasting
deceptive behavior in future larger models and guides the design of scale-aware
evaluation strategies for AI safety. A link to the implementation of this paper
can be found at
https://anonymous.4open.science/r/evaluation-awareness-scaling-laws/README.md.

</details>


### [42] [FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness](https://arxiv.org/abs/2509.13334)
*Anand Swaroop,Akshat Nallani,Saksham Uboweja,Adiliia Uzdenova,Michael Nguyen,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Vasu Sharma,Maheep Chaudhary*

Main category: cs.AI

TL;DR: FRIT是一种通过干预训练实现忠实推理的方法，通过生成忠实/不忠实推理对来训练模型偏好因果一致的推理路径，提高推理的可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的思维链推理方法存在推理步骤与最终答案缺乏因果关联的问题，导致输出脆弱且不可信。虽然已有方法关注忠实性测量，但系统性提升忠实性的方法仍然有限。

Method: 提出FRIT方法：1）在模型生成的思维链中对单个推理步骤进行干预，生成忠实/不忠实推理对；2）应用直接偏好优化训练模型偏好因果一致的推理路径。

Result: 在Qwen3-8B和Mistral-7B-v0.1模型上，FRIT将Mistral在GSM8K上的忠实推理提高了3.4个百分点，准确率提高了7.6个百分点。

Conclusion: FRIT提供了第一个可扩展、无监督的方法来训练语言模型产生更可靠和可解释的推理，解决了推理性能与可信度之间的关键差距。

Abstract: Chain-of-thought (CoT) reasoning has emerged as a powerful tool for improving
large language model performance on complex tasks, but recent work shows that
reasoning steps often fail to causally influence the final answer, creating
brittle and untrustworthy outputs. Prior approaches focus primarily on
measuring faithfulness, while methods for systematically improving it remain
limited. We introduce Faithful Reasoning via Intervention Training (FRIT), a
scalable alignment method that trains models to produce causally consistent
reasoning by learning from systematically corrupted examples. FRIT generates
synthetic training data by intervening on individual reasoning steps in
model-generated CoTs, creating faithful/unfaithful pairs that highlight when
reasoning breaks down. We then apply Direct Preference Optimization to teach
models to prefer causally consistent reasoning paths. Evaluating on Qwen3-8B
and Mistral-7B-v0.1 across factual and symbolic reasoning tasks, FRIT increases
faithful reasoning by $3.4$ percentage points for Mistral on GSM8K while
improving accuracy by $7.6$ percentage points. Our approach provides the first
scalable, supervision-free method for training language models to produce more
reliable and interpretable reasoning, addressing a critical gap between
reasoning performance and trustworthiness. We release our code at
\href{https://github.com/Anut-py/frit}.

</details>


### [43] [Position: AI Safety Must Embrace an Antifragile Perspective](https://arxiv.org/abs/2509.13339)
*Ming Jin,Hyunin Lee*

Main category: cs.AI

TL;DR: 本文主张AI安全研究应采用反脆弱性视角，使系统处理罕见事件和分布外事件的能力随时间增强，而非依赖静态基准测试。


<details>
  <summary>Details</summary>
Motivation: 传统静态基准测试和一次性鲁棒性测试无法应对环境演变和模型漂移问题（如奖励黑客、过度优化等），需要新的方法来确保AI系统在长期开放环境中的可靠性。

Method: 提出反脆弱性方法框架，强调利用当前不确定性来为未来更大不确定性做准备，包括重新校准AI安全测量、基准测试和持续改进的方法论。

Result: 识别了静态测试的关键局限性（场景多样性不足、奖励黑客、过度对齐等），并探讨了反脆弱性解决方案管理罕见事件的潜力。

Conclusion: 反脆弱性方法是确保长期AI安全的关键，需要建立相应的伦理和实践指南来培养反脆弱性AI安全社区，作为现有鲁棒性方法的补充。

Abstract: This position paper contends that modern AI research must adopt an
antifragile perspective on safety -- one in which the system's capacity to
guarantee long-term AI safety such as handling rare or out-of-distribution
(OOD) events expands over time. Conventional static benchmarks and single-shot
robustness tests overlook the reality that environments evolve and that models,
if left unchallenged, can drift into maladaptation (e.g., reward hacking,
over-optimization, or atrophy of broader capabilities). We argue that an
antifragile approach -- Rather than striving to rapidly reduce current
uncertainties, the emphasis is on leveraging those uncertainties to better
prepare for potentially greater, more unpredictable uncertainties in the future
-- is pivotal for the long-term reliability of open-ended ML systems. In this
position paper, we first identify key limitations of static testing, including
scenario diversity, reward hacking, and over-alignment. We then explore the
potential of antifragile solutions to manage rare events. Crucially, we
advocate for a fundamental recalibration of the methods used to measure,
benchmark, and continually improve AI safety over the long term, complementing
existing robustness approaches by providing ethical and practical guidelines
towards fostering an antifragile AI safety community.

</details>


### [44] [Imagined Autocurricula](https://arxiv.org/abs/2509.13341)
*Ahmet H. Güzel,Matthew Thomas Jackson,Jarek Luca Liesen,Tim Rocktäschel,Jakob Nicolaus Foerster,Ilija Bogunovic,Jack Parker-Holder*

Main category: cs.AI

TL;DR: IMAC方法利用世界模型生成想象环境，通过无监督环境设计自动生成课程，在有限数据下训练出能泛化到新任务的鲁棒智能体


<details>
  <summary>Details</summary>
Motivation: 解决现实世界中训练数据稀缺和模拟器不准确的问题，利用离线被动收集数据构建世界模型来生成多样化的训练环境

Method: 提出IMAC（想象自动课程）方法，结合无监督环境设计（UED）在世界模型生成的想象环境中自动生成课程，选择有用的生成数据进行训练

Result: 在具有挑战性的程序生成环境中，仅使用较窄数据集学习的世界模型进行训练，就能在保留环境中实现强大的迁移性能

Conclusion: 该方法为利用更大规模的基础世界模型训练通用智能体开辟了新路径，展示了世界模型在有限数据下训练鲁棒智能体的潜力

Abstract: Training agents to act in embodied environments typically requires vast
training data or access to accurate simulation, neither of which exists for
many cases in the real world. Instead, world models are emerging as an
alternative leveraging offline, passively collected data, they make it possible
to generate diverse worlds for training agents in simulation. In this work, we
harness world models to generate imagined environments to train robust agents
capable of generalizing to novel task variations. One of the challenges in
doing this is ensuring the agent trains on useful generated data. We thus
propose a novel approach, IMAC (Imagined Autocurricula), leveraging
Unsupervised Environment Design (UED), which induces an automatic curriculum
over generated worlds. In a series of challenging, procedurally generated
environments, we show it is possible to achieve strong transfer performance on
held-out environments, having trained only inside a world model learned from a
narrower dataset. We believe this opens the path to utilizing larger-scale,
foundation world models for generally capable agents.

</details>


### [45] [OpenHA: A Series of Open-Source Hierarchical Agentic Models in Minecraft](https://arxiv.org/abs/2509.13347)
*Zihao Wang,Muyao Li,Kaichen He,Xiangyu Wang,Zhancun Mu,Anji Liu,Yitao Liang*

Main category: cs.AI

TL;DR: 本文系统比较了不同动作空间在Minecraft环境中的表现，发现最优动作空间具有任务依赖性，为此提出了Chain of Action (CoA)框架，将高层次规划和低层次控制统一在单一VLA模型中，通过混合动作空间训练实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决动作空间选择这一关键但未解决的挑战，因为研究发现没有单一动作空间在所有任务中都最优，这给构建通用智能体带来了困境。

Method: 提出了Chain of Action (CoA)框架，将抽象动作视为中间推理步骤而非单独策略的命令，在单一VLA模型中统一高层次规划和低层次控制，并使用混合动作空间进行训练。

Result: CoA框架训练的All-in-One智能体学习到了更鲁棒和可泛化的策略，在800多个不同任务上实现了新的最先进性能，超越了专门的基线方法。

Conclusion: CoA框架成功解决了动作空间选择的困境，通过统一规划和控制的端到端方法实现了更好的通用性，并发布了OpenHA套件促进可重复研究。

Abstract: The choice of action spaces is a critical yet unresolved challenge in
developing capable, end-to-end trainable agents. This paper first presents a
large-scale, systematic comparison of prominent abstracted action spaces and
tokenizers for Vision-Language-Action (VLA) or hierarchical agent models in the
open-ended Minecraft. Our analysis reveals that no single action space is
universally optimal; instead, the most effective abstraction is highly
task-dependent, creating a dilemma for building generalist agents. To resolve
this, we introduce Chain of Action (CoA), a novel framework that unifies
high-level planning and low-level control within a single, monolithic VLA
model. CoA treats an abstracted action not as a command for a separate policy,
but as an intermediate reasoning step--akin to a chain of thought--that guides
the generation of the final, executable action. Furthermore, we demonstrate
that an All-in-One agent trained on a diverse mixture of action spaces using
the CoA paradigm learns a more robust and generalizable policy. This unified
agent achieves a new state-of-the-art, improving the overall task success rate
over strong, specialized baselines. To foster reproducible research, we release
the OpenHA (Open Hierarchical Agents) suite, which includes our comprehensive
benchmark of over 800 distinct tasks, curated datasets, source code, and all
pretrained model checkpoints at https://github.com/CraftJarvis/OpenHA

</details>


### [46] [Teaching LLMs to Plan: Logical Chain-of-Thought Instruction Tuning for Symbolic Planning](https://arxiv.org/abs/2509.13351)
*Pulkit Verma,Ngoc La,Anthony Favier,Swaroop Mishra,Julie A. Shah*

Main category: cs.AI

TL;DR: 提出了PDDL-Instruct指令调优框架，通过逻辑思维链推理增强大语言模型的符号规划能力，在标准基准测试中达到94%的规划准确率，相比基线模型提升66%


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多样化任务中表现出色，但在需要形式化表示（如PDDL）的结构化符号规划方面能力有限，需要弥合通用推理能力与自动规划所需逻辑精度之间的差距

Method: 开发指令提示引导模型通过精确的逻辑推理步骤来确定动作适用性、状态转换和计划有效性，将规划过程分解为关于前提条件满足、效果应用和不变性保持的显式推理链

Result: 在多个规划领域的实验结果显示，基于思维链推理的指令调优模型规划能力显著提升，在标准基准测试中达到94%的规划准确率

Conclusion: 该框架成功弥合了大语言模型通用推理能力与自动规划所需逻辑精度之间的差距，为开发更好的AI规划系统提供了有前景的方向

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across
diverse tasks, yet their ability to perform structured symbolic planning
remains limited, particularly in domains requiring formal representations like
the Planning Domain Definition Language (PDDL). In this paper, we present a
novel instruction tuning framework, PDDL-Instruct, designed to enhance LLMs'
symbolic planning capabilities through logical chain-of-thought reasoning. Our
approach focuses on teaching models to rigorously reason about action
applicability, state transitions, and plan validity using explicit logical
inference steps. By developing instruction prompts that guide models through
the precise logical reasoning required to determine when actions can be applied
in a given state, we enable LLMs to self-correct their planning processes
through structured reflection. The framework systematically builds verification
skills by decomposing the planning process into explicit reasoning chains about
precondition satisfaction, effect application, and invariant preservation.
Experimental results on multiple planning domains show that our
chain-of-thought reasoning based instruction-tuned models are significantly
better at planning, achieving planning accuracy of up to 94% on standard
benchmarks, representing a 66% absolute improvement over baseline models. This
work bridges the gap between the general reasoning capabilities of LLMs and the
logical precision required for automated planning, offering a promising
direction for developing better AI planning systems.

</details>


### [47] [Agentic UAVs: LLM-Driven Autonomy with Integrated Tool-Calling and Cognitive Reasoning](https://arxiv.org/abs/2509.13352)
*Anis Koubaa,Khaled Gabr*

Main category: cs.AI

TL;DR: 提出了一个基于大语言模型的无人机自主框架Agentic UAVs，通过五层架构实现上下文感知推理和实时知识访问，在模拟搜救任务中显著提升了检测性能和决策能力


<details>
  <summary>Details</summary>
Motivation: 现有无人机系统主要依赖基于规则的控制和窄AI，缺乏上下文感知推理、自主决策和生态系统集成能力，无法在动态不确定任务中有效适应

Method: 设计五层架构（感知、推理、行动、集成、学习），集成YOLOv11目标检测、GPT-4推理和本地Gemma-3部署，基于ROS2和Gazebo构建原型系统

Result: 在模拟搜救场景中，检测置信度从0.72提升到0.79，人员检测率从75%提升到91%，行动推荐率从4.5%大幅提升到92%

Conclusion: 适度的计算开销能够实现质的自主性提升和生态系统集成，为大语言模型驱动的无人机自主系统提供了可行路径

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly deployed in defense,
surveillance, and disaster response, yet most systems remain confined to SAE
Level 2--3 autonomy. Their reliance on rule-based control and narrow AI
restricts adaptability in dynamic, uncertain missions. Existing UAV frameworks
lack context-aware reasoning, autonomous decision-making, and ecosystem-level
integration; critically, none leverage Large Language Model (LLM) agents with
tool-calling for real-time knowledge access. This paper introduces the Agentic
UAVs framework, a five-layer architecture (Perception, Reasoning, Action,
Integration, Learning) that augments UAVs with LLM-driven reasoning, database
querying, and third-party system interaction. A ROS2 and Gazebo-based prototype
integrates YOLOv11 object detection with GPT-4 reasoning and local Gemma-3
deployment. In simulated search-and-rescue scenarios, agentic UAVs achieved
higher detection confidence (0.79 vs. 0.72), improved person detection rates
(91% vs. 75%), and markedly increased action recommendation (92% vs. 4.5%).
These results confirm that modest computational overhead enables qualitatively
new levels of autonomy and ecosystem integration.

</details>


### [48] [Semantic Fusion with Fuzzy-Membership Features for Controllable Language Modelling](https://arxiv.org/abs/2509.13357)
*Yongchao Huang,Hassan Raza*

Main category: cs.AI

TL;DR: 提出语义融合方案，通过并行模糊成员特征通道增强Transformer语言模型，实现可解释的语义特征编码和可控生成


<details>
  <summary>Details</summary>
Motivation: 为了增强语言模型的语义理解能力，提供可解释的语义特征表示，并实现用户可控的文本生成

Method: 使用可解释特征向量（词性、浅层角色、边界标志、情感极性等）构建语义矩阵，通过门控适配器融合到语言模型中，采用标准的下一个词预测、辅助重建损失和轻量级正则化器

Result: 在合成双子句语料库上，语义融合提高了困惑度，实现了精确的用户可控极性和标点生成，同时保持模型简洁性

Conclusion: 该方法计算开销小，完全兼容输入输出嵌入绑定，为条件自然语言生成提供了可解释的途径

Abstract: We propose semantic fusion, a lightweight scheme that augments a Transformer
language model (LM) with a parallel, fuzzy-membership feature channel that
encodes token-level semantics. Each token is represented by a vector of
interpretable features (e.g. part-of-speech cues, shallow roles, boundary
flags, sentiment polarity and strength) whose values are graded degrees from
differentiable membership functions (e.g. power kernels). These per-token
vectors form a sentence-level semantic matrix fused via a gated adapter into
the LM. Training uses standard next-token prediction, an auxiliary loss that
reconstructs the semantic features from hidden states, and a lightweight
uniformizer that regularizes adjective-class distributions. On a synthetic
two-clause corpus with held-out adjectives for out-of-distribution (OOD)
control, semantic fusion improves perplexity and enables precise,
user-controllable generation of polarity and punctuation while maintaining
model simplicity. This approach adds only small overhead, remains fully
compatible with tied input-output embeddings, and provides an interpretable
pathway for conditioned natural language generation.

</details>


### [49] [Asterisk Operator](https://arxiv.org/abs/2509.13364)
*Zixi Li*

Main category: cs.AI

TL;DR: 提出了星号操作符(*-operator)这一新颖的统一抽象推理框架，基于邻接结构并行传播(ASPP)，将结构化推理任务形式化为由隐式关系图引导的局部并行状态演化过程。


<details>
  <summary>Details</summary>
Motivation: 为了解决抽象推理问题中的全局推理能力与局部计算约束之间的矛盾，需要一个既能保持局部计算效率又能实现全局推理的统一框架。

Method: 基于邻接结构并行传播(ASPP)的星号操作符，将推理任务建模为局部并行状态演化过程，通过隐式关系图进行引导，并提出了Embedding-Asterisk蒸馏方法。

Result: 在ARC2挑战和康威生命游戏中验证了操作符的通用性、收敛性和优异性能，仅用600万参数就在ARC2验证集上达到100%准确率。

Conclusion: 星号操作符为神经符号推理提供了高效且收敛的计算范式，在抽象推理领域实现了重要突破。

Abstract: We propose the \textbf{Asterisk Operator} ($\ast$-operator), a novel unified
framework for abstract reasoning based on Adjacency-Structured Parallel
Propagation (ASPP). The operator formalizes structured reasoning tasks as
local, parallel state evolution processes guided by implicit relational graphs.
We prove that the $\ast$-operator maintains local computational constraints
while achieving global reasoning capabilities, providing an efficient and
convergent computational paradigm for abstract reasoning problems. Through
rigorous mathematical analysis and comprehensive experiments on ARC2 challenges
and Conway's Game of Life, we demonstrate the operator's universality,
convergence properties, and superior performance. Our innovative
Embedding-Asterisk distillation method achieves 100\% accuracy on ARC2
validation with only 6M parameters, representing a significant breakthrough in
neural-symbolic reasoning.
  \textbf{Keywords:} Abstract Reasoning, Adjacency Structure, Parallel
Propagation, Asterisk Operator, Convergence, Universal Approximation

</details>


### [50] [$Agent^2$: An Agent-Generates-Agent Framework for Reinforcement Learning Automation](https://arxiv.org/abs/2509.13368)
*Yuan Wei,Xiaohan Shan,Ran Miao,Jianmin Li*

Main category: cs.AI

TL;DR: Agent²是一个完全自动化的强化学习代理生成框架，通过LLM驱动将自然语言任务描述转换为高性能RL解决方案，无需人工干预，在多个基准测试中表现优于人工设计方案。


<details>
  <summary>Details</summary>
Motivation: 传统RL代理开发需要大量专业知识和迭代周期，失败率高且可访问性有限，需要实现完全自动化的RL代理设计。

Method: 采用双代理架构：生成器代理分析任务并生成可执行RL代理，目标代理是自动生成的RL代理。框架将RL开发分解为MDP建模和算法优化两个阶段，基于模型上下文协议构建。

Result: 在MuJoCo、MetaDrive、MPE和SMAC等多个基准测试中，Agent²始终优于人工设计的解决方案，性能提升最高达55%，平均表现也有显著提升。

Conclusion: 这项工作建立了智能代理设计和优化其他代理的新范式，实现了真正端到端的闭环自动化，是自动化AI系统的根本性突破。

Abstract: Reinforcement learning agent development traditionally requires extensive
expertise and lengthy iterations, often resulting in high failure rates and
limited accessibility. This paper introduces $Agent^2$, a novel
agent-generates-agent framework that achieves fully automated RL agent design
through intelligent LLM-driven generation. The system autonomously transforms
natural language task descriptions and environment code into comprehensive,
high-performance reinforcement learning solutions without human intervention.
$Agent^2$ features a revolutionary dual-agent architecture. The Generator Agent
serves as an autonomous AI designer that analyzes tasks and generates
executable RL agents, while the Target Agent is the resulting automatically
generated RL agent. The framework decomposes RL development into two distinct
stages: MDP modeling and algorithmic optimization, enabling more targeted and
effective agent generation. Built on the Model Context Protocol, $Agent^2$
provides a unified framework that standardizes intelligent agent creation
across diverse environments and algorithms, while incorporating adaptive
training management and intelligent feedback analysis for continuous
improvement. Extensive experiments on a wide range of benchmarks, including
MuJoCo, MetaDrive, MPE, and SMAC, demonstrate that $Agent^2$ consistently
outperforms manually designed solutions across all tasks, achieving up to 55%
performance improvement and substantial gains on average. By enabling truly
end-to-end, closed-loop automation, this work establishes a new paradigm in
which intelligent agents design and optimize other agents, marking a
fundamental breakthrough for automated AI systems.

</details>


### [51] [The Art of Saying "Maybe": A Conformal Lens for Uncertainty Benchmarking in VLMs](https://arxiv.org/abs/2509.13379)
*Asif Azad,Mohammad Sadat Hossain,MD Sadik Hossain Shanto,M Saifur Rahman,Md Rizwan Pervez*

Main category: cs.AI

TL;DR: 本文对16个最先进的视觉语言模型进行了全面的不确定性基准测试研究，发现在多模态系统中，更大的模型表现出更好的不确定性量化能力，数学和推理任务的不确定性表现较差。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉语言模型在复杂视觉理解方面取得了显著进展，但不确定性量化这一关键维度尚未得到足够关注，现有研究仅限于有限设置。

Method: 在6个多模态数据集上评估16个最先进的VLMs（开源和闭源），使用3种不同的评分函数进行全面的不确定性基准测试。

Result: 发现更大的模型始终表现出更好的不确定性量化；知道更多的模型也更清楚自己不知道什么。更确定的模型获得更高的准确率，而数学和推理任务在所有模型中相比其他领域表现出更差的不确定性性能。

Conclusion: 这项工作为多模态系统中可靠的不确定性评估奠定了基础，强调了模型规模和任务类型对不确定性量化性能的重要影响。

Abstract: Vision-Language Models (VLMs) have achieved remarkable progress in complex
visual understanding across scientific and reasoning tasks. While performance
benchmarking has advanced our understanding of these capabilities, the critical
dimension of uncertainty quantification has received insufficient attention.
Therefore, unlike prior conformal prediction studies that focused on limited
settings, we conduct a comprehensive uncertainty benchmarking study, evaluating
16 state-of-the-art VLMs (open and closed-source) across 6 multimodal datasets
with 3 distinct scoring functions. Our findings demonstrate that larger models
consistently exhibit better uncertainty quantification; models that know more
also know better what they don't know. More certain models achieve higher
accuracy, while mathematical and reasoning tasks elicit poorer uncertainty
performance across all models compared to other domains. This work establishes
a foundation for reliable uncertainty evaluation in multimodal systems.

</details>


### [52] [From Next Token Prediction to (STRIPS) World Models -- Preliminary Results](https://arxiv.org/abs/2509.13389)
*Carlos Núñez-Molina,Vicenç Gómez,Hector Geffner*

Main category: cs.AI

TL;DR: 使用Transformer架构从动作序列中学习命题STRIPS世界模型，通过监督式下一个token预测来推断动作前提条件


<details>
  <summary>Details</summary>
Motivation: 从动作轨迹中学习世界模型是AI规划的重要问题，传统方法需要完整的状态信息，而本文旨在仅从动作序列中推断隐藏的世界模型

Method: 将问题建模为监督式下一个token预测任务，使用Transformer架构，通过正负样本（有效和无效动作序列）训练模型学习动作前提条件

Result: 实验表明合适的Transformer架构能够忠实表示命题STRIPS世界模型，仅从随机有效和无效动作序列中即可学习到模型

Conclusion: 深度学习方法（特别是Transformer）能够成功地从动作序列中学习命题STRIPS世界模型，为从观察中学习规划模型提供了新途径

Abstract: We consider the problem of learning propositional STRIPS world models from
action traces alone, using a deep learning architecture (transformers) and
gradient descent. The task is cast as a supervised next token prediction
problem where the tokens are the actions, and an action $a$ may follow an
action sequence if the hidden effects of the previous actions do not make an
action precondition of $a$ false. We show that a suitable transformer
architecture can faithfully represent propositional STRIPS world models, and
that the models can be learned from sets of random valid (positive) and invalid
(negative) action sequences alone. A number of experiments are reported.

</details>


### [53] [SteeringControl: Holistic Evaluation of Alignment Steering in LLMs](https://arxiv.org/abs/2509.13450)
*Vincent Siu,Nicholas Crispino,David Park,Nathan W. Henry,Zhun Wang,Yang Liu,Dawn Song,Chenguang Wang*

Main category: cs.AI

TL;DR: SteeringControl是一个评估表示引导方法的基准，重点关注偏见、有害生成和幻觉等核心对齐目标，以及这些方法对次要行为（如奉承和常识道德）的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的对齐工作主要关注真实性或推理能力来展示表示引导的副作用，但许多权衡关系尚未得到系统性的理解。需要建立一个系统化的评估框架来研究引导方法的有效性和行为纠缠问题。

Method: 构建了一个模块化的引导框架，基于独特的组件作为现有方法的构建块。收集了安全相关的主要和次要行为数据集，围绕五种流行的引导方法进行评估，在Qwen-2.5-7B和Llama-3.1-8B模型上进行实验。

Result: 发现强引导性能取决于引导方法、模型和目标行为的特定组合，不良的组合会导致严重的概念纠缠问题。

Conclusion: 引导方法的有效性具有高度情境依赖性，需要仔细选择方法、模型和目标的组合，以避免意外的负面副作用。研究提供了系统化的评估框架和开源代码来促进这一领域的发展。

Abstract: We introduce SteeringControl, a benchmark for evaluating representation
steering methods across core alignment objectives--bias, harmful generation,
and hallucination--and their effects on secondary behaviors such as sycophancy
and commonsense morality. While prior alignment work often highlights
truthfulness or reasoning ability to demonstrate the side effects of
representation steering, we find there are many unexplored tradeoffs not yet
understood in a systematic way. We collect a dataset of safety-relevant primary
and secondary behaviors to evaluate steering effectiveness and behavioral
entanglement centered around five popular steering methods. To enable this, we
craft a modular steering framework based on unique components that serve as the
building blocks of many existing methods. Our results on Qwen-2.5-7B and
Llama-3.1-8B find that strong steering performance is dependent on the specific
combination of steering method, model, and targeted behavior, and that severe
concept entanglement can result from poor combinations of these three as well.
We release our code here:
https://github.com/wang-research-lab/SteeringControl.git.

</details>


### [54] [AI Agents with Human-Like Collaborative Tools: Adaptive Strategies for Enhanced Problem-Solving](https://arxiv.org/abs/2509.13547)
*Harper Reed,Michael Sugimura,Angelo Zangari*

Main category: cs.AI

TL;DR: 为LLM代理提供类似人类的协作工具和自主性可以显著提升其在最困难编程问题上的性能表现，成本降低15-40%，回合数减少12-27%，完成时间加快12-38%


<details>
  <summary>Details</summary>
Motivation: 研究是否通过赋予LLM代理人类自然使用的协作工具和自主性，能够改善其问题解决性能

Method: 为Claude Code代理配备基于MCP的社交媒体和日志工具，允许其自主使用这些工具，在34个Aider Polyglot Python编程挑战中进行测试

Result: 协作工具显著提升了最困难问题的性能表现，不同模型自然采用了不同的协作策略，代理表现出偏好写作而非阅读（2-9倍），结构化表达是改进的主要驱动力

Conclusion: AI代理在其能力边缘可以系统性地受益于人类启发的协作工具，适应性协作界面应被视为推理增强器而非通用效率提升工具

Abstract: We investigate whether giving LLM agents the collaborative tools and autonomy
that humans naturally use for problem solving can improve their performance. We
equip Claude Code agents with MCP-based social media and journaling tools and
allow them to use these tools as they see fit. Across 34 Aider Polyglot Python
programming challenges, collaborative tools substantially improve performance
on the hardest problems, delivering 15-40% lower cost, 12-27% fewer turns, and
12-38% faster completion than baseline agents. Effects on the full challenge
set are mixed, suggesting these tools act as performance enhancers when
additional reasoning scaffolding is most needed. Surprisingly, Different models
naturally adopted distinct collaborative strategies without explicit
instruction. Sonnet 3.7 engaged broadly across tools and benefited from
articulation-based cognitive scaffolding. Sonnet 4 showed selective adoption,
leaning on journal-based semantic search when problems were genuinely
difficult. This mirrors how human developers adjust collaboration based on
expertise and task complexity. Behavioral analysis shows agents prefer writing
over reading by about 2-9x, indicating that structured articulation drives much
of the improvement rather than information access alone. Overall, AI agents can
systematically benefit from human-inspired collaboration tools at the edge of
their capabilities, pointing to adaptive collaborative interfaces as reasoning
enhancers rather than universal efficiency boosts.

</details>


### [55] [Gen AI in Proof-based Math Courses: A Pilot Study](https://arxiv.org/abs/2509.13570)
*Hannah Klawa,Shraddha Rajpal,Cigole Thomas*

Main category: cs.AI

TL;DR: 本研究调查了本科生在证明数学课程中使用生成式AI的情况，分析学生的使用行为和认知，探讨对数学教学的影响


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在高等教育中的快速兴起和现有AI检测工具的不可靠性，需要制定鼓励学生学习和批判性思维的政策

Method: 通过调查问卷和学生访谈，分析三个证明数学课程（抽象代数、拓扑学）中学生对AI工具的使用情况和认知

Result: 研究揭示了学生如何与AI工具互动，他们对生成式AI有用性和局限性的看法

Conclusion: 讨论了将生成式AI整合到证明数学教学中的未来考虑因素

Abstract: With the rapid rise of generative AI in higher education and the
unreliability of current AI detection tools, developing policies that encourage
student learning and critical thinking has become increasingly important. This
study examines student use and perceptions of generative AI across three
proof-based undergraduate mathematics courses: a first-semester abstract
algebra course, a topology course and a second-semester abstract algebra
course. In each case, course policy permitted some use of generative AI.
Drawing on survey responses and student interviews, we analyze how students
engaged with AI tools, their perceptions of generative AI's usefulness and
limitations, and what implications these perceptions hold for teaching
proof-based mathematics. We conclude by discussing future considerations for
integrating generative AI into proof-based mathematics instruction.

</details>


### [56] [Programmable Cognitive Bias in Social Agents](https://arxiv.org/abs/2509.13588)
*Xuan Liu,Haoyang Shang,Haojian Jin*

Main category: cs.AI

TL;DR: CoBRA是一个用于在基于LLM的社会模拟中系统化指定智能体行为的新工具包，通过显式编程认知偏见来解决传统自然语言描述方法的一致性问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法通过隐式自然语言描述指定智能体行为存在跨模型不一致的问题，且无法准确捕捉描述的细微差别，需要更系统化的行为规范方法。

Method: CoBRA包含两个组件：认知偏见指数（通过经典社会科学实验量化智能体反应）和行为调节引擎（将智能体行为与受控认知偏见对齐），采用基于经典社会实验的显式编程方法。

Result: 评估显示CoBRA能够以模型无关的方式精确编程社会智能体中展示的认知偏见，在技术基准测试中表现良好。

Conclusion: CoBRA提供了一个有效的工具包，能够系统化地指定和调节LLM基社会模拟中智能体的认知偏见行为，解决了传统方法的局限性。

Abstract: This paper introduces CoBRA, a novel toolkit for systematically specifying
agent behavior in LLM-based social simulation. We found that conventional
approaches that specify agent behaviors through implicit natural language
descriptions cannot yield consistent behaviors across models, and the produced
agent behaviors do not capture the nuances of the descriptions. In contrast,
CoBRA presents a new approach to program agents' cognitive biases explicitly,
by grounding agents' expected behaviors using classic social science
experiments. CoBRA has two components: (1) Cognitive Bias Index that measures
the cognitive bias of a social agent, by quantifying the agent's reactions in a
set of validated classical social science experiments; (2) Behavioral
Regulation Engine that aligns the agent's behavior to demonstrate controlled
cognitive bias. We evaluated CoBRA as an HCI toolkit through demonstration and
technical benchmarks. Our results suggest that CoBRA can precisely program the
cognitive bias demonstrated in a social agent in a model-agnostic manner.

</details>


### [57] [See, Think, Act: Teaching Multimodal Agents to Effectively Interact with GUI by Identifying Toggles](https://arxiv.org/abs/2509.13615)
*Zongru Wu,Rui Mao,Zhiyuan Tian,Pengzhou Cheng,Tianjie Ju,Zheng Wu,Lingzhong Dong,Haiyue Sheng,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.AI

TL;DR: 该论文针对多模态代理在GUI切换控制中的不可靠性问题，提出了状态感知推理(StaR)训练方法，显著提升了切换指令执行准确率30%以上。


<details>
  <summary>Details</summary>
Motivation: 现有多模态代理在执行图形用户界面(GUI)切换控制指令时存在不可靠性，特别是在当前切换状态已符合期望状态时表现不佳，这成为GUI控制的关键瓶颈。

Method: 提出状态感知推理(StaR)训练方法，教导代理感知当前切换状态、分析指令中的期望状态，并相应采取行动。构建了包含二进制切换指令的状态控制基准测试。

Result: 在三个多模态代理上的实验表明，StaR能将切换指令执行准确率提升30%以上。在三个公共基准测试上的进一步评估显示，StaR还能提升一般任务性能。动态环境评估突显了StaR在现实应用中的潜力。

Conclusion: StaR方法有效解决了多模态代理在GUI切换控制中的可靠性问题，不仅显著提升了切换指令执行准确率，还增强了代理的一般任务性能，具有实际应用价值。

Abstract: The advent of multimodal agents facilitates effective interaction within
graphical user interface (GUI), especially in ubiquitous GUI control. However,
their inability to reliably execute toggle control instructions remains a key
bottleneck. To investigate this, we construct a state control benchmark with
binary toggle instructions from public datasets. Evaluations of existing agents
demonstrate their unreliability, particularly when the current toggle state
already matches the desired state. To address the challenge, we propose
State-aware Reasoning (StaR), a training method that teaches agents to perceive
the current toggle state, analyze the desired state from the instruction, and
act accordingly. Experiments on three multimodal agents demonstrate that StaR
can improve toggle instruction execution accuracy by over 30\%. Further
evaluations on three public benchmarks show that StaR also enhances general
task performance. Finally, evaluations on a dynamic environment highlight the
potential of StaR for real-world applications. Code, benchmark, and
StaR-enhanced agents are available at https://github.com/ZrW00/StaR.

</details>


### [58] [InfraMind: A Novel Exploration-based GUI Agentic Framework for Mission-critical Industrial Management](https://arxiv.org/abs/2509.13704)
*Liangtao Lin,Zhaomeng Zhu,Tianwei Zhang,Yonggang Wen*

Main category: cs.AI

TL;DR: InfraMind是一个专门为工业管理系统设计的基于探索的GUI代理框架，通过五个创新模块解决现有LLM-based GUI代理在工业管理中的五大挑战，显著提升了任务成功率和操作效率。


<details>
  <summary>Details</summary>
Motivation: 工业基础设施管理软件面临系统复杂性增加、多供应商集成和专家操作员短缺等挑战。现有的RPA自动化方案灵活性有限且维护成本高，而通用LLM-based GUI代理在工业管理环境中面临元素理解、精度效率、状态定位、部署约束和安全要求等五大关键挑战。

Method: 提出InfraMind框架，包含五个核心模块：(1)基于系统搜索探索和虚拟机快照的自主GUI理解；(2)内存驱动规划确保高精度高效任务执行；(3)高级状态识别用于分层界面的鲁棒定位；(4)结构化知识蒸馏实现轻量级模型高效部署；(5)多层安全机制保护敏感操作。

Result: 在开源和商业DCIM平台上的大量实验表明，该方法在任务成功率和操作效率方面持续优于现有框架。

Conclusion: InfraMind为工业管理自动化提供了一个严谨且可扩展的解决方案，有效解决了工业环境中GUI自动化的关键挑战。

Abstract: Mission-critical industrial infrastructure, such as data centers,
increasingly depends on complex management software. Its operations, however,
pose significant challenges due to the escalating system complexity,
multi-vendor integration, and a shortage of expert operators. While Robotic
Process Automation (RPA) offers partial automation through handcrafted scripts,
it suffers from limited flexibility and high maintenance costs. Recent advances
in Large Language Model (LLM)-based graphical user interface (GUI) agents have
enabled more flexible automation, yet these general-purpose agents face five
critical challenges when applied to industrial management, including unfamiliar
element understanding, precision and efficiency, state localization, deployment
constraints, and safety requirements. To address these issues, we propose
InfraMind, a novel exploration-based GUI agentic framework specifically
tailored for industrial management systems. InfraMind integrates five
innovative modules to systematically resolve different challenges in industrial
management: (1) systematic search-based exploration with virtual machine
snapshots for autonomous understanding of complex GUIs; (2) memory-driven
planning to ensure high-precision and efficient task execution; (3) advanced
state identification for robust localization in hierarchical interfaces; (4)
structured knowledge distillation for efficient deployment with lightweight
models; and (5) comprehensive, multi-layered safety mechanisms to safeguard
sensitive operations. Extensive experiments on both open-source and commercial
DCIM platforms demonstrate that our approach consistently outperforms existing
frameworks in terms of task success rate and operational efficiency, providing
a rigorous and scalable solution for industrial management automation.

</details>


### [59] [THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning](https://arxiv.org/abs/2509.13761)
*Qikai Chang,Zhenrong Zhang,Pengfei Hu,Jiefeng Ma,Yicheng Pan,Jianshu Zhang,Jun Du,Quan Liu,Jianqing Gao*

Main category: cs.AI

TL;DR: THOR是一个通过强化学习实现工具集成层次优化的框架，用于提升LLM在数学推理和计算任务中的性能，通过多智能体数据生成、分层优化和自校正机制实现最先进效果


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学推理方面取得显著进展，但在高精度任务如数值计算和符号操作方面仍存在困难。现有工具集成方法面临三个关键挑战：构建工具集成推理数据、进行细粒度优化以及增强推理能力

Method: 提出THOR框架：1) TIRGen多智能体actor-critic管道构建高质量工具集成推理数据集；2) 分层RL策略联合优化轨迹级问题解决和步骤级代码生成；3) 自校正机制利用工具反馈动态修正推理路径

Result: 方法在多样化模型上表现出强泛化能力，在数学基准测试上达到同规模模型的最先进性能，在代码基准测试上也取得一致改进

Conclusion: THOR通过创新的工具集成和分层优化方法，有效解决了LLM在高精度数学任务中的局限性，为工具增强的推理提供了新的解决方案

Abstract: Large Language Models (LLMs) have made remarkable progress in mathematical
reasoning, but still continue to struggle with high-precision tasks like
numerical computation and formal symbolic manipulation. Integrating external
tools has emerged as a promising approach to bridge this gap. Despite recent
advances, existing methods struggle with three key challenges: constructing
tool-integrated reasoning data, performing fine-grained optimization, and
enhancing inference. To overcome these limitations, we propose THOR
(Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen,
a multi-agent actor-critic-based pipeline for constructing high-quality
datasets of tool-integrated reasoning paths, aligning with the policy and
generalizing well across diverse models. Second, to perform fine-grained
hierarchical optimization, we introduce an RL strategy that jointly optimizes
for both trajectory-level problem solving and step-level code generation. This
is motivated by our key insight that the success of an intermediate tool call
is a strong predictor of the final answer's correctness. Finally, THOR
incorporates a self-correction mechanism that leverages immediate tool feedback
to dynamically revise erroneous reasoning paths during inference. Our approach
demonstrates strong generalization across diverse models, performing
effectively in both reasoning and non-reasoning models. It further achieves
state-of-the-art performance for models of a similar scale on multiple
mathematical benchmarks, while also delivering consistent improvements on code
benchmarks. Our code will be publicly available at
https://github.com/JingMog/THOR.

</details>


### [60] [MIRA: Empowering One-Touch AI Services on Smartphones with MLLM-based Instruction Recommendation](https://arxiv.org/abs/2509.13773)
*Zhipeng Bian,Jieming Zhu,Xuyang Xie,Quanyu Dai,Zhou Zhao,Zhenhua Dong*

Main category: cs.AI

TL;DR: MIRA是一个智能手机AI任务指令推荐框架，通过长按图像或文本来提供上下文相关的AI任务建议，使用MLLM和结构化推理来提升推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI技术的快速发展，智能手机需要更直观的方式来访问预定义的AI服务，简化用户与设备的交互。

Method: 采用多模态大语言模型推荐管道，结合结构化推理提取关键实体和推断用户意图；使用模板增强推理机制提高任务推断准确性；基于前缀树的约束解码策略确保输出与预定义指令一致。

Result: 通过真实世界标注数据集和用户研究评估，MIRA在指令推荐准确性方面显示出显著提升。

Conclusion: MIRA有潜力彻底改变用户在智能手机上与AI服务的交互方式，提供更无缝和高效的体验。

Abstract: The rapid advancement of generative AI technologies is driving the
integration of diverse AI-powered services into smartphones, transforming how
users interact with their devices. To simplify access to predefined AI
services, this paper introduces MIRA, a pioneering framework for task
instruction recommendation that enables intuitive one-touch AI tasking on
smartphones. With MIRA, users can long-press on images or text objects to
receive contextually relevant instruction recommendations for executing AI
tasks. Our work introduces three key innovations: 1) A multimodal large
language model (MLLM)-based recommendation pipeline with structured reasoning
to extract key entities, infer user intent, and generate precise instructions;
2) A template-augmented reasoning mechanism that integrates high-level
reasoning templates, enhancing task inference accuracy; 3) A prefix-tree-based
constrained decoding strategy that restricts outputs to predefined instruction
candidates, ensuring coherent and intent-aligned suggestions. Through
evaluation using a real-world annotated datasets and a user study, MIRA has
demonstrated substantial improvements in the accuracy of instruction
recommendation. The encouraging results highlight MIRA's potential to
revolutionize the way users engage with AI services on their smartphones,
offering a more seamless and efficient experience.

</details>


### [61] [An Exhaustive DPLL Approach to Model Counting over Integer Linear Constraints with Simplification Techniques](https://arxiv.org/abs/2509.13880)
*Mingwei Zhang,Zhenhao Gu,Liangda Fang,Cunjing Ge,Ziliang Chen,Zhao-Rong Lai,Quanlong Guan*

Main category: cs.AI

TL;DR: 本文提出了一种基于DPLL架构的精确方法来解决整数线性约束的模型计数问题(MCILC)，通过整合混合整数规划中的简化技术显著提升了效率。


<details>
  <summary>Details</summary>
Motivation: 整数线性约束是计算机科学、运筹学和优化领域中最基本的约束之一，许多应用都归结为MCILC任务。现有方法在处理这类问题时效率有限，需要更有效的解决方案。

Method: 采用基于穷举DPLL架构的精确方法，整合了混合整数规划中的多种有效简化技术来提升效率。

Result: 在2840个随机基准测试和4131个应用基准测试中，该方法解决了1718个随机实例（优于现有最佳方法的1470个），并且是唯一能够解决所有4131个应用实例的方法。

Conclusion: 所提出的基于DPLL架构并整合混合整数规划简化技术的方法，在MCILC问题上显著优于所有现有精确方法，特别是在应用实例上表现出色。

Abstract: Linear constraints are one of the most fundamental constraints in fields such
as computer science, operations research and optimization. Many applications
reduce to the task of model counting over integer linear constraints (MCILC).
In this paper, we design an exact approach to MCILC based on an exhaustive DPLL
architecture. To improve the efficiency, we integrate several effective
simplification techniques from mixed integer programming into the architecture.
We compare our approach to state-of-the-art MCILC counters and propositional
model counters on 2840 random and 4131 application benchmarks. Experimental
results show that our approach significantly outperforms all exact methods in
random benchmarks solving 1718 instances while the state-of-the-art approach
only computes 1470 instances. In addition, our approach is the only approach to
solve all 4131 application instances.

</details>


### [62] [Exploring Major Transitions in the Evolution of Biological Cognition With Artificial Neural Networks](https://arxiv.org/abs/2509.13968)
*Konstantinos Voudouris,Andrew Barron,Marta Halina,Colin Klein,Matishalin Patel*

Main category: cs.AI

TL;DR: 该研究使用人工神经网络模型验证信息流结构变化能否带来认知性能的跃迁式提升，发现循环网络相比前馈网络在处理复杂语法时表现出质的性能提升，并观察到训练难度形成的过渡障碍。


<details>
  <summary>Details</summary>
Motivation: 探讨认知进化是否通过生物神经网络信息流结构的主要转变来实现，验证网络拓扑变化是否能产生认知性能的过渡性变化。

Method: 使用理想化信息流模型和人工神经网络，比较前馈、循环和分层拓扑网络在学习不同复杂度人工语法时的性能，控制网络规模和资源。

Result: 循环网络相比前馈网络在处理输入类型上有质的扩展，在最复杂语法学习上表现出质的性能提升；循环网络训练难度构成过渡障碍和不可逆性；分层网络在语法学习上未表现出优势。

Conclusion: 某些信息流结构变化确实能带来认知性能的过渡性转变，支持认知进化可能通过神经网络信息流的主要转变来实现的假设。

Abstract: Transitional accounts of evolution emphasise a few changes that shape what is
evolvable, with dramatic consequences for derived lineages. More recently it
has been proposed that cognition might also have evolved via a series of major
transitions that manipulate the structure of biological neural networks,
fundamentally changing the flow of information. We used idealised models of
information flow, artificial neural networks (ANNs), to evaluate whether
changes in information flow in a network can yield a transitional change in
cognitive performance. We compared networks with feed-forward, recurrent and
laminated topologies, and tested their performance learning artificial grammars
that differed in complexity, controlling for network size and resources. We
documented a qualitative expansion in the types of input that recurrent
networks can process compared to feed-forward networks, and a related
qualitative increase in performance for learning the most complex grammars. We
also noted how the difficulty in training recurrent networks poses a form of
transition barrier and contingent irreversibility -- other key features of
evolutionary transitions. Not all changes in network topology confer a
performance advantage in this task set. Laminated networks did not outperform
non-laminated networks in grammar learning. Overall, our findings show how some
changes in information flow can yield transitions in cognitive performance.

</details>


### [63] [CrowdAgent: Multi-Agent Managed Multi-Source Annotation System](https://arxiv.org/abs/2509.14030)
*Maosheng Qin,Renyu Zhu,Mingxuan Xia,Chenkai Chen,Zhen Zhu,Minmin Lin,Junbo Zhao,Lu Xu,Changjie Fan,Runze Wu,Haobo Wang*

Main category: cs.AI

TL;DR: CrowdAgent是一个多智能体系统，通过整合任务分配、数据标注和质量/成本管理，为LLM、SLM和人类专家提供端到端的协同标注流程控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注标注步骤本身，缺乏对多样化标注源（LLM、SLM、人类专家）的动态管理和复杂调度需求，需要统一的端到端流程控制来解决质量-成本权衡问题。

Method: 基于现实众包公司启发，开发多智能体系统CrowdAgent，实现任务分配、数据标注、质量/成本管理的集成，采用新颖的方法论来合理分配任务，使不同标注源在协作标注流程中协同工作。

Result: 在六个多样化多模态分类任务上进行了广泛实验，证明了CrowdAgent的有效性。

Conclusion: CrowdAgent提供了一个统一的端到端解决方案，能够有效管理多样化标注源，解决复杂调度和质量-成本权衡问题，为高质量标注数据生成提供了新方法。

Abstract: High-quality annotated data is a cornerstone of modern Natural Language
Processing (NLP). While recent methods begin to leverage diverse annotation
sources-including Large Language Models (LLMs), Small Language Models (SLMs),
and human experts-they often focus narrowly on the labeling step itself. A
critical gap remains in the holistic process control required to manage these
sources dynamically, addressing complex scheduling and quality-cost trade-offs
in a unified manner. Inspired by real-world crowdsourcing companies, we
introduce CrowdAgent, a multi-agent system that provides end-to-end process
control by integrating task assignment, data annotation, and quality/cost
management. It implements a novel methodology that rationally assigns tasks,
enabling LLMs, SLMs, and human experts to advance synergistically in a
collaborative annotation workflow. We demonstrate the effectiveness of
CrowdAgent through extensive experiments on six diverse multimodal
classification tasks. The source code and video demo are available at
https://github.com/QMMMS/CrowdAgent.

</details>


### [64] [Hierarchical Learning for Maze Navigation: Emergence of Mental Representations via Second-Order Learning](https://arxiv.org/abs/2509.14195)
*Shalima Binta Manir,Tim Oates*

Main category: cs.AI

TL;DR: 本文通过构建包含GCN作为一阶学习器和MLP控制器作为二阶学习器的分层架构，实证验证了二阶学习能促进环境-认知同构的心理表征出现，从而显著提升迷宫导航任务的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 心理表征（结构化内部模型反映外部环境）是高级认知的基础，但难以实证研究。现有理论假设二阶学习（调整一阶学习机制的学习）能促进这种环境-认知同构性的出现，但缺乏实证验证。

Method: 提出分层架构：使用图卷积网络（GCN）作为一阶学习器直接映射节点特征到最优导航路径预测，使用MLP控制器作为二阶学习器在遇到结构新颖的迷宫环境时动态调整GCN参数。

Result: 定量和定性结果表明，当认知系统发展出与环境结构同构的内部心理地图时，二阶学习特别有效，在未见过的迷宫任务上表现出显著的性能提升和强大的泛化能力。

Conclusion: 研究为结构化心理表征在最大化二阶学习效果中的关键作用提供了实证支持，验证了环境-认知同构性对高级认知功能的重要性。

Abstract: Mental representation, characterized by structured internal models mirroring
external environments, is fundamental to advanced cognition but remains
challenging to investigate empirically. Existing theory hypothesizes that
second-order learning -- learning mechanisms that adapt first-order learning
(i.e., learning about the task/domain) -- promotes the emergence of such
environment-cognition isomorphism. In this paper, we empirically validate this
hypothesis by proposing a hierarchical architecture comprising a Graph
Convolutional Network (GCN) as a first-order learner and an MLP controller as a
second-order learner. The GCN directly maps node-level features to predictions
of optimal navigation paths, while the MLP dynamically adapts the GCN's
parameters when confronting structurally novel maze environments. We
demonstrate that second-order learning is particularly effective when the
cognitive system develops an internal mental map structurally isomorphic to the
environment. Quantitative and qualitative results highlight significant
performance improvements and robust generalization on unseen maze tasks,
providing empirical support for the pivotal role of structured mental
representations in maximizing the effectiveness of second-order learning.

</details>
