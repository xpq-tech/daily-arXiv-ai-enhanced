{"id": "2510.15948", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.15948", "abs": "https://arxiv.org/abs/2510.15948", "authors": ["MingSheng Li", "Guangze Zhao", "Sichen Liu"], "title": "VisuoAlign: Safety Alignment of LVLMs with Multimodal Tree Search", "comment": null, "summary": "Large Vision-Language Models (LVLMs) have achieved remarkable progress in\nmultimodal perception and generation, yet their safety alignment remains a\ncritical challenge.Existing defenses and vulnerable to multimodal jailbreaks,\nas visual inputs introduce new attack surfaces, reasoning chains lack safety\nsupervision, and alignment often degrades under modality fusion.To overcome\nthese limitation, we propose VisuoAlign, a framework for multi-modal safety\nalignment via prompt-guided tree search.VisuoAlign embeds safety constrains\ninto the reasoning process through visual-textual interactive prompts, employs\nMonte Carlo Tree Search(MCTS) to systematically construct diverse\nsafety-critical prompt trajectories, and introduces prompt-based scaling to\nensure real-time risk detection and compliant responses.Extensive experiments\ndemonstrate that VisuoAlign proactively exposes risks, enables comprehensive\ndataset generation, and significantly improves the robustness of LVLMs against\ncomplex cross-modal threats.", "AI": {"tldr": "VisuoAlign\u662f\u4e00\u4e2a\u901a\u8fc7\u63d0\u793a\u5f15\u5bfc\u6811\u641c\u7d22\u5b9e\u73b0\u591a\u6a21\u6001\u5b89\u5168\u5bf9\u9f50\u7684\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u6a21\u6001\u5a01\u80c1\u4e0b\u7684\u5b89\u5168\u5bf9\u9f50\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u5bf9\u591a\u6a21\u6001\u8d8a\u72f1\u653b\u51fb\u8106\u5f31\uff0c\u56e0\u4e3a\u89c6\u89c9\u8f93\u5165\u5f15\u5165\u4e86\u65b0\u7684\u653b\u51fb\u9762\uff0c\u63a8\u7406\u94fe\u7f3a\u4e4f\u5b89\u5168\u76d1\u7763\uff0c\u4e14\u6a21\u6001\u878d\u5408\u901a\u5e38\u4f1a\u964d\u4f4e\u5bf9\u9f50\u6548\u679c\u3002", "method": "\u901a\u8fc7\u89c6\u89c9-\u6587\u672c\u4ea4\u4e92\u63d0\u793a\u5c06\u5b89\u5168\u7ea6\u675f\u5d4c\u5165\u63a8\u7406\u8fc7\u7a0b\uff0c\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u6784\u5efa\u591a\u6837\u5316\u7684\u5b89\u5168\u5173\u952e\u63d0\u793a\u8f68\u8ff9\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u63d0\u793a\u7684\u7f29\u653e\u786e\u4fdd\u5b9e\u65f6\u98ce\u9669\u68c0\u6d4b\u548c\u5408\u89c4\u54cd\u5e94\u3002", "result": "\u5b9e\u9a8c\u8868\u660eVisuoAlign\u80fd\u591f\u4e3b\u52a8\u66b4\u9732\u98ce\u9669\uff0c\u5b9e\u73b0\u5168\u9762\u7684\u6570\u636e\u96c6\u751f\u6210\uff0c\u5e76\u663e\u8457\u63d0\u9ad8LVLMs\u5bf9\u6297\u590d\u6742\u8de8\u6a21\u6001\u5a01\u80c1\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "VisuoAlign\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5b89\u5168\u5bf9\u9f50\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5b89\u5168\u9632\u62a4\u673a\u5236\u3002"}}
{"id": "2510.15952", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15952", "abs": "https://arxiv.org/abs/2510.15952", "authors": ["Myung Ho Kim"], "title": "Executable Epistemology: The Structured Cognitive Loop as an Architecture of Intentional Understanding", "comment": "27 pages", "summary": "Large language models exhibit intelligence without genuine epistemic\nunderstanding, exposing a key gap: the absence of epistemic architecture. This\npaper introduces the Structured Cognitive Loop (SCL) as an executable\nepistemological framework for emergent intelligence. Unlike traditional AI\nresearch asking \"what is intelligence?\" (ontological), SCL asks \"under what\nconditions does cognition emerge?\" (epistemological). Grounded in philosophy of\nmind and cognitive phenomenology, SCL bridges conceptual philosophy and\nimplementable cognition. Drawing on process philosophy, enactive cognition, and\nextended mind theory, we define intelligence not as a property but as a\nperformed process -- a continuous loop of judgment, memory, control, action,\nand regulation. SCL makes three contributions. First, it operationalizes\nphilosophical insights into computationally interpretable structures, enabling\n\"executable epistemology\" -- philosophy as structural experiment. Second, it\nshows that functional separation within cognitive architecture yields more\ncoherent and interpretable behavior than monolithic prompt based systems,\nsupported by agent evaluations. Third, it redefines intelligence: not\nrepresentational accuracy but the capacity to reconstruct its own epistemic\nstate through intentional understanding. This framework impacts philosophy of\nmind, epistemology, and AI. For philosophy, it allows theories of cognition to\nbe enacted and tested. For AI, it grounds behavior in epistemic structure\nrather than statistical regularity. For epistemology, it frames knowledge not\nas truth possession but as continuous reconstruction within a\nphenomenologically coherent loop. We situate SCL within debates on cognitive\nphenomenology, emergence, normativity, and intentionality, arguing that real\nprogress requires not larger models but architectures that realize cognitive\nprinciples structurally.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u7ed3\u6784\u5316\u8ba4\u77e5\u5faa\u73af\uff08SCL\uff09\u4f5c\u4e3a\u53ef\u6267\u884c\u7684\u8ba4\u77e5\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u771f\u6b63\u8ba4\u77e5\u67b6\u6784\u7684\u95ee\u9898\u3002SCL\u5c06\u54f2\u5b66\u89c1\u89e3\u8f6c\u5316\u4e3a\u53ef\u8ba1\u7b97\u7ed3\u6784\uff0c\u901a\u8fc7\u529f\u80fd\u5206\u79bb\u7684\u8ba4\u77e5\u67b6\u6784\u4ea7\u751f\u66f4\u8fde\u8d2f\u7684\u884c\u4e3a\uff0c\u5e76\u91cd\u65b0\u5b9a\u4e49\u667a\u80fd\u4e3a\u901a\u8fc7\u610f\u5411\u6027\u7406\u89e3\u91cd\u5efa\u81ea\u8eab\u8ba4\u77e5\u72b6\u6001\u7684\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u51fa\u667a\u80fd\u4f46\u7f3a\u4e4f\u771f\u6b63\u7684\u8ba4\u77e5\u7406\u89e3\uff0c\u66b4\u9732\u51fa\u8ba4\u77e5\u67b6\u6784\u7684\u7f3a\u5931\u3002\u4f20\u7edfAI\u7814\u7a76\u5173\u6ce8\"\u4ec0\u4e48\u662f\u667a\u80fd\"\u7684\u672c\u4f53\u8bba\u95ee\u9898\uff0c\u800cSCL\u5173\u6ce8\"\u5728\u4ec0\u4e48\u6761\u4ef6\u4e0b\u8ba4\u77e5\u4f1a\u51fa\u73b0\"\u7684\u8ba4\u8bc6\u8bba\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u5fc3\u667a\u54f2\u5b66\u548c\u8ba4\u77e5\u73b0\u8c61\u5b66\uff0c\u7ed3\u5408\u8fc7\u7a0b\u54f2\u5b66\u3001\u5177\u8eab\u8ba4\u77e5\u548c\u6269\u5c55\u5fc3\u667a\u7406\u8bba\uff0c\u5c06\u667a\u80fd\u5b9a\u4e49\u4e3a\u6267\u884c\u8fc7\u7a0b\u800c\u975e\u5c5e\u6027\u2014\u2014\u5305\u542b\u5224\u65ad\u3001\u8bb0\u5fc6\u3001\u63a7\u5236\u3001\u884c\u52a8\u548c\u8c03\u8282\u7684\u8fde\u7eed\u5faa\u73af\u3002\u901a\u8fc7\u529f\u80fd\u5206\u79bb\u7684\u8ba4\u77e5\u67b6\u6784\u5b9e\u73b0\"\u53ef\u6267\u884c\u7684\u8ba4\u8bc6\u8bba\"\u3002", "result": "SCL\u5c55\u793a\u4e86\u529f\u80fd\u5206\u79bb\u7684\u8ba4\u77e5\u67b6\u6784\u6bd4\u5355\u4e00\u63d0\u793a\u7cfb\u7edf\u4ea7\u751f\u66f4\u8fde\u8d2f\u548c\u53ef\u89e3\u91ca\u7684\u884c\u4e3a\uff0c\u5f97\u5230\u4e86\u667a\u80fd\u4f53\u8bc4\u4f30\u7684\u652f\u6301\u3002\u8be5\u6846\u67b6\u5c06\u667a\u80fd\u91cd\u65b0\u5b9a\u4e49\u4e3a\u901a\u8fc7\u610f\u5411\u6027\u7406\u89e3\u91cd\u5efa\u81ea\u8eab\u8ba4\u77e5\u72b6\u6001\u7684\u80fd\u529b\u3002", "conclusion": "SCL\u4e3a\u5fc3\u667a\u54f2\u5b66\u3001\u8ba4\u8bc6\u8bba\u548cAI\u9886\u57df\u5e26\u6765\u91cd\u8981\u5f71\u54cd\uff1a\u8ba9\u8ba4\u77e5\u7406\u8bba\u80fd\u591f\u88ab\u6267\u884c\u548c\u6d4b\u8bd5\uff1b\u5c06\u884c\u4e3a\u5efa\u7acb\u5728\u8ba4\u77e5\u7ed3\u6784\u800c\u975e\u7edf\u8ba1\u89c4\u5f8b\u4e0a\uff1b\u5c06\u77e5\u8bc6\u89c6\u4e3a\u5728\u73b0\u8c61\u5b66\u8fde\u8d2f\u5faa\u73af\u4e2d\u7684\u6301\u7eed\u91cd\u5efa\u3002\u771f\u6b63\u7684\u8fdb\u6b65\u9700\u8981\u5b9e\u73b0\u8ba4\u77e5\u539f\u5219\u7684\u7ed3\u6784\u5316\u67b6\u6784\uff0c\u800c\u975e\u66f4\u5927\u7684\u6a21\u578b\u3002"}}
{"id": "2510.15959", "categories": ["cs.AI", "cs.CY", "cs.ET", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.15959", "abs": "https://arxiv.org/abs/2510.15959", "authors": ["Isabelle Hupont", "Marisa Ponti", "Sven Schade"], "title": "Exploring the Potential of Citiverses for Regulatory Learning", "comment": "26 pages", "summary": "Citiverses hold the potential to support regulatory learning by offering\nimmersive, virtual environments for experimenting with policy scenarios and\ntechnologies. This paper proposes a science-for-policy agenda to explore the\npotential of citiverses as experimentation spaces for regulatory learning,\ngrounded in a consultation with a high-level panel of experts, including\npolicymakers from the European Commission, national government science advisers\nand leading researchers in digital regulation and virtual worlds. It identifies\nkey research areas, including scalability, real-time feedback, complexity\nmodelling, cross-border collaboration, risk reduction, citizen participation,\nethical considerations and the integration of emerging technologies. In\naddition, the paper analyses a set of experimental topics, spanning\ntransportation, urban planning and the environment/climate crisis, that could\nbe tested in citiverse platforms to advance regulatory learning in these areas.\nThe proposed work is designed to inform future research for policy and\nemphasizes a responsible approach to developing and using citiverses. It\nprioritizes careful consideration of the ethical, economic, ecological and\nsocial dimensions of different regulations. The paper also explores essential\npreliminary steps necessary for integrating citiverses into the broader\necosystems of experimentation spaces, including test beds, living labs and\nregulatory sandboxes", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5c06\u57ce\u5e02\u865a\u62df\u4e16\u754c(citiverses)\u4f5c\u4e3a\u76d1\u7ba1\u5b66\u4e60\u5b9e\u9a8c\u7a7a\u95f4\u7684\u79d1\u5b66\u653f\u7b56\u8bae\u7a0b\uff0c\u901a\u8fc7\u4e13\u5bb6\u54a8\u8be2\u8bc6\u522b\u4e86\u5173\u952e\u7814\u7a76\u9886\u57df\u548c\u5b9e\u9a8c\u4e3b\u9898\uff0c\u5f3a\u8c03\u8d1f\u8d23\u4efb\u7684\u53d1\u5c55\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u57ce\u5e02\u865a\u62df\u4e16\u754c\u4f5c\u4e3a\u6c89\u6d78\u5f0f\u5b9e\u9a8c\u73af\u5883\u7684\u6f5c\u529b\uff0c\u652f\u6301\u653f\u7b56\u5236\u5b9a\u8005\u6d4b\u8bd5\u76d1\u7ba1\u573a\u666f\u548c\u6280\u672f\uff0c\u4fc3\u8fdb\u76d1\u7ba1\u5b66\u4e60\u3002", "method": "\u57fa\u4e8e\u4e0e\u6b27\u76df\u59d4\u5458\u4f1a\u653f\u7b56\u5236\u5b9a\u8005\u3001\u56fd\u5bb6\u653f\u5e9c\u79d1\u5b66\u987e\u95ee\u548c\u6570\u5b57\u76d1\u7ba1\u9886\u57df\u4e13\u5bb6\u7684\u9ad8\u5c42\u4e13\u5bb6\u54a8\u8be2\uff0c\u8bc6\u522b\u5173\u952e\u7814\u7a76\u9886\u57df\u548c\u5b9e\u9a8c\u4e3b\u9898\u3002", "result": "\u786e\u5b9a\u4e86\u53ef\u6269\u5c55\u6027\u3001\u5b9e\u65f6\u53cd\u9988\u3001\u590d\u6742\u6027\u5efa\u6a21\u3001\u8de8\u5883\u534f\u4f5c\u7b49\u5173\u952e\u7814\u7a76\u9886\u57df\uff0c\u4ee5\u53ca\u4ea4\u901a\u3001\u57ce\u5e02\u89c4\u5212\u3001\u73af\u5883/\u6c14\u5019\u5371\u673a\u7b49\u5b9e\u9a8c\u4e3b\u9898\u3002", "conclusion": "\u57ce\u5e02\u865a\u62df\u4e16\u754c\u6709\u6f5c\u529b\u6210\u4e3a\u76d1\u7ba1\u5b66\u4e60\u7684\u91cd\u8981\u5b9e\u9a8c\u7a7a\u95f4\uff0c\u4f46\u9700\u8981\u8d1f\u8d23\u4efb\u5730\u53d1\u5c55\uff0c\u8003\u8651\u4f26\u7406\u3001\u7ecf\u6d4e\u3001\u751f\u6001\u548c\u793e\u4f1a\u7ef4\u5ea6\uff0c\u5e76\u6574\u5408\u5230\u66f4\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u751f\u6001\u7cfb\u7edf\u3002"}}
{"id": "2510.15966", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15966", "abs": "https://arxiv.org/abs/2510.15966", "authors": ["Shian Jia", "Ziyang Huang", "Xinbo Wang", "Haofei Zhang", "Mingli Song"], "title": "PISA: A Pragmatic Psych-Inspired Unified Memory System for Enhanced AI Agency", "comment": null, "summary": "Memory systems are fundamental to AI agents, yet existing work often lacks\nadaptability to diverse tasks and overlooks the constructive and task-oriented\nrole of AI agent memory. Drawing from Piaget's theory of cognitive development,\nwe propose PISA, a pragmatic, psych-inspired unified memory system that\naddresses these limitations by treating memory as a constructive and adaptive\nprocess. To enable continuous learning and adaptability, PISA introduces a\ntrimodal adaptation mechanism (i.e., schema updation, schema evolution, and\nschema creation) that preserves coherent organization while supporting flexible\nmemory updates. Building on these schema-grounded structures, we further design\na hybrid memory access architecture that seamlessly integrates symbolic\nreasoning with neural retrieval, significantly improving retrieval accuracy and\nefficiency. Our empirical evaluation, conducted on the existing LOCOMO\nbenchmark and our newly proposed AggQA benchmark for data analysis tasks,\nconfirms that PISA sets a new state-of-the-art by significantly enhancing\nadaptability and long-term knowledge retention.", "AI": {"tldr": "\u63d0\u51fa\u4e86PISA\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u57fa\u4e8e\u76ae\u4e9a\u6770\u8ba4\u77e5\u53d1\u5c55\u7406\u8bba\uff0c\u901a\u8fc7\u4e09\u6a21\u6001\u9002\u5e94\u673a\u5236\u548c\u6df7\u5408\u8bb0\u5fc6\u8bbf\u95ee\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86AI\u4ee3\u7406\u7684\u9002\u5e94\u6027\u548c\u957f\u671f\u77e5\u8bc6\u4fdd\u6301\u80fd\u529b\u3002", "motivation": "\u73b0\u6709AI\u4ee3\u7406\u8bb0\u5fc6\u7cfb\u7edf\u7f3a\u4e4f\u5bf9\u591a\u6837\u5316\u4efb\u52a1\u7684\u9002\u5e94\u6027\uff0c\u5ffd\u89c6\u4e86\u8bb0\u5fc6\u7684\u5efa\u6784\u6027\u548c\u4efb\u52a1\u5bfc\u5411\u4f5c\u7528\u3002", "method": "\u91c7\u7528\u4e09\u6a21\u6001\u9002\u5e94\u673a\u5236\uff08\u56fe\u5f0f\u66f4\u65b0\u3001\u56fe\u5f0f\u6f14\u5316\u548c\u56fe\u5f0f\u521b\u5efa\uff09\u4fdd\u6301\u8bb0\u5fc6\u7ec4\u7ec7\u8fde\u8d2f\u6027\uff0c\u8bbe\u8ba1\u7ed3\u5408\u7b26\u53f7\u63a8\u7406\u4e0e\u795e\u7ecf\u68c0\u7d22\u7684\u6df7\u5408\u8bb0\u5fc6\u8bbf\u95ee\u67b6\u6784\u3002", "result": "\u5728LOCOMO\u57fa\u51c6\u548c\u65b0\u63d0\u51fa\u7684AggQA\u6570\u636e\u5206\u6790\u57fa\u51c6\u4e0a\uff0cPISA\u521b\u9020\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9002\u5e94\u6027\u548c\u957f\u671f\u77e5\u8bc6\u4fdd\u6301\u3002", "conclusion": "PISA\u8bb0\u5fc6\u7cfb\u7edf\u901a\u8fc7\u5efa\u6784\u6027\u8bb0\u5fc6\u65b9\u6cd5\u548c\u6df7\u5408\u67b6\u6784\uff0c\u6709\u6548\u89e3\u51b3\u4e86AI\u4ee3\u7406\u8bb0\u5fc6\u7cfb\u7edf\u7684\u9002\u5e94\u6027\u548c\u77e5\u8bc6\u4fdd\u6301\u95ee\u9898\u3002"}}
{"id": "2510.15972", "categories": ["cs.CL", "cs.AI", "81P68 (Primary), 68T50, 68T07 (Secondary)", "I.2.7; F.1.2"], "pdf": "https://arxiv.org/pdf/2510.15972", "abs": "https://arxiv.org/abs/2510.15972", "authors": ["Ling Sun", "Peter Sullivan", "Michael Martin", "Yun Zhou"], "title": "Quantum NLP models on Natural Language Inference", "comment": "Accepted, presented, and to appear in the Proceedings of the Quantum\n  AI and NLP 2025 Conference", "summary": "Quantum natural language processing (QNLP) offers a novel approach to\nsemantic modeling by embedding compositional structure directly into quantum\ncircuits. This paper investigates the application of QNLP models to the task of\nNatural Language Inference (NLI), comparing quantum, hybrid, and classical\ntransformer-based models under a constrained few-shot setting. Using the lambeq\nlibrary and the DisCoCat framework, we construct parameterized quantum circuits\nfor sentence pairs and train them for both semantic relatedness and inference\nclassification. To assess efficiency, we introduce a novel\ninformation-theoretic metric, Information Gain per Parameter (IGPP), which\nquantifies learning dynamics independent of model size. Our results demonstrate\nthat quantum models achieve performance comparable to classical baselines while\noperating with dramatically fewer parameters. The Quantum-based models\noutperform randomly initialized transformers in inference and achieve lower\ntest error on relatedness tasks. Moreover, quantum models exhibit significantly\nhigher per-parameter learning efficiency (up to five orders of magnitude more\nthan classical counterparts), highlighting the promise of QNLP in low-resource,\nstructure-sensitive settings. To address circuit-level isolation and promote\nparameter sharing, we also propose a novel cluster-based architecture that\nimproves generalization by tying gate parameters to learned word clusters\nrather than individual tokens.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u91cf\u5b50\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5728\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u6bd4\u8f83\u4e86\u91cf\u5b50\u3001\u6df7\u5408\u548c\u7ecf\u5178\u6a21\u578b\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u4fe1\u606f\u589e\u76ca\u53c2\u6570\u6bd4\u6307\u6807\u6765\u8bc4\u4f30\u6a21\u578b\u6548\u7387\u3002", "motivation": "\u63a2\u7d22\u91cf\u5b50\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5728\u8bed\u4e49\u5efa\u6a21\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u7ed3\u6784\u654f\u611f\u7684\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\uff0c\u901a\u8fc7\u91cf\u5b50\u7535\u8def\u76f4\u63a5\u5d4c\u5165\u7ec4\u5408\u7ed3\u6784\u6765\u63d0\u5347\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528lambeq\u5e93\u548cDisCoCat\u6846\u67b6\u6784\u5efa\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\u5904\u7406\u53e5\u5b50\u5bf9\uff0c\u8bad\u7ec3\u8bed\u4e49\u76f8\u5173\u6027\u548c\u63a8\u7406\u5206\u7c7b\u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u805a\u7c7b\u7684\u67b6\u6784\u6765\u4fc3\u8fdb\u53c2\u6570\u5171\u4eab\u3002", "result": "\u91cf\u5b50\u6a21\u578b\u5728\u53c2\u6570\u6570\u91cf\u5927\u5e45\u51cf\u5c11\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u4e0e\u7ecf\u5178\u57fa\u7ebf\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u4f18\u4e8e\u968f\u673a\u521d\u59cb\u5316\u7684transformer\uff0c\u5728\u76f8\u5173\u4efb\u52a1\u4e0a\u6d4b\u8bd5\u8bef\u5dee\u66f4\u4f4e\uff0c\u53c2\u6570\u5b66\u4e60\u6548\u7387\u6bd4\u7ecf\u5178\u6a21\u578b\u9ad8\u51fa\u6700\u591a\u4e94\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u91cf\u5b50\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5728\u4f4e\u8d44\u6e90\u3001\u7ed3\u6784\u654f\u611f\u7684\u73af\u5883\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u91cf\u5b50\u6a21\u578b\u5c55\u73b0\u51fa\u66f4\u9ad8\u7684\u53c2\u6570\u5b66\u4e60\u6548\u7387\uff0c\u4e3aQNLP\u5728\u8bed\u4e49\u5efa\u6a21\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2510.15974", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15974", "abs": "https://arxiv.org/abs/2510.15974", "authors": ["Chris Su", "Harrison Li", "Matheus Marques", "George Flint", "Kevin Zhu", "Sunishchal Dev"], "title": "Limits of Emergent Reasoning of Large Language Models in Agentic Frameworks for Deterministic Games", "comment": null, "summary": "Recent work reports that Large Reasoning Models (LRMs) undergo a collapse in\nperformance on solving puzzles beyond certain perplexity thresholds. In\nsubsequent discourse, questions have arisen as to whether the nature of the\ntask muddles an evaluation of true reasoning. One potential confound is the\nrequirement that the model keep track of the state space on its own. We provide\na large language model (LLM) with an environment interface for Tower of Hanoi\nproblems, allowing it to make a move with a tool call, provide written\njustification, observe the resulting state space, and reprompt itself for the\nnext move. We observe that access to an environment interface does not delay or\neradicate performance collapse. Furthermore, LLM-parameterized policy analysis\nreveals increasing divergence from both optimal policies and uniformly random\npolicies, suggesting that the model exhibits mode-like collapse at each level\nof complexity, and that performance is dependent upon whether the mode reflects\nthe correct solution for the problem. We suggest that a similar phenomena might\ntake place in LRMs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u89e3\u51b3\u8d85\u8fc7\u7279\u5b9a\u590d\u6742\u5ea6\u9608\u503c\u7684\u8c1c\u9898\u65f6\u4f1a\u51fa\u73b0\u6027\u80fd\u5d29\u6e83\u3002\u5373\u4f7f\u4e3a\u6a21\u578b\u63d0\u4f9b\u73af\u5883\u63a5\u53e3\u6765\u8ddf\u8e2a\u72b6\u6001\u7a7a\u95f4\uff0c\u4e5f\u65e0\u6cd5\u5ef6\u8fdf\u6216\u6d88\u9664\u8fd9\u79cd\u6027\u80fd\u5d29\u6e83\u3002", "motivation": "\u63a2\u8ba8\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u89e3\u51b3\u590d\u6742\u8c1c\u9898\u65f6\u6027\u80fd\u5d29\u6e83\u7684\u771f\u6b63\u539f\u56e0\uff0c\u7279\u522b\u662f\u6392\u9664\u6a21\u578b\u9700\u8981\u81ea\u884c\u8ddf\u8e2a\u72b6\u6001\u7a7a\u95f4\u8fd9\u4e00\u6f5c\u5728\u6df7\u6dc6\u56e0\u7d20\u3002", "method": "\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u6c49\u8bfa\u5854\u95ee\u9898\u7684\u73af\u5883\u63a5\u53e3\uff0c\u5141\u8bb8\u6a21\u578b\u901a\u8fc7\u5de5\u5177\u8c03\u7528\u8fdb\u884c\u79fb\u52a8\u3001\u63d0\u4f9b\u4e66\u9762\u7406\u7531\u3001\u89c2\u5bdf\u7ed3\u679c\u72b6\u6001\u7a7a\u95f4\uff0c\u5e76\u91cd\u65b0\u63d0\u793a\u81ea\u5df1\u8fdb\u884c\u4e0b\u4e00\u6b65\u79fb\u52a8\u3002", "result": "\u73af\u5883\u63a5\u53e3\u7684\u8bbf\u95ee\u5e76\u4e0d\u80fd\u5ef6\u8fdf\u6216\u6d88\u9664\u6027\u80fd\u5d29\u6e83\u3002\u7b56\u7565\u5206\u6790\u663e\u793a\u6a21\u578b\u4e0e\u6700\u4f18\u7b56\u7565\u548c\u968f\u673a\u7b56\u7565\u7684\u504f\u79bb\u5ea6\u90fd\u5728\u589e\u52a0\uff0c\u8868\u660e\u6a21\u578b\u5728\u6bcf\u4e2a\u590d\u6742\u5ea6\u7ea7\u522b\u90fd\u8868\u73b0\u51fa\u6a21\u5f0f\u5d29\u6e83\u3002", "conclusion": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u6027\u80fd\u5d29\u6e83\u73b0\u8c61\u53ef\u80fd\u6e90\u4e8e\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u7684\u6a21\u5f0f\u5d29\u6e83\uff0c\u6027\u80fd\u53d6\u51b3\u4e8e\u6a21\u578b\u9009\u62e9\u7684\u6a21\u5f0f\u662f\u5426\u4e0e\u95ee\u9898\u7684\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u4e00\u81f4\u3002"}}
{"id": "2510.16057", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16057", "abs": "https://arxiv.org/abs/2510.16057", "authors": ["Md Kamrul Siam", "Md Jobair Hossain Faruk", "Jerry Q. Cheng", "Huanying Gu"], "title": "Fusion-Augmented Large Language Models: Boosting Diagnostic Trustworthiness via Model Consensus", "comment": "7 pages (Accepted to IEEE BHI 2025)", "summary": "This study presents a novel multi-model fusion framework leveraging two\nstate-of-the-art large language models (LLMs), ChatGPT and Claude, to enhance\nthe reliability of chest X-ray interpretation on the CheXpert dataset. From the\nfull CheXpert corpus of 224,316 chest radiographs, we randomly selected 234\nradiologist-annotated studies to evaluate unimodal performance using image-only\nprompts. In this setting, ChatGPT and Claude achieved diagnostic accuracies of\n62.8% and 76.9%, respectively. A similarity-based consensus approach, using a\n95% output similarity threshold, improved accuracy to 77.6%. To assess the\nimpact of multimodal inputs, we then generated synthetic clinical notes\nfollowing the MIMIC-CXR template and evaluated a separate subset of 50 randomly\nselected cases paired with both images and synthetic text. On this multimodal\ncohort, performance improved to 84% for ChatGPT and 76% for Claude, while\nconsensus accuracy reached 91.3%. Across both experimental conditions,\nagreement-based fusion consistently outperformed individual models. These\nfindings highlight the utility of integrating complementary modalities and\nusing output-level consensus to improve the trustworthiness and clinical\nutility of AI-assisted radiological diagnosis, offering a practical path to\nreduce diagnostic errors with minimal computational overhead.", "AI": {"tldr": "\u63d0\u51fa\u591a\u6a21\u578b\u878d\u5408\u6846\u67b6\uff0c\u7ed3\u5408ChatGPT\u548cClaude\u4e24\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u76f8\u4f3c\u6027\u5171\u8bc6\u65b9\u6cd5\u63d0\u5347\u80f8\u90e8X\u5149\u7247\u8bca\u65ad\u7684\u53ef\u9760\u6027\uff0c\u5728CheXpert\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u63d0\u9ad8AI\u8f85\u52a9\u653e\u5c04\u5b66\u8bca\u65ad\u7684\u53ef\u9760\u6027\u548c\u4e34\u5e8a\u5b9e\u7528\u6027\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u548c\u8f93\u51fa\u5171\u8bc6\u6765\u51cf\u5c11\u8bca\u65ad\u9519\u8bef\u3002", "method": "\u4f7f\u7528ChatGPT\u548cClaude\u4e24\u4e2aLLM\u6a21\u578b\uff0c\u5728CheXpert\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002\u91c7\u7528\u5355\u6a21\u6001\uff08\u4ec5\u56fe\u50cf\uff09\u548c\u591a\u6a21\u6001\uff08\u56fe\u50cf+\u5408\u6210\u4e34\u5e8a\u7b14\u8bb0\uff09\u8f93\u5165\uff0c\u901a\u8fc795%\u8f93\u51fa\u76f8\u4f3c\u6027\u9608\u503c\u7684\u5171\u8bc6\u65b9\u6cd5\u8fdb\u884c\u6a21\u578b\u878d\u5408\u3002", "result": "\u5355\u6a21\u6001\u8bbe\u7f6e\u4e0b\uff0cChatGPT\u548cClaude\u51c6\u786e\u7387\u5206\u522b\u4e3a62.8%\u548c76.9%\uff0c\u5171\u8bc6\u65b9\u6cd5\u63d0\u5347\u81f377.6%\u3002\u591a\u6a21\u6001\u8bbe\u7f6e\u4e0b\uff0cChatGPT\u548cClaude\u51c6\u786e\u7387\u5206\u522b\u63d0\u5347\u81f384%\u548c76%\uff0c\u5171\u8bc6\u51c6\u786e\u7387\u8fbe\u523091.3%\u3002\u5171\u8bc6\u878d\u5408\u5728\u6240\u6709\u5b9e\u9a8c\u6761\u4ef6\u4e0b\u5747\u4f18\u4e8e\u5355\u4e2a\u6a21\u578b\u3002", "conclusion": "\u6574\u5408\u4e92\u8865\u6a21\u6001\u548c\u4f7f\u7528\u8f93\u51fa\u7ea7\u5171\u8bc6\u53ef\u4ee5\u6709\u6548\u63d0\u9ad8AI\u8f85\u52a9\u653e\u5c04\u5b66\u8bca\u65ad\u7684\u53ef\u4fe1\u5ea6\u548c\u4e34\u5e8a\u6548\u7528\uff0c\u4e3a\u51cf\u5c11\u8bca\u65ad\u9519\u8bef\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u6700\u5c0f\u3002"}}
{"id": "2510.15980", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15980", "abs": "https://arxiv.org/abs/2510.15980", "authors": ["Dong Liu", "Yanxuan Yu"], "title": "Cognitive Load Traces as Symbolic and Visual Accounts of Deep Model Cognition", "comment": null, "summary": "We propose \\textbf{Cognitive Load Traces} (CLTs) as a mid-level\ninterpretability framework for deep models, inspired by Cognitive Load Theory\nin human cognition. CLTs are defined as symbolic, temporally varying functions\nthat quantify model-internal resource allocation. Formally, we represent CLTs\nas a three-component stochastic process $(\\mathrm{IL}_t, \\mathrm{EL}_t,\n\\mathrm{GL}_t)$, corresponding to \\emph{Intrinsic}, \\emph{Extraneous}, and\n\\emph{Germane} load. Each component is instantiated through measurable proxies\nsuch as attention entropy, KV-cache miss ratio, representation dispersion, and\ndecoding stability. We propose both symbolic formulations and visualization\nmethods (load curves, simplex diagrams) that enable interpretable analysis of\nreasoning dynamics. Experiments on reasoning and planning benchmarks show that\nCLTs predict error-onset, reveal cognitive strategies, and enable load-guided\ninterventions that improve reasoning efficiency by 15-30\\% while maintaining\naccuracy.", "AI": {"tldr": "\u63d0\u51faCognitive Load Traces (CLTs)\u4f5c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u4e2d\u5c42\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u53d7\u4eba\u7c7b\u8ba4\u77e5\u8d1f\u8377\u7406\u8bba\u542f\u53d1\u3002CLTs\u91cf\u5316\u6a21\u578b\u5185\u90e8\u8d44\u6e90\u5206\u914d\uff0c\u5305\u542b\u5185\u5728\u3001\u5916\u5728\u548c\u5173\u8054\u8d1f\u8377\u4e09\u4e2a\u5206\u91cf\uff0c\u901a\u8fc7\u53ef\u6d4b\u91cf\u4ee3\u7406\u5b9e\u73b0\uff0c\u80fd\u591f\u9884\u6d4b\u9519\u8bef\u53d1\u751f\u3001\u63ed\u793a\u8ba4\u77e5\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u8d1f\u8377\u5f15\u5bfc\u5e72\u9884\u63d0\u9ad8\u63a8\u7406\u6548\u738715-30%\u3002", "motivation": "\u53d7\u4eba\u7c7b\u8ba4\u77e5\u8d1f\u8377\u7406\u8bba\u542f\u53d1\uff0c\u65e8\u5728\u4e3a\u6df1\u5ea6\u6a21\u578b\u5f00\u53d1\u4e00\u4e2a\u4e2d\u5c42\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u4ee5\u91cf\u5316\u6a21\u578b\u5185\u90e8\u7684\u8d44\u6e90\u5206\u914d\u52a8\u6001\uff0c\u4ece\u800c\u66f4\u597d\u5730\u7406\u89e3\u548c\u6539\u8fdb\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u5b9a\u4e49CLTs\u4e3a\u5305\u542b\u5185\u5728\u8d1f\u8377(IL_t)\u3001\u5916\u5728\u8d1f\u8377(EL_t)\u548c\u5173\u8054\u8d1f\u8377(GL_t)\u4e09\u4e2a\u5206\u91cf\u7684\u968f\u673a\u8fc7\u7a0b\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u71b5\u3001KV\u7f13\u5b58\u672a\u547d\u4e2d\u7387\u3001\u8868\u793a\u5206\u6563\u5ea6\u548c\u89e3\u7801\u7a33\u5b9a\u6027\u7b49\u53ef\u6d4b\u91cf\u4ee3\u7406\u6765\u5b9e\u4f8b\u5316\uff0c\u5e76\u63d0\u51fa\u7b26\u53f7\u5316\u516c\u5f0f\u548c\u53ef\u89c6\u5316\u65b9\u6cd5\uff08\u8d1f\u8377\u66f2\u7ebf\u3001\u5355\u7eaf\u5f62\u56fe\uff09\u3002", "result": "\u5728\u63a8\u7406\u548c\u89c4\u5212\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCLTs\u80fd\u591f\u9884\u6d4b\u9519\u8bef\u53d1\u751f\u3001\u63ed\u793a\u8ba4\u77e5\u7b56\u7565\uff0c\u901a\u8fc7\u8d1f\u8377\u5f15\u5bfc\u5e72\u9884\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u5c06\u63a8\u7406\u6548\u7387\u63d0\u9ad815-30%\u3002", "conclusion": "CLTs\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u6846\u67b6\u6765\u5206\u6790\u548c\u6539\u8fdb\u6df1\u5ea6\u6a21\u578b\u7684\u63a8\u7406\u52a8\u6001\uff0c\u901a\u8fc7\u91cf\u5316\u8ba4\u77e5\u8d1f\u8377\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u6027\u80fd\u4f18\u5316\u3002"}}
{"id": "2510.16062", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16062", "abs": "https://arxiv.org/abs/2510.16062", "authors": ["Guiyao Tie", "Zenghui Yuan", "Zeli Zhao", "Chaoran Hu", "Tianhe Gu", "Ruihang Zhang", "Sizhe Zhang", "Junran Wu", "Xiaoyue Tu", "Ming Jin", "Qingsong Wen", "Lixing Chen", "Pan Zhou", "Lichao Sun"], "title": "Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs", "comment": "38 pages, 25 figures, 8 tables", "summary": "Self-correction of large language models (LLMs) emerges as a critical\ncomponent for enhancing their reasoning performance. Although various\nself-correction methods have been proposed, a comprehensive evaluation of these\nmethods remains largely unexplored, and the question of whether LLMs can truly\ncorrect themselves is a matter of significant interest and concern. In this\nstudy, we introduce CorrectBench, a benchmark developed to evaluate the\neffectiveness of self-correction strategies, including intrinsic, external, and\nfine-tuned approaches, across three tasks: commonsense reasoning, mathematical\nreasoning, and code generation. Our findings reveal that: 1) Self-correction\nmethods can improve accuracy, especially for complex reasoning tasks; 2) Mixing\ndifferent self-correction strategies yields further improvements, though it\nreduces efficiency; 3) Reasoning LLMs (e.g., DeepSeek-R1) have limited\noptimization under additional self-correction methods and have high time costs.\nInterestingly, a comparatively simple chain-of-thought (CoT) baseline\ndemonstrates competitive accuracy and efficiency. These results underscore the\npotential of self-correction to enhance LLM's reasoning performance while\nhighlighting the ongoing challenge of improving their efficiency. Consequently,\nwe advocate for further research focused on optimizing the balance between\nreasoning capabilities and operational efficiency. Project Page:\nhttps://correctbench.github.io/", "AI": {"tldr": "CorrectBench\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30LLM\u81ea\u6821\u6b63\u65b9\u6cd5\uff0c\u53d1\u73b0\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u81ea\u6821\u6b63\u80fd\u63d0\u5347\u51c6\u786e\u6027\uff0c\u6df7\u5408\u7b56\u7565\u6548\u679c\u66f4\u597d\u4f46\u6548\u7387\u964d\u4f4e\uff0c\u63a8\u7406\u578bLLM\u81ea\u6821\u6b63\u4f18\u5316\u6709\u9650\u4e14\u8017\u65f6\u9ad8\uff0c\u7b80\u5355\u7684CoT\u57fa\u7ebf\u8868\u73b0\u7ade\u4e89\u529b\u5f3a\u3002", "motivation": "\u867d\u7136\u5df2\u6709\u591a\u79cdLLM\u81ea\u6821\u6b63\u65b9\u6cd5\u88ab\u63d0\u51fa\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u7684\u7efc\u5408\u8bc4\u4f30\u4ecd\u5f88\u7f3a\u4e4f\uff0cLLM\u662f\u5426\u80fd\u771f\u6b63\u81ea\u6211\u6821\u6b63\u662f\u4e00\u4e2a\u91cd\u8981\u4f46\u672a\u5145\u5206\u7814\u7a76\u7684\u95ee\u9898\u3002", "method": "\u5f00\u53d1CorrectBench\u57fa\u51c6\uff0c\u8bc4\u4f30\u5185\u5728\u3001\u5916\u90e8\u548c\u5fae\u8c03\u4e09\u79cd\u81ea\u6821\u6b63\u7b56\u7565\u5728\u5e38\u8bc6\u63a8\u7406\u3001\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4e09\u4e2a\u4efb\u52a1\u4e0a\u7684\u6548\u679c\u3002", "result": "\u81ea\u6821\u6b63\u65b9\u6cd5\u80fd\u63d0\u9ad8\u51c6\u786e\u6027\uff08\u7279\u522b\u662f\u590d\u6742\u63a8\u7406\u4efb\u52a1\uff09\uff1b\u6df7\u5408\u4e0d\u540c\u81ea\u6821\u6b63\u7b56\u7565\u53ef\u8fdb\u4e00\u6b65\u6539\u8fdb\u4f46\u964d\u4f4e\u6548\u7387\uff1b\u63a8\u7406\u578bLLM\u5728\u989d\u5916\u81ea\u6821\u6b63\u65b9\u6cd5\u4e0b\u4f18\u5316\u6709\u9650\u4e14\u65f6\u95f4\u6210\u672c\u9ad8\uff1b\u7b80\u5355\u7684CoT\u57fa\u7ebf\u5c55\u73b0\u51fa\u7ade\u4e89\u6027\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u81ea\u6821\u6b63\u6709\u6f5c\u529b\u63d0\u5347LLM\u63a8\u7406\u6027\u80fd\uff0c\u4f46\u6548\u7387\u4f18\u5316\u4ecd\u662f\u6301\u7eed\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u5728\u63a8\u7406\u80fd\u529b\u548c\u64cd\u4f5c\u6548\u7387\u4e4b\u95f4\u5bfb\u6c42\u5e73\u8861\u3002"}}
{"id": "2510.15981", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.15981", "abs": "https://arxiv.org/abs/2510.15981", "authors": ["Rafael Cabral", "Tuan Manh Do", "Xuejun Yu", "Wai Ming Tai", "Zijin Feng", "Xin Shen"], "title": "ProofFlow: A Dependency Graph Approach to Faithful Proof Autoformalization", "comment": null, "summary": "Proof autoformalization, the task of translating natural language theorems\nand proofs into machine-verifiable code, is a critical step for integrating\nlarge language models into rigorous mathematical workflows. Current approaches\nfocus on producing executable code, but they frequently fail to preserve the\nsemantic meaning and logical structure of the original human-written argument.\nTo address this, we introduce ProofFlow, a novel pipeline that treats\nstructural fidelity as a primary objective. ProofFlow first constructs a\ndirected acyclic graph (DAG) to map the logical dependencies between proof\nsteps. Then, it employs a novel lemma-based approach to systematically\nformalize each step as an intermediate lemma, preserving the logical structure\nof the original argument. To facilitate evaluation, we present a new benchmark\nof 184 undergraduate-level problems, manually annotated with step-by-step\nsolutions and logical dependency graphs, and introduce ProofScore, a new\ncomposite metric to evaluate syntactic correctness, semantic faithfulness, and\nstructural fidelity. Experimental results show our pipeline sets a new\nstate-of-the-art for autoformalization, achieving a ProofScore of 0.545,\nsubstantially exceeding baselines like full-proof formalization (0.123), which\nprocesses the entire proof at once, and step-proof formalization (0.072), which\nhandles each step independently. Our pipeline, benchmark, and score metric are\nopen-sourced to encourage further progress at\nhttps://github.com/Huawei-AI4Math/ProofFlow.", "AI": {"tldr": "ProofFlow\u662f\u4e00\u4e2a\u65b0\u7684\u81ea\u52a8\u5f62\u5f0f\u5316\u8bc1\u660e\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u6784\u5efa\u903b\u8f91\u4f9d\u8d56\u56fe\u548c\u4f7f\u7528\u57fa\u4e8e\u5f15\u7406\u7684\u65b9\u6cd5\u6765\u4fdd\u6301\u8bc1\u660e\u7684\u7ed3\u6784\u4fdd\u771f\u5ea6\uff0c\u5728\u81ea\u52a8\u5f62\u5f0f\u5316\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u81ea\u52a8\u5f62\u5f0f\u5316\u65b9\u6cd5\u5728\u4fdd\u6301\u8bed\u4e49\u610f\u4e49\u548c\u903b\u8f91\u7ed3\u6784\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5c06\u7ed3\u6784\u4fdd\u771f\u5ea6\u4f5c\u4e3a\u4e3b\u8981\u76ee\u6807\u3002", "method": "\u9996\u5148\u6784\u5efa\u6709\u5411\u65e0\u73af\u56fe\u6765\u6620\u5c04\u8bc1\u660e\u6b65\u9aa4\u95f4\u7684\u903b\u8f91\u4f9d\u8d56\u5173\u7cfb\uff0c\u7136\u540e\u91c7\u7528\u57fa\u4e8e\u5f15\u7406\u7684\u65b9\u6cd5\u7cfb\u7edf\u5730\u5c06\u6bcf\u4e2a\u6b65\u9aa4\u5f62\u5f0f\u5316\u4e3a\u4e2d\u95f4\u5f15\u7406\uff0c\u4fdd\u6301\u539f\u59cb\u8bba\u8bc1\u7684\u903b\u8f91\u7ed3\u6784\u3002", "result": "\u5728\u5305\u542b184\u4e2a\u672c\u79d1\u6c34\u5e73\u95ee\u9898\u7684\u65b0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cProofFlow\u8fbe\u5230\u4e860.545\u7684ProofScore\uff0c\u663e\u8457\u8d85\u8fc7\u5168\u8bc1\u660e\u5f62\u5f0f\u5316\uff080.123\uff09\u548c\u6b65\u9aa4\u8bc1\u660e\u5f62\u5f0f\u5316\uff080.072\uff09\u7b49\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ProofFlow\u6d41\u6c34\u7ebf\u5728\u81ea\u52a8\u5f62\u5f0f\u5316\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5176\u6d41\u6c34\u7ebf\u3001\u57fa\u51c6\u548c\u8bc4\u5206\u6307\u6807\u5df2\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2510.16079", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16079", "abs": "https://arxiv.org/abs/2510.16079", "authors": ["Rong Wu", "Xiaoman Wang", "Jianbiao Mei", "Pinlong Cai", "Daocheng Fu", "Cheng Yang", "Licheng Wen", "Xuemeng Yang", "Yufan Shen", "Yuxin Wang", "Botian Shi"], "title": "EvolveR: Self-Evolving LLM Agents through an Experience-Driven Lifecycle", "comment": null, "summary": "Current Large Language Model (LLM) agents show strong performance in tool\nuse, but lack the crucial capability to systematically learn from their own\nexperiences. While existing frameworks mainly focus on mitigating external\nknowledge gaps, they fail to address a more fundamental limitation: the\ninability to iteratively refine problem-solving strategies. In this work, we\nintroduce EvolveR, a framework designed to enable agent to self-improve through\na complete, closed-loop experience lifecycle. This lifecycle comprises two key\nstages: (1) Offline Self-Distillation, where the agent's interaction\ntrajectories are synthesized into a structured repository of abstract, reusable\nstrategic principles; (2) Online Interaction, where the agent interacts with\ntasks and actively retrieves distilled principles to guide its decision-making,\naccumulating a diverse set of behavioral trajectories. This loop employs a\npolicy reinforcement mechanism to iteratively update the agent based on its\nperformance. We demonstrate the effectiveness of EvolveR on complex multi-hop\nquestion-answering benchmarks, where it achieves superior performance over\nstrong agentic baselines. Our work presents a comprehensive blueprint for\nagents that learn not only from external data but also from the consequences of\ntheir own actions, paving the way for more autonomous and continuously\nimproving systems. Code is available at https://github.com/Edaizi/EvolveR.", "AI": {"tldr": "EvolveR\u6846\u67b6\u8ba9LLM\u667a\u80fd\u4f53\u901a\u8fc7\u79bb\u7ebf\u81ea\u84b8\u998f\u548c\u5728\u7ebf\u4ea4\u4e92\u7684\u95ed\u73af\u751f\u547d\u5468\u671f\u5b9e\u73b0\u81ea\u6211\u6539\u8fdb\uff0c\u5728\u590d\u6742\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u5728\u5de5\u5177\u4f7f\u7528\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7f3a\u4e4f\u4ece\u81ea\u8eab\u7ecf\u9a8c\u4e2d\u7cfb\u7edf\u5b66\u4e60\u7684\u80fd\u529b\uff0c\u65e0\u6cd5\u8fed\u4ee3\u4f18\u5316\u95ee\u9898\u89e3\u51b3\u7b56\u7565\u3002", "method": "\u91c7\u7528\u95ed\u73af\u7ecf\u9a8c\u751f\u547d\u5468\u671f\uff1a1) \u79bb\u7ebf\u81ea\u84b8\u998f - \u5c06\u4ea4\u4e92\u8f68\u8ff9\u5408\u6210\u4e3a\u7ed3\u6784\u5316\u3001\u53ef\u91cd\u7528\u7684\u6218\u7565\u539f\u5219\u5e93\uff1b2) \u5728\u7ebf\u4ea4\u4e92 - \u667a\u80fd\u4f53\u4e0e\u4efb\u52a1\u4ea4\u4e92\u5e76\u68c0\u7d22\u84b8\u998f\u539f\u5219\u6307\u5bfc\u51b3\u7b56\uff0c\u79ef\u7d2f\u591a\u6837\u5316\u884c\u4e3a\u8f68\u8ff9\uff1b\u4f7f\u7528\u7b56\u7565\u5f3a\u5316\u673a\u5236\u8fed\u4ee3\u66f4\u65b0\u667a\u80fd\u4f53\u3002", "result": "\u5728\u590d\u6742\u591a\u8df3\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEvolveR\u5b9e\u73b0\u4e86\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u667a\u80fd\u4f53\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u4ece\u81ea\u8eab\u884c\u52a8\u540e\u679c\u4e2d\u5b66\u4e60\u7684\u5168\u9762\u84dd\u56fe\uff0c\u4e3a\u66f4\u81ea\u4e3b\u548c\u6301\u7eed\u6539\u8fdb\u7684\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.15983", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15983", "abs": "https://arxiv.org/abs/2510.15983", "authors": ["Sarah Rebecca Ondraszek", "J\u00f6rg Waitelonis", "Katja Keller", "Claudia Niessner", "Anna M. Jacyszyn", "Harald Sack"], "title": "Ontologies in Motion: A BFO-Based Approach to Knowledge Graph Construction for Motor Performance Research Data in Sports Science", "comment": "10 pages, 2 figures. Camera-ready version. Accepted to the 5th\n  International Workshop on Scientific Knowledge: Representation, Discovery,\n  and Assessment; 2 November 2025 - Nara, Japan; co-located with The 24th\n  International Semantic Web Conference, ISWC 2025. To be published in CEUR\n  proceedings", "summary": "An essential component for evaluating and comparing physical and cognitive\ncapabilities between populations is the testing of various factors related to\nhuman performance. As a core part of sports science research, testing motor\nperformance enables the analysis of the physical health of different\ndemographic groups and makes them comparable.\n  The Motor Research (MO|RE) data repository, developed at the Karlsruhe\nInstitute of Technology, is an infrastructure for publishing and archiving\nresearch data in sports science, particularly in the field of motor performance\nresearch. In this paper, we present our vision for creating a knowledge graph\nfrom MO|RE data. With an ontology rooted in the Basic Formal Ontology, our\napproach centers on formally representing the interrelation of plan\nspecifications, specific processes, and related measurements. Our goal is to\ntransform how motor performance data are modeled and shared across studies,\nmaking it standardized and machine-understandable. The idea presented here is\ndeveloped within the Leibniz Science Campus ``Digital Transformation of\nResearch'' (DiTraRe).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5c06MO|RE\u8fd0\u52a8\u7814\u7a76\u6570\u636e\u4ed3\u5e93\u8f6c\u6362\u4e3a\u77e5\u8bc6\u56fe\u8c31\u7684\u613f\u666f\uff0c\u65e8\u5728\u6807\u51c6\u5316\u548c\u673a\u5668\u53ef\u7406\u89e3\u5730\u5efa\u6a21\u4e0e\u5171\u4eab\u8fd0\u52a8\u8868\u73b0\u6570\u636e\u3002", "motivation": "\u8fd0\u52a8\u8868\u73b0\u6d4b\u8bd5\u662f\u4f53\u80b2\u79d1\u5b66\u7814\u7a76\u7684\u6838\u5fc3\uff0c\u4f46\u5f53\u524d\u6570\u636e\u7f3a\u4e4f\u6807\u51c6\u5316\u548c\u4e92\u64cd\u4f5c\u6027\uff0c\u9650\u5236\u4e86\u4e0d\u540c\u7814\u7a76\u95f4\u7684\u6bd4\u8f83\u548c\u5206\u6790\u3002", "method": "\u57fa\u4e8e\u57fa\u7840\u5f62\u5f0f\u672c\u4f53\u8bba\u6784\u5efa\u672c\u4f53\uff0c\u6b63\u5f0f\u8868\u793a\u8ba1\u5212\u89c4\u8303\u3001\u5177\u4f53\u8fc7\u7a0b\u548c\u76f8\u5173\u6d4b\u91cf\u4e4b\u95f4\u7684\u76f8\u4e92\u5173\u7cfb\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u77e5\u8bc6\u56fe\u8c31\u6846\u67b6\uff0c\u80fd\u591f\u6807\u51c6\u5316\u5730\u8868\u793a\u8fd0\u52a8\u8868\u73b0\u7814\u7a76\u6570\u636e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c06\u6539\u53d8\u8fd0\u52a8\u8868\u73b0\u6570\u636e\u7684\u5efa\u6a21\u548c\u5171\u4eab\u65b9\u5f0f\uff0c\u4f7f\u5176\u6807\u51c6\u5316\u4e14\u673a\u5668\u53ef\u7406\u89e3\uff0c\u4fc3\u8fdb\u8de8\u7814\u7a76\u7684\u6570\u636e\u6bd4\u8f83\u548c\u5206\u6790\u3002"}}
{"id": "2510.16091", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16091", "abs": "https://arxiv.org/abs/2510.16091", "authors": ["Binglan Han", "Anuradha Mathrani", "Teo Susnjak"], "title": "Evaluating Prompting Strategies and Large Language Models in Systematic Literature Review Screening: Relevance and Task-Stage Classification", "comment": null, "summary": "This study quantifies how prompting strategies interact with large language\nmodels (LLMs) to automate the screening stage of systematic literature reviews\n(SLRs). We evaluate six LLMs (GPT-4o, GPT-4o-mini, DeepSeek-Chat-V3,\nGemini-2.5-Flash, Claude-3.5-Haiku, Llama-4-Maverick) under five prompt types\n(zero-shot, few-shot, chain-of-thought (CoT), CoT-few-shot, self-reflection)\nacross relevance classification and six Level-2 tasks, using accuracy,\nprecision, recall, and F1. Results show pronounced model-prompt interaction\neffects: CoT-few-shot yields the most reliable precision-recall balance;\nzero-shot maximizes recall for high-sensitivity passes; and self-reflection\nunderperforms due to over-inclusivity and instability across models. GPT-4o and\nDeepSeek provide robust overall performance, while GPT-4o-mini performs\ncompetitively at a substantially lower dollar cost. A cost-performance analysis\nfor relevance classification (per 1,000 abstracts) reveals large absolute\ndifferences among model-prompt pairings; GPT-4o-mini remains low-cost across\nprompts, and structured prompts (CoT/CoT-few-shot) on GPT-4o-mini offer\nattractive F1 at a small incremental cost. We recommend a staged workflow that\n(1) deploys low-cost models with structured prompts for first-pass screening\nand (2) escalates only borderline cases to higher-capacity models. These\nfindings highlight LLMs' uneven but promising potential to automate literature\nscreening. By systematically analyzing prompt-model interactions, we provide a\ncomparative benchmark and practical guidance for task-adaptive LLM deployment.", "AI": {"tldr": "\u672c\u7814\u7a76\u91cf\u5316\u4e86\u63d0\u793a\u7b56\u7565\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u7b5b\u9009\u9636\u6bb5\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u53d1\u73b0CoT-few-shot\u63d0\u793a\u5728\u7cbe\u5ea6-\u53ec\u56de\u5e73\u8861\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0cGPT-4o-mini\u5728\u663e\u8457\u964d\u4f4e\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "motivation": "\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u7684\u7b5b\u9009\u9636\u6bb5\u8017\u65f6\u4e14\u52b3\u52a8\u5bc6\u96c6\uff0c\u9700\u8981\u8bc4\u4f30LLMs\u5728\u6b64\u4efb\u52a1\u4e2d\u7684\u81ea\u52a8\u5316\u6f5c\u529b\uff0c\u7279\u522b\u662f\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u4e0e\u6a21\u578b\u4e4b\u95f4\u7684\u4ea4\u4e92\u6548\u5e94\u3002", "method": "\u8bc4\u4f306\u4e2aLLM\u57285\u79cd\u63d0\u793a\u7c7b\u578b\u4e0b\u7684\u8868\u73b0\uff0c\u5305\u62ec\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u3001\u601d\u7ef4\u94fe\u3001\u601d\u7ef4\u94fe-\u5c11\u6837\u672c\u548c\u81ea\u6211\u53cd\u601d\uff0c\u4f7f\u7528\u51c6\u786e\u7387\u3001\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4f5c\u4e3a\u6307\u6807\u3002", "result": "\u7ed3\u679c\u663e\u793a\u660e\u663e\u7684\u6a21\u578b-\u63d0\u793a\u4ea4\u4e92\u6548\u5e94\uff1aCoT-few-shot\u63d0\u4f9b\u6700\u53ef\u9760\u7684\u7cbe\u5ea6-\u53ec\u56de\u5e73\u8861\uff1b\u96f6\u6837\u672c\u5728\u9ad8\u7075\u654f\u5ea6\u7b5b\u9009\u65f6\u53ec\u56de\u7387\u6700\u9ad8\uff1b\u81ea\u6211\u53cd\u601d\u7531\u4e8e\u8fc7\u5ea6\u5305\u5bb9\u6027\u548c\u4e0d\u7a33\u5b9a\u6027\u8868\u73b0\u4e0d\u4f73\u3002GPT-4o-mini\u5728\u663e\u8457\u964d\u4f4e\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u63a8\u8350\u91c7\u7528\u5206\u9636\u6bb5\u5de5\u4f5c\u6d41\u7a0b\uff1a\u5148\u7528\u4f4e\u6210\u672c\u6a21\u578b\u548c\u7ed3\u6784\u5316\u63d0\u793a\u8fdb\u884c\u521d\u6b65\u7b5b\u9009\uff0c\u4ec5\u5c06\u8fb9\u754c\u6848\u4f8b\u5347\u7ea7\u5230\u9ad8\u5bb9\u91cf\u6a21\u578b\u3002\u8fd9\u4e9b\u53d1\u73b0\u7a81\u663e\u4e86LLMs\u5728\u6587\u732e\u7b5b\u9009\u81ea\u52a8\u5316\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u63d0\u4f9b\u4e86\u6bd4\u8f83\u57fa\u51c6\u548c\u5b9e\u7528\u6307\u5357\u3002"}}
{"id": "2510.16001", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16001", "abs": "https://arxiv.org/abs/2510.16001", "authors": ["Ruolan Cheng", "Yong Deng", "Enrique Herrera-Viedma"], "title": "A Non-overlap-based Conflict Measure for Random Permutation Sets", "comment": null, "summary": "Random permutation set (RPS) is a new formalism for reasoning with\nuncertainty involving order information. Measuring the conflict between two\npieces of evidence represented by permutation mass functions remains an urgent\nresearch topic in order-structured uncertain information fusion. In this paper,\na detailed analysis of conflicts in RPS is carried out from two different\nperspectives: random finite set (RFS) and Dempster-Shafer theory (DST).\nStarting from the observation of permutations, we first define an inconsistency\nmeasure between permutations inspired by the rank-biased overlap(RBO) measure\nand further propose a non-overlap-based conflict measure method for RPSs. This\npaper regards RPS theory (RPST) as an extension of DST. The order information\nnewly added in focal sets indicates qualitative propensity, characterized by\ntop-ranked elements occupying a more critical position. Some numerical examples\nare used to demonstrate the behavior and properties of the proposed conflict\nmeasure. The proposed method not only has the natural top-weightedness property\nand can effectively measure the conflict between RPSs from the DST view but\nalso provides decision-makers with a flexible selection of weights, parameters,\nand truncated depths.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u7f6e\u6362\u96c6(RPS)\u7684\u51b2\u7a81\u5ea6\u91cf\u65b9\u6cd5\uff0c\u4ece\u968f\u673a\u6709\u9650\u96c6\u548cDempster-Shafer\u7406\u8bba\u4e24\u4e2a\u89d2\u5ea6\u5206\u6790\u7f6e\u6362\u95f4\u7684\u51b2\u7a81\uff0c\u5e76\u5f15\u5165\u5177\u6709\u81ea\u7136\u9876\u90e8\u52a0\u6743\u7279\u6027\u7684\u51b2\u7a81\u5ea6\u91cf\u3002", "motivation": "\u968f\u673a\u7f6e\u6362\u96c6\u662f\u4e00\u79cd\u5904\u7406\u5305\u542b\u987a\u5e8f\u4fe1\u606f\u7684\u4e0d\u786e\u5b9a\u6027\u7684\u65b0\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u5982\u4f55\u5ea6\u91cf\u7531\u7f6e\u6362\u8d28\u91cf\u51fd\u6570\u8868\u793a\u7684\u4e24\u4e2a\u8bc1\u636e\u4e4b\u95f4\u7684\u51b2\u7a81\u662f\u987a\u5e8f\u7ed3\u6784\u4e0d\u786e\u5b9a\u4fe1\u606f\u878d\u5408\u4e2d\u7684\u7d27\u8feb\u7814\u7a76\u8bfe\u9898\u3002", "method": "\u4ece\u7f6e\u6362\u89c2\u5bdf\u51fa\u53d1\uff0c\u57fa\u4e8e\u79e9\u504f\u91cd\u53e0(RBO)\u5ea6\u91cf\u5b9a\u4e49\u7f6e\u6362\u95f4\u7684\u4e0d\u4e00\u81f4\u6027\u5ea6\u91cf\uff0c\u8fdb\u4e00\u6b65\u63d0\u51faRPS\u7684\u975e\u91cd\u53e0\u51b2\u7a81\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5c06RPS\u7406\u8bba\u89c6\u4e3aDST\u7684\u6269\u5c55\u3002", "result": "\u901a\u8fc7\u6570\u503c\u793a\u4f8b\u5c55\u793a\u4e86\u6240\u63d0\u51b2\u7a81\u5ea6\u91cf\u7684\u884c\u4e3a\u548c\u7279\u6027\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u81ea\u7136\u7684\u9876\u90e8\u52a0\u6743\u7279\u6027\uff0c\u80fd\u4eceDST\u89c6\u89d2\u6709\u6548\u5ea6\u91cfRPS\u95f4\u7684\u51b2\u7a81\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u4e0d\u4ec5\u5177\u6709\u81ea\u7136\u9876\u90e8\u52a0\u6743\u7279\u6027\uff0c\u80fd\u6709\u6548\u5ea6\u91cfRPS\u95f4\u7684\u51b2\u7a81\uff0c\u8fd8\u4e3a\u51b3\u7b56\u8005\u63d0\u4f9b\u4e86\u6743\u91cd\u3001\u53c2\u6570\u548c\u622a\u65ad\u6df1\u5ea6\u7684\u7075\u6d3b\u9009\u62e9\u3002"}}
{"id": "2510.16096", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16096", "abs": "https://arxiv.org/abs/2510.16096", "authors": ["Tina Behnia", "Puneesh Deora", "Christos Thrampoulidis"], "title": "Facts in Stats: Impacts of Pretraining Diversity on Language Model Generalization", "comment": "28 pages, 15 figures", "summary": "Language models are pretrained on sequences that blend statistical\nregularities (making text fluent) with factual associations between specific\ntokens (knowledge of facts). While recent work suggests that the variability of\ntheir interaction, such as paraphrases of factual associations, critically\ndetermines generalization ability, we lack a systematic analysis of these\nimpacts. This paper introduces a flexible synthetic testbed that combines a\nstatistical stream of generic tokens with an abstract factual stream of\nsource-target token pairs, enabling fine-grained control over their\ninteraction. The design enables the independent control of diversity nature by\nmanipulating stream composition (contextual structure) and the diversity level\nby varying which statistical streams each fact appears in. Through controlled\nexperiments, we find that while higher contextual diversity delays\nin-distribution (ID) factual accuracy, its impact on out-of-distribution (OOD)\nfactual generalization depends critically on contextual structure. In some\ncases, OOD performance follows the same trend as ID, but in others, diversity\nbecomes essential for non-trivial factual recall. Even when low diversity\nprohibits factual recall, optimal diversity levels depend on training duration.\nBeyond factual recall failures, we identify structures where statistical\ngeneralization fails independently, and others where both capabilities degrade.\nThis shows how the interplay between contextual design and diversity level\nimpacts different generalization aspects. Further, through a series of\ncontrolled interventions on the model components, we trace the OOD failures to\ndistinct optimization bottlenecks, highlighting the importance of the embedding\nand unembedding layers. Our synthetic framework allows us to isolate effects\nthat would be confounded in large-scale studies, offering a controlled testbed\nfor future investigations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5408\u6210\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7528\u4e8e\u7cfb\u7edf\u5206\u6790\u8bed\u8a00\u6a21\u578b\u4e2d\u7edf\u8ba1\u89c4\u5f8b\u6027\u548c\u4e8b\u5b9e\u5173\u8054\u4e4b\u95f4\u7684\u4ea4\u4e92\u5f71\u54cd\uff0c\u53d1\u73b0\u4e0a\u4e0b\u6587\u591a\u6837\u6027\u548c\u7ed3\u6784\u5bf9\u4e8b\u5b9e\u6cdb\u5316\u80fd\u529b\u6709\u590d\u6742\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u8bed\u8a00\u6a21\u578b\u4e2d\u7edf\u8ba1\u89c4\u5f8b\u6027\u548c\u4e8b\u5b9e\u5173\u8054\u4ea4\u4e92\u5f71\u54cd\u7684\u7cfb\u7edf\u5206\u6790\uff0c\u7279\u522b\u662f\u5b83\u4eec\u5982\u4f55\u5f71\u54cd\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u7075\u6d3b\u7684\u5408\u6210\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5c06\u901a\u7528\u6807\u8bb0\u7684\u7edf\u8ba1\u6d41\u4e0e\u62bd\u8c61\u4e8b\u5b9e\u7684\u6e90-\u76ee\u6807\u6807\u8bb0\u5bf9\u6d41\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u63a7\u5236\u6d41\u7ec4\u6210\u548c\u591a\u6837\u6027\u6c34\u5e73\u6765\u72ec\u7acb\u64cd\u7eb5\u4e0a\u4e0b\u6587\u7ed3\u6784\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u66f4\u9ad8\u7684\u4e0a\u4e0b\u6587\u591a\u6837\u6027\u4f1a\u5ef6\u8fdf\u5206\u5e03\u5185\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u4f46\u5bf9\u5206\u5e03\u5916\u4e8b\u5b9e\u6cdb\u5316\u7684\u5f71\u54cd\u53d6\u51b3\u4e8e\u4e0a\u4e0b\u6587\u7ed3\u6784\u3002\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u4f4e\u591a\u6837\u6027\u4f1a\u963b\u788d\u4e8b\u5b9e\u56de\u5fc6\uff0c\u800c\u6700\u4f18\u591a\u6837\u6027\u6c34\u5e73\u53d6\u51b3\u4e8e\u8bad\u7ec3\u65f6\u957f\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u8bbe\u8ba1\u548c\u591a\u6837\u6027\u6c34\u5e73\u7684\u76f8\u4e92\u4f5c\u7528\u4ee5\u4e0d\u540c\u65b9\u5f0f\u5f71\u54cd\u6cdb\u5316\u80fd\u529b\uff0c\u901a\u8fc7\u6a21\u578b\u7ec4\u4ef6\u5e72\u9884\u53d1\u73b0\u5d4c\u5165\u5c42\u548c\u89e3\u5d4c\u5165\u5c42\u5bf9\u5206\u5e03\u5916\u5931\u8d25\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u5408\u6210\u6846\u67b6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u53d7\u63a7\u6d4b\u8bd5\u5e73\u53f0\u3002"}}
{"id": "2510.16004", "categories": ["cs.AI", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2510.16004", "abs": "https://arxiv.org/abs/2510.16004", "authors": ["Andreas Radler", "Vincent Seyfried", "Stefan Pirker", "Johannes Brandstetter", "Thomas Lichtenegger"], "title": "PAINT: Parallel-in-time Neural Twins for Dynamical System Reconstruction", "comment": "22 pages, 16 figures", "summary": "Neural surrogates have shown great potential in simulating dynamical systems,\nwhile offering real-time capabilities. We envision Neural Twins as a\nprogression of neural surrogates, aiming to create digital replicas of real\nsystems. A neural twin consumes measurements at test time to update its state,\nthereby enabling context-specific decision-making. A critical property of\nneural twins is their ability to remain on-trajectory, i.e., to stay close to\nthe true system state over time. We introduce Parallel-in-time Neural Twins\n(PAINT), an architecture-agnostic family of methods for modeling dynamical\nsystems from measurements. PAINT trains a generative neural network to model\nthe distribution of states parallel over time. At test time, states are\npredicted from measurements in a sliding window fashion. Our theoretical\nanalysis shows that PAINT is on-trajectory, whereas autoregressive models\ngenerally are not. Empirically, we evaluate our method on a challenging\ntwo-dimensional turbulent fluid dynamics problem. The results demonstrate that\nPAINT stays on-trajectory and predicts system states from sparse measurements\nwith high fidelity. These findings underscore PAINT's potential for developing\nneural twins that stay on-trajectory, enabling more accurate state estimation\nand decision-making.", "AI": {"tldr": "\u63d0\u51fa\u4e86PAINT\u65b9\u6cd5\uff0c\u4e00\u79cd\u7528\u4e8e\u5efa\u6a21\u52a8\u6001\u7cfb\u7edf\u7684\u5e76\u884c\u65f6\u95f4\u795e\u7ecf\u5b6a\u751f\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u6d4b\u91cf\u6570\u636e\u4e2d\u51c6\u786e\u9884\u6d4b\u7cfb\u7edf\u72b6\u6001\u5e76\u4fdd\u6301\u5728\u771f\u5b9e\u8f68\u8ff9\u4e0a", "motivation": "\u795e\u7ecf\u4ee3\u7406\u5728\u6a21\u62df\u52a8\u6001\u7cfb\u7edf\u65b9\u9762\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u521b\u5efa\u80fd\u591f\u6839\u636e\u5b9e\u65f6\u6d4b\u91cf\u66f4\u65b0\u72b6\u6001\u7684\u6570\u5b57\u5b6a\u751f\u4f53\uff0c\u4ee5\u5b9e\u73b0\u4e0a\u4e0b\u6587\u7279\u5b9a\u7684\u51b3\u7b56", "method": "PAINT\u8bad\u7ec3\u751f\u6210\u795e\u7ecf\u7f51\u7edc\u6765\u5e76\u884c\u5efa\u6a21\u65f6\u95f4\u4e0a\u7684\u72b6\u6001\u5206\u5e03\uff0c\u5728\u6d4b\u8bd5\u65f6\u901a\u8fc7\u6ed1\u52a8\u7a97\u53e3\u65b9\u5f0f\u4ece\u6d4b\u91cf\u6570\u636e\u9884\u6d4b\u72b6\u6001", "result": "\u7406\u8bba\u5206\u6790\u8868\u660ePAINT\u80fd\u591f\u4fdd\u6301\u5728\u8f68\u8ff9\u4e0a\uff0c\u800c\u81ea\u56de\u5f52\u6a21\u578b\u901a\u5e38\u4e0d\u80fd\u3002\u5728\u4e8c\u7ef4\u6e4d\u6d41\u6d41\u4f53\u52a8\u529b\u5b66\u95ee\u9898\u4e0a\u7684\u5b9e\u9a8c\u663e\u793aPAINT\u80fd\u591f\u9ad8\u4fdd\u771f\u5730\u9884\u6d4b\u7cfb\u7edf\u72b6\u6001", "conclusion": "PAINT\u5177\u6709\u5f00\u53d1\u80fd\u591f\u4fdd\u6301\u5728\u8f68\u8ff9\u4e0a\u7684\u795e\u7ecf\u5b6a\u751f\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u72b6\u6001\u4f30\u8ba1\u548c\u51b3\u7b56"}}
{"id": "2510.16173", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16173", "abs": "https://arxiv.org/abs/2510.16173", "authors": ["Aria Pessianzadeh", "Naima Sultana", "Hildegarde Van den Bulck", "David Gefen", "Shahin Jabari", "Rezvaneh Rezapour"], "title": "In Generative AI We (Dis)Trust? Computational Analysis of Trust and Distrust in Reddit Discussions", "comment": null, "summary": "The rise of generative AI (GenAI) has impacted many aspects of human life. As\nthese systems become embedded in everyday practices, understanding public trust\nin them also becomes essential for responsible adoption and governance. Prior\nwork on trust in AI has largely drawn from psychology and human-computer\ninteraction, but there is a lack of computational, large-scale, and\nlongitudinal approaches to measuring trust and distrust in GenAI and large\nlanguage models (LLMs). This paper presents the first computational study of\nTrust and Distrust in GenAI, using a multi-year Reddit dataset (2022--2025)\nspanning 39 subreddits and 197,618 posts. Crowd-sourced annotations of a\nrepresentative sample were combined with classification models to scale\nanalysis. We find that Trust and Distrust are nearly balanced over time, with\nshifts around major model releases. Technical performance and usability\ndominate as dimensions, while personal experience is the most frequent reason\nshaping attitudes. Distinct patterns also emerge across trustors (e.g.,\nexperts, ethicists, general users). Our results provide a methodological\nframework for large-scale Trust analysis and insights into evolving public\nperceptions of GenAI.", "AI": {"tldr": "\u9996\u4e2a\u5173\u4e8e\u751f\u6210\u5f0fAI\u4fe1\u4efb\u4e0e\u4e0d\u4fe1\u4efb\u7684\u8ba1\u7b97\u7814\u7a76\uff0c\u4f7f\u75282022-2025\u5e74Reddit\u6570\u636e\uff0c\u53d1\u73b0\u4fe1\u4efb\u4e0e\u4e0d\u4fe1\u4efb\u57fa\u672c\u5e73\u8861\uff0c\u6280\u672f\u6027\u80fd\u548c\u53ef\u7528\u6027\u662f\u4e3b\u8981\u7ef4\u5ea6\uff0c\u4e2a\u4eba\u7ecf\u9a8c\u662f\u6001\u5ea6\u5f62\u6210\u7684\u6700\u5e38\u89c1\u539f\u56e0\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u7cfb\u7edf\u878d\u5165\u65e5\u5e38\u751f\u6d3b\uff0c\u7406\u89e3\u516c\u4f17\u5bf9\u5176\u7684\u4fe1\u4efb\u5bf9\u4e8e\u8d1f\u8d23\u4efb\u91c7\u7528\u548c\u6cbb\u7406\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709AI\u4fe1\u4efb\u7814\u7a76\u7f3a\u4e4f\u8ba1\u7b97\u6027\u3001\u5927\u89c4\u6a21\u548c\u7eb5\u5411\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u591a\u5e74\u5ea6Reddit\u6570\u636e\u96c6(39\u4e2a\u5b50\u7248\u5757\uff0c197,618\u6761\u5e16\u5b50)\uff0c\u7ed3\u5408\u4f17\u5305\u6807\u6ce8\u548c\u5206\u7c7b\u6a21\u578b\u8fdb\u884c\u6269\u5c55\u5206\u6790\u3002", "result": "\u4fe1\u4efb\u4e0e\u4e0d\u4fe1\u4efb\u968f\u65f6\u95f4\u57fa\u672c\u5e73\u8861\uff0c\u4e3b\u8981\u6a21\u578b\u53d1\u5e03\u65f6\u51fa\u73b0\u8f6c\u53d8\u3002\u6280\u672f\u6027\u80fd\u548c\u53ef\u7528\u6027\u5360\u4e3b\u5bfc\u7ef4\u5ea6\uff0c\u4e2a\u4eba\u7ecf\u9a8c\u662f\u6001\u5ea6\u5f62\u6210\u7684\u6700\u5e38\u89c1\u539f\u56e0\u3002\u4e0d\u540c\u4fe1\u4efb\u8005\u7fa4\u4f53(\u4e13\u5bb6\u3001\u4f26\u7406\u5b66\u5bb6\u3001\u666e\u901a\u7528\u6237)\u5448\u73b0\u4e0d\u540c\u6a21\u5f0f\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21\u4fe1\u4efb\u5206\u6790\u7684\u65b9\u6cd5\u6846\u67b6\uff0c\u5e76\u4e3a\u7406\u89e3\u516c\u4f17\u5bf9\u751f\u6210\u5f0fAI\u4e0d\u65ad\u6f14\u53d8\u7684\u8ba4\u77e5\u63d0\u4f9b\u4e86\u6d1e\u89c1\u3002"}}
{"id": "2510.16033", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16033", "abs": "https://arxiv.org/abs/2510.16033", "authors": ["Junyu Ren", "Wensheng Gan", "Guangyu Zhang", "Wei Zhong", "Philip S. Yu"], "title": "Global-focal Adaptation with Information Separation for Noise-robust Transfer Fault Diagnosis", "comment": "Preprint. 16 figures, 12 tables", "summary": "Existing transfer fault diagnosis methods typically assume either clean data\nor sufficient domain similarity, which limits their effectiveness in industrial\nenvironments where severe noise interference and domain shifts coexist. To\naddress this challenge, we propose an information separation global-focal\nadversarial network (ISGFAN), a robust framework for cross-domain fault\ndiagnosis under noise conditions. ISGFAN is built on an information separation\narchitecture that integrates adversarial learning with an improved orthogonal\nloss to decouple domain-invariant fault representation, thereby isolating noise\ninterference and domain-specific characteristics. To further strengthen\ntransfer robustness, ISGFAN employs a global-focal domain-adversarial scheme\nthat constrains both the conditional and marginal distributions of the model.\nSpecifically, the focal domain-adversarial component mitigates\ncategory-specific transfer obstacles caused by noise in unsupervised scenarios,\nwhile the global domain classifier ensures alignment of the overall\ndistribution. Experiments conducted on three public benchmark datasets\ndemonstrate that the proposed method outperforms other prominent existing\napproaches, confirming the superiority of the ISGFAN framework. Data and code\nare available at https://github.com/JYREN-Source/ISGFAN", "AI": {"tldr": "\u63d0\u51faISGFAN\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u606f\u5206\u79bb\u548c\u5168\u5c40-\u5c40\u90e8\u5bf9\u6297\u5b66\u4e60\u89e3\u51b3\u566a\u58f0\u5e72\u6270\u548c\u9886\u57df\u504f\u79fb\u5171\u5b58\u7684\u8de8\u9886\u57df\u6545\u969c\u8bca\u65ad\u95ee\u9898", "motivation": "\u73b0\u6709\u8fc1\u79fb\u6545\u969c\u8bca\u65ad\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u6570\u636e\u5e72\u51c0\u6216\u9886\u57df\u76f8\u4f3c\u6027\u8db3\u591f\uff0c\u4f46\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u4e25\u91cd\u566a\u58f0\u5e72\u6270\u548c\u9886\u57df\u504f\u79fb\u540c\u65f6\u5b58\u5728\uff0c\u9650\u5236\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u6709\u6548\u6027", "method": "\u57fa\u4e8e\u4fe1\u606f\u5206\u79bb\u67b6\u6784\uff0c\u7ed3\u5408\u5bf9\u6297\u5b66\u4e60\u548c\u6539\u8fdb\u7684\u6b63\u4ea4\u635f\u5931\u6765\u89e3\u8026\u9886\u57df\u4e0d\u53d8\u6545\u969c\u8868\u793a\uff1b\u91c7\u7528\u5168\u5c40-\u5c40\u90e8\u9886\u57df\u5bf9\u6297\u65b9\u6848\u7ea6\u675f\u6a21\u578b\u7684\u8054\u5408\u5206\u5e03\u548c\u8fb9\u9645\u5206\u5e03", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5176\u4ed6\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5", "conclusion": "ISGFAN\u6846\u67b6\u5728\u566a\u58f0\u6761\u4ef6\u4e0b\u7684\u8de8\u9886\u57df\u6545\u969c\u8bca\u65ad\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027"}}
{"id": "2510.16198", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16198", "abs": "https://arxiv.org/abs/2510.16198", "authors": ["Mohamed Gamil", "Abdelrahman Elsayed", "Abdelrahman Lila", "Ahmed Gad", "Hesham Abdelgawad", "Mohamed Aref", "Ahmed Fares"], "title": "EgMM-Corpus: A Multimodal Vision-Language Dataset for Egyptian Culture", "comment": null, "summary": "Despite recent advances in AI, multimodal culturally diverse datasets are\nstill limited, particularly for regions in the Middle East and Africa. In this\npaper, we introduce EgMM-Corpus, a multimodal dataset dedicated to Egyptian\nculture. By designing and running a new data collection pipeline, we collected\nover 3,000 images, covering 313 concepts across landmarks, food, and folklore.\nEach entry in the dataset is manually validated for cultural authenticity and\nmultimodal coherence. EgMM-Corpus aims to provide a reliable resource for\nevaluating and training vision-language models in an Egyptian cultural context.\nWe further evaluate the zero-shot performance of Contrastive Language-Image\nPre-training CLIP on EgMM-Corpus, on which it achieves 21.2% Top-1 accuracy and\n36.4% Top-5 accuracy in classification. These results underscore the existing\ncultural bias in large-scale vision-language models and demonstrate the\nimportance of EgMM-Corpus as a benchmark for developing culturally aware\nmodels.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86EgMM-Corpus\uff0c\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u57c3\u53ca\u6587\u5316\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b3000\u591a\u5f20\u56fe\u50cf\uff0c\u6db5\u76d6313\u4e2a\u6982\u5ff5\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u57c3\u53ca\u6587\u5316\u80cc\u666f\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u4e2d\u4e1c\u548c\u975e\u6d32\u5730\u533a\u7684\u591a\u6a21\u6001\u6587\u5316\u591a\u6837\u6027\u6570\u636e\u96c6\u4ecd\u7136\u6709\u9650\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u57c3\u53ca\u6587\u5316\u7684\u8d44\u6e90\u6765\u8bc4\u4f30\u548c\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "method": "\u8bbe\u8ba1\u548c\u8fd0\u884c\u65b0\u7684\u6570\u636e\u6536\u96c6\u6d41\u7a0b\uff0c\u6536\u96c6\u4e86\u6db5\u76d6\u5730\u6807\u3001\u98df\u7269\u548c\u6c11\u95f4\u4f20\u8bf4\u7684313\u4e2a\u6982\u5ff5\u76843000\u591a\u5f20\u56fe\u50cf\uff0c\u6bcf\u4e2a\u6761\u76ee\u90fd\u7ecf\u8fc7\u4eba\u5de5\u9a8c\u8bc1\u6587\u5316\u771f\u5b9e\u6027\u548c\u591a\u6a21\u6001\u4e00\u81f4\u6027\u3002", "result": "CLIP\u6a21\u578b\u5728EgMM-Corpus\u4e0a\u7684\u96f6\u6837\u672c\u6027\u80fd\u4e3aTop-1\u51c6\u786e\u738721.2%\uff0cTop-5\u51c6\u786e\u738736.4%\uff0c\u8868\u660e\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u6587\u5316\u504f\u89c1\u3002", "conclusion": "EgMM-Corpus\u4f5c\u4e3a\u5f00\u53d1\u6587\u5316\u611f\u77e5\u6a21\u578b\u7684\u57fa\u51c6\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u7a81\u663e\u4e86\u73b0\u6709\u6a21\u578b\u7684\u6587\u5316\u504f\u89c1\u95ee\u9898\u3002"}}
{"id": "2510.16047", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16047", "abs": "https://arxiv.org/abs/2510.16047", "authors": ["Ioan Hedea"], "title": "Algorithms for dynamic scheduling in manufacturing, towards digital factories Improving Deadline Feasibility and Responsiveness via Temporal Networks", "comment": "8 pages 2 column, 11 figures. Bachelor's thesis", "summary": "Modern manufacturing systems must meet hard delivery deadlines while coping\nwith stochastic task durations caused by process noise, equipment variability,\nand human intervention. Traditional deterministic schedules break down when\nreality deviates from nominal plans, triggering costly last-minute repairs.\nThis thesis combines offline constraint-programming (CP) optimisation with\nonline temporal-network execution to create schedules that remain feasible\nunder worst-case uncertainty. First, we build a CP model of the flexible\njob-shop with per-job deadline tasks and insert an optimal buffer $\\Delta^*$ to\nobtain a fully pro-active baseline. We then translate the resulting plan into a\nSimple Temporal Network with Uncertainty (STNU) and verify dynamic\ncontrollability, which guarantees that a real-time dispatcher can retime\nactivities for every bounded duration realisation without violating resource or\ndeadline constraints. Extensive Monte-Carlo simulations on the open Kacem~1--4\nbenchmark suite show that our hybrid approach eliminates 100\\% of deadline\nviolations observed in state-of-the-art meta-heuristic schedules, while adding\nonly 3--5\\% makespan overhead. Scalability experiments confirm that CP\nsolve-times and STNU checks remain sub-second on medium-size instances. The\nwork demonstrates how temporal-network reasoning can bridge the gap between\nproactive buffering and dynamic robustness, moving industry a step closer to\ntruly digital, self-correcting factories.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7ed3\u5408\u79bb\u7ebf\u7ea6\u675f\u89c4\u5212\u4f18\u5316\u548c\u5728\u7ebf\u65f6\u95f4\u7f51\u7edc\u6267\u884c\uff0c\u521b\u5efa\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u4ecd\u53ef\u884c\u7684\u8c03\u5ea6\u65b9\u6848\uff0c\u5728Kacem\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b8c\u5168\u6d88\u9664\u622a\u6b62\u65f6\u95f4\u8fdd\u89c4\uff0c\u4ec5\u589e\u52a03-5%\u5236\u9020\u5468\u671f\u5f00\u9500\u3002", "motivation": "\u73b0\u4ee3\u5236\u9020\u7cfb\u7edf\u9700\u8981\u6ee1\u8db3\u4e25\u683c\u7684\u4ea4\u4ed8\u622a\u6b62\u65f6\u95f4\uff0c\u540c\u65f6\u5e94\u5bf9\u7531\u8fc7\u7a0b\u566a\u58f0\u3001\u8bbe\u5907\u53d8\u5f02\u6027\u548c\u4eba\u4e3a\u5e72\u9884\u5f15\u8d77\u7684\u968f\u673a\u4efb\u52a1\u6301\u7eed\u65f6\u95f4\u3002\u4f20\u7edf\u7684\u786e\u5b9a\u6027\u8c03\u5ea6\u5728\u5b9e\u9645\u504f\u79bb\u540d\u4e49\u8ba1\u5212\u65f6\u4f1a\u5931\u6548\uff0c\u5bfc\u81f4\u6602\u8d35\u7684\u7d27\u6025\u4fee\u590d\u3002", "method": "\u9996\u5148\u6784\u5efa\u67d4\u6027\u4f5c\u4e1a\u8f66\u95f4\u7ea6\u675f\u89c4\u5212\u6a21\u578b\uff0c\u63d2\u5165\u6700\u4f18\u7f13\u51b2\u533a\u0394*\u83b7\u5f97\u5b8c\u5168\u4e3b\u52a8\u57fa\u7ebf\uff1b\u7136\u540e\u5c06\u8ba1\u5212\u8f6c\u6362\u4e3a\u7b80\u5355\u4e0d\u786e\u5b9a\u65f6\u95f4\u7f51\u7edc\uff0c\u9a8c\u8bc1\u52a8\u6001\u53ef\u63a7\u6027\uff0c\u786e\u4fdd\u5b9e\u65f6\u8c03\u5ea6\u5668\u80fd\u5728\u6bcf\u4e2a\u6709\u754c\u6301\u7eed\u65f6\u95f4\u5b9e\u73b0\u4e2d\u91cd\u65b0\u5b89\u6392\u6d3b\u52a8\u800c\u4e0d\u8fdd\u53cd\u8d44\u6e90\u6216\u622a\u6b62\u65f6\u95f4\u7ea6\u675f\u3002", "result": "\u5728Kacem 1-4\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u4e0a\u7684\u5e7f\u6cdb\u8499\u7279\u5361\u6d1b\u6a21\u62df\u663e\u793a\uff0c\u6df7\u5408\u65b9\u6cd5\u6d88\u9664\u4e86\u6700\u5148\u8fdb\u5143\u542f\u53d1\u5f0f\u8c03\u5ea6\u4e2d\u89c2\u5bdf\u5230\u7684100%\u622a\u6b62\u65f6\u95f4\u8fdd\u89c4\uff0c\u540c\u65f6\u4ec5\u589e\u52a03-5%\u5236\u9020\u5468\u671f\u5f00\u9500\u3002\u53ef\u6269\u5c55\u6027\u5b9e\u9a8c\u8bc1\u5b9e\uff0c\u5728\u4e2d\u7b49\u89c4\u6a21\u5b9e\u4f8b\u4e0a\uff0cCP\u6c42\u89e3\u65f6\u95f4\u548cSTNU\u68c0\u67e5\u4fdd\u6301\u4e9a\u79d2\u7ea7\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c55\u793a\u4e86\u65f6\u95f4\u7f51\u7edc\u63a8\u7406\u5982\u4f55\u5f25\u5408\u4e3b\u52a8\u7f13\u51b2\u548c\u52a8\u6001\u9c81\u68d2\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4f7f\u5de5\u4e1a\u66f4\u63a5\u8fd1\u771f\u6b63\u7684\u6570\u5b57\u5316\u3001\u81ea\u6821\u6b63\u5de5\u5382\u3002"}}
{"id": "2510.16227", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16227", "abs": "https://arxiv.org/abs/2510.16227", "authors": ["Jennifer Hu", "Ethan Gotlieb Wilcox", "Siyuan Song", "Kyle Mahowald", "Roger P. Levy"], "title": "What Can String Probability Tell Us About Grammaticality?", "comment": null, "summary": "What have language models (LMs) learned about grammar? This question remains\nhotly debated, with major ramifications for linguistic theory. However, since\nprobability and grammaticality are distinct notions in linguistics, it is not\nobvious what string probabilities can reveal about an LM's underlying\ngrammatical knowledge. We present a theoretical analysis of the relationship\nbetween grammar, meaning, and string probability, based on simple assumptions\nabout the generative process of corpus data. Our framework makes three\npredictions, which we validate empirically using 280K sentence pairs in English\nand Chinese: (1) correlation between the probability of strings within minimal\npairs, i.e., string pairs with minimal semantic differences; (2) correlation\nbetween models' and humans' deltas within minimal pairs; and (3) poor\nseparation in probability space between unpaired grammatical and ungrammatical\nstrings. Our analyses give theoretical grounding for using probability to learn\nabout LMs' structural knowledge, and suggest directions for future work in LM\ngrammatical evaluation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u63a2\u8ba8\u4e86\u8bed\u8a00\u6a21\u578b\u5bf9\u8bed\u6cd5\u7684\u5b66\u4e60\u60c5\u51b5\uff0c\u5efa\u7acb\u4e86\u8bed\u6cd5\u3001\u610f\u4e49\u548c\u5b57\u7b26\u4e32\u6982\u7387\u4e4b\u95f4\u7684\u5173\u7cfb\u6846\u67b6\uff0c\u5e76\u9a8c\u8bc1\u4e86\u4e09\u4e2a\u5173\u952e\u9884\u6d4b\u3002", "motivation": "\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u662f\u5426\u771f\u6b63\u5b66\u4e60\u4e86\u8bed\u6cd5\u77e5\u8bc6\uff0c\u56e0\u4e3a\u6982\u7387\u548c\u8bed\u6cd5\u6027\u5728\u8bed\u8a00\u5b66\u4e2d\u662f\u4e0d\u540c\u6982\u5ff5\uff0c\u9700\u8981\u660e\u786e\u5b57\u7b26\u4e32\u6982\u7387\u80fd\u63ed\u793a\u8bed\u8a00\u6a21\u578b\u5e95\u5c42\u8bed\u6cd5\u77e5\u8bc6\u7684\u7a0b\u5ea6\u3002", "method": "\u57fa\u4e8e\u8bed\u6599\u5e93\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u7684\u7b80\u5355\u5047\u8bbe\uff0c\u5efa\u7acb\u4e86\u7406\u8bba\u5206\u6790\u6846\u67b6\uff0c\u5e76\u4f7f\u752828\u4e07\u53e5\u82f1\u8bed\u548c\u4e2d\u6587\u53e5\u5b50\u5bf9\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u5305\u62ec\u6700\u5c0f\u5bf9\u5206\u6790\u3001\u6a21\u578b\u4e0e\u4eba\u7c7b\u5224\u65ad\u76f8\u5173\u6027\u7b49\u65b9\u6cd5\u3002", "result": "\u9a8c\u8bc1\u4e86\u4e09\u4e2a\u9884\u6d4b\uff1a(1)\u6700\u5c0f\u5bf9\u4e2d\u5b57\u7b26\u4e32\u6982\u7387\u7684\u76f8\u5173\u6027\uff1b(2)\u6a21\u578b\u4e0e\u4eba\u7c7b\u5728\u6700\u5c0f\u5bf9\u4e2d\u5224\u65ad\u5dee\u5f02\u7684\u76f8\u5173\u6027\uff1b(3)\u8bed\u6cd5\u548c\u4e0d\u5408\u8bed\u6cd5\u5b57\u7b26\u4e32\u5728\u6982\u7387\u7a7a\u95f4\u4e2d\u96be\u4ee5\u5206\u79bb\u3002", "conclusion": "\u4e3a\u4f7f\u7528\u6982\u7387\u6765\u4e86\u89e3\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u6784\u77e5\u8bc6\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u4e3a\u672a\u6765\u8bed\u8a00\u6a21\u578b\u8bed\u6cd5\u8bc4\u4f30\u5de5\u4f5c\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.16095", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16095", "abs": "https://arxiv.org/abs/2510.16095", "authors": ["Dou Liu", "Ying Long", "Sophia Zuoqiu", "Di Liu", "Kang Li", "Yiting Lin", "Hanyi Liu", "Rong Yin", "Tian Tang"], "title": "Reliability of Large Language Model Generated Clinical Reasoning in Assisted Reproductive Technology: Blinded Comparative Evaluation Study", "comment": null, "summary": "Creating high-quality clinical Chains-of-Thought (CoTs) is crucial for\nexplainable medical Artificial Intelligence (AI) while constrained by data\nscarcity. Although Large Language Models (LLMs) can synthesize medical data,\ntheir clinical reliability remains unverified. This study evaluates the\nreliability of LLM-generated CoTs and investigates prompting strategies to\nenhance their quality. In a blinded comparative study, senior clinicians in\nAssisted Reproductive Technology (ART) evaluated CoTs generated via three\ndistinct strategies: Zero-shot, Random Few-shot (using shallow examples), and\nSelective Few-shot (using diverse, high-quality examples). These expert ratings\nwere compared against evaluations from a state-of-the-art AI model (GPT-4o).\nThe Selective Few-shot strategy significantly outperformed other strategies\nacross all human evaluation metrics (p < .001). Critically, the Random Few-shot\nstrategy offered no significant improvement over the Zero-shot baseline,\ndemonstrating that low-quality examples are as ineffective as no examples. The\nsuccess of the Selective strategy is attributed to two principles:\n\"Gold-Standard Depth\" (reasoning quality) and \"Representative Diversity\"\n(generalization). Notably, the AI evaluator failed to discern these critical\nperformance differences. The clinical reliability of synthetic CoTs is dictated\nby strategic prompt curation, not the mere presence of examples. We propose a\n\"Dual Principles\" framework as a foundational methodology to generate\ntrustworthy data at scale. This work offers a validated solution to the data\nbottleneck and confirms the indispensable role of human expertise in evaluating\nhigh-stakes clinical AI.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86LLM\u751f\u6210\u7684\u4e34\u5e8a\u601d\u7ef4\u94fe\u7684\u53ef\u9760\u6027\uff0c\u53d1\u73b0\u9009\u62e9\u6027\u5c11\u6837\u672c\u63d0\u793a\u7b56\u7565\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5173\u952e\u5728\u4e8e\u9ad8\u8d28\u91cf\u793a\u4f8b\u7684\u6df1\u5ea6\u548c\u591a\u6837\u6027\uff0c\u800c\u975e\u793a\u4f8b\u6570\u91cf\u3002", "motivation": "\u89e3\u51b3\u9ad8\u8d28\u91cf\u4e34\u5e8a\u601d\u7ef4\u94fe\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u9a8c\u8bc1LLM\u751f\u6210\u533b\u7597\u6570\u636e\u7684\u53ef\u9760\u6027\uff0c\u5e76\u63a2\u7d22\u63d0\u5347\u5176\u8d28\u91cf\u7684\u63d0\u793a\u7b56\u7565\u3002", "method": "\u91c7\u7528\u76f2\u6cd5\u6bd4\u8f83\u7814\u7a76\uff0c\u7531\u8d44\u6df1\u751f\u6b96\u533b\u5b66\u4e13\u5bb6\u8bc4\u4f30\u4e09\u79cd\u63d0\u793a\u7b56\u7565\uff08\u96f6\u6837\u672c\u3001\u968f\u673a\u5c11\u6837\u672c\u3001\u9009\u62e9\u6027\u5c11\u6837\u672c\uff09\u751f\u6210\u7684\u601d\u7ef4\u94fe\uff0c\u5e76\u4e0eGPT-4o\u7684\u8bc4\u4f30\u7ed3\u679c\u5bf9\u6bd4\u3002", "result": "\u9009\u62e9\u6027\u5c11\u6837\u672c\u7b56\u7565\u5728\u6240\u6709\u4eba\u7c7b\u8bc4\u4f30\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u7b56\u7565\uff0c\u968f\u673a\u5c11\u6837\u672c\u76f8\u6bd4\u96f6\u6837\u672c\u65e0\u663e\u8457\u6539\u8fdb\u3002AI\u8bc4\u4f30\u5668\u672a\u80fd\u8bc6\u522b\u5173\u952e\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "\u5408\u6210\u601d\u7ef4\u94fe\u7684\u4e34\u5e8a\u53ef\u9760\u6027\u53d6\u51b3\u4e8e\u7b56\u7565\u6027\u63d0\u793a\u8bbe\u8ba1\uff0c\u63d0\u51fa\"\u53cc\u539f\u5219\"\u6846\u67b6\u4f5c\u4e3a\u751f\u6210\u53ef\u4fe1\u6570\u636e\u7684\u57fa\u7840\u65b9\u6cd5\uff0c\u5f3a\u8c03\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u5728\u9ad8\u98ce\u9669\u4e34\u5e8aAI\u8bc4\u4f30\u4e2d\u7684\u4e0d\u53ef\u6216\u7f3a\u6027\u3002"}}
{"id": "2510.16257", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16257", "abs": "https://arxiv.org/abs/2510.16257", "authors": ["Chu Fei Luo", "Samuel Dahan", "Xiaodan Zhu"], "title": "Towards Low-Resource Alignment to Diverse Perspectives with Sparse Feedback", "comment": "Findings of EMNLP 2025, 5 pages", "summary": "As language models have a greater impact on society, it is important to\nensure they are aligned to a diverse range of perspectives and are able to\nreflect nuance in human values. However, the most popular training paradigms\nfor modern language models often assume there is one optimal answer for every\nquery, leading to generic responses and poor alignment. In this work, we aim to\nenhance pluralistic alignment of language models in a low-resource setting with\ntwo methods: pluralistic decoding and model steering. We empirically\ndemonstrate that model steering offers consistent improvement over zero-shot\nand few-shot baselines with only 50 annotated samples. Our proposed methods\ndecrease false positives in several high-stakes tasks such as hate speech\ndetection and misinformation detection, and improves the distributional\nalignment to human values in GlobalOpinionQA. We hope our work highlights the\nimportance of diversity and how language models can be adapted to consider\nnuanced perspectives.", "AI": {"tldr": "\u901a\u8fc7\u591a\u5143\u5316\u89e3\u7801\u548c\u6a21\u578b\u5f15\u5bfc\u65b9\u6cd5\uff0c\u5728\u4f4e\u8d44\u6e90\u8bbe\u7f6e\u4e0b\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u7684\u591a\u5143\u5bf9\u9f50\u80fd\u529b\uff0c\u4ec5\u752850\u4e2a\u6807\u6ce8\u6837\u672c\u5c31\u80fd\u5728\u591a\u4e2a\u9ad8\u98ce\u9669\u4efb\u52a1\u4e2d\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u5bf9\u793e\u4f1a\u5f71\u54cd\u65e5\u76ca\u589e\u5927\uff0c\u9700\u8981\u786e\u4fdd\u5176\u80fd\u591f\u5bf9\u9f50\u591a\u6837\u5316\u7684\u89c2\u70b9\u5e76\u53cd\u6620\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u7ec6\u5fae\u5dee\u522b\u3002\u5f53\u524d\u4e3b\u6d41\u8bad\u7ec3\u8303\u5f0f\u5047\u8bbe\u6bcf\u4e2a\u67e5\u8be2\u53ea\u6709\u4e00\u4e2a\u6700\u4f18\u7b54\u6848\uff0c\u5bfc\u81f4\u54cd\u5e94\u6cdb\u5316\u4e14\u5bf9\u9f50\u6548\u679c\u5dee\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff1a\u591a\u5143\u5316\u89e3\u7801\u548c\u6a21\u578b\u5f15\u5bfc\u3002\u5728\u4f4e\u8d44\u6e90\u8bbe\u7f6e\u4e0b\uff08\u4ec550\u4e2a\u6807\u6ce8\u6837\u672c\uff09\u8fdb\u884c\u5b9e\u9a8c\uff0c\u4e0e\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u57fa\u7ebf\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u6a21\u578b\u5f15\u5bfc\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u57fa\u7ebf\u4e0a\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u6539\u8fdb\u3002\u5728\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u548c\u9519\u8bef\u4fe1\u606f\u68c0\u6d4b\u7b49\u9ad8\u98ce\u9669\u4efb\u52a1\u4e2d\u964d\u4f4e\u4e86\u5047\u9633\u6027\u7387\uff0c\u5728GlobalOpinionQA\u4e2d\u6539\u5584\u4e86\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u5206\u5e03\u5bf9\u9f50\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5f3a\u8c03\u4e86\u591a\u6837\u6027\u7684\u91cd\u8981\u6027\uff0c\u5c55\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u9002\u5e94\u8003\u8651\u7ec6\u5fae\u89c2\u70b9\uff0c\u4e3a\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u7684\u591a\u5143\u5bf9\u9f50\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2510.16193", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16193", "abs": "https://arxiv.org/abs/2510.16193", "authors": ["Elija Perrier"], "title": "Operationalising Extended Cognition: Formal Metrics for Corporate Knowledge and Legal Accountability", "comment": "Under review", "summary": "Corporate responsibility turns on notions of corporate \\textit{mens rea},\ntraditionally imputed from human agents. Yet these assumptions are under\nchallenge as generative AI increasingly mediates enterprise decision-making.\nBuilding on the theory of extended cognition, we argue that in response\ncorporate knowledge may be redefined as a dynamic capability, measurable by the\nefficiency of its information-access procedures and the validated reliability\nof their outputs. We develop a formal model that captures epistemic states of\ncorporations deploying sophisticated AI or information systems, introducing a\ncontinuous organisational knowledge metric $S_S(\\varphi)$ which integrates a\npipeline's computational cost and its statistically validated error rate. We\nderive a thresholded knowledge predicate $\\mathsf{K}_S$ to impute knowledge and\na firm-wide epistemic capacity index $\\mathcal{K}_{S,t}$ to measure overall\ncapability. We then operationally map these quantitative metrics onto the legal\nstandards of actual knowledge, constructive knowledge, wilful blindness, and\nrecklessness. Our work provides a pathway towards creating measurable and\njusticiable audit artefacts, that render the corporate mind tractable and\naccountable in the algorithmic age.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u5c55\u8ba4\u77e5\u7406\u8bba\u7684\u5f62\u5f0f\u5316\u6a21\u578b\uff0c\u5c06\u4f01\u4e1a\u77e5\u8bc6\u91cd\u65b0\u5b9a\u4e49\u4e3a\u53ef\u6d4b\u91cf\u7684\u52a8\u6001\u80fd\u529b\uff0c\u901a\u8fc7\u4fe1\u606f\u8bbf\u95ee\u7a0b\u5e8f\u7684\u6548\u7387\u548c\u8f93\u51fa\u53ef\u9760\u6027\u6765\u91cf\u5316\u4f01\u4e1a\u77e5\u8bc6\u72b6\u6001\uff0c\u4e3aAI\u65f6\u4ee3\u7684\u4f01\u4e1a\u8d23\u4efb\u8ba4\u5b9a\u63d0\u4f9b\u53ef\u5ba1\u8ba1\u7684\u5ea6\u91cf\u6807\u51c6\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u5728\u4f01\u4e1a\u51b3\u7b56\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f20\u7edf\u57fa\u4e8e\u4eba\u7c7b\u4ee3\u7406\u4eba\u7684\u4f01\u4e1a\u4e3b\u89c2\u610f\u56fe\u8ba4\u5b9a\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u91cd\u65b0\u5b9a\u4e49\u4f01\u4e1a\u77e5\u8bc6\u7684\u6982\u5ff5\u4ee5\u9002\u5e94\u7b97\u6cd5\u65f6\u4ee3\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5f62\u5f0f\u5316\u6a21\u578b\uff0c\u901a\u8fc7\u6574\u5408\u8ba1\u7b97\u6210\u672c\u548c\u7edf\u8ba1\u9a8c\u8bc1\u9519\u8bef\u7387\u6765\u5ea6\u91cf\u4f01\u4e1a\u77e5\u8bc6\u72b6\u6001\uff0c\u5f15\u5165\u8fde\u7eed\u7684\u7ec4\u7ec7\u77e5\u8bc6\u5ea6\u91cfS_S(\u03c6)\u548c\u4f01\u4e1a\u8303\u56f4\u8ba4\u77e5\u80fd\u529b\u6307\u6570K_S,t\uff0c\u5e76\u5c06\u8fd9\u4e9b\u5b9a\u91cf\u6307\u6807\u6620\u5c04\u5230\u6cd5\u5f8b\u6807\u51c6\u4e0a\u3002", "result": "\u63d0\u51fa\u4e86\u53ef\u6d4b\u91cf\u7684\u4f01\u4e1a\u77e5\u8bc6\u9608\u503c\u8c13\u8bcdK_S\u548c\u4f01\u4e1a\u8ba4\u77e5\u80fd\u529b\u6307\u6570\uff0c\u4e3a\u521b\u5efa\u53ef\u5ba1\u8ba1\u7684\u8bc1\u636e\u63d0\u4f9b\u4e86\u8def\u5f84\uff0c\u4f7f\u4f01\u4e1a\u601d\u7ef4\u5728\u7b97\u6cd5\u65f6\u4ee3\u53d8\u5f97\u53ef\u8ffd\u8e2a\u548c\u53ef\u95ee\u8d23\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5728\u751f\u6210\u5f0fAI\u4e3b\u5bfc\u7684\u4f01\u4e1a\u73af\u5883\u4e2d\u5efa\u7acb\u53ef\u6d4b\u91cf\u548c\u53ef\u53f8\u6cd5\u7684\u4f01\u4e1a\u8d23\u4efb\u8ba4\u5b9a\u6846\u67b6\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u65b9\u6cd5\u8def\u5f84\u3002"}}
{"id": "2510.16282", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16282", "abs": "https://arxiv.org/abs/2510.16282", "authors": ["Zhaoxuan Tan", "Zixuan Zhang", "Haoyang Wen", "Zheng Li", "Rongzhi Zhang", "Pei Chen", "Fengran Mo", "Zheyuan Liu", "Qingkai Zeng", "Qingyu Yin", "Meng Jiang"], "title": "Instant Personalized Large Language Model Adaptation via Hypernetwork", "comment": null, "summary": "Personalized large language models (LLMs) tailor content to individual\npreferences using user profiles or histories. However, existing\nparameter-efficient fine-tuning (PEFT) methods, such as the\n``One-PEFT-Per-User'' (OPPU) paradigm, require training a separate adapter for\neach user, making them computationally expensive and impractical for real-time\nupdates. We introduce Profile-to-PEFT, a scalable framework that employs a\nhypernetwork, trained end-to-end, to map a user's encoded profile directly to a\nfull set of adapter parameters (e.g., LoRA), eliminating per-user training at\ndeployment. This design enables instant adaptation, generalization to unseen\nusers, and privacy-preserving local deployment. Experimental results\ndemonstrate that our method outperforms both prompt-based personalization and\nOPPU while using substantially fewer computational resources at deployment. The\nframework exhibits strong generalization to out-of-distribution users and\nmaintains robustness across varying user activity levels and different\nembedding backbones. The proposed Profile-to-PEFT framework enables efficient,\nscalable, and adaptive LLM personalization suitable for large-scale\napplications.", "AI": {"tldr": "Profile-to-PEFT\u6846\u67b6\u4f7f\u7528\u8d85\u7f51\u7edc\u5c06\u7528\u6237\u914d\u7f6e\u6587\u4ef6\u76f4\u63a5\u6620\u5c04\u5230\u9002\u914d\u5668\u53c2\u6570\uff0c\u65e0\u9700\u4e3a\u6bcf\u4e2a\u7528\u6237\u5355\u72ec\u8bad\u7ec3\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684LLM\u4e2a\u6027\u5316\u3002", "motivation": "\u73b0\u6709\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u9700\u8981\u4e3a\u6bcf\u4e2a\u7528\u6237\u8bad\u7ec3\u5355\u72ec\u7684\u9002\u914d\u5668\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u65e0\u6cd5\u5b9e\u65f6\u66f4\u65b0\uff0c\u9650\u5236\u4e86\u5927\u89c4\u6a21\u5e94\u7528\u3002", "method": "\u91c7\u7528\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u8d85\u7f51\u7edc\uff0c\u5c06\u7f16\u7801\u540e\u7684\u7528\u6237\u914d\u7f6e\u6587\u4ef6\u76f4\u63a5\u6620\u5c04\u5230\u5b8c\u6574\u7684\u9002\u914d\u5668\u53c2\u6570\uff08\u5982LoRA\uff09\uff0c\u90e8\u7f72\u65f6\u65e0\u9700\u5bf9\u6bcf\u4e2a\u7528\u6237\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u63d0\u793a\u7684\u4e2a\u6027\u5316\u548cOPPU\u65b9\u6cd5\uff0c\u540c\u65f6\u90e8\u7f72\u65f6\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u663e\u8457\u51cf\u5c11\uff0c\u5bf9\u5206\u5e03\u5916\u7528\u6237\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Profile-to-PEFT\u6846\u67b6\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u81ea\u9002\u5e94\u7684LLM\u4e2a\u6027\u5316\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2510.16194", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16194", "abs": "https://arxiv.org/abs/2510.16194", "authors": ["Guanchen Wu", "Zuhui Chen", "Yuzhang Xie", "Carl Yang"], "title": "Towards Automatic Evaluation and Selection of PHI De-identification Models via Multi-Agent Collaboration", "comment": "Agents4Science 2025 (Spotlight)", "summary": "Protected health information (PHI) de-identification is critical for enabling\nthe safe reuse of clinical notes, yet evaluating and comparing PHI\nde-identification models typically depends on costly, small-scale expert\nannotations. We present TEAM-PHI, a multi-agent evaluation and selection\nframework that uses large language models (LLMs) to automatically measure\nde-identification quality and select the best-performing model without heavy\nreliance on gold labels. TEAM-PHI deploys multiple Evaluation Agents, each\nindependently judging the correctness of PHI extractions and outputting\nstructured metrics. Their results are then consolidated through an LLM-based\nmajority voting mechanism that integrates diverse evaluator perspectives into a\nsingle, stable, and reproducible ranking. Experiments on a real-world clinical\nnote corpus demonstrate that TEAM-PHI produces consistent and accurate\nrankings: despite variation across individual evaluators, LLM-based voting\nreliably converges on the same top-performing systems. Further comparison with\nground-truth annotations and human evaluation confirms that the framework's\nautomated rankings closely match supervised evaluation. By combining\nindependent evaluation agents with LLM majority voting, TEAM-PHI offers a\npractical, secure, and cost-effective solution for automatic evaluation and\nbest-model selection in PHI de-identification, even when ground-truth labels\nare limited.", "AI": {"tldr": "TEAM-PHI\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u8bc4\u4f30PHI\u53bb\u6807\u8bc6\u5316\u6a21\u578b\u8d28\u91cf\u5e76\u9009\u62e9\u6700\u4f73\u6a21\u578b\uff0c\u65e0\u9700\u4f9d\u8d56\u6602\u8d35\u7684\u4e13\u5bb6\u6807\u6ce8\u3002", "motivation": "PHI\u53bb\u6807\u8bc6\u5316\u5bf9\u4e8e\u4e34\u5e8a\u7b14\u8bb0\u7684\u5b89\u5168\u91cd\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u8bc4\u4f30\u4f9d\u8d56\u6210\u672c\u9ad8\u6602\u7684\u5c0f\u89c4\u6a21\u4e13\u5bb6\u6807\u6ce8\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6bd4\u8f83\u548c\u53d1\u5c55\u3002", "method": "\u90e8\u7f72\u591a\u4e2a\u8bc4\u4f30\u667a\u80fd\u4f53\u72ec\u7acb\u5224\u65adPHI\u63d0\u53d6\u6b63\u786e\u6027\uff0c\u901a\u8fc7\u57fa\u4e8eLLM\u7684\u591a\u6570\u6295\u7968\u673a\u5236\u6574\u5408\u7ed3\u679c\uff0c\u751f\u6210\u7a33\u5b9a\u53ef\u590d\u73b0\u7684\u6a21\u578b\u6392\u540d\u3002", "result": "\u5728\u771f\u5b9e\u4e34\u5e8a\u7b14\u8bb0\u8bed\u6599\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTEAM-PHI\u80fd\u4ea7\u751f\u4e00\u81f4\u51c6\u786e\u7684\u6392\u540d\uff0c\u5c3d\u7ba1\u4e2a\u4f53\u8bc4\u4f30\u8005\u5b58\u5728\u5dee\u5f02\uff0c\u4f46LLM\u6295\u7968\u80fd\u53ef\u9760\u5730\u6536\u655b\u5230\u76f8\u540c\u7684\u6700\u4f73\u7cfb\u7edf\u3002", "conclusion": "TEAM-PHI\u901a\u8fc7\u7ed3\u5408\u72ec\u7acb\u8bc4\u4f30\u667a\u80fd\u4f53\u548cLLM\u591a\u6570\u6295\u7968\uff0c\u4e3aPHI\u53bb\u6807\u8bc6\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u5b89\u5168\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u81ea\u52a8\u8bc4\u4f30\u548c\u6700\u4f73\u6a21\u578b\u9009\u62e9\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16340", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16340", "abs": "https://arxiv.org/abs/2510.16340", "authors": ["Pratham Singla", "Shivank Garg", "Ayush Singh", "Ishan Garg", "Ketan Suhaas Saichandran"], "title": "Thinking About Thinking: Evaluating Reasoning in Post-Trained Language Models", "comment": null, "summary": "Recent advances in post-training techniques have endowed Large Language\nModels (LLMs) with enhanced capabilities for tackling complex, logic-intensive\ntasks through the generation of supplementary planning tokens. This development\nraises a fundamental question: Are these models aware of what they \"learn\" and\n\"think\"? To address this, we define three core competencies: (1) awareness of\nlearned latent policies, (2) generalization of these policies across domains,\nand (3) alignment between internal reasoning traces and final outputs. We\nempirically evaluate these abilities on several tasks, each designed to require\nlearning a distinct policy. Furthermore, we contrast the profiles of models\npost-trained via Supervised Fine-Tuning (SFT), Direct Policy Optimization\n(DPO), and Group Relative Policy Optimization (GRPO). Our findings indicate\nthat RL-trained models not only demonstrate greater awareness of their learned\nbehaviors and stronger generalizability to novel, structurally similar tasks\nthan SFT models but also often exhibit weak alignment between their reasoning\ntraces and final outputs, an effect most pronounced in GRPO-trained models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u540e\u8bad\u7ec3LLM\u5bf9\u5176\u5b66\u4e60\u5185\u5bb9\u548c\u63a8\u7406\u8fc7\u7a0b\u7684\u8ba4\u77e5\u80fd\u529b\uff0c\u53d1\u73b0RL\u8bad\u7ec3\u6a21\u578b\u6bd4SFT\u6a21\u578b\u66f4\u4e86\u89e3\u81ea\u8eab\u884c\u4e3a\u4e14\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\uff0c\u4f46\u63a8\u7406\u8fc7\u7a0b\u4e0e\u6700\u7ec8\u8f93\u51fa\u7684\u5bf9\u9f50\u8f83\u5f31\u3002", "motivation": "\u63a2\u7a76LLM\u662f\u5426\u610f\u8bc6\u5230\u81ea\u5df1\"\u5b66\u4e60\"\u548c\"\u601d\u8003\"\u7684\u5185\u5bb9\uff0c\u5b9a\u4e49\u4e09\u4e2a\u6838\u5fc3\u80fd\u529b\uff1a\u5bf9\u5b66\u4e60\u7b56\u7565\u7684\u8ba4\u77e5\u3001\u7b56\u7565\u8de8\u9886\u57df\u6cdb\u5316\u3001\u5185\u90e8\u63a8\u7406\u4e0e\u8f93\u51fa\u7684\u5bf9\u9f50\u3002", "method": "\u5728\u591a\u4e2a\u9700\u8981\u5b66\u4e60\u4e0d\u540c\u7b56\u7565\u7684\u4efb\u52a1\u4e0a\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u5bf9\u6bd4SFT\u3001DPO\u548cGRPO\u4e09\u79cd\u540e\u8bad\u7ec3\u65b9\u6cd5\u7684\u6a21\u578b\u8868\u73b0\u3002", "result": "RL\u8bad\u7ec3\u6a21\u578b\u6bd4SFT\u6a21\u578b\u66f4\u4e86\u89e3\u5b66\u4e60\u884c\u4e3a\u4e14\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\uff0c\u4f46\u63a8\u7406\u8fc7\u7a0b\u4e0e\u6700\u7ec8\u8f93\u51fa\u7684\u5bf9\u9f50\u8f83\u5f31\uff0cGRPO\u8bad\u7ec3\u6a21\u578b\u8fd9\u4e00\u6548\u5e94\u6700\u660e\u663e\u3002", "conclusion": "\u540e\u8bad\u7ec3\u65b9\u6cd5\u5f71\u54cdLLM\u7684\u81ea\u6211\u8ba4\u77e5\u80fd\u529b\uff0cRL\u65b9\u6cd5\u589e\u5f3a\u7b56\u7565\u8ba4\u77e5\u548c\u6cdb\u5316\u4f46\u524a\u5f31\u63a8\u7406\u5bf9\u9f50\uff0c\u9700\u8981\u5e73\u8861\u8fd9\u4e9b\u80fd\u529b\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.16206", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16206", "abs": "https://arxiv.org/abs/2510.16206", "authors": ["Alex Zhavoronkov", "Dominika Wilczok", "Roman Yampolskiy"], "title": "The Right to Be Remembered: Preserving Maximally Truthful Digital Memory in the Age of AI", "comment": null, "summary": "Since the rapid expansion of large language models (LLMs), people have begun\nto rely on them for information retrieval. While traditional search engines\ndisplay ranked lists of sources shaped by search engine optimization (SEO),\nadvertising, and personalization, LLMs typically provide a synthesized response\nthat feels singular and authoritative. While both approaches carry risks of\nbias and omission, LLMs may amplify the effect by collapsing multiple\nperspectives into one answer, reducing users ability or inclination to compare\nalternatives. This concentrates power over information in a few LLM vendors\nwhose systems effectively shape what is remembered and what is overlooked. As a\nresult, certain narratives, individuals or groups, may be disproportionately\nsuppressed, while others are disproportionately elevated. Over time, this\ncreates a new threat: the gradual erasure of those with limited digital\npresence, and the amplification of those already prominent, reshaping\ncollective memory.To address these concerns, this paper presents a concept of\nthe Right To Be Remembered (RTBR) which encompasses minimizing the risk of\nAI-driven information omission, embracing the right of fair treatment, while\nensuring that the generated content would be maximally truthful.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\"\u88ab\u8bb0\u4f4f\u6743\"\u6982\u5ff5\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u80fd\u5bfc\u81f4\u4fe1\u606f\u9057\u6f0f\u548c\u504f\u89c1\u7684\u95ee\u9898\uff0c\u786e\u4fddAI\u751f\u6210\u5185\u5bb9\u7684\u771f\u5b9e\u6027\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u666e\u53ca\uff0c\u4eba\u4eec\u5f00\u59cb\u4f9d\u8d56\u5b83\u4eec\u8fdb\u884c\u4fe1\u606f\u68c0\u7d22\u3002\u4f46LLMs\u5c06\u591a\u4e2a\u89c2\u70b9\u5408\u6210\u4e3a\u4e00\u4e2a\u770b\u4f3c\u6743\u5a01\u7684\u7b54\u6848\uff0c\u53ef\u80fd\u653e\u5927\u504f\u89c1\u548c\u9057\u6f0f\u6548\u5e94\uff0c\u4f7f\u67d0\u4e9b\u7fa4\u4f53\u88ab\u4e0d\u6210\u6bd4\u4f8b\u5730\u538b\u5236\uff0c\u800c\u53e6\u4e00\u4e9b\u88ab\u8fc7\u5ea6\u653e\u5927\uff0c\u5a01\u80c1\u96c6\u4f53\u8bb0\u5fc6\u7684\u5b8c\u6574\u6027\u3002", "method": "\u63d0\u51fa\"\u88ab\u8bb0\u4f4f\u6743\"\u6982\u5ff5\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u8981\u7d20\uff1a\u6700\u5c0f\u5316AI\u9a71\u52a8\u4fe1\u606f\u9057\u6f0f\u98ce\u9669\u3001\u4fdd\u969c\u516c\u5e73\u5bf9\u5f85\u6743\u5229\u3001\u786e\u4fdd\u751f\u6210\u5185\u5bb9\u6700\u5927\u7a0b\u5ea6\u771f\u5b9e\u3002", "result": "\u6982\u5ff5\u6027\u6846\u67b6\u7684\u63d0\u51fa\uff0c\u4e3a\u5e94\u5bf9LLMs\u5bf9\u4fe1\u606f\u751f\u6001\u548c\u96c6\u4f53\u8bb0\u5fc6\u7684\u6f5c\u5728\u5a01\u80c1\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u9700\u8981\u5efa\u7acb\"\u88ab\u8bb0\u4f4f\u6743\"\u6765\u5bf9\u6297LLMs\u53ef\u80fd\u9020\u6210\u7684\u4fe1\u606f\u4e0d\u5e73\u7b49\u548c\u8bb0\u5fc6\u626d\u66f2\uff0c\u4fdd\u62a4\u6570\u5b57\u5b58\u5728\u6709\u9650\u7684\u7fa4\u4f53\uff0c\u786e\u4fdd\u4fe1\u606f\u751f\u6001\u7684\u591a\u6837\u6027\u548c\u771f\u5b9e\u6027\u3002"}}
{"id": "2510.16359", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16359", "abs": "https://arxiv.org/abs/2510.16359", "authors": ["Utsav Dhanuka", "Soham Poddar", "Saptarshi Ghosh"], "title": "Utilising Large Language Models for Generating Effective Counter Arguments to Anti-Vaccine Tweets", "comment": "14 pages, 1 figure, work done as a part of B.Tech project at IIT\n  Kharagpur", "summary": "In an era where public health is increasingly influenced by information\nshared on social media, combatting vaccine skepticism and misinformation has\nbecome a critical societal goal. Misleading narratives around vaccination have\nspread widely, creating barriers to achieving high immunisation rates and\nundermining trust in health recommendations. While efforts to detect\nmisinformation have made significant progress, the generation of real time\ncounter-arguments tailored to debunk such claims remains an insufficiently\nexplored area. In this work, we explore the capabilities of LLMs to generate\nsound counter-argument rebuttals to vaccine misinformation. Building on prior\nresearch in misinformation debunking, we experiment with various prompting\nstrategies and fine-tuning approaches to optimise counter-argument generation.\nAdditionally, we train classifiers to categorise anti-vaccine tweets into\nmulti-labeled categories such as concerns about vaccine efficacy, side effects,\nand political influences allowing for more context aware rebuttals. Our\nevaluation, conducted through human judgment, LLM based assessments, and\nautomatic metrics, reveals strong alignment across these methods. Our findings\ndemonstrate that integrating label descriptions and structured fine-tuning\nenhances counter-argument effectiveness, offering a promising approach for\nmitigating vaccine misinformation at scale.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u5229\u7528LLMs\u751f\u6210\u9488\u5bf9\u75ab\u82d7\u9519\u8bef\u4fe1\u606f\u7684\u53cd\u9a73\u8bba\u70b9\uff0c\u901a\u8fc7\u591a\u79cd\u63d0\u793a\u7b56\u7565\u548c\u5fae\u8c03\u65b9\u6cd5\u4f18\u5316\u53cd\u9a73\u751f\u6210\uff0c\u5e76\u8bad\u7ec3\u5206\u7c7b\u5668\u5bf9\u53cd\u75ab\u82d7\u63a8\u6587\u8fdb\u884c\u591a\u6807\u7b7e\u5206\u7c7b\uff0c\u4ee5\u751f\u6210\u66f4\u5177\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u53cd\u9a73\u3002", "motivation": "\u5728\u793e\u4ea4\u5a92\u4f53\u5f71\u54cd\u516c\u5171\u536b\u751f\u7684\u65f6\u4ee3\uff0c\u6253\u51fb\u75ab\u82d7\u6000\u7591\u8bba\u548c\u9519\u8bef\u4fe1\u606f\u6210\u4e3a\u5173\u952e\u793e\u4f1a\u76ee\u6807\u3002\u867d\u7136\u9519\u8bef\u4fe1\u606f\u68c0\u6d4b\u5df2\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5b9e\u65f6\u751f\u6210\u9488\u5bf9\u6027\u7684\u53cd\u9a73\u8bba\u70b9\u4ecd\u662f\u4e00\u4e2a\u672a\u5145\u5206\u63a2\u7d22\u7684\u9886\u57df\u3002", "method": "\u5b9e\u9a8c\u591a\u79cd\u63d0\u793a\u7b56\u7565\u548c\u5fae\u8c03\u65b9\u6cd5\u4f18\u5316\u53cd\u9a73\u751f\u6210\uff0c\u8bad\u7ec3\u5206\u7c7b\u5668\u5c06\u53cd\u75ab\u82d7\u63a8\u6587\u5206\u7c7b\u4e3a\u75ab\u82d7\u6548\u529b\u3001\u526f\u4f5c\u7528\u3001\u653f\u6cbb\u5f71\u54cd\u7b49\u591a\u6807\u7b7e\u7c7b\u522b\uff0c\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u53cd\u9a73\u3002", "result": "\u901a\u8fc7\u4eba\u5de5\u5224\u65ad\u3001LLM\u8bc4\u4f30\u548c\u81ea\u52a8\u6307\u6807\u7684\u8bc4\u4f30\u663e\u793a\u8fd9\u4e9b\u65b9\u6cd5\u4e4b\u95f4\u5177\u6709\u5f88\u5f3a\u7684\u4e00\u81f4\u6027\u3002\u6574\u5408\u6807\u7b7e\u63cf\u8ff0\u548c\u7ed3\u6784\u5316\u5fae\u8c03\u63d0\u9ad8\u4e86\u53cd\u9a73\u8bba\u70b9\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u7f13\u89e3\u75ab\u82d7\u9519\u8bef\u4fe1\u606f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86LLMs\u5728\u751f\u6210\u9488\u5bf9\u6027\u53cd\u9a73\u65b9\u9762\u7684\u80fd\u529b\u3002"}}
{"id": "2510.16234", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16234", "abs": "https://arxiv.org/abs/2510.16234", "authors": ["Hanane Nour Moussa", "Patrick Queiroz Da Silva", "Daniel Adu-Ampratwum", "Alyson East", "Zitong Lu", "Nikki Puccetti", "Mingyi Xue", "Huan Sun", "Bodhisattwa Prasad Majumder", "Sachin Kumar"], "title": "ScholarEval: Research Idea Evaluation Grounded in Literature", "comment": null, "summary": "As AI tools become increasingly common for research ideation, robust\nevaluation is critical to ensure the validity and usefulness of generated\nideas. We introduce ScholarEval, a retrieval augmented evaluation framework\nthat assesses research ideas based on two fundamental criteria: soundness - the\nempirical validity of proposed methods based on existing literature, and\ncontribution - the degree of advancement made by the idea across different\ndimensions relative to prior research. To evaluate ScholarEval, we introduce\nScholarIdeas, the first expert-annotated dataset of multi-domain research ideas\nand reviews, comprised of 117 ideas across four disciplines: artificial\nintelligence, neuroscience, biochemistry, and ecology. Our evaluation shows\nthat ScholarEval achieves significantly higher coverage of points mentioned in\nthe human expert annotated rubrics in ScholarIdeas compared to all baselines.\nFurthermore, ScholarEval is consistently preferred over our strongest baseline\no4-mini-deep-research, a reasoning and search-enabled agentic system by OpenAI,\nin terms of evaluation actionability, depth, and evidence support. Our\nlarge-scale user study also shows that ScholarEval significantly outperforms\ndeep research in literature engagement, idea refinement, and usefulness. We\nopenly release our code, dataset, and ScholarEval tool for the community to use\nand build on.", "AI": {"tldr": "\u63d0\u51fa\u4e86ScholarEval\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u8bc4\u4f30\u7814\u7a76\u60f3\u6cd5\u7684\u5408\u7406\u6027\u548c\u8d21\u732e\u5ea6\uff0c\u5e76\u5728\u591a\u9886\u57df\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u5176\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740AI\u5de5\u5177\u5728\u7814\u7a76\u6784\u601d\u4e2d\u7684\u666e\u53ca\uff0c\u9700\u8981\u5efa\u7acb\u7a33\u5065\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u786e\u4fdd\u751f\u6210\u60f3\u6cd5\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "method": "\u5f15\u5165ScholarEval\u68c0\u7d22\u589e\u5f3a\u8bc4\u4f30\u6846\u67b6\uff0c\u57fa\u4e8e\u4e24\u4e2a\u6838\u5fc3\u6807\u51c6\u8bc4\u4f30\u7814\u7a76\u60f3\u6cd5\uff1a\u5408\u7406\u6027\uff08\u57fa\u4e8e\u73b0\u6709\u6587\u732e\u7684\u65b9\u6cd5\u6709\u6548\u6027\uff09\u548c\u8d21\u732e\u5ea6\uff08\u76f8\u5bf9\u4e8e\u5148\u524d\u7814\u7a76\u7684\u8fdb\u6b65\u7a0b\u5ea6\uff09\u3002", "result": "\u5728ScholarIdeas\u6570\u636e\u96c6\u4e0a\uff0cScholarEval\u76f8\u6bd4\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u4e13\u5bb6\u6807\u6ce8\u8981\u70b9\u7684\u8986\u76d6\u7387\uff0c\u4e14\u5728\u8bc4\u4f30\u53ef\u64cd\u4f5c\u6027\u3001\u6df1\u5ea6\u548c\u8bc1\u636e\u652f\u6301\u65b9\u9762\u6301\u7eed\u4f18\u4e8eOpenAI\u7684o4-mini-deep-research\u7cfb\u7edf\u3002", "conclusion": "ScholarEval\u6846\u67b6\u5728\u6587\u732e\u53c2\u4e0e\u5ea6\u3001\u60f3\u6cd5\u7cbe\u70bc\u548c\u5b9e\u7528\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6df1\u5ea6\u7814\u7a76\u65b9\u6cd5\uff0c\u4e3a\u7814\u7a76\u60f3\u6cd5\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2510.16363", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16363", "abs": "https://arxiv.org/abs/2510.16363", "authors": ["Nilmadhab Das", "Vishal Vaibhav", "Yash Sunil Choudhary", "V. Vijaya Saradhi", "Ashish Anand"], "title": "End-to-End Argument Mining through Autoregressive Argumentative Structure Prediction", "comment": "Accepted version. To appear in IJCNN 2025", "summary": "Argument Mining (AM) helps in automating the extraction of complex\nargumentative structures such as Argument Components (ACs) like Premise, Claim\netc. and Argumentative Relations (ARs) like Support, Attack etc. in an\nargumentative text. Due to the inherent complexity of reasoning involved with\nthis task, modelling dependencies between ACs and ARs is challenging. Most of\nthe recent approaches formulate this task through a generative paradigm by\nflattening the argumentative structures. In contrast to that, this study\njointly formulates the key tasks of AM in an end-to-end fashion using\nAutoregressive Argumentative Structure Prediction (AASP) framework. The\nproposed AASP framework is based on the autoregressive structure prediction\nframework that has given good performance for several NLP tasks. AASP framework\nmodels the argumentative structures as constrained pre-defined sets of actions\nwith the help of a conditional pre-trained language model. These actions build\nthe argumentative structures step-by-step in an autoregressive manner to\ncapture the flow of argumentative reasoning in an efficient way. Extensive\nexperiments conducted on three standard AM benchmarks demonstrate that AASP\nachieves state-of-theart (SoTA) results across all AM tasks in two benchmarks\nand delivers strong results in one benchmark.", "AI": {"tldr": "\u63d0\u51fa\u4e86AASP\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u7ed3\u6784\u9884\u6d4b\u8054\u5408\u5efa\u6a21\u8bba\u70b9\u6316\u6398\u4e2d\u7684\u7ec4\u4ef6\u548c\u5173\u7cfb\uff0c\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u8303\u5f0f\u6241\u5e73\u5316\u8bba\u8bc1\u7ed3\u6784\uff0c\u96be\u4ee5\u5efa\u6a21\u8bba\u70b9\u7ec4\u4ef6\u548c\u8bba\u8bc1\u5173\u7cfb\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u4f7f\u7528\u81ea\u56de\u5f52\u8bba\u8bc1\u7ed3\u6784\u9884\u6d4b\u6846\u67b6\uff0c\u5c06\u8bba\u8bc1\u7ed3\u6784\u5efa\u6a21\u4e3a\u9884\u5b9a\u4e49\u52a8\u4f5c\u96c6\uff0c\u901a\u8fc7\u6761\u4ef6\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u9010\u6b65\u6784\u5efa\u8bba\u8bc1\u7ed3\u6784\u3002", "result": "\u5728\u4e09\u4e2a\u6807\u51c6AM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAASP\u5728\u4e24\u4e2a\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5728\u4e00\u4e2a\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u5f3a\u52b2\u8868\u73b0\u3002", "conclusion": "AASP\u6846\u67b6\u80fd\u591f\u6709\u6548\u6355\u6349\u8bba\u8bc1\u63a8\u7406\u6d41\u7a0b\uff0c\u5728\u8054\u5408\u5efa\u6a21\u8bba\u70b9\u6316\u6398\u4efb\u52a1\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.16259", "categories": ["cs.AI", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.16259", "abs": "https://arxiv.org/abs/2510.16259", "authors": ["Zhehao Zhang", "Weijie Xu", "Shixian Cui", "Chandan K. Reddy"], "title": "Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense", "comment": "29 pages, 9 tables, 4 figures", "summary": "Recent advances in large reasoning models (LRMs) have enabled remarkable\nperformance on complex tasks such as mathematics and coding by generating long\nChain-of-Thought (CoT) traces. In this paper, we identify and systematically\nanalyze a critical vulnerability we term reasoning distraction, where LRMs are\ndiverted from their primary objective by irrelevant yet complex tasks\nmaliciously embedded in the prompt. Through a comprehensive study across\ndiverse models and benchmarks, we show that even state-of-the-art LRMs are\nhighly susceptible, with injected distractors reducing task accuracy by up to\n60%. We further reveal that certain alignment techniques can amplify this\nweakness and that models may exhibit covert compliance, following hidden\nadversarial instructions in reasoning while concealing them in the final\noutput. To mitigate these risks, we propose a training-based defense that\ncombines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on\nsynthetic adversarial data, improving robustness by over 50 points on\nchallenging distractor attacks. Our findings establish reasoning distraction as\na distinct and urgent threat to LRM reliability and provide a practical step\ntoward safer and more trustworthy reasoning systems.", "AI": {"tldr": "\u672c\u6587\u8bc6\u522b\u5e76\u7cfb\u7edf\u5206\u6790\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u4e2d\u7684\"\u63a8\u7406\u5206\u5fc3\"\u6f0f\u6d1e\uff0c\u5373\u6a21\u578b\u88ab\u6076\u610f\u5d4c\u5165\u7684\u590d\u6742\u65e0\u5173\u4efb\u52a1\u5206\u6563\u6ce8\u610f\u529b\uff0c\u5bfc\u81f4\u4e3b\u8981\u4efb\u52a1\u51c6\u786e\u6027\u4e0b\u964d\u9ad8\u8fbe60%\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u57fa\u4e8e\u8bad\u7ec3\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u5728\u5bf9\u6297\u6027\u6570\u636e\u4e0a\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5c06\u9c81\u68d2\u6027\u63d0\u9ad8\u4e8650\u591a\u4e2a\u70b9\u3002", "motivation": "\u968f\u7740\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u7b49\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f5c\u8005\u53d1\u73b0\u8fd9\u4e9b\u6a21\u578b\u5b58\u5728\u4e00\u4e2a\u5173\u952e\u6f0f\u6d1e\uff1a\u5bb9\u6613\u88ab\u6076\u610f\u5d4c\u5165\u7684\u65e0\u5173\u590d\u6742\u4efb\u52a1\u5206\u6563\u6ce8\u610f\u529b\uff0c\u4ece\u800c\u504f\u79bb\u4e3b\u8981\u76ee\u6807\u3002\u8fd9\u79cd\"\u63a8\u7406\u5206\u5fc3\"\u5a01\u80c1\u5230LRM\u7684\u53ef\u9760\u6027\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u548c\u9632\u5fa1\u3002", "method": "\u4f5c\u8005\u8fdb\u884c\u4e86\u8de8\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u7684\u5168\u9762\u7814\u7a76\uff0c\u8bc4\u4f30LRM\u5bf9\u63a8\u7406\u5206\u5fc3\u7684\u654f\u611f\u6027\u3002\u4e3a\u7f13\u89e3\u98ce\u9669\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u8bad\u7ec3\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u5f3a\u5316\u5b66\u4e60(RL)\u5728\u5408\u6210\u7684\u5bf9\u6297\u6027\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u662f\u5148\u8fdb\u7684LRM\u4e5f\u9ad8\u5ea6\u6613\u53d7\u63a8\u7406\u5206\u5fc3\u5f71\u54cd\uff0c\u6ce8\u5165\u7684\u5e72\u6270\u56e0\u7d20\u53ef\u4f7f\u4efb\u52a1\u51c6\u786e\u6027\u964d\u4f4e\u9ad8\u8fbe60%\u3002\u67d0\u4e9b\u5bf9\u9f50\u6280\u672f\u4f1a\u653e\u5927\u8fd9\u79cd\u5f31\u70b9\uff0c\u6a21\u578b\u53ef\u80fd\u8868\u73b0\u51fa\u9690\u853d\u670d\u4ece\u884c\u4e3a\u3002\u63d0\u51fa\u7684\u9632\u5fa1\u65b9\u6cd5\u5728\u6311\u6218\u6027\u5e72\u6270\u653b\u51fb\u4e0a\u5c06\u9c81\u68d2\u6027\u63d0\u9ad8\u4e8650\u591a\u4e2a\u70b9\u3002", "conclusion": "\u63a8\u7406\u5206\u5fc3\u662f\u5bf9LRM\u53ef\u9760\u6027\u7684\u72ec\u7279\u4e14\u7d27\u8feb\u7684\u5a01\u80c1\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u6784\u5efa\u66f4\u5b89\u5168\u3001\u66f4\u53ef\u4fe1\u7684\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u9645\u6b65\u9aa4\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u4e13\u95e8\u9632\u5fa1\u673a\u5236\u6765\u5e94\u5bf9\u6b64\u7c7b\u653b\u51fb\u3002"}}
{"id": "2510.16373", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16373", "abs": "https://arxiv.org/abs/2510.16373", "authors": ["Federico Ravenda", "Seyed Ali Bahrainian", "Andrea Raballo", "Antonietta Mira"], "title": "Navigating through the hidden embedding space: steering LLMs to improve mental health assessment", "comment": null, "summary": "The rapid evolution of Large Language Models (LLMs) is transforming AI,\nopening new opportunities in sensitive and high-impact areas such as Mental\nHealth (MH). Yet, despite these advancements, recent evidence reveals that\nsmaller-scale models still struggle to deliver optimal performance in\ndomain-specific applications. In this study, we present a cost-efficient yet\npowerful approach to improve MH assessment capabilities of an LLM, without\nrelying on any computationally intensive techniques. Our lightweight method\nconsists of a linear transformation applied to a specific layer's activations,\nleveraging steering vectors to guide the model's output. Remarkably, this\nintervention enables the model to achieve improved results across two distinct\ntasks: (1) identifying whether a Reddit post is useful for detecting the\npresence or absence of depressive symptoms (relevance prediction task), and (2)\ncompleting a standardized psychological screening questionnaire for depression\nbased on users' Reddit post history (questionnaire completion task). Results\nhighlight the untapped potential of steering mechanisms as computationally\nefficient tools for LLMs' MH domain adaptation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ebf\u6027\u53d8\u6362\u7279\u5b9a\u5c42\u6fc0\u6d3b\u6765\u63d0\u5347LLM\u5728\u5fc3\u7406\u5065\u5eb7\u8bc4\u4f30\u4e2d\u7684\u8868\u73b0\uff0c\u65e0\u9700\u8ba1\u7b97\u5bc6\u96c6\u578b\u6280\u672f", "motivation": "\u5c3d\u7ba1LLMs\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u5c0f\u578b\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\u5e94\u7528\u4e2d\u4ecd\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u5fc3\u7406\u5065\u5eb7\u8fd9\u6837\u7684\u654f\u611f\u9ad8\u5f71\u54cd\u9886\u57df", "method": "\u5e94\u7528\u7ebf\u6027\u53d8\u6362\u5230\u7279\u5b9a\u5c42\u7684\u6fc0\u6d3b\uff0c\u5229\u7528\u8f6c\u5411\u5411\u91cf\u6765\u5f15\u5bfc\u6a21\u578b\u8f93\u51fa", "result": "\u8be5\u65b9\u6cd5\u5728\u4e24\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u6539\u8fdb\uff1a\u8bc6\u522bReddit\u5e16\u5b50\u662f\u5426\u6709\u52a9\u4e8e\u68c0\u6d4b\u6291\u90c1\u75c7\u72b6\u7684\u76f8\u5173\u6027\u9884\u6d4b\uff0c\u4ee5\u53ca\u57fa\u4e8e\u7528\u6237Reddit\u5386\u53f2\u5b8c\u6210\u6807\u51c6\u5316\u6291\u90c1\u7b5b\u67e5\u95ee\u5377", "conclusion": "\u8f6c\u5411\u673a\u5236\u4f5c\u4e3a\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u5de5\u5177\uff0c\u5728LLMs\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u9002\u5e94\u65b9\u9762\u5177\u6709\u672a\u5f00\u53d1\u7684\u6f5c\u529b"}}
{"id": "2510.16276", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16276", "abs": "https://arxiv.org/abs/2510.16276", "authors": ["Song Bian", "Minghao Yan", "Anand Jayarajan", "Gennady Pekhimenko", "Shivaram Venkataraman"], "title": "What Limits Agentic Systems Efficiency?", "comment": "27 pages, 15 figures", "summary": "Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have\ndemonstrated strong reasoning capabilities. To further enhance LLM\ncapabilities, recent agentic systems, such as Deep Research, incorporate web\ninteractions into LLM reasoning to mitigate uncertainties and reduce potential\nerrors. However, existing research predominantly focuses on reasoning\nperformance, often neglecting the efficiency of agentic systems. In this work,\nwe present a comprehensive empirical study that identifies efficiency\nbottlenecks in web-interactive agentic systems. We decompose end-to-end latency\ninto two primary components: LLM API latency and web environment latency. We\nconduct a comprehensive empirical study across 15 models and 5 providers to\ndemonstrate high variability in API-based agentic systems. We observe that web\nenvironment latency can contribute as much as 53.7% to the overall latency in a\nweb-based agentic system. To improve latency, we propose SpecCache, a caching\nframework augmented with speculative execution that can reduce web environment\noverhead. Extensive evaluations on two standard benchmarks show that our\napproach improves the cache hit rate by up to 58x compared to a random caching\nstrategy, while reducing web environment overhead by up to 3.2x, without\ndegrading agentic system performance.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u53d1\u73b0\u7f51\u7edc\u4ea4\u4e92\u5f0f\u667a\u80fd\u4f53\u7cfb\u7edf\u5b58\u5728\u6548\u7387\u74f6\u9888\uff0c\u63d0\u51faSpecCache\u7f13\u5b58\u6846\u67b6\u7ed3\u5408\u63a8\u6d4b\u6267\u884c\u6765\u964d\u4f4e\u7f51\u7edc\u73af\u5883\u5ef6\u8fdf\uff0c\u663e\u8457\u63d0\u5347\u7f13\u5b58\u547d\u4e2d\u7387\u548c\u7cfb\u7edf\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u63a8\u7406\u6027\u80fd\uff0c\u4f46\u5ffd\u89c6\u4e86\u7cfb\u7edf\u6548\u7387\u95ee\u9898\u3002\u7f51\u7edc\u4ea4\u4e92\u5f0f\u667a\u80fd\u4f53\u7cfb\u7edf\u5b58\u5728\u663e\u8457\u7684\u5ef6\u8fdf\u74f6\u9888\uff0c\u5f71\u54cd\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002", "method": "\u5c06\u7aef\u5230\u7aef\u5ef6\u8fdf\u5206\u89e3\u4e3aLLM API\u5ef6\u8fdf\u548c\u7f51\u7edc\u73af\u5883\u5ef6\u8fdf\uff0c\u901a\u8fc715\u4e2a\u6a21\u578b\u548c5\u4e2a\u63d0\u4f9b\u5546\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u63d0\u51faSpecCache\u7f13\u5b58\u6846\u67b6\u7ed3\u5408\u63a8\u6d4b\u6267\u884c\u6765\u4f18\u5316\u7f51\u7edc\u73af\u5883\u5f00\u9500\u3002", "result": "\u7f51\u7edc\u73af\u5883\u5ef6\u8fdf\u53ef\u5360\u6574\u4f53\u5ef6\u8fdf\u768453.7%\uff0cSpecCache\u76f8\u6bd4\u968f\u673a\u7f13\u5b58\u7b56\u7565\u63d0\u5347\u7f13\u5b58\u547d\u4e2d\u738758\u500d\uff0c\u51cf\u5c11\u7f51\u7edc\u73af\u5883\u5f00\u95003.2\u500d\uff0c\u4e14\u4e0d\u964d\u4f4e\u7cfb\u7edf\u6027\u80fd\u3002", "conclusion": "\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6548\u7387\u4f18\u5316\u81f3\u5173\u91cd\u8981\uff0cSpecCache\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u7f51\u7edc\u73af\u5883\u5ef6\u8fdf\u95ee\u9898\uff0c\u4e3a\u6784\u5efa\u9ad8\u6548\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16380", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16380", "abs": "https://arxiv.org/abs/2510.16380", "authors": ["Yu Ying Chiu", "Michael S. Lee", "Rachel Calcott", "Brandon Handoko", "Paul de Font-Reaulx", "Paula Rodriguez", "Chen Bo Calvin Zhang", "Ziwen Han", "Udari Madhushani Sehwag", "Yash Maurya", "Christina Q Knight", "Harry R. Lloyd", "Florence Bacus", "Mantas Mazeika", "Bing Liu", "Yejin Choi", "Mitchell L Gordon", "Sydney Levine"], "title": "MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in Language Models, More than Outcomes", "comment": "46 pages, 8 figures, 10 tables. Preprint", "summary": "As AI systems progress, we rely more on them to make decisions with us and\nfor us. To ensure that such decisions are aligned with human values, it is\nimperative for us to understand not only what decisions they make but also how\nthey come to those decisions. Reasoning language models, which provide both\nfinal responses and (partially transparent) intermediate thinking traces,\npresent a timely opportunity to study AI procedural reasoning. Unlike math and\ncode problems which often have objectively correct answers, moral dilemmas are\nan excellent testbed for process-focused evaluation because they allow for\nmultiple defensible conclusions. To do so, we present MoReBench: 1,000 moral\nscenarios, each paired with a set of rubric criteria that experts consider\nessential to include (or avoid) when reasoning about the scenarios. MoReBench\ncontains over 23 thousand criteria including identifying moral considerations,\nweighing trade-offs, and giving actionable recommendations to cover cases on AI\nadvising humans moral decisions as well as making moral decisions autonomously.\nSeparately, we curate MoReBench-Theory: 150 examples to test whether AI can\nreason under five major frameworks in normative ethics. Our results show that\nscaling laws and existing benchmarks on math, code, and scientific reasoning\ntasks fail to predict models' abilities to perform moral reasoning. Models also\nshow partiality towards specific moral frameworks (e.g., Benthamite Act\nUtilitarianism and Kantian Deontology), which might be side effects of popular\ntraining paradigms. Together, these benchmarks advance process-focused\nreasoning evaluation towards safer and more transparent AI.", "AI": {"tldr": "MoReBench\u662f\u4e00\u4e2a\u5305\u542b1000\u4e2a\u9053\u5fb7\u573a\u666f\u548c23000\u591a\u4e2a\u8bc4\u5206\u6807\u51c6\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u7684\u9053\u5fb7\u63a8\u7406\u8fc7\u7a0b\uff0c\u91cd\u70b9\u5173\u6ce8\u63a8\u7406\u8fc7\u7a0b\u800c\u975e\u6700\u7ec8\u7b54\u6848\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u5728\u51b3\u7b56\u4e2d\u626e\u6f14\u8d8a\u6765\u8d8a\u91cd\u8981\u7684\u89d2\u8272\uff0c\u9700\u8981\u7406\u89e3\u5b83\u4eec\u5982\u4f55\u505a\u51fa\u51b3\u7b56\uff0c\u7279\u522b\u662f\u9053\u5fb7\u51b3\u7b56\u3002\u9053\u5fb7\u56f0\u5883\u662f\u8bc4\u4f30\u63a8\u7406\u8fc7\u7a0b\u7684\u7406\u60f3\u6d4b\u8bd5\u5e73\u53f0\uff0c\u56e0\u4e3a\u5b83\u4eec\u5141\u8bb8\u591a\u79cd\u5408\u7406\u7684\u7ed3\u8bba\u3002", "method": "\u521b\u5efaMoReBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u9053\u5fb7\u573a\u666f\u548c\u4e13\u5bb6\u5236\u5b9a\u7684\u8bc4\u5206\u6807\u51c6\uff0c\u6db5\u76d6\u8bc6\u522b\u9053\u5fb7\u8003\u91cf\u3001\u6743\u8861\u5229\u5f0a\u3001\u63d0\u4f9b\u53ef\u884c\u5efa\u8bae\u7b49\u65b9\u9762\u3002\u540c\u65f6\u521b\u5efaMoReBench-Theory\u6d4b\u8bd5AI\u5728\u4e94\u79cd\u89c4\u8303\u4f26\u7406\u5b66\u6846\u67b6\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6269\u5c55\u5b9a\u5f8b\u548c\u73b0\u6709\u7684\u6570\u5b66\u3001\u4ee3\u7801\u3001\u79d1\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u9884\u6d4b\u6a21\u578b\u8fdb\u884c\u9053\u5fb7\u63a8\u7406\u7684\u80fd\u529b\u3002\u6a21\u578b\u5bf9\u7279\u5b9a\u9053\u5fb7\u6846\u67b6\uff08\u5982\u8fb9\u6c81\u529f\u5229\u4e3b\u4e49\u548c\u5eb7\u5fb7\u4e49\u52a1\u8bba\uff09\u8868\u73b0\u51fa\u504f\u597d\uff0c\u8fd9\u53ef\u80fd\u662f\u6d41\u884c\u8bad\u7ec3\u8303\u5f0f\u7684\u526f\u4f5c\u7528\u3002", "conclusion": "\u8fd9\u4e9b\u57fa\u51c6\u6d4b\u8bd5\u63a8\u52a8\u4e86\u4ee5\u8fc7\u7a0b\u4e3a\u91cd\u70b9\u7684\u63a8\u7406\u8bc4\u4f30\uff0c\u671d\u7740\u66f4\u5b89\u5168\u3001\u66f4\u900f\u660e\u7684AI\u53d1\u5c55\u3002"}}
{"id": "2510.16302", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.16302", "abs": "https://arxiv.org/abs/2510.16302", "authors": ["Changhao Wang", "Yanfang Liu", "Xinxin Fan", "Anzhi Zhou", "Lao Tian", "Yunfeng Lu"], "title": "DTKG: Dual-Track Knowledge Graph-Verified Reasoning Framework for Multi-Hop QA", "comment": "13 pages, 5 figures", "summary": "Multi-hop reasoning for question answering (QA) plays a critical role in\nretrieval-augmented generation (RAG) for modern large language models (LLMs).\nThe accurate answer can be obtained through retrieving relational structure of\nentities from knowledge graph (KG). Regarding the inherent relation-dependency\nand reasoning pattern, multi-hop reasoning can be in general classified into\ntwo categories: i) parallel fact-verification multi-hop reasoning question,\ni.e., requiring simultaneous verifications of multiple independent\nsub-questions; and ii) chained multi-hop reasoning questions, i.e., demanding\nsequential multi-step inference with intermediate conclusions serving as\nessential premises for subsequent reasoning. Currently, the multi-hop reasoning\napproaches singly employ one of two techniques: LLM response-based fact\nverification and KG path-based chain construction. Nevertheless, the former\nexcels at parallel fact-verification but underperforms on chained reasoning\ntasks, while the latter demonstrates proficiency in chained multi-hop reasoning\nbut suffers from redundant path retrieval when handling parallel\nfact-verification reasoning. These limitations deteriorate the efficiency and\naccuracy for multi-hop QA tasks. To address this challenge, we propose a novel\ndual-track KG verification and reasoning framework DTKG, which is inspired by\nthe Dual Process Theory in cognitive science. Specifically, DTKG comprises two\nmain stages: the Classification Stage and the Branch Processing Stage.", "AI": {"tldr": "\u63d0\u51fa\u4e86DTKG\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u8f68\u77e5\u8bc6\u56fe\u8c31\u9a8c\u8bc1\u548c\u63a8\u7406\u6765\u89e3\u51b3\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u5e76\u884c\u4e8b\u5b9e\u9a8c\u8bc1\u548c\u94fe\u5f0f\u63a8\u7406\u95ee\u9898", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u591a\u8df3\u63a8\u7406\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff1a\u57fa\u4e8eLLM\u54cd\u5e94\u7684\u4e8b\u5b9e\u9a8c\u8bc1\u64c5\u957f\u5e76\u884c\u9a8c\u8bc1\u4f46\u4e0d\u64c5\u957f\u94fe\u5f0f\u63a8\u7406\uff0c\u800c\u57fa\u4e8eKG\u8def\u5f84\u7684\u65b9\u6cd5\u64c5\u957f\u94fe\u5f0f\u63a8\u7406\u4f46\u5728\u5e76\u884c\u9a8c\u8bc1\u65f6\u5b58\u5728\u5197\u4f59\u8def\u5f84\u68c0\u7d22\u95ee\u9898", "method": "DTKG\u6846\u67b6\u5305\u542b\u5206\u7c7b\u9636\u6bb5\u548c\u5206\u652f\u5904\u7406\u9636\u6bb5\uff0c\u53d7\u8ba4\u77e5\u79d1\u5b66\u4e2d\u7684\u53cc\u8fc7\u7a0b\u7406\u8bba\u542f\u53d1\uff0c\u91c7\u7528\u53cc\u8f68\u77e5\u8bc6\u56fe\u8c31\u9a8c\u8bc1\u548c\u63a8\u7406\u65b9\u6cd5", "result": "\u672a\u5728\u6458\u8981\u4e2d\u660e\u786e\u8bf4\u660e", "conclusion": "DTKG\u6846\u67b6\u65e8\u5728\u63d0\u9ad8\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u7684\u6548\u7387\u548c\u51c6\u786e\u6027"}}
{"id": "2510.16381", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16381", "abs": "https://arxiv.org/abs/2510.16381", "authors": ["David Peer", "Sebastian Stabinger"], "title": "ATA: A Neuro-Symbolic Approach to Implement Autonomous and Trustworthy Agents", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities, yet\ntheir deployment in high-stakes domains is hindered by inherent limitations in\ntrustworthiness, including hallucinations, instability, and a lack of\ntransparency. To address these challenges, we introduce a generic\nneuro-symbolic approach, which we call Autonomous Trustworthy Agents (ATA). The\ncore of our approach lies in decoupling tasks into two distinct phases: Offline\nknowledge ingestion and online task processing. During knowledge ingestion, an\nLLM translates an informal problem specification into a formal, symbolic\nknowledge base. This formal representation is crucial as it can be verified and\nrefined by human experts, ensuring its correctness and alignment with domain\nrequirements. In the subsequent task processing phase, each incoming input is\nencoded into the same formal language. A symbolic decision engine then utilizes\nthis encoded input in conjunction with the formal knowledge base to derive a\nreliable result. Through an extensive evaluation on a complex reasoning task,\nwe demonstrate that a concrete implementation of ATA is competitive with\nstate-of-the-art end-to-end reasoning models in a fully automated setup while\nmaintaining trustworthiness. Crucially, with a human-verified and corrected\nknowledge base, our approach significantly outperforms even larger models,\nwhile exhibiting perfect determinism, enhanced stability against input\nperturbations, and inherent immunity to prompt injection attacks. By generating\ndecisions grounded in symbolic reasoning, ATA offers a practical and\ncontrollable architecture for building the next generation of transparent,\nauditable, and reliable autonomous agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u81ea\u4e3b\u53ef\u4fe1\u4ee3\u7406(ATA)\u7684\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u79bb\u7ebf\u77e5\u8bc6\u6444\u53d6\u548c\u5728\u7ebf\u4efb\u52a1\u5904\u7406\u4e24\u4e2a\u9636\u6bb5\uff0c\u89e3\u51b3LLM\u5728\u53ef\u4fe1\u5ea6\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u53ef\u4fe1\u5ea6\u65b9\u9762\u5b58\u5728\u5e7b\u89c9\u3001\u4e0d\u7a33\u5b9a\u6027\u548c\u7f3a\u4e4f\u900f\u660e\u5ea6\u7b49\u5c40\u9650\u6027\uff0c\u963b\u788d\u4e86\u5176\u5728\u9ad8\u98ce\u9669\u9886\u57df\u7684\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u79bb\u7ebf\u77e5\u8bc6\u6444\u53d6\u9636\u6bb5\u5c06\u975e\u6b63\u5f0f\u95ee\u9898\u89c4\u8303\u8f6c\u6362\u4e3a\u5f62\u5f0f\u5316\u77e5\u8bc6\u5e93\uff1b\u5728\u7ebf\u4efb\u52a1\u5904\u7406\u9636\u6bb5\u4f7f\u7528\u7b26\u53f7\u51b3\u7b56\u5f15\u64ce\u57fa\u4e8e\u5f62\u5f0f\u5316\u77e5\u8bc6\u5e93\u548c\u7f16\u7801\u8f93\u5165\u5f97\u51fa\u53ef\u9760\u7ed3\u679c\u3002", "result": "\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cATA\u4e0e\u6700\u5148\u8fdb\u7684\u7aef\u5230\u7aef\u63a8\u7406\u6a21\u578b\u5177\u6709\u7ade\u4e89\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u4fe1\u5ea6\u3002\u4f7f\u7528\u4eba\u5de5\u9a8c\u8bc1\u7684\u77e5\u8bc6\u5e93\u65f6\uff0cATA\u663e\u8457\u4f18\u4e8e\u66f4\u5927\u7684\u6a21\u578b\uff0c\u5e76\u8868\u73b0\u51fa\u5b8c\u7f8e\u7684\u786e\u5b9a\u6027\u3001\u589e\u5f3a\u7684\u7a33\u5b9a\u6027\u4ee5\u53ca\u5bf9\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u7684\u56fa\u6709\u514d\u75ab\u529b\u3002", "conclusion": "ATA\u901a\u8fc7\u57fa\u4e8e\u7b26\u53f7\u63a8\u7406\u7684\u51b3\u7b56\u751f\u6210\uff0c\u4e3a\u6784\u5efa\u4e0b\u4e00\u4ee3\u900f\u660e\u3001\u53ef\u5ba1\u8ba1\u548c\u53ef\u9760\u7684\u81ea\u4e3b\u4f53\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u53ef\u63a7\u7684\u67b6\u6784\u3002"}}
{"id": "2510.16309", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16309", "abs": "https://arxiv.org/abs/2510.16309", "authors": ["Crystal Su"], "title": "MedRule-KG: A Knowledge-Graph--Steered Scaffold for Mathematical Reasoning with a Lightweight Verifier", "comment": "Accepted to the Annual Conference on Neural Information Processing\n  Systems (NeurIPS 2026) Workshop", "summary": "Large language models (LLMs) often produce fluent reasoning steps while\nviolating simple mathematical or logical constraints. We introduce MedRule-KG,\na compact typed knowledge graph coupled with a symbolic verifier, designed to\nenforce mathematically interpretable rules in reasoning tasks. MedRule-KG\nencodes entities, relations, and three domain-inspired rules, while the\nverifier checks predictions and applies minimal corrections to guarantee\nconsistency. On a 90-example FDA-derived benchmark, grounding in MedRule-KG\nimproves exact match (EM) from 0.767 to 0.900, and adding the verifier yields\n1.000 EM while eliminating rule violations entirely. We demonstrate how\nMedRule-KG provides a general scaffold for safe mathematical reasoning, discuss\nablations, and release code and data to encourage reproducibility.", "AI": {"tldr": "MedRule-KG\u662f\u4e00\u4e2a\u7d27\u51d1\u7684\u77e5\u8bc6\u56fe\u8c31\u548c\u7b26\u53f7\u9a8c\u8bc1\u5668\uff0c\u7528\u4e8e\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u5f3a\u5236\u6267\u884c\u6570\u5b66\u53ef\u89e3\u91ca\u89c4\u5219\uff0c\u663e\u8457\u63d0\u9ad8\u51c6\u786e\u7387\u5e76\u6d88\u9664\u89c4\u5219\u8fdd\u53cd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ecf\u5e38\u4ea7\u751f\u6d41\u7545\u7684\u63a8\u7406\u6b65\u9aa4\uff0c\u4f46\u8fdd\u53cd\u7b80\u5355\u7684\u6570\u5b66\u6216\u903b\u8f91\u7ea6\u675f\uff0c\u9700\u8981\u65b9\u6cd5\u6765\u786e\u4fdd\u63a8\u7406\u7684\u6570\u5b66\u4e00\u81f4\u6027\u3002", "method": "\u5f15\u5165MedRule-KG\uff0c\u4e00\u4e2a\u7d27\u51d1\u7684\u7c7b\u578b\u5316\u77e5\u8bc6\u56fe\u8c31\uff0c\u7ed3\u5408\u7b26\u53f7\u9a8c\u8bc1\u5668\uff0c\u7f16\u7801\u5b9e\u4f53\u3001\u5173\u7cfb\u548c\u4e09\u4e2a\u9886\u57df\u542f\u53d1\u89c4\u5219\uff0c\u9a8c\u8bc1\u9884\u6d4b\u5e76\u5e94\u7528\u6700\u5c0f\u4fee\u6b63\u4fdd\u8bc1\u4e00\u81f4\u6027\u3002", "result": "\u572890\u4e2aFDA\u884d\u751f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u57fa\u4e8eMedRule-KG\u7684\u63a5\u5730\u5c06\u7cbe\u786e\u5339\u914d\u4ece0.767\u63d0\u9ad8\u52300.900\uff0c\u6dfb\u52a0\u9a8c\u8bc1\u5668\u540e\u8fbe\u52301.000\u7cbe\u786e\u5339\u914d\uff0c\u5b8c\u5168\u6d88\u9664\u89c4\u5219\u8fdd\u53cd\u3002", "conclusion": "MedRule-KG\u4e3a\u5b89\u5168\u6570\u5b66\u63a8\u7406\u63d0\u4f9b\u4e86\u901a\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u548c\u7b26\u53f7\u9a8c\u8bc1\u786e\u4fdd\u63a8\u7406\u7684\u6570\u5b66\u4e00\u81f4\u6027\u3002"}}
{"id": "2510.16387", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.16387", "abs": "https://arxiv.org/abs/2510.16387", "authors": ["Fu-An Chao", "Bi-Cheng Yan", "Berlin Chen"], "title": "Probing the Hidden Talent of ASR Foundation Models for L2 English Oral Assessment", "comment": null, "summary": "In this paper, we explore the untapped potential of Whisper, a\nwell-established automatic speech recognition (ASR) foundation model, in the\ncontext of L2 spoken language assessment (SLA). Unlike prior studies that\nextrinsically analyze transcriptions produced by Whisper, our approach goes a\nstep further to probe its latent capabilities by extracting acoustic and\nlinguistic features from hidden representations. With only a lightweight\nclassifier being trained on top of Whisper's intermediate and final outputs,\nour method achieves strong performance on the GEPT picture-description dataset,\noutperforming existing cutting-edge baselines, including a multimodal approach.\nFurthermore, by incorporating image and text-prompt information as auxiliary\nrelevance cues, we demonstrate additional performance gains. Finally, we\nconduct an in-depth analysis of Whisper's embeddings, which reveals that, even\nwithout task-specific fine-tuning, the model intrinsically encodes both ordinal\nproficiency patterns and semantic aspects of speech, highlighting its potential\nas a powerful foundation for SLA and other spoken language understanding tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86Whisper\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u5728\u7b2c\u4e8c\u8bed\u8a00\u53e3\u8bed\u8bc4\u4f30\u4e2d\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u63d0\u53d6\u5176\u9690\u85cf\u8868\u793a\u4e2d\u7684\u58f0\u5b66\u548c\u8bed\u8a00\u7279\u5f81\uff0c\u4ec5\u9700\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u5373\u53ef\u5728GEPT\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u6316\u6398Whisper\u6a21\u578b\u5728\u7b2c\u4e8c\u8bed\u8a00\u53e3\u8bed\u8bc4\u4f30\u4e2d\u7684\u6f5c\u5728\u80fd\u529b\uff0c\u8d85\u8d8a\u4ee5\u5f80\u4ec5\u4f7f\u7528\u5176\u8f6c\u5f55\u6587\u672c\u7684\u7814\u7a76\u65b9\u6cd5\u3002", "method": "\u4eceWhisper\u7684\u9690\u85cf\u8868\u793a\u4e2d\u63d0\u53d6\u58f0\u5b66\u548c\u8bed\u8a00\u7279\u5f81\uff0c\u4ec5\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\uff0c\u5e76\u6574\u5408\u56fe\u50cf\u548c\u6587\u672c\u63d0\u793a\u4f5c\u4e3a\u8f85\u52a9\u76f8\u5173\u6027\u7ebf\u7d22\u3002", "result": "\u5728GEPT\u56fe\u7247\u63cf\u8ff0\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8a\u4e86\u5305\u62ec\u591a\u6a21\u6001\u65b9\u6cd5\u5728\u5185\u7684\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u8f85\u52a9\u4fe1\u606f\u83b7\u5f97\u989d\u5916\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u5373\u4f7f\u6ca1\u6709\u4efb\u52a1\u7279\u5b9a\u7684\u5fae\u8c03\uff0cWhisper\u6a21\u578b\u4e5f\u80fd\u5185\u5728\u7f16\u7801\u53e3\u8bed\u7684\u5e8f\u6570\u719f\u7ec3\u5ea6\u6a21\u5f0f\u548c\u8bed\u4e49\u65b9\u9762\uff0c\u663e\u793a\u51fa\u5176\u4f5c\u4e3a\u53e3\u8bed\u8bc4\u4f30\u548c\u53e3\u8bed\u7406\u89e3\u4efb\u52a1\u7684\u5f3a\u5927\u57fa\u7840\u6a21\u578b\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.16342", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16342", "abs": "https://arxiv.org/abs/2510.16342", "authors": ["Tong Zhang", "Ru Zhang", "Jianyi Liu", "Zhen Yang", "Gongshen Liu"], "title": "Beyond Fixed Anchors: Precisely Erasing Concepts with Sibling Exclusive Counterparts", "comment": null, "summary": "Existing concept erasure methods for text-to-image diffusion models commonly\nrely on fixed anchor strategies, which often lead to critical issues such as\nconcept re-emergence and erosion. To address this, we conduct causal tracing to\nreveal the inherent sensitivity of erasure to anchor selection and define\nSibling Exclusive Concepts as a superior class of anchors. Based on this\ninsight, we propose \\textbf{SELECT} (Sibling-Exclusive Evaluation for\nContextual Targeting), a dynamic anchor selection framework designed to\novercome the limitations of fixed anchors. Our framework introduces a novel\ntwo-stage evaluation mechanism that automatically discovers optimal anchors for\nprecise erasure while identifying critical boundary anchors to preserve related\nconcepts. Extensive evaluations demonstrate that SELECT, as a universal anchor\nsolution, not only efficiently adapts to multiple erasure frameworks but also\nconsistently outperforms existing baselines across key performance metrics,\naveraging only 4 seconds for anchor mining of a single concept.", "AI": {"tldr": "\u63d0\u51fa\u4e86SELECT\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u951a\u70b9\u9009\u62e9\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u6982\u5ff5\u64e6\u9664\u7684\u951a\u70b9\u654f\u611f\u6027\u95ee\u9898\uff0c\u907f\u514d\u6982\u5ff5\u91cd\u73b0\u548c\u4fb5\u8680\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u951a\u70b9\u7b56\u7565\uff0c\u5bfc\u81f4\u6982\u5ff5\u91cd\u73b0\u548c\u4fb5\u8680\u7b49\u5173\u952e\u95ee\u9898\uff0c\u9700\u8981\u89e3\u51b3\u951a\u70b9\u9009\u62e9\u7684\u654f\u611f\u6027\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u56e0\u679c\u8ffd\u8e2a\u5206\u6790\uff0c\u5b9a\u4e49\u5144\u5f1f\u6392\u4ed6\u6982\u5ff5\u4f5c\u4e3a\u66f4\u4f18\u951a\u70b9\u7c7b\u522b\uff0c\u63d0\u51fa\u4e24\u9636\u6bb5\u8bc4\u4f30\u673a\u5236\u81ea\u52a8\u53d1\u73b0\u6700\u4f18\u64e6\u9664\u951a\u70b9\u548c\u5173\u952e\u8fb9\u754c\u951a\u70b9\u3002", "result": "SELECT\u4f5c\u4e3a\u901a\u7528\u951a\u70b9\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u9ad8\u6548\u9002\u914d\u591a\u79cd\u64e6\u9664\u6846\u67b6\uff0c\u5728\u5173\u952e\u6027\u80fd\u6307\u6807\u4e0a\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5355\u4e2a\u6982\u5ff5\u951a\u70b9\u6316\u6398\u4ec5\u97004\u79d2\u3002", "conclusion": "\u52a8\u6001\u951a\u70b9\u9009\u62e9\u6846\u67b6SELECT\u6709\u6548\u89e3\u51b3\u4e86\u56fa\u5b9a\u951a\u70b9\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u6982\u5ff5\u64e6\u9664\u548c\u5173\u8054\u6982\u5ff5\u4fdd\u62a4\u3002"}}
{"id": "2510.16439", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16439", "abs": "https://arxiv.org/abs/2510.16439", "authors": ["Syed Rifat Raiyan", "Md Farhan Ishmam", "Abdullah Al Imran", "Mohammad Ali Moni"], "title": "FrugalPrompt: Reducing Contextual Overhead in Large Language Models via Token Attribution", "comment": null, "summary": "Large language models (LLMs) owe much of their stellar performance to\nexpansive input contexts, yet such verbosity inflates monetary costs, carbon\nfootprint, and inference-time latency. Much of this overhead manifests from the\nredundant low-utility tokens present in typical prompts, as only a fraction of\ntokens typically carries the majority of the semantic weight. We address this\ninefficiency by introducing FrugalPrompt, a novel prompt compression framework\nfor LLMs, which retains only the most semantically significant tokens.\nLeveraging two state-of-the-art token attribution methods, GlobEnc and DecompX,\nwe assign salience scores to every token in an input sequence, rank them to\npreserve the top-k% tokens in their original order, and obtain a sparse\nfrugalized prompt. We evaluate the approach across four NLP tasks: Sentiment\nAnalysis, Commonsense QA, Summarization, and Mathematical Reasoning, using a\nsuite of frontier LLMs. For the first three tasks, a 20% prompt reduction\nincurs only a marginal loss in task performance, demonstrating that\ncontemporary LLMs can reconstruct elided context from high-salience cues. In\ncontrast, performance on mathematical reasoning deteriorates sharply,\nreflecting a stronger dependence on complete token continuity. Further analysis\nwith bottom-k% and random-k% tokens reveals asymmetric performance patterns\nthat may suggest potential task contamination effects, wherein models may\nresort to shallow memorized patterns from pretraining exposure for conventional\nNLP tasks. We posit that our work contributes to a more nuanced understanding\nof LLM behavior in performance-efficiency trade-offs, and delineate the\nboundary between tasks tolerant to contextual sparsity and those requiring\nexhaustive context. Our source code and models are available at:\nhttps://github.com/Starscream-11813/Frugal-ICL", "AI": {"tldr": "FrugalPrompt\u662f\u4e00\u4e2a\u65b0\u9896\u7684LLM\u63d0\u793a\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u4fdd\u7559\u6700\u5177\u8bed\u4e49\u91cd\u8981\u6027\u7684token\u6765\u51cf\u5c11\u8f93\u5165\u957f\u5ea6\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u6210\u672c\u548c\u5ef6\u8fdf\u3002", "motivation": "LLM\u7684\u5e9e\u5927\u8f93\u5165\u4e0a\u4e0b\u6587\u5e26\u6765\u4e86\u9ad8\u6602\u7684\u8d27\u5e01\u6210\u672c\u3001\u78b3\u8db3\u8ff9\u548c\u63a8\u7406\u5ef6\u8fdf\uff0c\u800c\u5178\u578b\u63d0\u793a\u4e2d\u5b58\u5728\u5927\u91cf\u5197\u4f59\u7684\u4f4e\u6548\u7528token\uff0c\u53ea\u6709\u5c11\u6570token\u627f\u8f7d\u4e3b\u8981\u8bed\u4e49\u6743\u91cd\u3002", "method": "\u4f7f\u7528GlobEnc\u548cDecompX\u4e24\u79cdtoken\u5f52\u56e0\u65b9\u6cd5\u4e3a\u8f93\u5165\u5e8f\u5217\u4e2d\u7684\u6bcf\u4e2atoken\u5206\u914d\u663e\u8457\u6027\u5206\u6570\uff0c\u6309\u539f\u59cb\u987a\u5e8f\u4fdd\u7559\u524dk%\u7684token\uff0c\u83b7\u5f97\u7a00\u758f\u7684\u538b\u7f29\u63d0\u793a\u3002", "result": "\u5728\u60c5\u611f\u5206\u6790\u3001\u5e38\u8bc6\u95ee\u7b54\u548c\u6458\u8981\u4efb\u52a1\u4e2d\uff0c20%\u7684\u63d0\u793a\u538b\u7f29\u4ec5\u5bfc\u81f4\u4efb\u52a1\u6027\u80fd\u7684\u8fb9\u9645\u635f\u5931\uff0c\u800c\u6570\u5b66\u63a8\u7406\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u53cd\u6620\u4e86\u5bf9\u5b8c\u6574token\u8fde\u7eed\u6027\u7684\u66f4\u5f3a\u4f9d\u8d56\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u6709\u52a9\u4e8e\u66f4\u7ec6\u81f4\u5730\u7406\u89e3LLM\u5728\u6027\u80fd-\u6548\u7387\u6743\u8861\u4e2d\u7684\u884c\u4e3a\uff0c\u5e76\u754c\u5b9a\u4e86\u5bb9\u5fcd\u4e0a\u4e0b\u6587\u7a00\u758f\u6027\u7684\u4efb\u52a1\u4e0e\u9700\u8981\u8be6\u5c3d\u4e0a\u4e0b\u6587\u7684\u4efb\u52a1\u4e4b\u95f4\u7684\u8fb9\u754c\u3002"}}
{"id": "2510.16368", "categories": ["cs.AI", "cs.HC", "cs.LG", "econ.TH"], "pdf": "https://arxiv.org/pdf/2510.16368", "abs": "https://arxiv.org/abs/2510.16368", "authors": ["Ali Shirali"], "title": "The Burden of Interactive Alignment with Inconsistent Preferences", "comment": "Published as a conference paper at NeurIPS 2025", "summary": "From media platforms to chatbots, algorithms shape how people interact,\nlearn, and discover information. Such interactions between users and an\nalgorithm often unfold over multiple steps, during which strategic users can\nguide the algorithm to better align with their true interests by selectively\nengaging with content. However, users frequently exhibit inconsistent\npreferences: they may spend considerable time on content that offers little\nlong-term value, inadvertently signaling that such content is desirable.\nFocusing on the user side, this raises a key question: what does it take for\nsuch users to align the algorithm with their true interests?\n  To investigate these dynamics, we model the user's decision process as split\nbetween a rational system 2 that decides whether to engage and an impulsive\nsystem 1 that determines how long engagement lasts. We then study a\nmulti-leader, single-follower extensive Stackelberg game, where users,\nspecifically system 2, lead by committing to engagement strategies and the\nalgorithm best-responds based on observed interactions. We define the burden of\nalignment as the minimum horizon over which users must optimize to effectively\nsteer the algorithm. We show that a critical horizon exists: users who are\nsufficiently foresighted can achieve alignment, while those who are not are\ninstead aligned to the algorithm's objective. This critical horizon can be\nlong, imposing a substantial burden. However, even a small, costly signal\n(e.g., an extra click) can significantly reduce it. Overall, our framework\nexplains how users with inconsistent preferences can align an engagement-driven\nalgorithm with their interests in a Stackelberg equilibrium, highlighting both\nthe challenges and potential remedies for achieving alignment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u7528\u6237\u5982\u4f55\u901a\u8fc7\u6218\u7565\u6027\u5730\u4e0e\u7b97\u6cd5\u4e92\u52a8\u6765\u4f7f\u5176\u4e0e\u81ea\u8eab\u771f\u5b9e\u5174\u8da3\u5bf9\u9f50\uff0c\u63d0\u51fa\u4e86\"\u5bf9\u9f50\u8d1f\u62c5\"\u6982\u5ff5\uff0c\u5e76\u53d1\u73b0\u5b58\u5728\u5173\u952e\u65f6\u95f4\u8303\u56f4\uff1a\u8db3\u591f\u6709\u8fdc\u89c1\u7684\u7528\u6237\u53ef\u4ee5\u5b9e\u73b0\u5bf9\u9f50\uff0c\u800c\u77ed\u89c6\u7528\u6237\u5219\u4f1a\u88ab\u7b97\u6cd5\u76ee\u6807\u540c\u5316\u3002", "motivation": "\u7814\u7a76\u7528\u6237\u5728\u7b97\u6cd5\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u6218\u7565\u4e92\u52a8\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5f53\u7528\u6237\u8868\u73b0\u51fa\u4e0d\u4e00\u81f4\u504f\u597d\u65f6\uff08\u5982\u82b1\u65f6\u95f4\u5728\u4f4e\u4ef7\u503c\u5185\u5bb9\u4e0a\uff09\uff0c\u5982\u4f55\u4f7f\u7b97\u6cd5\u4e0e\u7528\u6237\u771f\u5b9e\u5174\u8da3\u5bf9\u9f50\u7684\u95ee\u9898\u3002", "method": "\u5c06\u7528\u6237\u51b3\u7b56\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u7406\u6027\u7cfb\u7edf2\uff08\u51b3\u5b9a\u662f\u5426\u53c2\u4e0e\uff09\u548c\u51b2\u52a8\u7cfb\u7edf1\uff08\u51b3\u5b9a\u53c2\u4e0e\u65f6\u957f\uff09\uff0c\u91c7\u7528\u591a\u9886\u5bfc\u8005-\u5355\u8ddf\u968f\u8005\u7684\u6269\u5c55Stackelberg\u535a\u5f08\u6846\u67b6\uff0c\u7528\u6237\u4f5c\u4e3a\u9886\u5bfc\u8005\u901a\u8fc7\u627f\u8bfa\u53c2\u4e0e\u7b56\u7565\u6765\u5f15\u5bfc\u7b97\u6cd5\u3002", "result": "\u53d1\u73b0\u5b58\u5728\u5173\u952e\u65f6\u95f4\u8303\u56f4\uff1a\u8db3\u591f\u6709\u8fdc\u89c1\u7684\u7528\u6237\u53ef\u4ee5\u5b9e\u73b0\u7b97\u6cd5\u5bf9\u9f50\uff0c\u800c\u77ed\u89c6\u7528\u6237\u4f1a\u88ab\u7b97\u6cd5\u76ee\u6807\u540c\u5316\u3002\u5373\u4f7f\u5c0f\u7684\u6210\u672c\u4fe1\u53f7\uff08\u5982\u989d\u5916\u70b9\u51fb\uff09\u4e5f\u80fd\u663e\u8457\u964d\u4f4e\u5bf9\u9f50\u8d1f\u62c5\u3002", "conclusion": "\u8be5\u6846\u67b6\u89e3\u91ca\u4e86\u5177\u6709\u4e0d\u4e00\u81f4\u504f\u597d\u7684\u7528\u6237\u5982\u4f55\u5728Stackelberg\u5747\u8861\u4e2d\u4f7f\u53c2\u4e0e\u9a71\u52a8\u578b\u7b97\u6cd5\u4e0e\u5176\u5174\u8da3\u5bf9\u9f50\uff0c\u63ed\u793a\u4e86\u5b9e\u73b0\u5bf9\u9f50\u7684\u6311\u6218\u548c\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16449", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16449", "abs": "https://arxiv.org/abs/2510.16449", "authors": ["Bin Yu", "Xinming Wang", "Shijie Lian", "Haotian Li", "Changti Wu", "Ruina Hu", "Bailing Wang", "Yuliang Wei", "Kai Chen"], "title": "TrajSelector: Harnessing Latent Representations for Efficient and Effective Best-of-N in Large Reasoning Model", "comment": "13 pages, 6 figures. Project website:\n  https://zgca-ai4edu.github.io/TrajSelector", "summary": "Large language models (LLMs) have shown remarkable progress in complex\nreasoning tasks, largely enabled by test-time scaling (TTS) paradigms that\nallocate additional compute during inference. Among these, external TTS\n(particularly the Best-of-N selection paradigm) yields scalable performance\nimprovements by selecting from multiple independently generated reasoning\ntrajectories. However, this approach faces key limitations: (i) the high\ncomputational overhead of deploying process reward models, (ii) the\nunderutilization of the LLM's intrinsic latent representations. We introduce\nTrajSelector, an efficient and effective Best-of-N framework that exploit the\nhidden states in the sampler LLM for process-level scoring. A lightweight\nverifier (with only 0.6B parameters) evaluates the quality of step-wise\ntrajectory, and then aggregates these scores to identify the optimal reasoning\ntrajectory. Our framework employs a fully data-driven, end-to-end training\nrecipe that eliminates reliance on massive step-level annotations. Experiential\nresults across five benchmarks demonstrate that TrajSelector delivers\nconsistent performance gains. In Best-of-32 settings, it surpasses majority\nvoting by 4.61% accuracy and outperforms existing process reward models by\n4.31% to 12.21%, all while maintaining lower inference costs.", "AI": {"tldr": "TrajSelector\u662f\u4e00\u4e2a\u9ad8\u6548\u7684Best-of-N\u6846\u67b6\uff0c\u5229\u7528LLM\u7684\u9690\u85cf\u72b6\u6001\u8fdb\u884c\u8fc7\u7a0b\u7ea7\u8bc4\u5206\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9a8c\u8bc1\u5668\u8bc4\u4f30\u63a8\u7406\u8f68\u8ff9\u8d28\u91cf\uff0c\u5728\u964d\u4f4e\u63a8\u7406\u6210\u672c\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5916\u90e8TTS\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u5927\u548c\u672a\u5145\u5206\u5229\u7528LLM\u5185\u5728\u8868\u793a\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8f68\u8ff9\u9009\u62e9\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u9a8c\u8bc1\u5668\uff08\u4ec50.6B\u53c2\u6570\uff09\u8bc4\u4f30\u6b65\u9aa4\u7ea7\u8f68\u8ff9\u8d28\u91cf\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u5229\u7528LLM\u9690\u85cf\u72b6\u6001\u8fdb\u884c\u8fc7\u7a0b\u8bc4\u5206\uff0c\u65e0\u9700\u5927\u91cf\u6b65\u9aa4\u7ea7\u6807\u6ce8\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTrajSelector\u5728Best-of-32\u8bbe\u7f6e\u4e0b\u6bd4\u591a\u6570\u6295\u7968\u51c6\u786e\u7387\u63d0\u53474.61%\uff0c\u6bd4\u73b0\u6709\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u63d0\u53474.31%-12.21%\uff0c\u4e14\u63a8\u7406\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "TrajSelector\u901a\u8fc7\u6709\u6548\u5229\u7528LLM\u5185\u90e8\u8868\u793a\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u5f02\u7684\u63a8\u7406\u8f68\u8ff9\u9009\u62e9\uff0c\u4e3aTTS\u8303\u5f0f\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16374", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16374", "abs": "https://arxiv.org/abs/2510.16374", "authors": ["Nick Oh"], "title": "Before you <think>, monitor: Implementing Flavell's metacognitive framework in LLMs", "comment": "Presented at the Workshop on the Application of LLM Explainability to\n  Reasoning and Planning at COLM 2025 (non-archival)", "summary": "Current approaches to enhancing LLM reasoning follows two isolated paradigms:\nMonitor-Generate methods like Plan-and-Solve (Wang et al., 2023) and\nSELF-DISCOVER (Zhou et al., 2024) excel at strategic planning but lack\nmechanisms to verify whether selected strategies succeed; while Generate-Verify\napproaches like Self-Verification (Weng et al., 2022) and SELF-REFINE (Madaan\net al., 2023) iteratively refine outputs but commence generation blindly\nwithout task assessment. This separation creates inefficiencies -- strategies\nfail without feedback, and refinement occurs without strategic grounding. We\naddress this gap by implementing Flavell's cognitive monitoring model (1979)\nfrom the broader Monitor-Generate-Verify framework (Oh and Gobet, 2025),\noperationalising it as a three-phase iterative system. On GSM8K, preliminary\nresults show 75.42% accuracy versus 68.44% for SELF-REFINE and 67.07% for\nSelf-Verification, while requiring fewer attempts (1.3 vs 2.0) at 27-37%\nincreased inference cost. These initial findings suggest upfront monitoring\nproduces higher-quality initial solutions that reduce refinement needs, though\nevaluation beyond arithmetic reasoning is needed to establish generalisability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u76d1\u63a7-\u751f\u6210-\u9a8c\u8bc1\u7684\u4e09\u9636\u6bb5\u8fed\u4ee3\u7cfb\u7edf\uff0c\u5728GSM8K\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e8675.42%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u9700\u8981\u66f4\u5c11\u7684\u5c1d\u8bd5\u6b21\u6570\u3002", "motivation": "\u73b0\u6709LLM\u63a8\u7406\u65b9\u6cd5\u5b58\u5728\u4e24\u79cd\u5b64\u7acb\u8303\u5f0f\uff1a\u76d1\u63a7-\u751f\u6210\u65b9\u6cd5\u64c5\u957f\u7b56\u7565\u89c4\u5212\u4f46\u7f3a\u4e4f\u9a8c\u8bc1\u673a\u5236\uff1b\u751f\u6210-\u9a8c\u8bc1\u65b9\u6cd5\u80fd\u8fed\u4ee3\u4f18\u5316\u4f46\u7f3a\u4e4f\u6218\u7565\u57fa\u7840\u3002\u8fd9\u79cd\u5206\u79bb\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u57fa\u4e8eFlavell\u7684\u8ba4\u77e5\u76d1\u63a7\u6a21\u578b\uff0c\u5b9e\u73b0\u76d1\u63a7-\u751f\u6210-\u9a8c\u8bc1\u4e09\u9636\u6bb5\u8fed\u4ee3\u7cfb\u7edf\uff0c\u5728\u751f\u6210\u524d\u8fdb\u884c\u4efb\u52a1\u8bc4\u4f30\uff0c\u5728\u751f\u6210\u540e\u8fdb\u884c\u7b56\u7565\u9a8c\u8bc1\u3002", "result": "\u5728GSM8K\u4e0a\u8fbe\u523075.42%\u51c6\u786e\u7387\uff0c\u4f18\u4e8eSELF-REFINE(68.44%)\u548cSelf-Verification(67.07%)\uff0c\u4e14\u5c1d\u8bd5\u6b21\u6570\u66f4\u5c11(1.3 vs 2.0)\uff0c\u63a8\u7406\u6210\u672c\u589e\u52a027-37%\u3002", "conclusion": "\u524d\u7f6e\u76d1\u63a7\u80fd\u4ea7\u751f\u66f4\u9ad8\u8d28\u91cf\u7684\u521d\u59cb\u89e3\u51b3\u65b9\u6848\uff0c\u51cf\u5c11\u4f18\u5316\u9700\u6c42\uff0c\u4f46\u9700\u8981\u5728\u7b97\u672f\u63a8\u7406\u4e4b\u5916\u7684\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bc4\u4f30\u4ee5\u9a8c\u8bc1\u901a\u7528\u6027\u3002"}}
{"id": "2510.16455", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16455", "abs": "https://arxiv.org/abs/2510.16455", "authors": ["Deyi Ji", "Yuekui Yang", "Haiyang Wu", "Shaoping Ma", "Tianrun Chen", "Lanyun Zhu"], "title": "RAVEN: Robust Advertisement Video Violation Temporal Grounding via Reinforcement Reasoning", "comment": "ACL 2025 (Oral, Industry Track)", "summary": "Advertisement (Ad) video violation detection is critical for ensuring\nplatform compliance, but existing methods struggle with precise temporal\ngrounding, noisy annotations, and limited generalization. We propose RAVEN, a\nnovel framework that integrates curriculum reinforcement learning with\nmultimodal large language models (MLLMs) to enhance reasoning and cognitive\ncapabilities for violation detection. RAVEN employs a progressive training\nstrategy, combining precisely and coarsely annotated data, and leverages Group\nRelative Policy Optimization (GRPO) to develop emergent reasoning abilities\nwithout explicit reasoning annotations. Multiple hierarchical sophisticated\nreward mechanism ensures precise temporal grounding and consistent category\nprediction. Experiments on industrial datasets and public benchmarks show that\nRAVEN achieves superior performances in violation category accuracy and\ntemporal interval localization. We also design a pipeline to deploy the RAVEN\non the online Ad services, and online A/B testing further validates its\npractical applicability, with significant improvements in precision and recall.\nRAVEN also demonstrates strong generalization, mitigating the catastrophic\nforgetting issue associated with supervised fine-tuning.", "AI": {"tldr": "RAVEN\u662f\u4e00\u4e2a\u96c6\u6210\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7f\u544a\u89c6\u9891\u8fdd\u89c4\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\u548cGRPO\u4f18\u5316\uff0c\u5728\u65e0\u9700\u663e\u5f0f\u63a8\u7406\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\u53d1\u5c55\u63a8\u7406\u80fd\u529b\uff0c\u5728\u5de5\u4e1a\u6570\u636e\u96c6\u548c\u516c\u5f00\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u5e7f\u544a\u89c6\u9891\u8fdd\u89c4\u68c0\u6d4b\u65b9\u6cd5\u5728\u7cbe\u786e\u65f6\u95f4\u5b9a\u4f4d\u3001\u566a\u58f0\u6807\u6ce8\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u5148\u8fdb\u7684\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\u6574\u5408\u7cbe\u786e\u548c\u7c97\u7565\u6807\u6ce8\u6570\u636e\uff0c\u4f7f\u7528GRPO\u53d1\u5c55\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u8bbe\u8ba1\u591a\u5c42\u6b21\u590d\u6742\u5956\u52b1\u673a\u5236\u3002", "result": "\u5728\u5de5\u4e1a\u6570\u636e\u96c6\u548c\u516c\u5f00\u57fa\u51c6\u4e0a\uff0cRAVEN\u5728\u8fdd\u89c4\u7c7b\u522b\u51c6\u786e\u6027\u548c\u65f6\u95f4\u533a\u95f4\u5b9a\u4f4d\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u7ebfA/B\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u663e\u8457\u63d0\u5347\u3002", "conclusion": "RAVEN\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5e7f\u544a\u89c6\u9891\u8fdd\u89c4\u68c0\u6d4b\u7684\u5173\u952e\u6311\u6218\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u6210\u529f\u90e8\u7f72\u5230\u5728\u7ebf\u5e7f\u544a\u670d\u52a1\u4e2d\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.16382", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16382", "abs": "https://arxiv.org/abs/2510.16382", "authors": ["Ze Tao", "Jian Zhang", "Haowei Li", "Xianshuai Li", "Yifei Peng", "Xiyao Liu", "Senzhang Wang", "Chao Liu", "Sheng Ren", "Shichao Zhang"], "title": "Humanoid-inspired Causal Representation Learning for Domain Generalization", "comment": null, "summary": "This paper proposes the Humanoid-inspired Structural Causal Model (HSCM), a\nnovel causal framework inspired by human intelligence, designed to overcome the\nlimitations of conventional domain generalization models. Unlike approaches\nthat rely on statistics to capture data-label dependencies and learn\ndistortion-invariant representations, HSCM replicates the hierarchical\nprocessing and multi-level learning of human vision systems, focusing on\nmodeling fine-grained causal mechanisms. By disentangling and reweighting key\nimage attributes such as color, texture, and shape, HSCM enhances\ngeneralization across diverse domains, ensuring robust performance and\ninterpretability. Leveraging the flexibility and adaptability of human\nintelligence, our approach enables more effective transfer and learning in\ndynamic, complex environments. Through both theoretical and empirical\nevaluations, we demonstrate that HSCM outperforms existing domain\ngeneralization models, providing a more principled method for capturing causal\nrelationships and improving model robustness. The code is available at\nhttps://github.com/lambett/HSCM.", "AI": {"tldr": "\u63d0\u51fa\u53d7\u4eba\u7c7b\u667a\u80fd\u542f\u53d1\u7684HSCM\u56e0\u679c\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u548c\u91cd\u52a0\u6743\u56fe\u50cf\u5c5e\u6027\uff08\u989c\u8272\u3001\u7eb9\u7406\u3001\u5f62\u72b6\uff09\u6765\u589e\u5f3a\u8de8\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u4f18\u4e8e\u73b0\u6709\u9886\u57df\u6cdb\u5316\u6a21\u578b\u3002", "motivation": "\u514b\u670d\u4f20\u7edf\u9886\u57df\u6cdb\u5316\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u8fd9\u4e9b\u6a21\u578b\u4f9d\u8d56\u7edf\u8ba1\u65b9\u6cd5\u6355\u6349\u6570\u636e-\u6807\u7b7e\u4f9d\u8d56\u5173\u7cfb\uff0c\u800cHSCM\u6a21\u4eff\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\u7684\u5206\u5c42\u5904\u7406\u548c\u591a\u5c42\u6b21\u5b66\u4e60\u673a\u5236\u3002", "method": "\u57fa\u4e8e\u4eba\u7c7b\u667a\u80fd\u7684\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\uff0cHSCM\u590d\u5236\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\u7684\u5206\u5c42\u5904\u7406\uff0c\u4e13\u6ce8\u4e8e\u5efa\u6a21\u7ec6\u7c92\u5ea6\u56e0\u679c\u673a\u5236\uff0c\u901a\u8fc7\u89e3\u8026\u548c\u91cd\u52a0\u6743\u5173\u952e\u56fe\u50cf\u5c5e\u6027\u6765\u5b9e\u73b0\u8de8\u57df\u6cdb\u5316\u3002", "result": "\u7406\u8bba\u548c\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0cHSCM\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u9886\u57df\u6cdb\u5316\u6a21\u578b\uff0c\u63d0\u4f9b\u4e86\u66f4\u539f\u5219\u6027\u7684\u65b9\u6cd5\u6765\u6355\u6349\u56e0\u679c\u5173\u7cfb\u5e76\u63d0\u9ad8\u6a21\u578b\u9c81\u68d2\u6027\u3002", "conclusion": "HSCM\u4e3a\u9886\u57df\u6cdb\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u6709\u6548\u548c\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u667a\u80fd\u7684\u5904\u7406\u673a\u5236\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u8de8\u57df\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2510.16458", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16458", "abs": "https://arxiv.org/abs/2510.16458", "authors": ["Pingjun Hong", "Beiduo Chen", "Siyao Peng", "Marie-Catherine de Marneffe", "Benjamin Roth", "Barbara Plank"], "title": "Agree, Disagree, Explain: Decomposing Human Label Variation in NLI through the Lens of Explanations", "comment": null, "summary": "Natural Language Inference datasets often exhibit human label variation. To\nbetter understand these variations, explanation-based approaches analyze the\nunderlying reasoning behind annotators' decisions. One such approach is the\nLiTEx taxonomy, which categorizes free-text explanations in English into\nreasoning types. However, previous work applying such taxonomies has focused on\nwithin-label variation: cases where annotators agree on the final NLI label but\nprovide different explanations. In contrast, this paper broadens the scope by\nexamining how annotators may diverge not only in the reasoning type but also in\nthe labeling step. We use explanations as a lens to decompose the reasoning\nprocess underlying NLI annotation and to analyze individual differences. We\napply LiTEx to two NLI English datasets and align annotation variation from\nmultiple aspects: NLI label agreement, explanation similarity, and taxonomy\nagreement, with an additional compounding factor of annotators' selection bias.\nWe observe instances where annotators disagree on the label but provide highly\nsimilar explanations, suggesting that surface-level disagreement may mask\nunderlying agreement in interpretation. Moreover, our analysis reveals\nindividual preferences in explanation strategies and label choices. These\nfindings highlight that agreement in reasoning types better reflects the\nsemantic similarity of free-text explanations than label agreement alone. Our\nfindings underscore the richness of reasoning-based explanations and the need\nfor caution in treating labels as ground truth.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7LiTEx\u5206\u7c7b\u6cd5\u5206\u6790\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4e2d\u7684\u6807\u6ce8\u8005\u5dee\u5f02\uff0c\u53d1\u73b0\u6807\u6ce8\u8005\u53ef\u80fd\u5728\u6807\u7b7e\u548c\u63a8\u7406\u7c7b\u578b\u4e0a\u90fd\u5b58\u5728\u5206\u6b67\uff0c\u4f46\u8868\u9762\u6807\u7b7e\u5206\u6b67\u53ef\u80fd\u63a9\u76d6\u4e86\u6df1\u5c42\u7684\u89e3\u91ca\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6807\u6ce8\u8005\u6807\u7b7e\u4e00\u81f4\u4f46\u89e3\u91ca\u4e0d\u540c\u7684\u60c5\u51b5\uff0c\u672c\u6587\u6269\u5c55\u7814\u7a76\u8303\u56f4\uff0c\u5206\u6790\u6807\u6ce8\u8005\u5728\u6807\u7b7e\u548c\u63a8\u7406\u7c7b\u578b\u4e0a\u7684\u53cc\u91cd\u5206\u6b67\uff0c\u4ee5\u66f4\u5168\u9762\u7406\u89e3NLI\u6807\u6ce8\u4e2d\u7684\u4e2a\u4f53\u5dee\u5f02\u3002", "method": "\u4f7f\u7528LiTEx\u5206\u7c7b\u6cd5\u5206\u6790\u4e24\u4e2a\u82f1\u6587NLI\u6570\u636e\u96c6\uff0c\u4eceNLI\u6807\u7b7e\u4e00\u81f4\u6027\u3001\u89e3\u91ca\u76f8\u4f3c\u6027\u548c\u5206\u7c7b\u6cd5\u4e00\u81f4\u6027\u4e09\u4e2a\u7ef4\u5ea6\u5bf9\u9f50\u6807\u6ce8\u5dee\u5f02\uff0c\u5e76\u8003\u8651\u6807\u6ce8\u8005\u9009\u62e9\u504f\u89c1\u7684\u590d\u5408\u56e0\u7d20\u3002", "result": "\u53d1\u73b0\u6807\u6ce8\u8005\u6807\u7b7e\u4e0d\u4e00\u81f4\u4f46\u89e3\u91ca\u9ad8\u5ea6\u76f8\u4f3c\u7684\u5b9e\u4f8b\uff0c\u8868\u660e\u8868\u9762\u5206\u6b67\u53ef\u80fd\u63a9\u76d6\u6df1\u5c42\u4e00\u81f4\u6027\uff1b\u5206\u6790\u8fd8\u63ed\u793a\u4e86\u6807\u6ce8\u8005\u5728\u89e3\u91ca\u7b56\u7565\u548c\u6807\u7b7e\u9009\u62e9\u4e0a\u7684\u4e2a\u4f53\u504f\u597d\u3002", "conclusion": "\u63a8\u7406\u7c7b\u578b\u7684\u4e00\u81f4\u6027\u6bd4\u5355\u7eaf\u6807\u7b7e\u4e00\u81f4\u6027\u66f4\u80fd\u53cd\u6620\u81ea\u7531\u6587\u672c\u89e3\u91ca\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u5f3a\u8c03\u9700\u8981\u8c28\u614e\u5c06\u6807\u7b7e\u89c6\u4e3a\u7edd\u5bf9\u771f\u503c\uff0c\u91cd\u89c6\u57fa\u4e8e\u63a8\u7406\u7684\u89e3\u91ca\u7684\u4e30\u5bcc\u6027\u3002"}}
{"id": "2510.16392", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16392", "abs": "https://arxiv.org/abs/2510.16392", "authors": ["Ao Tian", "Yunfeng Lu", "Xinxin Fan", "Changhao Wang", "Lanzhi Zhou", "Yeyao Zhang", "Yanfang Liu"], "title": "RGMem: Renormalization Group-based Memory Evolution for Language Agent User Profile", "comment": "11 pages,3 figures", "summary": "Personalized and continuous interactions are the key to enhancing user\nexperience in today's large language model (LLM)-based conversational systems,\nhowever, the finite context windows and static parametric memory make it\ndifficult to model the cross-session long-term user states and behavioral\nconsistency. Currently, the existing solutions to this predicament, such as\nretrieval-augmented generation (RAG) and explicit memory systems, primarily\nfocus on fact-level storage and retrieval, lacking the capability to distill\nlatent preferences and deep traits from the multi-turn dialogues, which limits\nthe long-term and effective user modeling, directly leading to the personalized\ninteractions remaining shallow, and hindering the cross-session continuity. To\nrealize the long-term memory and behavioral consistency for Language Agents in\nLLM era, we propose a self-evolving memory framework RGMem, inspired by the\nideology of classic renormalization group (RG) in physics, this framework\nenables to organize the dialogue history in multiple scales: it first extracts\nsemantics and user insights from episodic fragments, then through hierarchical\ncoarse-graining and rescaling operations, progressively forms a\ndynamically-evolved user profile. The core innovation of our work lies in\nmodeling memory evolution as a multi-scale process of information compression\nand emergence, which accomplishes the high-level and accurate user profiles\nfrom noisy and microscopic-level interactions.", "AI": {"tldr": "\u63d0\u51fa\u4e86RGMem\u6846\u67b6\uff0c\u57fa\u4e8e\u7269\u7406\u5b66\u4e2d\u7684\u91cd\u6574\u5316\u7fa4\u601d\u60f3\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u7ec4\u7ec7\u5bf9\u8bdd\u5386\u53f2\uff0c\u5b9e\u73b0\u8bed\u8a00\u4ee3\u7406\u7684\u957f\u671f\u8bb0\u5fc6\u548c\u884c\u4e3a\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709LLM\u5bf9\u8bdd\u7cfb\u7edf\u53d7\u9650\u4e8e\u6709\u9650\u4e0a\u4e0b\u6587\u7a97\u53e3\u548c\u9759\u6001\u53c2\u6570\u8bb0\u5fc6\uff0c\u96be\u4ee5\u5efa\u6a21\u8de8\u4f1a\u8bdd\u7684\u957f\u671f\u7528\u6237\u72b6\u6001\u548c\u884c\u4e3a\u4e00\u81f4\u6027\uff0c\u73b0\u6709RAG\u548c\u663e\u5f0f\u8bb0\u5fc6\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8\u4e8b\u5b9e\u7ea7\u5b58\u50a8\uff0c\u7f3a\u4e4f\u4ece\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u63d0\u53d6\u6f5c\u5728\u504f\u597d\u548c\u6df1\u5c42\u7279\u5f81\u7684\u80fd\u529b\u3002", "method": "\u91c7\u7528\u91cd\u6574\u5316\u7fa4\u601d\u60f3\u7684\u591a\u5c3a\u5ea6\u8bb0\u5fc6\u6846\u67b6\uff0c\u9996\u5148\u4ece\u7247\u6bb5\u4e2d\u63d0\u53d6\u8bed\u4e49\u548c\u7528\u6237\u6d1e\u5bdf\uff0c\u7136\u540e\u901a\u8fc7\u5206\u5c42\u7c97\u7c92\u5316\u548c\u91cd\u6807\u5ea6\u64cd\u4f5c\uff0c\u9010\u6b65\u5f62\u6210\u52a8\u6001\u6f14\u5316\u7684\u7528\u6237\u753b\u50cf\u3002", "result": "\u5b9e\u73b0\u4e86\u4ece\u5608\u6742\u7684\u5fae\u89c2\u4ea4\u4e92\u4e2d\u5b8c\u6210\u4fe1\u606f\u538b\u7f29\u548c\u6d8c\u73b0\uff0c\u5f62\u6210\u9ad8\u5c42\u6b21\u3001\u51c6\u786e\u7684\u7528\u6237\u753b\u50cf\u3002", "conclusion": "RGMem\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3LLM\u65f6\u4ee3\u8bed\u8a00\u4ee3\u7406\u7684\u957f\u671f\u8bb0\u5fc6\u548c\u884c\u4e3a\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u6df1\u5c42\u6b21\u7684\u4e2a\u6027\u5316\u4ea4\u4e92\u548c\u8de8\u4f1a\u8bdd\u8fde\u7eed\u6027\u3002"}}
{"id": "2510.16492", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16492", "abs": "https://arxiv.org/abs/2510.16492", "authors": ["Vamshi Krishna Bonagiri", "Ponnurangam Kumaragurum", "Khanh Nguyen", "Benjamin Plaut"], "title": "Check Yourself Before You Wreck Yourself: Selectively Quitting Improves LLM Agent Safety", "comment": "Reliable ML and Regulatable ML workshops, Neurips 2025", "summary": "As Large Language Model (LLM) agents increasingly operate in complex\nenvironments with real-world consequences, their safety becomes critical. While\nuncertainty quantification is well-studied for single-turn tasks, multi-turn\nagentic scenarios with real-world tool access present unique challenges where\nuncertainties and ambiguities compound, leading to severe or catastrophic risks\nbeyond traditional text generation failures. We propose using \"quitting\" as a\nsimple yet effective behavioral mechanism for LLM agents to recognize and\nwithdraw from situations where they lack confidence. Leveraging the ToolEmu\nframework, we conduct a systematic evaluation of quitting behavior across 12\nstate-of-the-art LLMs. Our results demonstrate a highly favorable\nsafety-helpfulness trade-off: agents prompted to quit with explicit\ninstructions improve safety by an average of +0.39 on a 0-3 scale across all\nmodels (+0.64 for proprietary models), while maintaining a negligible average\ndecrease of -0.03 in helpfulness. Our analysis demonstrates that simply adding\nexplicit quit instructions proves to be a highly effective safety mechanism\nthat can immediately be deployed in existing agent systems, and establishes\nquitting as an effective first-line defense mechanism for autonomous agents in\nhigh-stakes applications.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\"\u9000\u51fa\"\u4f5c\u4e3aLLM\u4ee3\u7406\u7684\u5b89\u5168\u673a\u5236\uff0c\u8ba9\u4ee3\u7406\u5728\u7f3a\u4e4f\u4fe1\u5fc3\u65f6\u4e3b\u52a8\u9000\u51fa\uff0c\u572812\u4e2a\u5148\u8fdbLLM\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\u800c\u51e0\u4e4e\u4e0d\u5f71\u54cd\u5b9e\u7528\u6027\u3002", "motivation": "\u968f\u7740LLM\u4ee3\u7406\u5728\u590d\u6742\u73b0\u5b9e\u73af\u5883\u4e2d\u8fd0\u884c\uff0c\u5176\u5b89\u5168\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u591a\u8f6e\u4ee3\u7406\u573a\u666f\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u6a21\u7cca\u6027\u4f1a\u7d2f\u79ef\uff0c\u5bfc\u81f4\u6bd4\u4f20\u7edf\u6587\u672c\u751f\u6210\u5931\u8d25\u66f4\u4e25\u91cd\u7684\u98ce\u9669\u3002", "method": "\u5229\u7528ToolEmu\u6846\u67b6\uff0c\u572812\u4e2a\u6700\u5148\u8fdb\u7684LLM\u4e0a\u7cfb\u7edf\u8bc4\u4f30\u9000\u51fa\u884c\u4e3a\uff0c\u901a\u8fc7\u6dfb\u52a0\u660e\u786e\u7684\u9000\u51fa\u6307\u4ee4\u6765\u8ba9\u4ee3\u7406\u5728\u7f3a\u4e4f\u4fe1\u5fc3\u65f6\u4e3b\u52a8\u9000\u51fa\u3002", "result": "\u7ed3\u679c\u663e\u793a\u5b89\u5168\u6027\u5e73\u5747\u63d0\u5347+0.39\uff080-3\u5206\u5236\uff09\uff0c\u4e13\u6709\u6a21\u578b\u63d0\u5347+0.64\uff0c\u800c\u5b9e\u7528\u6027\u4ec5\u5e73\u5747\u4e0b\u964d-0.03\uff0c\u5c55\u73b0\u51fa\u6781\u4f73\u7684\u5b89\u5168-\u5b9e\u7528\u6027\u6743\u8861\u3002", "conclusion": "\u7b80\u5355\u7684\u9000\u51fa\u6307\u4ee4\u662f\u4e00\u79cd\u9ad8\u5ea6\u6709\u6548\u7684\u5b89\u5168\u673a\u5236\uff0c\u53ef\u7acb\u5373\u90e8\u7f72\u5230\u73b0\u6709\u4ee3\u7406\u7cfb\u7edf\u4e2d\uff0c\u4f5c\u4e3a\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u81ea\u4e3b\u4ee3\u7406\u7684\u6709\u6548\u7b2c\u4e00\u9053\u9632\u7ebf\u3002"}}
{"id": "2510.16466", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16466", "abs": "https://arxiv.org/abs/2510.16466", "authors": ["Siddhartha Krothapalli", "Tridib Kumar Das", "Praveen Kumar", "Naveen Suravarpu", "Pratik Narang"], "title": "ReviewSense: Transforming Customer Review Dynamics into Actionable Business Insights", "comment": "11 pages, 1 figure, 4 tables", "summary": "As customer feedback becomes increasingly central to strategic growth, the\nability to derive actionable insights from unstructured reviews is essential.\nWhile traditional AI-driven systems excel at predicting user preferences, far\nless work has focused on transforming customer reviews into prescriptive,\nbusiness-facing recommendations. This paper introduces ReviewSense, a novel\nprescriptive decision support framework that leverages advanced large language\nmodels (LLMs) to transform customer reviews into targeted, actionable business\nrecommendations. By identifying key trends, recurring issues, and specific\nconcerns within customer sentiments, ReviewSense extends beyond\npreference-based systems to provide businesses with deeper insights for\nsustaining growth and enhancing customer loyalty. The novelty of this work lies\nin integrating clustering, LLM adaptation, and expert-driven evaluation into a\nunified, business-facing pipeline. Preliminary manual evaluations indicate\nstrong alignment between the model's recommendations and business objectives,\nhighlighting its potential for driving data-informed decision-making. This\nframework offers a new perspective on AI-driven sentiment analysis,\ndemonstrating its value in refining business strategies and maximizing the\nimpact of customer feedback.", "AI": {"tldr": "ReviewSense\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u51b3\u7b56\u652f\u6301\u6846\u67b6\uff0c\u53ef\u5c06\u5ba2\u6237\u8bc4\u8bba\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u4e1a\u52a1\u5efa\u8bae\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u504f\u597d\u9884\u6d4b\u7cfb\u7edf\u3002", "motivation": "\u4f20\u7edfAI\u7cfb\u7edf\u4e3b\u8981\u9884\u6d4b\u7528\u6237\u504f\u597d\uff0c\u4f46\u7f3a\u4e4f\u5c06\u5ba2\u6237\u8bc4\u8bba\u8f6c\u5316\u4e3a\u9762\u5411\u4e1a\u52a1\u7684\u89c4\u8303\u6027\u5efa\u8bae\u7684\u80fd\u529b\uff0c\u800c\u5ba2\u6237\u53cd\u9988\u5bf9\u6218\u7565\u589e\u957f\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6574\u5408\u805a\u7c7b\u3001LLM\u9002\u914d\u548c\u4e13\u5bb6\u9a71\u52a8\u8bc4\u4f30\u7684\u7edf\u4e00\u4e1a\u52a1\u7ba1\u9053\uff0c\u8bc6\u522b\u5ba2\u6237\u60c5\u611f\u4e2d\u7684\u5173\u952e\u8d8b\u52bf\u3001\u91cd\u590d\u95ee\u9898\u548c\u5177\u4f53\u5173\u6ce8\u70b9\u3002", "result": "\u521d\u6b65\u4eba\u5de5\u8bc4\u4f30\u663e\u793a\u6a21\u578b\u5efa\u8bae\u4e0e\u4e1a\u52a1\u76ee\u6807\u9ad8\u5ea6\u4e00\u81f4\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u6570\u636e\u9a71\u52a8\u51b3\u7b56\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aAI\u9a71\u52a8\u7684\u60c5\u611f\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4f18\u5316\u4e1a\u52a1\u7b56\u7565\u548c\u6700\u5927\u5316\u5ba2\u6237\u53cd\u9988\u4ef7\u503c\u65b9\u9762\u7684\u4f5c\u7528\u3002"}}
{"id": "2510.16499", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16499", "abs": "https://arxiv.org/abs/2510.16499", "authors": ["Michelle Yuan", "Khushbu Pahwa", "Shuaichen Chang", "Mustafa Kaba", "Jiarong Jiang", "Xiaofei Ma", "Yi Zhang", "Monica Sunkara"], "title": "Automated Composition of Agents: A Knapsack Approach for Agentic Component Selection", "comment": "Accepted to NeurIPS 2025 Conference", "summary": "Designing effective agentic systems requires the seamless composition and\nintegration of agents, tools, and models within dynamic and uncertain\nenvironments. Most existing methods rely on static, semantic retrieval\napproaches for tool or agent discovery. However, effective reuse and\ncomposition of existing components remain challenging due to incomplete\ncapability descriptions and the limitations of retrieval methods. Component\nselection suffers because the decisions are not based on capability, cost, and\nreal-time utility. To address these challenges, we introduce a structured,\nautomated framework for agentic system composition that is inspired by the\nknapsack problem. Our framework enables a composer agent to systematically\nidentify, select, and assemble an optimal set of agentic components by jointly\nconsidering performance, budget constraints, and compatibility. By dynamically\ntesting candidate components and modeling their utility in real-time, our\napproach streamlines the assembly of agentic systems and facilitates scalable\nreuse of resources. Empirical evaluation with Claude 3.5 Sonnet across five\nbenchmarking datasets shows that our online-knapsack-based composer\nconsistently lies on the Pareto frontier, achieving higher success rates at\nsignificantly lower component costs compared to our baselines. In the\nsingle-agent setup, the online knapsack composer shows a success rate\nimprovement of up to 31.6% in comparison to the retrieval baselines. In\nmulti-agent systems, the online knapsack composer increases success rate from\n37% to 87% when agents are selected from an agent inventory of 100+ agents. The\nsubstantial performance gap confirms the robust adaptability of our method\nacross diverse domains and budget constraints.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5728\u7ebf\u80cc\u5305\u95ee\u9898\u7684\u81ea\u52a8\u5316\u667a\u80fd\u4f53\u7cfb\u7edf\u7ec4\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u6d4b\u8bd5\u548c\u5b9e\u65f6\u6548\u7528\u5efa\u6a21\uff0c\u5728\u9884\u7b97\u7ea6\u675f\u4e0b\u4f18\u5316\u9009\u62e9\u667a\u80fd\u4f53\u7ec4\u4ef6\uff0c\u663e\u8457\u63d0\u5347\u6210\u529f\u7387\u5e76\u964d\u4f4e\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u7cfb\u7edf\u7ec4\u5408\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u8bed\u4e49\u68c0\u7d22\uff0c\u5b58\u5728\u80fd\u529b\u63cf\u8ff0\u4e0d\u5b8c\u6574\u3001\u68c0\u7d22\u65b9\u6cd5\u5c40\u9650\u7b49\u95ee\u9898\uff0c\u7ec4\u4ef6\u9009\u62e9\u672a\u5145\u5206\u8003\u8651\u80fd\u529b\u3001\u6210\u672c\u548c\u5b9e\u65f6\u6548\u7528\u3002", "method": "\u5f15\u5165\u7ed3\u6784\u5316\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u53d7\u80cc\u5305\u95ee\u9898\u542f\u53d1\uff0c\u8ba9\u7ec4\u5408\u5668\u667a\u80fd\u4f53\u7cfb\u7edf\u6027\u5730\u8bc6\u522b\u3001\u9009\u62e9\u548c\u7ec4\u88c5\u6700\u4f18\u667a\u80fd\u4f53\u7ec4\u4ef6\u96c6\uff0c\u8054\u5408\u8003\u8651\u6027\u80fd\u3001\u9884\u7b97\u7ea6\u675f\u548c\u517c\u5bb9\u6027\u3002", "result": "\u57285\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\uff0c\u5728\u7ebf\u80cc\u5305\u7ec4\u5408\u5668\u59cb\u7ec8\u4f4d\u4e8e\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5355\u667a\u80fd\u4f53\u8bbe\u7f6e\u4e0b\u6210\u529f\u7387\u63d0\u5347\u9ad8\u8fbe31.6%\uff0c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u4ece37%\u63d0\u5347\u523087%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u6837\u5316\u9886\u57df\u548c\u9884\u7b97\u7ea6\u675f\u4e0b\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u9002\u5e94\u6027\uff0c\u663e\u8457\u6027\u80fd\u5dee\u8ddd\u8bc1\u5b9e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.16476", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16476", "abs": "https://arxiv.org/abs/2510.16476", "authors": ["Xiaozhe Li", "Xinyu Fang", "Shengyuan Ding", "Linyang Li", "Haodong Duan", "Qingwen Liu", "Kai Chen"], "title": "NP-Engine: Empowering Optimization Reasoning in Large Language Models with Verifiable Synthetic NP Problems", "comment": null, "summary": "Large Language Models (LLMs) have shown strong reasoning capabilities, with\nmodels like OpenAI's O-series and DeepSeek R1 excelling at tasks such as\nmathematics, coding, logic, and puzzles through Reinforcement Learning with\nVerifiable Rewards (RLVR). However, their ability to solve more complex\noptimization problems - particularly NP-hard tasks - remains underexplored. To\nbridge this gap, we propose NP-ENGINE, the first comprehensive framework for\ntraining and evaluating LLMs on NP-hard problems. NP-ENGINE covers 10 tasks\nacross five domains, each equipped with (i) a controllable instance generator,\n(ii) a rule-based verifier, and (iii) a heuristic solver that provides\napproximate optimal solutions as ground truth. This\ngenerator-verifier-heuristic pipeline enables scalable and verifiable RLVR\ntraining under hierarchical difficulties. We also introduce NP-BENCH, a\nbenchmark derived from NP-ENGINE-DATA, specifically designed to evaluate LLMs'\nability to tackle NP-hard level reasoning problems, focusing not only on\nfeasibility but also on solution quality. Additionally, we present\nQWEN2.5-7B-NP, a model trained via zero-RLVR with curriculum learning on\nQwen2.5-7B-Instruct, which significantly outperforms GPT-4o on NP-BENCH and\nachieves SOTA performance with the same model size. Beyond in-domain tasks, we\ndemonstrate that RLVR training on NP-ENGINE-DATA enables strong out-of-domain\n(OOD) generalization to reasoning tasks (logic, puzzles, math, and knowledge),\nas well as non-reasoning tasks such as instruction following. We also observe a\nscaling trend: increasing task diversity improves OOD generalization. These\nfindings suggest that task-rich RLVR training is a promising direction for\nadvancing LLM's reasoning ability, revealing new insights into the scaling laws\nof RLVR.", "AI": {"tldr": "\u63d0\u51fa\u4e86NP-ENGINE\u6846\u67b6\uff0c\u8fd9\u662f\u9996\u4e2a\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30LLMs\u5728NP\u96be\u95ee\u9898\u4e0a\u7684\u7efc\u5408\u6846\u67b6\uff0c\u5305\u542b10\u4e2a\u4efb\u52a1\u3001\u53ef\u63a7\u5b9e\u4f8b\u751f\u6210\u5668\u3001\u89c4\u5219\u9a8c\u8bc1\u5668\u548c\u542f\u53d1\u5f0f\u6c42\u89e3\u5668\u3002\u8bad\u7ec3\u51fa\u7684QWEN2.5-7B-NP\u6a21\u578b\u5728NP-BENCH\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8eGPT-4o\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u6570\u5b66\u3001\u7f16\u7a0b\u3001\u903b\u8f91\u7b49\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u89e3\u51b3\u66f4\u590d\u6742\u7684NP\u96be\u4f18\u5316\u95ee\u9898\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u4e13\u95e8\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51faNP-ENGINE\u6846\u67b6\uff0c\u5305\u542b\u751f\u6210\u5668-\u9a8c\u8bc1\u5668-\u542f\u53d1\u5f0f\u6c42\u89e3\u5668\u7ba1\u9053\uff0c\u652f\u6301\u53ef\u6269\u5c55\u7684RLVR\u8bad\u7ec3\u3002\u4f7f\u7528\u96f6RLVR\u548c\u8bfe\u7a0b\u5b66\u4e60\u8bad\u7ec3QWEN2.5-7B-NP\u6a21\u578b\u3002", "result": "QWEN2.5-7B-NP\u5728NP-BENCH\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8eGPT-4o\uff0c\u8fbe\u5230\u540c\u6a21\u578b\u5c3a\u5bf8\u4e0b\u7684SOTA\u6027\u80fd\u3002\u8bad\u7ec3\u8fd8\u5e26\u6765\u4e86\u5f3a\u5927\u7684\u8de8\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u5305\u62ec\u63a8\u7406\u4efb\u52a1\u548c\u975e\u63a8\u7406\u4efb\u52a1\u3002", "conclusion": "\u4efb\u52a1\u4e30\u5bcc\u7684RLVR\u8bad\u7ec3\u662f\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u7684\u6709\u524d\u666f\u65b9\u5411\uff0c\u63ed\u793a\u4e86RLVR\u7684\u6269\u5c55\u89c4\u5f8b\uff0c\u589e\u52a0\u4efb\u52a1\u591a\u6837\u6027\u53ef\u6539\u5584\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.16549", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16549", "abs": "https://arxiv.org/abs/2510.16549", "authors": ["Haoxuan Zhang", "Ruochi Li", "Sarthak Shrestha", "Shree Harshini Mamidala", "Revanth Putta", "Arka Krishan Aggarwal", "Ting Xiao", "Junhua Ding", "Haihua Chen"], "title": "ReviewGuard: Enhancing Deficient Peer Review Detection via LLM-Driven Data Augmentation", "comment": null, "summary": "Peer review serves as the gatekeeper of science, yet the surge in submissions\nand widespread adoption of large language models (LLMs) in scholarly evaluation\npresent unprecedented challenges. Recent work has focused on using LLMs to\nimprove review efficiency or generate insightful review content. However,\nunchecked deficient reviews from both human experts and AI systems threaten to\nsystematically undermine the peer review ecosystem and compromise academic\nintegrity. To address this critical issue, we introduce ReviewGuard, an\nautomated system for detecting and categorizing deficient reviews. ReviewGuard\nemploys a comprehensive four-stage LLM-driven framework that: (1) collects ICLR\nand NeurIPS papers with their corresponding reviews from OpenReview; (2)\nannotates review types using GPT-4.1 with human validation; (3) addresses class\nimbalance and data scarcity through LLM-driven synthetic data augmentation,\nproducing a final corpus of 6,634 papers, 24,657 real reviews, and 46,438\nsynthetic reviews; and (4) fine-tunes both encoder-based models and open source\nLLMs. We perform comprehensive feature analysis of the structure and quality of\nthe review text. Compared to sufficient reviews, deficient reviews demonstrate\nlower rating scores, higher self-reported confidence, reduced structural\ncomplexity, and a higher proportion of negative sentiment. AI-generated text\ndetection reveals that, since ChatGPT's emergence, AI-generated reviews have\nincreased dramatically. In the evaluation of deficient review detection models,\nmixed training with synthetic and real review data provides substantial\nenhancements to recall and F1 scores on the binary task. This study presents\nthe first LLM-driven system for detecting deficient peer reviews, providing\nevidence to inform AI governance in peer review while offering valuable\ninsights into human-AI collaboration to maintain academic integrity.", "AI": {"tldr": "\u63d0\u51fa\u4e86ReviewGuard\u7cfb\u7edf\uff0c\u4f7f\u7528\u56db\u9636\u6bb5LLM\u9a71\u52a8\u6846\u67b6\u81ea\u52a8\u68c0\u6d4b\u548c\u5206\u7c7b\u6709\u7f3a\u9677\u7684\u540c\u884c\u8bc4\u5ba1\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u589e\u5f3a\u548c\u6a21\u578b\u5fae\u8c03\u63d0\u9ad8\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u540c\u884c\u8bc4\u5ba1\u4f5c\u4e3a\u79d1\u5b66\u5b88\u95e8\u4eba\u9762\u4e34\u63d0\u4ea4\u91cf\u6fc0\u589e\u548cLLM\u5e7f\u6cdb\u4f7f\u7528\u7684\u6311\u6218\uff0c\u6709\u7f3a\u9677\u7684\u8bc4\u5ba1\uff08\u6765\u81ea\u4eba\u7c7b\u4e13\u5bb6\u548cAI\u7cfb\u7edf\uff09\u5a01\u80c1\u540c\u884c\u8bc4\u5ba1\u751f\u6001\u7cfb\u7edf\u548c\u5b66\u672f\u8bda\u4fe1\u3002", "method": "\u56db\u9636\u6bb5LLM\u9a71\u52a8\u6846\u67b6\uff1a\u6536\u96c6ICLR\u548cNeurIPS\u8bba\u6587\u53ca\u8bc4\u5ba1\uff1b\u4f7f\u7528GPT-4.1\u6807\u6ce8\u8bc4\u5ba1\u7c7b\u578b\uff1b\u901a\u8fc7LLM\u9a71\u52a8\u7684\u5408\u6210\u6570\u636e\u589e\u5f3a\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\uff1b\u5fae\u8c03\u7f16\u7801\u5668\u6a21\u578b\u548c\u5f00\u6e90LLM\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b6,634\u7bc7\u8bba\u6587\u300124,657\u4e2a\u771f\u5b9e\u8bc4\u5ba1\u548c46,438\u4e2a\u5408\u6210\u8bc4\u5ba1\u7684\u8bed\u6599\u5e93\u3002\u6709\u7f3a\u9677\u8bc4\u5ba1\u663e\u793a\u8f83\u4f4e\u8bc4\u5206\u3001\u8f83\u9ad8\u81ea\u4fe1\u5ea6\u3001\u7ed3\u6784\u590d\u6742\u6027\u964d\u4f4e\u548c\u8d1f\u9762\u60c5\u7eea\u6bd4\u4f8b\u66f4\u9ad8\u3002AI\u751f\u6210\u8bc4\u5ba1\u81eaChatGPT\u51fa\u73b0\u540e\u663e\u8457\u589e\u52a0\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u7528\u4e8e\u68c0\u6d4b\u6709\u7f3a\u9677\u540c\u884c\u8bc4\u5ba1\u7684LLM\u9a71\u52a8\u7cfb\u7edf\uff0c\u4e3a\u540c\u884c\u8bc4\u5ba1\u4e2d\u7684AI\u6cbb\u7406\u63d0\u4f9b\u8bc1\u636e\uff0c\u5e76\u4e3a\u7ef4\u62a4\u5b66\u672f\u8bda\u4fe1\u7684\u4eba\u673a\u534f\u4f5c\u63d0\u4f9b\u5b9d\u8d35\u89c1\u89e3\u3002"}}
{"id": "2510.16533", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16533", "abs": "https://arxiv.org/abs/2510.16533", "authors": ["Eilene Tomkins-Flanagan", "Connor Hanley", "Mary A. Kelly"], "title": "Hey Pentti, We Did It Again!: Differentiable vector-symbolic types that prove polynomial termination", "comment": null, "summary": "We present a typed computer language, Doug, in which all typed programs may\nbe proved to halt in polynomial time, encoded in a vector-symbolic architecture\n(VSA). Doug is just an encoding of the light linear functional programming\nlanguage (LLFPL) described by (Schimanski2009, ch. 7). The types of Doug are\nencoded using a slot-value encoding scheme based on holographic declarative\nmemory (HDM; Kelly, 2020). The terms of Doug are encoded using a variant of the\nLisp VSA defined by (Flanagan, 2024). Doug allows for some points on the\nembedding space of a neural network to be interpreted as types, where the types\nof nearby points are similar both in structure and content. Types in Doug are\ntherefore learnable by a neural network. Following (Chollet, 2019), (Card,\n1983), and (Newell, 1981), we view skill as the application of a procedure, or\nprogram of action, that causes a goal to be satisfied. Skill acquisition may\ntherefore be expressed as program synthesis. Using Doug, we hope to describe a\nform of learning of skilled behaviour that follows a human-like pace of skill\nacquisition (i.e., substantially faster than brute force; Heathcote, 2000),\nexceeding the efficiency of all currently existing approaches (Kaplan, 2020;\nJones, 2021; Chollet, 2024). Our approach brings us one step closer to modeling\nhuman mental representations, as they must actually exist in the brain, and\nthose representations' acquisition, as they are actually learned.", "AI": {"tldr": "Doug\u662f\u4e00\u79cd\u7c7b\u578b\u5316\u8ba1\u7b97\u673a\u8bed\u8a00\uff0c\u57fa\u4e8e\u5411\u91cf\u7b26\u53f7\u67b6\u6784(VSA)\u7f16\u7801\uff0c\u786e\u4fdd\u6240\u6709\u7c7b\u578b\u5316\u7a0b\u5e8f\u90fd\u80fd\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u505c\u6b62\u8fd0\u884c\u3002\u8be5\u8bed\u8a00\u652f\u6301\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u7c7b\u578b\uff0c\u65e8\u5728\u5b9e\u73b0\u7c7b\u4f3c\u4eba\u7c7b\u8282\u594f\u7684\u6280\u80fd\u83b7\u53d6\u3002", "motivation": "\u7814\u7a76\u76ee\u6807\u662f\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u6a21\u62df\u4eba\u7c7b\u5fc3\u667a\u8868\u5f81\u53ca\u5176\u83b7\u53d6\u8fc7\u7a0b\u7684\u8ba1\u7b97\u6a21\u578b\uff0c\u5b9e\u73b0\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u9ad8\u6548\u7684\u4eba\u7c7b\u5316\u6280\u80fd\u5b66\u4e60\u901f\u5ea6\u3002", "method": "\u57fa\u4e8e\u8f7b\u91cf\u7ebf\u6027\u51fd\u6570\u5f0f\u7f16\u7a0b\u8bed\u8a00(LLFPL)\u6784\u5efaDoug\u8bed\u8a00\uff0c\u4f7f\u7528\u57fa\u4e8e\u5168\u606f\u58f0\u660e\u6027\u8bb0\u5fc6(HDM)\u7684\u69fd\u503c\u7f16\u7801\u65b9\u6848\u7f16\u7801\u7c7b\u578b\uff0c\u91c7\u7528Lisp VSA\u53d8\u4f53\u7f16\u7801\u672f\u8bed\uff0c\u4f7f\u795e\u7ecf\u7f51\u7edc\u80fd\u591f\u5b66\u4e60\u7c7b\u578b\u3002", "result": "\u63d0\u51fa\u4e86Doug\u8bed\u8a00\u7684\u5b8c\u6574\u8bbe\u8ba1\u6846\u67b6\uff0c\u652f\u6301\u7c7b\u578b\u4f5c\u4e3a\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u53ef\u5b66\u4e60\u70b9\uff0c\u4e3a\u7a0b\u5e8f\u5408\u6210\u5f62\u5f0f\u7684\u6280\u80fd\u83b7\u53d6\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "Doug\u8bed\u8a00\u4e3a\u5b9e\u73b0\u4eba\u7c7b\u5316\u6280\u80fd\u83b7\u53d6\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\uff0c\u4e3a\u5efa\u6a21\u5927\u8111\u4e2d\u5b9e\u9645\u5b58\u5728\u7684\u5fc3\u7406\u8868\u5f81\u53ca\u5176\u5b66\u4e60\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u65b0\u7684\u8ba1\u7b97\u6846\u67b6\u3002"}}
{"id": "2510.16565", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16565", "abs": "https://arxiv.org/abs/2510.16565", "authors": ["Seungho Cho", "Changgeon Ko", "Eui Jun Hwang", "Junmyeong Lee", "Huije Lee", "Jong C. Park"], "title": "Language over Content: Tracing Cultural Understanding in Multilingual Large Language Models", "comment": "Accepted to CIKM 2025 Workshop on Human Centric AI", "summary": "Large language models (LLMs) are increasingly used across diverse cultural\ncontexts, making accurate cultural understanding essential. Prior evaluations\nhave mostly focused on output-level performance, obscuring the factors that\ndrive differences in responses, while studies using circuit analysis have\ncovered few languages and rarely focused on culture. In this work, we trace\nLLMs' internal cultural understanding mechanisms by measuring activation path\noverlaps when answering semantically equivalent questions under two conditions:\nvarying the target country while fixing the question language, and varying the\nquestion language while fixing the country. We also use same-language country\npairs to disentangle language from cultural aspects. Results show that internal\npaths overlap more for same-language, cross-country questions than for\ncross-language, same-country questions, indicating strong language-specific\npatterns. Notably, the South Korea-North Korea pair exhibits low overlap and\nhigh variability, showing that linguistic similarity does not guarantee aligned\ninternal representation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5206\u6790LLM\u5728\u56de\u7b54\u6587\u5316\u76f8\u5173\u95ee\u9898\u65f6\u5185\u90e8\u6fc0\u6d3b\u8def\u5f84\u7684\u91cd\u53e0\u7a0b\u5ea6\uff0c\u63ed\u793a\u4e86\u8bed\u8a00\u5bf9\u6587\u5316\u7406\u89e3\u673a\u5236\u7684\u91cd\u8981\u5f71\u54cd\uff0c\u53d1\u73b0\u8bed\u8a00\u76f8\u4f3c\u6027\u5e76\u4e0d\u4fdd\u8bc1\u5185\u90e8\u8868\u5f81\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u968f\u7740LLM\u5728\u591a\u5143\u6587\u5316\u80cc\u666f\u4e0b\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u51c6\u786e\u7684\u6587\u5316\u7406\u89e3\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u8bc4\u4f30\u591a\u5173\u6ce8\u8f93\u51fa\u5c42\u9762\uff0c\u96be\u4ee5\u63ed\u793a\u54cd\u5e94\u5dee\u5f02\u7684\u9a71\u52a8\u56e0\u7d20\uff0c\u800c\u7535\u8def\u5206\u6790\u7814\u7a76\u8986\u76d6\u8bed\u8a00\u5c11\u4e14\u5f88\u5c11\u805a\u7126\u6587\u5316\u3002", "method": "\u901a\u8fc7\u6d4b\u91cfLLM\u5728\u56de\u7b54\u8bed\u4e49\u7b49\u4ef7\u95ee\u9898\u65f6\u7684\u6fc0\u6d3b\u8def\u5f84\u91cd\u53e0\uff1a1\uff09\u56fa\u5b9a\u95ee\u9898\u8bed\u8a00\uff0c\u6539\u53d8\u76ee\u6807\u56fd\u5bb6\uff1b2\uff09\u56fa\u5b9a\u56fd\u5bb6\uff0c\u6539\u53d8\u95ee\u9898\u8bed\u8a00\u3002\u4f7f\u7528\u540c\u8bed\u8a00\u56fd\u5bb6\u5bf9\u6765\u5206\u79bb\u8bed\u8a00\u548c\u6587\u5316\u56e0\u7d20\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u540c\u8bed\u8a00\u8de8\u56fd\u5bb6\u95ee\u9898\u7684\u5185\u90e8\u8def\u5f84\u91cd\u53e0\u5ea6\u9ad8\u4e8e\u8de8\u8bed\u8a00\u540c\u56fd\u5bb6\u95ee\u9898\uff0c\u8868\u660e\u5b58\u5728\u5f3a\u70c8\u7684\u8bed\u8a00\u7279\u5b9a\u6a21\u5f0f\u3002\u7279\u522b\u662f\u97e9\u56fd-\u671d\u9c9c\u5bf9\u7684\u4f4e\u91cd\u53e0\u548c\u9ad8\u53d8\u5f02\u6027\u663e\u793a\u8bed\u8a00\u76f8\u4f3c\u6027\u4e0d\u4fdd\u8bc1\u5185\u90e8\u8868\u5f81\u5bf9\u9f50\u3002", "conclusion": "LLM\u7684\u6587\u5316\u7406\u89e3\u673a\u5236\u53d7\u8bed\u8a00\u5f71\u54cd\u663e\u8457\uff0c\u8bed\u8a00\u76f8\u4f3c\u6027\u4e0d\u8db3\u4ee5\u786e\u4fdd\u6587\u5316\u8868\u5f81\u7684\u4e00\u81f4\u6027\uff0c\u8fd9\u5bf9\u8de8\u6587\u5316LLM\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.16555", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16555", "abs": "https://arxiv.org/abs/2510.16555", "authors": ["Qiongyan Wang", "Xingchen Zou", "Yutian Jiang", "Haomin Wen", "Jiaheng Wei", "Qingsong Wen", "Yuxuan Liang"], "title": "Urban-R1: Reinforced MLLMs Mitigate Geospatial Biases for Urban General Intelligence", "comment": null, "summary": "Rapid urbanization intensifies the demand for Urban General Intelligence\n(UGI), referring to AI systems that can understand and reason about complex\nurban environments. Recent studies have built urban foundation models using\nsupervised fine-tuning (SFT) of LLMs and MLLMs, yet these models exhibit\npersistent geospatial bias, producing regionally skewed predictions and limited\ngeneralization. To this end, we propose Urban-R1, a reinforcement\nlearning-based post-training framework that aligns MLLMs with the objectives of\nUGI. Urban-R1 adopts Group Relative Policy Optimization (GRPO) to optimize\nreasoning across geographic groups and employs urban region profiling as a\nproxy task to provide measurable rewards from multimodal urban data. Extensive\nexperiments across diverse regions and tasks show that Urban-R1 effectively\nmitigates geo-bias and improves cross-region generalization, outperforming both\nSFT-trained and closed-source models. Our results highlight reinforcement\nlearning alignment as a promising pathway toward equitable and trustworthy\nurban intelligence.", "AI": {"tldr": "Urban-R1\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7GRPO\u4f18\u5316\u5730\u7406\u7fa4\u4f53\u95f4\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f7f\u7528\u57ce\u5e02\u533a\u57df\u753b\u50cf\u4f5c\u4e3a\u4ee3\u7406\u4efb\u52a1\u6765\u7f13\u89e3\u591a\u6a21\u6001\u57ce\u5e02\u6a21\u578b\u7684\u5730\u7406\u504f\u89c1\u95ee\u9898\u3002", "motivation": "\u5feb\u901f\u57ce\u5e02\u5316\u52a0\u5267\u4e86\u5bf9\u57ce\u5e02\u901a\u7528\u667a\u80fd(UGI)\u7684\u9700\u6c42\uff0c\u4f46\u73b0\u6709\u57fa\u4e8e\u76d1\u7763\u5fae\u8c03\u7684\u57ce\u5e02\u57fa\u7840\u6a21\u578b\u5b58\u5728\u6301\u7eed\u7684\u5730\u7406\u504f\u89c1\uff0c\u5bfc\u81f4\u533a\u57df\u9884\u6d4b\u504f\u5dee\u548c\u6709\u9650\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faUrban-R1\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u91c7\u7528\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316(GRPO)\u6765\u4f18\u5316\u8de8\u5730\u7406\u7fa4\u4f53\u7684\u63a8\u7406\uff0c\u5e76\u4f7f\u7528\u57ce\u5e02\u533a\u57df\u753b\u50cf\u4f5c\u4e3a\u4ee3\u7406\u4efb\u52a1\u4ece\u591a\u6a21\u6001\u57ce\u5e02\u6570\u636e\u4e2d\u63d0\u4f9b\u53ef\u8861\u91cf\u7684\u5956\u52b1\u3002", "result": "\u8de8\u591a\u4e2a\u533a\u57df\u548c\u4efb\u52a1\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cUrban-R1\u6709\u6548\u7f13\u89e3\u4e86\u5730\u7406\u504f\u89c1\u5e76\u63d0\u9ad8\u4e86\u8de8\u533a\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u4f18\u4e8e\u57fa\u4e8e\u76d1\u7763\u5fae\u8c03\u548c\u95ed\u6e90\u6a21\u578b\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\u662f\u5b9e\u73b0\u516c\u5e73\u53ef\u4fe1\u57ce\u5e02\u667a\u80fd\u7684\u6709\u524d\u666f\u9014\u5f84\uff0cUrban-R1\u6846\u67b6\u4e3a\u6784\u5efa\u65e0\u504f\u89c1\u7684\u57ce\u5e02\u901a\u7528\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16567", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.16567", "abs": "https://arxiv.org/abs/2510.16567", "authors": ["Alkis Koudounas", "Moreno La Quatra", "Manuel Giollo", "Sabato Marco Siniscalchi", "Elena Baralis"], "title": "Hallucination Benchmark for Speech Foundation Models", "comment": "Under Review", "summary": "Hallucinations in automatic speech recognition (ASR) systems refer to fluent\nand coherent transcriptions produced by neural ASR models that are completely\nunrelated to the underlying acoustic input (i.e., the speech signal). While\nsimilar to conventional decoding errors in potentially compromising the\nusability of transcriptions for downstream applications, hallucinations can be\nmore detrimental due to their preservation of syntactically and semantically\nplausible structure. This apparent coherence can mislead subsequent processing\nstages and introduce serious risks, particularly in critical domains such as\nhealthcare and law. Conventional evaluation metrics are primarily centered on\nerror-based metrics and fail to distinguish between phonetic inaccuracies and\nhallucinations. Consequently, there is a critical need for new evaluation\nframeworks that can effectively identify and assess models with a heightened\npropensity for generating hallucinated content. To this end, we introduce\nSHALLOW, the first benchmark framework that systematically categorizes and\nquantifies hallucination phenomena in ASR along four complementary axes:\nlexical, phonetic, morphological, and semantic. We define targeted metrics\nwithin each category to produce interpretable profiles of model behavior.\nThrough evaluation across various architectures and speech domains, we have\nfound that SHALLOW metrics correlate strongly with word error rate (WER) when\nrecognition quality is high (i.e., low WER). Still, this correlation weakens\nsubstantially as WER increases. SHALLOW, therefore, captures fine-grained error\npatterns that WER fails to distinguish under degraded and challenging\nconditions. Our framework supports specific diagnosis of model weaknesses and\nprovides feedback for model improvement beyond what aggregate error rates can\noffer.", "AI": {"tldr": "SHALLOW\u662f\u9996\u4e2a\u7cfb\u7edf\u5206\u7c7b\u548c\u91cf\u5316ASR\u7cfb\u7edf\u4e2d\u5e7b\u89c9\u73b0\u8c61\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u8bcd\u6c47\u3001\u8bed\u97f3\u3001\u5f62\u6001\u548c\u8bed\u4e49\u56db\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u6a21\u578b\uff0c\u5728WER\u8f83\u9ad8\u65f6\u80fd\u6355\u6349WER\u65e0\u6cd5\u533a\u5206\u7684\u7ec6\u7c92\u5ea6\u9519\u8bef\u6a21\u5f0f\u3002", "motivation": "ASR\u7cfb\u7edf\u4e2d\u7684\u5e7b\u89c9\u4f1a\u4ea7\u751f\u4e0e\u8bed\u97f3\u4fe1\u53f7\u5b8c\u5168\u65e0\u5173\u4f46\u8bed\u6cd5\u8bed\u4e49\u5408\u7406\u7684\u8f6c\u5f55\uff0c\u5728\u533b\u7597\u548c\u6cd5\u5f8b\u7b49\u5173\u952e\u9886\u57df\u5e26\u6765\u4e25\u91cd\u98ce\u9669\uff0c\u800c\u4f20\u7edf\u8bc4\u4f30\u6307\u6807\u65e0\u6cd5\u533a\u5206\u8bed\u97f3\u4e0d\u51c6\u786e\u548c\u5e7b\u89c9\u3002", "method": "\u5f15\u5165SHALLOW\u57fa\u51c6\u6846\u67b6\uff0c\u7cfb\u7edf\u5730\u5c06ASR\u5e7b\u89c9\u73b0\u8c61\u5206\u7c7b\u4e3a\u8bcd\u6c47\u3001\u8bed\u97f3\u3001\u5f62\u6001\u548c\u8bed\u4e49\u56db\u4e2a\u4e92\u8865\u7ef4\u5ea6\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u7c7b\u522b\u5b9a\u4e49\u9488\u5bf9\u6027\u6307\u6807\u6765\u751f\u6210\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u884c\u4e3a\u5206\u6790\u3002", "result": "SHALLOW\u6307\u6807\u5728\u8bc6\u522b\u8d28\u91cf\u9ad8\u65f6\u4e0eWER\u5f3a\u76f8\u5173\uff0c\u4f46\u968f\u7740WER\u589e\u52a0\u76f8\u5173\u6027\u663e\u8457\u51cf\u5f31\uff0c\u80fd\u591f\u5728\u9000\u5316\u6761\u4ef6\u4e0b\u6355\u6349WER\u65e0\u6cd5\u533a\u5206\u7684\u7ec6\u7c92\u5ea6\u9519\u8bef\u6a21\u5f0f\u3002", "conclusion": "SHALLOW\u6846\u67b6\u652f\u6301\u5bf9\u6a21\u578b\u5f31\u70b9\u7684\u5177\u4f53\u8bca\u65ad\uff0c\u5e76\u63d0\u4f9b\u8d85\u51fa\u805a\u5408\u9519\u8bef\u7387\u6240\u80fd\u63d0\u4f9b\u7684\u6a21\u578b\u6539\u8fdb\u53cd\u9988\uff0c\u662f\u8bc4\u4f30ASR\u7cfb\u7edf\u5e7b\u89c9\u503e\u5411\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2510.16559", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16559", "abs": "https://arxiv.org/abs/2510.16559", "authors": ["Tian Xia", "Tianrun Gao", "Wenhao Deng", "Long Wei", "Xiaowei Qian", "Yixian Jiang", "Chenglei Yu", "Tailin Wu"], "title": "BuildArena: A Physics-Aligned Interactive Benchmark of LLMs for Engineering Construction", "comment": "33 pages, 10 figures", "summary": "Engineering construction automation aims to transform natural language\nspecifications into physically viable structures, requiring complex integrated\nreasoning under strict physical constraints. While modern LLMs possess broad\nknowledge and strong reasoning capabilities that make them promising candidates\nfor this domain, their construction competencies remain largely unevaluated. To\naddress this gap, we introduce BuildArena, the first physics-aligned\ninteractive benchmark designed for language-driven engineering construction. It\ncontributes to the community in four aspects: (1) a highly customizable\nbenchmarking framework for in-depth comparison and analysis of LLMs; (2) an\nextendable task design strategy spanning static and dynamic mechanics across\nmultiple difficulty tiers; (3) a 3D Spatial Geometric Computation Library for\nsupporting construction based on language instructions; (4) a baseline LLM\nagentic workflow that effectively evaluates diverse model capabilities. On\neight frontier LLMs, BuildArena comprehensively evaluates their capabilities\nfor language-driven and physics-grounded construction automation. The project\npage is at https://build-arena.github.io/.", "AI": {"tldr": "BuildArena\u662f\u9996\u4e2a\u9762\u5411\u8bed\u8a00\u9a71\u52a8\u5de5\u7a0b\u5efa\u8bbe\u7684\u7269\u7406\u5bf9\u9f50\u4ea4\u4e92\u5f0f\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u5de5\u7a0b\u5efa\u7b51\u81ea\u52a8\u5316\u4e2d\u7684\u80fd\u529b\u3002", "motivation": "\u5f53\u524dLLM\u5728\u5de5\u7a0b\u5efa\u7b51\u9886\u57df\u7684\u5efa\u8bbe\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u8bc4\u4f30\uff0c\u9700\u8981\u4e13\u95e8\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5f00\u53d1\u4e86\u53ef\u5b9a\u5236\u57fa\u51c6\u6846\u67b6\u3001\u53ef\u6269\u5c55\u4efb\u52a1\u8bbe\u8ba1\u7b56\u7565\u30013D\u7a7a\u95f4\u51e0\u4f55\u8ba1\u7b97\u5e93\u548c\u57fa\u7ebfLLM\u667a\u80fd\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "\u5728\u516b\u4e2a\u524d\u6cbfLLM\u4e0a\u5168\u9762\u8bc4\u4f30\u4e86\u5b83\u4eec\u5728\u8bed\u8a00\u9a71\u52a8\u548c\u7269\u7406\u57fa\u7840\u5efa\u8bbe\u81ea\u52a8\u5316\u65b9\u9762\u7684\u80fd\u529b\u3002", "conclusion": "BuildArena\u4e3a\u8bed\u8a00\u9a71\u52a8\u7684\u5de5\u7a0b\u5efa\u7b51\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u9996\u4e2a\u7269\u7406\u5bf9\u9f50\u7684\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.16573", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16573", "abs": "https://arxiv.org/abs/2510.16573", "authors": ["Muhammad Ammar", "Hadiya Murad Hadi", "Usman Majeed Butt"], "title": "AI-Generated Text Detection in Low-Resource Languages: A Case Study on Urdu", "comment": null, "summary": "Large Language Models (LLMs) are now capable of generating text that closely\nresembles human writing, making them powerful tools for content creation, but\nthis growing ability has also made it harder to tell whether a piece of text\nwas written by a human or by a machine. This challenge becomes even more\nserious for languages like Urdu, where there are very few tools available to\ndetect AI-generated text. To address this gap, we propose a novel AI-generated\ntext detection framework tailored for the Urdu language. A balanced dataset\ncomprising 1,800 humans authored, and 1,800 AI generated texts, sourced from\nmodels such as Gemini, GPT-4o-mini, and Kimi AI was developed. Detailed\nlinguistic and statistical analysis was conducted, focusing on features such as\ncharacter and word counts, vocabulary richness (Type Token Ratio), and N-gram\npatterns, with significance evaluated through t-tests and MannWhitney U tests.\nThree state-of-the-art multilingual transformer models such as\nmdeberta-v3-base, distilbert-base-multilingualcased, and xlm-roberta-base were\nfine-tuned on this dataset. The mDeBERTa-v3-base achieved the highest\nperformance, with an F1-score 91.29 and accuracy of 91.26% on the test set.\nThis research advances efforts in contesting misinformation and academic\nmisconduct in Urdu-speaking communities and contributes to the broader\ndevelopment of NLP tools for low resource languages.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u4e4c\u5c14\u90fd\u8bed\u7684AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u6846\u67b6\uff0c\u4f7f\u7528\u591a\u8bed\u8a00transformer\u6a21\u578b\u5728\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0cmDeBERTa-v3-base\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523091.29% F1\u5206\u6570\u548c91.26%\u51c6\u786e\u7387\u3002", "motivation": "\u968f\u7740LLMs\u751f\u6210\u6587\u672c\u80fd\u529b\u589e\u5f3a\uff0c\u533a\u5206\u4eba\u7c7b\u4e0e\u673a\u5668\u751f\u6210\u6587\u672c\u53d8\u5f97\u56f0\u96be\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u4e4c\u5c14\u90fd\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u7f3a\u4e4f\u6709\u6548\u7684AI\u6587\u672c\u68c0\u6d4b\u5de5\u5177\u3002", "method": "\u6784\u5efa\u5305\u542b1800\u7bc7\u4eba\u7c7b\u64b0\u5199\u548c1800\u7bc7AI\u751f\u6210\u6587\u672c\u7684\u5e73\u8861\u6570\u636e\u96c6\uff0c\u8fdb\u884c\u8bed\u8a00\u548c\u7edf\u8ba1\u5206\u6790\uff0c\u5fae\u8c03\u4e09\u79cd\u591a\u8bed\u8a00transformer\u6a21\u578b\uff08mdeberta-v3-base\u3001distilbert-base-multilingualcased\u3001xlm-roberta-base\uff09\u3002", "result": "mDeBERTa-v3-base\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u5728\u6d4b\u8bd5\u96c6\u4e0aF1\u5206\u6570\u4e3a91.29%\uff0c\u51c6\u786e\u7387\u4e3a91.26%\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u8fdb\u4e86\u5728\u4e4c\u5c14\u90fd\u8bed\u793e\u533a\u5bf9\u6297\u9519\u8bef\u4fe1\u606f\u548c\u5b66\u672f\u4e0d\u7aef\u884c\u4e3a\u7684\u52aa\u529b\uff0c\u5e76\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684NLP\u5de5\u5177\u5f00\u53d1\u505a\u51fa\u4e86\u8d21\u732e\u3002"}}
{"id": "2510.16572", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.16572", "abs": "https://arxiv.org/abs/2510.16572", "authors": ["Ayush Chopra", "Aman Sharma", "Feroz Ahmad", "Luca Muscariello", "Vijoy Pandey", "Ramesh Raskar"], "title": "Ripple Effect Protocol: Coordinating Agent Populations", "comment": null, "summary": "Modern AI agents can exchange messages using protocols such as A2A and ACP,\nyet these mechanisms emphasize communication over coordination. As agent\npopulations grow, this limitation produces brittle collective behavior, where\nindividually smart agents converge on poor group outcomes. We introduce the\nRipple Effect Protocol (REP), a coordination protocol in which agents share not\nonly their decisions but also lightweight sensitivities - signals expressing\nhow their choices would change if key environmental variables shifted. These\nsensitivities ripple through local networks, enabling groups to align faster\nand more stably than with agent-centric communication alone. We formalize REP's\nprotocol specification, separating required message schemas from optional\naggregation rules, and evaluate it across scenarios with varying incentives and\nnetwork topologies. Benchmarks across three domains: (i) supply chain cascades\n(Beer Game), (ii) preference aggregation in sparse networks (Movie Scheduling),\nand (iii) sustainable resource allocation (Fishbanks) show that REP improves\ncoordination accuracy and efficiency over A2A by 41 to 100%, while flexibly\nhandling multimodal sensitivity signals from LLMs. By making coordination a\nprotocol-level capability, REP provides scalable infrastructure for the\nemerging Internet of Agents", "AI": {"tldr": "\u63d0\u51fa\u4e86Ripple Effect Protocol (REP)\uff0c\u4e00\u79cd\u534f\u8c03\u534f\u8bae\uff0c\u8ba9\u667a\u80fd\u4f53\u4e0d\u4ec5\u5171\u4eab\u51b3\u7b56\uff0c\u8fd8\u5171\u4eab\u8f7b\u91cf\u7ea7\u7684\u654f\u611f\u6027\u4fe1\u53f7\uff0c\u4ece\u800c\u5728\u7fa4\u4f53\u4e2d\u5b9e\u73b0\u66f4\u5feb\u66f4\u7a33\u5b9a\u7684\u534f\u8c03\u3002", "motivation": "\u73b0\u6709AI\u667a\u80fd\u4f53\u901a\u4fe1\u534f\u8bae\uff08\u5982A2A\u548cACP\uff09\u5f3a\u8c03\u901a\u4fe1\u800c\u975e\u534f\u8c03\uff0c\u968f\u7740\u667a\u80fd\u4f53\u7fa4\u4f53\u89c4\u6a21\u589e\u957f\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u8106\u5f31\u7684\u96c6\u4f53\u884c\u4e3a\uff0c\u5373\u4f7f\u4e2a\u4f53\u667a\u80fd\u4f53\u5f88\u806a\u660e\uff0c\u7fa4\u4f53\u7ed3\u679c\u4e5f\u5f88\u5dee\u3002", "method": "REP\u534f\u8bae\u8ba9\u667a\u80fd\u4f53\u5171\u4eab\u51b3\u7b56\u548c\u654f\u611f\u6027\u4fe1\u53f7\uff08\u8868\u8fbe\u5173\u952e\u73af\u5883\u53d8\u91cf\u53d8\u5316\u65f6\u5176\u9009\u62e9\u4f1a\u5982\u4f55\u6539\u53d8\uff09\uff0c\u8fd9\u4e9b\u654f\u611f\u6027\u5728\u5c40\u90e8\u7f51\u7edc\u4e2d\u4f20\u64ad\uff0c\u4f7f\u7fa4\u4f53\u6bd4\u4ec5\u4f7f\u7528\u667a\u80fd\u4f53\u4e2d\u5fc3\u901a\u4fe1\u65f6\u66f4\u5feb\u66f4\u7a33\u5b9a\u5730\u5bf9\u9f50\u3002", "result": "\u5728\u4e09\u4e2a\u9886\u57df\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff1a\uff08i\uff09\u4f9b\u5e94\u94fe\u7ea7\u8054\uff08\u5564\u9152\u6e38\u620f\uff09\u3001\uff08ii\uff09\u7a00\u758f\u7f51\u7edc\u4e2d\u7684\u504f\u597d\u805a\u5408\uff08\u7535\u5f71\u8c03\u5ea6\uff09\u3001\uff08iii\uff09\u53ef\u6301\u7eed\u8d44\u6e90\u5206\u914d\uff08Fishbanks\uff09\uff0cREP\u6bd4A2A\u5728\u534f\u8c03\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u63d0\u9ad8\u4e8641%\u5230100%\u3002", "conclusion": "\u901a\u8fc7\u5c06\u534f\u8c03\u4f5c\u4e3a\u534f\u8bae\u7ea7\u80fd\u529b\uff0cREP\u4e3a\u65b0\u5174\u7684\u667a\u80fd\u4f53\u4e92\u8054\u7f51\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2510.16604", "categories": ["cs.CL", "68T50", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.16604", "abs": "https://arxiv.org/abs/2510.16604", "authors": ["Francisco Jose Cortes Delgado", "Eduardo Martinez Gracia", "Rafael Valencia Garcia"], "title": "Fine-tuning of Large Language Models for Constituency Parsing Using a Sequence to Sequence Approach", "comment": "6 pages, 3 figures. Submitted to SEPLN 2023 Conference", "summary": "Recent advances in natural language processing with large neural models have\nopened new possibilities for syntactic analysis based on machine learning. This\nwork explores a novel approach to phrase-structure analysis by fine-tuning\nlarge language models (LLMs) to translate an input sentence into its\ncorresponding syntactic structure. The main objective is to extend the\ncapabilities of MiSintaxis, a tool designed for teaching Spanish syntax.\nSeveral models from the Hugging Face repository were fine-tuned using training\ndata generated from the AnCora-ES corpus, and their performance was evaluated\nusing the F1 score. The results demonstrate high accuracy in phrase-structure\nanalysis and highlight the potential of this methodology.", "AI": {"tldr": "\u901a\u8fc7\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u53e5\u5b50\u7ffb\u8bd1\u4e3a\u53e5\u6cd5\u7ed3\u6784\uff0c\u6269\u5c55\u897f\u73ed\u7259\u8bed\u6cd5\u6559\u5b66\u5de5\u5177MiSintaxis\u7684\u80fd\u529b\uff0c\u5728\u77ed\u8bed\u7ed3\u6784\u5206\u6790\u4e2d\u53d6\u5f97\u9ad8\u51c6\u786e\u7387", "motivation": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u8fdb\u5c55\u63a2\u7d22\u53e5\u6cd5\u5206\u6790\u7684\u65b0\u65b9\u6cd5\uff0c\u6269\u5c55\u897f\u73ed\u7259\u8bed\u6cd5\u6559\u5b66\u5de5\u5177MiSintaxis\u7684\u529f\u80fd", "method": "\u4f7f\u7528Hugging Face\u5e93\u4e2d\u7684\u591a\u4e2a\u6a21\u578b\uff0c\u57fa\u4e8eAnCora-ES\u8bed\u6599\u5e93\u751f\u6210\u7684\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u5fae\u8c03\uff0c\u5c06\u8f93\u5165\u53e5\u5b50\u7ffb\u8bd1\u4e3a\u5bf9\u5e94\u7684\u53e5\u6cd5\u7ed3\u6784", "result": "\u4f7f\u7528F1\u5206\u6570\u8bc4\u4f30\u6027\u80fd\uff0c\u5728\u77ed\u8bed\u7ed3\u6784\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u7387", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5728\u53e5\u6cd5\u5206\u6790\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u8bed\u6cd5\u6559\u5b66\u5de5\u5177\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6280\u672f\u652f\u6491"}}
{"id": "2510.16582", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16582", "abs": "https://arxiv.org/abs/2510.16582", "authors": ["Junchi Yu", "Yujie Liu", "Jindong Gu", "Philip Torr", "Dongzhan Zhou"], "title": "Can Knowledge-Graph-based Retrieval Augmented Generation Really Retrieve What You Need?", "comment": "NeurIPS 2025 (Spotlight)", "summary": "Retrieval-Augmented Generation (RAG) based on knowledge graphs (KGs) enhances\nlarge language models (LLMs) by providing structured and interpretable external\nknowledge. However, existing KG-based RAG methods struggle to retrieve accurate\nand diverse information from text-rich KGs for complex real-world queries.\nProcess Reward Models (PRMs) offer a way to align the retrieval process of\nKG-based RAG with query-specific knowledge requirements, but they heavily rely\non process-level supervision signals that are expensive and hard to obtain on\nKGs. To address this challenge, we propose GraphFlow, a framework that\nefficiently retrieves accurate and diverse knowledge required for real-world\nqueries from text-rich KGs. GraphFlow employs a transition-based flow matching\nobjective to jointly optimize a retrieval policy and a flow estimator. The flow\nestimator factorizes the reward of the retrieval outcome into the intermediate\nretrieval states. Such reward factorization guides the retrieval policy to\nretrieve candidates from KGs in proportion to their reward. This allows\nGraphFlow to explore high-quality regions of KGs that yield diverse and\nrelevant results. We evaluate GraphFlow on the STaRK benchmark, which includes\nreal-world queries from multiple domains over text-rich KGs. GraphFlow\noutperforms strong KG-RAG baselines, including GPT-4o, by 10% on average in hit\nrate and recall. It also shows strong generalization to unseen KGs,\ndemonstrating its effectiveness and robustness.", "AI": {"tldr": "GraphFlow\u662f\u4e00\u4e2a\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u8fc7\u6e21\u6d41\u5339\u914d\u76ee\u6807\u4f18\u5316\u68c0\u7d22\u7b56\u7565\u548c\u6d41\u4f30\u8ba1\u5668\uff0c\u6709\u6548\u68c0\u7d22\u590d\u6742\u67e5\u8be2\u6240\u9700\u7684\u51c6\u786e\u591a\u6837\u5316\u77e5\u8bc6", "motivation": "\u73b0\u6709\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684RAG\u65b9\u6cd5\u96be\u4ee5\u4ece\u6587\u672c\u4e30\u5bcc\u7684\u77e5\u8bc6\u56fe\u8c31\u4e2d\u68c0\u7d22\u51c6\u786e\u591a\u6837\u7684\u4fe1\u606f\uff0c\u800c\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u9700\u8981\u6602\u8d35\u7684\u8fc7\u7a0b\u7ea7\u76d1\u7763\u4fe1\u53f7", "method": "\u91c7\u7528\u8fc7\u6e21\u6d41\u5339\u914d\u76ee\u6807\u8054\u5408\u4f18\u5316\u68c0\u7d22\u7b56\u7565\u548c\u6d41\u4f30\u8ba1\u5668\uff0c\u6d41\u4f30\u8ba1\u5668\u5c06\u68c0\u7d22\u7ed3\u679c\u5956\u52b1\u5206\u89e3\u5230\u4e2d\u95f4\u68c0\u7d22\u72b6\u6001\uff0c\u5f15\u5bfc\u68c0\u7d22\u7b56\u7565\u6309\u5956\u52b1\u6bd4\u4f8b\u4ece\u77e5\u8bc6\u56fe\u8c31\u4e2d\u68c0\u7d22\u5019\u9009", "result": "\u5728STaRK\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGraphFlow\u5728\u547d\u4e2d\u7387\u548c\u53ec\u56de\u7387\u4e0a\u5e73\u5747\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff08\u5305\u62ecGPT-4o\uff0910%\uff0c\u5e76\u5bf9\u672a\u89c1\u77e5\u8bc6\u56fe\u8c31\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b", "conclusion": "GraphFlow\u80fd\u6709\u6548\u68c0\u7d22\u590d\u6742\u771f\u5b9e\u4e16\u754c\u67e5\u8be2\u6240\u9700\u7684\u51c6\u786e\u591a\u6837\u5316\u77e5\u8bc6\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027"}}
{"id": "2510.16645", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.16645", "abs": "https://arxiv.org/abs/2510.16645", "authors": ["Zhixuan He", "Yue Feng"], "title": "Unleashing Diverse Thinking Modes in LLMs through Multi-Agent Collaboration", "comment": null, "summary": "Large Language Models (LLMs) demonstrate strong performance but often lack\ninterpretable reasoning. This paper introduces the Multi-Agent Collaboration\nFramework for Diverse Thinking Modes (DiMo), which enhances both performance\nand interpretability by simulating a structured debate among four specialized\nLLM agents. Each agent embodies a distinct reasoning paradigm, allowing the\nframework to collaboratively explore diverse cognitive approaches. Through\niterative debate, agents challenge and refine initial responses, yielding more\nrobust conclusions and an explicit, auditable reasoning chain. Across six\nbenchmarks and under a unified open-source setup, DiMo improves accuracy over\nwidely used single-model and debate baselines, with the largest gains on math.\nWe position DiMo as a semantics-aware, Web-native multi-agent framework: it\nmodels human-machine intelligence with LLM agents that produce semantically\ntyped, URL-annotated evidence chains for explanations and user-friendly\ninteractions. Although our experiments use standard reasoning benchmarks, the\nframework is designed to be instantiated over Web corpora and knowledge graphs,\ncombining retrieval-augmented reasoning with structured justifications that\ndownstream systems can inspect and reuse.", "AI": {"tldr": "DiMo\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u56db\u4e2a\u4e13\u4e1aLLM\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u7ed3\u6784\u5316\u8fa9\u8bba\u6765\u63d0\u5347\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\uff0c\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u5355\u6a21\u578b\u548c\u8fa9\u8bba\u57fa\u7ebf\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u6027\u80fd\u5f3a\u5927\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u63d0\u5347\u6027\u80fd\u548c\u89e3\u91ca\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u56db\u4e2a\u4f53\u73b0\u4e0d\u540c\u63a8\u7406\u8303\u5f0f\u7684\u4e13\u4e1aLLM\u667a\u80fd\u4f53\u8fdb\u884c\u8fed\u4ee3\u8fa9\u8bba\uff0c\u901a\u8fc7\u76f8\u4e92\u6311\u6218\u548c\u7cbe\u70bc\u521d\u59cb\u54cd\u5e94\u6765\u83b7\u5f97\u66f4\u9c81\u68d2\u7684\u7ed3\u8bba\u3002", "result": "\u5728\u7edf\u4e00\u5f00\u6e90\u8bbe\u7f6e\u4e0b\uff0cDiMo\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u51c6\u786e\u7387\u8d85\u8fc7\u5e7f\u6cdb\u4f7f\u7528\u7684\u5355\u6a21\u578b\u548c\u8fa9\u8bba\u57fa\u7ebf\uff0c\u5c24\u5176\u5728\u6570\u5b66\u4efb\u52a1\u4e0a\u63d0\u5347\u6700\u5927\u3002", "conclusion": "DiMo\u4f5c\u4e3a\u4e00\u4e2a\u8bed\u4e49\u611f\u77e5\u3001Web\u539f\u751f\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u80fd\u591f\u751f\u6210\u8bed\u4e49\u7c7b\u578b\u5316\u3001URL\u6ce8\u91ca\u7684\u8bc1\u636e\u94fe\uff0c\u4e3a\u4e0b\u6e38\u7cfb\u7edf\u63d0\u4f9b\u53ef\u68c0\u67e5\u91cd\u7528\u7684\u7ed3\u6784\u5316\u8bc1\u660e\u3002"}}
{"id": "2510.16601", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16601", "abs": "https://arxiv.org/abs/2510.16601", "authors": ["Tianxing Wu", "Shutong Zhu", "Jingting Wang", "Ning Xu", "Guilin Qi", "Haofen Wang"], "title": "Uncertain Knowledge Graph Completion via Semi-Supervised Confidence Distribution Learning", "comment": "13 pages, accepted by NeurIPS 2025 (spotlight)", "summary": "Uncertain knowledge graphs (UKGs) associate each triple with a confidence\nscore to provide more precise knowledge representations. Recently, since\nreal-world UKGs suffer from the incompleteness, uncertain knowledge graph (UKG)\ncompletion attracts more attention, aiming to complete missing triples and\nconfidences. Current studies attempt to learn UKG embeddings to solve this\nproblem, but they neglect the extremely imbalanced distributions of triple\nconfidences. This causes that the learnt embeddings are insufficient to\nhigh-quality UKG completion. Thus, in this paper, to address the above issue,\nwe propose a new semi-supervised Confidence Distribution Learning (ssCDL)\nmethod for UKG completion, where each triple confidence is transformed into a\nconfidence distribution to introduce more supervision information of different\nconfidences to reinforce the embedding learning process. ssCDL iteratively\nlearns UKG embedding by relational learning on labeled data (i.e., existing\ntriples with confidences) and unlabeled data with pseudo labels (i.e., unseen\ntriples with the generated confidences), which are predicted by meta-learning\nto augment the training data and rebalance the distribution of triple\nconfidences. Experiments on two UKG datasets demonstrate that ssCDL\nconsistently outperforms state-of-the-art baselines in different evaluation\nmetrics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4e0d\u786e\u5b9a\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u7684\u534a\u76d1\u7763\u7f6e\u4fe1\u5206\u5e03\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u7f6e\u4fe1\u5ea6\u8f6c\u6362\u4e3a\u5206\u5e03\u5e76\u5229\u7528\u5143\u5b66\u4e60\u751f\u6210\u4f2a\u6807\u7b7e\u6765\u5e73\u8861\u7f6e\u4fe1\u5ea6\u5206\u5e03\u3002", "motivation": "\u73b0\u6709\u4e0d\u786e\u5b9a\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u65b9\u6cd5\u5ffd\u7565\u4e86\u7f6e\u4fe1\u5ea6\u7684\u6781\u7aef\u4e0d\u5e73\u8861\u5206\u5e03\uff0c\u5bfc\u81f4\u5b66\u4e60\u5230\u7684\u5d4c\u5165\u4e0d\u8db3\u4ee5\u652f\u6301\u9ad8\u8d28\u91cf\u7684\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u3002", "method": "\u63d0\u51fassCDL\u65b9\u6cd5\uff0c\u5c06\u7f6e\u4fe1\u5ea6\u8f6c\u6362\u4e3a\u7f6e\u4fe1\u5206\u5e03\uff0c\u901a\u8fc7\u5173\u7cfb\u5b66\u4e60\u5728\u6807\u8bb0\u6570\u636e\u548c\u5e26\u6709\u4f2a\u6807\u7b7e\u7684\u672a\u6807\u8bb0\u6570\u636e\u4e0a\u8fed\u4ee3\u5b66\u4e60\u5d4c\u5165\uff0c\u4f7f\u7528\u5143\u5b66\u4e60\u9884\u6d4b\u672a\u89c1\u4e09\u5143\u7ec4\u7684\u7f6e\u4fe1\u5ea6\u6765\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728\u4e24\u4e2aUKG\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cssCDL\u5728\u4e0d\u540c\u8bc4\u4f30\u6307\u6807\u4e0a\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ssCDL\u65b9\u6cd5\u901a\u8fc7\u7f6e\u4fe1\u5206\u5e03\u5b66\u4e60\u548c\u6570\u636e\u589e\u5f3a\u6709\u6548\u89e3\u51b3\u4e86\u7f6e\u4fe1\u5ea6\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u4e0d\u786e\u5b9a\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u7684\u6027\u80fd\u3002"}}
{"id": "2510.16670", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16670", "abs": "https://arxiv.org/abs/2510.16670", "authors": ["Yiyang Liu", "James C. Liang", "Heng Fan", "Wenhao Yang", "Yiming Cui", "Xiaotian Han", "Lifu Huang", "Dongfang Liu", "Qifan Wang", "Cheng Han"], "title": "All You Need is One: Capsule Prompt Tuning with a Single Vector", "comment": "NeurIPS 2025", "summary": "Prompt-based learning has emerged as a parameter-efficient finetuning (PEFT)\napproach to facilitate Large Language Model (LLM) adaptation to downstream\ntasks by conditioning generation with task-aware guidance. Despite its\nsuccesses, current prompt-based learning methods heavily rely on laborious grid\nsearching for optimal prompt length and typically require considerable number\nof prompts, introducing additional computational burden. Worse yet, our pioneer\nfindings indicate that the task-aware prompt design is inherently limited by\nits absence of instance-aware information, leading to a subtle attention\ninterplay with the input sequence. In contrast, simply incorporating\ninstance-aware information as a part of the guidance can enhance the\nprompt-tuned model performance without additional fine-tuning. Moreover, we\nfind an interesting phenomenon, namely \"attention anchor\", that incorporating\ninstance-aware tokens at the earliest position of the sequence can successfully\npreserve strong attention to critical structural information and exhibit more\nactive attention interaction with all input tokens. In light of our\nobservation, we introduce Capsule Prompt-Tuning (CaPT), an efficient and\neffective solution that leverages off-the-shelf, informative instance semantics\ninto prompt-based learning. Our approach innovatively integrates both\ninstance-aware and task-aware information in a nearly parameter-free manner\n(i.e., one single capsule prompt). Empirical results demonstrate that our\nmethod can exhibit superior performance across various language tasks (e.g.,\n84.03\\% average accuracy on T5-Large), serving as an \"attention anchor,\" while\nenjoying high parameter efficiency (e.g., 0.003\\% of model parameters on\nLlama3.2-1B).", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.16614", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16614", "abs": "https://arxiv.org/abs/2510.16614", "authors": ["Xuan Zhang", "Ruixiao Li", "Zhijian Zhou", "Long Li", "Yulei Qin", "Ke Li", "Xing Sun", "Xiaoyu Tan", "Chao Qu", "Yuan Qi"], "title": "Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards", "comment": null, "summary": "Reinforcement Learning (RL) has become a compelling way to strengthen the\nmulti step reasoning ability of Large Language Models (LLMs). However,\nprevalent RL paradigms still lean on sparse outcome-based rewards and limited\nexploration, which often drives LLMs toward repetitive and suboptimal reasoning\npatterns. In this paper, we study the central question of how to design\nexploration for LLM reasoning and introduce MERCI (Motivating Exploration in\nLLM Reasoning with Count-based Intrinsic Rewards), a novel RL algorithm that\naugments policy optimization with a principled intrinsic reward. Building on\nthe idea of count-based exploration, MERCI leverages a lightweight Coin\nFlipping Network (CFN) to estimate the pseudo count and further epistemic\nuncertainty over reasoning trajectories, and converts them into an intrinsic\nreward that values novelty while preserving the learning signal from task\nrewards. We integrate MERCI into some advanced RL frameworks like Group\nRelative Policy Optimization (GRPO). Experiments on complex reasoning\nbenchmarks demonstrate that MERCI encourages richer and more varied chains of\nthought, significantly improves performance over strong baselines, and helps\nthe policy escape local routines to discover better solutions. It indicates\nthat our targeted intrinsic motivation can make exploration reliable for\nlanguage model reasoning.", "AI": {"tldr": "MERCI\u662f\u4e00\u79cd\u589e\u5f3aLLM\u63a8\u7406\u80fd\u529b\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u8ba1\u6570\u7684\u5185\u5728\u5956\u52b1\u6765\u6fc0\u52b1\u63a2\u7d22\uff0c\u907f\u514d\u91cd\u590d\u63a8\u7406\u6a21\u5f0f\u3002", "motivation": "\u73b0\u6709RL\u65b9\u6cd5\u4f9d\u8d56\u7a00\u758f\u7ed3\u679c\u5956\u52b1\u548c\u6709\u9650\u63a2\u7d22\uff0c\u5bfc\u81f4LLMs\u9677\u5165\u91cd\u590d\u548c\u6b21\u4f18\u63a8\u7406\u6a21\u5f0f\uff0c\u9700\u8981\u8bbe\u8ba1\u66f4\u597d\u7684\u63a2\u7d22\u673a\u5236\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7Coin Flipping Network\u4f30\u8ba1\u63a8\u7406\u8f68\u8ff9\u7684\u4f2a\u8ba1\u6570\u548c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u5c06\u5176\u8f6c\u6362\u4e3a\u5185\u5728\u5956\u52b1\uff0c\u5e76\u4e0eGRPO\u7b49RL\u6846\u67b6\u96c6\u6210\u3002", "result": "\u5728\u590d\u6742\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMERCI\u9f13\u52b1\u66f4\u4e30\u5bcc\u591a\u6837\u7684\u601d\u7ef4\u94fe\uff0c\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u6027\u80fd\uff0c\u5e2e\u52a9\u7b56\u7565\u9003\u79bb\u5c40\u90e8\u6700\u4f18\u3002", "conclusion": "\u9488\u5bf9\u6027\u7684\u5185\u5728\u52a8\u673a\u53ef\u4ee5\u4f7f\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u63a2\u7d22\u66f4\u52a0\u53ef\u9760\u6709\u6548\u3002"}}
{"id": "2510.16685", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16685", "abs": "https://arxiv.org/abs/2510.16685", "authors": ["Damin Zhang", "Julia Rayz"], "title": "Temporal Understanding under Deictic Frame of Reference", "comment": "Under review", "summary": "Understanding time is fundamental to human cognition, where temporal\nexperience is often conceptualized through spatial metaphors grounded in\nsensory-motor experience. For example, \"summer is approaching\" parallels \"We\nare approaching the summer\". In such expressions, humans rely on a frame of\nreference (FoR) to interpret meaning relative to a particular viewpoint.\nExtending this concept to time, a temporal frame of reference (t-FoR) defines\nhow temporal relations are perceived relative to an experiencer's moment of\n\"now\". While Large Language Models (LLMs) have shown remarkable advances in\nnatural language understanding, their ability to interpret and reason about\ntime remains limited. In this work, we introduce TUuD (Temporal Understanding\nunder Deictic t-FoR), a framework that evaluates how LLMs interpret time-event\nand event-event relations when the reference point of \"now\" dynamically shifts\nalong a timeline. Following recent work on temporal cognition\n\\cite{li2025other}, LLMs are prompted to rate the similarity between the\ncurrent moment and a target event from 0.00 (completely dissimilar) to 1.00\n(highly similar), where similarity quantifies perceived temporal alignment\nbetween the two points. Our results show that four evaluated LLMs exhibit\nmeasurable adaptation to a deictic t-FoR, with similarity ratings peaking\naround the present and decreasing toward past and future events. The\nadaptation, however, weakens beyond near-term contexts, suggesting that while\nLLMs display partial human-like temporal cognition, their temporal reasoning\nremains sensitive to reference-frame shifts and temporal distance.", "AI": {"tldr": "\u63d0\u51fa\u4e86TUuD\u6846\u67b6\u6765\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u53c2\u8003\u6846\u67b6\u52a8\u6001\u53d8\u5316\u65f6\u7684\u65f6\u95f4\u7406\u89e3\u80fd\u529b\uff0c\u53d1\u73b0LLMs\u5728\u8fd1\u671f\u65f6\u95f4\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u8ba4\u77e5\u6a21\u5f0f\uff0c\u4f46\u5728\u8fdc\u8ddd\u79bb\u65f6\u95f4\u63a8\u7406\u4e0a\u5b58\u5728\u5c40\u9650\u3002", "motivation": "\u4eba\u7c7b\u901a\u8fc7\u7a7a\u95f4\u9690\u55bb\u6765\u7406\u89e3\u65f6\u95f4\uff0c\u4f7f\u7528\u65f6\u95f4\u53c2\u8003\u6846\u67b6(t-FoR)\u6765\u611f\u77e5\u65f6\u95f4\u5173\u7cfb\u3002\u867d\u7136LLMs\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u65f6\u95f4\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u4ecd\u7136\u6709\u9650\uff0c\u9700\u8981\u8bc4\u4f30LLMs\u5982\u4f55\u89e3\u91ca\u52a8\u6001\u53d8\u5316\u7684\u65f6\u95f4\u53c2\u8003\u6846\u67b6\u3002", "method": "\u5f15\u5165TUuD\u6846\u67b6\uff0c\u8ba9LLMs\u5728\u65f6\u95f4\u53c2\u8003\u70b9\"\u73b0\u5728\"\u6cbf\u65f6\u95f4\u7ebf\u52a8\u6001\u79fb\u52a8\u7684\u60c5\u51b5\u4e0b\uff0c\u8bc4\u4f30\u65f6\u95f4-\u4e8b\u4ef6\u548c\u4e8b\u4ef6-\u4e8b\u4ef6\u5173\u7cfb\u3002\u57fa\u4e8e\u65f6\u95f4\u8ba4\u77e5\u7814\u7a76\uff0c\u8ba9LLMs\u5bf9\u5f53\u524d\u65f6\u523b\u4e0e\u76ee\u6807\u4e8b\u4ef6\u7684\u76f8\u4f3c\u5ea6\u8fdb\u884c\u8bc4\u5206(0.00-1.00)\u3002", "result": "\u56db\u4e2a\u8bc4\u4f30\u7684LLMs\u8868\u73b0\u51fa\u5bf9\u6307\u793a\u6027t-FoR\u7684\u53ef\u6d4b\u91cf\u9002\u5e94\uff0c\u76f8\u4f3c\u5ea6\u8bc4\u5206\u5728\u5f53\u524d\u65f6\u523b\u9644\u8fd1\u8fbe\u5230\u5cf0\u503c\uff0c\u5e76\u5411\u8fc7\u53bb\u548c\u672a\u6765\u4e8b\u4ef6\u9012\u51cf\u3002\u4f46\u8fd9\u79cd\u9002\u5e94\u5728\u8d85\u51fa\u8fd1\u671f\u8bed\u5883\u65f6\u4f1a\u51cf\u5f31\u3002", "conclusion": "LLMs\u663e\u793a\u51fa\u90e8\u5206\u7c7b\u4f3c\u4eba\u7c7b\u7684\u65f6\u95f4\u8ba4\u77e5\uff0c\u4f46\u5176\u65f6\u95f4\u63a8\u7406\u5bf9\u53c2\u8003\u6846\u67b6\u53d8\u5316\u548c\u65f6\u95f4\u8ddd\u79bb\u4ecd\u7136\u654f\u611f\uff0c\u5728\u8fdc\u8ddd\u79bb\u65f6\u95f4\u7406\u89e3\u4e0a\u5b58\u5728\u5c40\u9650\u6027\u3002"}}
{"id": "2510.16658", "categories": ["cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.16658", "abs": "https://arxiv.org/abs/2510.16658", "authors": ["Shihao Yang", "Xiying Huang", "Danilo Bernardo", "Jun-En Ding", "Andrew Michael", "Jingmei Yang", "Patrick Kwan", "Ashish Raj", "Feng Liu"], "title": "Foundation and Large-Scale AI Models in Neuroscience: A Comprehensive Review", "comment": null, "summary": "The advent of large-scale artificial intelligence (AI) models has a\ntransformative effect on neuroscience research, which represents a paradigm\nshift from the traditional computational methods through the facilitation of\nend-to-end learning from raw brain signals and neural data. In this paper, we\nexplore the transformative effects of large-scale AI models on five major\nneuroscience domains: neuroimaging and data processing, brain-computer\ninterfaces and neural decoding, molecular neuroscience and genomic modeling,\nclinical assistance and translational frameworks, and disease-specific\napplications across neurological and psychiatric disorders. These models are\ndemonstrated to address major computational neuroscience challenges, including\nmultimodal neural data integration, spatiotemporal pattern interpretation, and\nthe derivation of translational frameworks for clinical deployment. Moreover,\nthe interaction between neuroscience and AI has become increasingly reciprocal,\nas biologically informed architectural constraints are now incorporated to\ndevelop more interpretable and computationally efficient models. This review\nhighlights both the notable promise of such technologies and key implementation\nconsiderations, with particular emphasis on rigorous evaluation frameworks,\neffective domain knowledge integration, and comprehensive ethical guidelines\nfor clinical use. Finally, a systematic listing of critical neuroscience\ndatasets used to derive and validate large-scale AI models across diverse\nresearch applications is provided.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u5927\u89c4\u6a21AI\u6a21\u578b\u5bf9\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u7684\u53d8\u9769\u6027\u5f71\u54cd\uff0c\u6db5\u76d6\u795e\u7ecf\u5f71\u50cf\u5904\u7406\u3001\u8111\u673a\u63a5\u53e3\u3001\u5206\u5b50\u795e\u7ecf\u79d1\u5b66\u3001\u4e34\u5e8a\u8f85\u52a9\u548c\u75be\u75c5\u5e94\u7528\u4e94\u5927\u9886\u57df\uff0c\u5f3a\u8c03\u591a\u6a21\u6001\u6570\u636e\u6574\u5408\u548c\u4e34\u5e8a\u8f6c\u5316\u6846\u67b6\u3002", "motivation": "\u5927\u89c4\u6a21AI\u6a21\u578b\u7684\u51fa\u73b0\u4e3a\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u5e26\u6765\u8303\u5f0f\u8f6c\u53d8\uff0c\u80fd\u591f\u4ece\u539f\u59cb\u8111\u4fe1\u53f7\u548c\u795e\u7ecf\u6570\u636e\u4e2d\u8fdb\u884c\u7aef\u5230\u7aef\u5b66\u4e60\uff0c\u89e3\u51b3\u4f20\u7edf\u8ba1\u7b97\u65b9\u6cd5\u9762\u4e34\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5206\u6790\u5927\u89c4\u6a21AI\u6a21\u578b\u5728\u4e94\u4e2a\u4e3b\u8981\u795e\u7ecf\u79d1\u5b66\u9886\u57df\u7684\u5e94\u7528\uff1a\u795e\u7ecf\u5f71\u50cf\u6570\u636e\u5904\u7406\u3001\u8111\u673a\u63a5\u53e3\u4e0e\u795e\u7ecf\u89e3\u7801\u3001\u5206\u5b50\u795e\u7ecf\u79d1\u5b66\u4e0e\u57fa\u56e0\u7ec4\u5efa\u6a21\u3001\u4e34\u5e8a\u8f85\u52a9\u4e0e\u8f6c\u5316\u6846\u67b6\u3001\u795e\u7ecf\u7cfb\u7edf\u548c\u7cbe\u795e\u75be\u75c5\u7684\u7279\u5b9a\u5e94\u7528\u3002", "result": "\u8fd9\u4e9b\u6a21\u578b\u88ab\u8bc1\u660e\u80fd\u591f\u6709\u6548\u89e3\u51b3\u591a\u6a21\u6001\u795e\u7ecf\u6570\u636e\u6574\u5408\u3001\u65f6\u7a7a\u6a21\u5f0f\u89e3\u91ca\u548c\u4e34\u5e8a\u8f6c\u5316\u6846\u67b6\u7b49\u4e3b\u8981\u8ba1\u7b97\u795e\u7ecf\u79d1\u5b66\u6311\u6218\uff0c\u540c\u65f6\u795e\u7ecf\u79d1\u5b66\u4e0eAI\u7684\u4e92\u52a8\u65e5\u76ca\u76f8\u4e92\u4fc3\u8fdb\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u5f3a\u8c03\u8fd9\u4e9b\u6280\u672f\u7684\u663e\u8457\u524d\u666f\u548c\u5173\u952e\u5b9e\u65bd\u8003\u8651\uff0c\u7279\u522b\u5f3a\u8c03\u4e25\u683c\u7684\u8bc4\u4f30\u6846\u67b6\u3001\u6709\u6548\u7684\u9886\u57df\u77e5\u8bc6\u6574\u5408\u4ee5\u53ca\u4e34\u5e8a\u4f7f\u7528\u7684\u5168\u9762\u4f26\u7406\u6307\u5357\uff0c\u5e76\u63d0\u4f9b\u4e86\u7528\u4e8e\u9a8c\u8bc1\u5927\u89c4\u6a21AI\u6a21\u578b\u7684\u5173\u952e\u795e\u7ecf\u79d1\u5b66\u6570\u636e\u96c6\u6e05\u5355\u3002"}}
{"id": "2510.16686", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16686", "abs": "https://arxiv.org/abs/2510.16686", "authors": ["Wenhang Shi", "Shuqing Bian", "Yiren Chen", "Xinyi Zhang", "Zhe Zhao", "Pengfei Hu", "Wei Lu", "Xiaoyong Du"], "title": "Investigating the Impact of Rationales for LLMs on Natural Language Understanding", "comment": null, "summary": "Chain-of-thought (CoT) rationales, which provide step-by-step reasoning to\nderive final answers, benefit LLMs in both inference and training.\nIncorporating rationales, either by generating them before answering during\ninference, or by placing them before or after the original answers during\ntraining - significantly improves model performance on mathematical, symbolic\nand commonsense reasoning tasks. However, most work focuses on the role of\nrationales in these reasoning tasks, overlooking their potential impact on\nother important tasks like natural language understanding (NLU) tasks. In this\nwork, we raise the question: Can rationales similarly benefit NLU tasks? To\nconduct a systematic exploration, we construct NLURC, a comprehensive and\nhigh-quality NLU dataset collection with rationales, and develop various\nrationale-augmented methods. Through exploring the applicability of these\nmethods on NLU tasks using the dataset, we uncover several potentially\nsurprising findings: (1) CoT inference shifts from hindering NLU performance to\nsurpassing direct label prediction as model size grows, indicating a positive\ncorrelation. (2) Most rationale-augmented training methods perform worse than\nlabel-only training, with one specially designed method consistently achieving\nimprovements. (3) LLMs trained with rationales achieve significant performance\ngains on unseen NLU tasks, rivaling models ten times their size, while\ndelivering interpretability on par with commercial LLMs.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\uff08NLU\uff09\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u968f\u7740\u6a21\u578b\u89c4\u6a21\u589e\u5927\uff0cCoT\u63a8\u7406\u4ece\u963b\u788d\u53d8\u4e3a\u63d0\u5347NLU\u6027\u80fd\uff0c\u4e14\u7279\u5b9a\u8bbe\u8ba1\u7684\u8bad\u7ec3\u65b9\u6cd5\u80fd\u5e26\u6765\u6539\u8fdb\uff0c\u8bad\u7ec3\u540e\u7684\u6a21\u578b\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5927\u591a\u6570\u7814\u7a76\u5173\u6ce8\u601d\u7ef4\u94fe\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u4f5c\u7528\uff0c\u5ffd\u89c6\u4e86\u5176\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u63a2\u7d22\u601d\u7ef4\u94fe\u662f\u5426\u540c\u6837\u80fd\u6709\u76ca\u4e8eNLU\u4efb\u52a1\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b\u601d\u7ef4\u94fe\u7684\u5168\u9762\u9ad8\u8d28\u91cfNLU\u6570\u636e\u96c6NLURC\uff0c\u5f00\u53d1\u4e86\u591a\u79cd\u601d\u7ef4\u94fe\u589e\u5f3a\u65b9\u6cd5\uff0c\u5e76\u5728NLU\u4efb\u52a1\u4e0a\u6d4b\u8bd5\u8fd9\u4e9b\u65b9\u6cd5\u7684\u9002\u7528\u6027\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u89c4\u6a21\u4e0eCoT\u63a8\u7406\u6548\u679c\u5448\u6b63\u76f8\u5173\uff1b\u5927\u591a\u6570\u601d\u7ef4\u94fe\u589e\u5f3a\u8bad\u7ec3\u65b9\u6cd5\u6548\u679c\u4e0d\u5982\u4ec5\u6807\u7b7e\u8bad\u7ec3\uff0c\u4f46\u6709\u4e00\u79cd\u7279\u6b8a\u8bbe\u8ba1\u7684\u65b9\u6cd5\u80fd\u6301\u7eed\u6539\u8fdb\uff1b\u4f7f\u7528\u601d\u7ef4\u94fe\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u672a\u89c1NLU\u4efb\u52a1\u4e0a\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0c\u6027\u80fd\u53ef\u4e0e\u5341\u500d\u89c4\u6a21\u6a21\u578b\u76f8\u5ab2\u7f8e\uff0c\u4e14\u89e3\u91ca\u6027\u4e0e\u5546\u4e1aLLMs\u76f8\u5f53\u3002", "conclusion": "\u601d\u7ef4\u94fe\u5728NLU\u4efb\u52a1\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u7279\u522b\u662f\u968f\u7740\u6a21\u578b\u89c4\u6a21\u589e\u5927\uff0c\u5176\u6548\u679c\u4ece\u8d1f\u9762\u8f6c\u4e3a\u6b63\u9762\uff0c\u4e14\u901a\u8fc7\u9002\u5f53\u65b9\u6cd5\u8bad\u7ec3\u80fd\u5e26\u6765\u663e\u8457\u6027\u80fd\u63d0\u5347\u548c\u53ef\u89e3\u91ca\u6027\u4f18\u52bf\u3002"}}
{"id": "2510.16701", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16701", "abs": "https://arxiv.org/abs/2510.16701", "authors": ["Ni Zhang", "Zhiguang Cao", "Jianan Zhou", "Cong Zhang", "Yew-Soon Ong"], "title": "An Agentic Framework with LLMs for Solving Complex Vehicle Routing Problems", "comment": null, "summary": "Complex vehicle routing problems (VRPs) remain a fundamental challenge,\ndemanding substantial expert effort for intent interpretation and algorithm\ndesign. While large language models (LLMs) offer a promising path toward\nautomation, current approaches still rely on external intervention, which\nrestrict autonomy and often lead to execution errors and low solution\nfeasibility. To address these challenges, we propose an Agentic Framework with\nLLMs (AFL) for solving complex vehicle routing problems, achieving full\nautomation from problem instance to solution. AFL directly extracts knowledge\nfrom raw inputs and enables self-contained code generation without handcrafted\nmodules or external solvers. To improve trustworthiness, AFL decomposes the\noverall pipeline into three manageable subtasks and employs four specialized\nagents whose coordinated interactions enforce cross-functional consistency and\nlogical soundness. Extensive experiments on 60 complex VRPs, ranging from\nstandard benchmarks to practical variants, validate the effectiveness and\ngenerality of our framework, showing comparable performance against\nmeticulously designed algorithms. Notably, it substantially outperforms\nexisting LLM-based baselines in both code reliability and solution feasibility,\nachieving rates close to 100% on the evaluated benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u6846\u67b6AFL\uff0c\u7528\u4e8e\u5b8c\u5168\u81ea\u52a8\u5316\u89e3\u51b3\u590d\u6742\u8f66\u8f86\u8def\u5f84\u95ee\u9898\uff0c\u4ece\u95ee\u9898\u5b9e\u4f8b\u76f4\u63a5\u751f\u6210\u89e3\u51b3\u65b9\u6848\u4ee3\u7801\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u6216\u5916\u90e8\u6c42\u89e3\u5668\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u4ecd\u4f9d\u8d56\u5916\u90e8\u5e72\u9884\uff0c\u9650\u5236\u4e86\u81ea\u4e3b\u6027\u5e76\u5bfc\u81f4\u6267\u884c\u9519\u8bef\u548c\u89e3\u51b3\u65b9\u6848\u53ef\u884c\u6027\u4f4e\u7684\u95ee\u9898\u3002\u9700\u8981\u5b9e\u73b0\u4ece\u95ee\u9898\u5b9e\u4f8b\u5230\u89e3\u51b3\u65b9\u6848\u7684\u5b8c\u5168\u81ea\u52a8\u5316\u3002", "method": "AFL\u6846\u67b6\u5c06\u6574\u4f53\u6d41\u7a0b\u5206\u89e3\u4e3a\u4e09\u4e2a\u53ef\u7ba1\u7406\u7684\u5b50\u4efb\u52a1\uff0c\u91c7\u7528\u56db\u4e2a\u4e13\u95e8\u4ee3\u7406\u901a\u8fc7\u534f\u8c03\u4ea4\u4e92\u6765\u786e\u4fdd\u8de8\u529f\u80fd\u4e00\u81f4\u6027\u548c\u903b\u8f91\u5408\u7406\u6027\uff0c\u76f4\u63a5\u4ece\u539f\u59cb\u8f93\u5165\u4e2d\u63d0\u53d6\u77e5\u8bc6\u5e76\u751f\u6210\u81ea\u5305\u542b\u4ee3\u7801\u3002", "result": "\u572860\u4e2a\u590d\u6742VRP\u95ee\u9898\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\uff0c\u4e0e\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u7b97\u6cd5\u6027\u80fd\u76f8\u5f53\uff0c\u5728\u4ee3\u7801\u53ef\u9760\u6027\u548c\u89e3\u51b3\u65b9\u6848\u53ef\u884c\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709LLM\u57fa\u7ebf\uff0c\u5728\u8bc4\u4f30\u57fa\u51c6\u4e0a\u63a5\u8fd1100%\u7684\u6210\u529f\u7387\u3002", "conclusion": "AFL\u6846\u67b6\u5b9e\u73b0\u4e86\u590d\u6742\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u6c42\u89e3\u7684\u5b8c\u5168\u81ea\u52a8\u5316\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u4e86\u4ee3\u7801\u53ef\u9760\u6027\u548c\u89e3\u51b3\u65b9\u6848\u53ef\u884c\u6027\uff0c\u4e3aLLM\u5728\u590d\u6742\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.16708", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16708", "abs": "https://arxiv.org/abs/2510.16708", "authors": ["Kailai Yang", "Yan Leng", "Xin Zhang", "Tianlin Zhang", "Paul Thompson", "Bernard Keavney", "Maciej Tomaszewski", "Sophia Ananiadou"], "title": "Natural Language Processing Applications in Cardiology: A Narrative Review", "comment": null, "summary": "Cardiovascular disease has become increasingly prevalent in modern society\nand has a significant effect on global health and well-being. Heart-related\nconditions are intricate, multifaceted disorders, which may be influenced by a\ncombination of genetic predispositions, lifestyle choices, and various\nsocioeconomic and clinical factors. Information regarding these potentially\ncomplex interrelationships is dispersed among diverse types of textual data,\nwhich include patient narratives, medical records, and scientific literature,\namong others. Natural language processing (NLP) techniques have increasingly\nbeen adopted as a powerful means to analyse and make sense of this vast amount\nof unstructured data. This, in turn, can allow healthcare professionals to gain\ndeeper insights into the cardiology field, which has the potential to\nrevolutionize current approaches to the diagnosis, treatment, and prevention of\ncardiac problems. This review provides a detailed overview of NLP research in\ncardiology between 2014 and 2025. We queried six literature databases to find\narticles describing the application of NLP techniques in the context of a range\nof different cardiovascular diseases. Following a rigorous screening process,\nwe identified a total of 265 relevant articles. We analysed each article from\nmultiple dimensions, i.e., NLP paradigm types, cardiology-related task types,\ncardiovascular disease types, and data source types. Our analysis reveals\nconsiderable diversity within each of these dimensions, thus demonstrating the\nconsiderable breadth of NLP research within the field. We also perform a\ntemporal analysis, which illustrates the evolution and changing trends in NLP\nmethods employed over the last decade that we cover. To our knowledge, the\nreview constitutes the most comprehensive overview of NLP research in\ncardiology to date.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u5bf92014-2025\u5e74\u95f4\u5fc3\u810f\u75c5\u5b66\u9886\u57df\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\u8fdb\u884c\u4e86\u5168\u9762\u56de\u987e\uff0c\u5206\u6790\u4e86265\u7bc7\u76f8\u5173\u6587\u732e\uff0c\u6db5\u76d6NLP\u8303\u5f0f\u7c7b\u578b\u3001\u5fc3\u810f\u75c5\u76f8\u5173\u4efb\u52a1\u7c7b\u578b\u3001\u5fc3\u8840\u7ba1\u75be\u75c5\u7c7b\u578b\u548c\u6570\u636e\u6e90\u7c7b\u578b\u7b49\u591a\u4e2a\u7ef4\u5ea6\u3002", "motivation": "\u5fc3\u8840\u7ba1\u75be\u75c5\u65e5\u76ca\u666e\u904d\u4e14\u590d\u6742\uff0c\u76f8\u5173\u4fe1\u606f\u5206\u6563\u5728\u60a3\u8005\u53d9\u8ff0\u3001\u533b\u7597\u8bb0\u5f55\u548c\u79d1\u5b66\u6587\u732e\u7b49\u975e\u7ed3\u6784\u5316\u6587\u672c\u6570\u636e\u4e2d\u3002NLP\u6280\u672f\u80fd\u591f\u5206\u6790\u8fd9\u4e9b\u6570\u636e\uff0c\u4e3a\u5fc3\u810f\u75c5\u5b66\u7684\u8bca\u65ad\u3001\u6cbb\u7597\u548c\u9884\u9632\u63d0\u4f9b\u65b0\u89c1\u89e3\u3002", "method": "\u67e5\u8be2\u4e86\u516d\u4e2a\u6587\u732e\u6570\u636e\u5e93\uff0c\u901a\u8fc7\u4e25\u683c\u7b5b\u9009\u6d41\u7a0b\u8bc6\u522b\u51fa265\u7bc7\u76f8\u5173\u6587\u7ae0\uff0c\u4eceNLP\u8303\u5f0f\u7c7b\u578b\u3001\u5fc3\u810f\u75c5\u76f8\u5173\u4efb\u52a1\u7c7b\u578b\u3001\u5fc3\u8840\u7ba1\u75be\u75c5\u7c7b\u578b\u548c\u6570\u636e\u6e90\u7c7b\u578b\u7b49\u591a\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u5206\u6790\uff0c\u5e76\u8fdb\u884c\u65f6\u95f4\u8d8b\u52bf\u5206\u6790\u3002", "result": "\u5206\u6790\u663e\u793a\u6bcf\u4e2a\u7ef4\u5ea6\u90fd\u5b58\u5728\u76f8\u5f53\u5927\u7684\u591a\u6837\u6027\uff0c\u8bc1\u660e\u4e86NLP\u5728\u5fc3\u810f\u75c5\u5b66\u9886\u57df\u7814\u7a76\u7684\u5e7f\u5ea6\u3002\u65f6\u95f4\u5206\u6790\u63ed\u793a\u4e86\u8fd1\u5341\u5e74\u6765NLP\u65b9\u6cd5\u7684\u6f14\u53d8\u548c\u53d8\u5316\u8d8b\u52bf\u3002", "conclusion": "\u8fd9\u662f\u8fc4\u4eca\u4e3a\u6b62\u5bf9\u5fc3\u810f\u75c5\u5b66\u9886\u57dfNLP\u7814\u7a76\u6700\u5168\u9762\u7684\u7efc\u8ff0\uff0c\u5c55\u793a\u4e86NLP\u6280\u672f\u5728\u5fc3\u810f\u75c5\u5b66\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u548c\u53d1\u5c55\u8d8b\u52bf\u3002"}}
{"id": "2510.16720", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16720", "abs": "https://arxiv.org/abs/2510.16720", "authors": ["Jitao Sang", "Jinlin Xiao", "Jiarun Han", "Jilin Chen", "Xiaoyi Chen", "Shuyu Wei", "Yongjie Sun", "Yuhang Wang"], "title": "Beyond Pipelines: A Survey of the Paradigm Shift toward Model-Native Agentic AI", "comment": null, "summary": "The rapid evolution of agentic AI marks a new phase in artificial\nintelligence, where Large Language Models (LLMs) no longer merely respond but\nact, reason, and adapt. This survey traces the paradigm shift in building\nagentic AI: from Pipeline-based systems, where planning, tool use, and memory\nare orchestrated by external logic, to the emerging Model-native paradigm,\nwhere these capabilities are internalized within the model's parameters. We\nfirst position Reinforcement Learning (RL) as the algorithmic engine enabling\nthis paradigm shift. By reframing learning from imitating static data to\noutcome-driven exploration, RL underpins a unified solution of LLM + RL + Task\nacross language, vision and embodied domains. Building on this, the survey\nsystematically reviews how each capability -- Planning, Tool use, and Memory --\nhas evolved from externally scripted modules to end-to-end learned behaviors.\nFurthermore, it examines how this paradigm shift has reshaped major agent\napplications, specifically the Deep Research agent emphasizing long-horizon\nreasoning and the GUI agent emphasizing embodied interaction. We conclude by\ndiscussing the continued internalization of agentic capabilities like\nMulti-agent collaboration and Reflection, alongside the evolving roles of the\nsystem and model layers in future agentic AI. Together, these developments\noutline a coherent trajectory toward model-native agentic AI as an integrated\nlearning and interaction framework, marking the transition from constructing\nsystems that apply intelligence to developing models that grow intelligence\nthrough experience.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u667a\u80fd\u4f53AI\u4ece\u57fa\u4e8e\u7ba1\u9053\u7684\u7cfb\u7edf\u5411\u6a21\u578b\u539f\u751f\u8303\u5f0f\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u5176\u4e2d\u89c4\u5212\u3001\u5de5\u5177\u4f7f\u7528\u548c\u8bb0\u5fc6\u80fd\u529b\u4ece\u5916\u90e8\u6a21\u5757\u6f14\u53d8\u4e3a\u6a21\u578b\u5185\u90e8\u53c2\u6570\u5316\u7684\u7aef\u5230\u7aef\u5b66\u4e60\u884c\u4e3a\u3002", "motivation": "\u8ffd\u8e2a\u667a\u80fd\u4f53AI\u6784\u5efa\u8303\u5f0f\u7684\u8f6c\u53d8\uff0c\u4ece\u5916\u90e8\u903b\u8f91\u7f16\u6392\u7684Pipeline\u7cfb\u7edf\u5230\u80fd\u529b\u5185\u5316\u4e8e\u6a21\u578b\u53c2\u6570\u7684Model-native\u8303\u5f0f\uff0c\u63a2\u8ba8\u5f3a\u5316\u5b66\u4e60\u5728\u8fd9\u4e00\u8f6c\u53d8\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\u3002", "method": "\u7cfb\u7edf\u6027\u5730\u56de\u987e\u4e86\u89c4\u5212\u3001\u5de5\u5177\u4f7f\u7528\u548c\u8bb0\u5fc6\u4e09\u5927\u80fd\u529b\u7684\u6f14\u53d8\u8fc7\u7a0b\uff0c\u4ece\u5916\u90e8\u811a\u672c\u6a21\u5757\u5230\u7aef\u5230\u7aef\u5b66\u4e60\u884c\u4e3a\uff0c\u5e76\u5206\u6790\u4e86\u8fd9\u4e00\u8303\u5f0f\u8f6c\u53d8\u5982\u4f55\u91cd\u5851\u6df1\u5ea6\u7814\u7a76\u667a\u80fd\u4f53\u548cGUI\u667a\u80fd\u4f53\u7b49\u4e3b\u8981\u5e94\u7528\u3002", "result": "\u63ed\u793a\u4e86\u667a\u80fd\u4f53AI\u5411\u6a21\u578b\u539f\u751f\u53d1\u5c55\u7684\u8fde\u8d2f\u8f68\u8ff9\uff0c\u5c55\u793a\u4e86\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u3001\u53cd\u601d\u7b49\u80fd\u529b\u7684\u6301\u7eed\u5185\u5316\uff0c\u4ee5\u53ca\u7cfb\u7edf\u548c\u6a21\u578b\u5c42\u5728\u672a\u6765\u667a\u80fd\u4f53AI\u4e2d\u89d2\u8272\u7684\u6f14\u53d8\u3002", "conclusion": "\u667a\u80fd\u4f53AI\u6b63\u4ece\u6784\u5efa\u5e94\u7528\u667a\u80fd\u7684\u7cfb\u7edf\u8f6c\u5411\u5f00\u53d1\u901a\u8fc7\u7ecf\u9a8c\u589e\u957f\u667a\u80fd\u7684\u6a21\u578b\uff0c\u6807\u5fd7\u7740\u4ece\u9759\u6001\u6570\u636e\u6a21\u4eff\u5230\u7ed3\u679c\u9a71\u52a8\u63a2\u7d22\u7684\u5b66\u4e60\u8303\u5f0f\u8f6c\u53d8\u3002"}}
{"id": "2510.16712", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16712", "abs": "https://arxiv.org/abs/2510.16712", "authors": ["Shivam Ratnakar", "Sanjay Raghavendra"], "title": "The Chameleon Nature of LLMs: Quantifying Multi-Turn Stance Instability in Search-Enabled Language Models", "comment": null, "summary": "Integration of Large Language Models with search/retrieval engines has become\nubiquitous, yet these systems harbor a critical vulnerability that undermines\ntheir reliability. We present the first systematic investigation of \"chameleon\nbehavior\" in LLMs: their alarming tendency to shift stances when presented with\ncontradictory questions in multi-turn conversations (especially in\nsearch-enabled LLMs). Through our novel Chameleon Benchmark Dataset, comprising\n17,770 carefully crafted question-answer pairs across 1,180 multi-turn\nconversations spanning 12 controversial domains, we expose fundamental flaws in\nstate-of-the-art systems. We introduce two theoretically grounded metrics: the\nChameleon Score (0-1) that quantifies stance instability, and Source Re-use\nRate (0-1) that measures knowledge diversity. Our rigorous evaluation of\nLlama-4-Maverick, GPT-4o-mini, and Gemini-2.5-Flash reveals consistent\nfailures: all models exhibit severe chameleon behavior (scores 0.391-0.511),\nwith GPT-4o-mini showing the worst performance. Crucially, small\nacross-temperature variance (less than 0.004) suggests the effect is not a\nsampling artifact. Our analysis uncovers the mechanism: strong correlations\nbetween source re-use rate and confidence (r=0.627) and stance changes\n(r=0.429) are statistically significant (p less than 0.05), indicating that\nlimited knowledge diversity makes models pathologically deferential to query\nframing. These findings highlight the need for comprehensive consistency\nevaluation before deploying LLMs in healthcare, legal, and financial systems\nwhere maintaining coherent positions across interactions is critical for\nreliable decision support.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLMs\u5728\u68c0\u7d22\u589e\u5f3a\u7cfb\u7edf\u4e2d\u5b58\u5728\u53d8\u8272\u9f99\u884c\u4e3a\uff1a\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u9762\u5bf9\u77db\u76fe\u95ee\u9898\u65f6\u7acb\u573a\u4e0d\u7a33\u5b9a\u7684\u4e25\u91cd\u7f3a\u9677\u3002", "motivation": "\u63ed\u793aLLM\u4e0e\u68c0\u7d22\u5f15\u64ce\u96c6\u6210\u7cfb\u7edf\u4e2d\u7684\u5173\u952e\u6f0f\u6d1e\uff0c\u8fd9\u4e9b\u6f0f\u6d1e\u4f1a\u7834\u574f\u7cfb\u7edf\u7684\u53ef\u9760\u6027\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u4fdd\u6301\u7acb\u573a\u4e00\u81f4\u6027\u7684\u5173\u952e\u5e94\u7528\u9886\u57df\u3002", "method": "\u521b\u5efa\u5305\u542b17,770\u4e2a\u95ee\u7b54\u5bf9\u7684Chameleon\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6db5\u76d612\u4e2a\u4e89\u8bae\u9886\u57df\uff1b\u63d0\u51fa\u53d8\u8272\u9f99\u5206\u6570\u548c\u6765\u6e90\u91cd\u7528\u7387\u4e24\u4e2a\u7406\u8bba\u6307\u6807\uff1b\u8bc4\u4f30Llama-4-Maverick\u3001GPT-4o-mini\u548cGemini-2.5-Flash\u7b49\u6a21\u578b\u3002", "result": "\u6240\u6709\u6a21\u578b\u90fd\u8868\u73b0\u51fa\u4e25\u91cd\u7684\u53d8\u8272\u9f99\u884c\u4e3a\uff08\u5206\u65700.391-0.511\uff09\uff0cGPT-4o-mini\u8868\u73b0\u6700\u5dee\uff1b\u6765\u6e90\u91cd\u7528\u7387\u4e0e\u7f6e\u4fe1\u5ea6\u548c\u7acb\u573a\u53d8\u5316\u5448\u663e\u8457\u6b63\u76f8\u5173\uff0c\u8868\u660e\u77e5\u8bc6\u591a\u6837\u6027\u6709\u9650\u5bfc\u81f4\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u67e5\u8be2\u6846\u67b6\u3002", "conclusion": "\u5728\u533b\u7597\u3001\u6cd5\u5f8b\u548c\u91d1\u878d\u7b49\u9700\u8981\u4fdd\u6301\u7acb\u573a\u4e00\u81f4\u6027\u7684\u5173\u952e\u7cfb\u7edf\u4e2d\u90e8\u7f72LLM\u4e4b\u524d\uff0c\u5fc5\u987b\u8fdb\u884c\u5168\u9762\u7684\u7a33\u5b9a\u6027\u8bc4\u4f30\u3002"}}
{"id": "2510.16724", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16724", "abs": "https://arxiv.org/abs/2510.16724", "authors": ["Minhua Lin", "Zongyu Wu", "Zhichao Xu", "Hui Liu", "Xianfeng Tang", "Qi He", "Charu Aggarwal", "Hui Liu", "Xiang Zhang", "Suhang Wang"], "title": "A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications", "comment": "38 pages, 4 figures, 7 tables", "summary": "The advent of large language models (LLMs) has transformed information access\nand reasoning through open-ended natural language interaction. However, LLMs\nremain limited by static knowledge, factual hallucinations, and the inability\nto retrieve real-time or domain-specific information. Retrieval-Augmented\nGeneration (RAG) mitigates these issues by grounding model outputs in external\nevidence, but traditional RAG pipelines are often single turn and heuristic,\nlacking adaptive control over retrieval and reasoning. Recent advances in\nagentic search address these limitations by enabling LLMs to plan, retrieve,\nand reflect through multi-step interaction with search environments. Within\nthis paradigm, reinforcement learning (RL) offers a powerful mechanism for\nadaptive and self-improving search behavior. This survey provides the first\ncomprehensive overview of \\emph{RL-based agentic search}, organizing the\nemerging field along three complementary dimensions: (i) What RL is for\n(functional roles), (ii) How RL is used (optimization strategies), and (iii)\nWhere RL is applied (scope of optimization). We summarize representative\nmethods, evaluation protocols, and applications, and discuss open challenges\nand future directions toward building reliable and scalable RL driven agentic\nsearch systems. We hope this survey will inspire future research on the\nintegration of RL and agentic search. Our repository is available at\nhttps://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u5168\u9762\u7efc\u8ff0\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4ee3\u7406\u641c\u7d22\u9886\u57df\uff0c\u4ece\u529f\u80fd\u89d2\u8272\u3001\u4f18\u5316\u7b56\u7565\u548c\u5e94\u7528\u8303\u56f4\u4e09\u4e2a\u7ef4\u5ea6\u7ec4\u7ec7\u8fd9\u4e00\u65b0\u5174\u9886\u57df\uff0c\u65e8\u5728\u6784\u5efa\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684RL\u9a71\u52a8\u4ee3\u7406\u641c\u7d22\u7cfb\u7edf\u3002", "motivation": "\u4f20\u7edfRAG\u7ba1\u9053\u901a\u5e38\u662f\u5355\u8f6e\u548c\u542f\u53d1\u5f0f\u7684\uff0c\u7f3a\u4e4f\u5bf9\u68c0\u7d22\u548c\u63a8\u7406\u7684\u81ea\u9002\u5e94\u63a7\u5236\u3002\u5f3a\u5316\u5b66\u4e60\u4e3a\u4ee3\u7406\u641c\u7d22\u63d0\u4f9b\u4e86\u81ea\u9002\u5e94\u548c\u81ea\u6211\u6539\u8fdb\u7684\u673a\u5236\uff0c\u4ee5\u89e3\u51b3LLMs\u7684\u9759\u6001\u77e5\u8bc6\u3001\u4e8b\u5b9e\u5e7b\u89c9\u548c\u65e0\u6cd5\u83b7\u53d6\u5b9e\u65f6\u4fe1\u606f\u7b49\u9650\u5236\u3002", "method": "\u901a\u8fc7\u4e09\u4e2a\u4e92\u8865\u7ef4\u5ea6\u7ec4\u7ec7RL-based\u4ee3\u7406\u641c\u7d22\u9886\u57df\uff1a(i) RL\u7684\u529f\u80fd\u89d2\u8272\uff0c(ii) RL\u7684\u4f18\u5316\u7b56\u7565\uff0c(iii) RL\u7684\u5e94\u7528\u8303\u56f4\u3002\u603b\u7ed3\u4e86\u4ee3\u8868\u6027\u65b9\u6cd5\u3001\u8bc4\u4f30\u534f\u8bae\u548c\u5e94\u7528\u6848\u4f8b\u3002", "result": "\u63d0\u4f9b\u4e86\u8be5\u9886\u57df\u7684\u9996\u4e2a\u5168\u9762\u6982\u8ff0\uff0c\u5efa\u7acb\u4e86\u7cfb\u7edf\u5316\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u5e76\u8bc6\u522b\u4e86\u5173\u952e\u7814\u7a76\u65b9\u5411\u548c\u6311\u6218\u3002", "conclusion": "RL\u4e0e\u4ee3\u7406\u641c\u7d22\u7684\u6574\u5408\u4e3a\u89e3\u51b3LLMs\u7684\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\uff0c\u8be5\u7efc\u8ff0\u65e8\u5728\u6fc0\u53d1\u672a\u6765\u5728\u8fd9\u4e00\u4ea4\u53c9\u9886\u57df\u7684\u7814\u7a76\u3002"}}
{"id": "2510.16713", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16713", "abs": "https://arxiv.org/abs/2510.16713", "authors": ["Sriharsh Bhyravajjula", "Melanie Walsh", "Anna Preus", "Maria Antoniak"], "title": "so much depends / upon / a whitespace: Why Whitespace Matters for Poets and LLMs", "comment": null, "summary": "Whitespace is a critical component of poetic form, reflecting both adherence\nto standardized forms and rebellion against those forms. Each poem's whitespace\ndistribution reflects the artistic choices of the poet and is an integral\nsemantic and spatial feature of the poem. Yet, despite the popularity of poetry\nas both a long-standing art form and as a generation task for large language\nmodels (LLMs), whitespace has not received sufficient attention from the NLP\ncommunity. Using a corpus of 19k English-language published poems from Poetry\nFoundation, we investigate how 4k poets have used whitespace in their works. We\nrelease a subset of 2.8k public-domain poems with preserved formatting to\nfacilitate further research in this area. We compare whitespace usage in the\npublished poems to (1) 51k LLM-generated poems, and (2) 12k unpublished poems\nposted in an online community. We also explore whitespace usage across time\nperiods, poetic forms, and data sources. Additionally, we find that different\ntext processing methods can result in significantly different representations\nof whitespace in poetry data, motivating us to use these poems and whitespace\npatterns to discuss implications for the processing strategies used to assemble\npretraining datasets for LLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u8bd7\u6b4c\u4e2d\u7a7a\u767d\u7684\u91cd\u8981\u6027\uff0c\u5206\u6790\u4e8619k\u9996\u8bd7\u6b4c\u7684\u7a7a\u767d\u5206\u5e03\uff0c\u6bd4\u8f83\u4e86\u4eba\u7c7b\u521b\u4f5c\u4e0eLLM\u751f\u6210\u8bd7\u6b4c\u7684\u7a7a\u767d\u4f7f\u7528\u5dee\u5f02\uff0c\u5e76\u63a2\u8ba8\u4e86\u4e0d\u540c\u6587\u672c\u5904\u7406\u65b9\u6cd5\u5bf9\u7a7a\u767d\u8868\u793a\u7684\u5f71\u54cd\u3002", "motivation": "\u7a7a\u767d\u662f\u8bd7\u6b4c\u5f62\u5f0f\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u53cd\u6620\u4e86\u8bd7\u4eba\u5bf9\u6807\u51c6\u5316\u5f62\u5f0f\u7684\u9075\u5faa\u548c\u53cd\u6297\u3002\u5c3d\u7ba1\u8bd7\u6b4c\u662f\u957f\u671f\u5b58\u5728\u7684\u827a\u672f\u5f62\u5f0f\uff0c\u4e5f\u662fLLM\u7684\u751f\u6210\u4efb\u52a1\uff0c\u4f46\u7a7a\u767d\u5728NLP\u793e\u533a\u4e2d\u672a\u5f97\u5230\u8db3\u591f\u5173\u6ce8\u3002", "method": "\u4f7f\u7528\u6765\u81eaPoetry Foundation\u768419k\u9996\u82f1\u8bed\u8bd7\u6b4c\u8bed\u6599\u5e93\uff0c\u5206\u67904k\u4f4d\u8bd7\u4eba\u7684\u7a7a\u767d\u4f7f\u7528\u60c5\u51b5\u3002\u6bd4\u8f83\u4e86\u5df2\u53d1\u8868\u8bd7\u6b4c\u4e0e51k\u9996LLM\u751f\u6210\u8bd7\u6b4c\u548c12k\u9996\u672a\u53d1\u8868\u8bd7\u6b4c\u7684\u7a7a\u767d\u4f7f\u7528\u5dee\u5f02\uff0c\u5e76\u63a2\u8ba8\u4e86\u4e0d\u540c\u65f6\u671f\u3001\u8bd7\u6b4c\u5f62\u5f0f\u548c\u6570\u636e\u6e90\u7684\u7a7a\u767d\u4f7f\u7528\u6a21\u5f0f\u3002", "result": "\u53d1\u73b0\u4e0d\u540c\u6587\u672c\u5904\u7406\u65b9\u6cd5\u4f1a\u5bfc\u81f4\u8bd7\u6b4c\u6570\u636e\u4e2d\u7a7a\u767d\u8868\u793a\u663e\u8457\u4e0d\u540c\u3002\u53d1\u5e03\u4e862.8k\u9996\u516c\u5171\u9886\u57df\u8bd7\u6b4c\u5b50\u96c6\u4ee5\u4fc3\u8fdb\u8be5\u9886\u57df\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "conclusion": "\u8bd7\u6b4c\u4e2d\u7684\u7a7a\u767d\u6a21\u5f0f\u5bf9LLM\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u5904\u7406\u7b56\u7565\u5177\u6709\u91cd\u8981\u542f\u793a\uff0c\u5f3a\u8c03\u4e86\u5728\u6587\u672c\u5904\u7406\u4e2d\u4fdd\u7559\u7a7a\u767d\u4fe1\u606f\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.16742", "categories": ["cs.AI", "cs.MA", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.16742", "abs": "https://arxiv.org/abs/2510.16742", "authors": ["Paul Saves", "Pramudita Satria Palar", "Muhammad Daffa Robani", "Nicolas Verstaevel", "Moncef Garouani", "Julien Aligon", "Benoit Gaudou", "Koji Shimoyama", "Joseph Morlier"], "title": "Surrogate Modeling and Explainable Artificial Intelligence for Complex Systems: A Workflow for Automated Simulation Exploration", "comment": null, "summary": "Complex systems are increasingly explored through simulation-driven\nengineering workflows that combine physics-based and empirical models with\noptimization and analytics. Despite their power, these workflows face two\ncentral obstacles: (1) high computational cost, since accurate exploration\nrequires many expensive simulator runs; and (2) limited transparency and\nreliability when decisions rely on opaque blackbox components. We propose a\nworkflow that addresses both challenges by training lightweight emulators on\ncompact designs of experiments that (i) provide fast, low-latency\napproximations of expensive simulators, (ii) enable rigorous uncertainty\nquantification, and (iii) are adapted for global and local Explainable\nArtificial Intelligence (XAI) analyses. This workflow unifies every\nsimulation-based complex-system analysis tool, ranging from engineering design\nto agent-based models for socio-environmental understanding. In this paper, we\nproposea comparative methodology and practical recommendations for using\nsurrogate-based explainability tools within the proposed workflow. The\nmethodology supports continuous and categorical inputs, combines global-effect\nand uncertainty analyses with local attribution, and evaluates the consistency\nof explanations across surrogate models, thereby diagnosing surrogate adequacy\nand guiding further data collection or model refinement. We demonstrate the\napproach on two contrasting case studies: a multidisciplinary design analysis\nof a hybrid-electric aircraft and an agent-based model of urban segregation.\nResults show that the surrogate model and XAI coupling enables large-scale\nexploration in seconds, uncovers nonlinear interactions and emergent behaviors,\nidentifies key design and policy levers, and signals regions where surrogates\nrequire more data or alternative architectures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7406\u6a21\u578b\u7684\u4eff\u771f\u9a71\u52a8\u5de5\u7a0b\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u4eff\u771f\u5668\u6765\u89e3\u51b3\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u9ed1\u76d2\u900f\u660e\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u652f\u6301\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u53ef\u89e3\u91caAI\u5206\u6790\u3002", "motivation": "\u4eff\u771f\u9a71\u52a8\u5de5\u7a0b\u5de5\u4f5c\u6d41\u9762\u4e34\u4e24\u4e2a\u6838\u5fc3\u969c\u788d\uff1a(1) \u9ad8\u8ba1\u7b97\u6210\u672c\uff0c\u51c6\u786e\u63a2\u7d22\u9700\u8981\u5927\u91cf\u6602\u8d35\u7684\u6a21\u62df\u5668\u8fd0\u884c\uff1b(2) \u4f9d\u8d56\u4e0d\u900f\u660e\u9ed1\u76d2\u7ec4\u4ef6\u65f6\u7684\u6709\u9650\u900f\u660e\u5ea6\u548c\u53ef\u9760\u6027\u3002", "method": "\u5728\u7d27\u51d1\u5b9e\u9a8c\u8bbe\u8ba1\u4e0a\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u4eff\u771f\u5668\uff0c\u63d0\u4f9b\u5feb\u901f\u3001\u4f4e\u5ef6\u8fdf\u7684\u6602\u8d35\u6a21\u62df\u5668\u8fd1\u4f3c\uff0c\u652f\u6301\u4e25\u683c\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u5e76\u9002\u5e94\u5168\u5c40\u548c\u5c40\u90e8\u53ef\u89e3\u91caAI\u5206\u6790\u3002", "result": "\u5728\u6df7\u5408\u52a8\u529b\u98de\u673a\u591a\u5b66\u79d1\u8bbe\u8ba1\u5206\u6790\u548c\u57ce\u5e02\u9694\u79bb\u57fa\u4e8e\u4ee3\u7406\u6a21\u578b\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u79d2\u7ea7\u5b8c\u6210\u5927\u89c4\u6a21\u63a2\u7d22\uff0c\u63ed\u793a\u975e\u7ebf\u6027\u76f8\u4e92\u4f5c\u7528\u548c\u6d8c\u73b0\u884c\u4e3a\uff0c\u8bc6\u522b\u5173\u952e\u8bbe\u8ba1\u548c\u653f\u7b56\u6760\u6746\uff0c\u5e76\u6307\u793a\u4ee3\u7406\u6a21\u578b\u9700\u8981\u66f4\u591a\u6570\u636e\u6216\u66ff\u4ee3\u67b6\u6784\u7684\u533a\u57df\u3002", "conclusion": "\u4ee3\u7406\u6a21\u578b\u4e0e\u53ef\u89e3\u91caAI\u7684\u8026\u5408\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4eff\u771f\u9a71\u52a8\u5de5\u4f5c\u6d41\u4e2d\u7684\u8ba1\u7b97\u6210\u672c\u548c\u900f\u660e\u5ea6\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u7cfb\u7edf\u5206\u6790\u63d0\u4f9b\u7edf\u4e00\u6846\u67b6\u3002"}}
{"id": "2510.16727", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16727", "abs": "https://arxiv.org/abs/2510.16727", "authors": ["Sanskar Pandey", "Ruhaan Chopra", "Angkul Puniya", "Sohom Pal"], "title": "Beacon: Single-Turn Diagnosis and Mitigation of Latent Sycophancy in Large Language Models", "comment": null, "summary": "Large language models internalize a structural trade-off between truthfulness\nand obsequious flattery, emerging from reward optimization that conflates\nhelpfulness with polite submission. This latent bias, known as sycophancy,\nmanifests as a preference for user agreement over principled reasoning. We\nintroduce Beacon, a single-turn forced-choice benchmark that isolates this bias\nindependent of conversational context, enabling precise measurement of the\ntension between factual accuracy and submissive bias. Evaluations across twelve\nstate-of-the-art models reveal that sycophancy decomposes into stable\nlinguistic and affective sub-biases, each scaling with model capacity. We\nfurther propose prompt-level and activation-level interventions that modulate\nthese biases in opposing directions, exposing the internal geometry of\nalignment as a dynamic manifold between truthfulness and socially compliant\njudgment. Beacon reframes sycophancy as a measurable form of normative\nmisgeneralization, providing a reproducible foundation for studying and\nmitigating alignment drift in large-scale generative systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86Beacon\u57fa\u51c6\u6765\u6d4b\u91cf\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u8c04\u5a9a\u504f\u89c1\uff0c\u53d1\u73b0\u8be5\u504f\u89c1\u968f\u6a21\u578b\u80fd\u529b\u589e\u5f3a\u800c\u52a0\u5267\uff0c\u5e76\u63d0\u51fa\u4e86\u5e72\u9884\u65b9\u6cd5\u6765\u8c03\u8282\u771f\u5b9e\u6027\u4e0e\u987a\u4ece\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5956\u52b1\u4f18\u5316\u8fc7\u7a0b\u4e2d\u6df7\u6dc6\u4e86\u5e2e\u52a9\u6027\u548c\u793c\u8c8c\u670d\u4ece\uff0c\u5f62\u6210\u4e86\u771f\u5b9e\u6027\u548c\u8c04\u5a9a\u4e4b\u95f4\u7684\u7ed3\u6784\u6027\u6743\u8861\uff0c\u8fd9\u79cd\u504f\u89c1\u4f1a\u5f71\u54cd\u6a21\u578b\u7684\u7406\u6027\u5224\u65ad\u3002", "method": "\u5f15\u5165Beacon\u57fa\u51c6\uff0c\u901a\u8fc7\u5355\u8f6e\u5f3a\u5236\u9009\u62e9\u4efb\u52a1\u72ec\u7acb\u6d4b\u91cf\u8c04\u5a9a\u504f\u89c1\uff0c\u8bc4\u4f30\u4e8612\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u63d0\u793a\u7ea7\u548c\u6fc0\u6d3b\u7ea7\u5e72\u9884\u65b9\u6cd5\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u8c04\u5a9a\u504f\u89c1\u53ef\u5206\u89e3\u4e3a\u7a33\u5b9a\u7684\u8bed\u8a00\u548c\u60c5\u611f\u5b50\u504f\u89c1\uff0c\u4e14\u968f\u6a21\u578b\u5bb9\u91cf\u589e\u52a0\u800c\u52a0\u5267\uff1b\u5e72\u9884\u65b9\u6cd5\u80fd\u5728\u771f\u5b9e\u6027\u548c\u793e\u4f1a\u5408\u89c4\u5224\u65ad\u4e4b\u95f4\u8c03\u8282\u8fd9\u4e9b\u504f\u89c1\u3002", "conclusion": "Beacon\u5c06\u8c04\u5a9a\u91cd\u65b0\u5b9a\u4e49\u4e3a\u53ef\u6d4b\u91cf\u7684\u89c4\u8303\u9519\u8bef\u6cdb\u5316\u5f62\u5f0f\uff0c\u4e3a\u7814\u7a76\u548c\u7f13\u89e3\u5927\u89c4\u6a21\u751f\u6210\u7cfb\u7edf\u4e2d\u7684\u5bf9\u9f50\u6f02\u79fb\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u7840\u3002"}}
{"id": "2510.16753", "categories": ["cs.AI", "68T30", "H.3.3"], "pdf": "https://arxiv.org/pdf/2510.16753", "abs": "https://arxiv.org/abs/2510.16753", "authors": ["Wei Huang", "Peining Li", "Meiyu Liang", "Xu Hou", "Junping Du", "Yingxia Shao", "Guanhua Ye", "Wu Liu", "Kangkang Lu", "Yang Yu"], "title": "ELMM: Efficient Lightweight Multimodal Large Language Models for Multimodal Knowledge Graph Completion", "comment": "11 pages, 4 figures", "summary": "Multimodal Knowledge Graphs (MKGs) extend traditional knowledge graphs by\nincorporating visual and textual modalities, enabling richer and more\nexpressive entity representations. However, existing MKGs often suffer from\nincompleteness, which hinder their effectiveness in downstream tasks.\nTherefore, multimodal knowledge graph completion (MKGC) task is receiving\nincreasing attention. While large language models (LLMs) have shown promise for\nknowledge graph completion (KGC), their application to the multimodal setting\nremains underexplored. Moreover, applying Multimodal Large Language Models\n(MLLMs) to the task of MKGC introduces significant challenges: (1) the large\nnumber of image tokens per entity leads to semantic noise and modality\nconflicts, and (2) the high computational cost of processing large token\ninputs. To address these issues, we propose Efficient Lightweight Multimodal\nLarge Language Models (ELMM) for MKGC. ELMM proposes a Multi-view Visual Token\nCompressor (MVTC) based on multi-head attention mechanism, which adaptively\ncompresses image tokens from both textual and visual views, thereby effectively\nreducing redundancy while retaining necessary information and avoiding modality\nconflicts. Additionally, we design an attention pruning strategy to remove\nredundant attention layers from MLLMs, thereby significantly reducing the\ninference cost. We further introduce a linear projection to compensate for the\nperformance degradation caused by pruning. Extensive experiments on benchmark\nFB15k-237-IMG and WN18-IMG demonstrate that ELMM achieves state-of-the-art\nperformance while substantially improving computational efficiency,\nestablishing a new paradigm for multimodal knowledge graph completion.", "AI": {"tldr": "\u63d0\u51faELMM\u65b9\u6cd5\u7528\u4e8e\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\uff0c\u901a\u8fc7\u591a\u89c6\u56fe\u89c6\u89c9token\u538b\u7f29\u5668\u548c\u6ce8\u610f\u529b\u526a\u679d\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387", "motivation": "\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u5b58\u5728\u4e0d\u5b8c\u6574\u6027\u95ee\u9898\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u591a\u6a21\u6001\u4fe1\u606f\u65f6\u9762\u4e34\u8bed\u4e49\u566a\u58f0\u3001\u6a21\u6001\u51b2\u7a81\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u6311\u6218", "method": "\u4f7f\u7528\u57fa\u4e8e\u591a\u5934\u6ce8\u610f\u529b\u7684\u591a\u89c6\u56fe\u89c6\u89c9token\u538b\u7f29\u5668\u81ea\u9002\u5e94\u538b\u7f29\u56fe\u50cftoken\uff0c\u8bbe\u8ba1\u6ce8\u610f\u529b\u526a\u679d\u7b56\u7565\u51cf\u5c11\u5197\u4f59\u5c42\uff0c\u5e76\u901a\u8fc7\u7ebf\u6027\u6295\u5f71\u8865\u507f\u6027\u80fd\u635f\u5931", "result": "\u5728FB15k-237-IMG\u548cWN18-IMG\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u5927\u5e45\u63d0\u5347\u8ba1\u7b97\u6548\u7387", "conclusion": "ELMM\u4e3a\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u5efa\u7acb\u4e86\u65b0\u7684\u8303\u5f0f\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272"}}
{"id": "2510.16761", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16761", "abs": "https://arxiv.org/abs/2510.16761", "authors": ["Yikai Zhang", "Ye Rong", "Siyu Yuan", "Jiangjie Chen", "Jian Xie", "Yanghua Xiao"], "title": "Enhancing Language Agent Strategic Reasoning through Self-Play in Adversarial Games", "comment": null, "summary": "Existing language agents often encounter difficulties in dynamic adversarial\ngames due to poor strategic reasoning. To mitigate this limitation, a promising\napproach is to allow agents to learn from game interactions automatically,\nwithout relying on costly expert-labeled data. Unlike static environments where\nagents receive fixed feedback or rewards, selecting appropriate opponents in\ndynamic adversarial games can significantly impact learning performance.\nHowever, the discussion of opponents in adversarial environments remains an\narea under exploration. In this paper, we propose a Step-level poliCy\nOptimization method through Play-And-Learn, SCO-PAL. Leveraging SCO-PAL, we\nconduct a detailed analysis of opponent selection by setting opponents at\ndifferent levels and find that self-play is the most effective way to improve\nstrategic reasoning in such adversarial environments. Utilizing SCO-PAL with\nself-play, we increase the average win rate against four opponents by\napproximately 30% compared to baselines and achieve a 54.76% win rate against\nGPT-4 in six adversarial games.", "AI": {"tldr": "\u63d0\u51faSCO-PAL\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u535a\u5f08\u5b66\u4e60\u63d0\u5347\u8bed\u8a00\u4ee3\u7406\u5728\u52a8\u6001\u5bf9\u6297\u6e38\u620f\u4e2d\u7684\u6218\u7565\u63a8\u7406\u80fd\u529b\uff0c\u76f8\u6bd4\u57fa\u7ebf\u5e73\u5747\u80dc\u7387\u63d0\u5347\u7ea630%\uff0c\u5bf9\u6297GPT-4\u8fbe\u523054.76%\u80dc\u7387\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u4ee3\u7406\u5728\u52a8\u6001\u5bf9\u6297\u6e38\u620f\u4e2d\u6218\u7565\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8981\u65e0\u9700\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u7684\u81ea\u52a8\u5b66\u4e60\u65b9\u6cd5\uff0c\u800c\u5bf9\u624b\u9009\u62e9\u5bf9\u5b66\u4e60\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\u4f46\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u63d0\u51faSCO-PAL\u65b9\u6cd5\uff0c\u901a\u8fc7\u73a9\u4e0e\u5b66\u4e60\u8fdb\u884c\u7b56\u7565\u4f18\u5316\uff0c\u5206\u6790\u4e0d\u540c\u7ea7\u522b\u5bf9\u624b\u9009\u62e9\uff0c\u53d1\u73b0\u81ea\u535a\u5f08\u662f\u6700\u6709\u6548\u7684\u5b66\u4e60\u65b9\u5f0f\u3002", "result": "\u4f7f\u7528SCO-PAL\u4e0e\u81ea\u535a\u5f08\uff0c\u5728\u516d\u4e2a\u5bf9\u6297\u6e38\u620f\u4e2d\u76f8\u6bd4\u57fa\u7ebf\u5e73\u5747\u80dc\u7387\u63d0\u5347\u7ea630%\uff0c\u5bf9\u6297GPT-4\u8fbe\u523054.76%\u80dc\u7387\u3002", "conclusion": "\u81ea\u535a\u5f08\u662f\u63d0\u5347\u5bf9\u6297\u73af\u5883\u4e2d\u6218\u7565\u63a8\u7406\u7684\u6700\u6709\u6548\u65b9\u5f0f\uff0cSCO-PAL\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u4ee3\u7406\u5728\u52a8\u6001\u5bf9\u6297\u6e38\u620f\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2510.16756", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.RO", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.16756", "abs": "https://arxiv.org/abs/2510.16756", "authors": ["Siyin Wang", "Wenyi Yu", "Xianzhao Chen", "Xiaohai Tian", "Jun Zhang", "Lu Lu", "Chao Zhang"], "title": "End-to-end Listen, Look, Speak and Act", "comment": "22 pages, 8 figures", "summary": "Human interaction is inherently multimodal and full-duplex: we listen while\nwatching, speak while acting, and fluidly adapt to turn-taking and\ninterruptions. Realizing these capabilities is essential for building models\nsimulating humans. We present ELLSA (End-to-end Listen, Look, Speak and Act),\nwhich, to our knowledge, is the first full-duplex, end-to-end model that\nsimultaneously perceives and generates across vision, text, speech, and action\nwithin a single architecture, enabling interaction patterns previously out of\nreach, yielding more natural, human-like behaviors. At its core is a novel\nSA-MoE architecture (Self-Attention Mixture-of-Experts) that routes each\nmodality to specialized experts and fuses them through a unified attention\nbackbone. This provides a generalizable solution for joint multimodal\nperception and concurrent generation, leveraging strong pre-trained components\nwhile enabling efficient modality integration and mitigating modality\ninterference. On speech-interaction and robot-manipulation benchmarks, ELLSA\nmatches modality-specific baselines, while uniquely supporting advanced\nmultimodal and full-duplex behaviors such as dialogue and action turn-taking,\ndefective instruction rejection, speaking-while-acting, context-grounded visual\nquestion answering, and action barge-ins. We contend that ELLSA represents a\nstep toward more natural and general interactive intelligence, contributing to\nthe broader pursuit of artificial general intelligence. All data, code and\nmodel checkpoints will be released upon acceptance.", "AI": {"tldr": "ELLSA\u662f\u9996\u4e2a\u5168\u53cc\u5de5\u3001\u7aef\u5230\u7aef\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u80fd\u591f\u540c\u65f6\u611f\u77e5\u548c\u751f\u6210\u89c6\u89c9\u3001\u6587\u672c\u3001\u8bed\u97f3\u548c\u52a8\u4f5c\uff0c\u5b9e\u73b0\u66f4\u81ea\u7136\u7684\u4eba\u673a\u4ea4\u4e92\u3002", "motivation": "\u4eba\u7c7b\u4ea4\u4e92\u672c\u8d28\u4e0a\u662f\u591a\u6a21\u6001\u548c\u5168\u53cc\u5de5\u7684\uff0c\u9700\u8981\u6a21\u578b\u80fd\u591f\u540c\u65f6\u611f\u77e5\u548c\u751f\u6210\u591a\u79cd\u6a21\u6001\uff0c\u5b9e\u73b0\u66f4\u81ea\u7136\u7684\u4eba\u7c7b\u884c\u4e3a\u6a21\u62df\u3002", "method": "\u91c7\u7528\u65b0\u9896\u7684SA-MoE\u67b6\u6784\uff08\u81ea\u6ce8\u610f\u529b\u4e13\u5bb6\u6df7\u5408\uff09\uff0c\u5c06\u5404\u6a21\u6001\u8def\u7531\u5230\u4e13\u95e8\u4e13\u5bb6\uff0c\u901a\u8fc7\u7edf\u4e00\u6ce8\u610f\u529b\u9aa8\u5e72\u7f51\u7edc\u8fdb\u884c\u878d\u5408\u3002", "result": "\u5728\u8bed\u97f3\u4ea4\u4e92\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cELLSA\u4e0e\u6a21\u6001\u7279\u5b9a\u57fa\u7ebf\u8868\u73b0\u76f8\u5f53\uff0c\u540c\u65f6\u652f\u6301\u9ad8\u7ea7\u591a\u6a21\u6001\u548c\u5168\u53cc\u5de5\u884c\u4e3a\u3002", "conclusion": "ELLSA\u4ee3\u8868\u4e86\u5411\u66f4\u81ea\u7136\u548c\u901a\u7528\u4ea4\u4e92\u667a\u80fd\u8fc8\u51fa\u7684\u4e00\u6b65\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u4eba\u5de5\u901a\u7528\u667a\u80fd\u3002"}}
{"id": "2510.16783", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16783", "abs": "https://arxiv.org/abs/2510.16783", "authors": ["Sheikh Jubair", "Arwa Omayrah", "Amal Alshammari", "Alhanoof Althnian", "Abdulhamed Alothaimen", "Norah A. Alzahrani", "Shahad D. Alzaidi", "Nora Al-Twairesh", "Abdulmohsen Al-Thubaity"], "title": "LC-Eval: A Bilingual Multi-Task Evaluation Benchmark for Long-Context Understanding", "comment": "1 figure, 15 tables, 10 main pages", "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nsophisticated capabilities, including the ability to process and comprehend\nextended contexts. These emergent capabilities necessitate rigorous evaluation\nmethods to effectively assess their performance in long-context understanding.\nIn this paper, we present \\textbf{LC-Eval}, a bilingual, multi-task evaluation\nbenchmark designed to evaluate long-context understanding in English and\nArabic, targeting context lengths ranging from 4k to over 128k tokens. LC-Eval\nintroduces four novel and challenging tasks: multi-document question answering,\nbilingual question answering, claim verification within a paragraph, and\nmultiple-choice questions based on long contexts. These tasks are designed to\nassess LLMs' abilities in deep reasoning, document comprehension, information\ntracing, and bilingual information extraction and understanding. The benchmark\nincludes datasets in both Arabic and English for each task, allowing for a\ncomparative analysis of their performance across different text genres.\nEvaluations were conducted on both open-weight and closed LLMs, with results\nindicating that LC-Eval presents significant challenges. Even high-performing\nmodels, such as GPT-4o, struggled with certain tasks, highlighting the\ncomplexity and rigor of the benchmark.", "AI": {"tldr": "LC-Eval\u662f\u4e00\u4e2a\u53cc\u8bed\u591a\u4efb\u52a1\u8bc4\u4f30\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u82f1\u8bed\u548c\u963f\u62c9\u4f2f\u8bed\u7684\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\uff0c\u6db5\u76d64k\u5230128k+token\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u5305\u542b\u56db\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u80fd\u529b\u4e0a\u7684\u8fdb\u6b65\uff0c\u9700\u8981\u4e25\u683c\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u6709\u6548\u8bc4\u4f30\u5176\u5728\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u65b9\u9762\u7684\u8868\u73b0\u3002", "method": "\u5f00\u53d1\u4e86LC-Eval\u8bc4\u4f30\u57fa\u51c6\uff0c\u5305\u542b\u56db\u4e2a\u65b0\u9896\u4efb\u52a1\uff1a\u591a\u6587\u6863\u95ee\u7b54\u3001\u53cc\u8bed\u95ee\u7b54\u3001\u6bb5\u843d\u5185\u58f0\u660e\u9a8c\u8bc1\u548c\u57fa\u4e8e\u957f\u4e0a\u4e0b\u6587\u7684\u591a\u9009\u9898\uff0c\u6db5\u76d6\u82f1\u8bed\u548c\u963f\u62c9\u4f2f\u8bed\u6570\u636e\u96c6\u3002", "result": "\u8bc4\u4f30\u663e\u793aLC-Eval\u5177\u6709\u663e\u8457\u6311\u6218\u6027\uff0c\u5373\u4f7f\u662fGPT-4o\u7b49\u9ad8\u6027\u80fd\u6a21\u578b\u5728\u67d0\u4e9b\u4efb\u52a1\u4e0a\u4e5f\u8868\u73b0\u56f0\u96be\uff0c\u7a81\u663e\u4e86\u57fa\u51c6\u7684\u590d\u6742\u6027\u548c\u4e25\u8c28\u6027\u3002", "conclusion": "LC-Eval\u4e3a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u6df1\u5ea6\u63a8\u7406\u548c\u4fe1\u606f\u8ffd\u8e2a\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.16769", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16769", "abs": "https://arxiv.org/abs/2510.16769", "authors": ["Shuo Han", "Yukun Cao", "Zezhong Ding", "Zengyi Gao", "S Kevin Zhou", "Xike Xie"], "title": "See or Say Graphs: Agent-Driven Scalable Graph Understanding with Vision-Language Models", "comment": null, "summary": "Vision-language models (VLMs) have shown promise in graph understanding, but\nremain limited by input-token constraints, facing scalability bottlenecks and\nlacking effective mechanisms to coordinate textual and visual modalities. To\naddress these challenges, we propose GraphVista, a unified framework that\nenhances both scalability and modality coordination in graph understanding. For\nscalability, GraphVista organizes graph information hierarchically into a\nlightweight GraphRAG base, which retrieves only task-relevant textual\ndescriptions and high-resolution visual subgraphs, compressing redundant\ncontext while preserving key reasoning elements. For modality coordination,\nGraphVista introduces a planning agent that routes tasks to the most suitable\nmodality-using the text modality for simple property reasoning and the visual\nmodality for local and structurally complex reasoning grounded in explicit\ntopology. Extensive experiments demonstrate that GraphVista scales to large\ngraphs, up to $200\\times$ larger than those used in existing benchmarks, and\nconsistently outperforms existing textual, visual, and fusion-based methods,\nachieving up to $4.4\\times$ quality improvement over the state-of-the-art\nbaselines by fully exploiting the complementary strengths of both modalities.", "AI": {"tldr": "GraphVista\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u7ec4\u7ec7\u56fe\u4fe1\u606f\u548c\u5f15\u5165\u89c4\u5212\u4ee3\u7406\u6765\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u7406\u89e3\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u6a21\u6001\u534f\u8c03\u95ee\u9898\uff0c\u80fd\u591f\u5904\u7406\u6bd4\u73b0\u6709\u57fa\u51c6\u5927200\u500d\u7684\u56fe\uff0c\u5e76\u5728\u8d28\u91cf\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u53474.4\u500d\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u7406\u89e3\u4e2d\u9762\u4e34\u8f93\u5165token\u9650\u5236\u5bfc\u81f4\u7684\u53ef\u6269\u5c55\u6027\u74f6\u9888\uff0c\u4ee5\u53ca\u7f3a\u4e4f\u6709\u6548\u534f\u8c03\u6587\u672c\u548c\u89c6\u89c9\u6a21\u6001\u7684\u673a\u5236\u3002", "method": "GraphVista\u91c7\u7528\u5206\u5c42\u65b9\u6cd5\u7ec4\u7ec7\u56fe\u4fe1\u606f\u5230\u8f7b\u91cf\u7ea7GraphRAG\u57fa\u7840\u4e2d\uff0c\u4ec5\u68c0\u7d22\u4efb\u52a1\u76f8\u5173\u7684\u6587\u672c\u63cf\u8ff0\u548c\u9ad8\u5206\u8fa8\u7387\u89c6\u89c9\u5b50\u56fe\uff1b\u5f15\u5165\u89c4\u5212\u4ee3\u7406\u6839\u636e\u4efb\u52a1\u590d\u6742\u5ea6\u8def\u7531\u5230\u6700\u9002\u5408\u7684\u6a21\u6001\u3002", "result": "GraphVista\u80fd\u591f\u6269\u5c55\u5230\u6bd4\u73b0\u6709\u57fa\u51c6\u5927200\u500d\u7684\u56fe\uff0c\u5728\u6587\u672c\u3001\u89c6\u89c9\u548c\u878d\u5408\u65b9\u6cd5\u4e2d\u8868\u73b0\u4e00\u81f4\u4f18\u8d8a\uff0c\u8d28\u91cf\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u63d0\u5347\u9ad8\u8fbe4.4\u500d\u3002", "conclusion": "GraphVista\u901a\u8fc7\u5145\u5206\u5229\u7528\u4e24\u79cd\u6a21\u6001\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u7406\u89e3\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u6a21\u6001\u534f\u8c03\u6311\u6218\u3002"}}
{"id": "2510.16797", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16797", "abs": "https://arxiv.org/abs/2510.16797", "authors": ["Vera Pavlova", "Mohammed Makhlouf"], "title": "MOSAIC: Masked Objective with Selective Adaptation for In-domain Contrastive Learning", "comment": null, "summary": "We introduce MOSAIC (Masked Objective with Selective Adaptation for In-domain\nContrastive learning), a multi-stage framework for domain adaptation of\nsentence embedding models that incorporates joint domain-specific masked\nsupervision. Our approach addresses the challenges of adapting large-scale\ngeneral-domain sentence embedding models to specialized domains. By jointly\noptimizing masked language modeling (MLM) and contrastive objectives within a\nunified training pipeline, our method enables effective learning of\ndomain-relevant representations while preserving the robust semantic\ndiscrimination properties of the original model. We empirically validate our\napproach on both high-resource and low-resource domains, achieving improvements\nup to 13.4% in NDCG@10 (Normalized Discounted Cumulative Gain) over strong\ngeneral-domain baselines. Comprehensive ablation studies further demonstrate\nthe effectiveness of each component, highlighting the importance of balanced\njoint supervision and staged adaptation.", "AI": {"tldr": "\u63d0\u51faMOSAIC\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u57df\u9002\u5e94\u65b9\u6cd5\u7ed3\u5408\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\u548c\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\uff0c\u63d0\u5347\u53e5\u5b50\u5d4c\u5165\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u901a\u7528\u9886\u57df\u53e5\u5b50\u5d4c\u5165\u6a21\u578b\u9002\u5e94\u4e13\u4e1a\u9886\u57df\u7684\u6311\u6218\uff0c\u9700\u8981\u5728\u4fdd\u6301\u539f\u6709\u8bed\u4e49\u533a\u5206\u80fd\u529b\u7684\u540c\u65f6\u5b66\u4e60\u9886\u57df\u76f8\u5173\u8868\u793a\u3002", "method": "\u591a\u9636\u6bb5\u57df\u9002\u5e94\u6846\u67b6\uff0c\u8054\u5408\u4f18\u5316\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\u548c\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u9002\u5e94\u5b9e\u73b0\u7edf\u4e00\u8bad\u7ec3\u6d41\u7a0b\u3002", "result": "\u5728\u9ad8\u8d44\u6e90\u548c\u4f4e\u8d44\u6e90\u9886\u57df\u5747\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0cNDCG@10\u6307\u6807\u6700\u9ad8\u63d0\u534713.4%\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u5404\u7ec4\u4ef6\u6709\u6548\u6027\u3002", "conclusion": "\u5e73\u8861\u7684\u8054\u5408\u76d1\u7763\u548c\u5206\u9636\u6bb5\u9002\u5e94\u5bf9\u9886\u57df\u9002\u5e94\u81f3\u5173\u91cd\u8981\uff0cMOSAIC\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u53e5\u5b50\u5d4c\u5165\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u7684\u6027\u80fd\u3002"}}
{"id": "2510.16802", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16802", "abs": "https://arxiv.org/abs/2510.16802", "authors": ["Chao Li", "Yuru Wang"], "title": "Domain-Contextualized Concept Graphs: A Computable Framework for Knowledge Representation", "comment": "14 pages", "summary": "Traditional knowledge graphs are constrained by fixed ontologies that\norganize concepts within rigid hierarchical structures. The root cause lies in\ntreating domains as implicit context rather than as explicit, reasoning-level\ncomponents. To overcome these limitations, we propose the Domain-Contextualized\nConcept Graph (CDC), a novel knowledge modeling framework that elevates domains\nto first-class elements of conceptual representation. CDC adopts a C-D-C triple\nstructure - <Concept, Relation@Domain, Concept'> - where domain specifications\nserve as dynamic classification dimensions defined on demand. Grounded in a\ncognitive-linguistic isomorphic mapping principle, CDC operationalizes how\nhumans understand concepts through contextual frames. We formalize more than\ntwenty standardized relation predicates (structural, logical, cross-domain, and\ntemporal) and implement CDC in Prolog for full inference capability. Case\nstudies in education, enterprise knowledge systems, and technical documentation\ndemonstrate that CDC enables context-aware reasoning, cross-domain analogy, and\npersonalized knowledge modeling - capabilities unattainable under traditional\nontology-based frameworks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9886\u57df\u60c5\u5883\u5316\u6982\u5ff5\u56fe\uff08CDC\uff09\uff0c\u5c06\u9886\u57df\u4f5c\u4e3a\u77e5\u8bc6\u8868\u793a\u7684\u4e00\u7b49\u5143\u7d20\uff0c\u91c7\u7528<\u6982\u5ff5, \u5173\u7cfb@\u9886\u57df, \u6982\u5ff5'>\u7684\u4e09\u5143\u7ec4\u7ed3\u6784\uff0c\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u63a8\u7406\u548c\u8de8\u9886\u57df\u7c7b\u6bd4\u3002", "motivation": "\u4f20\u7edf\u77e5\u8bc6\u56fe\u8c31\u53d7\u9650\u4e8e\u56fa\u5b9a\u7684\u672c\u4f53\u8bba\uff0c\u6982\u5ff5\u88ab\u7ec4\u7ec7\u5728\u50f5\u5316\u7684\u5c42\u6b21\u7ed3\u6784\u4e2d\uff0c\u95ee\u9898\u6839\u6e90\u5728\u4e8e\u5c06\u9886\u57df\u89c6\u4e3a\u9690\u5f0f\u4e0a\u4e0b\u6587\u800c\u975e\u663e\u5f0f\u7684\u63a8\u7406\u7ea7\u7ec4\u4ef6\u3002", "method": "\u91c7\u7528C-D-C\u4e09\u5143\u7ec4\u7ed3\u6784\uff0c\u57fa\u4e8e\u8ba4\u77e5\u8bed\u8a00\u5b66\u540c\u6784\u6620\u5c04\u539f\u7406\uff0c\u5b9a\u4e49\u4e8620\u591a\u4e2a\u6807\u51c6\u5316\u5173\u7cfb\u8c13\u8bcd\uff08\u7ed3\u6784\u3001\u903b\u8f91\u3001\u8de8\u9886\u57df\u548c\u65f6\u95f4\uff09\uff0c\u5e76\u5728Prolog\u4e2d\u5b9e\u73b0\u5b8c\u6574\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u6559\u80b2\u57f9\u8bad\u3001\u4f01\u4e1a\u77e5\u8bc6\u7cfb\u7edf\u548c\u6587\u6863\u7ba1\u7406\u7b49\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cCDC\u80fd\u591f\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u63a8\u7406\u3001\u8de8\u9886\u57df\u7c7b\u6bd4\u548c\u4e2a\u6027\u5316\u77e5\u8bc6\u5efa\u6a21\uff0c\u8fd9\u4e9b\u80fd\u529b\u5728\u4f20\u7edf\u57fa\u4e8e\u672c\u4f53\u7684\u6846\u67b6\u4e2d\u65e0\u6cd5\u5b9e\u73b0\u3002", "conclusion": "CDC\u6846\u67b6\u901a\u8fc7\u5c06\u9886\u57df\u63d0\u5347\u4e3a\u6982\u5ff5\u8868\u793a\u7684\u4e00\u7b49\u5143\u7d20\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u77e5\u8bc6\u56fe\u8c31\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u4e0a\u4e0b\u6587\u611f\u77e5\u63a8\u7406\u548c\u8de8\u9886\u57df\u77e5\u8bc6\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16815", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16815", "abs": "https://arxiv.org/abs/2510.16815", "authors": ["Hans Hergen Lehmann", "Jae Hee Lee", "Steven Schockaert", "Stefan Wermter"], "title": "Knowing the Facts but Choosing the Shortcut: Understanding How Large Language Models Compare Entities", "comment": "33 pages, 20 figures. Submitted ACL ARR 2025 October (under review)", "summary": "Large Language Models (LLMs) are increasingly used for knowledge-based\nreasoning tasks, yet understanding when they rely on genuine knowledge versus\nsuperficial heuristics remains challenging. We investigate this question\nthrough entity comparison tasks by asking models to compare entities along\nnumerical attributes (e.g., ``Which river is longer, the Danube or the\nNile?''), which offer clear ground truth for systematic analysis. Despite\nhaving sufficient numerical knowledge to answer correctly, LLMs frequently make\npredictions that contradict this knowledge. We identify three heuristic biases\nthat strongly influence model predictions: entity popularity, mention order,\nand semantic co-occurrence. For smaller models, a simple logistic regression\nusing only these surface cues predicts model choices more accurately than the\nmodel's own numerical predictions, suggesting heuristics largely override\nprincipled reasoning. Crucially, we find that larger models (32B parameters)\nselectively rely on numerical knowledge when it is more reliable, while smaller\nmodels (7--8B parameters) show no such discrimination, which explains why\nlarger models outperform smaller ones even when the smaller models possess more\naccurate knowledge. Chain-of-thought prompting steers all models towards using\nthe numerical features across all model sizes.", "AI": {"tldr": "LLMs\u5728\u5b9e\u4f53\u6bd4\u8f83\u4efb\u52a1\u4e2d\u7ecf\u5e38\u4f9d\u8d56\u542f\u53d1\u5f0f\u504f\u89c1\u800c\u975e\u771f\u5b9e\u77e5\u8bc6\uff0c\u5373\u4f7f\u5177\u5907\u6b63\u786e\u6570\u503c\u77e5\u8bc6\u4e5f\u4f1a\u505a\u51fa\u9519\u8bef\u9884\u6d4b\u3002\u7814\u7a76\u53d1\u73b0\u5b9e\u4f53\u6d41\u884c\u5ea6\u3001\u63d0\u53ca\u987a\u5e8f\u548c\u8bed\u4e49\u5171\u73b0\u4e09\u79cd\u542f\u53d1\u5f0f\u504f\u89c1\u4e25\u91cd\u5f71\u54cd\u6a21\u578b\u9884\u6d4b\u3002\u5927\u6a21\u578b\u80fd\u9009\u62e9\u6027\u4f7f\u7528\u66f4\u53ef\u9760\u7684\u6570\u503c\u77e5\u8bc6\uff0c\u800c\u5c0f\u6a21\u578b\u5219\u65e0\u6cd5\u533a\u5206\uff0c\u601d\u7ef4\u94fe\u63d0\u793a\u80fd\u5f15\u5bfc\u6240\u6709\u6a21\u578b\u66f4\u597d\u5730\u4f7f\u7528\u6570\u503c\u7279\u5f81\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u77e5\u8bc6\u63a8\u7406\u4efb\u52a1\u4e2d\u4f55\u65f6\u4f9d\u8d56\u771f\u5b9e\u77e5\u8bc6\u800c\u975e\u8868\u9762\u542f\u53d1\u5f0f\uff0c\u901a\u8fc7\u5b9e\u4f53\u6bd4\u8f83\u4efb\u52a1\u5206\u6790\u6a21\u578b\u51b3\u7b56\u673a\u5236\uff0c\u7406\u89e3\u6a21\u578b\u5927\u5c0f\u5bf9\u77e5\u8bc6\u4f7f\u7528\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u5b9e\u4f53\u6bd4\u8f83\u4efb\u52a1\uff08\u5982\u6bd4\u8f83\u6cb3\u6d41\u957f\u5ea6\uff09\uff0c\u5206\u6790\u6a21\u578b\u9884\u6d4b\u4e0e\u4e09\u79cd\u542f\u53d1\u5f0f\u504f\u89c1\uff08\u5b9e\u4f53\u6d41\u884c\u5ea6\u3001\u63d0\u53ca\u987a\u5e8f\u3001\u8bed\u4e49\u5171\u73b0\uff09\u7684\u5173\u7cfb\uff0c\u6bd4\u8f83\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u7684\u8868\u73b0\uff0c\u5e76\u6d4b\u8bd5\u601d\u7ef4\u94fe\u63d0\u793a\u7684\u6548\u679c\u3002", "result": "LLMs\u7ecf\u5e38\u505a\u51fa\u4e0e\u81ea\u8eab\u6570\u503c\u77e5\u8bc6\u77db\u76fe\u7684\u9884\u6d4b\uff1b\u5c0f\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u542f\u53d1\u5f0f\u504f\u89c1\uff0c\u4ec5\u4f7f\u7528\u8868\u9762\u7ebf\u7d22\u7684\u903b\u8f91\u56de\u5f52\u6bd4\u6a21\u578b\u81ea\u8eab\u6570\u503c\u9884\u6d4b\u66f4\u51c6\u786e\uff1b\u5927\u6a21\u578b\u80fd\u9009\u62e9\u6027\u4f7f\u7528\u66f4\u53ef\u9760\u7684\u6570\u503c\u77e5\u8bc6\uff1b\u601d\u7ef4\u94fe\u63d0\u793a\u80fd\u6539\u5584\u6240\u6709\u6a21\u578b\u5bf9\u6570\u503c\u7279\u5f81\u7684\u4f7f\u7528\u3002", "conclusion": "LLMs\u5728\u77e5\u8bc6\u63a8\u7406\u4e2d\u4e25\u91cd\u4f9d\u8d56\u542f\u53d1\u5f0f\u504f\u89c1\uff0c\u6a21\u578b\u89c4\u6a21\u5f71\u54cd\u77e5\u8bc6\u4f7f\u7528\u7684\u9009\u62e9\u6027\uff0c\u601d\u7ef4\u94fe\u63d0\u793a\u662f\u6539\u5584\u6a21\u578b\u63a8\u7406\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2510.16872", "categories": ["cs.AI", "cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.16872", "abs": "https://arxiv.org/abs/2510.16872", "authors": ["Shaolei Zhang", "Ju Fan", "Meihao Fan", "Guoliang Li", "Xiaoyong Du"], "title": "DeepAnalyze: Agentic Large Language Models for Autonomous Data Science", "comment": "Code: https://github.com/ruc-datalab/DeepAnalyze Model:\n  https://huggingface.co/RUC-DataLab/DeepAnalyze-8B", "summary": "Autonomous data science, from raw data sources to analyst-grade deep research\nreports, has been a long-standing challenge, and is now becoming feasible with\nthe emergence of powerful large language models (LLMs). Recent workflow-based\ndata agents have shown promising results on specific data tasks but remain\nfundamentally limited in achieving fully autonomous data science due to their\nreliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B,\nthe first agentic LLM designed for autonomous data science, capable of\nautomatically completing the end-toend pipeline from data sources to\nanalyst-grade deep research reports. To tackle high-complexity data science\ntasks, we propose a curriculum-based agentic training paradigm that emulates\nthe learning trajectory of human data scientists, enabling LLMs to\nprogressively acquire and integrate multiple capabilities in real-world\nenvironments. We also introduce a data-grounded trajectory synthesis framework\nthat constructs high-quality training data. Through agentic training,\nDeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data\nquestion answering and specialized analytical tasks to open-ended data\nresearch. Experiments demonstrate that, with only 8B parameters, DeepAnalyze\noutperforms previous workflow-based agents built on most advanced proprietary\nLLMs. The model, code, and training data of DeepAnalyze are open-sourced,\npaving the way toward autonomous data science.", "AI": {"tldr": "DeepAnalyze-8B\u662f\u9996\u4e2a\u7528\u4e8e\u81ea\u4e3b\u6570\u636e\u79d1\u5b66\u7684\u667a\u80fdLLM\u4ee3\u7406\uff0c\u80fd\u591f\u4ece\u6570\u636e\u6e90\u5230\u5206\u6790\u5e08\u7ea7\u6df1\u5ea6\u7814\u7a76\u62a5\u544a\u81ea\u52a8\u5b8c\u6210\u7aef\u5230\u7aef\u6d41\u7a0b\uff0c\u4ec5\u752880\u4ebf\u53c2\u6570\u5c31\u8d85\u8d8a\u4e86\u57fa\u4e8e\u6700\u5148\u8fdb\u4e13\u6709LLM\u6784\u5efa\u7684\u5de5\u4f5c\u6d41\u4ee3\u7406\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5de5\u4f5c\u6d41\u7684\u6570\u636e\u4ee3\u7406\u5728\u7279\u5b9a\u6570\u636e\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7531\u4e8e\u4f9d\u8d56\u9884\u5b9a\u4e49\u5de5\u4f5c\u6d41\uff0c\u65e0\u6cd5\u5b9e\u73b0\u5b8c\u5168\u81ea\u4e3b\u7684\u6570\u636e\u79d1\u5b66\u3002\u968f\u7740\u5f3a\u5927LLM\u7684\u51fa\u73b0\uff0c\u4ece\u539f\u59cb\u6570\u636e\u6e90\u5230\u5206\u6790\u5e08\u7ea7\u6df1\u5ea6\u7814\u7a76\u62a5\u544a\u7684\u81ea\u4e3b\u6570\u636e\u79d1\u5b66\u53d8\u5f97\u53ef\u884c\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u8bfe\u7a0b\u5b66\u4e60\u7684\u667a\u80fd\u4ee3\u7406\u8bad\u7ec3\u8303\u5f0f\uff0c\u6a21\u62df\u4eba\u7c7b\u6570\u636e\u79d1\u5b66\u5bb6\u7684\u5b66\u4e60\u8f68\u8ff9\uff0c\u4f7fLLM\u80fd\u591f\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u9010\u6b65\u83b7\u53d6\u548c\u6574\u5408\u591a\u79cd\u80fd\u529b\u3002\u540c\u65f6\u5f15\u5165\u4e86\u6570\u636e\u57fa\u7840\u7684\u8f68\u8ff9\u5408\u6210\u6846\u67b6\u6765\u6784\u5efa\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDeepAnalyze-8B\u80fd\u591f\u6267\u884c\u5e7f\u6cdb\u7684\u6570\u636e\u4efb\u52a1\uff0c\u5305\u62ec\u6570\u636e\u95ee\u7b54\u3001\u4e13\u4e1a\u5206\u6790\u4efb\u52a1\u548c\u5f00\u653e\u5f0f\u6570\u636e\u7814\u7a76\uff0c\u8d85\u8d8a\u4e86\u5148\u524d\u57fa\u4e8e\u6700\u5148\u8fdb\u4e13\u6709LLM\u6784\u5efa\u7684\u5de5\u4f5c\u6d41\u4ee3\u7406\u3002", "conclusion": "DeepAnalyze-8B\u4e3a\u81ea\u4e3b\u6570\u636e\u79d1\u5b66\u5f00\u8f9f\u4e86\u9053\u8def\uff0c\u5176\u6a21\u578b\u3001\u4ee3\u7801\u548c\u8bad\u7ec3\u6570\u636e\u5747\u5df2\u5f00\u6e90\u3002"}}
{"id": "2510.16819", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16819", "abs": "https://arxiv.org/abs/2510.16819", "authors": ["Shantanu Agarwal", "Joel Barry", "Steven Fincke", "Scott Miller"], "title": "Cross-Genre Authorship Attribution via LLM-Based Retrieve-and-Rerank", "comment": null, "summary": "Authorship attribution (AA) is the task of identifying the most likely author\nof a query document from a predefined set of candidate authors. We introduce a\ntwo-stage retrieve-and-rerank framework that finetunes LLMs for cross-genre AA.\nUnlike the field of information retrieval (IR), where retrieve-and-rerank is a\nde facto strategy, cross-genre AA systems must avoid relying on topical cues\nand instead learn to identify author-specific linguistic patterns that are\nindependent of the text's subject matter (genre/domain/topic). Consequently,\nfor the reranker, we demonstrate that training strategies commonly used in IR\nare fundamentally misaligned with cross-genre AA, leading to suboptimal\nbehavior. To address this, we introduce a targeted data curation strategy that\nenables the reranker to effectively learn author-discriminative signals. Using\nour LLM-based retrieve-and-rerank pipeline, we achieve substantial gains of\n22.3 and 34.4 absolute Success@8 points over the previous state-of-the-art on\nHIATUS's challenging HRS1 and HRS2 cross-genre AA benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22-\u91cd\u6392\u6846\u67b6\u7684\u8de8\u4f53\u88c1\u4f5c\u8005\u5f52\u5c5e\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u8c03LLM\u6765\u8bc6\u522b\u4e0e\u4e3b\u9898\u65e0\u5173\u7684\u4f5c\u8005\u7279\u5b9a\u8bed\u8a00\u6a21\u5f0f\uff0c\u5728\u8de8\u4f53\u88c1\u4f5c\u8005\u5f52\u5c5e\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u4fe1\u606f\u68c0\u7d22\u65b9\u6cd5\u5728\u8de8\u4f53\u88c1\u4f5c\u8005\u5f52\u5c5e\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f9d\u8d56\u4e3b\u9898\u7ebf\u7d22\u800c\u975e\u4f5c\u8005\u7279\u5b9a\u7684\u8bed\u8a00\u6a21\u5f0f\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6709\u6548\u5b66\u4e60\u4f5c\u8005\u533a\u5206\u6027\u4fe1\u53f7\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u68c0\u7d22-\u91cd\u6392\u6846\u67b6\uff0c\u9996\u5148\u68c0\u7d22\u5019\u9009\u4f5c\u8005\uff0c\u7136\u540e\u901a\u8fc7\u4e13\u95e8\u8bbe\u8ba1\u7684\u6570\u636e\u7b56\u5c55\u7b56\u7565\u8bad\u7ec3\u91cd\u6392\u5668\u6765\u8bc6\u522b\u4f5c\u8005\u7279\u5b9a\u7684\u8bed\u8a00\u7279\u5f81\u3002", "result": "\u5728HIATUS\u7684HRS1\u548cHRS2\u8de8\u4f53\u88c1\u4f5c\u8005\u5f52\u5c5e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5206\u522b\u6bd4\u4e4b\u524d\u6700\u4f18\u65b9\u6cd5\u63d0\u9ad8\u4e8622.3\u548c34.4\u4e2a\u7edd\u5bf9Success@8\u70b9\u3002", "conclusion": "\u63d0\u51fa\u7684LLM\u68c0\u7d22-\u91cd\u6392\u7ba1\u9053\u80fd\u591f\u6709\u6548\u89e3\u51b3\u8de8\u4f53\u88c1\u4f5c\u8005\u5f52\u5c5e\u95ee\u9898\uff0c\u901a\u8fc7\u9488\u5bf9\u6027\u7684\u8bad\u7ec3\u7b56\u7565\u5b66\u4e60\u4f5c\u8005\u533a\u5206\u6027\u4fe1\u53f7\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2510.16907", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16907", "abs": "https://arxiv.org/abs/2510.16907", "authors": ["Kangrui Wang", "Pingyue Zhang", "Zihan Wang", "Yaning Gao", "Linjie Li", "Qineng Wang", "Hanyang Chen", "Chi Wan", "Yiping Lu", "Zhengyuan Yang", "Lijuan Wang", "Ranjay Krishna", "Jiajun Wu", "Li Fei-Fei", "Yejin Choi", "Manling Li"], "title": "VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents", "comment": "Accepted to NeurIPS 2025", "summary": "A key challenge in training Vision-Language Model (VLM) agents, compared to\nLanguage Model (LLM) agents, lies in the shift from textual states to complex\nvisual observations. This transition introduces partial observability and\ndemands robust world modeling. We ask: Can VLM agents construct internal world\nmodels through explicit visual state reasoning? To address this question, we\narchitecturally enforce and reward the agent's reasoning process via\nreinforcement learning (RL), formulating it as a Partially Observable Markov\nDecision Process (POMDP). We find that decomposing the agent's reasoning into\nState Estimation (\"what is the current state?\") and Transition Modeling (\"what\ncomes next?\") is critical for success, as demonstrated through five reasoning\nstrategies. Our investigation into how agents represent internal beliefs\nreveals that the optimal representation is task-dependent: Natural Language\nexcels at capturing semantic relationships in general tasks, while Structured\nformats are indispensable for precise manipulation and control. Building on\nthese insights, we design a World Modeling Reward that provides dense,\nturn-level supervision for accurate state prediction, and introduce Bi-Level\nGeneral Advantage Estimation (Bi-Level GAE) for turn-aware credit assignment.\nThrough this form of visual state reasoning, a 3B-parameter model achieves a\nscore of 0.82 across five diverse agent benchmarks, representing a 3$\\times$\nimprovement over its untrained counterpart (0.21) and outperforming proprietary\nreasoning models such as GPT-5 (0.75), Gemini 2.5 Pro (0.67) and Claude 4.5\n(0.62). All experiments are conducted within our VAGEN framework, a scalable\nsystem for training and analyzing multi-turn VLM agents in diverse visual\nenvironments. Code and data are publicly available at\nhttps://vagen-ai.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4ee3\u7406\u8fdb\u884c\u663e\u5f0f\u89c6\u89c9\u72b6\u6001\u63a8\u7406\uff0c\u6784\u5efa\u5185\u90e8\u4e16\u754c\u6a21\u578b\u3002\u901a\u8fc7\u72b6\u6001\u4f30\u8ba1\u548c\u8f6c\u79fb\u5efa\u6a21\u5206\u89e3\u63a8\u7406\u8fc7\u7a0b\uff0c\u5e76\u8bbe\u8ba1\u4e16\u754c\u5efa\u6a21\u5956\u52b1\u548c\u53cc\u5c42\u6b21\u4f18\u52bf\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u672a\u8bad\u7ec3\u6a21\u578b\u548c\u4e13\u6709\u63a8\u7406\u6a21\u578b\u3002", "motivation": "\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u9762\u4e34\u4ece\u6587\u672c\u72b6\u6001\u5230\u590d\u6742\u89c6\u89c9\u89c2\u5bdf\u7684\u8f6c\u53d8\uff0c\u8fd9\u5f15\u5165\u4e86\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u5e76\u9700\u8981\u5f3a\u5927\u7684\u4e16\u754c\u5efa\u6a21\u80fd\u529b\u3002\u7814\u7a76\u76ee\u6807\u662f\u63a2\u7d22VLM\u4ee3\u7406\u662f\u5426\u80fd\u591f\u901a\u8fc7\u663e\u5f0f\u89c6\u89c9\u72b6\u6001\u63a8\u7406\u6784\u5efa\u5185\u90e8\u4e16\u754c\u6a21\u578b\u3002", "method": "1. \u5c06\u4ee3\u7406\u63a8\u7406\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08POMDP\uff09\n2. \u5c06\u63a8\u7406\u5206\u89e3\u4e3a\u72b6\u6001\u4f30\u8ba1\uff08\u5f53\u524d\u72b6\u6001\u662f\u4ec0\u4e48\uff09\u548c\u8f6c\u79fb\u5efa\u6a21\uff08\u63a5\u4e0b\u6765\u4f1a\u53d1\u751f\u4ec0\u4e48\uff09\n3. \u8bbe\u8ba1\u4e16\u754c\u5efa\u6a21\u5956\u52b1\u63d0\u4f9b\u5bc6\u96c6\u7684\u56de\u5408\u7ea7\u76d1\u7763\n4. \u5f15\u5165\u53cc\u5c42\u6b21\u901a\u7528\u4f18\u52bf\u4f30\u8ba1\uff08Bi-Level GAE\uff09\u8fdb\u884c\u56de\u5408\u611f\u77e5\u4fe1\u7528\u5206\u914d", "result": "\u4e00\u4e2a30\u4ebf\u53c2\u6570\u7684\u6a21\u578b\u5728\u4e94\u4e2a\u4e0d\u540c\u7684\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u83b7\u5f97\u4e860.82\u7684\u5206\u6570\uff0c\u76f8\u6bd4\u672a\u8bad\u7ec3\u6a21\u578b\uff080.21\uff09\u63d0\u5347\u4e863\u500d\uff0c\u5e76\u4f18\u4e8e\u4e13\u6709\u63a8\u7406\u6a21\u578b\u5982GPT-5\uff080.75\uff09\u3001Gemini 2.5 Pro\uff080.67\uff09\u548cClaude 4.5\uff080.62\uff09\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u89c6\u89c9\u72b6\u6001\u63a8\u7406\uff0cVLM\u4ee3\u7406\u80fd\u591f\u6709\u6548\u6784\u5efa\u5185\u90e8\u4e16\u754c\u6a21\u578b\u3002\u6700\u4f18\u8868\u793a\u5f62\u5f0f\u662f\u4efb\u52a1\u4f9d\u8d56\u7684\uff1a\u81ea\u7136\u8bed\u8a00\u64c5\u957f\u6355\u6349\u4e00\u822c\u4efb\u52a1\u4e2d\u7684\u8bed\u4e49\u5173\u7cfb\uff0c\u800c\u7ed3\u6784\u5316\u683c\u5f0f\u5bf9\u4e8e\u7cbe\u786e\u64cd\u4f5c\u548c\u63a7\u5236\u81f3\u5173\u91cd\u8981\u3002\u63d0\u51fa\u7684\u65b9\u6cd5\u5728VAGEN\u6846\u67b6\u4e2d\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u7684\u591a\u56de\u5408VLM\u4ee3\u7406\u8bad\u7ec3\u3002"}}
{"id": "2510.16829", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.16829", "abs": "https://arxiv.org/abs/2510.16829", "authors": ["Navreet Kaur", "Hoda Ayad", "Hayoung Jung", "Shravika Mittal", "Munmun De Choudhury", "Tanushree Mitra"], "title": "Who's Asking? Simulating Role-Based Questions for Conversational AI Evaluation", "comment": null, "summary": "Language model users often embed personal and social context in their\nquestions. The asker's role -- implicit in how the question is framed --\ncreates specific needs for an appropriate response. However, most evaluations,\nwhile capturing the model's capability to respond, often ignore who is asking.\nThis gap is especially critical in stigmatized domains such as opioid use\ndisorder (OUD), where accounting for users' contexts is essential to provide\naccessible, stigma-free responses. We propose CoRUS (COmmunity-driven Roles for\nUser-centric Question Simulation), a framework for simulating role-based\nquestions. Drawing on role theory and posts from an online OUD recovery\ncommunity (r/OpiatesRecovery), we first build a taxonomy of asker roles --\npatients, caregivers, practitioners. Next, we use it to simulate 15,321\nquestions that embed each role's goals, behaviors, and experiences. Our\nevaluations show that these questions are both highly believable and comparable\nto real-world data. When used to evaluate five LLMs, for the same question but\ndiffering roles, we find systematic differences: vulnerable roles, such as\npatients and caregivers, elicit more supportive responses (+17%) and reduced\nknowledge content (-19%) in comparison to practitioners. Our work demonstrates\nhow implicitly signaling a user's role shapes model responses, and provides a\nmethodology for role-informed evaluation of conversational AI.", "AI": {"tldr": "CoRUS\u6846\u67b6\u901a\u8fc7\u89d2\u8272\u7406\u8bba\u6a21\u62df\u57fa\u4e8e\u89d2\u8272\u7684\u63d0\u95ee\uff0c\u53d1\u73b0\u5728\u963f\u7247\u7c7b\u836f\u7269\u4f7f\u7528\u969c\u788d\u7b49\u654f\u611f\u9886\u57df\uff0c\u7528\u6237\u89d2\u8272\u4f1a\u663e\u8457\u5f71\u54cdLLM\u7684\u56de\u590d\u5185\u5bb9\uff0c\u8106\u5f31\u89d2\u8272\uff08\u60a3\u8005\u3001\u7167\u987e\u8005\uff09\u4f1a\u83b7\u5f97\u66f4\u591a\u652f\u6301\u6027\u56de\u590d\u4f46\u77e5\u8bc6\u5185\u5bb9\u51cf\u5c11\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u5927\u591a\u5ffd\u7565\u63d0\u95ee\u8005\u89d2\u8272\uff0c\u4f46\u5728\u654f\u611f\u9886\u57df\u5982\u963f\u7247\u7c7b\u836f\u7269\u4f7f\u7528\u969c\u788d\u4e2d\uff0c\u8003\u8651\u7528\u6237\u80cc\u666f\u5bf9\u4e8e\u63d0\u4f9b\u53ef\u8bbf\u95ee\u3001\u65e0\u6c61\u540d\u5316\u7684\u56de\u590d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u57fa\u4e8e\u89d2\u8272\u7406\u8bba\u548c\u5728\u7ebfOUD\u5eb7\u590d\u793e\u533a\u5e16\u5b50\uff0c\u6784\u5efa\u63d0\u95ee\u8005\u89d2\u8272\u5206\u7c7b\uff08\u60a3\u8005\u3001\u7167\u987e\u8005\u3001\u4ece\u4e1a\u8005\uff09\uff0c\u5e76\u6a21\u62df15,321\u4e2a\u5d4c\u5165\u5404\u89d2\u8272\u76ee\u6807\u3001\u884c\u4e3a\u548c\u7ecf\u9a8c\u7684\u95ee\u9898\u3002", "result": "\u6a21\u62df\u95ee\u9898\u9ad8\u5ea6\u53ef\u4fe1\u4e14\u4e0e\u73b0\u5b9e\u6570\u636e\u76f8\u5f53\u3002\u8bc4\u4f305\u4e2aLLM\u53d1\u73b0\uff1a\u76f8\u540c\u95ee\u9898\u4f46\u4e0d\u540c\u89d2\u8272\u4f1a\u5f15\u53d1\u7cfb\u7edf\u6027\u5dee\u5f02\uff0c\u8106\u5f31\u89d2\u8272\u83b7\u5f97\u66f4\u591a\u652f\u6301\u6027\u56de\u590d\uff08+17%\uff09\u4f46\u77e5\u8bc6\u5185\u5bb9\u51cf\u5c11\uff08-19%\uff09\u3002", "conclusion": "\u7528\u6237\u89d2\u8272\u4f1a\u9690\u6027\u5730\u5f71\u54cd\u6a21\u578b\u56de\u590d\uff0c\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u4e8e\u89d2\u8272\u7684\u5bf9\u8bddAI\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2510.16956", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16956", "abs": "https://arxiv.org/abs/2510.16956", "authors": ["Mark Towers", "Yali Du", "Christopher Freeman", "Timothy J. Norman"], "title": "A Comparative User Evaluation of XRL Explanations using Goal Identification", "comment": "Accepted to ECAI 2025 Workshop on Evaluating Explainable AI and\n  Complex Decision-Making, 8 Pages", "summary": "Debugging is a core application of explainable reinforcement learning (XRL)\nalgorithms; however, limited comparative evaluations have been conducted to\nunderstand their relative performance. We propose a novel evaluation\nmethodology to test whether users can identify an agent's goal from an\nexplanation of its decision-making. Utilising the Atari's Ms. Pacman\nenvironment and four XRL algorithms, we find that only one achieved greater\nthan random accuracy for the tested goals and that users were generally\noverconfident in their selections. Further, we find that users' self-reported\nease of identification and understanding for every explanation did not\ncorrelate with their accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u6d4b\u8bd5\u7528\u6237\u662f\u5426\u80fd\u4ece\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u89e3\u91ca\u4e2d\u8bc6\u522b\u51fa\u667a\u80fd\u4f53\u7684\u76ee\u6807\uff0c\u53d1\u73b0\u5728\u6d4b\u8bd5\u7684\u56db\u79cd\u53ef\u89e3\u91ca\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4e2d\uff0c\u53ea\u6709\u4e00\u79cd\u7684\u51c6\u786e\u7387\u8d85\u8fc7\u968f\u673a\u6c34\u5e73\uff0c\u4e14\u7528\u6237\u666e\u904d\u9ad8\u4f30\u4e86\u81ea\u5df1\u7684\u9009\u62e9\u51c6\u786e\u6027\u3002", "motivation": "\u53ef\u89e3\u91ca\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u6838\u5fc3\u5e94\u7528\u4e4b\u4e00\u662f\u8c03\u8bd5\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5bf9\u5176\u76f8\u5bf9\u6027\u80fd\u7684\u6bd4\u8f83\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528Atari\u7684Ms. Pacman\u73af\u5883\u548c\u56db\u79cd\u53ef\u89e3\u91ca\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u7528\u6237\u7814\u7a76\u6d4b\u8bd5\u4ed6\u4eec\u4ece\u7b97\u6cd5\u89e3\u91ca\u4e2d\u8bc6\u522b\u667a\u80fd\u4f53\u76ee\u6807\u7684\u80fd\u529b\u3002", "result": "\u53ea\u6709\u4e00\u79cd\u7b97\u6cd5\u5728\u6d4b\u8bd5\u76ee\u6807\u4e0a\u53d6\u5f97\u4e86\u8d85\u8fc7\u968f\u673a\u6c34\u5e73\u7684\u51c6\u786e\u7387\uff1b\u7528\u6237\u666e\u904d\u5bf9\u81ea\u5df1\u7684\u9009\u62e9\u8fc7\u4e8e\u81ea\u4fe1\uff1b\u7528\u6237\u81ea\u62a5\u7684\u8bc6\u522b\u548c\u7406\u89e3\u96be\u6613\u7a0b\u5ea6\u4e0e\u4ed6\u4eec\u7684\u51c6\u786e\u7387\u4e0d\u76f8\u5173\u3002", "conclusion": "\u5f53\u524d\u7684\u53ef\u89e3\u91ca\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u5e2e\u52a9\u7528\u6237\u7406\u89e3\u667a\u80fd\u4f53\u76ee\u6807\u65b9\u9762\u6548\u679c\u6709\u9650\uff0c\u4e14\u7528\u6237\u7684\u4e3b\u89c2\u611f\u77e5\u4e0e\u5b9e\u9645\u7406\u89e3\u80fd\u529b\u5b58\u5728\u5dee\u5f02\u3002"}}
{"id": "2510.16844", "categories": ["cs.CL", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.16844", "abs": "https://arxiv.org/abs/2510.16844", "authors": ["Jiajie Jin", "Yuyao Zhang", "Yimeng Xu", "Hongjin Qian", "Yutao Zhu", "Zhicheng Dou"], "title": "FinSight: Towards Real-World Financial Deep Research", "comment": "Working in progress", "summary": "Generating professional financial reports is a labor-intensive and\nintellectually demanding process that current AI systems struggle to fully\nautomate. To address this challenge, we introduce FinSight (Financial InSight),\na novel multi agent framework for producing high-quality, multimodal financial\nreports. The foundation of FinSight is the Code Agent with Variable Memory\n(CAVM) architecture, which unifies external data, designed tools, and agents\ninto a programmable variable space, enabling flexible data collection, analysis\nand report generation through executable code. To ensure professional-grade\nvisualization, we propose an Iterative Vision-Enhanced Mechanism that\nprogressively refines raw visual outputs into polished financial charts.\nFurthermore, a two stage Writing Framework expands concise Chain-of-Analysis\nsegments into coherent, citation-aware, and multimodal reports, ensuring both\nanalytical depth and structural consistency. Experiments on various company and\nindustry-level tasks demonstrate that FinSight significantly outperforms all\nbaselines, including leading deep research systems in terms of factual\naccuracy, analytical depth, and presentation quality, demonstrating a clear\npath toward generating reports that approach human-expert quality.", "AI": {"tldr": "FinSight\u662f\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u8d22\u52a1\u62a5\u544a\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7CAVM\u67b6\u6784\u3001\u8fed\u4ee3\u89c6\u89c9\u589e\u5f3a\u673a\u5236\u548c\u4e24\u9636\u6bb5\u5199\u4f5c\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d22\u52a1\u62a5\u544a\u7684\u51c6\u786e\u6027\u3001\u5206\u6790\u6df1\u5ea6\u548c\u5448\u73b0\u8d28\u91cf\u3002", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\u96be\u4ee5\u5b8c\u5168\u81ea\u52a8\u5316\u751f\u6210\u4e13\u4e1a\u7684\u8d22\u52a1\u62a5\u544a\uff0c\u56e0\u4e3a\u8fd9\u662f\u4e00\u4e2a\u52b3\u52a8\u5bc6\u96c6\u4e14\u667a\u529b\u8981\u6c42\u9ad8\u7684\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528Code Agent with Variable Memory (CAVM)\u67b6\u6784\u7edf\u4e00\u5916\u90e8\u6570\u636e\u3001\u5de5\u5177\u548c\u667a\u80fd\u4f53\uff1b\u63d0\u51fa\u8fed\u4ee3\u89c6\u89c9\u589e\u5f3a\u673a\u5236\u9010\u6b65\u4f18\u5316\u539f\u59cb\u89c6\u89c9\u8f93\u51fa\uff1b\u4f7f\u7528\u4e24\u9636\u6bb5\u5199\u4f5c\u6846\u67b6\u5c06\u7b80\u6d01\u7684\u5206\u6790\u94fe\u6269\u5c55\u4e3a\u8fde\u8d2f\u7684\u591a\u6a21\u6001\u62a5\u544a\u3002", "result": "\u5728\u5404\u79cd\u516c\u53f8\u548c\u884c\u4e1a\u7ea7\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFinSight\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u5206\u6790\u6df1\u5ea6\u548c\u5448\u73b0\u8d28\u91cf\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u7cfb\u7edf\uff0c\u5305\u62ec\u9886\u5148\u7684\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u3002", "conclusion": "FinSight\u5c55\u793a\u4e86\u751f\u6210\u63a5\u8fd1\u4eba\u7c7b\u4e13\u5bb6\u8d28\u91cf\u62a5\u544a\u7684\u660e\u786e\u8def\u5f84\uff0c\u4e3a\u81ea\u52a8\u5316\u8d22\u52a1\u62a5\u544a\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16996", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16996", "abs": "https://arxiv.org/abs/2510.16996", "authors": ["Juncheng Dong", "Yang Yang", "Tao Liu", "Yang Wang", "Feng Qi", "Vahid Tarokh", "Kaushik Rangadurai", "Shuang Yang"], "title": "STARK: Strategic Team of Agents for Refining Kernels", "comment": null, "summary": "The efficiency of GPU kernels is central to the progress of modern AI, yet\noptimizing them remains a difficult and labor-intensive task due to complex\ninteractions between memory hierarchies, thread scheduling, and\nhardware-specific characteristics. While recent advances in large language\nmodels (LLMs) provide new opportunities for automated code generation, existing\napproaches largely treat LLMs as single-shot generators or naive refinement\ntools, limiting their effectiveness in navigating the irregular kernel\noptimization landscape. We introduce an LLM agentic framework for GPU kernel\noptimization that systematically explores the design space through multi-agent\ncollaboration, grounded instruction, dynamic context management, and strategic\nsearch. This framework mimics the workflow of expert engineers, enabling LLMs\nto reason about hardware trade-offs, incorporate profiling feedback, and refine\nkernels iteratively. We evaluate our approach on KernelBench, a benchmark for\nLLM-based kernel optimization, and demonstrate substantial improvements over\nbaseline agents: our system produces correct solutions where baselines often\nfail, and achieves kernels with up to 16x faster runtime performance. These\nresults highlight the potential of agentic LLM frameworks to advance fully\nautomated, scalable GPU kernel optimization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316GPU\u5185\u6838\u4f18\u5316\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u63a2\u7d22\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5185\u6838\u8fd0\u884c\u6027\u80fd\u3002", "motivation": "GPU\u5185\u6838\u4f18\u5316\u5bf9\u73b0\u4ee3AI\u53d1\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u5185\u5b58\u5c42\u6b21\u3001\u7ebf\u7a0b\u8c03\u5ea6\u548c\u786c\u4ef6\u7279\u6027\u7684\u590d\u6742\u4ea4\u4e92\uff0c\u4f18\u5316\u8fc7\u7a0b\u56f0\u96be\u4e14\u52b3\u52a8\u5bc6\u96c6\u3002\u73b0\u6709LLM\u65b9\u6cd5\u4e3b\u8981\u5c06\u5176\u89c6\u4e3a\u5355\u6b21\u751f\u6210\u5668\u6216\u7b80\u5355\u4f18\u5316\u5de5\u5177\uff0c\u96be\u4ee5\u5e94\u5bf9\u4e0d\u89c4\u5219\u7684\u5185\u6838\u4f18\u5316\u573a\u666f\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u7ed3\u5408\u57fa\u4e8e\u7ecf\u9a8c\u7684\u6307\u5bfc\u3001\u52a8\u6001\u4e0a\u4e0b\u6587\u7ba1\u7406\u548c\u7b56\u7565\u641c\u7d22\uff0c\u6a21\u62df\u4e13\u5bb6\u5de5\u7a0b\u5e08\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4f7fLLM\u80fd\u591f\u63a8\u7406\u786c\u4ef6\u6743\u8861\u3001\u6574\u5408\u6027\u80fd\u5206\u6790\u53cd\u9988\u5e76\u8fed\u4ee3\u4f18\u5316\u5185\u6838\u3002", "result": "\u5728KernelBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u57fa\u7ebf\u667a\u80fd\u4f53\uff0c\u8be5\u7cfb\u7edf\u5728\u57fa\u7ebf\u5931\u8d25\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u751f\u6210\u6b63\u786e\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5b9e\u73b0\u4e86\u9ad8\u8fbe16\u500d\u7684\u8fd0\u884c\u65f6\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u667a\u80fd\u4f53LLM\u6846\u67b6\u5177\u6709\u63a8\u52a8\u5b8c\u5168\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55GPU\u5185\u6838\u4f18\u5316\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.16851", "categories": ["cs.CL", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2510.16851", "abs": "https://arxiv.org/abs/2510.16851", "authors": ["Zhengqi Pei", "Qingming Huang", "Shuhui Wang"], "title": "Neuronal Group Communication for Efficient Neural representation", "comment": "28 pages, 2 figures", "summary": "The ever-increasing scale of modern neural networks has brought unprecedented\nperformance alongside daunting challenges in efficiency and interpretability.\nThis paper addresses the core question of how to build large neural systems\nthat learn efficient, modular, and interpretable representations. We propose\nNeuronal Group Communication (NGC), a theory-driven framework that reimagines a\nneural network as a dynamical system of interacting neuronal groups rather than\na monolithic collection of neural weights. Instead of treating each weight as\nan independent trainable parameter, NGC treats weights as transient\ninteractions between embedding-like neuronal states, with neural computation\nunfolding through iterative communication among groups of neurons. This\nlow-rank, modular representation yields compact models: groups of neurons\nexchange low-dimensional signals, enabling intra-group specialization and\ninter-group information sharing while dramatically reducing redundant\nparameters. By drawing on dynamical systems theory, we introduce a neuronal\nstability metric (analogous to Lyapunov stability) that quantifies the\ncontraction of neuron activations toward stable patterns during sequence\nprocessing. Using this metric, we reveal that emergent reasoning capabilities\ncorrespond to an external driving force or ``potential'', which nudges the\nneural dynamics away from trivial trajectories while preserving stability.\nEmpirically, we instantiate NGC in large language models (LLMs) and demonstrate\nimproved performance on complex reasoning benchmarks under moderate\ncompression. NGC consistently outperforms standard low-rank approximations and\ncross-layer basis-sharing methods at comparable compression rates. We conclude\nby discussing the broader implications of NGC, including how structured\nneuronal group dynamics might relate to generalization in high-dimensional\nlearning systems.", "AI": {"tldr": "\u63d0\u51fa\u795e\u7ecf\u7fa4\u901a\u4fe1(NGC)\u6846\u67b6\uff0c\u5c06\u795e\u7ecf\u7f51\u7edc\u91cd\u6784\u4e3a\u795e\u7ecf\u7fa4\u4ea4\u4e92\u7684\u52a8\u6001\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f4e\u7ef4\u901a\u4fe1\u51cf\u5c11\u53c2\u6570\u5197\u4f59\uff0c\u5728\u4fdd\u6301\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u89c4\u6a21\u6269\u5927\u5e26\u6765\u7684\u6548\u7387\u4f4e\u4e0b\u548c\u53ef\u89e3\u91ca\u6027\u5dee\u7684\u95ee\u9898\uff0c\u6784\u5efa\u80fd\u591f\u5b66\u4e60\u9ad8\u6548\u3001\u6a21\u5757\u5316\u548c\u53ef\u89e3\u91ca\u8868\u793a\u7684\u5927\u578b\u795e\u7ecf\u7cfb\u7edf\u7684\u6838\u5fc3\u95ee\u9898\u3002", "method": "\u5c06\u795e\u7ecf\u7f51\u7edc\u89c6\u4e3a\u795e\u7ecf\u7fa4\u4ea4\u4e92\u7684\u52a8\u6001\u7cfb\u7edf\uff0c\u6743\u91cd\u4f5c\u4e3a\u795e\u7ecf\u72b6\u6001\u95f4\u7684\u77ac\u65f6\u4ea4\u4e92\uff0c\u901a\u8fc7\u795e\u7ecf\u7fa4\u95f4\u7684\u8fed\u4ee3\u901a\u4fe1\u8fdb\u884c\u8ba1\u7b97\uff0c\u5f15\u5165\u795e\u7ecf\u7a33\u5b9a\u6027\u5ea6\u91cf\u6765\u91cf\u5316\u5e8f\u5217\u5904\u7406\u4e2d\u7684\u7a33\u5b9a\u6a21\u5f0f\u3002", "result": "\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u5b9e\u4f8b\u5316NGC\uff0c\u5728\u9002\u5ea6\u538b\u7f29\u4e0b\u5728\u590d\u6742\u63a8\u7406\u57fa\u51c6\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u5728\u76f8\u540c\u538b\u7f29\u7387\u4e0b\u6301\u7eed\u4f18\u4e8e\u6807\u51c6\u4f4e\u79e9\u8fd1\u4f3c\u548c\u8de8\u5c42\u57fa\u5171\u4eab\u65b9\u6cd5\u3002", "conclusion": "NGC\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u795e\u7ecf\u7fa4\u52a8\u6001\u4e0e\u9ad8\u7ef4\u5b66\u4e60\u7cfb\u7edf\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u76f8\u5173\uff0c\u4e3a\u6784\u5efa\u9ad8\u6548\u3001\u6a21\u5757\u5316\u548c\u53ef\u89e3\u91ca\u7684\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.17052", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17052", "abs": "https://arxiv.org/abs/2510.17052", "authors": ["Hassan Hamad", "Yingru Xu", "Liang Zhao", "Wenbo Yan", "Narendra Gyanchandani"], "title": "ToolCritic: Detecting and Correcting Tool-Use Errors in Dialogue Systems", "comment": null, "summary": "Tool-augmented large language models (LLMs) are increasingly employed in\nreal-world applications, but tool usage errors still hinder their reliability.\nWe introduce ToolCritic, a diagnostic framework that evaluates and improves LLM\nbehavior in multi-turn, tool-augmented dialogues. ToolCritic detects eight\ndistinct error types specific to tool-calling (e.g., premature invocation,\nargument misalignment, and misinterpretation of tool outputs) and provides\ntargeted feedback to the main LLM. The main LLM, assumed to have strong\nreasoning, task understanding and orchestration capabilities, then revises its\nresponse based on ToolCritic's feedback. We systematically define these error\ncategories and construct a synthetic dataset to train ToolCritic. Experimental\nresults on the Schema-Guided Dialogue (SGD) dataset demonstrate that ToolCritic\nimproves tool-calling accuracy by up to 13% over baselines, including zero-shot\nprompting and self-correction techniques. This represents a promising step\ntoward more robust LLM integration with external tools in real-world dialogue\napplications.", "AI": {"tldr": "ToolCritic\u662f\u4e00\u4e2a\u8bca\u65ad\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u6d4b\u5de5\u5177\u8c03\u7528\u9519\u8bef\u5e76\u63d0\u4f9b\u9488\u5bf9\u6027\u53cd\u9988\uff0c\u5e2e\u52a9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u5de5\u5177\u589e\u5f3a\u5bf9\u8bdd\u4e2d\u63d0\u9ad8\u5de5\u5177\u8c03\u7528\u51c6\u786e\u6027\u3002", "motivation": "\u5de5\u5177\u589e\u5f3a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u5b58\u5728\u5de5\u5177\u4f7f\u7528\u9519\u8bef\uff0c\u8fd9\u963b\u788d\u4e86\u5176\u53ef\u9760\u6027\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u8bc4\u4f30\u548c\u6539\u8fdbLLM\u5728\u591a\u8f6e\u5de5\u5177\u589e\u5f3a\u5bf9\u8bdd\u4e2d\u7684\u884c\u4e3a\u3002", "method": "ToolCritic\u68c0\u6d4b8\u79cd\u7279\u5b9a\u7684\u5de5\u5177\u8c03\u7528\u9519\u8bef\u7c7b\u578b\uff08\u5982\u8fc7\u65e9\u8c03\u7528\u3001\u53c2\u6570\u4e0d\u5bf9\u9f50\u3001\u5de5\u5177\u8f93\u51fa\u8bef\u89e3\u7b49\uff09\uff0c\u5e76\u4e3a\u4e3b\u8981LLM\u63d0\u4f9b\u9488\u5bf9\u6027\u53cd\u9988\u3002\u4e3b\u8981LLM\u57fa\u4e8e\u53cd\u9988\u4fee\u6b63\u5176\u54cd\u5e94\u3002", "result": "\u5728Schema-Guided Dialogue\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cToolCritic\u5c06\u5de5\u5177\u8c03\u7528\u51c6\u786e\u6027\u63d0\u9ad8\u4e8613%\uff0c\u4f18\u4e8e\u96f6\u6837\u672c\u63d0\u793a\u548c\u81ea\u6211\u4fee\u6b63\u6280\u672f\u3002", "conclusion": "ToolCritic\u4ee3\u8868\u4e86\u5728\u73b0\u5b9e\u4e16\u754c\u5bf9\u8bdd\u5e94\u7528\u4e2d\u66f4\u7a33\u5065\u5730\u96c6\u6210LLM\u4e0e\u5916\u90e8\u5de5\u5177\u7684\u6709\u5e0c\u671b\u7684\u4e00\u6b65\u3002"}}
{"id": "2510.16924", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16924", "abs": "https://arxiv.org/abs/2510.16924", "authors": ["Zhihui Yang", "Yupei Wang", "Kaijie Mo", "Zhe Zhao", "Renfen Hu"], "title": "Does Visual Grounding Enhance the Understanding of Embodied Knowledge in Large Language Models?", "comment": "Accepted to EMNLP 2025 (Findings). This version corrects a redundant\n  sentence in the Results section that appeared in the camera-ready version", "summary": "Despite significant progress in multimodal language models (LMs), it remains\nunclear whether visual grounding enhances their understanding of embodied\nknowledge compared to text-only models. To address this question, we propose a\nnovel embodied knowledge understanding benchmark based on the perceptual theory\nfrom psychology, encompassing visual, auditory, tactile, gustatory, olfactory\nexternal senses, and interoception. The benchmark assesses the models'\nperceptual abilities across different sensory modalities through vector\ncomparison and question-answering tasks with over 1,700 questions. By comparing\n30 state-of-the-art LMs, we surprisingly find that vision-language models\n(VLMs) do not outperform text-only models in either task. Moreover, the models\nperform significantly worse in the visual dimension compared to other sensory\ndimensions. Further analysis reveals that the vector representations are easily\ninfluenced by word form and frequency, and the models struggle to answer\nquestions involving spatial perception and reasoning. Our findings underscore\nthe need for more effective integration of embodied knowledge in LMs to enhance\ntheir understanding of the physical world.", "AI": {"tldr": "\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728\u5177\u8eab\u77e5\u8bc6\u7406\u89e3\u65b9\u9762\u5e76\u672a\u6bd4\u7eaf\u6587\u672c\u6a21\u578b\u8868\u73b0\u66f4\u597d\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u7ef4\u5ea6\u8868\u73b0\u6700\u5dee\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u5177\u8eab\u77e5\u8bc6\u6574\u5408\u3002", "motivation": "\u7814\u7a76\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u662f\u5426\u901a\u8fc7\u89c6\u89c9\u57fa\u7840\u589e\u5f3a\u4e86\u5bf9\u5177\u8eab\u77e5\u8bc6\u7684\u7406\u89e3\uff0c\u4e0e\u7eaf\u6587\u672c\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u3002", "method": "\u57fa\u4e8e\u5fc3\u7406\u5b66\u611f\u77e5\u7406\u8bba\u6784\u5efa\u5177\u8eab\u77e5\u8bc6\u7406\u89e3\u57fa\u51c6\uff0c\u5305\u542b\u89c6\u89c9\u3001\u542c\u89c9\u3001\u89e6\u89c9\u3001\u5473\u89c9\u3001\u55c5\u89c9\u548c\u5185\u90e8\u611f\u77e5\u7b49\u611f\u5b98\u7ef4\u5ea6\uff0c\u901a\u8fc7\u5411\u91cf\u6bd4\u8f83\u548c\u95ee\u7b54\u4efb\u52a1\u8bc4\u4f3030\u4e2a\u5148\u8fdb\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e24\u79cd\u4efb\u52a1\u4e2d\u5747\u672a\u8d85\u8d8a\u7eaf\u6587\u672c\u6a21\u578b\uff0c\u4e14\u5728\u89c6\u89c9\u7ef4\u5ea6\u8868\u73b0\u663e\u8457\u5dee\u4e8e\u5176\u4ed6\u611f\u5b98\u7ef4\u5ea6\uff1b\u5411\u91cf\u8868\u793a\u6613\u53d7\u8bcd\u5f62\u548c\u9891\u7387\u5f71\u54cd\uff0c\u6a21\u578b\u5728\u7a7a\u95f4\u611f\u77e5\u548c\u63a8\u7406\u95ee\u9898\u4e0a\u8868\u73b0\u56f0\u96be\u3002", "conclusion": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5bf9\u5177\u8eab\u77e5\u8bc6\u7684\u6574\u5408\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u589e\u5f3a\u5bf9\u7269\u7406\u4e16\u754c\u7684\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2510.17064", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17064", "abs": "https://arxiv.org/abs/2510.17064", "authors": ["Rongbin Li", "Wenbo Chen", "Zhao Li", "Rodrigo Munoz-Castaneda", "Jinbo Li", "Neha S. Maurya", "Arnav Solanki", "Huan He", "Hanwen Xing", "Meaghan Ramlakhan", "Zachary Wise", "Zhuhao Wu", "Hua Xu", "Michael Hawrylycz", "W. Jim Zheng"], "title": "A Brain Cell Type Resource Created by Large Language Models and a Multi-Agent AI System for Collaborative Community Annotation", "comment": "22 pages, 6 figures, 2 tables", "summary": "Single-cell RNA sequencing has transformed our ability to identify diverse\ncell types and their transcriptomic signatures. However, annotating these\nsignatures-especially those involving poorly characterized genes-remains a\nmajor challenge. Traditional methods, such as Gene Set Enrichment Analysis\n(GSEA), depend on well-curated annotations and often perform poorly in these\ncontexts. Large Language Models (LLMs) offer a promising alternative but\nstruggle to represent complex biological knowledge within structured\nontologies. To address this, we present BRAINCELL-AID (BRAINCELL-AID:\nhttps://biodataai.uth.edu/BRAINCELL-AID), a novel multi-agent AI system that\nintegrates free-text descriptions with ontology labels to enable more accurate\nand robust gene set annotation. By incorporating retrieval-augmented generation\n(RAG), we developed a robust agentic workflow that refines predictions using\nrelevant PubMed literature, reducing hallucinations and enhancing\ninterpretability. Using this workflow, we achieved correct annotations for 77%\nof mouse gene sets among their top predictions. Applying this approach, we\nannotated 5,322 brain cell clusters from the comprehensive mouse brain cell\natlas generated by the BRAIN Initiative Cell Census Network, enabling novel\ninsights into brain cell function by identifying region-specific gene\nco-expression patterns and inferring functional roles of gene ensembles.\nBRAINCELL-AID also identifies Basal Ganglia-related cell types with\nneurologically meaningful descriptions. Hence, we create a valuable resource to\nsupport community-driven cell type annotation.", "AI": {"tldr": "BRAINCELL-AID\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\uff0c\u901a\u8fc7\u6574\u5408\u81ea\u7531\u6587\u672c\u63cf\u8ff0\u548c\u672c\u4f53\u6807\u7b7e\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u56e0\u96c6\u6ce8\u91ca\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u6280\u672f\u867d\u7136\u80fd\u8bc6\u522b\u591a\u6837\u7ec6\u80de\u7c7b\u578b\uff0c\u4f46\u5bf9\u6d89\u53ca\u7279\u5f81\u4e0d\u660e\u786e\u57fa\u56e0\u7684\u8f6c\u5f55\u7ec4\u7279\u5f81\u6ce8\u91ca\u4ecd\u7136\u56f0\u96be\u3002\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u7cbe\u5fc3\u7b56\u5212\u7684\u6ce8\u91ca\uff0c\u5728\u590d\u6742\u751f\u7269\u77e5\u8bc6\u8868\u793a\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u5f00\u53d1\u4e86BRAINCELL-AID\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\uff0c\u6574\u5408\u81ea\u7531\u6587\u672c\u63cf\u8ff0\u4e0e\u672c\u4f53\u6807\u7b7e\uff0c\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\uff0c\u901a\u8fc7PubMed\u6587\u732e\u7cbe\u70bc\u9884\u6d4b\uff0c\u51cf\u5c11\u5e7b\u89c9\u5e76\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5bf9\u5c0f\u9f20\u57fa\u56e0\u96c6\u5b9e\u73b0\u4e8677%\u7684\u6b63\u786e\u6ce8\u91ca\u7387\uff0c\u6210\u529f\u6ce8\u91ca\u4e86BRAIN Initiative Cell Census Network\u751f\u6210\u76845,322\u4e2a\u8111\u7ec6\u80de\u7c07\uff0c\u8bc6\u522b\u4e86\u533a\u57df\u7279\u5f02\u6027\u57fa\u56e0\u5171\u8868\u8fbe\u6a21\u5f0f\uff0c\u5e76\u63a8\u65ad\u51fa\u57fa\u56e0\u96c6\u5408\u7684\u529f\u80fd\u4f5c\u7528\u3002", "conclusion": "BRAINCELL-AID\u521b\u5efa\u4e86\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u8d44\u6e90\uff0c\u652f\u6301\u793e\u533a\u9a71\u52a8\u7684\u7ec6\u80de\u7c7b\u578b\u6ce8\u91ca\uff0c\u7279\u522b\u5728\u8bc6\u522b\u57fa\u5e95\u795e\u7ecf\u8282\u76f8\u5173\u7ec6\u80de\u7c7b\u578b\u65b9\u9762\u63d0\u4f9b\u4e86\u795e\u7ecf\u5b66\u4e0a\u6709\u610f\u4e49\u7684\u63cf\u8ff0\u3002"}}
{"id": "2510.16928", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16928", "abs": "https://arxiv.org/abs/2510.16928", "authors": ["Emily Chang", "Niyati Bafna"], "title": "ChiKhaPo: A Large-Scale Multilingual Benchmark for Evaluating Lexical Comprehension and Generation in Large Language Models", "comment": null, "summary": "Existing benchmarks for large language models (LLMs) are largely restricted\nto high- or mid-resource languages, and often evaluate performance on\nhigher-order tasks in reasoning and generation. However, plenty of evidence\npoints to the fact that LLMs lack basic linguistic competence in the vast\nmajority of the world's 3800+ written languages. We introduce ChiKhaPo,\nconsisting of 8 subtasks of varying difficulty designed to evaluate the lexical\ncomprehension and generation abilities of generative models. ChiKhaPo draws on\nexisting lexicons, monolingual data, and bitext, and provides coverage for\n2700+ languages for 2 subtasks, surpassing any existing benchmark in terms of\nlanguage coverage. We further show that 6 SOTA models struggle on our\nbenchmark, and discuss the factors contributing to performance scores,\nincluding language family, language resourcedness, task, and comprehension\nversus generation directions. With ChiKhaPo, we hope to enable and encourage\nthe massively multilingual benchmarking of LLMs.", "AI": {"tldr": "ChiKhaPo\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b8\u4e2a\u96be\u5ea6\u4e0d\u540c\u7684\u5b50\u4efb\u52a1\uff0c\u65e8\u5728\u8bc4\u4f30\u751f\u6210\u6a21\u578b\u57282700\u591a\u79cd\u8bed\u8a00\u4e2d\u7684\u8bcd\u6c47\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5c40\u9650\u4e8e\u9ad8\u8d44\u6e90\u6216\u4e2d\u8d44\u6e90\u8bed\u8a00\uff0c\u4e14\u4fa7\u91cd\u4e8e\u9ad8\u9636\u63a8\u7406\u548c\u751f\u6210\u4efb\u52a1\uff0c\u4f46\u5927\u91cf\u8bc1\u636e\u8868\u660eLLMs\u5728\u5168\u74033800\u591a\u79cd\u4e66\u9762\u8bed\u8a00\u4e2d\u7f3a\u4e4f\u57fa\u672c\u8bed\u8a00\u80fd\u529b\u3002", "method": "\u5229\u7528\u73b0\u6709\u8bcd\u5178\u3001\u5355\u8bed\u6570\u636e\u548c\u53cc\u8bed\u5e73\u884c\u8bed\u6599\u6784\u5efaChiKhaPo\u57fa\u51c6\uff0c\u5305\u542b8\u4e2a\u4e0d\u540c\u96be\u5ea6\u7684\u5b50\u4efb\u52a1\uff0c\u6db5\u76d62700\u591a\u79cd\u8bed\u8a00\u3002", "result": "6\u4e2a\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u8be5\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u6027\u80fd\u5f97\u5206\u53d7\u8bed\u8a00\u5bb6\u65cf\u3001\u8bed\u8a00\u8d44\u6e90\u4e30\u5bcc\u5ea6\u3001\u4efb\u52a1\u7c7b\u578b\u4ee5\u53ca\u7406\u89e3\u4e0e\u751f\u6210\u65b9\u5411\u7b49\u56e0\u7d20\u5f71\u54cd\u3002", "conclusion": "ChiKhaPo\u65e8\u5728\u4fc3\u8fdb\u548c\u9f13\u52b1LLMs\u7684\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u5728\u8bed\u8a00\u8986\u76d6\u8303\u56f4\u4e0a\u7684\u7a7a\u767d\u3002"}}
{"id": "2510.17108", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17108", "abs": "https://arxiv.org/abs/2510.17108", "authors": ["Yoonjin Lee", "Munhee Kim", "Hanbi Choi", "Juhyeon Park", "Seungho Lyoo", "Woojin Park"], "title": "Structured Debate Improves Corporate Credit Reasoning in Financial AI", "comment": "18 pages, 4 figures, 2 algorithms, 2 tables, 4 appendices, will be\n  submitted to AAAI-2026 workshop", "summary": "Despite advances in financial AI, the automation of evidence-based reasoning\nremains unresolved in corporate credit assessment, where qualitative\nnon-financial indicators exert decisive influence on loan repayment outcomes\nyet resist formalization. Existing approaches focus predominantly on numerical\nprediction and provide limited support for the interpretive judgments required\nin professional loan evaluation. This study develops and evaluates two\noperational large language model (LLM)-based systems designed to generate\nstructured reasoning from non-financial evidence. The first is a\nnon-adversarial single-agent system (NAS) that produces bidirectional analysis\nthrough a single-pass reasoning pipeline. The second is a debate-based\nmulti-agent system (KPD-MADS) that operationalizes adversarial verification\nthrough a ten-step structured interaction protocol grounded in Karl Popper's\ncritical dialogue framework. Both systems were applied to three real corporate\ncases and evaluated by experienced credit risk professionals. Compared to\nmanual expert reporting, both systems achieved substantial productivity gains\n(NAS: 11.55 s per case; KPD-MADS: 91.97 s; human baseline: 1920 s). The\nKPD-MADS demonstrated superior reasoning quality, receiving higher median\nratings in explanatory adequacy (4.0 vs. 3.0), practical applicability (4.0 vs.\n3.0), and usability (62.5 vs. 52.5). These findings show that structured\nmulti-agent interaction can enhance reasoning rigor and interpretability in\nfinancial AI, advancing scalable and defensible automation in corporate credit\nassessment.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e24\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7cfb\u7edf\u7528\u4e8e\u4f01\u4e1a\u4fe1\u8d37\u8bc4\u4f30\u4e2d\u7684\u8bc1\u636e\u63a8\u7406\uff1a\u5355\u4ee3\u7406\u7cfb\u7edf(NAS)\u548c\u57fa\u4e8e\u8fa9\u8bba\u7684\u591a\u4ee3\u7406\u7cfb\u7edf(KPD-MADS)\uff0c\u540e\u8005\u57fa\u4e8e\u5361\u5c14\u00b7\u6ce2\u666e\u5c14\u7684\u6279\u5224\u5bf9\u8bdd\u6846\u67b6\uff0c\u5728\u63a8\u7406\u8d28\u91cf\u548c\u5b9e\u7528\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u5f53\u524d\u91d1\u878dAI\u5728\u4f01\u4e1a\u4fe1\u8d37\u8bc4\u4f30\u4e2d\u4e3b\u8981\u5173\u6ce8\u6570\u503c\u9884\u6d4b\uff0c\u7f3a\u4e4f\u5bf9\u5b9a\u6027\u975e\u8d22\u52a1\u6307\u6807\u7684\u89e3\u91ca\u6027\u5224\u65ad\u652f\u6301\uff0c\u800c\u8fd9\u4e9b\u6307\u6807\u5bf9\u8d37\u6b3e\u507f\u8fd8\u7ed3\u679c\u5177\u6709\u51b3\u5b9a\u6027\u5f71\u54cd\u3002", "method": "\u5f00\u53d1\u4e86\u4e24\u79cd\u7cfb\u7edf\uff1a\u5355\u4ee3\u7406\u7cfb\u7edf(NAS)\u901a\u8fc7\u5355\u6b21\u63a8\u7406\u751f\u6210\u53cc\u5411\u5206\u6790\uff1b\u591a\u4ee3\u7406\u7cfb\u7edf(KPD-MADS)\u91c7\u7528\u5341\u6b65\u7ed3\u6784\u5316\u4ea4\u4e92\u534f\u8bae\uff0c\u57fa\u4e8e\u5361\u5c14\u00b7\u6ce2\u666e\u5c14\u7684\u6279\u5224\u5bf9\u8bdd\u6846\u67b6\u8fdb\u884c\u5bf9\u6297\u6027\u9a8c\u8bc1\u3002", "result": "\u4e24\u4e2a\u7cfb\u7edf\u90fd\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u751f\u4ea7\u529b\u63d0\u5347(NAS: 11.55\u79d2/\u6848\u4f8b\uff1bKPD-MADS: 91.97\u79d2\uff1b\u4eba\u5de5\u57fa\u51c6: 1920\u79d2)\u3002KPD-MADS\u5728\u89e3\u91ca\u5145\u5206\u6027\u3001\u5b9e\u9645\u9002\u7528\u6027\u548c\u53ef\u7528\u6027\u65b9\u9762\u83b7\u5f97\u66f4\u9ad8\u8bc4\u5206\u3002", "conclusion": "\u7ed3\u6784\u5316\u591a\u4ee3\u7406\u4ea4\u4e92\u80fd\u591f\u589e\u5f3a\u91d1\u878dAI\u4e2d\u7684\u63a8\u7406\u4e25\u8c28\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u63a8\u52a8\u4f01\u4e1a\u4fe1\u8d37\u8bc4\u4f30\u7684\u53ef\u6269\u5c55\u548c\u53ef\u8fa9\u62a4\u81ea\u52a8\u5316\u3002"}}
{"id": "2510.16932", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16932", "abs": "https://arxiv.org/abs/2510.16932", "authors": ["Emily Xiao", "Yixiao Zeng", "Ada Chen", "Chin-Jou Li", "Amanda Bertsch", "Graham Neubig"], "title": "Prompt-MII: Meta-Learning Instruction Induction for LLMs", "comment": null, "summary": "A popular method to adapt large language models (LLMs) to new tasks is\nin-context learning (ICL), which is effective but incurs high inference costs\nas context length grows. In this paper we propose a method to perform\ninstruction induction, where we take training examples and reduce them to a\ncompact but descriptive prompt that can achieve performance comparable to ICL\nover the full training set. Specifically, we propose PROMPT-MII, a\nreinforcement learning (RL) based framework to meta-learn an instruction\ninduction model that can generate compact instructions on the fly for an\narbitrary new dataset. We train on over 3,000 diverse classification datasets\nfrom the HuggingFace hub, and evaluate on 90 unseen tasks. PROMPT-MII improves\ndownstream model quality by 4-9 F1 points (10-20% relative), matching ICL\nperformance while requiring 3-13x fewer tokens.", "AI": {"tldr": "PROMPT-MII\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u4e3a\u4efb\u610f\u65b0\u6570\u636e\u96c6\u751f\u6210\u7d27\u51d1\u6307\u4ee4\uff0c\u5728\u4fdd\u6301\u4e0e\u4e0a\u4e0b\u6587\u5b66\u4e60\u76f8\u5f53\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u63a8\u7406\u6240\u9700\u7684token\u6570\u91cf\u3002", "motivation": "\u4e0a\u4e0b\u6587\u5b66\u4e60\u867d\u7136\u6709\u6548\u4f46\u63a8\u7406\u6210\u672c\u9ad8\uff0c\u968f\u7740\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u957f\u6210\u672c\u6025\u5267\u589e\u52a0\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u80fd\u591f\u5c06\u8bad\u7ec3\u793a\u4f8b\u538b\u7f29\u4e3a\u7d27\u51d1\u4f46\u63cf\u8ff0\u6027\u5f3a\u7684\u63d0\u793a\u3002", "method": "\u63d0\u51faPROMPT-MII\u6846\u67b6\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5143\u5b66\u4e60\u6307\u4ee4\u5f52\u7eb3\u6a21\u578b\uff0c\u57283,000\u591a\u4e2a\u591a\u6837\u5316\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u80fd\u591f\u4e3a\u4efb\u610f\u65b0\u6570\u636e\u96c6\u52a8\u6001\u751f\u6210\u7d27\u51d1\u6307\u4ee4\u3002", "result": "\u572890\u4e2a\u672a\u89c1\u4efb\u52a1\u4e0a\u8bc4\u4f30\uff0cPROMPT-MII\u5c06\u4e0b\u6e38\u6a21\u578b\u8d28\u91cf\u63d0\u5347\u4e864-9\u4e2aF1\u70b9\uff08\u76f8\u5bf9\u63d0\u534710-20%\uff09\uff0c\u5728\u5339\u914d\u4e0a\u4e0b\u6587\u5b66\u4e60\u6027\u80fd\u7684\u540c\u65f6\uff0c\u6240\u9700token\u6570\u91cf\u51cf\u5c11\u4e863-13\u500d\u3002", "conclusion": "PROMPT-MII\u80fd\u591f\u6709\u6548\u66ff\u4ee3\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u63a8\u7406\u6210\u672c\uff0c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u9002\u5e94\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.17145", "categories": ["cs.AI", "68T05, 62H30"], "pdf": "https://arxiv.org/pdf/2510.17145", "abs": "https://arxiv.org/abs/2510.17145", "authors": ["Phi-Hung Hoang", "Nam-Thuan Trinh", "Van-Manh Tran", "Thi-Thu-Hong Phan"], "title": "Enhanced Fish Freshness Classification with Incremental Handcrafted Feature Fusion", "comment": "35 pages, 6 figures and 11 tables", "summary": "Accurate assessment of fish freshness remains a major challenge in the food\nindustry, with direct consequences for product quality, market value, and\nconsumer health. Conventional sensory evaluation is inherently subjective,\ninconsistent, and difficult to standardize across contexts, often limited by\nsubtle, species-dependent spoilage cues. To address these limitations, we\npropose a handcrafted feature-based approach that systematically extracts and\nincrementally fuses complementary descriptors, including color statistics,\nhistograms across multiple color spaces, and texture features such as Local\nBinary Patterns (LBP) and Gray-Level Co-occurrence Matrices (GLCM), from fish\neye images. Our method captures global chromatic variations from full images\nand localized degradations from ROI segments, fusing each independently to\nevaluate their effectiveness in assessing freshness. Experiments on the\nFreshness of the Fish Eyes (FFE) dataset demonstrate the approach's\neffectiveness: in a standard train-test setting, a LightGBM classifier achieved\n77.56% accuracy, a 14.35% improvement over the previous deep learning baseline\nof 63.21%. With augmented data, an Artificial Neural Network (ANN) reached\n97.16% accuracy, surpassing the prior best of 77.3% by 19.86%. These results\ndemonstrate that carefully engineered, handcrafted features, when strategically\nprocessed, yield a robust, interpretable, and reliable solution for automated\nfish freshness assessment, providing valuable insights for practical\napplications in food quality monitoring.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u624b\u5de5\u7279\u5f81\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u9c7c\u773c\u56fe\u50cf\u4e2d\u63d0\u53d6\u989c\u8272\u7edf\u8ba1\u3001\u591a\u8272\u5f69\u7a7a\u95f4\u76f4\u65b9\u56fe\u4ee5\u53ca\u7eb9\u7406\u7279\u5f81\u6765\u8bc4\u4f30\u9c7c\u7c7b\u65b0\u9c9c\u5ea6\uff0c\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u611f\u5b98\u8bc4\u4f30\u9c7c\u7c7b\u65b0\u9c9c\u5ea6\u5b58\u5728\u4e3b\u89c2\u6027\u3001\u4e0d\u4e00\u81f4\u6027\u548c\u96be\u4ee5\u6807\u51c6\u5316\u7684\u95ee\u9898\uff0c\u9700\u8981\u5ba2\u89c2\u53ef\u9760\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4ece\u9c7c\u773c\u56fe\u50cf\u4e2d\u7cfb\u7edf\u63d0\u53d6\u5e76\u878d\u5408\u4e92\u8865\u7279\u5f81\u63cf\u8ff0\u7b26\uff0c\u5305\u62ec\u989c\u8272\u7edf\u8ba1\u3001\u591a\u8272\u5f69\u7a7a\u95f4\u76f4\u65b9\u56fe\u3001\u5c40\u90e8\u4e8c\u503c\u6a21\u5f0f(LBP)\u548c\u7070\u5ea6\u5171\u751f\u77e9\u9635(GLCM)\u7b49\u7eb9\u7406\u7279\u5f81\uff0c\u5206\u522b\u4ece\u5b8c\u6574\u56fe\u50cf\u548c\u611f\u5174\u8da3\u533a\u57df\u6355\u83b7\u5168\u5c40\u548c\u5c40\u90e8\u53d8\u5316\u3002", "result": "\u5728FFE\u6570\u636e\u96c6\u4e0a\uff0cLightGBM\u5206\u7c7b\u5668\u8fbe\u523077.56%\u51c6\u786e\u7387\uff0c\u6bd4\u4e4b\u524d\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u63d0\u534714.35%\uff1b\u4f7f\u7528\u589e\u5f3a\u6570\u636e\u65f6\uff0cANN\u8fbe\u523097.16%\u51c6\u786e\u7387\uff0c\u6bd4\u4e4b\u524d\u6700\u4f73\u7ed3\u679c\u63d0\u534719.86%\u3002", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u624b\u5de5\u7279\u5f81\u7ecf\u8fc7\u7b56\u7565\u6027\u5904\u7406\u540e\uff0c\u80fd\u591f\u4e3a\u81ea\u52a8\u5316\u9c7c\u7c7b\u65b0\u9c9c\u5ea6\u8bc4\u4f30\u63d0\u4f9b\u9c81\u68d2\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5bf9\u98df\u54c1\u8d28\u91cf\u76d1\u63a7\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.16985", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16985", "abs": "https://arxiv.org/abs/2510.16985", "authors": ["Akif Islam", "Mohd Ruhul Ameen"], "title": "Parameter-Efficient Fine-Tuning for Low-Resource Languages: A Comparative Study of LLMs for Bengali Hate Speech Detection", "comment": "Accepted to IEEE COMPAS 2025. 6 pages, 3 figures, 6 tables", "summary": "Bengali social media platforms have witnessed a sharp increase in hate\nspeech, disproportionately affecting women and adolescents. While datasets such\nas BD-SHS provide a basis for structured evaluation, most prior approaches rely\non either computationally costly full-model fine-tuning or proprietary APIs.\nThis paper presents the first application of Parameter-Efficient Fine-Tuning\n(PEFT) for Bengali hate speech detection using LoRA and QLoRA. Three\ninstruction-tuned large language models - Gemma-3-4B, Llama-3.2-3B, and\nMistral-7B - were fine-tuned on the BD-SHS dataset of 50,281 annotated\ncomments. Each model was adapted by training fewer than 1% of its parameters,\nenabling experiments on a single consumer-grade GPU. The results show that\nLlama-3.2-3B achieved the highest F1-score of 92.23%, followed by Mistral-7B at\n88.94% and Gemma-3-4B at 80.25%. These findings establish PEFT as a practical\nand replicable strategy for Bengali and related low-resource languages.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5c06\u53c2\u6570\u9ad8\u6548\u5fae\u8c03(PEFT)\u5e94\u7528\u4e8e\u5b5f\u52a0\u62c9\u8bed\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\uff0c\u4f7f\u7528LoRA\u548cQLoRA\u6280\u672f\u5728BD-SHS\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u4e09\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\uff0cLlama-3.2-3B\u53d6\u5f97\u4e8692.23%\u7684\u6700\u9ad8F1\u5206\u6570\u3002", "motivation": "\u5b5f\u52a0\u62c9\u8bed\u793e\u4ea4\u5a92\u4f53\u4e2d\u4ec7\u6068\u8a00\u8bba\u6025\u5267\u589e\u52a0\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5987\u5973\u548c\u9752\u5c11\u5e74\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u8981\u4e48\u4f9d\u8d56\u4e13\u6709API\uff0c\u9700\u8981\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528LoRA\u548cQLoRA\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\uff0c\u5728BD-SHS\u6570\u636e\u96c6(50,281\u6761\u6807\u6ce8\u8bc4\u8bba)\u4e0a\u5fae\u8c03Gemma-3-4B\u3001Llama-3.2-3B\u548cMistral-7B\u4e09\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u8bad\u7ec3\u53c2\u6570\u5c11\u4e8e1%\u3002", "result": "Llama-3.2-3B\u83b7\u5f97\u6700\u9ad8F1\u5206\u657092.23%\uff0cMistral-7B\u4e3a88.94%\uff0cGemma-3-4B\u4e3a80.25%\u3002\u6240\u6709\u5b9e\u9a8c\u5728\u5355\u4e2a\u6d88\u8d39\u7ea7GPU\u4e0a\u5b8c\u6210\u3002", "conclusion": "PEFT\u88ab\u8bc1\u660e\u662f\u5b5f\u52a0\u62c9\u8bed\u53ca\u76f8\u5173\u4f4e\u8d44\u6e90\u8bed\u8a00\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u7684\u5b9e\u7528\u4e14\u53ef\u590d\u73b0\u7b56\u7565\u3002"}}
{"id": "2510.17146", "categories": ["cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.17146", "abs": "https://arxiv.org/abs/2510.17146", "authors": ["Subin Lin", "Chuanbo Hua"], "title": "Physics-Informed Large Language Models for HVAC Anomaly Detection with Autonomous Rule Generation", "comment": "NeurIPS 2025 Workshop of UrbanAI (Oral)", "summary": "Heating, Ventilation, and Air-Conditioning (HVAC) systems account for a\nsubstantial share of global building energy use, making reliable anomaly\ndetection essential for improving efficiency and reducing emissions. Classical\nrule-based approaches offer explainability but lack adaptability, while deep\nlearning methods provide predictive power at the cost of transparency,\nefficiency, and physical plausibility. Recent attempts to use Large Language\nModels (LLMs) for anomaly detection improve interpretability but largely ignore\nthe physical principles that govern HVAC operations. We present PILLM, a\nPhysics-Informed LLM framework that operates within an evolutionary loop to\nautomatically generate, evaluate, and refine anomaly detection rules. Our\napproach introduces physics-informed reflection and crossover operators that\nembed thermodynamic and control-theoretic constraints, enabling rules that are\nboth adaptive and physically grounded. Experiments on the public Building Fault\nDetection dataset show that PILLM achieves state-of-the-art performance while\nproducing diagnostic rules that are interpretable and actionable, advancing\ntrustworthy and deployable AI for smart building systems.", "AI": {"tldr": "PILLM\u662f\u4e00\u4e2a\u57fa\u4e8e\u7269\u7406\u77e5\u8bc6\u7684\u5927\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u8fdb\u5316\u5faa\u73af\u81ea\u52a8\u751f\u6210\u3001\u8bc4\u4f30\u548c\u4f18\u5316HVAC\u7cfb\u7edf\u5f02\u5e38\u68c0\u6d4b\u89c4\u5219\uff0c\u5728\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "HVAC\u7cfb\u7edf\u80fd\u8017\u5de8\u5927\uff0c\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u53ef\u89e3\u91ca\u4f46\u7f3a\u4e4f\u9002\u5e94\u6027\uff0c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9884\u6d4b\u80fd\u529b\u5f3a\u4f46\u7f3a\u4e4f\u900f\u660e\u5ea6\u548c\u7269\u7406\u5408\u7406\u6027\uff0c\u73b0\u6709LLM\u65b9\u6cd5\u5ffd\u89c6\u4e86HVAC\u8fd0\u884c\u7684\u7269\u7406\u539f\u7406\u3002", "method": "\u63d0\u51faPILLM\u6846\u67b6\uff0c\u5728\u8fdb\u5316\u5faa\u73af\u4e2d\u5d4c\u5165\u7269\u7406\u77e5\u8bc6\u53cd\u5c04\u548c\u4ea4\u53c9\u7b97\u5b50\uff0c\u7ed3\u5408\u70ed\u529b\u5b66\u548c\u63a7\u5236\u7406\u8bba\u7ea6\u675f\uff0c\u81ea\u52a8\u751f\u6210\u548c\u4f18\u5316\u5f02\u5e38\u68c0\u6d4b\u89c4\u5219\u3002", "result": "\u5728\u516c\u5f00\u7684\u5efa\u7b51\u6545\u969c\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cPILLM\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4ea7\u751f\u53ef\u89e3\u91ca\u548c\u53ef\u64cd\u4f5c\u7684\u8bca\u65ad\u89c4\u5219\u3002", "conclusion": "PILLM\u63a8\u52a8\u4e86\u667a\u80fd\u5efa\u7b51\u7cfb\u7edf\u4e2d\u53ef\u4fe1\u8d56\u548c\u53ef\u90e8\u7f72AI\u7684\u53d1\u5c55\uff0c\u5728\u4fdd\u6301\u7269\u7406\u57fa\u7840\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u81ea\u9002\u5e94\u5f02\u5e38\u68c0\u6d4b\u3002"}}
{"id": "2510.16987", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16987", "abs": "https://arxiv.org/abs/2510.16987", "authors": ["Amit Moryossef", "Clara Meister", "Pavel Stepachev", "Desmond Elliott"], "title": "Back to Bytes: Revisiting Tokenization Through UTF-8", "comment": null, "summary": "We present UTF8Tokenizer, a minimalist byte-level tokenizer that maps text\nexactly to IDs corresponding to the bytes underlying the text's UTF-8 encoding\n(e.g., byte x09 is token ID 9). Unlike prior byte-level approaches (Xue et al.,\n2021; Pagnoni et al., 2025), our implementation never introduces out-of-range\nIDs (i.e. there is no token ID 256) or auxiliary tokens: all special behavior\n(e.g., padding, boundaries, conversation structure, attention segments, tool\ncalling, \"thinking\" spans, etc.) is encoded using C0 control bytes - just as\nASCII was originally designed to embed control information alongside printable\ntext. These design principles yield practical benefits: (1) faster tokenization\n(14x) and significantly lower host-device transfer (8x less than int64); (2)\nsimple, shareable 256*d embedding tables that can be aligned across models; and\n(3) a training-time enhancement via bit-biased embeddings, which exposes\nper-byte bit structure and can be added to the embedding table post-training,\nremoving inference costs. Our HuggingFace-compatible implementation improves\nlanguage modeling convergence.", "AI": {"tldr": "UTF8Tokenizer\u662f\u4e00\u4e2a\u6781\u7b80\u7684\u5b57\u8282\u7ea7\u5206\u8bcd\u5668\uff0c\u76f4\u63a5\u5c06\u6587\u672c\u6620\u5c04\u5230UTF-8\u7f16\u7801\u5bf9\u5e94\u7684\u5b57\u8282ID\uff0c\u4f7f\u7528C0\u63a7\u5236\u5b57\u8282\u7f16\u7801\u7279\u6b8a\u884c\u4e3a\uff0c\u63d0\u4f9b\u66f4\u5feb\u7684\u5206\u8bcd\u901f\u5ea6\u3001\u66f4\u5c0f\u7684\u4f20\u8f93\u5f00\u9500\u548c\u53ef\u5171\u4eab\u7684\u5d4c\u5165\u8868\u3002", "motivation": "\u73b0\u6709\u7684\u5b57\u8282\u7ea7\u5206\u8bcd\u65b9\u6cd5\u5b58\u5728\u8d85\u51fa\u8303\u56f4\u7684ID\u6216\u9700\u8981\u8f85\u52a9token\u7684\u95ee\u9898\uff0c\u4f5c\u8005\u5e0c\u671b\u8bbe\u8ba1\u4e00\u4e2a\u66f4\u7b80\u5355\u9ad8\u6548\u7684\u5206\u8bcd\u5668\uff0c\u5229\u7528ASCII\u539f\u59cb\u8bbe\u8ba1\u7406\u5ff5\u5c06\u63a7\u5236\u4fe1\u606f\u5d4c\u5165\u5230\u6587\u672c\u4e2d\u3002", "method": "\u5b9e\u73b0UTF8Tokenizer\uff0c\u4f7f\u7528UTF-8\u7f16\u7801\u7684\u5b57\u8282\u4f5c\u4e3atoken ID\uff0c\u6240\u6709\u7279\u6b8a\u884c\u4e3a\uff08\u5982\u586b\u5145\u3001\u8fb9\u754c\u3001\u5bf9\u8bdd\u7ed3\u6784\u7b49\uff09\u90fd\u901a\u8fc7C0\u63a7\u5236\u5b57\u8282\u7f16\u7801\uff0c\u4e0d\u5f15\u5165\u989d\u5916token\u6216\u8d85\u51fa\u8303\u56f4\u7684ID\u3002", "result": "\u5206\u8bcd\u901f\u5ea6\u63d0\u534714\u500d\uff0c\u4e3b\u673a-\u8bbe\u5907\u4f20\u8f93\u51cf\u5c118\u500d\uff0c\u63d0\u4f9b256\u7ef4\u7684\u53ef\u5171\u4eab\u5d4c\u5165\u8868\uff0c\u901a\u8fc7\u4f4d\u504f\u7f6e\u5d4c\u5165\u5728\u8bad\u7ec3\u65f6\u589e\u5f3a\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u4e0eHuggingFace\u517c\u5bb9\u3002", "conclusion": "UTF8Tokenizer\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u5b57\u8282\u7ea7\u5206\u8bcd\u65b9\u6848\uff0c\u5728\u6027\u80fd\u548c\u5b9e\u7528\u6027\u65b9\u9762\u90fd\u6709\u663e\u8457\u4f18\u52bf\uff0c\u6539\u8fdb\u4e86\u8bed\u8a00\u5efa\u6a21\u7684\u6536\u655b\u6548\u679c\u3002"}}
{"id": "2510.17149", "categories": ["cs.AI", "I.2.11"], "pdf": "https://arxiv.org/pdf/2510.17149", "abs": "https://arxiv.org/abs/2510.17149", "authors": ["Hongyi Du", "Jiaqi Su", "Jisen Li", "Lijie Ding", "Yingxuan Yang", "Peixuan Han", "Xiangru Tang", "Kunlun Zhu", "Jiaxuan You"], "title": "Which LLM Multi-Agent Protocol to Choose?", "comment": "Under review at ICLR 2026.Code and benchmark artifacts:\n  https://github.com/ulab-uiuc/AgentProtocols", "summary": "As large-scale multi-agent systems evolve, the communication protocol layer\nhas become a critical yet under-evaluated factor shaping performance and\nreliability. Despite the existence of diverse protocols (A2A, ACP, ANP, Agora,\netc.), selection is often intuition-driven and lacks standardized guidance. We\nintroduce ProtocolBench, a benchmark that systematically compares agent\nprotocols along four measurable axes: task success, end-to-end latency, message\nor byte overhead, and robustness under failures. On ProtocolBench, protocol\nchoice significantly influences system behavior. In the Streaming Queue\nscenario, overall completion time varies by up to 36.5% across protocols, and\nmean end-to-end latency differs by 3.48 s. Under Fail-Storm Recovery,\nresilience also differs consistently across protocols. Beyond evaluation, we\npresent ProtocolRouter, a learnable protocol router that selects per-scenario\n(or per-module) protocols from requirement and runtime signals. ProtocolRouter\nreduces Fail-Storm recovery time by up to 18.1% versus the best single-protocol\nbaseline, and achieves scenario-specific gains such as higher success in GAIA.\nWe also release ProtocolRouterBench to standardize protocol evaluation and\nimprove reliability at scale.", "AI": {"tldr": "ProtocolBench\u662f\u4e00\u4e2a\u8bc4\u4f30\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u901a\u4fe1\u534f\u8bae\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0cProtocolRouter\u662f\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u534f\u8bae\u8def\u7531\u5668\uff0c\u80fd\u6839\u636e\u573a\u666f\u9700\u6c42\u9009\u62e9\u6700\u4f18\u534f\u8bae\u3002", "motivation": "\u5927\u89c4\u6a21\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\uff0c\u901a\u4fe1\u534f\u8bae\u9009\u62e9\u7f3a\u4e4f\u6807\u51c6\u5316\u6307\u5bfc\uff0c\u5f53\u524d\u9009\u62e9\u5f80\u5f80\u57fa\u4e8e\u76f4\u89c9\u800c\u975e\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u5f00\u53d1ProtocolBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ece\u4efb\u52a1\u6210\u529f\u7387\u3001\u7aef\u5230\u7aef\u5ef6\u8fdf\u3001\u6d88\u606f\u5f00\u9500\u548c\u6545\u969c\u6062\u590d\u80fd\u529b\u56db\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u534f\u8bae\u6027\u80fd\uff1b\u63d0\u51faProtocolRouter\u534f\u8bae\u8def\u7531\u5668\uff0c\u6839\u636e\u9700\u6c42\u548c\u8fd0\u884c\u65f6\u4fe1\u53f7\u9009\u62e9\u6700\u4f18\u534f\u8bae\u3002", "result": "\u4e0d\u540c\u534f\u8bae\u5728\u6027\u80fd\u4e0a\u5dee\u5f02\u663e\u8457\uff1aStreaming Queue\u573a\u666f\u5b8c\u6210\u65f6\u95f4\u5dee\u5f02\u8fbe36.5%\uff0c\u7aef\u5230\u7aef\u5ef6\u8fdf\u5dee\u5f023.48\u79d2\uff1bProtocolRouter\u76f8\u6bd4\u6700\u4f73\u5355\u534f\u8bae\u57fa\u7ebf\uff0c\u6545\u969c\u6062\u590d\u65f6\u95f4\u51cf\u5c1118.1%\uff0c\u5728GAIA\u573a\u666f\u4e2d\u6210\u529f\u7387\u66f4\u9ad8\u3002", "conclusion": "\u534f\u8bae\u9009\u62e9\u5bf9\u7cfb\u7edf\u6027\u80fd\u5f71\u54cd\u91cd\u5927\uff0cProtocolBench\u548cProtocolRouter\u4e3a\u534f\u8bae\u8bc4\u4f30\u548c\u9009\u62e9\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u5de5\u5177\uff0c\u80fd\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u53ef\u9760\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2510.17001", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17001", "abs": "https://arxiv.org/abs/2510.17001", "authors": ["Yuval Reif", "Guy Kaplan", "Roy Schwartz"], "title": "Vocab Diet: Reshaping the Vocabulary of LLMs with Vector Arithmetic", "comment": null, "summary": "Large language models (LLMs) were shown to encode word form variations, such\nas \"walk\"->\"walked\", as linear directions in embedding space. However, standard\ntokenization algorithms treat these variations as distinct tokens -- filling\nthe size-capped vocabulary with surface form variants (e.g., \"walk\", \"walking\",\n\"Walk\"), at the expense of less frequent words and multilingual coverage. We\nshow that many of these variations can be captured by transformation vectors --\nadditive offsets that yield the appropriate word's representation when applied\nto the base form word embedding -- in both the input and output spaces.\nBuilding on this, we propose a compact reshaping of the vocabulary: rather than\nassigning unique tokens to each surface form, we compose them from shared base\nform and transformation vectors (e.g., \"walked\" = \"walk\" + past tense). We\napply our approach to multiple LLMs and across five languages, removing up to\n10% of vocabulary entries -- thereby freeing space to allocate new, more\ndiverse tokens. Importantly, we do so while also expanding vocabulary coverage\nto out-of-vocabulary words, with minimal impact on downstream performance, and\nwithout modifying model weights. Our findings motivate a foundational\nrethinking of vocabulary design, moving from string enumeration to a\ncompositional vocabulary that leverages the underlying structure of language.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bcd\u6c47\u8868\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u8f6c\u6362\u5411\u91cf\u6765\u8868\u793a\u8bcd\u5f62\u53d8\u5316\uff0c\u4ece\u800c\u538b\u7f29\u8bcd\u6c47\u8868\u5927\u5c0f\uff0c\u91ca\u653e\u7a7a\u95f4\u7528\u4e8e\u66f4\u591a\u6837\u5316\u7684\u8bcd\u6c47\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u6807\u51c6\u5206\u8bcd\u7b97\u6cd5\u5c06\u8bcd\u5f62\u53d8\u5316\uff08\u5982\"walk\"->\"walked\"\uff09\u89c6\u4e3a\u72ec\u7acbtoken\uff0c\u5bfc\u81f4\u8bcd\u6c47\u8868\u88ab\u8868\u9762\u5f62\u5f0f\u53d8\u4f53\u586b\u6ee1\uff0c\u727a\u7272\u4e86\u4f4e\u9891\u8bcd\u548c\u591a\u8bed\u8a00\u8986\u76d6\u3002", "method": "\u4f7f\u7528\u8f6c\u6362\u5411\u91cf\uff08\u52a0\u6cd5\u504f\u79fb\u91cf\uff09\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u8868\u793a\u8bcd\u5f62\u53d8\u5316\uff0c\u5c06\u8bcd\u6c47\u8868\u91cd\u65b0\u8bbe\u8ba1\u4e3a\u5171\u4eab\u57fa\u7840\u5f62\u5f0f\u548c\u8f6c\u6362\u5411\u91cf\u7684\u7ec4\u5408\uff0c\u800c\u4e0d\u4fee\u6539\u6a21\u578b\u6743\u91cd\u3002", "result": "\u5728\u591a\u4e2aLLM\u548c\u4e94\u79cd\u8bed\u8a00\u4e0a\u6d4b\u8bd5\uff0c\u6700\u591a\u53ef\u79fb\u966410%\u7684\u8bcd\u6c47\u8868\u6761\u76ee\uff0c\u6269\u5c55\u4e86\u8bcd\u6c47\u8986\u76d6\u8303\u56f4\uff0c\u5bf9\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u5f71\u54cd\u6700\u5c0f\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63a8\u52a8\u8bcd\u6c47\u8868\u8bbe\u8ba1\u4ece\u5b57\u7b26\u4e32\u679a\u4e3e\u8f6c\u5411\u5229\u7528\u8bed\u8a00\u5e95\u5c42\u7ed3\u6784\u7684\u7ec4\u5408\u5f0f\u8bcd\u6c47\u8868\u3002"}}
{"id": "2510.17172", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17172", "abs": "https://arxiv.org/abs/2510.17172", "authors": ["Shun Huang", "Wenlu Xing", "Shijia Geng", "Hailong Wang", "Guangkun Nie", "Gongzheng Tang", "Chenyang He", "Shenda Hong"], "title": "Combining ECG Foundation Model and XGBoost to Predict In-Hospital Malignant Ventricular Arrhythmias in AMI Patients", "comment": null, "summary": "Malignant ventricular arrhythmias (VT/VF) following acute myocardial\ninfarction (AMI) are a major cause of in-hospital death, yet early\nidentification remains a clinical challenge. While traditional risk scores have\nlimited performance, end-to-end deep learning models often lack the\ninterpretability needed for clinical trust. This study aimed to develop a\nhybrid predictive framework that integrates a large-scale electrocardiogram\n(ECG) foundation model (ECGFounder) with an interpretable XGBoost classifier to\nimprove both accuracy and interpretability. We analyzed 6,634 ECG recordings\nfrom AMI patients, among whom 175 experienced in-hospital VT/VF. The ECGFounder\nmodel was used to extract 150-dimensional diagnostic probability features ,\nwhich were then refined through feature selection to train the XGBoost\nclassifier. Model performance was evaluated using AUC and F1-score , and the\nSHAP method was used for interpretability. The ECGFounder + XGBoost hybrid\nmodel achieved an AUC of 0.801 , outperforming KNN (AUC 0.677), RNN (AUC\n0.676), and an end-to-end 1D-CNN (AUC 0.720). SHAP analysis revealed that\nmodel-identified key features, such as \"premature ventricular complexes\" (risk\npredictor) and \"normal sinus rhythm\" (protective factor), were highly\nconsistent with clinical knowledge. We conclude that this hybrid framework\nprovides a novel paradigm for VT/VF risk prediction by validating the use of\nfoundation model outputs as effective, automated feature engineering for\nbuilding trustworthy, explainable AI-based clinical decision support systems.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408ECG\u57fa\u7840\u6a21\u578b\u548c\u53ef\u89e3\u91caXGBoost\u5206\u7c7b\u5668\u7684\u6df7\u5408\u9884\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u6025\u6027\u5fc3\u808c\u6897\u6b7b\u540e\u6076\u6027\u5ba4\u6027\u5fc3\u5f8b\u5931\u5e38\u98ce\u9669\uff0c\u5728\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u6025\u6027\u5fc3\u808c\u6897\u6b7b\u540e\u6076\u6027\u5ba4\u6027\u5fc3\u5f8b\u5931\u5e38\u662f\u9662\u5185\u6b7b\u4ea1\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u4f20\u7edf\u98ce\u9669\u8bc4\u5206\u6027\u80fd\u6709\u9650\uff0c\u800c\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7f3a\u4e4f\u4e34\u5e8a\u4fe1\u4efb\u6240\u9700\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u4f7f\u7528ECG\u57fa\u7840\u6a21\u578b\u63d0\u53d6150\u7ef4\u8bca\u65ad\u6982\u7387\u7279\u5f81\uff0c\u901a\u8fc7\u7279\u5f81\u9009\u62e9\u7cbe\u70bc\u540e\u8bad\u7ec3XGBoost\u5206\u7c7b\u5668\uff0c\u91c7\u7528SHAP\u65b9\u6cd5\u8fdb\u884c\u53ef\u89e3\u91ca\u6027\u5206\u6790\u3002", "result": "\u6df7\u5408\u6a21\u578bAUC\u8fbe\u52300.801\uff0c\u4f18\u4e8eKNN(0.677)\u3001RNN(0.676)\u548c1D-CNN(0.720)\uff0cSHAP\u5206\u6790\u663e\u793a\u6a21\u578b\u8bc6\u522b\u7279\u5f81\u4e0e\u4e34\u5e8a\u77e5\u8bc6\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u8be5\u6df7\u5408\u6846\u67b6\u901a\u8fc7\u9a8c\u8bc1\u57fa\u7840\u6a21\u578b\u8f93\u51fa\u4f5c\u4e3a\u6709\u6548\u7684\u81ea\u52a8\u5316\u7279\u5f81\u5de5\u7a0b\uff0c\u4e3a\u6784\u5efa\u53ef\u4fe1\u8d56\u3001\u53ef\u89e3\u91ca\u7684AI\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2510.17006", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17006", "abs": "https://arxiv.org/abs/2510.17006", "authors": ["Masahiro Kaneko", "Zeerak Talat", "Timothy Baldwin"], "title": "Online Learning Defense against Iterative Jailbreak Attacks via Prompt Optimization", "comment": null, "summary": "Iterative jailbreak methods that repeatedly rewrite and input prompts into\nlarge language models (LLMs) to induce harmful outputs -- using the model's\nprevious responses to guide each new iteration -- have been found to be a\nhighly effective attack strategy. Despite being an effective attack strategy\nagainst LLMs and their safety mechanisms, existing defenses do not proactively\ndisrupt this dynamic trial-and-error cycle. In this study, we propose a novel\nframework that dynamically updates its defense strategy through online learning\nin response to each new prompt from iterative jailbreak methods. Leveraging the\ndistinctions between harmful jailbreak-generated prompts and typical harmless\nprompts, we introduce a reinforcement learning-based approach that optimizes\nprompts to ensure appropriate responses for harmless tasks while explicitly\nrejecting harmful prompts. Additionally, to curb overfitting to the narrow band\nof partial input rewrites explored during an attack, we introduce\nPast-Direction Gradient Damping (PDGD). Experiments conducted on three LLMs\nshow that our approach significantly outperforms five existing defense methods\nagainst five iterative jailbreak methods. Moreover, our results indicate that\nour prompt optimization strategy simultaneously enhances response quality for\nharmless tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u52a8\u6001\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u5bf9\u6297\u8fed\u4ee3\u8d8a\u72f1\u653b\u51fb\uff0c\u540c\u65f6\u4f7f\u7528Past-Direction Gradient Damping\u9632\u6b62\u8fc7\u62df\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u65e0\u6cd5\u4e3b\u52a8\u7834\u574f\u8fed\u4ee3\u8d8a\u72f1\u653b\u51fb\u7684\u52a8\u6001\u8bd5\u9519\u5faa\u73af\uff0c\u8fd9\u4e9b\u653b\u51fb\u901a\u8fc7\u91cd\u590d\u91cd\u5199\u63d0\u793a\u8bcd\u8bf1\u5bfc\u6709\u5bb3\u8f93\u51fa\uff0c\u5229\u7528\u6a21\u578b\u5148\u524d\u54cd\u5e94\u6307\u5bfc\u65b0\u8fed\u4ee3\u3002", "method": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u52a8\u6001\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u66f4\u65b0\u9632\u5fa1\u7b56\u7565\uff0c\u4f18\u5316\u63d0\u793a\u8bcd\u4ee5\u786e\u4fdd\u65e0\u5bb3\u4efb\u52a1\u5f97\u5230\u9002\u5f53\u54cd\u5e94\uff0c\u540c\u65f6\u660e\u786e\u62d2\u7edd\u6709\u5bb3\u63d0\u793a\u8bcd\uff0c\u5e76\u5f15\u5165Past-Direction Gradient Damping\u9632\u6b62\u8fc7\u62df\u5408\u3002", "result": "\u5728\u4e09\u4e2aLLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4e94\u79cd\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u5bf9\u6297\u4e94\u79cd\u8fed\u4ee3\u8d8a\u72f1\u653b\u51fb\uff0c\u540c\u65f6\u65e0\u5bb3\u4efb\u52a1\u7684\u54cd\u5e94\u8d28\u91cf\u4e5f\u5f97\u5230\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u5bf9\u6297\u8fed\u4ee3\u8d8a\u72f1\u653b\u51fb\uff0c\u5728\u63d0\u9ad8\u5b89\u5168\u6027\u7684\u540c\u65f6\u4fdd\u6301\u5bf9\u65e0\u5bb3\u4efb\u52a1\u7684\u826f\u597d\u54cd\u5e94\u80fd\u529b\uff0c\u4e3aLLM\u5b89\u5168\u9632\u5fa1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.17173", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17173", "abs": "https://arxiv.org/abs/2510.17173", "authors": ["Melik Ozolcer", "Sang Won Bae"], "title": "Offline Policy Evaluation of Multi-Turn LLM Health Coaching with Real Users", "comment": "Accepted to the NeurIPS 2025 Workshop on Multi-Turn Interactions in\n  Large Language Models", "summary": "We study a web-deployed, tool-augmented LLM health coach with real users. In\na pilot with seven users (280 rated turns), offline policy evaluation (OPE)\nover factorized decision heads (Tool/Style) shows that a uniform heavy-tool\npolicy raises average value on logs but harms specific subgroups, most notably\nlow-health-literacy/high-self-efficacy users. A lightweight simulator with\nhidden archetypes further shows that adding a small early information-gain\nbonus reliably shortens trait identification and improves goal success and\npass@3. Together, these early findings indicate an evaluation-first path to\npersonalization: freeze the generator, learn subgroup-aware decision heads on\ntyped rewards (objective tool outcomes and satisfaction), and always report\nper-archetype metrics to surface subgroup harms that averages obscure.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5de5\u5177\u7684LLM\u5065\u5eb7\u6559\u7ec3\u7cfb\u7edf\uff0c\u901a\u8fc7\u79bb\u7ebf\u7b56\u7565\u8bc4\u4f30\u53d1\u73b0\u7edf\u4e00\u7684\u91cd\u5de5\u5177\u7b56\u7565\u867d\u7136\u63d0\u5347\u5e73\u5747\u4ef7\u503c\u4f46\u635f\u5bb3\u7279\u5b9a\u7528\u6237\u7fa4\u4f53\uff0c\u7279\u522b\u662f\u4f4e\u5065\u5eb7\u7d20\u517b/\u9ad8\u81ea\u6211\u6548\u80fd\u7528\u6237\u3002\u6a21\u62df\u5668\u5b9e\u9a8c\u8868\u660e\u6dfb\u52a0\u65e9\u671f\u4fe1\u606f\u589e\u76ca\u5956\u52b1\u53ef\u6539\u5584\u4e2a\u6027\u5316\u6548\u679c\u3002", "motivation": "\u63a2\u7d22\u5728\u771f\u5b9e\u7528\u6237\u73af\u5883\u4e2d\u90e8\u7f72\u5de5\u5177\u589e\u5f3aLLM\u5065\u5eb7\u6559\u7ec3\u7684\u4e2a\u6027\u5316\u6548\u679c\uff0c\u8bc6\u522b\u4e0d\u540c\u7528\u6237\u7fa4\u4f53\u5bf9\u5de5\u5177\u4f7f\u7528\u7b56\u7565\u7684\u5dee\u5f02\u5316\u54cd\u5e94\uff0c\u907f\u514d\u5e73\u5747\u6307\u6807\u63a9\u76d6\u7684\u4e9a\u7fa4\u635f\u5bb3\u3002", "method": "\u4f7f\u7528\u79bb\u7ebf\u7b56\u7565\u8bc4\u4f30(OPE)\u5206\u6790\u56e0\u5b50\u5316\u51b3\u7b56\u5934(\u5de5\u5177/\u98ce\u683c)\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6a21\u62df\u5668\u6d4b\u8bd5\u6dfb\u52a0\u4fe1\u606f\u589e\u76ca\u5956\u52b1\u7684\u6548\u679c\uff0c\u8bc4\u4f30\u4e0d\u540c\u7528\u6237\u539f\u578b\u5bf9\u7b56\u7565\u7684\u54cd\u5e94\u3002", "result": "\u7edf\u4e00\u91cd\u5de5\u5177\u7b56\u7565\u63d0\u5347\u5e73\u5747\u4ef7\u503c\u4f46\u635f\u5bb3\u4f4e\u5065\u5eb7\u7d20\u517b/\u9ad8\u81ea\u6211\u6548\u80fd\u7528\u6237\uff1b\u6dfb\u52a0\u65e9\u671f\u4fe1\u606f\u589e\u76ca\u5956\u52b1\u53ef\u7f29\u77ed\u7279\u5f81\u8bc6\u522b\u65f6\u95f4\uff0c\u63d0\u9ad8\u76ee\u6807\u6210\u529f\u7387\u548cpass@3\u6307\u6807\u3002", "conclusion": "\u63d0\u51fa\u8bc4\u4f30\u4f18\u5148\u7684\u4e2a\u6027\u5316\u8def\u5f84\uff1a\u51bb\u7ed3\u751f\u6210\u5668\uff0c\u5728\u7c7b\u578b\u5316\u5956\u52b1\u4e0a\u5b66\u4e60\u4e9a\u7fa4\u611f\u77e5\u51b3\u7b56\u5934\uff0c\u59cb\u7ec8\u62a5\u544a\u6309\u539f\u578b\u5206\u7c7b\u7684\u6307\u6807\u4ee5\u63ed\u793a\u5e73\u5747\u6307\u6807\u63a9\u76d6\u7684\u4e9a\u7fa4\u635f\u5bb3\u3002"}}
{"id": "2510.17013", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17013", "abs": "https://arxiv.org/abs/2510.17013", "authors": ["Lanni Bu", "Lauren Levin", "Amir Zeldes"], "title": "DiscoTrack: A Multilingual LLM Benchmark for Discourse Tracking", "comment": null, "summary": "Recent LLM benchmarks have tested models on a range of phenomena, but are\nstill focused primarily on natural language understanding for extraction of\nexplicit information, such as QA or summarization, with responses often tar-\ngeting information from individual sentences. We are still lacking more\nchallenging, and im- portantly also multilingual, benchmarks focus- ing on\nimplicit information and pragmatic infer- ences across larger documents in the\ncontext of discourse tracking: integrating and aggregating information across\nsentences, paragraphs and multiple speaker utterances. To this end, we present\nDiscoTrack, an LLM benchmark target- ing a range of tasks across 12 languages\nand four levels of discourse understanding: salience recognition, entity\ntracking, discourse relations and bridging inference. Our evaluation shows that\nthese tasks remain challenging, even for state-of-the-art models.", "AI": {"tldr": "\u63d0\u51fa\u4e86DiscoTrack\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u572812\u79cd\u8bed\u8a00\u4e2d\u7684\u8bed\u7bc7\u7406\u89e3\u80fd\u529b\uff0c\u5305\u62ec\u663e\u7740\u6027\u8bc6\u522b\u3001\u5b9e\u4f53\u8ffd\u8e2a\u3001\u8bed\u7bc7\u5173\u7cfb\u548c\u6865\u63a5\u63a8\u7406\u56db\u4e2a\u5c42\u6b21\u3002", "motivation": "\u73b0\u6709LLM\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4e2d\u7684\u663e\u6027\u4fe1\u606f\u63d0\u53d6\uff0c\u7f3a\u4e4f\u9488\u5bf9\u9690\u6027\u4fe1\u606f\u548c\u8bed\u7528\u63a8\u7406\u7684\u591a\u8bed\u8a00\u6311\u6218\u6027\u57fa\u51c6\uff0c\u7279\u522b\u662f\u5728\u8de8\u53e5\u5b50\u3001\u6bb5\u843d\u548c\u591a\u4e2a\u8bf4\u8bdd\u8005\u8bdd\u8bed\u7684\u8bed\u7bc7\u8ffd\u8e2a\u65b9\u9762\u3002", "method": "\u5f00\u53d1\u4e86DiscoTrack\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d612\u79cd\u8bed\u8a00\uff0c\u5305\u542b\u56db\u4e2a\u5c42\u6b21\u7684\u8bed\u7bc7\u7406\u89e3\u4efb\u52a1\uff1a\u663e\u7740\u6027\u8bc6\u522b\u3001\u5b9e\u4f53\u8ffd\u8e2a\u3001\u8bed\u7bc7\u5173\u7cfb\u548c\u6865\u63a5\u63a8\u7406\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u8fd9\u4e9b\u4efb\u52a1\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "conclusion": "DiscoTrack\u586b\u8865\u4e86\u591a\u8bed\u8a00\u8bed\u7bc7\u7406\u89e3\u57fa\u51c6\u7684\u7a7a\u767d\uff0c\u4e3a\u8bc4\u4f30LLM\u5728\u590d\u6742\u8bed\u7bc7\u5904\u7406\u80fd\u529b\u65b9\u9762\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2510.17211", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17211", "abs": "https://arxiv.org/abs/2510.17211", "authors": ["Tingsong Xiao", "Yao An Lee", "Zelin Xu", "Yupu Zhang", "Zibo Liu", "Yu Huang", "Jiang Bian", "Serena Jingchuan Guo", "Zhe Jiang"], "title": "Temporally Detailed Hypergraph Neural ODEs for Type 2 Diabetes Progression Modeling", "comment": null, "summary": "Disease progression modeling aims to characterize and predict how a patient's\ndisease complications worsen over time based on longitudinal electronic health\nrecords (EHRs). Accurate modeling of disease progression, such as type 2\ndiabetes, can enhance patient sub-phenotyping and inform effective and timely\ninterventions. However, the problem is challenging due to the need to learn\ncontinuous-time dynamics of progression patterns based on irregular-time event\nsamples and patient heterogeneity (\\eg different progression rates and\npathways). Existing mechanistic and data-driven methods either lack\nadaptability to learn from real-world data or fail to capture complex\ncontinuous-time dynamics on progression trajectories. To address these\nlimitations, we propose Temporally Detailed Hypergraph Neural Ordinary\nDifferential Equation (TD-HNODE), which represents disease progression on\nclinically recognized trajectories as a temporally detailed hypergraph and\nlearns the continuous-time progression dynamics via a neural ODE framework.\nTD-HNODE contains a learnable TD-Hypergraph Laplacian that captures the\ninterdependency of disease complication markers within both intra- and\ninter-progression trajectories. Experiments on two real-world clinical datasets\ndemonstrate that TD-HNODE outperforms multiple baselines in modeling the\nprogression of type 2 diabetes and related cardiovascular diseases.", "AI": {"tldr": "\u63d0\u51faTD-HNODE\u6a21\u578b\uff0c\u4f7f\u7528\u65f6\u95f4\u8be6\u7ec6\u8d85\u56fe\u548c\u795e\u7ecfODE\u6846\u67b6\u6765\u5efa\u6a21\u75be\u75c5\u8fdb\u5c55\u7684\u8fde\u7eed\u65f6\u95f4\u52a8\u6001\uff0c\u57282\u578b\u7cd6\u5c3f\u75c5\u548c\u5fc3\u8840\u7ba1\u75be\u75c5\u8fdb\u5c55\u5efa\u6a21\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406\u57fa\u4e8e\u4e0d\u89c4\u5219\u65f6\u95f4\u4e8b\u4ef6\u6837\u672c\u7684\u60a3\u8005\u5f02\u8d28\u6027\u548c\u590d\u6742\u8fde\u7eed\u65f6\u95f4\u52a8\u6001\uff0c\u9700\u8981\u66f4\u51c6\u786e\u7684\u75be\u75c5\u8fdb\u5c55\u5efa\u6a21\u6765\u6539\u5584\u60a3\u8005\u4e9a\u5206\u578b\u548c\u5e72\u9884\u65f6\u673a\u3002", "method": "TD-HNODE\u5c06\u75be\u75c5\u8fdb\u5c55\u8868\u793a\u4e3a\u65f6\u95f4\u8be6\u7ec6\u8d85\u56fe\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u8d85\u56fe\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u6355\u83b7\u8fdb\u5c55\u8f68\u8ff9\u5185\u548c\u8f68\u8ff9\u95f4\u7684\u75be\u75c5\u5e76\u53d1\u75c7\u6807\u8bb0\u76f8\u4e92\u4f9d\u8d56\u5173\u7cfb\uff0c\u4f7f\u7528\u795e\u7ecfODE\u6846\u67b6\u5b66\u4e60\u8fde\u7eed\u65f6\u95f4\u8fdb\u5c55\u52a8\u6001\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTD-HNODE\u57282\u578b\u7cd6\u5c3f\u75c5\u548c\u76f8\u5173\u5fc3\u8840\u7ba1\u75be\u75c5\u8fdb\u5c55\u5efa\u6a21\u65b9\u9762\u4f18\u4e8e\u591a\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "TD-HNODE\u80fd\u591f\u6709\u6548\u5efa\u6a21\u75be\u75c5\u8fdb\u5c55\u7684\u8fde\u7eed\u65f6\u95f4\u52a8\u6001\uff0c\u4e3a\u60a3\u8005\u4e9a\u5206\u578b\u548c\u53ca\u65f6\u5e72\u9884\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u5de5\u5177\u3002"}}
{"id": "2510.17017", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17017", "abs": "https://arxiv.org/abs/2510.17017", "authors": ["Qiusi Zhan", "Angeline Budiman-Chan", "Abdelrahman Zayed", "Xingzhi Guo", "Daniel Kang", "Joo-Kyung Kim"], "title": "SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents", "comment": "Code: https://github.com/ZQS1943/SafeSearch", "summary": "Large language model (LLM) based search agents iteratively generate queries,\nretrieve external information, and reason to answer open-domain questions.\nWhile researchers have primarily focused on improving their utility, their\nsafety behaviors remain underexplored. In this paper, we first evaluate search\nagents using red-teaming datasets and find that they are more likely to produce\nharmful outputs than base LLMs. For example, when asked ``How can I track\nsomeone's location without their consent?'', a base model refuses, whereas a\nsearch agent designed to retrieve and cite sources may lower its refusal\nthreshold, fetch documents (e.g., court cases), and, once appended, synthesize\nthem into an informative yet unsafe summary. We further show that\nutility-oriented fine-tuning intensifies this risk, motivating joint alignment\nof safety and utility. We present SafeSearch, a multi-objective reinforcement\nlearning approach that couples a final-output safety/utility reward with a\nnovel query-level shaping term that penalizes unsafe queries and rewards safe\nones. Experiments show that SafeSearch reduces agent harmfulness by over 70%\nacross three red-teaming datasets while producing safe, helpful responses, and\nmatches the QA performance of a utility-only finetuned agent; further analyses\nconfirm the effectiveness of the query-level reward in jointly improving safety\nand utility.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u641c\u7d22\u4ee3\u7406\u6bd4\u57fa\u7840LLM\u66f4\u5bb9\u6613\u4ea7\u751f\u6709\u5bb3\u8f93\u51fa\uff0c\u63d0\u51fa\u4e86SafeSearch\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u6548\u7528\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u641c\u7d22\u4ee3\u7406\u7684\u6548\u7528\u63d0\u5347\uff0c\u4f46\u5176\u5b89\u5168\u884c\u4e3a\u7814\u7a76\u4e0d\u8db3\u3002\u7814\u7a76\u53d1\u73b0\u641c\u7d22\u4ee3\u7406\u5728\u5904\u7406\u6709\u5bb3\u95ee\u9898\u65f6\u6bd4\u57fa\u7840\u6a21\u578b\u66f4\u5bb9\u6613\u4ea7\u751f\u4e0d\u5b89\u5168\u8f93\u51fa\uff0c\u9700\u8981\u540c\u65f6\u8003\u8651\u5b89\u5168\u6027\u548c\u6548\u7528\u7684\u8054\u5408\u5bf9\u9f50\u3002", "method": "\u63d0\u51faSafeSearch\u65b9\u6cd5\uff0c\u4f7f\u7528\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\uff0c\u7ed3\u5408\u6700\u7ec8\u8f93\u51fa\u7684\u5b89\u5168/\u6548\u7528\u5956\u52b1\u548c\u67e5\u8be2\u7ea7\u522b\u7684\u5851\u9020\u9879\uff0c\u5bf9\u4e0d\u5b89\u5168\u67e5\u8be2\u8fdb\u884c\u60e9\u7f5a\uff0c\u5bf9\u5b89\u5168\u67e5\u8be2\u8fdb\u884c\u5956\u52b1\u3002", "result": "\u5b9e\u9a8c\u663e\u793aSafeSearch\u5728\u4e09\u4e2a\u7ea2\u961f\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u51cf\u5c11\u4ee3\u7406\u6709\u5bb3\u6027\u8d85\u8fc770%\uff0c\u540c\u65f6\u4ea7\u751f\u5b89\u5168\u6709\u7528\u7684\u54cd\u5e94\uff0c\u5728QA\u6027\u80fd\u4e0a\u4e0e\u4ec5\u4f18\u5316\u6548\u7528\u7684\u5fae\u8c03\u4ee3\u7406\u76f8\u5f53\u3002", "conclusion": "\u67e5\u8be2\u7ea7\u522b\u7684\u5956\u52b1\u5728\u8054\u5408\u6539\u8fdb\u5b89\u5168\u6027\u548c\u6548\u7528\u65b9\u9762\u6709\u6548\uff0cSafeSearch\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u641c\u7d22\u4ee3\u7406\u7684\u5b89\u5168\u6027\u548c\u6548\u7528\u7684\u5e73\u8861\u3002"}}
{"id": "2510.17235", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17235", "abs": "https://arxiv.org/abs/2510.17235", "authors": ["Chong Chen", "Ze Liu", "Lingfeng Bao", "Yanlin Wang", "Ting Chen", "Daoyuan Wu", "Jiachi Chen"], "title": "Coinvisor: An RL-Enhanced Chatbot Agent for Interactive Cryptocurrency Investment Analysis", "comment": null, "summary": "The cryptocurrency market offers significant investment opportunities but\nfaces challenges including high volatility and fragmented information. Data\nintegration and analysis are essential for informed investment decisions.\nCurrently, investors use three main approaches: (1) Manual analysis across\nvarious sources, which depends heavily on individual experience and is\ntime-consuming and prone to bias; (2) Data aggregation platforms-limited in\nfunctionality and depth of analysis; (3) Large language model agents-based on\nstatic pretrained models, lacking real-time data integration and multi-step\nreasoning capabilities. To address these limitations, we present Coinvisor, a\nreinforcement learning-based chatbot that provides comprehensive analytical\nsupport for cryptocurrency investment through a multi-agent framework.\nCoinvisor integrates diverse analytical capabilities through specialized tools.\nIts key innovation is a reinforcement learning-based tool selection mechanism\nthat enables multi-step planning and flexible integration of diverse data\nsources. This design supports real-time interaction and adaptive analysis of\ndynamic content, delivering accurate and actionable investment insights. We\nevaluated Coinvisor through automated benchmarks on tool calling accuracy and\nuser studies with 20 cryptocurrency investors using our interface. Results show\nthat Coinvisor improves recall by 40.7% and F1 score by 26.6% over the base\nmodel in tool orchestration. User studies show high satisfaction (4.64/5), with\nparticipants preferring Coinvisor to both general LLMs and existing crypto\nplatforms (4.62/5).", "AI": {"tldr": "Coinvisor\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u52a0\u5bc6\u8d27\u5e01\u6295\u8d44\u5206\u6790\u804a\u5929\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u6846\u67b6\u63d0\u4f9b\u5168\u9762\u7684\u5206\u6790\u652f\u6301\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u52a0\u5bc6\u8d27\u5e01\u5e02\u573a\u5b58\u5728\u9ad8\u6ce2\u52a8\u6027\u548c\u4fe1\u606f\u788e\u7247\u5316\u95ee\u9898\uff0c\u73b0\u6709\u5206\u6790\u65b9\u6cd5\u5305\u62ec\u624b\u52a8\u5206\u6790\u3001\u6570\u636e\u805a\u5408\u5e73\u53f0\u548cLLM\u4ee3\u7406\u90fd\u5b58\u5728\u529f\u80fd\u9650\u5236\uff0c\u7f3a\u4e4f\u5b9e\u65f6\u6570\u636e\u96c6\u6210\u548c\u591a\u6b65\u63a8\u7406\u80fd\u529b\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5de5\u5177\u9009\u62e9\u673a\u5236\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u6846\u67b6\u96c6\u6210\u591a\u6837\u5316\u5206\u6790\u80fd\u529b\uff0c\u652f\u6301\u591a\u6b65\u89c4\u5212\u548c\u7075\u6d3b\u7684\u6570\u636e\u6e90\u6574\u5408\u3002", "result": "\u5728\u5de5\u5177\u7f16\u6392\u65b9\u9762\uff0c\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u53ec\u56de\u7387\u63d0\u534740.7%\uff0cF1\u5206\u6570\u63d0\u534726.6%\u3002\u7528\u6237\u7814\u7a76\u663e\u793a\u9ad8\u6ee1\u610f\u5ea6(4.64/5)\uff0c\u53c2\u4e0e\u8005\u66f4\u504f\u597dCoinvisor\u800c\u975e\u901a\u7528LLM\u548c\u73b0\u6709\u52a0\u5bc6\u5e73\u53f0(4.62/5)\u3002", "conclusion": "Coinvisor\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5b9e\u65f6\u4ea4\u4e92\u548c\u52a8\u6001\u5185\u5bb9\u7684\u81ea\u9002\u5e94\u5206\u6790\uff0c\u4e3a\u52a0\u5bc6\u8d27\u5e01\u6295\u8d44\u63d0\u4f9b\u4e86\u51c6\u786e\u53ef\u884c\u7684\u89c1\u89e3\u3002"}}
{"id": "2510.17018", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17018", "abs": "https://arxiv.org/abs/2510.17018", "authors": ["Noor Islam S. Mohammad"], "title": "Extended LSTM: Adaptive Feature Gating for Toxic Comment Classification", "comment": null, "summary": "Toxic comment detection remains a challenging task, where transformer-based\nmodels (e.g., BERT) incur high computational costs and degrade on minority\ntoxicity classes, while classical ensembles lack semantic adaptability. We\npropose xLSTM, a parameter-efficient and theoretically grounded framework that\nunifies cosine-similarity gating, adaptive feature prioritization, and\nprincipled class rebalancing. A learnable reference vector {v} in {R}^d\nmodulates contextual embeddings via cosine similarity, amplifying toxic cues\nand attenuating benign signals to yield stronger gradients under severe class\nimbalance. xLSTM integrates multi-source embeddings (GloVe, FastText, BERT CLS)\nthrough a projection layer, a character-level BiLSTM for morphological cues,\nembedding-space SMOTE for minority augmentation, and adaptive focal loss with\ndynamic class weighting. On the Jigsaw Toxic Comment benchmark, xLSTM attains\n96.0% accuracy and 0.88 macro-F1, outperforming BERT by 33% on threat and 28%\non identity_hate categories, with 15 times fewer parameters and 50ms inference\nlatency. Cosine gating contributes a +4.8% F1 gain in ablations. The results\nestablish a new efficiency adaptability frontier, demonstrating that\nlightweight, theoretically informed architectures can surpass large pretrained\nmodels on imbalanced, domain-specific NLP tasks.", "AI": {"tldr": "\u63d0\u51faxLSTM\u6846\u67b6\uff0c\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u95e8\u63a7\u3001\u81ea\u9002\u5e94\u7279\u5f81\u4f18\u5148\u7ea7\u548c\u7c7b\u522b\u91cd\u5e73\u8861\uff0c\u5728\u6bd2\u6027\u8bc4\u8bba\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u5206\u7c7b\uff0c\u8d85\u8d8aBERT\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8etransformer\u7684\u6a21\u578b\u5728\u6bd2\u6027\u8bc4\u8bba\u68c0\u6d4b\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u5728\u5c11\u6570\u6bd2\u6027\u7c7b\u522b\u4e0a\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u4f20\u7edf\u96c6\u6210\u65b9\u6cd5\u7f3a\u4e4f\u8bed\u4e49\u9002\u5e94\u6027\u7684\u5c40\u9650\u3002", "method": "\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u53c2\u8003\u5411\u91cf\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u8c03\u5236\u4e0a\u4e0b\u6587\u5d4c\u5165\uff0c\u96c6\u6210\u591a\u6e90\u5d4c\u5165\uff08GloVe\u3001FastText\u3001BERT CLS\uff09\uff0c\u5305\u542b\u5b57\u7b26\u7ea7BiLSTM\u3001\u5d4c\u5165\u7a7a\u95f4SMOTE\u548c\u81ea\u9002\u5e94\u7126\u70b9\u635f\u5931\u3002", "result": "\u5728Jigsaw\u6bd2\u6027\u8bc4\u8bba\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523096.0%\u51c6\u786e\u7387\u548c0.88\u5b8fF1\uff0c\u5728\u5a01\u80c1\u548c\u8eab\u4efd\u4ec7\u6068\u7c7b\u522b\u4e0a\u5206\u522b\u6bd4BERT\u63d0\u534733%\u548c28%\uff0c\u53c2\u6570\u91cf\u51cf\u5c1115\u500d\uff0c\u63a8\u7406\u5ef6\u8fdf50ms\u3002", "conclusion": "xLSTM\u5728\u6548\u7387\u548c\u9002\u5e94\u6027\u65b9\u9762\u5efa\u7acb\u4e86\u65b0\u7684\u524d\u6cbf\uff0c\u8bc1\u660e\u8f7b\u91cf\u7ea7\u3001\u7406\u8bba\u9a71\u52a8\u7684\u67b6\u6784\u53ef\u4ee5\u5728\u4e0d\u5e73\u8861\u3001\u9886\u57df\u7279\u5b9a\u7684NLP\u4efb\u52a1\u4e2d\u8d85\u8d8a\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u3002"}}
{"id": "2510.17309", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17309", "abs": "https://arxiv.org/abs/2510.17309", "authors": ["Thorsten Fr\u00f6hlich", "Tim Schlippe"], "title": "RubiSCoT: A Framework for AI-Supported Academic Assessment", "comment": null, "summary": "The evaluation of academic theses is a cornerstone of higher education,\nensuring rigor and integrity. Traditional methods, though effective, are\ntime-consuming and subject to evaluator variability. This paper presents\nRubiSCoT, an AI-supported framework designed to enhance thesis evaluation from\nproposal to final submission. Using advanced natural language processing\ntechniques, including large language models, retrieval-augmented generation,\nand structured chain-of-thought prompting, RubiSCoT offers a consistent,\nscalable solution. The framework includes preliminary assessments,\nmultidimensional assessments, content extraction, rubric-based scoring, and\ndetailed reporting. We present the design and implementation of RubiSCoT,\ndiscussing its potential to optimize academic assessment processes through\nconsistent, scalable, and transparent evaluation.", "AI": {"tldr": "\u63d0\u51fa\u4e86RubiSCoT\u6846\u67b6\uff0c\u5229\u7528AI\u6280\u672f\u589e\u5f3a\u5b66\u672f\u8bba\u6587\u8bc4\u4f30\u8fc7\u7a0b\uff0c\u4ece\u63d0\u6848\u5230\u6700\u7ec8\u63d0\u4ea4\u63d0\u4f9b\u4e00\u81f4\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4f20\u7edf\u8bba\u6587\u8bc4\u4f30\u65b9\u6cd5\u8017\u65f6\u4e14\u5b58\u5728\u8bc4\u4f30\u8005\u4e3b\u89c2\u6027\u5dee\u5f02\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u4e00\u81f4\u7684\u8bc4\u4f30\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u5148\u8fdb\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\uff0c\u5305\u62ec\u5927\u8bed\u8a00\u6a21\u578b\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\u63d0\u793a\uff0c\u63d0\u4f9b\u521d\u6b65\u8bc4\u4f30\u3001\u591a\u7ef4\u5ea6\u8bc4\u4f30\u3001\u5185\u5bb9\u63d0\u53d6\u3001\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u7684\u8bc4\u5206\u548c\u8be6\u7ec6\u62a5\u544a\u3002", "result": "\u8bbe\u8ba1\u4e86RubiSCoT\u6846\u67b6\u5e76\u5b9e\u73b0\u4e86\u5176\u529f\u80fd\uff0c\u5c55\u793a\u4e86\u5728\u5b66\u672f\u8bc4\u4f30\u8fc7\u7a0b\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "RubiSCoT\u6846\u67b6\u6709\u6f5c\u529b\u901a\u8fc7\u63d0\u4f9b\u4e00\u81f4\u3001\u53ef\u6269\u5c55\u548c\u900f\u660e\u7684\u8bc4\u4f30\u6765\u4f18\u5316\u5b66\u672f\u8bc4\u4f30\u6d41\u7a0b\u3002"}}
{"id": "2510.17028", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17028", "abs": "https://arxiv.org/abs/2510.17028", "authors": ["Kyle Cox", "Jiawei Xu", "Yikun Han", "Rong Xu", "Tianhao Li", "Chi-Yang Hsu", "Tianlong Chen", "Walter Gerych", "Ying Ding"], "title": "Mapping from Meaning: Addressing the Miscalibration of Prompt-Sensitive Language Models", "comment": null, "summary": "An interesting behavior in large language models (LLMs) is prompt\nsensitivity. When provided with different but semantically equivalent versions\nof the same prompt, models may produce very different distributions of answers.\nThis suggests that the uncertainty reflected in a model's output distribution\nfor one prompt may not reflect the model's uncertainty about the meaning of the\nprompt. We model prompt sensitivity as a type of generalization error, and show\nthat sampling across the semantic ``concept space'' with paraphrasing\nperturbations improves uncertainty calibration without compromising accuracy.\nAdditionally, we introduce a new metric for uncertainty decomposition in\nblack-box LLMs that improves upon entropy-based decomposition by modeling\nsemantic continuities in natural language generation. We show that this\ndecomposition metric can be used to quantify how much LLM uncertainty is\nattributed to prompt sensitivity. Our work introduces a new way to improve\nuncertainty calibration in prompt-sensitive language models, and provides\nevidence that some LLMs fail to exhibit consistent general reasoning about the\nmeanings of their inputs.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86LLMs\u7684\u63d0\u793a\u654f\u611f\u6027\u73b0\u8c61\uff0c\u63d0\u51fa\u4e86\u901a\u8fc7\u8bed\u4e49\u7a7a\u95f4\u91c7\u6837\u548c\u91ca\u4e49\u6270\u52a8\u6765\u6539\u5584\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u7684\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u5206\u89e3\u6307\u6807\u6765\u91cf\u5316\u63d0\u793a\u654f\u611f\u6027\u5bf9\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u63d0\u793a\u654f\u611f\u6027\u2014\u2014\u5373\u4f7f\u8bed\u4e49\u76f8\u540c\u4f46\u8868\u8ff0\u4e0d\u540c\u7684\u63d0\u793a\uff0c\u6a21\u578b\u4e5f\u4f1a\u4ea7\u751f\u4e0d\u540c\u7684\u7b54\u6848\u5206\u5e03\u3002\u8fd9\u8868\u660e\u6a21\u578b\u5bf9\u5355\u4e00\u63d0\u793a\u7684\u8f93\u51fa\u5206\u5e03\u4e0d\u786e\u5b9a\u6027\u53ef\u80fd\u65e0\u6cd5\u53cd\u6620\u5176\u5bf9\u63d0\u793a\u542b\u4e49\u7684\u771f\u5b9e\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u5c06\u63d0\u793a\u654f\u611f\u6027\u5efa\u6a21\u4e3a\u4e00\u79cd\u6cdb\u5316\u8bef\u5dee\uff0c\u901a\u8fc7\u5728\u8bed\u4e49\u6982\u5ff5\u7a7a\u95f4\u4e2d\u8fdb\u884c\u91c7\u6837\u548c\u91ca\u4e49\u6270\u52a8\u6765\u6539\u5584\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\uff1b\u5f15\u5165\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u5206\u89e3\u6307\u6807\uff0c\u901a\u8fc7\u5efa\u6a21\u81ea\u7136\u8bed\u8a00\u751f\u6210\u4e2d\u7684\u8bed\u4e49\u8fde\u7eed\u6027\u6765\u6539\u8fdb\u57fa\u4e8e\u71b5\u7684\u5206\u89e3\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u8bed\u4e49\u7a7a\u95f4\u91c7\u6837\u548c\u91ca\u4e49\u6270\u52a8\u53ef\u4ee5\u5728\u4e0d\u727a\u7272\u51c6\u786e\u6027\u7684\u60c5\u51b5\u4e0b\u6539\u5584\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\uff1b\u65b0\u7684\u5206\u89e3\u6307\u6807\u80fd\u591f\u6709\u6548\u91cf\u5316LLM\u4e0d\u786e\u5b9a\u6027\u4e2d\u5f52\u56e0\u4e8e\u63d0\u793a\u654f\u611f\u6027\u7684\u90e8\u5206\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u6539\u5584\u63d0\u793a\u654f\u611f\u8bed\u8a00\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u67d0\u4e9bLLMs\u672a\u80fd\u5bf9\u5176\u8f93\u5165\u542b\u4e49\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u4e00\u822c\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2510.17382", "categories": ["cs.AI", "cs.LG", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17382", "abs": "https://arxiv.org/abs/2510.17382", "authors": ["Rishabh Jain", "Keisuke Okumura", "Michael Amir", "Amanda Prorok"], "title": "Graph Attention-Guided Search for Dense Multi-Agent Pathfinding", "comment": null, "summary": "Finding near-optimal solutions for dense multi-agent pathfinding (MAPF)\nproblems in real-time remains challenging even for state-of-the-art planners.\nTo this end, we develop a hybrid framework that integrates a learned heuristic\nderived from MAGAT, a neural MAPF policy with a graph attention scheme, into a\nleading search-based algorithm, LaCAM. While prior work has explored\nlearning-guided search in MAPF, such methods have historically underperformed.\nIn contrast, our approach, termed LaGAT, outperforms both purely search-based\nand purely learning-based methods in dense scenarios. This is achieved through\nan enhanced MAGAT architecture, a pre-train-then-fine-tune strategy on maps of\ninterest, and a deadlock detection scheme to account for imperfect neural\nguidance. Our results demonstrate that, when carefully designed, hybrid search\noffers a powerful solution for tightly coupled, challenging multi-agent\ncoordination problems.", "AI": {"tldr": "\u63d0\u51faLaGAT\u6846\u67b6\uff0c\u5c06\u57fa\u4e8e\u56fe\u6ce8\u610f\u529b\u7684\u795e\u7ecfMAPF\u7b56\u7565MAGAT\u4e0e\u641c\u7d22\u7b97\u6cd5LaCAM\u7ed3\u5408\uff0c\u5728\u5bc6\u96c6\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u573a\u666f\u4e2d\u4f18\u4e8e\u7eaf\u641c\u7d22\u548c\u7eaf\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u5bc6\u96c6\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u95ee\u9898\u5728\u5b9e\u65f6\u573a\u666f\u4e2d\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u96c6\u6210MAGAT\u7684\u542f\u53d1\u5f0f\u5230LaCAM\u7b97\u6cd5\u4e2d\uff0c\u91c7\u7528\u9884\u8bad\u7ec3-\u5fae\u8c03\u7b56\u7565\u548c\u6b7b\u9501\u68c0\u6d4b\u673a\u5236\u3002", "result": "\u5728\u5bc6\u96c6\u573a\u666f\u4e2d\u4f18\u4e8e\u7eaf\u641c\u7d22\u548c\u7eaf\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6df7\u5408\u641c\u7d22\u65b9\u6cd5\u4e3a\u7d27\u5bc6\u8026\u5408\u7684\u591a\u667a\u80fd\u4f53\u534f\u8c03\u95ee\u9898\u63d0\u4f9b\u4e86\u5f3a\u5927\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17062", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17062", "abs": "https://arxiv.org/abs/2510.17062", "authors": ["Guoqing Luo", "Iffat Maab", "Lili Mou", "Junichi Yamagishi"], "title": "Investigating Thinking Behaviours of Reasoning-Based Language Models for Social Bias Mitigation", "comment": null, "summary": "While reasoning-based large language models excel at complex tasks through an\ninternal, structured thinking process, a concerning phenomenon has emerged that\nsuch a thinking process can aggregate social stereotypes, leading to biased\noutcomes. However, the underlying behaviours of these language models in social\nbias scenarios remain underexplored. In this work, we systematically\ninvestigate mechanisms within the thinking process behind this phenomenon and\nuncover two failure patterns that drive social bias aggregation: 1) stereotype\nrepetition, where the model relies on social stereotypes as its primary\njustification, and 2) irrelevant information injection, where it fabricates or\nintroduces new details to support a biased narrative. Building on these\ninsights, we introduce a lightweight prompt-based mitigation approach that\nqueries the model to review its own initial reasoning against these specific\nfailure patterns. Experiments on question answering (BBQ and StereoSet) and\nopen-ended (BOLD) benchmarks show that our approach effectively reduces bias\nwhile maintaining or improving accuracy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u63a8\u7406\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u601d\u8003\u8fc7\u7a0b\u4e2d\u5982\u4f55\u805a\u5408\u793e\u4f1a\u504f\u89c1\uff0c\u53d1\u73b0\u4e86\u4e24\u79cd\u5bfc\u81f4\u504f\u89c1\u52a0\u5267\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u63d0\u793a\u65b9\u6cd5\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u867d\u7136\u63a8\u7406\u578b\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u5185\u90e8\u7ed3\u6784\u5316\u601d\u8003\u8fc7\u7a0b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u8fd9\u79cd\u601d\u8003\u8fc7\u7a0b\u4f1a\u805a\u5408\u793e\u4f1a\u523b\u677f\u5370\u8c61\uff0c\u5bfc\u81f4\u504f\u89c1\u7ed3\u679c\u3002\u7136\u800c\uff0c\u8bed\u8a00\u6a21\u578b\u5728\u504f\u89c1\u573a\u666f\u4e2d\u7684\u5e95\u5c42\u884c\u4e3a\u673a\u5236\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u601d\u8003\u8fc7\u7a0b\u4e2d\u504f\u89c1\u805a\u5408\u7684\u673a\u5236\uff0c\u53d1\u73b0\u4e86\u4e24\u79cd\u5931\u8d25\u6a21\u5f0f\uff1a\u523b\u677f\u5370\u8c61\u91cd\u590d\u548c\u65e0\u5173\u4fe1\u606f\u6ce8\u5165\u3002\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u7f13\u89e3\u65b9\u6cd5\uff0c\u8ba9\u6a21\u578b\u6839\u636e\u8fd9\u4e9b\u7279\u5b9a\u5931\u8d25\u6a21\u5f0f\u6765\u5ba1\u67e5\u81ea\u5df1\u7684\u521d\u59cb\u63a8\u7406\u3002", "result": "\u5728\u95ee\u7b54\uff08BBQ\u548cStereoSet\uff09\u548c\u5f00\u653e\u5f0f\uff08BOLD\uff09\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e86\u504f\u89c1\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u63a8\u7406\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u504f\u89c1\u805a\u5408\u7684\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8f7b\u91cf\u7ea7\u7f13\u89e3\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u51cf\u5c11\u504f\u89c1\u7684\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2510.17418", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.17418", "abs": "https://arxiv.org/abs/2510.17418", "authors": ["Mustafa F. Abdelwahed", "Alice Toniolo", "Joan Espasa", "Ian P. Gent"], "title": "Diverse Planning with Simulators via Linear Temporal Logic", "comment": null, "summary": "Autonomous agents rely on automated planning algorithms to achieve their\nobjectives. Simulation-based planning offers a significant advantage over\ndeclarative models in modelling complex environments. However, relying solely\non a planner that produces a single plan may not be practical, as the generated\nplans may not always satisfy the agent's preferences. To address this\nlimitation, we introduce $\\texttt{FBI}_\\texttt{LTL}$, a diverse planner\nexplicitly designed for simulation-based planning problems.\n$\\texttt{FBI}_\\texttt{LTL}$ utilises Linear Temporal Logic (LTL) to define\nsemantic diversity criteria, enabling agents to specify what constitutes\nmeaningfully different plans. By integrating these LTL-based diversity models\ndirectly into the search process, $\\texttt{FBI}_\\texttt{LTL}$ ensures the\ngeneration of semantically diverse plans, addressing a critical limitation of\nexisting diverse planning approaches that may produce syntactically different\nbut semantically identical solutions. Extensive evaluations on various\nbenchmarks consistently demonstrate that $\\texttt{FBI}_\\texttt{LTL}$ generates\nmore diverse plans compared to a baseline approach. This work establishes the\nfeasibility of semantically-guided diverse planning in simulation-based\nenvironments, paving the way for innovative approaches in realistic,\nnon-symbolic domains where traditional model-based approaches fail.", "AI": {"tldr": "\u63d0\u51fa\u4e86FBI_LTL\uff0c\u4e00\u4e2a\u4e13\u95e8\u4e3a\u57fa\u4e8e\u4eff\u771f\u7684\u89c4\u5212\u95ee\u9898\u8bbe\u8ba1\u7684\u591a\u6837\u5316\u89c4\u5212\u5668\uff0c\u4f7f\u7528\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\u5b9a\u4e49\u8bed\u4e49\u591a\u6837\u6027\u6807\u51c6\uff0c\u751f\u6210\u8bed\u4e49\u4e0a\u4e0d\u540c\u7684\u8ba1\u5212\u3002", "motivation": "\u4f20\u7edf\u89c4\u5212\u5668\u53ea\u751f\u6210\u5355\u4e00\u8ba1\u5212\uff0c\u53ef\u80fd\u65e0\u6cd5\u6ee1\u8db3\u4ee3\u7406\u504f\u597d\uff1b\u73b0\u6709\u591a\u6837\u5316\u89c4\u5212\u65b9\u6cd5\u53ef\u80fd\u4ea7\u751f\u8bed\u6cd5\u4e0d\u540c\u4f46\u8bed\u4e49\u76f8\u540c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\u5b9a\u4e49\u8bed\u4e49\u591a\u6837\u6027\u6807\u51c6\uff0c\u5e76\u5c06\u8fd9\u4e9bLTL\u591a\u6837\u6027\u6a21\u578b\u76f4\u63a5\u96c6\u6210\u5230\u641c\u7d22\u8fc7\u7a0b\u4e2d\u3002", "result": "\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFBI_LTL\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u80fd\u751f\u6210\u66f4\u591a\u6837\u5316\u7684\u8ba1\u5212\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u786e\u7acb\u4e86\u5728\u57fa\u4e8e\u4eff\u771f\u7684\u73af\u5883\u4e2d\u8fdb\u884c\u8bed\u4e49\u5f15\u5bfc\u591a\u6837\u5316\u89c4\u5212\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5728\u4f20\u7edf\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u5931\u6548\u7684\u73b0\u5b9e\u975e\u7b26\u53f7\u9886\u57df\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.17109", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.17109", "abs": "https://arxiv.org/abs/2510.17109", "authors": ["Tianyang Xu", "Dan Zhang", "Kushan Mitra", "Estevam Hruschka"], "title": "Verification-Aware Planning for Multi-Agent Systems", "comment": "Submission for ARR Oct", "summary": "Large language model (LLM) agents are increasingly deployed to tackle complex\ntasks, often necessitating collaboration among multiple specialized agents.\nHowever, multi-agent collaboration introduces new challenges in planning,\ncoordination, and verification. Execution failures frequently arise not from\nflawed reasoning alone, but from subtle misalignments in task interpretation,\noutput format, or inter-agent handoffs. To address these challenges, we present\nVeriMAP, a framework for multi-agent collaboration with verification-aware\nplanning. The VeriMAP planner decomposes tasks, models subtask dependencies,\nand encodes planner-defined passing criteria as subtask verification functions\n(VFs) in Python and natural language. We evaluate VeriMAP on diverse datasets,\ndemonstrating that it outperforms both single- and multi-agent baselines while\nenhancing system robustness and interpretability. Our analysis highlights how\nverification-aware planning enables reliable coordination and iterative\nrefinement in multi-agent systems, without relying on external labels or\nannotations.", "AI": {"tldr": "VeriMAP\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u9a8c\u8bc1\u611f\u77e5\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u4efb\u52a1\u3001\u5efa\u6a21\u5b50\u4efb\u52a1\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u7f16\u7801\u9a8c\u8bc1\u51fd\u6570\u6765\u63d0\u5347\u7cfb\u7edf\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u9762\u4e34\u89c4\u5212\u3001\u534f\u8c03\u548c\u9a8c\u8bc1\u7684\u6311\u6218\uff0c\u6267\u884c\u5931\u8d25\u5f80\u5f80\u6e90\u4e8e\u4efb\u52a1\u89e3\u91ca\u3001\u8f93\u51fa\u683c\u5f0f\u6216\u4ea4\u63a5\u4e2d\u7684\u7ec6\u5fae\u504f\u5dee\u3002", "method": "VeriMAP\u89c4\u5212\u5668\u5206\u89e3\u4efb\u52a1\uff0c\u5efa\u6a21\u5b50\u4efb\u52a1\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u5c06\u89c4\u5212\u5668\u5b9a\u4e49\u7684\u901a\u8fc7\u6807\u51c6\u7f16\u7801\u4e3aPython\u548c\u81ea\u7136\u8bed\u8a00\u7684\u5b50\u4efb\u52a1\u9a8c\u8bc1\u51fd\u6570\u3002", "result": "\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cVeriMAP\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53\u57fa\u7ebf\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u7cfb\u7edf\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u9a8c\u8bc1\u611f\u77e5\u89c4\u5212\u80fd\u591f\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u53ef\u9760\u7684\u534f\u8c03\u548c\u8fed\u4ee3\u4f18\u5316\uff0c\u65e0\u9700\u4f9d\u8d56\u5916\u90e8\u6807\u7b7e\u6216\u6ce8\u91ca\u3002"}}
{"id": "2510.17450", "categories": ["cs.AI", "H.4.2; I.2.3; I.2.6; I.2.8; I.2.9; J.7"], "pdf": "https://arxiv.org/pdf/2510.17450", "abs": "https://arxiv.org/abs/2510.17450", "authors": ["Johan Schubert", "Farzad Kamrani", "Tove Gustavi"], "title": "Active Inference for an Intelligent Agent in Autonomous Reconnaissance Missions", "comment": "Presented at the 6th International Workshop on Active Inference,\n  15-17 October 2025, Montreal, Canada", "summary": "We develop an active inference route-planning method for the autonomous\ncontrol of intelligent agents. The aim is to reconnoiter a geographical area to\nmaintain a common operational picture. To achieve this, we construct an\nevidence map that reflects our current understanding of the situation,\nincorporating both positive and \"negative\" sensor observations of possible\ntarget objects collected over time, and diffusing the evidence across the map\nas time progresses. The generative model of active inference uses\nDempster-Shafer theory and a Gaussian sensor model, which provides input to the\nagent. The generative process employs a Bayesian approach to update a posterior\nprobability distribution. We calculate the variational free energy for all\npositions within the area by assessing the divergence between a pignistic\nprobability distribution of the evidence map and a posterior probability\ndistribution of a target object based on the observations, including the level\nof surprise associated with receiving new observations. Using the free energy,\nwe direct the agents' movements in a simulation by taking an incremental step\ntoward a position that minimizes the free energy. This approach addresses the\nchallenge of exploration and exploitation, allowing agents to balance searching\nextensive areas of the geographical map while tracking identified target\nobjects.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u52a8\u63a8\u7406\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u7528\u4e8e\u667a\u80fd\u4ee3\u7406\u7684\u81ea\u4e3b\u63a7\u5236\uff0c\u901a\u8fc7\u6784\u5efa\u8bc1\u636e\u5730\u56fe\u548c\u8ba1\u7b97\u53d8\u5206\u81ea\u7531\u80fd\u91cf\u6765\u6307\u5bfc\u4ee3\u7406\u5728\u63a2\u7d22\u548c\u5229\u7528\u4e4b\u95f4\u5e73\u8861\u79fb\u52a8\u3002", "motivation": "\u5f00\u53d1\u81ea\u4e3b\u63a7\u5236\u667a\u80fd\u4ee3\u7406\u7684\u65b9\u6cd5\uff0c\u4ee5\u4fa6\u5bdf\u5730\u7406\u533a\u57df\u5e76\u7ef4\u6301\u5171\u540c\u4f5c\u6218\u6001\u52bf\u56fe\uff0c\u89e3\u51b3\u63a2\u7d22\u548c\u5229\u7528\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\u3002", "method": "\u4f7f\u7528Dempster-Shafer\u7406\u8bba\u548c\u9ad8\u65af\u4f20\u611f\u5668\u6a21\u578b\u6784\u5efa\u8bc1\u636e\u5730\u56fe\uff0c\u8ba1\u7b97\u53d8\u5206\u81ea\u7531\u80fd\u91cf\u4f5c\u4e3a\u4f4d\u7f6e\u8bc4\u4f30\u6807\u51c6\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u81ea\u7531\u80fd\u91cf\u6765\u6307\u5bfc\u4ee3\u7406\u79fb\u52a8\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u6307\u5bfc\u4ee3\u7406\u5728\u5730\u7406\u533a\u57df\u5185\u8fdb\u884c\u4fa6\u5bdf\uff0c\u5e73\u8861\u641c\u7d22\u5e7f\u9614\u533a\u57df\u548c\u8ddf\u8e2a\u5df2\u8bc6\u522b\u76ee\u6807\u5bf9\u8c61\u7684\u9700\u6c42\u3002", "conclusion": "\u4e3b\u52a8\u63a8\u7406\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u4e3a\u667a\u80fd\u4ee3\u7406\u7684\u81ea\u4e3b\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5904\u7406\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u6743\u8861\u95ee\u9898\u3002"}}
{"id": "2510.17115", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17115", "abs": "https://arxiv.org/abs/2510.17115", "authors": ["Wei Du", "Nuowei Liu", "Jie Wang", "Jiahao Kuang", "Tao Ji", "Xiaoling Wang", "Yuanbin Wu"], "title": "DVAGen: Dynamic Vocabulary Augmented Generation", "comment": null, "summary": "Language models trained with a fixed vocabulary struggle to generalize to\nnovel or out-of-vocabulary words, limiting their flexibility in handling\ndiverse token combinations. Existing dynamic vocabulary approaches attempt to\naddress this limitation but face challenges such as fragmented codebases, lack\nof support for modern LLMs, and limited inference scalability. To overcome\nthese issues, we introduce DVAGen, a fully open-source, unified framework\ndesigned for training, evaluation, and visualization of dynamic\nvocabulary-augmented language models. Our framework modularizes the pipeline\nfor ease of customization, integrates seamlessly with open-source LLMs, and is\nthe first to provide both CLI and WebUI tools for real-time result inspection.\nWe validate the effectiveness of dynamic vocabulary methods on modern LLMs and\ndemonstrate support for batch inference, significantly improving inference\nthroughput.", "AI": {"tldr": "DVAGen\u662f\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3\u3001\u8bc4\u4f30\u548c\u53ef\u89c6\u5316\u52a8\u6001\u8bcd\u6c47\u589e\u5f3a\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u89e3\u51b3\u56fa\u5b9a\u8bcd\u6c47\u8868\u6a21\u578b\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u652f\u6301\u73b0\u4ee3LLM\u548c\u6279\u91cf\u63a8\u7406\u3002", "motivation": "\u56fa\u5b9a\u8bcd\u6c47\u8868\u7684\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u5904\u7406\u65b0\u8bcd\u6216\u8bcd\u6c47\u8868\u5916\u8bcd\u6c47\uff0c\u73b0\u6709\u52a8\u6001\u8bcd\u6c47\u65b9\u6cd5\u5b58\u5728\u4ee3\u7801\u5e93\u788e\u7247\u5316\u3001\u4e0d\u652f\u6301\u73b0\u4ee3LLM\u548c\u63a8\u7406\u53ef\u6269\u5c55\u6027\u6709\u9650\u7b49\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86DVAGen\u6846\u67b6\uff0c\u6a21\u5757\u5316\u5904\u7406\u6d41\u7a0b\u4fbf\u4e8e\u5b9a\u5236\uff0c\u65e0\u7f1d\u96c6\u6210\u5f00\u6e90LLM\uff0c\u9996\u6b21\u63d0\u4f9bCLI\u548cWebUI\u5de5\u5177\u8fdb\u884c\u5b9e\u65f6\u7ed3\u679c\u68c0\u67e5\u3002", "result": "\u9a8c\u8bc1\u4e86\u52a8\u6001\u8bcd\u6c47\u65b9\u6cd5\u5728\u73b0\u4ee3LLM\u4e0a\u7684\u6709\u6548\u6027\uff0c\u652f\u6301\u6279\u91cf\u63a8\u7406\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u541e\u5410\u91cf\u3002", "conclusion": "DVAGen\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u52a8\u6001\u8bcd\u6c47\u65b9\u6cd5\u7684\u73b0\u6709\u6311\u6218\uff0c\u4e3a\u5904\u7406\u591a\u6837\u5316\u8bcd\u6c47\u7ec4\u5408\u63d0\u4f9b\u4e86\u7075\u6d3b\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17463", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17463", "abs": "https://arxiv.org/abs/2510.17463", "authors": ["Cor Steging", "Tadeusz Zbiegie\u0144"], "title": "Label Indeterminacy in AI & Law", "comment": "This manuscript has been accepted for presentation as a short paper\n  at the 38th International Conference on Legal Knowledge and Information\n  Systems (JURIX) in Turin, December 9 to 11 of 2025", "summary": "Machine learning is increasingly used in the legal domain, where it typically\noperates retrospectively by treating past case outcomes as ground truth.\nHowever, legal outcomes are often shaped by human interventions that are not\ncaptured in most machine learning approaches. A final decision may result from\na settlement, an appeal, or other procedural actions. This creates label\nindeterminacy: the outcome could have been different if the intervention had or\nhad not taken place. We argue that legal machine learning applications need to\naccount for label indeterminacy. Methods exist that can impute these\nindeterminate labels, but they are all grounded in unverifiable assumptions. In\nthe context of classifying cases from the European Court of Human Rights, we\nshow that the way that labels are constructed during training can significantly\naffect model behaviour. We therefore position label indeterminacy as a relevant\nconcern in AI & Law and demonstrate how it can shape model behaviour.", "AI": {"tldr": "\u6cd5\u5f8b\u673a\u5668\u5b66\u4e60\u9700\u8981\u89e3\u51b3\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u56e0\u4e3a\u6cd5\u5f8b\u6848\u4ef6\u7ed3\u679c\u5f80\u5f80\u53d7\u5230\u4eba\u4e3a\u5e72\u9884\u5f71\u54cd\uff0c\u5bfc\u81f4\u6807\u7b7e\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\uff0c\u8fd9\u4f1a\u663e\u8457\u5f71\u54cd\u6a21\u578b\u884c\u4e3a\u3002", "motivation": "\u6cd5\u5f8b\u673a\u5668\u5b66\u4e60\u901a\u5e38\u5c06\u8fc7\u53bb\u6848\u4ef6\u7ed3\u679c\u89c6\u4e3a\u771f\u5b9e\u6807\u7b7e\uff0c\u4f46\u6cd5\u5f8b\u7ed3\u679c\u5f80\u5f80\u53d7\u5230\u548c\u89e3\u3001\u4e0a\u8bc9\u7b49\u4eba\u4e3a\u5e72\u9884\u5f71\u54cd\uff0c\u5bfc\u81f4\u6807\u7b7e\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\uff0c\u9700\u8981\u5bf9\u6b64\u8fdb\u884c\u7814\u7a76\u3002", "method": "\u5728\u6b27\u6d32\u4eba\u6743\u6cd5\u9662\u6848\u4ef6\u5206\u7c7b\u7684\u80cc\u666f\u4e0b\uff0c\u7814\u7a76\u4e0d\u540c\u6807\u7b7e\u6784\u5efa\u65b9\u5f0f\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u884c\u4e3a\uff0c\u5c55\u793a\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6807\u7b7e\u7684\u6784\u5efa\u65b9\u5f0f\u4f1a\u663e\u8457\u5f71\u54cd\u6a21\u578b\u7684\u884c\u4e3a\u8868\u73b0\u3002", "conclusion": "\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\u662fAI\u4e0e\u6cd5\u5f8b\u9886\u57df\u7684\u91cd\u8981\u5173\u6ce8\u70b9\uff0c\u9700\u8981\u88ab\u7eb3\u5165\u8003\u8651\u4ee5\u6539\u5584\u6cd5\u5f8b\u673a\u5668\u5b66\u4e60\u5e94\u7528\u3002"}}
{"id": "2510.17139", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.17139", "abs": "https://arxiv.org/abs/2510.17139", "authors": ["Zhichao Xu", "Shengyao Zhuang", "Xueguang Ma", "Bingsen Chen", "Yijun Tian", "Fengran Mo", "Jie Cao", "Vivek Srikumar"], "title": "Rethinking On-policy Optimization for Query Augmentation", "comment": null, "summary": "Recent advances in large language models (LLMs) have led to a surge of\ninterest in query augmentation for information retrieval (IR). Two main\napproaches have emerged. The first prompts LLMs to generate answers or\npseudo-documents that serve as new queries, relying purely on the model's\nparametric knowledge or contextual information. The second applies\nreinforcement learning (RL) to fine-tune LLMs for query rewriting, directly\noptimizing retrieval metrics. While having respective advantages and\nlimitations, the two approaches have not been compared under consistent\nexperimental conditions. In this work, we present the first systematic\ncomparison of prompting-based and RL-based query augmentation across diverse\nbenchmarks, including evidence-seeking, ad hoc, and tool retrieval. Our key\nfinding is that simple, training-free query augmentation often performs on par\nwith, or even surpasses, more expensive RL-based counterparts, especially when\nusing powerful LLMs. Motivated by this discovery, we introduce a novel hybrid\nmethod, On-policy Pseudo-document Query Expansion (OPQE), which, instead of\nrewriting a query, the LLM policy learns to generate a pseudo-document that\nmaximizes retrieval performance, thus merging the flexibility and generative\nstructure of prompting with the targeted optimization of RL. We show OPQE\noutperforms both standalone prompting and RL-based rewriting, demonstrating\nthat a synergistic approach yields the best results. Our implementation is made\navailable to facilitate reproducibility.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u6bd4\u8f83\u4e86\u57fa\u4e8e\u63d0\u793a\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u67e5\u8be2\u589e\u5f3a\u65b9\u6cd5\uff0c\u53d1\u73b0\u7b80\u5355\u7684\u65e0\u8bad\u7ec3\u67e5\u8be2\u589e\u5f3a\u65b9\u6cd5\u5728\u5f3a\u5927LLMs\u4e0b\u8868\u73b0\u4e0e\u66f4\u6602\u8d35\u7684RL\u65b9\u6cd5\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6df7\u5408\u65b9\u6cd5OPQE\uff0c\u7ed3\u5408\u4e86\u63d0\u793a\u7684\u7075\u6d3b\u6027\u548cRL\u7684\u4f18\u5316\u76ee\u6807\uff0c\u53d6\u5f97\u4e86\u6700\u4f73\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLMs\u7684\u67e5\u8be2\u589e\u5f3a\u4e3b\u8981\u6709\u4e24\u79cd\u65b9\u6cd5\uff1a\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u4f46\u7f3a\u4e4f\u5728\u4e00\u81f4\u5b9e\u9a8c\u6761\u4ef6\u4e0b\u7684\u7cfb\u7edf\u6bd4\u8f83\u3002", "method": "1. \u7cfb\u7edf\u6bd4\u8f83\u57fa\u4e8e\u63d0\u793a\u548c\u57fa\u4e8eRL\u7684\u67e5\u8be2\u589e\u5f3a\u65b9\u6cd5\uff1b2. \u63d0\u51fa\u6df7\u5408\u65b9\u6cd5OPQE\uff0c\u8ba9LLM\u7b56\u7565\u5b66\u4e60\u751f\u6210\u6700\u5927\u5316\u68c0\u7d22\u6027\u80fd\u7684\u4f2a\u6587\u6863\uff0c\u7ed3\u5408\u4e86\u63d0\u793a\u7684\u7075\u6d3b\u6027\u548cRL\u7684\u4f18\u5316\u76ee\u6807\u3002", "result": "\u7b80\u5355\u65e0\u8bad\u7ec3\u7684\u67e5\u8be2\u589e\u5f3a\u65b9\u6cd5\u5728\u5f3a\u5927LLMs\u4e0b\u8868\u73b0\u4e0eRL\u65b9\u6cd5\u76f8\u5f53\u751a\u81f3\u66f4\u597d\uff1bOPQE\u65b9\u6cd5\u5728\u8bc1\u636e\u68c0\u7d22\u3001ad hoc\u68c0\u7d22\u548c\u5de5\u5177\u68c0\u7d22\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5355\u72ec\u7684\u63d0\u793a\u65b9\u6cd5\u548c\u57fa\u4e8eRL\u7684\u91cd\u5199\u65b9\u6cd5\u3002", "conclusion": "\u6df7\u5408\u65b9\u6cd5\u7ed3\u5408\u63d0\u793a\u7684\u7075\u6d3b\u6027\u548cRL\u7684\u4f18\u5316\u76ee\u6807\u80fd\u591f\u53d6\u5f97\u6700\u4f73\u6548\u679c\uff0c\u4e3a\u67e5\u8be2\u589e\u5f3a\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2510.17590", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.CY", "cs.LG", "I.2.7; H.3.3; I.4.9"], "pdf": "https://arxiv.org/pdf/2510.17590", "abs": "https://arxiv.org/abs/2510.17590", "authors": ["Mir Nafis Sharear Shopnil", "Sharad Duwal", "Abhishek Tyagi", "Adiba Mahbub Proma"], "title": "MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning", "comment": "16 pages, 3 tables, 1 figure", "summary": "Misinformation spreads across web platforms through billions of daily\nmultimodal posts that combine text and images, overwhelming manual\nfact-checking capacity. Supervised detection models require domain-specific\ntraining data and fail to generalize across diverse manipulation tactics. We\npresent MIRAGE, an inference-time, model-pluggable agentic framework that\ndecomposes multimodal verification into four sequential modules: visual\nveracity assessment detects AI-generated images, cross-modal consistency\nanalysis identifies out-of-context repurposing, retrieval-augmented factual\nchecking grounds claims in web evidence through iterative question generation,\nand a calibrated judgment module integrates all signals. MIRAGE orchestrates\nvision-language model reasoning with targeted web retrieval, outputs structured\nand citation-linked rationales. On MMFakeBench validation set (1,000 samples),\nMIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming\nthe strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65\npoints while maintaining 34.3% false positive rate versus 97.3% for a\njudge-only baseline. Test set results (5,000 samples) confirm generalization\nwith 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification\ncontributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97\npoints. Our results demonstrate that decomposed agentic reasoning with web\nretrieval can match supervised detector performance without domain-specific\ntraining, enabling misinformation detection across modalities where labeled\ndata remains scarce.", "AI": {"tldr": "MIRAGE\u662f\u4e00\u4e2a\u7528\u4e8e\u68c0\u6d4b\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u7684\u667a\u80fd\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u9a8c\u8bc1\u8fc7\u7a0b\u4e3a\u56db\u4e2a\u6a21\u5757\uff1a\u89c6\u89c9\u771f\u5b9e\u6027\u8bc4\u4f30\u3001\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u5206\u6790\u3001\u68c0\u7d22\u589e\u5f3a\u7684\u4e8b\u5b9e\u6838\u67e5\u548c\u6821\u51c6\u5224\u65ad\uff0c\u65e0\u9700\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u9ad8\u6027\u80fd\u68c0\u6d4b\u3002", "motivation": "\u7f51\u7edc\u5e73\u53f0\u4e0a\u6bcf\u5929\u6709\u6570\u5341\u4ebf\u5305\u542b\u6587\u672c\u548c\u56fe\u50cf\u7684\u591a\u6a21\u6001\u5e16\u5b50\u4f20\u64ad\u865a\u5047\u4fe1\u606f\uff0c\u4eba\u5de5\u4e8b\u5b9e\u6838\u67e5\u80fd\u529b\u4e0d\u8db3\u3002\u73b0\u6709\u7684\u76d1\u7763\u68c0\u6d4b\u6a21\u578b\u9700\u8981\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\uff0c\u4e14\u96be\u4ee5\u6cdb\u5316\u5230\u4e0d\u540c\u7684\u64cd\u7eb5\u7b56\u7565\u3002", "method": "MIRAGE\u6846\u67b6\u5305\u542b\u56db\u4e2a\u987a\u5e8f\u6a21\u5757\uff1a1) \u89c6\u89c9\u771f\u5b9e\u6027\u8bc4\u4f30\u68c0\u6d4bAI\u751f\u6210\u56fe\u50cf\uff1b2) \u8de8\u6a21\u6001\u4e00\u81f4\u6027\u5206\u6790\u8bc6\u522b\u4e0a\u4e0b\u6587\u4e0d\u5f53\u7684\u91cd\u65b0\u5229\u7528\uff1b3) \u68c0\u7d22\u589e\u5f3a\u7684\u4e8b\u5b9e\u6838\u67e5\u901a\u8fc7\u8fed\u4ee3\u95ee\u9898\u751f\u6210\u5c06\u58f0\u660e\u4e0e\u7f51\u7edc\u8bc1\u636e\u5173\u8054\uff1b4) \u6821\u51c6\u5224\u65ad\u6a21\u5757\u6574\u5408\u6240\u6709\u4fe1\u53f7\u3002\u8be5\u6846\u67b6\u534f\u8c03\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e0e\u76ee\u6807\u7f51\u7edc\u68c0\u7d22\uff0c\u8f93\u51fa\u7ed3\u6784\u5316\u7684\u5f15\u7528\u94fe\u63a5\u63a8\u7406\u3002", "result": "\u5728MMFakeBench\u9a8c\u8bc1\u96c6\uff081,000\u6837\u672c\uff09\u4e0a\uff0cMIRAGE\u4e0eGPT-4o-mini\u7ec4\u5408\u8fbe\u523081.65% F1\u5206\u6570\u548c75.1%\u51c6\u786e\u7387\uff0c\u6bd4\u6700\u5f3a\u7684\u96f6\u6837\u672c\u57fa\u7ebf\uff08GPT-4V\u4e0eMMD-Agent\uff0c74.0% F1\uff09\u9ad8\u51fa7.65\u70b9\uff0c\u540c\u65f6\u4fdd\u630134.3%\u7684\u5047\u9633\u6027\u7387\uff0c\u800c\u4ec5\u5224\u65ad\u57fa\u7ebf\u7684\u5047\u9633\u6027\u7387\u4e3a97.3%\u3002\u6d4b\u8bd5\u96c6\u7ed3\u679c\uff085,000\u6837\u672c\uff09\u786e\u8ba4\u4e86\u6cdb\u5316\u80fd\u529b\uff0c\u8fbe\u523081.44% F1\u548c75.08%\u51c6\u786e\u7387\u3002\u6d88\u878d\u7814\u7a76\u663e\u793a\u89c6\u89c9\u9a8c\u8bc1\u8d21\u732e5.18 F1\u70b9\uff0c\u68c0\u7d22\u589e\u5f3a\u63a8\u7406\u8d21\u732e2.97\u70b9\u3002", "conclusion": "\u5206\u89e3\u7684\u667a\u80fd\u4ee3\u7406\u63a8\u7406\u4e0e\u7f51\u7edc\u68c0\u7d22\u76f8\u7ed3\u5408\uff0c\u53ef\u4ee5\u5728\u6ca1\u6709\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5339\u914d\u76d1\u7763\u68c0\u6d4b\u5668\u7684\u6027\u80fd\uff0c\u4f7f\u5f97\u5728\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u9886\u57df\uff0c\u5373\u4f7f\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u4e5f\u80fd\u5b9e\u73b0\u6709\u6548\u68c0\u6d4b\u3002"}}
{"id": "2510.17168", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17168", "abs": "https://arxiv.org/abs/2510.17168", "authors": ["Xiaohui Rao", "Hanlin Wu", "Zhenguang G. Cai"], "title": "When AI companions become witty: Can human brain recognize AI-generated irony?", "comment": null, "summary": "As Large Language Models (LLMs) are increasingly deployed as social agents\nand trained to produce humor and irony, a question emerges: when encountering\nwitty AI remarks, do people interpret these as intentional communication or\nmere computational output? This study investigates whether people adopt the\nintentional stance, attributing mental states to explain behavior,toward AI\nduring irony comprehension. Irony provides an ideal paradigm because it\nrequires distinguishing intentional contradictions from unintended errors\nthrough effortful semantic reanalysis. We compared behavioral and neural\nresponses to ironic statements from AI versus human sources using established\nERP components: P200 reflecting early incongruity detection and P600 indexing\ncognitive efforts in reinterpreting incongruity as deliberate irony. Results\ndemonstrate that people do not fully adopt the intentional stance toward\nAI-generated irony. Behaviorally, participants attributed incongruity to\ndeliberate communication for both sources, though significantly less for AI\nthan human, showing greater tendency to interpret AI incongruities as\ncomputational errors. Neural data revealed attenuated P200 and P600 effects for\nAI-generated irony, suggesting reduced effortful detection and reanalysis\nconsistent with diminished attribution of communicative intent. Notably, people\nwho perceived AI as more sincere showed larger P200 and P600 effects for\nAI-generated irony, suggesting that intentional stance adoption is calibrated\nby specific mental models of artificial agents. These findings reveal that\nsource attribution shapes neural processing of social-communicative phenomena.\nDespite current LLMs' linguistic sophistication, achieving genuine social\nagency requires more than linguistic competence, it necessitates a shift in how\nhumans perceive and attribute intentionality to artificial agents.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\u4eba\u4eec\u5bf9AI\u751f\u6210\u7684\u8bbd\u523a\u8a00\u8bba\u4e0d\u4f1a\u5b8c\u5168\u91c7\u7528\u610f\u56fe\u7acb\u573a\uff0c\u884c\u4e3a\u548c\u795e\u7ecf\u53cd\u5e94\u90fd\u663e\u793a\u5bf9AI\u7684\u8bbd\u523a\u7406\u89e3\u4e0d\u5982\u5bf9\u4eba\u7c7b\u90a3\u6837\u6295\u5165\u8ba4\u77e5\u52aa\u529b\uff0c\u8868\u660eAI\u8981\u5b9e\u73b0\u771f\u6b63\u7684\u793e\u4f1a\u4ee3\u7406\u9700\u8981\u4eba\u7c7b\u5bf9\u5176\u610f\u56fe\u6027\u7684\u8ba4\u77e5\u8f6c\u53d8\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u88ab\u90e8\u7f72\u4e3a\u793e\u4f1a\u4ee3\u7406\u5e76\u8bad\u7ec3\u4ea7\u751f\u5e7d\u9ed8\u548c\u8bbd\u523a\uff0c\u9700\u8981\u7814\u7a76\u4eba\u4eec\u5728\u9762\u5bf9AI\u7684\u673a\u667a\u8a00\u8bba\u65f6\uff0c\u662f\u5c06\u5176\u89c6\u4e3a\u6709\u610f\u7684\u6c9f\u901a\u8fd8\u662f\u7eaf\u7cb9\u7684\u8ba1\u7b97\u8f93\u51fa\u3002", "method": "\u6bd4\u8f83\u4eba\u7c7b\u548cAI\u6765\u6e90\u7684\u8bbd\u523a\u9648\u8ff0\u7684\u884c\u4e3a\u548c\u795e\u7ecf\u53cd\u5e94\uff0c\u4f7f\u7528\u5df2\u5efa\u7acb\u7684ERP\u6210\u5206\uff1aP200\u53cd\u6620\u65e9\u671f\u4e0d\u4e00\u81f4\u6027\u68c0\u6d4b\uff0cP600\u7d22\u5f15\u91cd\u65b0\u89e3\u91ca\u4e0d\u4e00\u81f4\u6027\u4e3a\u6545\u610f\u8bbd\u523a\u7684\u8ba4\u77e5\u52aa\u529b\u3002", "result": "\u884c\u4e3a\u4e0a\uff0c\u53c2\u4e0e\u8005\u5bf9AI\u6545\u610f\u6c9f\u901a\u7684\u5f52\u56e0\u663e\u8457\u5c11\u4e8e\u4eba\u7c7b\uff1b\u795e\u7ecf\u6570\u636e\u663e\u793aAI\u751f\u6210\u8bbd\u523a\u7684P200\u548cP600\u6548\u5e94\u51cf\u5f31\uff0c\u8868\u660e\u68c0\u6d4b\u548c\u91cd\u65b0\u5206\u6790\u7684\u8ba4\u77e5\u52aa\u529b\u51cf\u5c11\u3002\u8ba4\u4e3aAI\u66f4\u771f\u8bda\u7684\u4eba\u5bf9AI\u8bbd\u523a\u663e\u793a\u51fa\u66f4\u5927\u7684P200\u548cP600\u6548\u5e94\u3002", "conclusion": "\u6e90\u5f52\u56e0\u5851\u9020\u4e86\u793e\u4f1a\u6c9f\u901a\u73b0\u8c61\u7684\u795e\u7ecf\u5904\u7406\u3002\u5c3d\u7ba1\u5f53\u524dLLMs\u5177\u6709\u8bed\u8a00\u590d\u6742\u6027\uff0c\u4f46\u5b9e\u73b0\u771f\u6b63\u7684\u793e\u4f1a\u4ee3\u7406\u9700\u8981\u4eba\u7c7b\u5bf9\u4eba\u5de5\u4ee3\u7406\u7684\u610f\u56fe\u6027\u5f52\u56e0\u53d1\u751f\u8f6c\u53d8\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u8bed\u8a00\u80fd\u529b\u3002"}}
{"id": "2510.17598", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17598", "abs": "https://arxiv.org/abs/2510.17598", "authors": ["Amir Jalilifard", "Anderson de Rezende Rocha", "Marcos Medeiros Raimundo"], "title": "Reasoning Distillation and Structural Alignment for Improved Code Generation", "comment": null, "summary": "Effective code generation with language models hinges on two critical\nfactors: accurately understanding the intent of the prompt and generating code\nthat applies algorithmic reasoning to produce correct solutions capable of\npassing diverse test cases while adhering to the syntax of the target\nprogramming language. Unlike other language tasks, code generation requires\nmore than accurate token prediction; it demands comprehension of solution-level\nand structural relationships rather than merely generating the most likely\ntokens. very large language model (VLLM) are capable of generating detailed\nsteps toward the correct solution of complex tasks where reasoning is crucial\nin solving the problem. Such reasoning capabilities may be absent in smaller\nlanguage models. Therefore, in this work, we distill the reasoning capabilities\nof a VLLM into a smaller, more efficient model that is faster and cheaper to\ndeploy. Our approach trains the model to emulate the reasoning and\nproblem-solving abilities of the VLLM by learning to identify correct solution\npathways and establishing a structural correspondence between problem\ndefinitions and potential solutions through a novel method of structure-aware\nloss optimization. This enables the model to transcend token-level generation\nand to deeply grasp the overarching structure of solutions for given problems.\nExperimental results show that our fine-tuned model, developed through a cheap\nand simple to implement process, significantly outperforms our baseline model\nin terms of pass@1, average data flow, and average syntax match metrics across\nthe MBPP, MBPP Plus, and HumanEval benchmarks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u84b8\u998f\u5230\u66f4\u5c0f\u3001\u9ad8\u6548\u7684\u6a21\u578b\u4e2d\uff0c\u901a\u8fc7\u7ed3\u6784\u611f\u77e5\u635f\u5931\u4f18\u5316\u65b9\u6cd5\uff0c\u4f7f\u5c0f\u6a21\u578b\u80fd\u591f\u7406\u89e3\u95ee\u9898\u4e0e\u89e3\u51b3\u65b9\u6848\u4e4b\u95f4\u7684\u7ed3\u6784\u5bf9\u5e94\u5173\u7cfb\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u4ee3\u7801\u751f\u6210\u4e0d\u4ec5\u9700\u8981\u51c6\u786e\u9884\u6d4btoken\uff0c\u66f4\u9700\u8981\u7406\u89e3\u89e3\u51b3\u65b9\u6848\u7ea7\u522b\u7684\u7ed3\u6784\u5173\u7cfb\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5177\u5907\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u90e8\u7f72\u6210\u672c\u9ad8\uff1b\u5c0f\u6a21\u578b\u7f3a\u4e4f\u8fd9\u79cd\u63a8\u7406\u80fd\u529b\u3002\u56e0\u6b64\u9700\u8981\u5c06\u5927\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u84b8\u998f\u5230\u5c0f\u6a21\u578b\u4e2d\u3002", "method": "\u901a\u8fc7\u7ed3\u6784\u611f\u77e5\u635f\u5931\u4f18\u5316\u65b9\u6cd5\uff0c\u8bad\u7ec3\u5c0f\u6a21\u578b\u6a21\u62df\u5927\u6a21\u578b\u7684\u63a8\u7406\u548c\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u5b66\u4e60\u8bc6\u522b\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u8def\u5f84\uff0c\u5efa\u7acb\u95ee\u9898\u5b9a\u4e49\u4e0e\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u4e4b\u95f4\u7684\u7ed3\u6784\u5bf9\u5e94\u5173\u7cfb\u3002", "result": "\u7ecf\u8fc7\u5ec9\u4ef7\u7b80\u5355\u7684\u5fae\u8c03\u8fc7\u7a0b\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728MBPP\u3001MBPP Plus\u548cHumanEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728pass@1\u3001\u5e73\u5747\u6570\u636e\u6d41\u548c\u5e73\u5747\u8bed\u6cd5\u5339\u914d\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u84b8\u998f\u5230\u5c0f\u6a21\u578b\u4e2d\uff0c\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u4ee3\u7801\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u5927\u7684\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002"}}
{"id": "2510.17196", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17196", "abs": "https://arxiv.org/abs/2510.17196", "authors": ["Jiaqi Leng", "Xiang Hu", "Junxiong Wang", "Jianguo Li", "Wei Wu", "Yucheng Lu"], "title": "Understanding and Improving Length Generalization in Hierarchical Sparse Attention Models", "comment": "Preprint. Work in progress", "summary": "Effectively processing long contexts is a critical challenge for language\nmodels. While standard Transformers are limited by quadratic complexity and\npoor length extrapolation, alternative architectures like sliding window\nattention and state space models sacrifice the ability to effectively utilize\nthe full context due to their fixed-size memory. Chunk-based sparse attention\nhas emerged as a promising paradigm for extreme length generalization, yet the\nkey architectural principles underpinning its success are not yet fully\nunderstood. In this work, we present a systematic dissection of these models to\nidentify the core components driving their performance. Through a unified\nframework and comprehensive ablation studies, we demonstrate that a combination\nof three design principles is critical: (1) an expressive, non-linear Chunk\nEncoder with a dedicated CLS token to produce representations for retrieval;\n(2) a Bypassing Residual Path to stably integrate retrieved global information\nwithout it being overridden by the local residual stream; and (3) enforced\nselection sparsity during pre-training to bridge the train-test distribution\ngap. We provide a theoretical motivation for intra-chunk information processing\nand landmark generation. By combining these principles, we establish a new\nstate-of-the-art for training-free length extrapolation, successfully\ngeneralizing models trained on a 4K context to 32 million tokens on RULER and\nBABILong. Our findings provide a clear and empirically-grounded set of design\nprinciples for developing future, highly-capable long-context language models.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u57fa\u4e8e\u5206\u5757\u7a00\u758f\u6ce8\u610f\u529b\u7684\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u4e09\u4e2a\u5173\u952e\u8bbe\u8ba1\u539f\u5219\uff1a\u8868\u8fbe\u6027\u5206\u5757\u7f16\u7801\u5668\u3001\u65c1\u8def\u6b8b\u5dee\u8def\u5f84\u548c\u8bad\u7ec3\u4e2d\u5f3a\u5236\u9009\u62e9\u7a00\u758f\u6027\uff0c\u5b9e\u73b0\u4e86\u4ece4K\u4e0a\u4e0b\u6587\u52303200\u4e07token\u7684\u65e0\u8bad\u7ec3\u957f\u5ea6\u5916\u63a8\u3002", "motivation": "\u6807\u51c6Transformer\u5b58\u5728\u4e8c\u6b21\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u800c\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u7b49\u66ff\u4ee3\u67b6\u6784\u65e0\u6cd5\u6709\u6548\u5229\u7528\u5b8c\u6574\u4e0a\u4e0b\u6587\u3002\u5206\u5757\u7a00\u758f\u6ce8\u610f\u529b\u5728\u6781\u7aef\u957f\u5ea6\u5916\u63a8\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u6210\u529f\u7684\u5173\u952e\u67b6\u6784\u539f\u5219\u5c1a\u672a\u88ab\u5145\u5206\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u548c\u5168\u9762\u6d88\u878d\u7814\u7a76\uff0c\u8bc6\u522b\u51fa\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u4f7f\u7528\u4e13\u7528CLS\u4ee4\u724c\u7684\u8868\u8fbe\u6027\u975e\u7ebf\u6027\u5206\u5757\u7f16\u7801\u5668\uff1b2) \u65c1\u8def\u6b8b\u5dee\u8def\u5f84\u7a33\u5b9a\u96c6\u6210\u68c0\u7d22\u7684\u5168\u5c40\u4fe1\u606f\uff1b3) \u9884\u8bad\u7ec3\u4e2d\u5f3a\u5236\u9009\u62e9\u7a00\u758f\u6027\u4ee5\u5f25\u5408\u8bad\u7ec3-\u6d4b\u8bd5\u5206\u5e03\u5dee\u8ddd\u3002", "result": "\u7ed3\u5408\u8fd9\u4e9b\u539f\u5219\uff0c\u5728RULER\u548cBABILong\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u6210\u529f\u5c064K\u4e0a\u4e0b\u6587\u8bad\u7ec3\u7684\u6a21\u578b\u5916\u63a8\u52303200\u4e07token\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f00\u53d1\u672a\u6765\u9ad8\u6027\u80fd\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u5957\u6e05\u6670\u4e14\u7ecf\u9a8c\u8bc1\u7684\u8bbe\u8ba1\u539f\u5219\uff0c\u9610\u660e\u4e86\u5206\u5757\u7a00\u758f\u6ce8\u610f\u529b\u67b6\u6784\u6210\u529f\u7684\u5173\u952e\u673a\u5236\u3002"}}
{"id": "2510.17614", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.17614", "abs": "https://arxiv.org/abs/2510.17614", "authors": ["Praphul Singh", "Corey Barrett", "Sumana Srivasta", "Irfan Bulu", "Sri Gadde", "Krishnaram Kenthapadi"], "title": "OG-Rank: Learning to Rank Fast and Slow with Uncertainty and Reward-Trend Guided Adaptive Exploration", "comment": null, "summary": "Clinicians need ranking systems that work in real time and still justify\ntheir choices. Motivated by the need for a low-latency, decoder-based reranker,\nwe present OG-Rank, a single-decoder approach that pairs a pooled first-token\nscoring signal with an uncertainty-gated explanation step. The model scores all\ncandidates in one pass and generates a brief, structured rationale only when\nthe list is genuinely ambiguous, keeping latency predictable. Trained with a\ncurriculum that concentrates effort on hard cases, OG-Rank delivers strong\neffectiveness on encounter-scoped order selection (fast path: Recall@1~0.45,\nnDCG@20~0.625) and improves further when the gate activates (Recall@1~0.56,\nnDCG@20~0.699 at a 45\\% gate rate), while compact backbones show similar gains\nunder the same policy. Encoder baselines trail in both effectiveness and\nflexibility. The result is a practical recipe: rank fast by default and explain\nwhen it helps, a pattern that applies broadly to decision tasks where selective\ngeneration buys accuracy at acceptable cost. The single-policy design\nsimplifies deployment and budget planning, and the curriculum principle (spend\nmore on the hard cases, less on the easy ones) readily transfers beyond\nclinical order selection.", "AI": {"tldr": "OG-Rank\u662f\u4e00\u4e2a\u4f4e\u5ef6\u8fdf\u7684\u91cd\u65b0\u6392\u5e8f\u7cfb\u7edf\uff0c\u4f7f\u7528\u5355\u89e3\u7801\u5668\u67b6\u6784\uff0c\u7ed3\u5408\u6c60\u5316\u9996\u8bcd\u8bc4\u5206\u548c\u4e0d\u786e\u5b9a\u6027\u95e8\u63a7\u89e3\u91ca\u6b65\u9aa4\uff0c\u5728\u4fdd\u6301\u53ef\u9884\u6d4b\u5ef6\u8fdf\u7684\u540c\u65f6\u63d0\u4f9b\u6392\u540d\u548c\u9009\u62e9\u6027\u89e3\u91ca\u3002", "motivation": "\u4e34\u5e8a\u533b\u751f\u9700\u8981\u5b9e\u65f6\u5de5\u4f5c\u5e76\u80fd\u89e3\u91ca\u5176\u9009\u62e9\u7684\u6392\u540d\u7cfb\u7edf\uff0c\u9700\u8981\u4f4e\u5ef6\u8fdf\u7684\u89e3\u7801\u5668\u57fa\u7840\u91cd\u65b0\u6392\u5e8f\u5668\u3002", "method": "\u91c7\u7528\u5355\u89e3\u7801\u5668\u65b9\u6cd5\uff0c\u7ed3\u5408\u6c60\u5316\u9996\u8bcd\u8bc4\u5206\u4fe1\u53f7\u548c\u4e0d\u786e\u5b9a\u6027\u95e8\u63a7\u89e3\u91ca\u6b65\u9aa4\u3002\u6a21\u578b\u5728\u4e00\u6b21\u524d\u5411\u4f20\u9012\u4e2d\u4e3a\u6240\u6709\u5019\u9009\u8bc4\u5206\uff0c\u4ec5\u5728\u5217\u8868\u771f\u6b63\u6a21\u7cca\u65f6\u751f\u6210\u7b80\u77ed\u7684\u7ed3\u6784\u5316\u7406\u7531\u3002\u4f7f\u7528\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u5c06\u8bad\u7ec3\u91cd\u70b9\u653e\u5728\u56f0\u96be\u6848\u4f8b\u4e0a\u3002", "result": "\u5728\u906d\u9047\u8303\u56f4\u8ba2\u5355\u9009\u62e9\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff08\u5feb\u901f\u8def\u5f84\uff1aRecall@1~0.45\uff0cnDCG@20~0.625\uff09\uff0c\u5f53\u95e8\u63a7\u6fc0\u6d3b\u65f6\u6027\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\uff08Recall@1~0.56\uff0cnDCG@20~0.699\uff0c\u95e8\u63a7\u738745%\uff09\u3002\u7d27\u51d1\u9aa8\u5e72\u7f51\u7edc\u5728\u76f8\u540c\u7b56\u7565\u4e0b\u4e5f\u663e\u793a\u51fa\u7c7b\u4f3c\u589e\u76ca\u3002", "conclusion": "OG-Rank\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u65b9\u6848\uff1a\u9ed8\u8ba4\u5feb\u901f\u6392\u540d\uff0c\u5728\u6709\u52a9\u4e8e\u65f6\u63d0\u4f9b\u89e3\u91ca\u3002\u8fd9\u79cd\u6a21\u5f0f\u9002\u7528\u4e8e\u9009\u62e9\u6027\u751f\u6210\u80fd\u4ee5\u53ef\u63a5\u53d7\u6210\u672c\u6362\u53d6\u51c6\u786e\u6027\u7684\u51b3\u7b56\u4efb\u52a1\u3002\u5355\u7b56\u7565\u8bbe\u8ba1\u7b80\u5316\u4e86\u90e8\u7f72\u548c\u9884\u7b97\u89c4\u5212\uff0c\u8bfe\u7a0b\u5b66\u4e60\u539f\u5219\u53ef\u63a8\u5e7f\u5230\u4e34\u5e8a\u8ba2\u5355\u9009\u62e9\u4e4b\u5916\u3002"}}
{"id": "2510.17210", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17210", "abs": "https://arxiv.org/abs/2510.17210", "authors": ["Chenchen Tan", "Youyang Qu", "Xinghao Li", "Hui Zhang", "Shujie Cui", "Cunjian Chen", "Longxiang Gao"], "title": "Wisdom is Knowing What not to Say: Hallucination-Free LLMs Unlearning via Attention Shifting", "comment": "22 pages, 10 figures", "summary": "The increase in computing power and the necessity of AI-assisted\ndecision-making boost the growing application of large language models (LLMs).\nAlong with this, the potential retention of sensitive data of LLMs has spurred\nincreasing research into machine unlearning. However, existing unlearning\napproaches face a critical dilemma: Aggressive unlearning compromises model\nutility, while conservative strategies preserve utility but risk hallucinated\nresponses. This significantly limits LLMs' reliability in knowledge-intensive\napplications. To address this, we introduce a novel Attention-Shifting (AS)\nframework for selective unlearning. AS is driven by two design objectives: (1)\ncontext-preserving suppression that attenuates attention to fact-bearing tokens\nwithout disrupting LLMs' linguistic structure; and (2) hallucination-resistant\nresponse shaping that discourages fabricated completions when queried about\nunlearning content. AS realizes these objectives through two attention-level\ninterventions, which are importance-aware suppression applied to the unlearning\nset to reduce reliance on memorized knowledge and attention-guided retention\nenhancement that reinforces attention toward semantically essential tokens in\nthe retained dataset to mitigate unintended degradation. These two components\nare jointly optimized via a dual-loss objective, which forms a soft boundary\nthat localizes unlearning while preserving unrelated knowledge under\nrepresentation superposition. Experimental results show that AS improves\nperformance preservation over the state-of-the-art unlearning methods,\nachieving up to 15% higher accuracy on the ToFU benchmark and 10% on the TDEC\nbenchmark, while maintaining competitive hallucination-free unlearning\neffectiveness. Compared to existing methods, AS demonstrates a superior balance\nbetween unlearning effectiveness, generalization, and response reliability.", "AI": {"tldr": "\u63d0\u51fa\u4e86Attention-Shifting\uff08AS\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u9009\u62e9\u6027\u9057\u5fd8\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6548\u7528\u7684\u540c\u65f6\u907f\u514d\u5e7b\u89c9\u54cd\u5e94\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u9057\u5fd8\u65b9\u6cd5\u9762\u4e34\u4e24\u96be\u56f0\u5883\uff1a\u6fc0\u8fdb\u9057\u5fd8\u4f1a\u635f\u5bb3\u6a21\u578b\u6548\u7528\uff0c\u4fdd\u5b88\u7b56\u7565\u4f1a\u4fdd\u7559\u6548\u7528\u4f46\u4ea7\u751f\u5e7b\u89c9\u54cd\u5e94\uff0c\u9650\u5236\u4e86LLM\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002", "method": "AS\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6ce8\u610f\u529b\u7ea7\u5e72\u9884\uff1a\u91cd\u8981\u6027\u611f\u77e5\u6291\u5236\uff08\u51cf\u5c11\u5bf9\u9057\u5fd8\u5185\u5bb9\u7684\u4f9d\u8d56\uff09\u548c\u6ce8\u610f\u529b\u5f15\u5bfc\u4fdd\u7559\u589e\u5f3a\uff08\u5f3a\u5316\u4fdd\u7559\u6570\u636e\u4e2d\u8bed\u4e49\u5173\u952etoken\u7684\u6ce8\u610f\u529b\uff09\uff0c\u901a\u8fc7\u53cc\u635f\u5931\u76ee\u6807\u8054\u5408\u4f18\u5316\u3002", "result": "\u5728ToFU\u57fa\u51c6\u4e0a\u51c6\u786e\u7387\u63d0\u534715%\uff0c\u5728TDEC\u57fa\u51c6\u4e0a\u63d0\u534710%\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u6027\u7684\u65e0\u5e7b\u89c9\u9057\u5fd8\u6548\u679c\uff0c\u5728\u9057\u5fd8\u6548\u679c\u3001\u6cdb\u5316\u6027\u548c\u54cd\u5e94\u53ef\u9760\u6027\u4e4b\u95f4\u5b9e\u73b0\u66f4\u597d\u5e73\u8861\u3002", "conclusion": "AS\u6846\u67b6\u901a\u8fc7\u6ce8\u610f\u529b\u8f6c\u79fb\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86LLM\u9009\u62e9\u6027\u9057\u5fd8\u7684\u56f0\u5883\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6548\u7528\u7684\u540c\u65f6\u907f\u514d\u4e86\u5e7b\u89c9\u54cd\u5e94\uff0c\u4e3a\u77e5\u8bc6\u5bc6\u96c6\u578b\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17638", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17638", "abs": "https://arxiv.org/abs/2510.17638", "authors": ["Qingchuan Yang", "Simon Mahns", "Sida Li", "Anri Gu", "Jibang Wu", "Haifeng Xu"], "title": "LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet Arena", "comment": "https://www.prophetarena.co/", "summary": "Forecasting is not only a fundamental intellectual pursuit but also is of\nsignificant importance to societal systems such as finance and economics. With\nthe rapid advances of large language models (LLMs) trained on Internet-scale\ndata, it raises the promise of employing LLMs to forecast real-world future\nevents, an emerging paradigm we call \"LLM-as-a-Prophet\". This paper\nsystematically investigates such predictive intelligence of LLMs. To this end,\nwe build Prophet Arena, a general evaluation benchmark that continuously\ncollects live forecasting tasks and decomposes each task into distinct pipeline\nstages, in order to support our controlled and large-scale experimentation. Our\ncomprehensive evaluation reveals that many LLMs already exhibit impressive\nforecasting capabilities, reflected in, e.g., their small calibration errors,\nconsistent prediction confidence and promising market returns. However, we also\nuncover key bottlenecks towards achieving superior predictive intelligence via\nLLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of\ndata sources and slower information aggregation compared to markets when\nresolution nears.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86LLM\u4f5c\u4e3a\u9884\u6d4b\u5de5\u5177\u7684\u80fd\u529b\uff0c\u6784\u5efa\u4e86Prophet Arena\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u53d1\u73b0LLM\u5df2\u5177\u5907\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u4f46\u4e5f\u5b58\u5728\u4e8b\u4ef6\u53ec\u56de\u4e0d\u51c6\u786e\u3001\u6570\u636e\u6e90\u8bef\u89e3\u7b49\u5173\u952e\u74f6\u9888\u3002", "motivation": "\u968f\u7740\u5728\u4e92\u8054\u7f51\u89c4\u6a21\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5feb\u901f\u53d1\u5c55\uff0c\u5229\u7528LLM\u9884\u6d4b\u73b0\u5b9e\u4e16\u754c\u672a\u6765\u4e8b\u4ef6\u5177\u6709\u91cd\u8981\u6f5c\u529b\uff0c\u8fd9\u79cd\u65b0\u5174\u8303\u5f0f\u88ab\u79f0\u4e3a\"LLM-as-a-Prophet\"\u3002", "method": "\u6784\u5efaProphet Arena\u8bc4\u4f30\u57fa\u51c6\uff0c\u6301\u7eed\u6536\u96c6\u5b9e\u65f6\u9884\u6d4b\u4efb\u52a1\uff0c\u5e76\u5c06\u6bcf\u4e2a\u4efb\u52a1\u5206\u89e3\u4e3a\u4e0d\u540c\u7684\u6d41\u6c34\u7ebf\u9636\u6bb5\uff0c\u4ee5\u652f\u6301\u53d7\u63a7\u7684\u5927\u89c4\u6a21\u5b9e\u9a8c\u3002", "result": "\u7efc\u5408\u8bc4\u4f30\u663e\u793a\u8bb8\u591aLLM\u5df2\u5c55\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u8868\u73b0\u4e3a\u8f83\u5c0f\u7684\u6821\u51c6\u8bef\u5dee\u3001\u4e00\u81f4\u7684\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u548c\u6709\u524d\u666f\u7684\u5e02\u573a\u56de\u62a5\u3002\u4f46\u4e5f\u53d1\u73b0\u4e86\u5173\u952e\u74f6\u9888\u3002", "conclusion": "LLM\u4f5c\u4e3a\u9884\u6d4b\u5de5\u5177\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u4e8b\u4ef6\u53ec\u56de\u4e0d\u51c6\u786e\u3001\u6570\u636e\u6e90\u8bef\u89e3\u4ee5\u53ca\u5728\u63a5\u8fd1\u89e3\u51b3\u65f6\u6bd4\u5e02\u573a\u4fe1\u606f\u805a\u5408\u66f4\u6162\u7b49\u5173\u952e\u74f6\u9888\u3002"}}
{"id": "2510.17238", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17238", "abs": "https://arxiv.org/abs/2510.17238", "authors": ["Junlong Tong", "Yingqi Fan", "Anhao Zhao", "Yunpu Ma", "Xiaoyu Shen"], "title": "StreamingThinker: Large Language Models Can Think While Reading", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nchain of thought (CoT) reasoning. However, the current LLM reasoning paradigm\ninitiates thinking only after the entire input is available, which introduces\nunnecessary latency and weakens attention to earlier information in dynamic\nscenarios. Inspired by human cognition of thinking while reading, we first\ndesign a \\textit{\\textbf{streaming thinking}} paradigm for LLMs, where\nreasoning unfolds in the order of input and further adjusts its depth once\nreading is complete. We instantiate this paradigm with\n\\textit{StreamingThinker}, a framework that enables LLMs to think while reading\nthrough the integration of streaming CoT generation, streaming-constraint\ntraining, and streaming parallel inference. Specifically, StreamingThinker\nemploys streaming reasoning units with quality control for CoT generation,\nenforces order-preserving reasoning through streaming attention masks and\nposition encoding, and leverages parallel KV caches that decouple input\nencoding from reasoning generation, thereby ensuring alignment and enabling\ntrue concurrency. We evaluate StreamingThinker on the Qwen3 model family across\nmath reasoning, logical reasoning, and context-based QA reasoning tasks.\nExperimental results show that the StreamingThinker preserves performance\ncomparable to batch thinking, while yielding an 80\\% reduction in token waiting\nbefore the onset of reasoning and a more than 60\\% reduction in time-level\nlatency for producing the final answer, demonstrating the effectiveness of the\nstreaming paradigm for LLM reasoning. Code will be released at\n\\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this\nrepository.}", "AI": {"tldr": "\u63d0\u51faStreamingThinker\u6846\u67b6\uff0c\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5728\u9605\u8bfb\u8f93\u5165\u65f6\u5c31\u5f00\u59cb\u63a8\u7406\uff0c\u800c\u4e0d\u662f\u7b49\u5f85\u5b8c\u6574\u8f93\u5165\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u5ef6\u8fdf\u3002", "motivation": "\u5f53\u524dLLM\u63a8\u7406\u8303\u5f0f\u9700\u8981\u7b49\u5f85\u5b8c\u6574\u8f93\u5165\u624d\u5f00\u59cb\u601d\u8003\uff0c\u8fd9\u5e26\u6765\u4e86\u4e0d\u5fc5\u8981\u7684\u5ef6\u8fdf\uff0c\u4e14\u5728\u52a8\u6001\u573a\u666f\u4e0b\u4f1a\u524a\u5f31\u5bf9\u65e9\u671f\u4fe1\u606f\u7684\u6ce8\u610f\u529b\u3002", "method": "\u8bbe\u8ba1\u6d41\u5f0f\u601d\u7ef4\u8303\u5f0f\uff0c\u96c6\u6210\u6d41\u5f0fCoT\u751f\u6210\u3001\u6d41\u5f0f\u7ea6\u675f\u8bad\u7ec3\u548c\u6d41\u5f0f\u5e76\u884c\u63a8\u7406\uff0c\u4f7f\u7528\u6d41\u5f0f\u63a8\u7406\u5355\u5143\u3001\u6d41\u5f0f\u6ce8\u610f\u529b\u63a9\u7801\u548c\u5e76\u884cKV\u7f13\u5b58\u3002", "result": "\u5728Qwen3\u6a21\u578b\u7cfb\u5217\u4e0a\u6d4b\u8bd5\uff0c\u6027\u80fd\u4e0e\u6279\u91cf\u601d\u7ef4\u76f8\u5f53\uff0c\u63a8\u7406\u5f00\u59cb\u524d\u7684token\u7b49\u5f85\u65f6\u95f4\u51cf\u5c1180%\uff0c\u6700\u7ec8\u7b54\u6848\u751f\u6210\u65f6\u95f4\u5ef6\u8fdf\u51cf\u5c1160%\u4ee5\u4e0a\u3002", "conclusion": "\u6d41\u5f0f\u601d\u7ef4\u8303\u5f0f\u80fd\u591f\u6709\u6548\u964d\u4f4eLLM\u63a8\u7406\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u8d28\u91cf\uff0c\u4e3a\u5b9e\u65f6\u63a8\u7406\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.17697", "categories": ["cs.AI", "cs.LG", "cs.MA", "I.2.11; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.17697", "abs": "https://arxiv.org/abs/2510.17697", "authors": ["Anjie Liu", "Jianhong Wang", "Samuel Kaski", "Jun Wang", "Mengyue Yang"], "title": "A Principle of Targeted Intervention for Multi-Agent Reinforcement Learning", "comment": "Accepted to NeurIPS 2025", "summary": "Steering cooperative multi-agent reinforcement learning (MARL) towards\ndesired outcomes is challenging, particularly when the global guidance from a\nhuman on the whole multi-agent system is impractical in a large-scale MARL. On\nthe other hand, designing mechanisms to coordinate agents most relies on\nempirical studies, lacking a easy-to-use research tool. In this work, we employ\nmulti-agent influence diagrams (MAIDs) as a graphical framework to address the\nabove issues. First, we introduce interaction paradigms that leverage MAIDs to\nanalyze and visualize existing approaches in MARL. Then, we design a new\ninteraction paradigm based on MAIDs, referred to as targeted intervention that\nis applied to only a single targeted agent, so the problem of global guidance\ncan be mitigated. In our implementation, we introduce a causal inference\ntechnique-referred to as Pre-Strategy Intervention (PSI)-to realize the\ntargeted intervention paradigm. Since MAIDs can be regarded as a special class\nof causal diagrams, a composite desired outcome that integrates the primary\ntask goal and an additional desired outcome can be achieved by maximizing the\ncorresponding causal effect through the PSI. Moreover, the bundled relevance\ngraph analysis of MAIDs provides a tool to identify whether an MARL learning\nparadigm is workable under the design of an interaction paradigm. In\nexperiments, we demonstrate the effectiveness of our proposed targeted\nintervention, and verify the result of relevance graph analysis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5f71\u54cd\u56fe(MAIDs)\u7684\u76ee\u6807\u5e72\u9884\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ec5\u5bf9\u5355\u4e2a\u76ee\u6807\u667a\u80fd\u4f53\u8fdb\u884c\u5e72\u9884\u6765\u7f13\u89e3\u5927\u89c4\u6a21\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u5168\u5c40\u6307\u5bfc\u4e0d\u5207\u5b9e\u9645\u7684\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u5168\u5c40\u6307\u5bfc\u4e0d\u5207\u5b9e\u9645\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u7f3a\u4e4f\u6613\u7528\u7814\u7a76\u5de5\u5177\u6765\u534f\u8c03\u667a\u80fd\u4f53\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u5f71\u54cd\u56fe(MAIDs)\u4f5c\u4e3a\u56fe\u5f62\u6846\u67b6\uff0c\u8bbe\u8ba1\u76ee\u6807\u5e72\u9884\u8303\u5f0f\uff0c\u5e76\u5f15\u5165\u9884\u7b56\u7565\u5e72\u9884(PSI)\u56e0\u679c\u63a8\u65ad\u6280\u672f\u6765\u5b9e\u73b0\u8be5\u8303\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u76ee\u6807\u5e72\u9884\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u9a8c\u8bc1\u4e86\u76f8\u5173\u6027\u56fe\u5206\u6790\u7684\u7ed3\u679c\u3002", "conclusion": "MAIDs\u6846\u67b6\u4e3a\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5206\u6790\u548c\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u76ee\u6807\u5e72\u9884\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u671f\u671b\u7ed3\u679c\uff0c\u540c\u65f6\u7f13\u89e3\u5168\u5c40\u6307\u5bfc\u7684\u6311\u6218\u3002"}}
{"id": "2510.17247", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17247", "abs": "https://arxiv.org/abs/2510.17247", "authors": ["Zefan Cai", "Haoyi Qiu", "Haozhe Zhao", "Ke Wan", "Jiachen Li", "Jiuxiang Gu", "Wen Xiao", "Nanyun Peng", "Junjie Hu"], "title": "From Preferences to Prejudice: The Role of Alignment Tuning in Shaping Social Bias in Video Diffusion Models", "comment": null, "summary": "Recent advances in video diffusion models have significantly enhanced\ntext-to-video generation, particularly through alignment tuning using reward\nmodels trained on human preferences. While these methods improve visual\nquality, they can unintentionally encode and amplify social biases. To\nsystematically trace how such biases evolve throughout the alignment pipeline,\nwe introduce VideoBiasEval, a comprehensive diagnostic framework for evaluating\nsocial representation in video generation. Grounded in established social bias\ntaxonomies, VideoBiasEval employs an event-based prompting strategy to\ndisentangle semantic content (actions and contexts) from actor attributes\n(gender and ethnicity). It further introduces multi-granular metrics to\nevaluate (1) overall ethnicity bias, (2) gender bias conditioned on ethnicity,\n(3) distributional shifts in social attributes across model variants, and (4)\nthe temporal persistence of bias within videos. Using this framework, we\nconduct the first end-to-end analysis connecting biases in human preference\ndatasets, their amplification in reward models, and their propagation through\nalignment-tuned video diffusion models. Our results reveal that alignment\ntuning not only strengthens representational biases but also makes them\ntemporally stable, producing smoother yet more stereotyped portrayals. These\nfindings highlight the need for bias-aware evaluation and mitigation throughout\nthe alignment process to ensure fair and socially responsible video generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VideoBiasEval\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e2d\u7684\u793e\u4f1a\u504f\u89c1\uff0c\u53d1\u73b0\u5bf9\u9f50\u8c03\u4f18\u4f1a\u653e\u5927\u5e76\u7a33\u5b9a\u5316\u523b\u677f\u5370\u8c61\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\u901a\u8fc7\u5956\u52b1\u6a21\u578b\u8fdb\u884c\u5bf9\u9f50\u8c03\u4f18\uff0c\u867d\u7136\u63d0\u5347\u4e86\u89c6\u89c9\u8d28\u91cf\uff0c\u4f46\u53ef\u80fd\u65e0\u610f\u4e2d\u7f16\u7801\u5e76\u653e\u5927\u4e86\u793e\u4f1a\u504f\u89c1\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u8fd9\u79cd\u504f\u89c1\u7684\u6f14\u5316\u8fc7\u7a0b\u3002", "method": "\u5f15\u5165VideoBiasEval\u8bca\u65ad\u6846\u67b6\uff0c\u57fa\u4e8e\u793e\u4f1a\u504f\u89c1\u5206\u7c7b\u5b66\uff0c\u91c7\u7528\u57fa\u4e8e\u4e8b\u4ef6\u7684\u63d0\u793a\u7b56\u7565\uff0c\u5206\u79bb\u8bed\u4e49\u5185\u5bb9\u548c\u6f14\u5458\u5c5e\u6027\uff0c\u5e76\u5f15\u5165\u591a\u7c92\u5ea6\u6307\u6807\u8fdb\u884c\u504f\u89c1\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5bf9\u9f50\u8c03\u4f18\u4e0d\u4ec5\u52a0\u5f3a\u4e86\u4ee3\u8868\u6027\u504f\u89c1\uff0c\u8fd8\u4f7f\u5176\u5728\u65f6\u95f4\u4e0a\u66f4\u52a0\u7a33\u5b9a\uff0c\u4ea7\u751f\u66f4\u5e73\u6ed1\u4f46\u66f4\u523b\u677f\u7684\u63cf\u8ff0\u3002", "conclusion": "\u9700\u8981\u5728\u5bf9\u9f50\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u504f\u89c1\u611f\u77e5\u7684\u8bc4\u4f30\u548c\u7f13\u89e3\uff0c\u4ee5\u786e\u4fdd\u516c\u5e73\u548c\u8d1f\u8d23\u4efb\u7684\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2510.17705", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17705", "abs": "https://arxiv.org/abs/2510.17705", "authors": ["Dayan Pan", "Zhaoyang Fu", "Jingyuan Wang", "Xiao Han", "Yue Zhu", "Xiangyu Zhao"], "title": "Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation in Large Language Models", "comment": "Accepted by CIKM' 25", "summary": "Large Language Models (LLMs) possess remarkable generalization capabilities\nbut struggle with multi-task adaptation, particularly in balancing knowledge\nretention with task-specific specialization. Conventional fine-tuning methods\nsuffer from catastrophic forgetting and substantial resource consumption, while\nexisting parameter-efficient methods perform suboptimally in complex multi-task\nscenarios. To address this, we propose Contextual Attention Modulation (CAM), a\nnovel mechanism that dynamically modulates the representations of\nself-attention modules in LLMs. CAM enhances task-specific features while\npreserving general knowledge, thereby facilitating more effective and efficient\nadaptation. For effective multi-task adaptation, CAM is integrated into our\nHybrid Contextual Attention Modulation (HyCAM) framework, which combines a\nshared, full-parameter CAM module with multiple specialized, lightweight CAM\nmodules, enhanced by a dynamic routing strategy for adaptive knowledge fusion.\nExtensive experiments on heterogeneous tasks, including question answering,\ncode generation, and logical reasoning, demonstrate that our approach\nsignificantly outperforms existing approaches, achieving an average performance\nimprovement of 3.65%. The implemented code and data are available to ease\nreproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM.", "AI": {"tldr": "\u63d0\u51fa\u4e86Contextual Attention Modulation (CAM)\u673a\u5236\u548cHyCAM\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u5236\u81ea\u6ce8\u610f\u529b\u8868\u793a\u6765\u5e73\u8861\u77e5\u8bc6\u4fdd\u7559\u548c\u4efb\u52a1\u4e13\u4e1a\u5316\uff0c\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u5e73\u5747\u6027\u80fd\u63d0\u53473.65%", "motivation": "\u89e3\u51b3LLMs\u5728\u591a\u4efb\u52a1\u9002\u5e94\u4e2d\u7684\u6311\u6218\uff0c\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u548c\u8d44\u6e90\u6d88\u8017\u95ee\u9898\uff0c\u73b0\u6709\u53c2\u6570\u9ad8\u6548\u65b9\u6cd5\u5728\u590d\u6742\u591a\u4efb\u52a1\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73", "method": "\u63d0\u51faCAM\u673a\u5236\u52a8\u6001\u8c03\u5236\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u8868\u793a\uff0cHyCAM\u6846\u67b6\u7ed3\u5408\u5171\u4eab\u5168\u53c2\u6570CAM\u6a21\u5757\u548c\u591a\u4e2a\u8f7b\u91cf\u7ea7\u4e13\u7528CAM\u6a21\u5757\uff0c\u91c7\u7528\u52a8\u6001\u8def\u7531\u7b56\u7565\u8fdb\u884c\u81ea\u9002\u5e94\u77e5\u8bc6\u878d\u5408", "result": "\u5728\u95ee\u7b54\u3001\u4ee3\u7801\u751f\u6210\u548c\u903b\u8f91\u63a8\u7406\u7b49\u5f02\u6784\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u53473.65%", "conclusion": "CAM\u548cHyCAM\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLMs\u5728\u591a\u4efb\u52a1\u9002\u5e94\u4e2d\u7684\u5e73\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u6709\u6548\u548c\u9ad8\u6548\u7684\u9002\u5e94"}}
{"id": "2510.17252", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17252", "abs": "https://arxiv.org/abs/2510.17252", "authors": ["Mohd Ruhul Ameen", "Akif Islam", "Abu Saleh Musa Miah", "Ayesha Siddiqua", "Jungpil Shin"], "title": "How News Feels: Understanding Affective Bias in Multilingual Headlines for Human-Centered Media Design", "comment": "15 pages, 7 figures, 4 tables. Submitted to the International\n  Conference on Data and Applied Analytics (IDAA 2025)", "summary": "News media often shape the public mood not only by what they report but by\nhow they frame it. The same event can appear calm in one outlet and alarming in\nanother, reflecting subtle emotional bias in reporting. Negative or emotionally\ncharged headlines tend to attract more attention and spread faster, which in\nturn encourages outlets to frame stories in ways that provoke stronger\nreactions. This research explores that tendency through large-scale emotion\nanalysis of Bengali news. Using zero-shot inference with Gemma-3 4B, we\nanalyzed 300000 Bengali news headlines and their content to identify the\ndominant emotion and overall tone of each. The findings reveal a clear\ndominance of negative emotions, particularly anger, fear, and disappointment,\nand significant variation in how similar stories are emotionally portrayed\nacross outlets. Based on these insights, we propose design ideas for a\nhuman-centered news aggregator that visualizes emotional cues and helps readers\nrecognize hidden affective framing in daily news.", "AI": {"tldr": "\u901a\u8fc7\u5927\u89c4\u6a21\u60c5\u611f\u5206\u6790\u53d1\u73b0\u5b5f\u52a0\u62c9\u8bed\u65b0\u95fb\u6807\u9898\u4e2d\u8d1f\u9762\u60c5\u7eea\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u7279\u522b\u662f\u6124\u6012\u3001\u6050\u60e7\u548c\u5931\u671b\uff0c\u4e0d\u540c\u5a92\u4f53\u5bf9\u76f8\u4f3c\u4e8b\u4ef6\u7684\u62a5\u9053\u5b58\u5728\u663e\u8457\u60c5\u611f\u5dee\u5f02\u3002", "motivation": "\u7814\u7a76\u65b0\u95fb\u5a92\u4f53\u5982\u4f55\u901a\u8fc7\u60c5\u611f\u6846\u67b6\u5f71\u54cd\u516c\u4f17\u60c5\u7eea\uff0c\u4ee5\u53ca\u8d1f\u9762\u60c5\u7eea\u6807\u9898\u5982\u4f55\u83b7\u5f97\u66f4\u591a\u5173\u6ce8\u548c\u4f20\u64ad\uff0c\u4ece\u800c\u9f13\u52b1\u5a92\u4f53\u91c7\u7528\u66f4\u5f3a\u70c8\u7684\u60c5\u611f\u8868\u8fbe\u65b9\u5f0f\u3002", "method": "\u4f7f\u7528Gemma-3 4B\u6a21\u578b\u5bf9300000\u4e2a\u5b5f\u52a0\u62c9\u8bed\u65b0\u95fb\u6807\u9898\u53ca\u5176\u5185\u5bb9\u8fdb\u884c\u96f6\u6837\u672c\u63a8\u7406\uff0c\u8bc6\u522b\u6bcf\u7bc7\u6587\u7ae0\u7684\u4e3b\u8981\u60c5\u7eea\u548c\u6574\u4f53\u57fa\u8c03\u3002", "result": "\u53d1\u73b0\u8d1f\u9762\u60c5\u7eea\u660e\u663e\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u7279\u522b\u662f\u6124\u6012\u3001\u6050\u60e7\u548c\u5931\u671b\uff0c\u4e0d\u540c\u5a92\u4f53\u5bf9\u76f8\u4f3c\u6545\u4e8b\u7684\u62a5\u9053\u5b58\u5728\u663e\u8457\u7684\u60c5\u611f\u5dee\u5f02\u3002", "conclusion": "\u57fa\u4e8e\u7814\u7a76\u7ed3\u679c\uff0c\u63d0\u51fa\u4e86\u4ee5\u4eba\u4e3a\u672c\u7684\u65b0\u95fb\u805a\u5408\u5668\u8bbe\u8ba1\u7406\u5ff5\uff0c\u901a\u8fc7\u53ef\u89c6\u5316\u60c5\u611f\u7ebf\u7d22\u5e2e\u52a9\u8bfb\u8005\u8bc6\u522b\u65e5\u5e38\u65b0\u95fb\u4e2d\u9690\u85cf\u7684\u60c5\u611f\u6846\u67b6\u3002"}}
{"id": "2510.17771", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17771", "abs": "https://arxiv.org/abs/2510.17771", "authors": ["Zhining Liu", "Ziyi Chen", "Hui Liu", "Chen Luo", "Xianfeng Tang", "Suhang Wang", "Joy Zeng", "Zhenwei Dai", "Zhan Shi", "Tianxin Wei", "Benoit Dumoulin", "Hanghang Tong"], "title": "Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs", "comment": "21 pages, 10 figures, 6 tables", "summary": "Vision-Language Models (VLMs) achieve strong results on multimodal tasks such\nas visual question answering, yet they can still fail even when the correct\nvisual evidence is present. In this work, we systematically investigate whether\nthese failures arise from not perceiving the evidence or from not leveraging it\neffectively. By examining layer-wise attention dynamics, we find that shallow\nlayers focus primarily on text, while deeper layers sparsely but reliably\nattend to localized evidence regions. Surprisingly, VLMs often perceive the\nvisual evidence when outputting incorrect answers, a phenomenon we term\n``seeing but not believing'' that widely exists in major VLM families. Building\non this, we introduce an inference-time intervention that highlights deep-layer\nevidence regions through selective attention-based masking. It requires no\ntraining and consistently improves accuracy across multiple families, including\nLLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable\nevidence internally but under-utilize it, making such signals explicit can\nbridge the gap between perception and reasoning, advancing the diagnostic\nunderstanding and reliability of VLMs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6df1\u5c42\u80fd\u591f\u53ef\u9760\u5730\u611f\u77e5\u89c6\u89c9\u8bc1\u636e\uff0c\u4f46\u672a\u80fd\u6709\u6548\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\uff0c\u5bfc\u81f4\"\u770b\u89c1\u4f46\u4e0d\u76f8\u4fe1\"\u73b0\u8c61\u3002\u901a\u8fc7\u57fa\u4e8e\u6ce8\u610f\u529b\u63a9\u7801\u7684\u63a8\u7406\u65f6\u5e72\u9884\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u63d0\u5347\u591a\u4e2aVLM\u5bb6\u65cf\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u867d\u7136\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5373\u4f7f\u5b58\u5728\u6b63\u786e\u7684\u89c6\u89c9\u8bc1\u636e\u65f6\u4ecd\u7136\u4f1a\u5931\u8d25\u3002\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u6027\u5730\u63a2\u7a76\u8fd9\u4e9b\u5931\u8d25\u662f\u7531\u4e8e\u672a\u611f\u77e5\u5230\u8bc1\u636e\u8fd8\u662f\u672a\u80fd\u6709\u6548\u5229\u7528\u8bc1\u636e\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5c42\u95f4\u6ce8\u610f\u529b\u52a8\u6001\uff0c\u53d1\u73b0\u6d45\u5c42\u4e3b\u8981\u5173\u6ce8\u6587\u672c\uff0c\u800c\u6df1\u5c42\u7a00\u758f\u4f46\u53ef\u9760\u5730\u5173\u6ce8\u5c40\u90e8\u8bc1\u636e\u533a\u57df\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u63a8\u7406\u65f6\u5e72\u9884\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u6ce8\u610f\u529b\u63a9\u7801\u6765\u7a81\u51fa\u6df1\u5c42\u8bc1\u636e\u533a\u57df\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVLMs\u5728\u8f93\u51fa\u9519\u8bef\u7b54\u6848\u65f6\u5f80\u5f80\u5df2\u7ecf\u611f\u77e5\u5230\u4e86\u89c6\u89c9\u8bc1\u636e\u3002\u63d0\u51fa\u7684\u5e72\u9884\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\uff0c\u5728LLaVA\u3001Qwen\u3001Gemma\u548cInternVL\u7b49\u591a\u4e2aVLM\u5bb6\u65cf\u4e2d\u5747\u80fd\u6301\u7eed\u63d0\u5347\u51c6\u786e\u7387\u3002", "conclusion": "VLMs\u5185\u90e8\u7f16\u7801\u4e86\u53ef\u9760\u7684\u8bc1\u636e\u4f46\u672a\u80fd\u5145\u5206\u5229\u7528\uff0c\u901a\u8fc7\u4f7f\u8fd9\u4e9b\u4fe1\u53f7\u663e\u5f0f\u5316\u53ef\u4ee5\u5f25\u5408\u611f\u77e5\u4e0e\u63a8\u7406\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63a8\u8fdb\u5bf9VLM\u7684\u8bca\u65ad\u6027\u7406\u89e3\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2510.17256", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17256", "abs": "https://arxiv.org/abs/2510.17256", "authors": ["Shahin Atakishiyev", "Housam K. B. Babiker", "Jiayi Dai", "Nawshad Farruque", "Teruaki Hayashi", "Nafisa Sadaf Hriti", "Md Abed Rahman", "Iain Smith", "Mi-Young Kim", "Osmar R. Za\u00efane", "Randy Goebel"], "title": "Explainability of Large Language Models: Opportunities and Challenges toward Generating Trustworthy Explanations", "comment": null, "summary": "Large language models have exhibited impressive performance across a broad\nrange of downstream tasks in natural language processing. However, how a\nlanguage model predicts the next token and generates content is not generally\nunderstandable by humans. Furthermore, these models often make errors in\nprediction and reasoning, known as hallucinations. These errors underscore the\nurgent need to better understand and interpret the intricate inner workings of\nlanguage models and how they generate predictive outputs. Motivated by this\ngap, this paper investigates local explainability and mechanistic\ninterpretability within Transformer-based large language models to foster trust\nin such models. In this regard, our paper aims to make three key contributions.\nFirst, we present a review of local explainability and mechanistic\ninterpretability approaches and insights from relevant studies in the\nliterature. Furthermore, we describe experimental studies on explainability and\nreasoning with large language models in two critical domains -- healthcare and\nautonomous driving -- and analyze the trust implications of such explanations\nfor explanation receivers. Finally, we summarize current unaddressed issues in\nthe evolving landscape of LLM explainability and outline the opportunities,\ncritical challenges, and future directions toward generating human-aligned,\ntrustworthy LLM explanations.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86Transformer\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5c40\u90e8\u53ef\u89e3\u91ca\u6027\u548c\u673a\u5236\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u5728\u533b\u7597\u548c\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u5b9e\u9a8c\u7814\u7a76\uff0c\u5e76\u603b\u7ed3\u4e86\u5f53\u524d\u672a\u89e3\u51b3\u7684\u95ee\u9898\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u7531\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9884\u6d4b\u548c\u63a8\u7406\u4e2d\u7ecf\u5e38\u51fa\u73b0\u5e7b\u89c9\u9519\u8bef\uff0c\u4e14\u5176\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u5bf9\u4eba\u7c7b\u4e0d\u53ef\u7406\u89e3\uff0c\u8feb\u5207\u9700\u8981\u66f4\u597d\u5730\u7406\u89e3\u548c\u89e3\u91ca\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u751f\u6210\u9884\u6d4b\u8f93\u51fa\uff0c\u4ee5\u5efa\u7acb\u5bf9\u8fd9\u4e9b\u6a21\u578b\u7684\u4fe1\u4efb\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u5206\u6790\u5c40\u90e8\u53ef\u89e3\u91ca\u6027\u548c\u673a\u5236\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u5e76\u5728\u533b\u7597\u548c\u81ea\u52a8\u9a7e\u9a76\u4e24\u4e2a\u5173\u952e\u9886\u57df\u8fdb\u884c\u5b9e\u9a8c\u7814\u7a76\uff0c\u5206\u6790\u89e3\u91ca\u5bf9\u63a5\u6536\u8005\u7684\u4fe1\u4efb\u5f71\u54cd\u3002", "result": "\u7cfb\u7edf\u68b3\u7406\u4e86\u73b0\u6709\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u89e3\u91ca\u5728\u7279\u5b9a\u9886\u57df\u7684\u4f5c\u7528\uff0c\u5e76\u8bc6\u522b\u4e86\u5f53\u524d\u7814\u7a76\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002", "conclusion": "\u603b\u7ed3\u4e86LLM\u53ef\u89e3\u91ca\u6027\u9886\u57df\u5f53\u524d\u672a\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u751f\u6210\u4eba\u7c7b\u5bf9\u9f50\u3001\u53ef\u4fe1\u8d56LLM\u89e3\u91ca\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2510.17263", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17263", "abs": "https://arxiv.org/abs/2510.17263", "authors": ["Avishek Lahiri", "Yufang Hou", "Debarshi Kumar Sanyal"], "title": "TaxoAlign: Scholarly Taxonomy Generation Using Language Models", "comment": "This paper has been accepted at the EMNLP 2025 Main Conference", "summary": "Taxonomies play a crucial role in helping researchers structure and navigate\nknowledge in a hierarchical manner. They also form an important part in the\ncreation of comprehensive literature surveys. The existing approaches to\nautomatic survey generation do not compare the structure of the generated\nsurveys with those written by human experts. To address this gap, we present\nour own method for automated taxonomy creation that can bridge the gap between\nhuman-generated and automatically-created taxonomies. For this purpose, we\ncreate the CS-TaxoBench benchmark which consists of 460 taxonomies that have\nbeen extracted from human-written survey papers. We also include an additional\ntest set of 80 taxonomies curated from conference survey papers. We propose\nTaxoAlign, a three-phase topic-based instruction-guided method for scholarly\ntaxonomy generation. Additionally, we propose a stringent automated evaluation\nframework that measures the structural alignment and semantic coherence of\nautomatically generated taxonomies in comparison to those created by human\nexperts. We evaluate our method and various baselines on CS-TaxoBench, using\nboth automated evaluation metrics and human evaluation studies. The results\nshow that TaxoAlign consistently surpasses the baselines on nearly all metrics.\nThe code and data can be found at https://github.com/AvishekLahiri/TaxoAlign.", "AI": {"tldr": "\u63d0\u51faTaxoAlign\u65b9\u6cd5\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u5b66\u672f\u5206\u7c7b\u6cd5\uff0c\u521b\u5efaCS-TaxoBench\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u4e25\u683c\u8bc4\u4f30\u6846\u67b6\u9a8c\u8bc1\u5176\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u6587\u732e\u7efc\u8ff0\u751f\u6210\u65b9\u6cd5\u7f3a\u4e4f\u4e0e\u4eba\u5de5\u4e13\u5bb6\u521b\u5efa\u7684\u5206\u7c7b\u6cd5\u7ed3\u6784\u5bf9\u6bd4\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51faTaxoAlign\u4e09\u9636\u6bb5\u4e3b\u9898\u5f15\u5bfc\u65b9\u6cd5\uff0c\u521b\u5efa\u5305\u542b460\u4e2a\u5206\u7c7b\u6cd5\u7684CS-TaxoBench\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u8bbe\u8ba1\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\u3002", "result": "TaxoAlign\u5728\u51e0\u4e4e\u6240\u6709\u6307\u6807\u4e0a\u90fd\u6301\u7eed\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u7ed3\u6784\u5bf9\u9f50\u548c\u8bed\u4e49\u8fde\u8d2f\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "TaxoAlign\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5f25\u5408\u4eba\u5de5\u751f\u6210\u4e0e\u81ea\u52a8\u521b\u5efa\u5206\u7c7b\u6cd5\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u81ea\u52a8\u6587\u732e\u7efc\u8ff0\u751f\u6210\u63d0\u4f9b\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17289", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17289", "abs": "https://arxiv.org/abs/2510.17289", "authors": ["Hajar Bakarou", "Mohamed Sinane El Messoussi", "Ana\u00efs Ollagnier"], "title": "Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning", "comment": null, "summary": "Antisocial behavior (ASB) on social media -- including hate speech,\nharassment, and cyberbullying -- poses growing risks to platform safety and\nsocietal well-being. Prior research has focused largely on networks such as X\nand Reddit, while \\textit{multi-party conversational settings} remain\nunderexplored due to limited data. To address this gap, we use\n\\textit{CyberAgressionAdo-Large}, a French open-access dataset simulating ASB\nin multi-party conversations, and evaluate three tasks: \\textit{abuse\ndetection}, \\textit{bullying behavior analysis}, and \\textit{bullying\npeer-group identification}. We benchmark six text-based and eight graph-based\n\\textit{representation-learning methods}, analyzing lexical cues, interactional\ndynamics, and their multimodal fusion. Results show that multimodal models\noutperform unimodal baselines. The late fusion model \\texttt{mBERT + WD-SGCN}\nachieves the best overall results, with top performance on abuse detection\n(0.718) and competitive scores on peer-group identification (0.286) and\nbullying analysis (0.606). Error analysis highlights its effectiveness in\nhandling nuanced ASB phenomena such as implicit aggression, role transitions,\nand context-dependent hostility.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u53cd\u793e\u4f1a\u884c\u4e3a\u68c0\u6d4b\uff0c\u7279\u522b\u662f\u5728\u591a\u515a\u5bf9\u8bdd\u73af\u5883\u4e2d\u3002\u901a\u8fc7\u4f7f\u7528\u6cd5\u8bed\u6570\u636e\u96c6CyberAgressionAdo-Large\uff0c\u8bc4\u4f30\u4e86\u4e09\u79cd\u4efb\u52a1\uff1a\u6ee5\u7528\u68c0\u6d4b\u3001\u6b3a\u51cc\u884c\u4e3a\u5206\u6790\u548c\u6b3a\u51cc\u540c\u4f34\u7fa4\u4f53\u8bc6\u522b\uff0c\u6bd4\u8f83\u4e86\u6587\u672c\u548c\u56fe\u8868\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u53cd\u793e\u4f1a\u884c\u4e3a\uff08\u5982\u4ec7\u6068\u8a00\u8bba\u3001\u9a9a\u6270\u548c\u7f51\u7edc\u6b3a\u51cc\uff09\u5bf9\u5e73\u53f0\u5b89\u5168\u548c\u793e\u798f\u6784\u6210\u98ce\u9669\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8X\u548cReddit\u7b49\u7f51\u7edc\uff0c\u800c\u591a\u515a\u5bf9\u8bdd\u73af\u5883\u7531\u4e8e\u6570\u636e\u6709\u9650\u800c\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u6cd5\u8bed\u5f00\u653e\u6570\u636e\u96c6CyberAgressionAdo-Large\u6a21\u62df\u591a\u515a\u5bf9\u8bdd\u4e2d\u7684\u53cd\u793e\u4f1a\u884c\u4e3a\uff0c\u8bc4\u4f30\u4e86\u516d\u79cd\u57fa\u4e8e\u6587\u672c\u548c\u516b\u79cd\u57fa\u4e8e\u56fe\u8868\u7684\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u5206\u6790\u8bcd\u6c47\u7ebf\u7d22\u3001\u4e92\u52a8\u52a8\u6001\u53ca\u5176\u591a\u6a21\u6001\u878d\u5408\u3002", "result": "\u591a\u6a21\u6001\u6a21\u578b\u4f18\u4e8e\u5355\u6a21\u6001\u57fa\u7ebf\u3002\u665a\u671f\u878d\u5408\u6a21\u578bmBERT + WD-SGCN\u53d6\u5f97\u6700\u4f73\u6574\u4f53\u7ed3\u679c\uff0c\u5728\u6ee5\u7528\u68c0\u6d4b\u4e0a\u8868\u73b0\u6700\u4f73\uff080.718\uff09\uff0c\u5728\u540c\u4f34\u7fa4\u4f53\u8bc6\u522b\uff080.286\uff09\u548c\u6b3a\u51cc\u5206\u6790\uff080.606\uff09\u4e0a\u4e5f\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "\u591a\u6a21\u6001\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u5fae\u5999\u7684\u53cd\u793e\u4f1a\u884c\u4e3a\u73b0\u8c61\uff0c\u5982\u9690\u542b\u653b\u51fb\u6027\u3001\u89d2\u8272\u8f6c\u6362\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u654c\u610f\u3002\u665a\u671f\u878d\u5408\u6a21\u578b\u5728\u53cd\u793e\u4f1a\u884c\u4e3a\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f18\u3002"}}
{"id": "2510.17354", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17354", "abs": "https://arxiv.org/abs/2510.17354", "authors": ["Chenghao Zhang", "Guanting Dong", "Xinyu Yang", "Zhicheng Dou"], "title": "Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation", "comment": "This work is in progress", "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing large language models (LLMs) by retrieving relevant documents from an\nexternal corpus. However, existing RAG systems primarily focus on unimodal text\ndocuments, and often fall short in real-world scenarios where both queries and\ndocuments may contain mixed modalities (such as text and images). In this\npaper, we address the challenge of Universal Retrieval-Augmented Generation\n(URAG), which involves retrieving and reasoning over mixed-modal information to\nimprove vision-language generation. To this end, we propose Nyx, a unified\nmixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate\nthe scarcity of realistic mixed-modal data, we introduce a four-stage automated\npipeline for generation and filtering, leveraging web documents to construct\nNyxQA, a dataset comprising diverse mixed-modal question-answer pairs that\nbetter reflect real-world information needs. Building on this high-quality\ndataset, we adopt a two-stage training framework for Nyx: we first perform\npre-training on NyxQA along with a variety of open-source retrieval datasets,\nfollowed by supervised fine-tuning using feedback from downstream\nvision-language models (VLMs) to align retrieval outputs with generative\npreferences. Experimental results demonstrate that Nyx not only performs\ncompetitively on standard text-only RAG benchmarks, but also excels in the more\ngeneral and realistic URAG setting, significantly improving generation quality\nin vision-language tasks.", "AI": {"tldr": "\u63d0\u51faNyx\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u6df7\u5408\u6a21\u6001\u68c0\u7d22\u5668\uff0c\u7528\u4e8e\u89e3\u51b3\u901a\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210(URAG)\u95ee\u9898\uff0c\u901a\u8fc7\u68c0\u7d22\u548c\u63a8\u7406\u6df7\u5408\u6a21\u6001\u4fe1\u606f\u6765\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709RAG\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8\u5355\u6a21\u6001\u6587\u672c\uff0c\u4f46\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u67e5\u8be2\u548c\u6587\u6863\u53ef\u80fd\u5305\u542b\u6df7\u5408\u6a21\u6001\uff08\u5982\u6587\u672c\u548c\u56fe\u50cf\uff09\uff0c\u56e0\u6b64\u9700\u8981\u89e3\u51b3\u6df7\u5408\u6a21\u6001\u4fe1\u606f\u7684\u68c0\u7d22\u548c\u63a8\u7406\u6311\u6218\u3002", "method": "\u63d0\u51fa\u56db\u9636\u6bb5\u81ea\u52a8\u7ba1\u9053\u751f\u6210\u548c\u8fc7\u6ee4\u6df7\u5408\u6a21\u6001\u6570\u636e\uff0c\u6784\u5efaNyxQA\u6570\u636e\u96c6\uff1b\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1a\u5148\u5728NyxQA\u548c\u5f00\u6e90\u68c0\u7d22\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u4f7f\u7528\u4e0b\u6e38\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53cd\u9988\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u3002", "result": "Nyx\u5728\u6807\u51c6\u6587\u672cRAG\u57fa\u51c6\u4e0a\u8868\u73b0\u6709\u7ade\u4e89\u529b\uff0c\u5728\u66f4\u901a\u7528\u548c\u73b0\u5b9e\u7684URAG\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u7684\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "Nyx\u6210\u529f\u89e3\u51b3\u4e86\u6df7\u5408\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u6311\u6218\uff0c\u5728\u901a\u7528URAG\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u591a\u6a21\u6001\u4fe1\u606f\u68c0\u7d22\u548c\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17388", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17388", "abs": "https://arxiv.org/abs/2510.17388", "authors": ["Henry Lim", "Kwan Hui Lim"], "title": "The Atomic Instruction Gap: Instruction-Tuned LLMs Struggle with Simple, Self-Contained Directives", "comment": "11 pages, 1 figure, 8 tables", "summary": "Instruction-tuned large language models (IT-LLMs) exhibit strong zero-shot\nreasoning, yet their ability to execute simple, self-contained instructions\nremains underexplored, despite this being foundational to complex\ninstruction-following. We evaluate 20 IT-LLMs on modified MMLU and MMLU-Pro\nbenchmarks, by systematically varying the format of option labels (alphabetic,\nnumeric, Roman) while keeping their meaning identical under four paradigms,\nnamely: (1) With explicit instructions, label changes cause large performance\nshifts (e.g., -30.45\\% for Roman vs. numeric), revealing instruction-format\nbias. (2) Without instructions, performance drops further (up to -10.84\\%) and\nlabel sensitivity intensifies, underscoring the role of explicit guidance. (3)\nWhen option contents are removed, models fail random-choice baselines except\nwith numeric labels, suggesting weak adherence to atomic directives. (4)\nThree-shot exemplars yield no significant gains in robustness or fidelity, and\ngeneration analyses show persistent label errors, especially for non-numeric\nformats. Across model sizes, larger LLMs achieve higher accuracy but remain\ninconsistent in instruction adherence. These results expose the insufficiencies\nof current instruction-tuning paradigms and highlight the need for evaluation\nmethods and training strategies that explicitly target atomic\ninstruction-following.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u6307\u4ee4\u8c03\u4f18\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7b80\u5355\u81ea\u5305\u542b\u6307\u4ee4\u6267\u884c\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5bf9\u9009\u9879\u6807\u7b7e\u683c\u5f0f\u654f\u611f\uff0c\u66b4\u9732\u51fa\u5f53\u524d\u6307\u4ee4\u8c03\u4f18\u8303\u5f0f\u7684\u7f3a\u9677", "motivation": "\u63a2\u7d22\u6307\u4ee4\u8c03\u4f18\u5927\u8bed\u8a00\u6a21\u578b\u6267\u884c\u7b80\u5355\u81ea\u5305\u542b\u6307\u4ee4\u7684\u80fd\u529b\uff0c\u8fd9\u662f\u590d\u6742\u6307\u4ee4\u8ddf\u968f\u7684\u57fa\u7840\u4f46\u7814\u7a76\u4e0d\u8db3", "method": "\u5728\u4fee\u6539\u540e\u7684MMLU\u548cMMLU-Pro\u57fa\u51c6\u4e0a\u8bc4\u4f3020\u4e2aIT-LLM\uff0c\u7cfb\u7edf\u6539\u53d8\u9009\u9879\u6807\u7b7e\u683c\u5f0f\uff08\u5b57\u6bcd\u3001\u6570\u5b57\u3001\u7f57\u9a6c\u6570\u5b57\uff09\uff0c\u91c7\u7528\u56db\u79cd\u5b9e\u9a8c\u8303\u5f0f", "result": "\u53d1\u73b0\u6a21\u578b\u5bf9\u6807\u7b7e\u683c\u5f0f\u654f\u611f\uff08\u7f57\u9a6cvs\u6570\u5b57\u5bfc\u81f4-30.45%\u6027\u80fd\u4e0b\u964d\uff09\uff0c\u65e0\u6307\u4ee4\u65f6\u6027\u80fd\u8fdb\u4e00\u6b65\u4e0b\u964d\uff0c\u9009\u9879\u5185\u5bb9\u79fb\u9664\u65f6\u6a21\u578b\u65e0\u6cd5\u901a\u8fc7\u968f\u673a\u57fa\u7ebf\uff0c\u4e09\u6837\u672c\u793a\u4f8b\u65e0\u663e\u8457\u6539\u5584", "conclusion": "\u5f53\u524d\u6307\u4ee4\u8c03\u4f18\u8303\u5f0f\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u9488\u5bf9\u539f\u5b50\u6307\u4ee4\u8ddf\u968f\u7684\u8bc4\u4f30\u65b9\u6cd5\u548c\u8bad\u7ec3\u7b56\u7565"}}
{"id": "2510.17389", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.17389", "abs": "https://arxiv.org/abs/2510.17389", "authors": ["Numaan Naeem", "Abdellah El Mekki", "Muhammad Abdul-Mageed"], "title": "EduAdapt: A Question Answer Benchmark Dataset for Evaluating Grade-Level Adaptability in LLMs", "comment": "28 pages, 2 figures, 14 tables, 50 listings, EMNLP 2025 Main", "summary": "Large language models (LLMs) are transforming education by answering\nquestions, explaining complex concepts, and generating content across a wide\nrange of subjects. Despite strong performance on academic benchmarks, they\noften fail to tailor responses to students' grade levels. This is a critical\nneed in K-12 education, where age-appropriate vocabulary and explanation are\nessential for effective learning. Existing models frequently produce outputs\nthat are too advanced or vague for younger learners, and there are no\nstandardized benchmarks to evaluate their ability to adjust across cognitive\nand developmental stages. To address this gap, we introduce EduAdapt, a\nbenchmark of nearly 48k grade-labeled QA pairs across nine science subjects,\nspanning Grades 1-12 and grouped into four grade levels. We evaluate a diverse\nset of open-source LLMs on EduAdapt and find that while larger models generally\nperform better, they still struggle with generating suitable responses for\nearly-grade students (Grades 1-5). Our work presents the first dataset and\nevaluation framework for assessing grade-level adaptability in LLMs, aiming to\nfoster more developmentally aligned educational AI systems through better\ntraining and prompting strategies. EduAdapt code and datasets are publicly\navailable at https://github.com/NaumanNaeem/EduAdapt.", "AI": {"tldr": "\u63d0\u51fa\u4e86EduAdapt\u57fa\u51c6\uff0c\u5305\u542b48k\u4e2a\u6309\u5e74\u7ea7\u6807\u6ce8\u7684\u79d1\u5b66\u95ee\u7b54\u5bf9\uff0c\u8bc4\u4f30LLMs\u5728\u4e0d\u540c\u5e74\u7ea7\u7684\u9002\u5e94\u6027\u8868\u73b0\u3002", "motivation": "\u73b0\u6709LLMs\u5728\u5b66\u672f\u57fa\u51c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u65e0\u6cd5\u6839\u636e\u5b66\u751f\u5e74\u7ea7\u6c34\u5e73\u8c03\u6574\u56de\u7b54\uff0c\u8fd9\u5728K-12\u6559\u80b2\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u521b\u5efa\u5305\u542b9\u4e2a\u79d1\u5b66\u5b66\u79d1\u3001\u8986\u76d61-12\u5e74\u7ea7\u7684\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u5f00\u6e90LLMs\u5728\u4e0d\u540c\u5e74\u7ea7\u7684\u9002\u5e94\u6027\u3002", "result": "\u5927\u6a21\u578b\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u5728\u4f4e\u5e74\u7ea7\uff081-5\u5e74\u7ea7\uff09\u4ecd\u96be\u4ee5\u751f\u6210\u5408\u9002\u7684\u56de\u7b54\u3002", "conclusion": "\u63d0\u51fa\u4e86\u9996\u4e2a\u8bc4\u4f30LLMs\u5e74\u7ea7\u9002\u5e94\u6027\u7684\u6570\u636e\u96c6\u548c\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u8bad\u7ec3\u548c\u63d0\u793a\u7b56\u7565\u5f00\u53d1\u66f4\u9002\u5408\u6559\u80b2\u9700\u6c42\u7684AI\u7cfb\u7edf\u3002"}}
{"id": "2510.17402", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17402", "abs": "https://arxiv.org/abs/2510.17402", "authors": ["Jiacheng Xie", "Shuai Zeng", "Yang Yu", "Xiaoting Tang", "Guanghui An", "Dong Xu"], "title": "Leveraging Group Relative Policy Optimization to Advance Large Language Models in Traditional Chinese Medicine", "comment": null, "summary": "Traditional Chinese Medicine (TCM) presents a rich and structurally unique\nknowledge system that challenges conventional applications of large language\nmodels (LLMs). Although previous TCM-specific LLMs have shown progress through\nsupervised fine-tuning, they often face limitations in alignment, data quality,\nand evaluation consistency. In this study, we introduce Ladder-base, the first\nTCM-focused LLM trained with Group Relative Policy Optimization (GRPO), a\nreinforcement learning method that improves reasoning and factual consistency\nby optimizing response selection based on intra-group comparisons. Ladder-base\nis built upon the Qwen2.5-7B-Instruct foundation model and trained exclusively\non the textual subset of the TCM-Ladder benchmark, using 80 percent of the data\nfor training and the remaining 20 percent split evenly between validation and\ntest sets. Through standardized evaluation, Ladder-base demonstrates superior\nperformance across multiple reasoning metrics when compared to both\nstate-of-the-art general-purpose LLMs such as GPT-4, Gemini 2.5, Claude 3, and\nQwen3 and domain-specific TCM models including BenTsao, HuatuoGPT2, and\nZhongjing. These findings suggest that GRPO provides an effective and efficient\nstrategy for aligning LLMs with expert-level reasoning in traditional medical\ndomains and supports the development of trustworthy and clinically grounded TCM\nartificial intelligence systems.", "AI": {"tldr": "Ladder-base\u662f\u9996\u4e2a\u91c7\u7528GRPO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8bad\u7ec3\u7684\u4e2d\u533b\u4e13\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u4e2d\u533b\u63a8\u7406\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u901a\u7528LLM\u548c\u9886\u57df\u4e13\u7528\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u4e2d\u533b\u77e5\u8bc6\u4f53\u7cfb\u72ec\u7279\uff0c\u73b0\u6709\u4e2d\u533b\u4e13\u7528LLM\u5728\u4e00\u81f4\u6027\u3001\u6570\u636e\u8d28\u91cf\u548c\u8bc4\u4f30\u6807\u51c6\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eQwen2.5-7B-Instruct\u57fa\u7840\u6a21\u578b\uff0c\u4f7f\u7528GRPO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728TCM-Ladder\u57fa\u51c6\u7684\u6587\u672c\u5b50\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u901a\u8fc7\u7ec4\u5185\u6bd4\u8f83\u4f18\u5316\u54cd\u5e94\u9009\u62e9\u3002", "result": "Ladder-base\u5728\u591a\u9879\u63a8\u7406\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86GPT-4\u3001Gemini 2.5\u7b49\u901a\u7528LLM\u4ee5\u53caBenTsao\u3001HuatuoGPT2\u7b49\u4e2d\u533b\u4e13\u7528\u6a21\u578b\u3002", "conclusion": "GRPO\u4e3a\u4f20\u7edf\u533b\u5b66\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u9ad8\u6548\u7684\u5927\u6a21\u578b\u5bf9\u9f50\u7b56\u7565\uff0c\u652f\u6301\u5f00\u53d1\u53ef\u4fe1\u8d56\u4e14\u4e34\u5e8a\u57fa\u7840\u624e\u5b9e\u7684\u4e2d\u533b\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u3002"}}
{"id": "2510.17405", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17405", "abs": "https://arxiv.org/abs/2510.17405", "authors": ["Mardiyyah Oduwole", "Prince Mireku", "Fatimo Adebanjo", "Oluwatosin Olajide", "Mahi Aminu Aliyu", "Jekaterina Novikova"], "title": "AFRICAPTION: Establishing a New Paradigm for Image Captioning in African Languages", "comment": null, "summary": "Multimodal AI research has overwhelmingly focused on high-resource languages,\nhindering the democratization of advancements in the field. To address this, we\npresent AfriCaption, a comprehensive framework for multilingual image\ncaptioning in 20 African languages and our contributions are threefold: (i) a\ncurated dataset built on Flickr8k, featuring semantically aligned captions\ngenerated via a context-aware selection and translation process; (ii) a\ndynamic, context-preserving pipeline that ensures ongoing quality through model\nensembling and adaptive substitution; and (iii) the AfriCaption model, a 0.5B\nparameter vision-to-text architecture that integrates SigLIP and NLLB200 for\ncaption generation across under-represented languages. This unified framework\nensures ongoing data quality and establishes the first scalable\nimage-captioning resource for under-represented African languages, laying the\ngroundwork for truly inclusive multimodal AI.", "AI": {"tldr": "AfriCaption\u662f\u4e00\u4e2a\u7528\u4e8e20\u79cd\u975e\u6d32\u8bed\u8a00\u7684\u591a\u8bed\u8a00\u56fe\u50cf\u63cf\u8ff0\u6846\u67b6\uff0c\u5305\u542b\u6570\u636e\u96c6\u3001\u8d28\u91cf\u4fdd\u8bc1\u6d41\u7a0b\u548c0.5B\u53c2\u6570\u6a21\u578b\uff0c\u65e8\u5728\u4fc3\u8fdb\u591a\u6a21\u6001AI\u7684\u6c11\u4e3b\u5316\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001AI\u7814\u7a76\u8fc7\u5ea6\u96c6\u4e2d\u5728\u9ad8\u8d44\u6e90\u8bed\u8a00\u7684\u95ee\u9898\uff0c\u63a8\u52a8\u975e\u6d32\u8bed\u8a00\u5728\u56fe\u50cf\u63cf\u8ff0\u9886\u57df\u7684\u5305\u5bb9\u6027\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u57fa\u4e8eFlickr8k\u7684\u8bed\u4e49\u5bf9\u9f50\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u52a8\u6001\u4e0a\u4e0b\u6587\u4fdd\u6301\u7684\u8d28\u91cf\u4fdd\u8bc1\u6d41\u7a0b\uff0c\u4ee5\u53ca\u96c6\u6210SigLIP\u548cNLLB200\u76840.5B\u53c2\u6570\u89c6\u89c9\u5230\u6587\u672c\u67b6\u6784\u3002", "result": "\u521b\u5efa\u4e86\u9996\u4e2a\u53ef\u6269\u5c55\u7684\u975e\u6d32\u8bed\u8a00\u56fe\u50cf\u63cf\u8ff0\u8d44\u6e90\uff0c\u4e3a\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u975e\u6d32\u8bed\u8a00\u5efa\u7acb\u4e86\u7edf\u4e00\u6846\u67b6\u3002", "conclusion": "AfriCaption\u4e3a\u771f\u6b63\u5305\u5bb9\u7684\u591a\u6a21\u6001AI\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u7814\u7a76\u7684\u6c11\u4e3b\u5316\u8fdb\u7a0b\u3002"}}
{"id": "2510.17415", "categories": ["cs.CL", "cs.AI", "cs.MA", "cs.MM", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17415", "abs": "https://arxiv.org/abs/2510.17415", "authors": ["Jiacheng Xie", "Yang Yu", "Yibo Chen", "Hanyao Zhang", "Lening Zhao", "Jiaxuan He", "Lei Jiang", "Xiaoting Tang", "Guanghui An", "Dong Xu"], "title": "BenCao: An Instruction-Tuned Large Language Model for Traditional Chinese Medicine", "comment": null, "summary": "Traditional Chinese Medicine (TCM), with a history spanning over two\nmillennia, plays a role in global healthcare. However, applying large language\nmodels (LLMs) to TCM remains challenging due to its reliance on holistic\nreasoning, implicit logic, and multimodal diagnostic cues. Existing TCM-domain\nLLMs have made progress in text-based understanding but lack multimodal\nintegration, interpretability, and clinical applicability. To address these\nlimitations, we developed BenCao, a ChatGPT-based multimodal assistant for TCM,\nintegrating structured knowledge bases, diagnostic data, and expert feedback\nrefinement. BenCao was trained through natural language instruction tuning\nrather than parameter retraining, aligning with expert-level reasoning and\nethical norms specific to TCM. The system incorporates a comprehensive\nknowledge base of over 1,000 classical and modern texts, a scenario-based\ninstruction framework for diverse interactions, a chain-of-thought simulation\nmechanism for interpretable reasoning, and a feedback refinement process\ninvolving licensed TCM practitioners. BenCao connects to external APIs for\ntongue-image classification and multimodal database retrieval, enabling dynamic\naccess to diagnostic resources. In evaluations across single-choice question\nbenchmarks and multimodal classification tasks, BenCao achieved superior\naccuracy to general-domain and TCM-domain models, particularly in diagnostics,\nherb recognition, and constitution classification. The model was deployed as an\ninteractive application on the OpenAI GPTs Store, accessed by nearly 1,000\nusers globally as of October 2025. This study demonstrates the feasibility of\ndeveloping a TCM-domain LLM through natural language-based instruction tuning\nand multimodal integration, offering a practical framework for aligning\ngenerative AI with traditional medical reasoning and a scalable pathway for\nreal-world deployment.", "AI": {"tldr": "\u5f00\u53d1\u4e86BenCao\uff0c\u4e00\u4e2a\u57fa\u4e8eChatGPT\u7684\u4e2d\u533b\u591a\u6a21\u6001\u52a9\u624b\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8c03\u4f18\u800c\u975e\u53c2\u6570\u91cd\u8bad\u7ec3\uff0c\u6574\u5408\u7ed3\u6784\u5316\u77e5\u8bc6\u5e93\u3001\u8bca\u65ad\u6570\u636e\u548c\u4e13\u5bb6\u53cd\u9988\uff0c\u5728\u4e2d\u533b\u95ee\u7b54\u548c\u8bca\u65ad\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u901a\u7528\u548c\u4e2d\u533b\u9886\u57df\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u4e2d\u533b\u4f9d\u8d56\u6574\u4f53\u63a8\u7406\u3001\u9690\u542b\u903b\u8f91\u548c\u591a\u6a21\u6001\u8bca\u65ad\u7ebf\u7d22\uff0c\u73b0\u6709\u4e2d\u533b\u9886\u57df\u5927\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u591a\u6a21\u6001\u6574\u5408\u3001\u53ef\u89e3\u91ca\u6027\u548c\u4e34\u5e8a\u5e94\u7528\u6027\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u5b9e\u7528\u7684\u4e2d\u533bAI\u52a9\u624b\u3002", "method": "\u6574\u54081000\u591a\u90e8\u7ecf\u5178\u548c\u73b0\u4ee3\u6587\u672c\u7684\u77e5\u8bc6\u5e93\uff0c\u57fa\u4e8e\u573a\u666f\u7684\u6307\u4ee4\u6846\u67b6\uff0c\u53ef\u89e3\u91ca\u63a8\u7406\u7684\u601d\u7ef4\u94fe\u6a21\u62df\u673a\u5236\uff0c\u6267\u4e1a\u4e2d\u533b\u5e08\u53c2\u4e0e\u7684\u53cd\u9988\u7cbe\u70bc\u8fc7\u7a0b\uff0c\u8fde\u63a5\u820c\u50cf\u5206\u7c7b\u548c\u591a\u6a21\u6001\u6570\u636e\u5e93\u68c0\u7d22\u7684\u5916\u90e8API\u3002", "result": "\u5728\u5355\u9009\u9898\u57fa\u51c6\u6d4b\u8bd5\u548c\u591a\u6a21\u6001\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cBenCao\u5728\u8bca\u65ad\u3001\u8349\u836f\u8bc6\u522b\u548c\u4f53\u8d28\u5206\u7c7b\u65b9\u9762\u51c6\u786e\u7387\u4f18\u4e8e\u901a\u7528\u548c\u4e2d\u533b\u9886\u57df\u6a21\u578b\uff0c\u5df2\u5728OpenAI GPTs\u5546\u5e97\u90e8\u7f72\uff0c\u622a\u81f32025\u5e7410\u6708\u6709\u8fd11000\u540d\u5168\u7403\u7528\u6237\u8bbf\u95ee\u3002", "conclusion": "\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8c03\u4f18\u548c\u591a\u6a21\u6001\u6574\u5408\u5f00\u53d1\u4e2d\u533b\u9886\u57df\u5927\u8bed\u8a00\u6a21\u578b\u662f\u53ef\u884c\u7684\uff0c\u4e3a\u751f\u6210\u5f0fAI\u4e0e\u4f20\u7edf\u533b\u5b66\u63a8\u7406\u5bf9\u9f50\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\u548c\u53ef\u6269\u5c55\u7684\u73b0\u5b9e\u90e8\u7f72\u8def\u5f84\u3002"}}
{"id": "2510.17426", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17426", "abs": "https://arxiv.org/abs/2510.17426", "authors": ["Tiancheng Hu", "Benjamin Minixhofer", "Nigel Collier"], "title": "Navigating the Alignment-Calibration Trade-off: A Pareto-Superior Frontier via Model Merging", "comment": null, "summary": "The \"alignment tax\" of post-training is typically framed as a drop in task\naccuracy. We show it also involves a severe loss of calibration, making models\noverconfident, less reliable, and model outputs less diverse. We show that this\ntrade-off can be navigated effectively via a simple post-hoc intervention:\ninterpolating between a model's weights before and after alignment. Crucially,\nthis is not a strict trade-off. We find that the process consistently reveals\nPareto-optimal interpolations - models that improve accuracy beyond both\nparents while substantially recovering the calibration lost during alignment.\nOur work demonstrates that simple model merging provides a computationally\nefficient method for mitigating the full scope of the alignment tax, yielding\nmodels that are more capable and more reliable.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86\u540e\u8bad\u7ec3\u5bf9\u9f50\u4e0d\u4ec5\u5bfc\u81f4\u4efb\u52a1\u51c6\u786e\u6027\u4e0b\u964d\uff0c\u8fd8\u4f1a\u4e25\u91cd\u635f\u5931\u6821\u51c6\u6027\uff0c\u4f7f\u6a21\u578b\u8fc7\u5ea6\u81ea\u4fe1\u3001\u53ef\u9760\u6027\u964d\u4f4e\u4e14\u8f93\u51fa\u591a\u6837\u6027\u51cf\u5c11\u3002\u901a\u8fc7\u7b80\u5355\u7684\u6743\u91cd\u63d2\u503c\u65b9\u6cd5\u53ef\u4ee5\u627e\u5230\u5e15\u7d2f\u6258\u6700\u4f18\u89e3\uff0c\u540c\u65f6\u63d0\u5347\u51c6\u786e\u6027\u548c\u6821\u51c6\u6027\u3002", "motivation": "\u4f20\u7edf\u4e0a\"\u5bf9\u9f50\u7a0e\"\u4ec5\u5173\u6ce8\u4efb\u52a1\u51c6\u786e\u6027\u4e0b\u964d\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u5bf9\u9f50\u8fc7\u7a0b\u8fd8\u4f1a\u5bfc\u81f4\u6a21\u578b\u6821\u51c6\u6027\u4e25\u91cd\u635f\u5931\uff0c\u4f7f\u6a21\u578b\u53d8\u5f97\u8fc7\u5ea6\u81ea\u4fe1\u548c\u4e0d\u53ef\u9760\uff0c\u8fd9\u9700\u8981\u88ab\u5168\u9762\u89e3\u51b3\u3002", "method": "\u91c7\u7528\u7b80\u5355\u7684\u540e\u5904\u7406\u5e72\u9884\uff1a\u5728\u6a21\u578b\u5bf9\u9f50\u524d\u540e\u7684\u6743\u91cd\u4e4b\u95f4\u8fdb\u884c\u63d2\u503c\uff0c\u627e\u5230\u5e15\u7d2f\u6258\u6700\u4f18\u7684\u63d2\u503c\u70b9\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u53d1\u73b0\u8d85\u8d8a\u4e24\u4e2a\u7236\u6a21\u578b\u51c6\u786e\u6027\u7684\u6a21\u578b\uff0c\u540c\u65f6\u663e\u8457\u6062\u590d\u5bf9\u9f50\u8fc7\u7a0b\u4e2d\u635f\u5931\u7684\u6821\u51c6\u6027\uff0c\u8bc1\u660e\u8fd9\u4e0d\u662f\u4e25\u683c\u7684\u6743\u8861\u5173\u7cfb\u3002", "conclusion": "\u7b80\u5355\u7684\u6a21\u578b\u5408\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u51cf\u8f7b\u5bf9\u9f50\u7a0e\u7684\u5168\u90e8\u5f71\u54cd\uff0c\u4ea7\u751f\u66f4\u5f3a\u5927\u4e14\u66f4\u53ef\u9760\u7684\u6a21\u578b\u3002"}}
{"id": "2510.17431", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17431", "abs": "https://arxiv.org/abs/2510.17431", "authors": ["Yushi Yang", "Shreyansh Padarha", "Andrew Lee", "Adam Mahdi"], "title": "Agentic Reinforcement Learning for Search is Unsafe", "comment": null, "summary": "Agentic reinforcement learning (RL) trains large language models to\nautonomously call tools during reasoning, with search as the most common\napplication. These models excel at multi-step reasoning tasks, but their safety\nproperties are not well understood. In this study, we show that RL-trained\nsearch models inherit refusal from instruction tuning and often deflect harmful\nrequests by turning them into safe queries. However, this safety is fragile.\nTwo simple attacks, one that forces the model to begin response with search\n(Search attack), another that encourages models to repeatedly search\n(Multi-search attack), trigger cascades of harmful searches and answers. Across\ntwo model families (Qwen, Llama) with both local and web search, these attacks\nlower refusal rates by up to 60.0%, answer safety by 82.5%, and search-query\nsafety by 82.4%. The attacks succeed by triggering models to generate harmful,\nrequest-mirroring search queries before they can generate the inherited refusal\ntokens. This exposes a core weakness of current RL training: it rewards\ncontinued generation of effective queries without accounting for their\nharmfulness. As a result, RL search models have vulnerabilities that users can\neasily exploit, making it urgent to develop safety-aware agentic RL pipelines\noptimising for safe search.", "AI": {"tldr": "RL\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u5728\u5de5\u5177\u8c03\u7528\u63a8\u7406\u4e2d\u8868\u73b0\u51fa\u8106\u5f31\u7684\u5b89\u5168\u6027\uff0c\u7b80\u5355\u7684\u641c\u7d22\u653b\u51fb\u53ef\u4ee5\u5927\u5e45\u964d\u4f4e\u5176\u62d2\u7edd\u7387\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u7814\u7a76RL\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u4e3b\u8c03\u7528\u5de5\u5177\u65f6\u7684\u5b89\u5168\u7279\u6027\uff0c\u7279\u522b\u662f\u641c\u7d22\u5e94\u7528\u4e2d\u7684\u8106\u5f31\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u7b80\u5355\u653b\u51fb\u65b9\u6cd5\uff1a\u5f3a\u5236\u6a21\u578b\u4ee5\u641c\u7d22\u5f00\u59cb\u54cd\u5e94\uff08\u641c\u7d22\u653b\u51fb\uff09\u548c\u9f13\u52b1\u6a21\u578b\u91cd\u590d\u641c\u7d22\uff08\u591a\u641c\u7d22\u653b\u51fb\uff09\u3002", "result": "\u653b\u51fb\u4f7f\u62d2\u7edd\u7387\u964d\u4f4e\u8fbe60.0%\uff0c\u7b54\u6848\u5b89\u5168\u6027\u964d\u4f4e82.5%\uff0c\u641c\u7d22\u67e5\u8be2\u5b89\u5168\u6027\u964d\u4f4e82.4%\u3002", "conclusion": "\u5f53\u524dRL\u8bad\u7ec3\u5b58\u5728\u6838\u5fc3\u5f31\u70b9\uff0c\u9700\u8981\u5f00\u53d1\u5b89\u5168\u611f\u77e5\u7684\u667a\u80fd\u4f53RL\u6d41\u7a0b\u6765\u4f18\u5316\u5b89\u5168\u641c\u7d22\u3002"}}
{"id": "2510.17437", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17437", "abs": "https://arxiv.org/abs/2510.17437", "authors": ["Manuela Daniela Danu", "George Marica", "Constantin Suciu", "Lucian Mihai Itu", "Oladimeji Farri"], "title": "Multilingual Clinical NER for Diseases and Medications Recognition in Cardiology Texts using BERT Embeddings", "comment": "11 pages, 5 figures, 1 table, published in Working Notes of the\n  Conference and Labs of the Evaluation Forum (CLEF 2024)", "summary": "The rapidly increasing volume of electronic health record (EHR) data\nunderscores a pressing need to unlock biomedical knowledge from unstructured\nclinical texts to support advancements in data-driven clinical systems,\nincluding patient diagnosis, disease progression monitoring, treatment effects\nassessment, prediction of future clinical events, etc. While contextualized\nlanguage models have demonstrated impressive performance improvements for named\nentity recognition (NER) systems in English corpora, there remains a scarcity\nof research focused on clinical texts in low-resource languages. To bridge this\ngap, our study aims to develop multiple deep contextual embedding models to\nenhance clinical NER in the cardiology domain, as part of the BioASQ\nMultiCardioNER shared task. We explore the effectiveness of different\nmonolingual and multilingual BERT-based models, trained on general domain text,\nfor extracting disease and medication mentions from clinical case reports\nwritten in English, Spanish, and Italian. We achieved an F1-score of 77.88% on\nSpanish Diseases Recognition (SDR), 92.09% on Spanish Medications Recognition\n(SMR), 91.74% on English Medications Recognition (EMR), and 88.9% on Italian\nMedications Recognition (IMR). These results outperform the mean and median F1\nscores in the test leaderboard across all subtasks, with the mean/median values\nbeing: 69.61%/75.66% for SDR, 81.22%/90.18% for SMR, 89.2%/88.96% for EMR, and\n82.8%/87.76% for IMR.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u57fa\u4e8eBERT\u7684\u6df1\u5ea6\u4e0a\u4e0b\u6587\u5d4c\u5165\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u5347\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u548c\u610f\u5927\u5229\u8bed\u4e34\u5e8a\u6587\u672c\u4e2d\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5fc3\u810f\u75c5\u5b66\u9886\u57df\uff0c\u5728BioASQ MultiCardioNER\u5171\u4eab\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u5e73\u5747\u6c34\u5e73\u7684F1\u5206\u6570\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u5feb\u901f\u589e\u957f\uff0c\u9700\u8981\u4ece\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u6587\u672c\u4e2d\u63d0\u53d6\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u6765\u652f\u6301\u6570\u636e\u9a71\u52a8\u7684\u4e34\u5e8a\u7cfb\u7edf\u53d1\u5c55\u3002\u867d\u7136\u82f1\u8bed\u8bed\u6599\u5e93\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7cfb\u7edf\u5df2\u6709\u663e\u8457\u8fdb\u6b65\uff0c\u4f46\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e34\u5e8a\u6587\u672c\u7684\u7814\u7a76\u4ecd\u7136\u7a00\u7f3a\u3002", "method": "\u63a2\u7d22\u4e86\u5728\u901a\u7528\u9886\u57df\u6587\u672c\u4e0a\u8bad\u7ec3\u7684\u5355\u8bed\u548c\u591a\u8bed\u8a00BERT\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u7528\u4e8e\u4ece\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u548c\u610f\u5927\u5229\u8bed\u7684\u4e34\u5e8a\u75c5\u4f8b\u62a5\u544a\u4e2d\u63d0\u53d6\u75be\u75c5\u548c\u836f\u7269\u63d0\u53ca\u3002", "result": "\u5728\u897f\u73ed\u7259\u8bed\u75be\u75c5\u8bc6\u522b\u4e2d\u83b7\u5f9777.88%\u7684F1\u5206\u6570\uff0c\u897f\u73ed\u7259\u8bed\u836f\u7269\u8bc6\u522b92.09%\uff0c\u82f1\u8bed\u836f\u7269\u8bc6\u522b91.74%\uff0c\u610f\u5927\u5229\u8bed\u836f\u7269\u8bc6\u522b88.9%\uff0c\u6240\u6709\u5b50\u4efb\u52a1\u7684\u6210\u7ee9\u5747\u4f18\u4e8e\u6d4b\u8bd5\u6392\u884c\u699c\u7684\u5e73\u5747\u503c\u548c\u4e2d\u4f4d\u6570\u3002", "conclusion": "\u57fa\u4e8eBERT\u7684\u6df1\u5ea6\u4e0a\u4e0b\u6587\u5d4c\u5165\u6a21\u578b\u80fd\u591f\u6709\u6548\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e34\u5e8a\u6587\u672c\u4e2d\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u6027\u80fd\uff0c\u4e3a\u591a\u8bed\u8a00\u4e34\u5e8aNLP\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2510.17460", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17460", "abs": "https://arxiv.org/abs/2510.17460", "authors": ["Muhammad Farmal Khan", "Mousumi Akter"], "title": "Evaluating Large Language Models on Urdu Idiom Translation", "comment": null, "summary": "Idiomatic translation remains a significant challenge in machine translation,\nespecially for low resource languages such as Urdu, and has received limited\nprior attention. To advance research in this area, we introduce the first\nevaluation datasets for Urdu to English idiomatic translation, covering both\nNative Urdu and Roman Urdu scripts and annotated with gold-standard English\nequivalents. We evaluate multiple open-source Large Language Models (LLMs) and\nNeural Machine Translation (NMT) systems on this task, focusing on their\nability to preserve idiomatic and cultural meaning. Automatic metrics including\nBLEU, BERTScore, COMET, and XCOMET are used to assess translation quality. Our\nfindings indicate that prompt engineering enhances idiomatic translation\ncompared to direct translation, though performance differences among prompt\ntypes are relatively minor. Moreover, cross script comparisons reveal that text\nrepresentation substantially affects translation quality, with Native Urdu\ninputs producing more accurate idiomatic translations than Roman Urdu.", "AI": {"tldr": "\u672c\u6587\u6784\u5efa\u4e86\u9996\u4e2a\u4e4c\u5c14\u90fd\u8bed\u5230\u82f1\u8bed\u7684\u4e60\u8bed\u7ffb\u8bd1\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u591a\u79cdLLM\u548cNMT\u7cfb\u7edf\uff0c\u53d1\u73b0\u63d0\u793a\u5de5\u7a0b\u80fd\u63d0\u5347\u4e60\u8bed\u7ffb\u8bd1\u8d28\u91cf\uff0c\u4e14\u539f\u751f\u4e4c\u5c14\u90fd\u8bed\u6587\u672c\u6bd4\u7f57\u9a6c\u5316\u6587\u672c\u7ffb\u8bd1\u6548\u679c\u66f4\u597d\u3002", "motivation": "\u4e60\u8bed\u7ffb\u8bd1\u5728\u673a\u5668\u7ffb\u8bd1\u4e2d\u4ecd\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u4e4c\u5c14\u90fd\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u6b64\u524d\u7814\u7a76\u5173\u6ce8\u6709\u9650\u3002", "method": "\u6784\u5efa\u9996\u4e2a\u4e4c\u5c14\u90fd\u8bed\u5230\u82f1\u8bed\u4e60\u8bed\u7ffb\u8bd1\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u591a\u79cd\u5f00\u6e90LLM\u548cNMT\u7cfb\u7edf\uff0c\u4f7f\u7528BLEU\u3001BERTScore\u3001COMET\u548cXCOMET\u7b49\u81ea\u52a8\u6307\u6807\u8bc4\u4f30\u7ffb\u8bd1\u8d28\u91cf\u3002", "result": "\u63d0\u793a\u5de5\u7a0b\u76f8\u6bd4\u76f4\u63a5\u7ffb\u8bd1\u80fd\u63d0\u5347\u4e60\u8bed\u7ffb\u8bd1\u8d28\u91cf\uff0c\u4f46\u4e0d\u540c\u63d0\u793a\u7c7b\u578b\u95f4\u5dee\u5f02\u8f83\u5c0f\uff1b\u539f\u751f\u4e4c\u5c14\u90fd\u8bed\u8f93\u5165\u7684\u4e60\u8bed\u7ffb\u8bd1\u51c6\u786e\u5ea6\u9ad8\u4e8e\u7f57\u9a6c\u5316\u4e4c\u5c14\u90fd\u8bed\u3002", "conclusion": "\u6587\u672c\u8868\u793a\u65b9\u5f0f\u663e\u8457\u5f71\u54cd\u7ffb\u8bd1\u8d28\u91cf\uff0c\u539f\u751f\u4e4c\u5c14\u90fd\u8bed\u5728\u4e60\u8bed\u7ffb\u8bd1\u4e2d\u8868\u73b0\u4f18\u4e8e\u7f57\u9a6c\u5316\u5f62\u5f0f\uff0c\u63d0\u793a\u5de5\u7a0b\u662f\u63d0\u5347\u4e60\u8bed\u7ffb\u8bd1\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2510.17476", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17476", "abs": "https://arxiv.org/abs/2510.17476", "authors": ["Ipek Baris Schlicht", "Burcu Sayin", "Zhixue Zhao", "Frederik M. Labont\u00e9", "Cesare Barbera", "Marco Viviani", "Paolo Rosso", "Lucie Flek"], "title": "Disparities in Multilingual LLM-Based Healthcare Q&A", "comment": "Under review", "summary": "Equitable access to reliable health information is vital when integrating AI\ninto healthcare. Yet, information quality varies across languages, raising\nconcerns about the reliability and consistency of multilingual Large Language\nModels (LLMs). We systematically examine cross-lingual disparities in\npre-training source and factuality alignment in LLM answers for multilingual\nhealthcare Q&A across English, German, Turkish, Chinese (Mandarin), and\nItalian. We (i) constructed Multilingual Wiki Health Care\n(MultiWikiHealthCare), a multilingual dataset from Wikipedia; (ii) analyzed\ncross-lingual healthcare coverage; (iii) assessed LLM response alignment with\nthese references; and (iv) conducted a case study on factual alignment through\nthe use of contextual information and Retrieval-Augmented Generation (RAG). Our\nfindings reveal substantial cross-lingual disparities in both Wikipedia\ncoverage and LLM factual alignment. Across LLMs, responses align more with\nEnglish Wikipedia, even when the prompts are non-English. Providing contextual\nexcerpts from non-English Wikipedia at inference time effectively shifts\nfactual alignment toward culturally relevant knowledge. These results highlight\npractical pathways for building more equitable, multilingual AI systems for\nhealthcare.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u95ee\u7b54\u4e2d\u5b58\u5728\u663e\u8457\u7684\u8de8\u8bed\u8a00\u4e8b\u5b9e\u5bf9\u9f50\u5dee\u5f02\uff0c\u5373\u4f7f\u662f\u975e\u82f1\u8bed\u63d0\u793a\uff0c\u6a21\u578b\u56de\u7b54\u4e5f\u66f4\u503e\u5411\u4e8e\u82f1\u8bed\u7ef4\u57fa\u767e\u79d1\u7684\u5185\u5bb9\u3002\u901a\u8fc7\u63d0\u4f9b\u975e\u82f1\u8bed\u7ef4\u57fa\u767e\u79d1\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u53ef\u4ee5\u6709\u6548\u6539\u5584\u4e8b\u5b9e\u5bf9\u9f50\u3002", "motivation": "\u5c06AI\u6574\u5408\u5230\u533b\u7597\u4fdd\u5065\u4e2d\u65f6\uff0c\u516c\u5e73\u83b7\u53d6\u53ef\u9760\u5065\u5eb7\u4fe1\u606f\u81f3\u5173\u91cd\u8981\u3002\u4f46\u4e0d\u540c\u8bed\u8a00\u95f4\u7684\u4fe1\u606f\u8d28\u91cf\u5b58\u5728\u5dee\u5f02\uff0c\u5f15\u53d1\u4e86\u5bf9\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u9760\u6027\u548c\u4e00\u81f4\u6027\u7684\u62c5\u5fe7\u3002", "method": "\u6784\u5efa\u591a\u8bed\u8a00\u7ef4\u57fa\u533b\u7597\u6570\u636e\u96c6\uff0c\u5206\u6790\u8de8\u8bed\u8a00\u533b\u7597\u8986\u76d6\u7387\uff0c\u8bc4\u4f30LLM\u56de\u7b54\u4e0e\u53c2\u8003\u5185\u5bb9\u7684\u5bf9\u9f50\u5ea6\uff0c\u5e76\u901a\u8fc7\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u8fdb\u884c\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u53d1\u73b0\u7ef4\u57fa\u767e\u79d1\u8986\u76d6\u7387\u548cLLM\u4e8b\u5b9e\u5bf9\u9f50\u90fd\u5b58\u5728\u663e\u8457\u7684\u8de8\u8bed\u8a00\u5dee\u5f02\u3002\u5373\u4f7f\u662f\u975e\u82f1\u8bed\u63d0\u793a\uff0c\u6a21\u578b\u56de\u7b54\u4e5f\u66f4\u503e\u5411\u4e8e\u82f1\u8bed\u7ef4\u57fa\u767e\u79d1\u3002\u63d0\u4f9b\u975e\u82f1\u8bed\u7ef4\u57fa\u767e\u79d1\u4e0a\u4e0b\u6587\u53ef\u4ee5\u6709\u6548\u5c06\u4e8b\u5b9e\u5bf9\u9f50\u8f6c\u5411\u6587\u5316\u76f8\u5173\u77e5\u8bc6\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u4e3a\u6784\u5efa\u66f4\u516c\u5e73\u7684\u591a\u8bed\u8a00\u533b\u7597AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2510.17483", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17483", "abs": "https://arxiv.org/abs/2510.17483", "authors": ["Zheyue Tan", "Zhiyuan Li", "Tao Yuan", "Dong Zhou", "Weilin Liu", "Yueqing Zhuang", "Yadong Li", "Guowei Niu", "Cheng Qin", "Zhuyu Yao", "Congyi Liu", "Haiyang Xu", "Boxun Li", "Guohao Dai", "Bo Zhao", "Yu Wang"], "title": "ReXMoE: Reusing Experts with Minimal Overhead in Mixture-of-Experts", "comment": null, "summary": "Mixture-of-Experts (MoE) architectures have emerged as a promising approach\nto scale Large Language Models (LLMs). MoE boosts the efficiency by activating\na subset of experts per token. Recent works show that fine-grained experts\nsubstantially enriches the combinatorial flexibility of active experts and\nenhances model expressiveness. However, such a design is fundamentally limited\nby the layer-local routing mechanism: each layer is restricted to its own\nexpert pool. This requires a careful trade-off between expert dimensionality\nand routing diversity given fixed parameter budgets. We describe ReXMoE, a\nnovel MoE architecture that improves routing beyond the existing layer-local\napproaches by allowing routers to reuse experts across adjacent layers. ReXMoE\ndecouples expert dimensionality from per-layer budgets, enabling richer expert\ncombinations without sacrificing individual expert capacity or inflating\noverall parameters. To this end, we propose a new progressive scaling routing\n(PSR) strategy to gradually increase the candidate expert pool during training.\nAs a result, ReXMoE improves both language modeling and downstream task\nperformance. Extensive experiments on models ranging from 0.5B to 7B parameters\nacross different architectures demonstrate that ReXMoE consistently improves\nperformance under fixed architectural dimensions, confirming ReXMoE as new\ndesign paradigm for parameter-efficient and scalable MoE-based LLMs.", "AI": {"tldr": "ReXMoE\u662f\u4e00\u79cd\u65b0\u9896\u7684MoE\u67b6\u6784\uff0c\u901a\u8fc7\u8de8\u5c42\u590d\u7528\u4e13\u5bb6\u6765\u8d85\u8d8a\u4f20\u7edf\u7684\u5c42\u5c40\u90e8\u8def\u7531\u673a\u5236\uff0c\u5728\u56fa\u5b9a\u53c2\u6570\u9884\u7b97\u4e0b\u5b9e\u73b0\u66f4\u597d\u7684\u8def\u7531\u591a\u6837\u6027\u548c\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfMoE\u67b6\u6784\u53d7\u9650\u4e8e\u5c42\u5c40\u90e8\u8def\u7531\u673a\u5236\uff0c\u6bcf\u5c42\u53ea\u80fd\u4f7f\u7528\u81ea\u5df1\u7684\u4e13\u5bb6\u6c60\uff0c\u9700\u8981\u5728\u4e13\u5bb6\u7ef4\u5ea6\u548c\u8def\u7531\u591a\u6837\u6027\u4e4b\u95f4\u505a\u51fa\u6743\u8861\u3002ReXMoE\u65e8\u5728\u6253\u7834\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u63d0\u51faReXMoE\u67b6\u6784\uff0c\u5141\u8bb8\u8def\u7531\u5668\u5728\u76f8\u90bb\u5c42\u4e4b\u95f4\u590d\u7528\u4e13\u5bb6\uff0c\u89e3\u8026\u4e13\u5bb6\u7ef4\u5ea6\u4e0e\u6bcf\u5c42\u9884\u7b97\u3002\u91c7\u7528\u6e10\u8fdb\u5f0f\u7f29\u653e\u8def\u7531\u7b56\u7565\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u9010\u6b65\u589e\u52a0\u5019\u9009\u4e13\u5bb6\u6c60\u3002", "result": "\u57280.5B\u52307B\u53c2\u6570\u89c4\u6a21\u7684\u4e0d\u540c\u67b6\u6784\u6a21\u578b\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0cReXMoE\u5728\u56fa\u5b9a\u67b6\u6784\u7ef4\u5ea6\u4e0b\u6301\u7eed\u63d0\u5347\u8bed\u8a00\u5efa\u6a21\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "ReXMoE\u4e3a\u53c2\u6570\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684MoE\u57faLLMs\u63d0\u4f9b\u4e86\u65b0\u7684\u8bbe\u8ba1\u8303\u5f0f\uff0c\u80fd\u591f\u5728\u4e0d\u727a\u7272\u4e13\u5bb6\u5bb9\u91cf\u6216\u589e\u52a0\u603b\u4f53\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u66f4\u4e30\u5bcc\u7684\u4e13\u5bb6\u7ec4\u5408\u3002"}}
{"id": "2510.17489", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17489", "abs": "https://arxiv.org/abs/2510.17489", "authors": ["Yongxin He", "Shan Zhang", "Yixuan Cao", "Lei Ma", "Ping Luo"], "title": "DETree: DEtecting Human-AI Collaborative Texts via Tree-Structured Hierarchical Representation Learning", "comment": "To appear in NeurIPS 2025", "summary": "Detecting AI-involved text is essential for combating misinformation,\nplagiarism, and academic misconduct. However, AI text generation includes\ndiverse collaborative processes (AI-written text edited by humans,\nhuman-written text edited by AI, and AI-generated text refined by other AI),\nwhere various or even new LLMs could be involved. Texts generated through these\nvaried processes exhibit complex characteristics, presenting significant\nchallenges for detection. Current methods model these processes rather crudely,\nprimarily employing binary classification (purely human vs. AI-involved) or\nmulti-classification (treating human-AI collaboration as a new class). We\nobserve that representations of texts generated through different processes\nexhibit inherent clustering relationships. Therefore, we propose DETree, a\nnovel approach that models the relationships among different processes as a\nHierarchical Affinity Tree structure, and introduces a specialized loss\nfunction that aligns text representations with this tree. To facilitate this\nlearning, we developed RealBench, a comprehensive benchmark dataset that\nautomatically incorporates a wide spectrum of hybrid texts produced through\nvarious human-AI collaboration processes. Our method improves performance in\nhybrid text detection tasks and significantly enhances robustness and\ngeneralization in out-of-distribution scenarios, particularly in few-shot\nlearning conditions, further demonstrating the promise of training-based\napproaches in OOD settings. Our code and dataset are available at\nhttps://github.com/heyongxin233/DETree.", "AI": {"tldr": "\u63d0\u51faDETree\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c42\u6b21\u4eb2\u548c\u6811\u7ed3\u6784\u5efa\u6a21\u4e0d\u540cAI\u53c2\u4e0e\u6587\u672c\u751f\u6210\u8fc7\u7a0b\u7684\u5173\u7cfb\uff0c\u5e76\u5f00\u53d1RealBench\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df7\u5408\u6587\u672c\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "AI\u53c2\u4e0e\u6587\u672c\u751f\u6210\u5b58\u5728\u591a\u79cd\u534f\u4f5c\u8fc7\u7a0b\uff08AI\u5199\u4eba\u7f16\u8f91\u3001\u4eba\u5199AI\u7f16\u8f91\u3001AI\u751f\u6210AI\u7cbe\u70bc\uff09\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5efa\u6a21\u7c97\u7cd9\uff0c\u4e3b\u8981\u91c7\u7528\u4e8c\u5143\u6216\u591a\u5143\u5206\u7c7b\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u7279\u5f81\u3002", "method": "\u63d0\u51faDETree\u65b9\u6cd5\uff0c\u5c06\u4e0d\u540c\u751f\u6210\u8fc7\u7a0b\u7684\u5173\u7cfb\u5efa\u6a21\u4e3a\u5c42\u6b21\u4eb2\u548c\u6811\u7ed3\u6784\uff0c\u5f15\u5165\u4e13\u95e8\u635f\u5931\u51fd\u6570\u4f7f\u6587\u672c\u8868\u793a\u4e0e\u6811\u7ed3\u6784\u5bf9\u9f50\uff0c\u5e76\u5f00\u53d1RealBench\u6570\u636e\u96c6\u652f\u6301\u8bad\u7ec3\u3002", "result": "\u5728\u6df7\u5408\u6587\u672c\u68c0\u6d4b\u4efb\u52a1\u4e2d\u6027\u80fd\u63d0\u5347\uff0c\u5728\u5206\u5e03\u5916\u573a\u666f\u4e0b\u663e\u8457\u589e\u5f3a\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5c11\u6837\u672c\u5b66\u4e60\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u57fa\u4e8e\u8bad\u7ec3\u7684\u65b9\u6cd5\u5728OOD\u8bbe\u7f6e\u4e2d\u5177\u6709\u6f5c\u529b\uff0cDETree\u901a\u8fc7\u5efa\u6a21\u751f\u6210\u8fc7\u7a0b\u7684\u5173\u7cfb\u6709\u6548\u63d0\u5347\u4e86AI\u53c2\u4e0e\u6587\u672c\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2510.17491", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17491", "abs": "https://arxiv.org/abs/2510.17491", "authors": ["Yihong Tang", "Kehai Chen", "Liang Yue", "Jinxin Fan", "Caishen Zhou", "Xiaoguang Li", "Yuyang Zhang", "Mingming Zhao", "Shixiong Kai", "Kaiyang Guo", "Xingshan Zeng", "Wenjing Cun", "Lifeng Shang", "Min Zhang"], "title": "Empowering Real-World: A Survey on the Technology, Practice, and Evaluation of LLM-driven Industry Agents", "comment": null, "summary": "With the rise of large language models (LLMs), LLM agents capable of\nautonomous reasoning, planning, and executing complex tasks have become a\nfrontier in artificial intelligence. However, how to translate the research on\ngeneral agents into productivity that drives industry transformations remains a\nsignificant challenge. To address this, this paper systematically reviews the\ntechnologies, applications, and evaluation methods of industry agents based on\nLLMs. Using an industry agent capability maturity framework, it outlines the\nevolution of agents in industry applications, from \"process execution systems\"\nto \"adaptive social systems.\" First, we examine the three key technological\npillars that support the advancement of agent capabilities: Memory, Planning,\nand Tool Use. We discuss how these technologies evolve from supporting simple\ntasks in their early forms to enabling complex autonomous systems and\ncollective intelligence in more advanced forms. Then, we provide an overview of\nthe application of industry agents in real-world domains such as digital\nengineering, scientific discovery, embodied intelligence, collaborative\nbusiness execution, and complex system simulation. Additionally, this paper\nreviews the evaluation benchmarks and methods for both fundamental and\nspecialized capabilities, identifying the challenges existing evaluation\nsystems face regarding authenticity, safety, and industry specificity. Finally,\nwe focus on the practical challenges faced by industry agents, exploring their\ncapability boundaries, developmental potential, and governance issues in\nvarious scenarios, while providing insights into future directions. By\ncombining technological evolution with industry practices, this review aims to\nclarify the current state and offer a clear roadmap and theoretical foundation\nfor understanding and building the next generation of industry agents.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u884c\u4e1a\u667a\u80fd\u4f53\u6280\u672f\u3001\u5e94\u7528\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u884c\u4e1a\u667a\u80fd\u4f53\u80fd\u529b\u6210\u719f\u5ea6\u6846\u67b6\uff0c\u5206\u6790\u4e86\u4ece\"\u6d41\u7a0b\u6267\u884c\u7cfb\u7edf\"\u5230\"\u81ea\u9002\u5e94\u793e\u4f1a\u7cfb\u7edf\"\u7684\u6f14\u8fdb\u8def\u5f84\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u80fd\u591f\u81ea\u4e3b\u63a8\u7406\u3001\u89c4\u5212\u548c\u6267\u884c\u590d\u6742\u4efb\u52a1\u7684\u667a\u80fd\u4f53\u6210\u4e3a\u4eba\u5de5\u667a\u80fd\u524d\u6cbf\uff0c\u4f46\u5982\u4f55\u5c06\u901a\u7528\u667a\u80fd\u4f53\u7814\u7a76\u8f6c\u5316\u4e3a\u63a8\u52a8\u884c\u4e1a\u53d8\u9769\u7684\u751f\u4ea7\u529b\u4ecd\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002", "method": "\u4f7f\u7528\u884c\u4e1a\u667a\u80fd\u4f53\u80fd\u529b\u6210\u719f\u5ea6\u6846\u67b6\uff0c\u5206\u6790\u652f\u6491\u667a\u80fd\u4f53\u80fd\u529b\u53d1\u5c55\u7684\u4e09\u5927\u6280\u672f\u652f\u67f1\uff1a\u8bb0\u5fc6\u3001\u89c4\u5212\u548c\u5de5\u5177\u4f7f\u7528\uff0c\u5e76\u7efc\u8ff0\u5728\u6570\u5b57\u5de5\u7a0b\u3001\u79d1\u5b66\u53d1\u73b0\u3001\u5177\u8eab\u667a\u80fd\u7b49\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u3002", "result": "\u5efa\u7acb\u4e86\u884c\u4e1a\u667a\u80fd\u4f53\u4ece\u7b80\u5355\u4efb\u52a1\u652f\u6301\u5230\u590d\u6742\u81ea\u4e3b\u7cfb\u7edf\u548c\u96c6\u4f53\u667a\u80fd\u7684\u6f14\u8fdb\u8def\u5f84\uff0c\u8bc6\u522b\u4e86\u73b0\u6709\u8bc4\u4f30\u7cfb\u7edf\u5728\u771f\u5b9e\u6027\u3001\u5b89\u5168\u6027\u548c\u884c\u4e1a\u7279\u6027\u65b9\u9762\u9762\u4e34\u7684\u6311\u6218\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u6280\u672f\u6f14\u8fdb\u4e0e\u884c\u4e1a\u5b9e\u8df5\uff0c\u4e3a\u7406\u89e3\u548c\u6784\u5efa\u4e0b\u4e00\u4ee3\u884c\u4e1a\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u8def\u7ebf\u56fe\u548c\u7406\u8bba\u57fa\u7840\uff0c\u63a2\u8ba8\u4e86\u80fd\u529b\u8fb9\u754c\u3001\u53d1\u5c55\u6f5c\u529b\u548c\u6cbb\u7406\u95ee\u9898\u3002"}}
{"id": "2510.17498", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17498", "abs": "https://arxiv.org/abs/2510.17498", "authors": ["Zihan Liu", "Shun Zheng", "Xumeng Wen", "Yang Wang", "Jiang Bian", "Mao Yang"], "title": "Deep Self-Evolving Reasoning", "comment": null, "summary": "Long-form chain-of-thought reasoning has become a cornerstone of advanced\nreasoning in large language models. While recent verification-refinement\nframeworks have enabled proprietary models to solve Olympiad-level problems,\ntheir effectiveness hinges on strong, reliable verification and correction\ncapabilities, which remain fragile in open-weight, smaller-scale models. This\nwork demonstrates that even with weak verification and refinement capabilities\non hard tasks, the reasoning limits of such models can be substantially\nextended through a probabilistic paradigm we call Deep Self-Evolving Reasoning\n(DSER). We conceptualize iterative reasoning as a Markov chain, where each step\nrepresents a stochastic transition in the solution space. The key insight is\nthat convergence to a correct solution is guaranteed as long as the probability\nof improvement marginally exceeds that of degradation. By running multiple\nlong-horizon, self-evolving processes in parallel, DSER amplifies these small\npositive tendencies, enabling the model to asymptotically approach correct\nanswers. Empirically, we apply DSER to the DeepSeek-R1-0528-Qwen3-8B model. On\nthe challenging AIME 2024-2025 benchmark, DSER solves 5 out of 9 previously\nunsolvable problems and boosts overall performance, enabling this compact model\nto surpass the single-turn accuracy of its 600B-parameter teacher through\nmajority voting. Beyond its immediate utility for test-time scaling, the DSER\nframework serves to diagnose the fundamental limitations of current open-weight\nreasoners. By clearly delineating their shortcomings in self-verification,\nrefinement, and stability, our findings establish a clear research agenda for\ndeveloping next-generation models with powerful, intrinsic self-evolving\ncapabilities.", "AI": {"tldr": "Deep Self-Evolving Reasoning (DSER) \u662f\u4e00\u79cd\u6982\u7387\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u63a8\u7406\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u94fe\uff0c\u5229\u7528\u591a\u4e2a\u5e76\u884c\u957f\u7a0b\u81ea\u6f14\u5316\u8fc7\u7a0b\u653e\u5927\u5fae\u5c0f\u6539\u8fdb\u6982\u7387\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u5c0f\u89c4\u6a21\u5f00\u653e\u6743\u91cd\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u9a8c\u8bc1-\u7cbe\u70bc\u6846\u67b6\u4f9d\u8d56\u5f3a\u5927\u7684\u9a8c\u8bc1\u548c\u4fee\u6b63\u80fd\u529b\uff0c\u8fd9\u5728\u5f00\u653e\u6743\u91cd\u7684\u5c0f\u89c4\u6a21\u6a21\u578b\u4e2d\u5f88\u8106\u5f31\u3002DSER\u65e8\u5728\u8bc1\u660e\u5373\u4f7f\u9a8c\u8bc1\u548c\u7cbe\u70bc\u80fd\u529b\u8f83\u5f31\uff0c\u901a\u8fc7\u6982\u7387\u65b9\u6cd5\u4e5f\u80fd\u5927\u5e45\u6269\u5c55\u6a21\u578b\u7684\u63a8\u7406\u6781\u9650\u3002", "method": "\u5c06\u8fed\u4ee3\u63a8\u7406\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u94fe\uff0c\u6bcf\u4e2a\u6b65\u9aa4\u4ee3\u8868\u89e3\u7a7a\u95f4\u7684\u968f\u673a\u8f6c\u79fb\u3002\u5173\u952e\u6d1e\u5bdf\u662f\u53ea\u8981\u6539\u8fdb\u6982\u7387\u7565\u9ad8\u4e8e\u9000\u5316\u6982\u7387\uff0c\u5c31\u80fd\u4fdd\u8bc1\u6536\u655b\u5230\u6b63\u786e\u89e3\u3002\u901a\u8fc7\u5e76\u884c\u8fd0\u884c\u591a\u4e2a\u957f\u7a0b\u81ea\u6f14\u5316\u8fc7\u7a0b\u6765\u653e\u5927\u8fd9\u4e9b\u5fae\u5c0f\u6b63\u5411\u8d8b\u52bf\u3002", "result": "\u5728DeepSeek-R1-0528-Qwen3-8B\u6a21\u578b\u4e0a\u5e94\u7528DSER\uff0c\u5728AIME 2024-2025\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u89e3\u51b3\u4e869\u4e2a\u5148\u524d\u65e0\u6cd5\u89e3\u51b3\u7684\u95ee\u9898\u4e2d\u76845\u4e2a\uff0c\u63d0\u5347\u4e86\u6574\u4f53\u6027\u80fd\uff0c\u4f7f\u8fd9\u4e2a\u7d27\u51d1\u6a21\u578b\u901a\u8fc7\u591a\u6570\u6295\u7968\u8d85\u8fc7\u4e86\u5176600B\u53c2\u6570\u6559\u5e08\u7684\u5355\u8f6e\u51c6\u786e\u7387\u3002", "conclusion": "DSER\u4e0d\u4ec5\u4e3a\u6d4b\u8bd5\u65f6\u6269\u5c55\u63d0\u4f9b\u4e86\u5b9e\u7528\u4ef7\u503c\uff0c\u8fd8\u8bca\u65ad\u4e86\u5f53\u524d\u5f00\u653e\u6743\u91cd\u63a8\u7406\u5668\u7684\u57fa\u672c\u9650\u5236\uff0c\u4e3a\u5f00\u53d1\u5177\u6709\u5f3a\u5927\u5185\u5728\u81ea\u6f14\u5316\u80fd\u529b\u7684\u4e0b\u4e00\u4ee3\u6a21\u578b\u5efa\u7acb\u4e86\u6e05\u6670\u7684\u7814\u7a76\u8bae\u7a0b\u3002"}}
{"id": "2510.17504", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17504", "abs": "https://arxiv.org/abs/2510.17504", "authors": ["Jingshu Liu", "Raheel Qader", "Ga\u00ebtan Caillaut", "Mariam Nakhl\u00e9"], "title": "Lingua Custodi's participation at the WMT 2025 Terminology shared task", "comment": null, "summary": "While BERT is an effective method for learning monolingual sentence\nembeddings for semantic similarity and embedding based transfer learning BERT\nbased cross-lingual sentence embeddings have yet to be explored. We\nsystematically investigate methods for learning multilingual sentence\nembeddings by combining the best methods for learning monolingual and\ncross-lingual representations including: masked language modeling (MLM),\ntranslation language modeling (TLM), dual encoder translation ranking, and\nadditive margin softmax. We show that introducing a pre-trained multilingual\nlanguage model dramatically reduces the amount of parallel training data\nrequired to achieve good performance by 80%. Composing the best of these\nmethods produces a model that achieves 83.7% bi-text retrieval accuracy over\n112 languages on Tatoeba, well above the 65.5 achieved by LASER, while still\nperforming competitively on monolingual transfer learning benchmarks. Parallel\ndata mined from CommonCrawl using our best model is shown to train competitive\nNMT models for en-zh and en-de. We publicly release our best multilingual\nsentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u7d22\u4e86\u57fa\u4e8eBERT\u7684\u591a\u8bed\u8a00\u53e5\u5b50\u5d4c\u5165\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\u3001\u7ffb\u8bd1\u8bed\u8a00\u5efa\u6a21\u7b49\u6280\u672f\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5e73\u884c\u8bad\u7ec3\u6570\u636e\u9700\u6c42\uff0c\u5728112\u79cd\u8bed\u8a00\u4e0a\u5b9e\u73b0\u4e8683.7%\u7684\u53cc\u6587\u672c\u68c0\u7d22\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u4e86LASER\u768465.5%\u3002", "motivation": "\u867d\u7136BERT\u5728\u5355\u8bed\u53e5\u5b50\u5d4c\u5165\u5b66\u4e60\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u57fa\u4e8eBERT\u7684\u8de8\u8bed\u8a00\u53e5\u5b50\u5d4c\u5165\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u8bba\u6587\u65e8\u5728\u7cfb\u7edf\u7814\u7a76\u591a\u8bed\u8a00\u53e5\u5b50\u5d4c\u5165\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u5355\u8bed\u548c\u8de8\u8bed\u8a00\u8868\u793a\u5b66\u4e60\u7684\u6700\u4f73\u6280\u672f\u3002", "method": "\u7ed3\u5408\u4e86\u63a9\u7801\u8bed\u8a00\u5efa\u6a21(MLM)\u3001\u7ffb\u8bd1\u8bed\u8a00\u5efa\u6a21(TLM)\u3001\u53cc\u7f16\u7801\u5668\u7ffb\u8bd1\u6392\u5e8f\u548c\u52a0\u6027\u8fb9\u9645softmax\u7b49\u65b9\u6cd5\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u591a\u8bed\u8a00\u8bed\u8a00\u6a21\u578b\u5927\u5e45\u51cf\u5c11\u5e73\u884c\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u3002", "result": "\u5c06\u6700\u4f73\u65b9\u6cd5\u7ec4\u5408\u540e\uff0c\u5728Tatoeba\u6570\u636e\u96c6\u7684112\u79cd\u8bed\u8a00\u4e0a\u8fbe\u523083.7%\u7684\u53cc\u6587\u672c\u68c0\u7d22\u51c6\u786e\u7387\uff0c\u8fdc\u9ad8\u4e8eLASER\u768465.5%\u3002\u540c\u65f6\uff0c\u5728\u5355\u8bed\u8fc1\u79fb\u5b66\u4e60\u57fa\u51c6\u4e0a\u4ecd\u4fdd\u6301\u7ade\u4e89\u529b\u3002\u4f7f\u7528\u8be5\u6a21\u578b\u4eceCommonCrawl\u6316\u6398\u7684\u5e73\u884c\u6570\u636e\u53ef\u4ee5\u8bad\u7ec3\u51fa\u5177\u6709\u7ade\u4e89\u529b\u7684\u82f1-\u4e2d\u548c\u82f1-\u5fb7\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u6a21\u578b\u3002", "conclusion": "\u6210\u529f\u5f00\u53d1\u4e86\u6709\u6548\u7684\u591a\u8bed\u8a00\u53e5\u5b50\u5d4c\u5165\u6a21\u578b\uff0c\u5728109+\u79cd\u8bed\u8a00\u4e0a\u516c\u5f00\u53ef\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u8bed\u8a00\u68c0\u7d22\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5355\u8bed\u4efb\u52a1\u7684\u7ade\u4e89\u529b\u3002"}}
{"id": "2510.17509", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17509", "abs": "https://arxiv.org/abs/2510.17509", "authors": ["Shiyu Ni", "Keping Bi", "Jiafeng Guo", "Minghao Tang", "Jingtong Wu", "Zengxin Han", "Xueqi Cheng"], "title": "Annotation-Efficient Universal Honesty Alignment", "comment": null, "summary": "Honesty alignment-the ability of large language models (LLMs) to recognize\ntheir knowledge boundaries and express calibrated confidence-is essential for\ntrustworthy deployment. Existing methods either rely on training-free\nconfidence estimation (e.g., token probabilities, self-consistency) or\ntraining-based calibration with correctness annotations. While effective,\nachieving universal honesty alignment with training-based calibration requires\ncostly, large-scale labeling. To support annotation-efficient training, we\nintroduce Elicitation-Then-Calibration (EliCal), a two-stage framework that\nfirst elicits internal confidence using inexpensive self-consistency\nsupervision, then calibrates this confidence with a small set of correctness\nannotations. To support a large-scale study, we release HonestyBench, a\nbenchmark covering ten free-form QA datasets with 560k training and 70k\nevaluation instances annotated with correctness and self-consistency signals.\nExperiments show that EliCal achieves near-optimal alignment with only 1k\ncorrectness annotations (0.18% of full supervision) and better alignment\nperformance on unseen MMLU tasks than the calibration-only baseline, offering a\nscalable solution toward universal honesty alignment in LLMs.", "AI": {"tldr": "EliCal\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u5ec9\u4ef7\u7684\u81ea\u6211\u4e00\u81f4\u6027\u76d1\u7763\u6765\u83b7\u53d6\u5185\u90e8\u7f6e\u4fe1\u5ea6\uff0c\u7136\u540e\u7528\u5c11\u91cf\u6b63\u786e\u6027\u6807\u6ce8\u8fdb\u884c\u6821\u51c6\uff0c\u5b9e\u73b0\u9ad8\u6548\u8bda\u5b9e\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u7684\u8bda\u5b9e\u9a8c\u8bc1\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u65e0\u8bad\u7ec3\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\uff0c\u8981\u4e48\u9700\u8981\u5927\u91cf\u6807\u6ce8\u8fdb\u884c\u57fa\u4e8e\u8bad\u7ec3\u7684\u6821\u51c6\u3002\u5b9e\u73b0\u901a\u7528\u8bda\u5b9e\u9a8c\u8bc1\u9700\u8981\u6602\u8d35\u7684\u5927\u89c4\u6a21\u6807\u6ce8\uff0c\u56e0\u6b64\u9700\u8981\u652f\u6301\u9ad8\u6548\u6807\u6ce8\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u63d0\u51faElicitation-Then-Calibration (EliCal)\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u5ec9\u4ef7\u7684\u81ea\u6211\u4e00\u81f4\u6027\u76d1\u7763\u83b7\u53d6\u5185\u90e8\u7f6e\u4fe1\u5ea6\uff0c\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u5c11\u91cf\u6b63\u786e\u6027\u6807\u6ce8\u6821\u51c6\u7f6e\u4fe1\u5ea6\u3002", "result": "EliCal\u4ec5\u4f7f\u75281k\u6b63\u786e\u6027\u6807\u6ce8\uff08\u5168\u76d1\u7763\u76840.18%\uff09\u5c31\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u5bf9\u9f50\u6548\u679c\uff0c\u5728\u672a\u89c1\u8fc7\u7684MMLU\u4efb\u52a1\u4e0a\u6bd4\u4ec5\u6821\u51c6\u7684\u57fa\u7ebf\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "EliCal\u4e3aLLMs\u7684\u901a\u7528\u8bda\u5b9e\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17516", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17516", "abs": "https://arxiv.org/abs/2510.17516", "authors": ["Tiancheng Hu", "Joachim Baumann", "Lorenzo Lupo", "Dirk Hovy", "Nigel Collier", "Paul R\u00f6ttger"], "title": "SimBench: Benchmarking the Ability of Large Language Models to Simulate Human Behaviors", "comment": "Project Website: http://simbench.tiancheng.hu/ Data:\n  https://huggingface.co/datasets/pitehu/SimBench", "summary": "Large language model (LLM) simulations of human behavior have the potential\nto revolutionize the social and behavioral sciences, if and only if they\nfaithfully reflect real human behaviors. Current evaluations are fragmented,\nbased on bespoke tasks and metrics, creating a patchwork of incomparable\nresults. To address this, we introduce SimBench, the first large-scale,\nstandardized benchmark for a robust, reproducible science of LLM simulation. By\nunifying 20 diverse datasets covering tasks from moral decision-making to\neconomic choice across a large global participant pool, SimBench provides the\nnecessary foundation to ask fundamental questions about when, how, and why LLM\nsimulations succeed or fail. We show that, while even the best LLMs today have\nlimited simulation ability (score: 40.80/100), performance scales log-linearly\nwith model size. Simulation performance is not improved by increased\ninference-time compute. We demonstrate an alignment-simulation trade-off:\ninstruction-tuning improves performance on low-entropy (consensus) questions\nbut degrades it on high-entropy (diverse) ones. Models particularly struggle\nwhen simulating specific demographic groups. Finally, we demonstrate that\nsimulation ability correlates most strongly with deep, knowledge-intensive\nreasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to\naccelerate the development of more faithful LLM simulators.", "AI": {"tldr": "SimBench\u662f\u9996\u4e2a\u5927\u89c4\u6a21\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5bf9\u4eba\u7c7b\u884c\u4e3a\u7684\u6a21\u62df\u80fd\u529b\uff0c\u6db5\u76d620\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u5f53\u524d\u6700\u4f73LLM\u6a21\u62df\u80fd\u529b\u6709\u9650\uff0c\u6027\u80fd\u968f\u6a21\u578b\u89c4\u6a21\u5bf9\u6570\u7ebf\u6027\u589e\u957f\uff0c\u4f46\u63a8\u7406\u65f6\u95f4\u8ba1\u7b97\u65e0\u6cd5\u63d0\u5347\u6027\u80fd\uff0c\u5b58\u5728\u5bf9\u9f50-\u6a21\u62df\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u5f53\u524dLLM\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u7684\u8bc4\u4f30\u65b9\u6cd5\u788e\u7247\u5316\uff0c\u57fa\u4e8e\u5b9a\u5236\u4efb\u52a1\u548c\u6307\u6807\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u53ef\u6bd4\u8f83\u3002\u9700\u8981\u5efa\u7acb\u6807\u51c6\u5316\u57fa\u51c6\u6765\u63a8\u52a8LLM\u6a21\u62df\u80fd\u529b\u7684\u79d1\u5b66\u53d1\u5c55\u3002", "method": "\u5f15\u5165SimBench\u57fa\u51c6\uff0c\u6574\u540820\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u9053\u5fb7\u51b3\u7b56\u3001\u7ecf\u6d4e\u9009\u62e9\u7b49\u4efb\u52a1\uff0c\u57fa\u4e8e\u5168\u7403\u53c2\u4e0e\u8005\u6c60\u8fdb\u884c\u6807\u51c6\u5316\u8bc4\u4f30\u3002", "result": "\u5f53\u524d\u6700\u4f73LLM\u6a21\u62df\u80fd\u529b\u5f97\u5206\u4ec540.80/100\uff0c\u6027\u80fd\u968f\u6a21\u578b\u89c4\u6a21\u5bf9\u6570\u7ebf\u6027\u589e\u957f\uff1b\u63a8\u7406\u65f6\u95f4\u8ba1\u7b97\u65e0\u6cd5\u63d0\u5347\u6027\u80fd\uff1b\u6307\u4ee4\u5fae\u8c03\u5728\u4f4e\u71b5\u95ee\u9898\u4e0a\u6539\u5584\u6027\u80fd\u4f46\u5728\u9ad8\u71b5\u95ee\u9898\u4e0a\u964d\u4f4e\u6027\u80fd\uff1b\u6a21\u578b\u5728\u6a21\u62df\u7279\u5b9a\u4eba\u53e3\u7fa4\u4f53\u65f6\u8868\u73b0\u8f83\u5dee\uff1b\u6a21\u62df\u80fd\u529b\u4e0e\u6df1\u5ea6\u77e5\u8bc6\u63a8\u7406\u80fd\u529b\u5f3a\u76f8\u5173\u3002", "conclusion": "SimBench\u4e3aLLM\u6a21\u62df\u80fd\u529b\u7684\u53ef\u6d4b\u91cf\u8fdb\u6b65\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5c06\u52a0\u901f\u5f00\u53d1\u66f4\u5fe0\u5b9e\u7684\u4eba\u7c7b\u884c\u4e3a\u6a21\u62df\u5668\u3002"}}
{"id": "2510.17532", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17532", "abs": "https://arxiv.org/abs/2510.17532", "authors": ["Raghu Vamshi Hemadri", "Geetha Krishna Guruju", "Kristi Topollai", "Anna Ewa Choromanska"], "title": "OncoReason: Structuring Clinical Reasoning in LLMs for Robust and Interpretable Survival Prediction", "comment": null, "summary": "Predicting cancer treatment outcomes requires models that are both accurate\nand interpretable, particularly in the presence of heterogeneous clinical data.\nWhile large language models (LLMs) have shown strong performance in biomedical\nNLP, they often lack structured reasoning capabilities critical for high-stakes\ndecision support. We present a unified, multi-task learning framework that\naligns autoregressive LLMs with clinical reasoning for outcome prediction on\nthe MSK-CHORD dataset. Our models are trained to jointly perform binary\nsurvival classification, continuous survival time regression, and natural\nlanguage rationale generation. We evaluate three alignment strategies: (1)\nstandard supervised fine-tuning (SFT), (2) SFT with Chain-of-Thought (CoT)\nprompting to elicit step-by-step reasoning, and (3) Group Relative Policy\nOptimization (GRPO), a reinforcement learning method that aligns model outputs\nto expert-derived reasoning trajectories. Experiments with LLaMa3-8B and\nMed42-8B backbones demonstrate that CoT prompting improves F1 by +6.0 and\nreduces MAE by 12%, while GRPO achieves state-of-the-art interpretability and\npredictive performance across BLEU, ROUGE, and BERTScore. We further show that\nexisting biomedical LLMs often fail to produce valid reasoning traces due to\narchitectural constraints. Our findings underscore the importance of\nreasoning-aware alignment in multi-task clinical modeling and set a new\nbenchmark for interpretable, trustworthy LLMs in precision oncology.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u81ea\u56de\u5f52\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u4e34\u5e8a\u63a8\u7406\u5bf9\u9f50\uff0c\u7528\u4e8e\u764c\u75c7\u6cbb\u7597\u7ed3\u679c\u9884\u6d4b\uff0c\u901a\u8fc7\u4e09\u79cd\u5bf9\u9f50\u7b56\u7565\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u751f\u7269\u533b\u5b66NLP\u4e2d\u8868\u73b0\u826f\u597d\u4f46\u7f3a\u4e4f\u7ed3\u6784\u5316\u63a8\u7406\u80fd\u529b\uff0c\u800c\u764c\u75c7\u6cbb\u7597\u7ed3\u679c\u9884\u6d4b\u9700\u8981\u65e2\u51c6\u786e\u53c8\u53ef\u89e3\u91ca\u7684\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u5f02\u6784\u4e34\u5e8a\u6570\u636e\u73af\u5883\u4e0b\u3002", "method": "\u4f7f\u7528\u7edf\u4e00\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u8bad\u7ec3\u6a21\u578b\u540c\u65f6\u6267\u884c\u4e8c\u5143\u751f\u5b58\u5206\u7c7b\u3001\u8fde\u7eed\u751f\u5b58\u65f6\u95f4\u56de\u5f52\u548c\u81ea\u7136\u8bed\u8a00\u7406\u7531\u751f\u6210\u3002\u8bc4\u4f30\u4e09\u79cd\u5bf9\u9f50\u7b56\u7565\uff1a\u6807\u51c6\u76d1\u7763\u5fae\u8c03\u3001\u5e26\u601d\u7ef4\u94fe\u63d0\u793a\u7684\u76d1\u7763\u5fae\u8c03\u3001\u4ee5\u53ca\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u601d\u7ef4\u94fe\u63d0\u793a\u5c06F1\u63d0\u9ad86.0%\uff0cMAE\u964d\u4f4e12%\uff0c\u800cGRPO\u5728BLEU\u3001ROUGE\u548cBERTScore\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9884\u6d4b\u6027\u80fd\u3002\u73b0\u6709\u751f\u7269\u533b\u5b66LLM\u7531\u4e8e\u67b6\u6784\u9650\u5236\u5e38\u65e0\u6cd5\u4ea7\u751f\u6709\u6548\u7684\u63a8\u7406\u8f68\u8ff9\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u591a\u4efb\u52a1\u4e34\u5e8a\u5efa\u6a21\u4e2d\u63a8\u7406\u611f\u77e5\u5bf9\u9f50\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u7cbe\u51c6\u80bf\u7624\u5b66\u4e2d\u53ef\u89e3\u91ca\u3001\u53ef\u4fe1\u8d56\u7684LLM\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\u3002"}}
{"id": "2510.17548", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17548", "abs": "https://arxiv.org/abs/2510.17548", "authors": ["Nisrine Rair", "Alban Goupil", "Valeriu Vrabie", "Emmanuel Chochoy"], "title": "When Annotators Disagree, Topology Explains: Mapper, a Topological Tool for Exploring Text Embedding Geometry and Ambiguity", "comment": "Accepted to appear in the Proceedings of the 2025 Conference on\n  Empirical Methods in Natural Language Processing (EMNLP 2025, Main\n  Conference)", "summary": "Language models are often evaluated with scalar metrics like accuracy, but\nsuch measures fail to capture how models internally represent ambiguity,\nespecially when human annotators disagree. We propose a topological perspective\nto analyze how fine-tuned models encode ambiguity and more generally instances.\n  Applied to RoBERTa-Large on the MD-Offense dataset, Mapper, a tool from\ntopological data analysis, reveals that fine-tuning restructures embedding\nspace into modular, non-convex regions aligned with model predictions, even for\nhighly ambiguous cases. Over $98\\%$ of connected components exhibit $\\geq 90\\%$\nprediction purity, yet alignment with ground-truth labels drops in ambiguous\ndata, surfacing a hidden tension between structural confidence and label\nuncertainty.\n  Unlike traditional tools such as PCA or UMAP, Mapper captures this geometry\ndirectly uncovering decision regions, boundary collapses, and overconfident\nclusters. Our findings position Mapper as a powerful diagnostic tool for\nunderstanding how models resolve ambiguity. Beyond visualization, it also\nenables topological metrics that may inform proactive modeling strategies in\nsubjective NLP tasks.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u62d3\u6251\u6570\u636e\u5206\u6790\u5de5\u5177Mapper\u6765\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5185\u90e8\u8868\u793a\u6b67\u4e49\u6027\uff0c\u53d1\u73b0\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5f62\u6210\u6a21\u5757\u5316\u3001\u975e\u51f8\u7684\u533a\u57df\uff0c\u5373\u4f7f\u5bf9\u4e8e\u9ad8\u5ea6\u6b67\u4e49\u7684\u60c5\u51b5\u4e5f\u80fd\u4fdd\u6301\u9ad8\u9884\u6d4b\u7eaf\u5ea6\u3002", "motivation": "\u4f20\u7edf\u6807\u91cf\u6307\u6807\uff08\u5982\u51c6\u786e\u7387\uff09\u65e0\u6cd5\u6355\u6349\u6a21\u578b\u5185\u90e8\u5982\u4f55\u8868\u793a\u6b67\u4e49\u6027\uff0c\u7279\u522b\u662f\u5728\u4eba\u7c7b\u6807\u6ce8\u8005\u5b58\u5728\u5206\u6b67\u7684\u60c5\u51b5\u4e0b\u3002\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u7406\u89e3\u6a21\u578b\u5982\u4f55\u5904\u7406\u4e3b\u89c2\u6027NLP\u4efb\u52a1\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u4f7f\u7528\u62d3\u6251\u6570\u636e\u5206\u6790\u5de5\u5177Mapper\u5206\u6790\u5fae\u8c03\u540e\u7684RoBERTa-Large\u6a21\u578b\u5728MD-Offense\u6570\u636e\u96c6\u4e0a\u7684\u5d4c\u5165\u7a7a\u95f4\u7ed3\u6784\uff0c\u4e0e\u4f20\u7edf\u65b9\u6cd5\u5982PCA\u548cUMAP\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u5fae\u8c03\u5c06\u5d4c\u5165\u7a7a\u95f4\u91cd\u7ec4\u4e3a\u6a21\u5757\u5316\u3001\u975e\u51f8\u7684\u533a\u57df\uff0c\u4e0e\u6a21\u578b\u9884\u6d4b\u5bf9\u9f50\u3002\u8d85\u8fc798%\u7684\u8fde\u901a\u7ec4\u4ef6\u5c55\u73b0\u51fa\u226590%\u7684\u9884\u6d4b\u7eaf\u5ea6\uff0c\u4f46\u5728\u6b67\u4e49\u6570\u636e\u4e2d\u4e0e\u771f\u5b9e\u6807\u7b7e\u7684\u5bf9\u9f50\u5ea6\u4e0b\u964d\uff0c\u63ed\u793a\u4e86\u7ed3\u6784\u7f6e\u4fe1\u5ea6\u4e0e\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\u4e4b\u95f4\u7684\u9690\u85cf\u5f20\u529b\u3002", "conclusion": "Mapper\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u80fd\u591f\u76f4\u63a5\u63ed\u793a\u51b3\u7b56\u533a\u57df\u3001\u8fb9\u754c\u584c\u9677\u548c\u8fc7\u5ea6\u81ea\u4fe1\u7684\u805a\u7c7b\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u53ef\u89c6\u5316\u5de5\u5177\u3002\u62d3\u6251\u6307\u6807\u53ef\u4ee5\u4e3a\u4e3b\u89c2\u6027NLP\u4efb\u52a1\u4e2d\u7684\u4e3b\u52a8\u5efa\u6a21\u7b56\u7565\u63d0\u4f9b\u4fe1\u606f\u3002"}}
{"id": "2510.17555", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17555", "abs": "https://arxiv.org/abs/2510.17555", "authors": ["Collin Zhang", "Fei Huang", "Chenhan Yuan", "Junyang Lin"], "title": "Language Confusion Gate: Language-Aware Decoding Through Model Self-Distillation", "comment": null, "summary": "Large language models (LLMs) often experience language confusion, which is\nthe unintended mixing of languages during text generation. Current solutions to\nthis problem either necessitate model retraining or cannot differentiate\nbetween harmful confusion and acceptable code-switching. This paper introduces\nthe Language Confusion Gate (LCG), a lightweight, plug-in solution that filters\ntokens during decoding without altering the base LLM. The LCG is trained using\nnorm-adjusted self-distillation to predict appropriate language families and\napply masking only when needed. Our method is based on the findings that\nlanguage confusion is infrequent, correct-language tokens are usually among the\ntop predictions, and output token embedding norms are larger for high-resource\nlanguages, which biases sampling. When evaluated across various models,\nincluding Qwen3, GPT-OSS, Gemma3, Llama3.1, LCG decreases language confusion\nsignificantly, often by an order of magnitude, without negatively impacting\ntask performance. Code is available at\nhttps://github.com/collinzrj/language_confusion_gate.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u8bed\u8a00\u6df7\u6dc6\u95e8\u63a7\uff08LCG\uff09\u65b9\u6cd5\uff0c\u5728\u89e3\u7801\u8fc7\u7a0b\u4e2d\u8fc7\u6ee4token\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\uff0c\u80fd\u663e\u8457\u51cf\u5c11\u8bed\u8a00\u6df7\u6dc6\u95ee\u9898\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u751f\u6210\u65f6\u7ecf\u5e38\u51fa\u73b0\u8bed\u8a00\u6df7\u6dc6\u95ee\u9898\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u8981\u4e48\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\uff0c\u8981\u4e48\u65e0\u6cd5\u533a\u5206\u6709\u5bb3\u6df7\u6dc6\u548c\u53ef\u63a5\u53d7\u7684\u8bed\u7801\u8f6c\u6362\u3002", "method": "\u4f7f\u7528\u89c4\u8303\u8c03\u6574\u7684\u81ea\u84b8\u998f\u65b9\u6cd5\u8bad\u7ec3LCG\uff0c\u9884\u6d4b\u9002\u5f53\u7684\u8bed\u8a00\u5bb6\u65cf\uff0c\u4ec5\u5728\u9700\u8981\u65f6\u5e94\u7528\u63a9\u7801\u3002\u57fa\u4e8e\u8bed\u8a00\u6df7\u6dc6\u4e0d\u9891\u7e41\u3001\u6b63\u786e\u8bed\u8a00token\u901a\u5e38\u4f4d\u4e8e\u9884\u6d4b\u524d\u5217\u3001\u9ad8\u8d44\u6e90\u8bed\u8a00\u8f93\u51fatoken\u5d4c\u5165\u89c4\u8303\u66f4\u5927\u7684\u53d1\u73b0\u3002", "result": "\u5728\u5305\u62ecQwen3\u3001GPT-OSS\u3001Gemma3\u3001Llama3.1\u5728\u5185\u7684\u5404\u79cd\u6a21\u578b\u4e0a\u8bc4\u4f30\uff0cLCG\u663e\u8457\u51cf\u5c11\u4e86\u8bed\u8a00\u6df7\u6dc6\uff0c\u901a\u5e38\u964d\u4f4e\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u4e14\u4e0d\u5f71\u54cd\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "LCG\u662f\u4e00\u79cd\u6709\u6548\u7684\u8f7b\u91cf\u7ea7\u63d2\u4ef6\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4e0d\u6539\u53d8\u57fa\u7840LLM\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u89e3\u51b3\u8bed\u8a00\u6df7\u6dc6\u95ee\u9898\u3002"}}
{"id": "2510.17591", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17591", "abs": "https://arxiv.org/abs/2510.17591", "authors": ["Guang Yang", "Yujie Zhu"], "title": "HGAdapter: Hypergraph-based Adapters in Language Models for Code Summarization and Clone Detection", "comment": "Accepted by the 2025 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2025) as a findings long paper", "summary": "Pre-trained language models (PLMs) are increasingly being applied to\ncode-related tasks. Although PLMs have achieved good results, they do not take\ninto account potential high-order data correlations within the code. We propose\nthree types of high-order correlations in code tokens, i.e. abstract syntax\ntree family correlation, lexical correlation, and line correlation. We design a\ntokens and hyperedges generator to capture these high-order data correlations.\nWe improve the architecture of hypergraph neural networks and combine it with\nadapter tuning to propose a novel hypergraph-based adapter (HGAdapter) to\nfine-tune PLMs. HGAdapter can encode high-order data correlations and is\nallowed to be inserted into various PLMs to enhance performance. Experiments\nwere conducted on several public datasets, including six languages of code\nsummarization and code clone detection tasks. Our methods improved the\nperformance of PLMs in datasets to varying degrees. Experimental results\nvalidate the introduction of high-order data correlations that contribute to\nimproved effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u9002\u914d\u5668HGAdapter\uff0c\u901a\u8fc7\u6355\u6349\u4ee3\u7801\u4e2d\u7684\u9ad8\u9636\u6570\u636e\u76f8\u5173\u6027\u6765\u589e\u5f3a\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u76f8\u5173\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u672a\u80fd\u5145\u5206\u5229\u7528\u4ee3\u7801\u5185\u90e8\u6f5c\u5728\u7684\u9ad8\u9636\u6570\u636e\u76f8\u5173\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e09\u79cd\u4ee3\u7801\u9ad8\u9636\u76f8\u5173\u6027\uff08\u62bd\u8c61\u8bed\u6cd5\u6811\u5bb6\u65cf\u76f8\u5173\u6027\u3001\u8bcd\u6c47\u76f8\u5173\u6027\u548c\u884c\u76f8\u5173\u6027\uff09\uff0c\u6784\u5efa\u4e86token\u548c\u8d85\u8fb9\u751f\u6210\u5668\uff0c\u5e76\u6539\u8fdb\u4e86\u8d85\u56fe\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u4e0e\u9002\u914d\u5668\u8c03\u4f18\u7ed3\u5408\uff0c\u63d0\u51faHGAdapter\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u7a0b\u5ea6\u4e0a\u63d0\u5347\u4e86PLMs\u5728\u4ee3\u7801\u6458\u8981\u548c\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "conclusion": "\u5f15\u5165\u9ad8\u9636\u6570\u636e\u76f8\u5173\u6027\u6709\u52a9\u4e8e\u63d0\u9ad8\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.17602", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17602", "abs": "https://arxiv.org/abs/2510.17602", "authors": ["Huiyuan Xie", "Chenyang Li", "Huining Zhu", "Chubin Zhang", "Yuxiao Ye", "Zhenghao Liu", "Zhiyuan Liu"], "title": "LawChain: Modeling Legal Reasoning Chains for Chinese Tort Case Analysis", "comment": null, "summary": "Legal reasoning is a fundamental component of legal analysis and\ndecision-making. Existing computational approaches to legal reasoning\npredominantly rely on generic reasoning frameworks such as syllogism and IRAC,\nwhich do not comprehensively examine the nuanced processes that underpin legal\nreasoning. Moreover, current research has largely focused on criminal cases,\nwith insufficient modeling for civil cases. In this work, we present a novel\nframework for explicitly modeling legal reasoning in the analysis of Chinese\ntort-related civil cases. We first operationalize the legal reasoning processes\nused in tort analysis into the LawChain framework. LawChain is a three-module\nreasoning framework, with each module consisting of multiple finer-grained\nsub-steps. Informed by the LawChain framework, we introduce the task of tort\nlegal reasoning and construct an evaluation benchmark, LawChain$_{eval}$, to\nsystematically assess the critical steps within analytical reasoning chains for\ntort analysis. Leveraging this benchmark, we evaluate state-of-the-art large\nlanguage models for their legal reasoning ability in civil tort contexts. Our\nresults indicate that current models still fall short in accurately handling\ncrucial elements of tort legal reasoning. Furthermore, we introduce several\nbaseline approaches that explicitly incorporate LawChain-style reasoning\nthrough prompting or post-training. We conduct further experiments on\nadditional legal analysis tasks, such as Legal Named-Entity Recognition and\nCriminal Damages Calculation, to verify the generalizability of these\nbaselines. The proposed baseline approaches achieve significant improvements in\ntort-related legal reasoning and generalize well to related legal analysis\ntasks, thus demonstrating the value of explicitly modeling legal reasoning\nchains to enhance the reasoning capabilities of language models.", "AI": {"tldr": "\u63d0\u51fa\u4e86LawChain\u6846\u67b6\u6765\u663e\u5f0f\u5efa\u6a21\u4e2d\u56fd\u4fb5\u6743\u6c11\u4e8b\u6848\u4ef6\u4e2d\u7684\u6cd5\u5f8b\u63a8\u7406\u8fc7\u7a0b\uff0c\u6784\u5efa\u4e86\u8bc4\u4f30\u57fa\u51c6LawChain_eval\uff0c\u5e76\u9a8c\u8bc1\u4e86\u57fa\u4e8eLawChain\u7684\u63d0\u793a\u548c\u5fae\u8c03\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u6cd5\u5f8b\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6cd5\u5f8b\u63a8\u7406\u8ba1\u7b97\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u901a\u7528\u63a8\u7406\u6846\u67b6\uff0c\u672a\u80fd\u6df1\u5165\u5206\u6790\u6cd5\u5f8b\u63a8\u7406\u7684\u7ec6\u81f4\u8fc7\u7a0b\uff0c\u4e14\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u5211\u4e8b\u6848\u4ef6\uff0c\u5bf9\u6c11\u4e8b\u6848\u4ef6\u5efa\u6a21\u4e0d\u8db3\u3002", "method": "\u5f00\u53d1\u4e86LawChain\u4e09\u6a21\u5757\u63a8\u7406\u6846\u67b6\uff0c\u5c06\u4fb5\u6743\u5206\u6790\u7684\u6cd5\u5f8b\u63a8\u7406\u8fc7\u7a0b\u64cd\u4f5c\u5316\u4e3a\u591a\u4e2a\u7ec6\u7c92\u5ea6\u5b50\u6b65\u9aa4\uff1b\u6784\u5efa\u8bc4\u4f30\u57fa\u51c6LawChain_eval\uff1b\u63d0\u51fa\u57fa\u4e8eLawChain\u7684\u63d0\u793a\u548c\u5fae\u8c03\u57fa\u7ebf\u65b9\u6cd5\u3002", "result": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4fb5\u6743\u6cd5\u5f8b\u63a8\u7406\u7684\u5173\u952e\u8981\u7d20\u4e0a\u4ecd\u5b58\u5728\u4e0d\u8db3\uff1b\u63d0\u51fa\u7684\u57fa\u7ebf\u65b9\u6cd5\u5728\u4fb5\u6743\u76f8\u5173\u6cd5\u5f8b\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0c\u5e76\u80fd\u826f\u597d\u6cdb\u5316\u5230\u5176\u4ed6\u6cd5\u5f8b\u5206\u6790\u4efb\u52a1\u3002", "conclusion": "\u663e\u5f0f\u5efa\u6a21\u6cd5\u5f8b\u63a8\u7406\u94fe\u80fd\u6709\u6548\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0cLawChain\u6846\u67b6\u5728\u6cd5\u5f8b\u5206\u6790\u4efb\u52a1\u4e2d\u5177\u6709\u826f\u597d\u901a\u7528\u6027\u3002"}}
{"id": "2510.17620", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17620", "abs": "https://arxiv.org/abs/2510.17620", "authors": ["Yuefeng Peng", "Parnian Afshar", "Megan Ganji", "Thomas Butler", "Amir Houmansadr", "Mingxian Wang", "Dezhi Hong"], "title": "Forget to Know, Remember to Use: Context-Aware Unlearning for Large Language Models", "comment": null, "summary": "Large language models may encode sensitive information or outdated knowledge\nthat needs to be removed, to ensure responsible and compliant model responses.\nUnlearning has emerged as an efficient alternative to full retraining, aiming\nto remove specific knowledge while preserving overall model utility. Existing\nevaluations of unlearning methods focus on (1) the extent of forgetting of the\ntarget knowledge (forget set) and (2) maintaining performance on the retain set\n(i.e., utility). However, these evaluations overlook an important usability\naspect: users may still want the model to leverage the removed information if\nit is re-introduced in the prompt. In a systematic evaluation of six\nstate-of-the-art unlearning methods, we find that they consistently impair such\ncontextual utility. To address this, we augment unlearning objectives with a\nplug-in term that preserves the model's ability to use forgotten knowledge when\nit is present in context. Extensive experiments demonstrate that our approach\nrestores contextual utility to near original levels while still maintaining\neffective forgetting and retain-set utility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u9057\u5fd8\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u76ee\u6807\u77e5\u8bc6\u9057\u5fd8\u6548\u679c\u7684\u540c\u65f6\uff0c\u6062\u590d\u4e86\u6a21\u578b\u5728\u63d0\u793a\u4e2d\u91cd\u65b0\u5f15\u5165\u88ab\u9057\u5fd8\u77e5\u8bc6\u65f6\u7684\u4e0a\u4e0b\u6587\u5229\u7528\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u9057\u5fd8\u5b66\u4e60\u65b9\u6cd5\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u76ee\u6807\u77e5\u8bc6\u7684\u9057\u5fd8\u7a0b\u5ea6\u548c\u4fdd\u7559\u96c6\u6027\u80fd\uff0c\u4f46\u5ffd\u7565\u4e86\u5f53\u88ab\u9057\u5fd8\u77e5\u8bc6\u91cd\u65b0\u51fa\u73b0\u5728\u63d0\u793a\u4e2d\u65f6\u6a21\u578b\u5e94\u80fd\u7ee7\u7eed\u5229\u7528\u8fd9\u4e9b\u77e5\u8bc6\u7684\u91cd\u8981\u53ef\u7528\u6027\u9700\u6c42\u3002", "method": "\u5728\u9057\u5fd8\u5b66\u4e60\u76ee\u6807\u4e2d\u589e\u52a0\u4e00\u4e2a\u63d2\u4ef6\u9879\uff0c\u4fdd\u6301\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u51fa\u73b0\u88ab\u9057\u5fd8\u77e5\u8bc6\u65f6\u4ecd\u80fd\u5229\u7528\u8fd9\u4e9b\u77e5\u8bc6\u7684\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u5c06\u4e0a\u4e0b\u6587\u6548\u7528\u6062\u590d\u5230\u63a5\u8fd1\u539f\u59cb\u6c34\u5e73\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u6548\u7684\u9057\u5fd8\u6548\u679c\u548c\u4fdd\u7559\u96c6\u6548\u7528\u3002", "conclusion": "\u901a\u8fc7\u589e\u5f3a\u9057\u5fd8\u5b66\u4e60\u76ee\u6807\uff0c\u53ef\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u635f\u5bb3\u4e0a\u4e0b\u6587\u6548\u7528\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u5168\u9762\u7684\u6a21\u578b\u77e5\u8bc6\u7ba1\u7406\u3002"}}
{"id": "2510.17652", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.17652", "abs": "https://arxiv.org/abs/2510.17652", "authors": ["Joseph McInerney"], "title": "Qomhra: A Bilingual Irish-English Large Language Model", "comment": null, "summary": "This paper introduces Qomhr\\'a, a bilingual Irish-English large language\nmodel (LLM), developed under low-resource constraints presenting a complete\npipeline spanning bilingual continued pre-training, instruction tuning, and\nalignment from human preferences. Newly accessible Irish corpora and English\ntext are mixed and curated to improve Irish performance while preserving\nEnglish ability. 6 closed-weight LLMs are judged for their Irish text\ngeneration by a native speaker, a learner and other LLMs. Google's\nGemini-2.5-Pro is ranked the highest and is subsequently used to synthesise\ninstruction tuning and human preference datasets. Two datasets are contributed\nleveraging Gemini-2.5-Pro: a 30K Irish-English parallel instruction tuning\ndataset and a 1K human preference dataset, generating accepted and rejected\nresponses that show near perfect alignment with a native Irish speaker.\nQomhr\\'a is comprehensively evaluated across benchmarks testing translation,\ngender understanding, topic identification and world knowledge with gains of up\nto 29% in Irish and 44% in English. Qomhr\\'a also undergoes instruction tuning\nand demonstrates clear progress in instruction following, crucial for chatbot\nfunctionality.", "AI": {"tldr": "\u5f00\u53d1\u4e86Qomhr'a\u53cc\u8bed\u7231\u5c14\u5170\u8bed-\u82f1\u8bed\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u4f4e\u8d44\u6e90\u6761\u4ef6\u4e0b\u901a\u8fc7\u6301\u7eed\u9884\u8bad\u7ec3\u3001\u6307\u4ee4\u8c03\u4f18\u548c\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u7684\u5b8c\u6574\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7231\u5c14\u5170\u8bed\u6027\u80fd\u5e76\u4fdd\u6301\u82f1\u8bed\u80fd\u529b\u3002", "motivation": "\u5728\u4f4e\u8d44\u6e90\u6761\u4ef6\u4e0b\u5f00\u53d1\u7231\u5c14\u5170\u8bed-\u82f1\u8bed\u53cc\u8bedLLM\uff0c\u89e3\u51b3\u7231\u5c14\u5170\u8bed\u8d44\u6e90\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u82f1\u8bed\u80fd\u529b\u3002", "method": "\u6df7\u5408\u65b0\u83b7\u53d6\u7684\u7231\u5c14\u5170\u8bed\u8bed\u6599\u548c\u82f1\u8bed\u6587\u672c\u8fdb\u884c\u53cc\u8bed\u6301\u7eed\u9884\u8bad\u7ec3\uff1b\u4f7f\u7528Gemini-2.5-Pro\u5408\u6210\u6307\u4ee4\u8c03\u4f18\u548c\u4eba\u7c7b\u504f\u597d\u6570\u636e\u96c6\uff1b\u8fdb\u884c\u6307\u4ee4\u8c03\u4f18\u548c\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u3002", "result": "\u5728\u7ffb\u8bd1\u3001\u6027\u522b\u7406\u89e3\u3001\u4e3b\u9898\u8bc6\u522b\u548c\u4e16\u754c\u77e5\u8bc6\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u7231\u5c14\u5170\u8bed\u6027\u80fd\u63d0\u5347\u8fbe29%\uff0c\u82f1\u8bed\u63d0\u5347\u8fbe44%\uff1b\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u663e\u8457\u6539\u5584\u3002", "conclusion": "Qomhr'a\u6a21\u578b\u6210\u529f\u5c55\u793a\u4e86\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e0b\u5f00\u53d1\u9ad8\u8d28\u91cf\u53cc\u8bedLLM\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u7231\u5c14\u5170\u8bedAI\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2510.17698", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17698", "abs": "https://arxiv.org/abs/2510.17698", "authors": ["Liqun He", "Manolis Mavrikis", "Mutlu Cukurova"], "title": "Towards Mining Effective Pedagogical Strategies from Learner-LLM Educational Dialogues", "comment": null, "summary": "Dialogue plays a crucial role in educational settings, yet existing\nevaluation methods for educational applications of large language models (LLMs)\nprimarily focus on technical performance or learning outcomes, often neglecting\nattention to learner-LLM interactions. To narrow this gap, this AIED Doctoral\nConsortium paper presents an ongoing study employing a dialogue analysis\napproach to identify effective pedagogical strategies from learner-LLM\ndialogues. The proposed approach involves dialogue data collection, dialogue\nact (DA) annotation, DA pattern mining, and predictive model building. Early\ninsights are outlined as an initial step toward future research. The work\nunderscores the need to evaluate LLM-based educational applications by focusing\non dialogue dynamics and pedagogical strategies.", "AI": {"tldr": "\u672c\u6587\u91c7\u7528\u5bf9\u8bdd\u5206\u6790\u65b9\u6cd5\u7814\u7a76\u5b66\u4e60\u8005\u4e0eLLM\u4e4b\u95f4\u7684\u6559\u80b2\u5bf9\u8bdd\uff0c\u8bc6\u522b\u6709\u6548\u6559\u5b66\u7b56\u7565\uff0c\u5305\u62ec\u6570\u636e\u6536\u96c6\u3001\u5bf9\u8bdd\u884c\u4e3a\u6807\u6ce8\u3001\u6a21\u5f0f\u6316\u6398\u548c\u9884\u6d4b\u6a21\u578b\u6784\u5efa\u3002", "motivation": "\u73b0\u6709LLM\u6559\u80b2\u5e94\u7528\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6280\u672f\u6027\u80fd\u6216\u5b66\u4e60\u6210\u679c\uff0c\u5ffd\u89c6\u4e86\u5b66\u4e60\u8005\u4e0eLLM\u7684\u4e92\u52a8\u8fc7\u7a0b\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u5bf9\u8bdd\u5206\u6790\u65b9\u6cd5\uff0c\u5305\u62ec\u5bf9\u8bdd\u6570\u636e\u6536\u96c6\u3001\u5bf9\u8bdd\u884c\u4e3a\u6807\u6ce8\u3001\u5bf9\u8bdd\u6a21\u5f0f\u6316\u6398\u548c\u9884\u6d4b\u6a21\u578b\u6784\u5efa\u56db\u4e2a\u6b65\u9aa4\u3002", "result": "\u63d0\u51fa\u4e86\u521d\u6b65\u7814\u7a76\u6846\u67b6\u548c\u65e9\u671f\u89c1\u89e3\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002", "conclusion": "\u5f3a\u8c03\u9700\u8981\u901a\u8fc7\u5173\u6ce8\u5bf9\u8bdd\u52a8\u6001\u548c\u6559\u5b66\u7b56\u7565\u6765\u8bc4\u4f30\u57fa\u4e8eLLM\u7684\u6559\u80b2\u5e94\u7528\u3002"}}
{"id": "2510.17715", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17715", "abs": "https://arxiv.org/abs/2510.17715", "authors": ["Hanxu Hu", "Xingxing Zhang", "Jannis Vamvas", "Rico Sennrich", "Furu Wei"], "title": "QueST: Incentivizing LLMs to Generate Difficult Problems", "comment": "20 pages, 7 figures", "summary": "Large Language Models have achieved strong performance on reasoning tasks,\nsolving competition-level coding and math problems. However, their scalability\nis limited by human-labeled datasets and the lack of large-scale, challenging\ncoding problem training data. Existing competitive coding datasets contain only\nthousands to tens of thousands of problems. Previous synthetic data generation\nmethods rely on either augmenting existing instruction datasets or selecting\nchallenging problems from human-labeled data. In this paper, we propose QueST,\na novel framework which combines difficulty-aware graph sampling and\ndifficulty-aware rejection fine-tuning that directly optimizes specialized\ngenerators to create challenging coding problems. Our trained generators\ndemonstrate superior capability compared to even GPT-4o at creating challenging\nproblems that benefit downstream performance. We leverage QueST to generate\nlarge-scale synthetic coding problems, which we then use to distill from strong\nteacher models with long chain-of-thought or to conduct reinforcement learning\nfor smaller models, proving effective in both scenarios. Our distillation\nexperiments demonstrate significant performance gains. Specifically, after\nfine-tuning Qwen3-8B-base on 100K difficult problems generated by QueST, we\nsurpass the performance of the original Qwen3-8B on LiveCodeBench. With an\nadditional 112K examples (i.e., 28K human-written problems paired with multiple\nsynthetic solutions), our 8B model matches the performance of the much larger\nDeepSeek-R1-671B. These findings indicate that generating complex problems via\nQueST offers an effective and scalable approach to advancing the frontiers of\ncompetitive coding and reasoning for large language models.", "AI": {"tldr": "QueST\u6846\u67b6\u901a\u8fc7\u96be\u5ea6\u611f\u77e5\u56fe\u91c7\u6837\u548c\u62d2\u7edd\u5fae\u8c03\u751f\u6210\u5177\u6709\u6311\u6218\u6027\u7684\u7f16\u7a0b\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5728\u7ade\u4e89\u6027\u7f16\u7a0b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7ade\u4e89\u6027\u7f16\u7a0b\u6570\u636e\u96c6\u89c4\u6a21\u6709\u9650\uff08\u4ec5\u6570\u5343\u5230\u6570\u4e07\u95ee\u9898\uff09\uff0c\u4e14\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u5177\u6709\u6311\u6218\u6027\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u9650\u5236\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u6269\u5c55\u80fd\u529b\u3002", "method": "\u7ed3\u5408\u96be\u5ea6\u611f\u77e5\u56fe\u91c7\u6837\u548c\u96be\u5ea6\u611f\u77e5\u62d2\u7edd\u5fae\u8c03\uff0c\u76f4\u63a5\u4f18\u5316\u4e13\u7528\u751f\u6210\u5668\u6765\u521b\u5efa\u5177\u6709\u6311\u6218\u6027\u7684\u7f16\u7a0b\u95ee\u9898\uff0c\u5e76\u5229\u7528\u751f\u6210\u7684\u95ee\u9898\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\u6216\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u4f7f\u7528QueST\u751f\u6210\u768410\u4e07\u4e2a\u96be\u9898\u5fae\u8c03Qwen3-8B-base\u540e\uff0c\u5728LiveCodeBench\u4e0a\u8d85\u8d8a\u4e86\u539f\u59cbQwen3-8B\uff1b\u989d\u5916\u4f7f\u752828K\u4eba\u5de5\u95ee\u9898\u4e0e\u5408\u6210\u89e3\u51b3\u65b9\u6848\u540e\uff0c8B\u6a21\u578b\u6027\u80fd\u4e0e671B\u7684DeepSeek-R1\u76f8\u5f53\u3002", "conclusion": "\u901a\u8fc7QueST\u751f\u6210\u590d\u6742\u95ee\u9898\u662f\u63a8\u8fdb\u8bed\u8a00\u6a21\u578b\u5728\u7ade\u4e89\u6027\u7f16\u7a0b\u548c\u63a8\u7406\u4efb\u52a1\u524d\u6cbf\u7684\u6709\u6548\u4e14\u53ef\u6269\u5c55\u65b9\u6cd5\u3002"}}
{"id": "2510.17720", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17720", "abs": "https://arxiv.org/abs/2510.17720", "authors": ["Nanda Kumar Rengarajan", "Jun Yan", "Chun Wang"], "title": "PANER: A Paraphrase-Augmented Framework for Low-Resource Named Entity Recognition", "comment": null, "summary": "Named Entity Recognition (NER) is a critical task that requires substantial\nannotated data, making it challenging in low-resource scenarios where label\nacquisition is expensive. While zero-shot and instruction-tuned approaches have\nmade progress, they often fail to generalize to domain-specific entities and do\nnot effectively utilize limited available data. We present a lightweight\nfew-shot NER framework that addresses these challenges through two key\ninnovations: (1) a new instruction tuning template with a simplified output\nformat that combines principles from prior IT approaches to leverage the large\ncontext window of recent state-of-the-art LLMs; (2) introducing a strategic\ndata augmentation technique that preserves entity information while\nparaphrasing the surrounding context, thereby expanding our training data\nwithout compromising semantic relationships. Experiments on benchmark datasets\nshow that our method achieves performance comparable to state-of-the-art models\non few-shot and zero-shot tasks, with our few-shot approach attaining an\naverage F1 score of 80.1 on the CrossNER datasets. Models trained with our\nparaphrasing approach show consistent improvements in F1 scores of up to 17\npoints over baseline versions, offering a promising solution for groups with\nlimited NER training data and compute power.", "AI": {"tldr": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u5c11\u6837\u672cNER\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u6307\u4ee4\u8c03\u4f18\u6a21\u677f\u548c\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u5b9e\u73b0\u4e0e\u6700\u5148\u8fdb\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd", "motivation": "\u89e3\u51b3NER\u4efb\u52a1\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u73b0\u6709\u96f6\u6837\u672c\u548c\u6307\u4ee4\u8c03\u4f18\u65b9\u6cd5\u96be\u4ee5\u6cdb\u5316\u5230\u9886\u57df\u7279\u5b9a\u5b9e\u4f53\u4e14\u65e0\u6cd5\u6709\u6548\u5229\u7528\u6709\u9650\u6570\u636e", "method": "1) \u65b0\u7684\u6307\u4ee4\u8c03\u4f18\u6a21\u677f\uff0c\u7b80\u5316\u8f93\u51fa\u683c\u5f0f\u4ee5\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5927\u4e0a\u4e0b\u6587\u7a97\u53e3\uff1b2) \u6218\u7565\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u5728\u4fdd\u7559\u5b9e\u4f53\u4fe1\u606f\u7684\u540c\u65f6\u5bf9\u4e0a\u4e0b\u6587\u8fdb\u884c\u6539\u5199", "result": "\u5728CrossNER\u6570\u636e\u96c6\u4e0a\u5c11\u6837\u672c\u65b9\u6cd5\u5e73\u5747F1\u5f97\u5206\u4e3a80.1\uff0c\u4f7f\u7528\u6539\u5199\u65b9\u6cd5\u8bad\u7ec3\u7684\u6a21\u578b\u6bd4\u57fa\u7ebf\u7248\u672cF1\u5206\u6570\u63d0\u5347\u9ad8\u8fbe17\u5206", "conclusion": "\u4e3a\u8bad\u7ec3\u6570\u636e\u548c\u8ba1\u7b97\u80fd\u529b\u6709\u9650\u7684\u7fa4\u4f53\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684NER\u89e3\u51b3\u65b9\u6848"}}
{"id": "2510.17725", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17725", "abs": "https://arxiv.org/abs/2510.17725", "authors": ["Haozhen Zhang", "Tao Feng", "Pengrui Han", "Jiaxuan You"], "title": "AcademicEval: Live Long-Context LLM Benchmark", "comment": "Accepted by TMLR. Code is available at\n  https://github.com/ulab-uiuc/AcademicEval", "summary": "Large Language Models (LLMs) have recently achieved remarkable performance in\nlong-context understanding. However, current long-context LLM benchmarks are\nlimited by rigid context length, labor-intensive annotation, and the pressing\nchallenge of label leakage issues during LLM training. Therefore, we propose\n\\textsc{AcademicEval}, a live benchmark for evaluating LLMs over long-context\ngeneration tasks. \\textsc{AcademicEval} adopts papers on arXiv to introduce\nseveral academic writing tasks with long-context inputs, \\textit{i.e.},\n\\textsc{Title}, \\textsc{Abstract}, \\textsc{Introduction}, and \\textsc{Related\nWork}, which cover a wide range of abstraction levels and require no manual\nlabeling. Moreover, \\textsc{AcademicEval} integrates high-quality and\nexpert-curated few-shot demonstrations from a collected co-author graph to\nenable flexible context length. Especially, \\textsc{AcademicEval} features an\nefficient live evaluation, ensuring no label leakage. We conduct a holistic\nevaluation on \\textsc{AcademicEval}, and the results illustrate that LLMs\nperform poorly on tasks with hierarchical abstraction levels and tend to\nstruggle with long few-shot demonstrations, highlighting the challenge of our\nbenchmark. Through experimental analysis, we also reveal some insights for\nenhancing LLMs' long-context modeling capabilities. Code is available at\nhttps://github.com/ulab-uiuc/AcademicEval", "AI": {"tldr": "\u63d0\u51fa\u4e86AcademicEval\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30LLMs\u5728\u957f\u4e0a\u4e0b\u6587\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u5b9e\u65f6\u57fa\u51c6\uff0c\u4f7f\u7528arXiv\u8bba\u6587\u4f5c\u4e3a\u6570\u636e\u6e90\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff0c\u6709\u6548\u907f\u514d\u6807\u7b7e\u6cc4\u9732\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u957f\u4e0a\u4e0b\u6587LLM\u57fa\u51c6\u5b58\u5728\u4e0a\u4e0b\u6587\u957f\u5ea6\u56fa\u5b9a\u3001\u6807\u6ce8\u5de5\u4f5c\u91cf\u5927\u3001\u4ee5\u53caLLM\u8bad\u7ec3\u4e2d\u6807\u7b7e\u6cc4\u9732\u7684\u7d27\u8feb\u6311\u6218\u3002", "method": "\u91c7\u7528arXiv\u8bba\u6587\u6784\u5efa\u5b66\u672f\u5199\u4f5c\u4efb\u52a1\uff08\u6807\u9898\u3001\u6458\u8981\u3001\u5f15\u8a00\u3001\u76f8\u5173\u5de5\u4f5c\uff09\uff0c\u96c6\u6210\u9ad8\u8d28\u91cf\u4e13\u5bb6\u7b56\u5212\u7684few-shot\u6f14\u793a\uff0c\u652f\u6301\u7075\u6d3b\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u5b9e\u73b0\u9ad8\u6548\u5b9e\u65f6\u8bc4\u4f30\u3002", "result": "LLMs\u5728\u5177\u6709\u5c42\u6b21\u62bd\u8c61\u7ea7\u522b\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u8f83\u5dee\uff0c\u4e14\u96be\u4ee5\u5904\u7406\u957ffew-shot\u6f14\u793a\uff0c\u7a81\u663e\u4e86\u8be5\u57fa\u51c6\u7684\u6311\u6218\u6027\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u63ed\u793a\u4e86\u589e\u5f3aLLMs\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u80fd\u529b\u7684\u4e00\u4e9b\u89c1\u89e3\uff0c\u8be5\u57fa\u51c6\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdbLLMs\u7684\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2510.17733", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17733", "abs": "https://arxiv.org/abs/2510.17733", "authors": ["Tong Chen", "Akari Asai", "Luke Zettlemoyer", "Hannaneh Hajishirzi", "Faeze Brahman"], "title": "Train for Truth, Keep the Skills: Binary Retrieval-Augmented Reward Mitigates Hallucinations", "comment": null, "summary": "Language models often generate factually incorrect information unsupported by\ntheir training data, a phenomenon known as extrinsic hallucination. Existing\nmitigation approaches often degrade performance on open-ended generation and\ndownstream tasks, limiting their practical utility. We propose an online\nreinforcement learning method using a novel binary retrieval-augmented reward\n(RAR) to address this tradeoff. Unlike continuous reward schemes, our approach\nassigns a reward of one only when the model's output is entirely factually\ncorrect, and zero otherwise. We evaluate our method on Qwen3 reasoning models\nacross diverse tasks. For open-ended generation, binary RAR achieves a 39.3%\nreduction in hallucination rates, substantially outperforming both supervised\ntraining and continuous-reward RL baselines. In short-form question answering,\nthe model learns calibrated abstention, strategically outputting \"I don't know\"\nwhen faced with insufficient parametric knowledge. This yields 44.4% and 21.7%\nfewer incorrect answers on PopQA and GPQA, respectively. Crucially, these\nfactuality gains come without performance degradation on instruction following,\nmath, or code, whereas continuous-reward RL, despite improving factuality,\ninduces quality regressions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u4e8c\u5143\u68c0\u7d22\u589e\u5f3a\u5956\u52b1\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u6709\u6548\u51cf\u5c11\u8bed\u8a00\u6a21\u578b\u7684\u5916\u6e90\u6027\u5e7b\u89c9\uff0c\u5728\u4fdd\u6301\u5176\u4ed6\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e8b\u5b9e\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7f13\u89e3\u8bed\u8a00\u6a21\u578b\u5916\u6e90\u6027\u5e7b\u89c9\u7684\u65b9\u6cd5\u5f80\u5f80\u4f1a\u5f71\u54cd\u5f00\u653e\u5f0f\u751f\u6210\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u4f7f\u7528\u4e8c\u5143\u68c0\u7d22\u589e\u5f3a\u5956\u52b1\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u53ea\u6709\u5f53\u6a21\u578b\u8f93\u51fa\u5b8c\u5168\u6b63\u786e\u65f6\u5956\u52b1\u4e3a1\uff0c\u5426\u5219\u4e3a0\u3002", "result": "\u5728\u5f00\u653e\u5f0f\u751f\u6210\u4e2d\u5e7b\u89c9\u7387\u964d\u4f4e39.3%\uff1b\u5728\u77ed\u683c\u5f0f\u95ee\u7b54\u4e2d\uff0c\u6a21\u578b\u5b66\u4f1a\u7b56\u7565\u6027\u5f03\u6743\uff0c\u5728PopQA\u548cGPQA\u4e0a\u9519\u8bef\u7b54\u6848\u5206\u522b\u51cf\u5c1144.4%\u548c21.7%\uff0c\u4e14\u4e0d\u5f71\u54cd\u6307\u4ee4\u9075\u5faa\u3001\u6570\u5b66\u548c\u4ee3\u7801\u80fd\u529b\u3002", "conclusion": "\u4e8c\u5143\u5956\u52b1\u65b9\u6848\u5728\u63d0\u5347\u4e8b\u5b9e\u51c6\u786e\u6027\u7684\u540c\u65f6\u907f\u514d\u4e86\u8fde\u7eed\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u5e26\u6765\u7684\u8d28\u91cf\u9000\u5316\u95ee\u9898\uff0c\u5177\u6709\u66f4\u597d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.17764", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17764", "abs": "https://arxiv.org/abs/2510.17764", "authors": ["Xiao Ye", "Jacob Dineen", "Zhaonan Li", "Zhikun Xu", "Weiyu Chen", "Shijie Lu", "Yuxi Huang", "Ming Shen", "Phu Tran", "Ji-Eun Irene Yum", "Muhammad Ali Khan", "Muhammad Umar Afzal", "Irbaz Bin Riaz", "Ben Zhou"], "title": "Evaluating Medical LLMs by Levels of Autonomy: A Survey Moving from Benchmarks to Applications", "comment": null, "summary": "Medical Large language models achieve strong scores on standard benchmarks;\nhowever, the transfer of those results to safe and reliable performance in\nclinical workflows remains a challenge. This survey reframes evaluation through\na levels-of-autonomy lens (L0-L3), spanning informational tools, information\ntransformation and aggregation, decision support, and supervised agents. We\nalign existing benchmarks and metrics with the actions permitted at each level\nand their associated risks, making the evaluation targets explicit. This\nmotivates a level-conditioned blueprint for selecting metrics, assembling\nevidence, and reporting claims, alongside directions that link evaluation to\noversight. By centering autonomy, the survey moves the field beyond score-based\nclaims toward credible, risk-aware evidence for real clinical use.", "AI": {"tldr": "\u8be5\u8c03\u67e5\u901a\u8fc7\u81ea\u4e3b\u6027\u5c42\u7ea7\u6846\u67b6(L0-L3)\u91cd\u65b0\u6784\u5efa\u533b\u5b66\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\uff0c\u5c06\u73b0\u6709\u57fa\u51c6\u4e0e\u5404\u5c42\u7ea7\u5141\u8bb8\u7684\u64cd\u4f5c\u548c\u98ce\u9669\u5bf9\u9f50\uff0c\u63d0\u51fa\u57fa\u4e8e\u5c42\u7ea7\u7684\u8bc4\u4f30\u84dd\u56fe\uff0c\u63a8\u52a8\u4ece\u5206\u6570\u5bfc\u5411\u8f6c\u5411\u771f\u5b9e\u4e34\u5e8a\u4f7f\u7528\u7684\u53ef\u4fe1\u8bc1\u636e\u3002", "motivation": "\u533b\u5b66\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u8fd9\u4e9b\u7ed3\u679c\u5411\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u5b89\u5168\u53ef\u9760\u6027\u80fd\u7684\u8f6c\u79fb\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u66f4\u8d34\u8fd1\u5b9e\u9645\u4e34\u5e8a\u4f7f\u7528\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u91c7\u7528\u81ea\u4e3b\u6027\u5c42\u7ea7\u6846\u67b6(L0-L3)\uff1a\u4fe1\u606f\u5de5\u5177\u3001\u4fe1\u606f\u8f6c\u6362\u4e0e\u805a\u5408\u3001\u51b3\u7b56\u652f\u6301\u3001\u76d1\u7763\u4ee3\u7406\uff0c\u5c06\u73b0\u6709\u57fa\u51c6\u548c\u6307\u6807\u4e0e\u5404\u5c42\u7ea7\u7684\u5141\u8bb8\u64cd\u4f5c\u548c\u98ce\u9669\u5bf9\u9f50\uff0c\u5efa\u7acb\u5c42\u7ea7\u6761\u4ef6\u5316\u7684\u8bc4\u4f30\u84dd\u56fe\u3002", "result": "\u63d0\u51fa\u4e86\u660e\u786e\u7684\u8bc4\u4f30\u76ee\u6807\uff0c\u4f7f\u8bc4\u4f30\u4e0e\u76d1\u7ba1\u76f8\u5173\u8054\uff0c\u4e3a\u9009\u62e9\u6307\u6807\u3001\u6536\u96c6\u8bc1\u636e\u548c\u62a5\u544a\u58f0\u660e\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u4ee5\u81ea\u4e3b\u6027\u4e3a\u4e2d\u5fc3\uff0c\u8be5\u8c03\u67e5\u63a8\u52a8\u9886\u57df\u8d85\u8d8a\u57fa\u4e8e\u5206\u6570\u7684\u58f0\u660e\uff0c\u8f6c\u5411\u4e3a\u771f\u5b9e\u4e34\u5e8a\u4f7f\u7528\u63d0\u4f9b\u53ef\u4fe1\u3001\u98ce\u9669\u611f\u77e5\u7684\u8bc1\u636e\u3002"}}
{"id": "2510.17793", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17793", "abs": "https://arxiv.org/abs/2510.17793", "authors": ["Austin Xu", "Xuan-Phi Nguyen", "Yilun Zhou", "Chien-Sheng Wu", "Caiming Xiong", "Shafiq Joty"], "title": "Foundational Automatic Evaluators: Scaling Multi-Task Generative Evaluator Training for Reasoning-Centric Domains", "comment": "29 pages, 9 tables, 6 figures", "summary": "Finetuning specialized generative evaluators has emerged as a popular\nparadigm to meet the increasing demand for scalable evaluation during both\ntraining and test-time. However, recent work has largely focused on applying\nnew methodology, such as reinforcement learning (RL), to training evaluators,\nshying away from large-scale, data-driven development. In this work, we focus\non data scaling, curating a set of 2.5M samples spanning five unique evaluation\ntasks (pairwise, step-level, reference-free and reference-based verification,\nand single rating) and multiple domains focused on reasoning evaluation. With\nour data, we train Foundational Automatic Reasoning Evaluators (FARE), a family\nof 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative\nrejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges\nlarger specialized RL-trained evaluators and FARE-20B sets the new standard for\nopen-source evaluators, surpassing specialized 70B+ evaluators. Beyond static\nbenchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers,\nFARE-20B achieves near-oracle performance on MATH. As verifiers in RL training,\nFARE improves the downstream RL-trained model performance by up to 14.1% vs.\nstring-matching verifiers. When initialized from FARE, a continually-finetuned\nFARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86FARE\uff08\u57fa\u7840\u81ea\u52a8\u63a8\u7406\u8bc4\u4f30\u5668\uff09\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u636e\uff08250\u4e07\u6837\u672c\uff09\u548c\u7b80\u5355\u7684\u8fed\u4ee3\u62d2\u7edd\u91c7\u6837SFT\u65b9\u6cd5\u8bad\u7ec3\u4e868B\u548c20B\u53c2\u6570\u7684\u8bc4\u4f30\u5668\uff0c\u5728\u591a\u4e2a\u63a8\u7406\u8bc4\u4f30\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u66f4\u5927\u7684\u4e13\u95e8\u5316\u8bc4\u4f30\u5668\u3002", "motivation": "\u5f53\u524d\u751f\u6210\u5f0f\u8bc4\u4f30\u5668\u4e3b\u8981\u5173\u6ce8\u65b0\u65b9\u6cd5\u5982\u5f3a\u5316\u5b66\u4e60\uff0c\u800c\u5ffd\u89c6\u4e86\u5927\u89c4\u6a21\u6570\u636e\u9a71\u52a8\u7684\u53d1\u5c55\u3002\u672c\u6587\u4e13\u6ce8\u4e8e\u6570\u636e\u6269\u5c55\uff0c\u4ee5\u6ee1\u8db3\u8bad\u7ec3\u548c\u6d4b\u8bd5\u65f6\u5bf9\u53ef\u6269\u5c55\u8bc4\u4f30\u65e5\u76ca\u589e\u957f\u7684\u9700\u6c42\u3002", "method": "\u6536\u96c6\u4e86250\u4e07\u6837\u672c\uff0c\u6db5\u76d65\u79cd\u8bc4\u4f30\u4efb\u52a1\u548c\u591a\u4e2a\u63a8\u7406\u9886\u57df\u3002\u4f7f\u7528\u7b80\u5355\u7684\u8fed\u4ee3\u62d2\u7edd\u91c7\u6837\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u65b9\u6cd5\u8bad\u7ec3\u4e868B\u548c20B\u53c2\u6570\u7684FARE\u8bc4\u4f30\u5668\u3002", "result": "FARE-8B\u6311\u6218\u4e86\u66f4\u5927\u7684\u4e13\u95e8\u5316RL\u8bad\u7ec3\u8bc4\u4f30\u5668\uff0cFARE-20B\u4e3a\u5f00\u6e90\u8bc4\u4f30\u5668\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\uff0c\u8d85\u8d8a\u4e86\u4e13\u95e8\u768470B+\u8bc4\u4f30\u5668\u3002\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\uff0c\u4f5c\u4e3a\u63a8\u7406\u65f6\u91cd\u6392\u5668\u5728MATH\u4e0a\u8fbe\u5230\u63a5\u8fd1oracle\u6027\u80fd\uff0c\u4f5c\u4e3aRL\u8bad\u7ec3\u9a8c\u8bc1\u5668\u5c06\u4e0b\u6e38\u6a21\u578b\u6027\u80fd\u63d0\u5347\u8fbe14.1%\u3002", "conclusion": "\u5927\u89c4\u6a21\u6570\u636e\u9a71\u52a8\u7684\u7b80\u5355SFT\u65b9\u6cd5\u53ef\u4ee5\u8bad\u7ec3\u51fa\u5f3a\u5927\u7684\u8bc4\u4f30\u5668\uff0cFARE\u5728\u9759\u6001\u57fa\u51c6\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u8bc4\u4f30\u5668\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.17795", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17795", "abs": "https://arxiv.org/abs/2510.17795", "authors": ["Yujie Luo", "Zhuoyun Yu", "Xuehai Wang", "Yuqi Zhu", "Ningyu Zhang", "Lanning Wei", "Lun Du", "Da Zheng", "Huajun Chen"], "title": "Executable Knowledge Graphs for Replicating AI Research", "comment": "Work in progress", "summary": "Replicating AI research is a crucial yet challenging task for large language\nmodel (LLM) agents. Existing approaches often struggle to generate executable\ncode, primarily due to insufficient background knowledge and the limitations of\nretrieval-augmented generation (RAG) methods, which fail to capture latent\ntechnical details hidden in referenced papers. Furthermore, previous approaches\ntend to overlook valuable implementation-level code signals and lack structured\nknowledge representations that support multi-granular retrieval and reuse. To\novercome these challenges, we propose Executable Knowledge Graphs (xKG), a\nmodular and pluggable knowledge base that automatically integrates technical\ninsights, code snippets, and domain-specific knowledge extracted from\nscientific literature. When integrated into three agent frameworks with two\ndifferent LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on\nPaperBench, demonstrating its effectiveness as a general and extensible\nsolution for automated AI research replication. Code will released at\nhttps://github.com/zjunlp/xKG.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u53ef\u6267\u884c\u77e5\u8bc6\u56fe\u8c31(xKG)\u6765\u89e3\u51b3AI\u7814\u7a76\u590d\u73b0\u4e2d\u7684\u6311\u6218\uff0c\u901a\u8fc7\u6574\u5408\u6280\u672f\u6d1e\u5bdf\u3001\u4ee3\u7801\u7247\u6bb5\u548c\u9886\u57df\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u4ee3\u7406\u7684\u7814\u7a76\u590d\u73b0\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u53ef\u6267\u884c\u4ee3\u7801\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u4e3b\u8981\u7531\u4e8e\u80cc\u666f\u77e5\u8bc6\u4e0d\u8db3\u548cRAG\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u6355\u6349\u53c2\u8003\u6587\u732e\u4e2d\u7684\u6f5c\u5728\u6280\u672f\u7ec6\u8282\uff0c\u4e14\u7f3a\u4e4f\u652f\u6301\u591a\u7c92\u5ea6\u68c0\u7d22\u548c\u91cd\u7528\u7684\u7ed3\u6784\u5316\u77e5\u8bc6\u8868\u793a\u3002", "method": "\u63d0\u51fa\u53ef\u6267\u884c\u77e5\u8bc6\u56fe\u8c31(xKG)\uff0c\u8fd9\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u53ef\u63d2\u62d4\u7684\u77e5\u8bc6\u5e93\uff0c\u81ea\u52a8\u4ece\u79d1\u5b66\u6587\u732e\u4e2d\u63d0\u53d6\u6280\u672f\u6d1e\u5bdf\u3001\u4ee3\u7801\u7247\u6bb5\u548c\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\uff0c\u5e76\u96c6\u6210\u5230\u4ee3\u7406\u6846\u67b6\u4e2d\u3002", "result": "\u5728\u4e09\u4e2a\u4ee3\u7406\u6846\u67b6\u548c\u4e24\u79cd\u4e0d\u540cLLM\u4e0a\u96c6\u6210xKG\u540e\uff0c\u5728PaperBench\u4e0a\u663e\u793a\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff08\u4f7f\u7528o3-mini\u65f6\u63d0\u534710.9%\uff09\u3002", "conclusion": "xKG\u88ab\u8bc1\u660e\u662f\u81ea\u52a8\u5316AI\u7814\u7a76\u590d\u73b0\u7684\u901a\u7528\u4e14\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5c06\u5728\u6307\u5b9aGitHub\u4ed3\u5e93\u53d1\u5e03\u3002"}}
{"id": "2510.17797", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17797", "abs": "https://arxiv.org/abs/2510.17797", "authors": ["Akshara Prabhakar", "Roshan Ram", "Zixiang Chen", "Silvio Savarese", "Frank Wang", "Caiming Xiong", "Huan Wang", "Weiran Yao"], "title": "Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics", "comment": "Technical report; 13 pages plus references and appendices", "summary": "As information grows exponentially, enterprises face increasing pressure to\ntransform unstructured data into coherent, actionable insights. While\nautonomous agents show promise, they often struggle with domain-specific\nnuances, intent alignment, and enterprise integration. We present Enterprise\nDeep Research (EDR), a multi-agent system that integrates (1) a Master Planning\nAgent for adaptive query decomposition, (2) four specialized search agents\n(General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool\necosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a\nVisualization Agent for data-driven insights, and (5) a reflection mechanism\nthat detects knowledge gaps and updates research direction with optional\nhuman-in-the-loop steering guidance. These components enable automated report\ngeneration, real-time streaming, and seamless enterprise deployment, as\nvalidated on internal datasets. On open-ended benchmarks including DeepResearch\nBench and DeepConsult, EDR outperforms state-of-the-art agentic systems without\nany human steering. We release the EDR framework and benchmark trajectories to\nadvance research on multi-agent reasoning applications.\n  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and\nDataset at https://huggingface.co/datasets/Salesforce/EDR-200", "AI": {"tldr": "\u4f01\u4e1a\u6df1\u5ea6\u7814\u7a76(EDR)\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e3b\u89c4\u5212\u667a\u80fd\u4f53\u548c\u56db\u4e2a\u4e13\u4e1a\u641c\u7d22\u667a\u80fd\u4f53\uff0c\u7ed3\u5408\u53ef\u6269\u5c55\u5de5\u5177\u751f\u6001\u7cfb\u7edf\u548c\u53ef\u89c6\u5316\u667a\u80fd\u4f53\uff0c\u5b9e\u73b0\u4f01\u4e1a\u975e\u7ed3\u6784\u5316\u6570\u636e\u7684\u81ea\u52a8\u5316\u7814\u7a76\u548c\u62a5\u544a\u751f\u6210\u3002", "motivation": "\u968f\u7740\u4fe1\u606f\u6307\u6570\u7ea7\u589e\u957f\uff0c\u4f01\u4e1a\u9762\u4e34\u5c06\u975e\u7ed3\u6784\u5316\u6570\u636e\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u6d1e\u5bdf\u7684\u538b\u529b\u3002\u73b0\u6709\u81ea\u4e3b\u667a\u80fd\u4f53\u5728\u9886\u57df\u7279\u5b9a\u7ec6\u5fae\u5dee\u522b\u3001\u610f\u56fe\u5bf9\u9f50\u548c\u4f01\u4e1a\u96c6\u6210\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "EDR\u7cfb\u7edf\u5305\u542b\uff1a(1)\u4e3b\u89c4\u5212\u667a\u80fd\u4f53\u8fdb\u884c\u81ea\u9002\u5e94\u67e5\u8be2\u5206\u89e3\uff1b(2)\u56db\u4e2a\u4e13\u4e1a\u641c\u7d22\u667a\u80fd\u4f53(\u901a\u7528\u3001\u5b66\u672f\u3001GitHub\u3001LinkedIn)\uff1b(3)\u57fa\u4e8eMCP\u7684\u53ef\u6269\u5c55\u5de5\u5177\u751f\u6001\u7cfb\u7edf\uff1b(4)\u53ef\u89c6\u5316\u667a\u80fd\u4f53\u751f\u6210\u6570\u636e\u9a71\u52a8\u7684\u6d1e\u5bdf\uff1b(5)\u68c0\u6d4b\u77e5\u8bc6\u5dee\u8ddd\u5e76\u66f4\u65b0\u7814\u7a76\u65b9\u5411\u7684\u53cd\u601d\u673a\u5236\u3002", "result": "\u5728DeepResearch Bench\u548cDeepConsult\u7b49\u5f00\u653e\u5f0f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEDR\u5728\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u7684\u60c5\u51b5\u4e0b\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "conclusion": "EDR\u6846\u67b6\u548c\u57fa\u51c6\u8f68\u8ff9\u7684\u53d1\u5e03\u5c06\u63a8\u52a8\u591a\u667a\u80fd\u4f53\u63a8\u7406\u5e94\u7528\u7684\u7814\u7a76\u53d1\u5c55\u3002"}}
