<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 90]
- [cs.AI](#cs.AI) [Total: 55]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [PlotCraft: Pushing the Limits of LLMs for Complex and Interactive Data Visualization](https://arxiv.org/abs/2511.00010)
*Jiajun Zhang,Jianke Zhang,Zeyu Cui,Jiaxi Yang,Lei Zhang,Binyuan Hui,Qiang Liu,Zilei Wang,Liang Wang,Junyang Lin*

Main category: cs.CL

TL;DR: PlotCraft是一个包含1000个挑战性可视化任务的基准测试，评估LLMs在复杂数据可视化方面的能力。研究开发了SynthVis-30K数据集和PlotCraftor模型，在复杂可视化任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在代码生成方面表现出色，但在处理复杂结构化数据的可视化任务方面能力尚未充分评估和发展。

Method: 创建PlotCraft基准测试，包含1k个可视化任务，涵盖7个高级任务和48种图表类型；开发SynthVis-30K数据集；构建PlotCraftor代码生成模型。

Result: 对23个领先LLMs的评估显示在复杂可视化任务上存在明显性能缺陷；PlotCraftor模型在多个基准测试中表现与领先专有方法相当，在困难任务上性能提升超过50%。

Conclusion: PlotCraft基准测试揭示了LLMs在复杂可视化方面的局限性，PlotCraftor模型通过专门训练显著提升了复杂数据可视化的代码生成能力。

Abstract: Recent Large Language Models (LLMs) have demonstrated remarkable profi-
ciency in code generation. However, their ability to create complex visualiza-
tions for scaled and structured data remains largely unevaluated and
underdevel- oped. To address this gap, we introduce PlotCraft, a new benchmark
featuring 1k challenging visualization tasks that cover a wide range of topics,
such as fi- nance, scientific research, and sociology. The benchmark is
structured around seven high-level visualization tasks and encompasses 48
distinct chart types. Cru- cially, it is the first to systematically evaluate
both single-turn generation and multi-turn refinement across a diverse spectrum
of task complexities. Our com- prehensive evaluation of 23 leading LLMs on
PlotCraft reveals obvious per- formance deficiencies in handling sophisticated
visualization tasks. To bridge this performance gap, we develope SynthVis-30K,
a large-scale, high-quality dataset of complex visualization code synthesized
via a collaborative agent frame- work. Building upon this dataset, we develope
PlotCraftor, a novel code gener- ation model that achieves strong capabilities
in complex data visualization with a remarkably small size. Across VisEval,
PandasPlotBench, and our proposed PlotCraft, PlotCraftor shows performance
comparable to that of leading propri- etary approaches. Especially, on hard
task, Our model achieves over 50% per- formance improvement. We will release
the benchmark, dataset, and code at
https://github.com/Speakn0w/PlotCraft-Benchmark.

</details>


### [2] [Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference](https://arxiv.org/abs/2511.00115)
*Haoyuan Li,Yuanbo Tong,Yuchen Li,Zirui Wang,Chunhou Liu,Jiamou Liu*

Main category: cs.CL

TL;DR: ProtoMBTI是一个基于原型理论的MBTI人格识别框架，通过LLM引导的数据增强、原型标准化和检索-修正循环，在文本人格识别任务中实现了更好的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统的人格识别方法将任务视为硬标签分类，这掩盖了人类人格判断的渐进性和原型性质。本文旨在开发一个更符合认知心理学的原型理论框架。

Method: 1）通过LLM引导的多维度增强构建平衡语料库；2）使用LoRA微调轻量级编码器学习判别性嵌入和标准化人格原型；3）在推理时执行检索-重用-修正-保留循环，通过基于提示的投票聚合原型证据。

Result: 在Kaggle和Pandora基准测试中，ProtoMBTI在四个MBTI二分法和完整16类型任务上都优于基线方法，并展现出强大的跨数据集泛化能力。

Conclusion: 将推理过程与心理原型推理对齐，可以在基于文本的人格建模中提高准确性、可解释性和迁移能力。

Abstract: Personality recognition from text is typically cast as hard-label
classification, which obscures the graded, prototype-like nature of human
personality judgments. We present ProtoMBTI, a cognitively aligned framework
for MBTI inference that operationalizes prototype theory within an LLM-based
pipeline. First, we construct a balanced, quality-controlled corpus via
LLM-guided multi-dimensional augmentation (semantic, linguistic, sentiment).
Next, we LoRA-fine-tune a lightweight (<=2B) encoder to learn discriminative
embeddings and to standardize a bank of personality prototypes. At inference,
we retrieve top-k prototypes for a query post and perform a
retrieve--reuse--revise--retain cycle: the model aggregates prototype evidence
via prompt-based voting, revises when inconsistencies arise, and, upon correct
prediction, retains the sample to continually enrich the prototype library.
Across Kaggle and Pandora benchmarks, ProtoMBTI improves over baselines on both
the four MBTI dichotomies and the full 16-type task, and exhibits robust
cross-dataset generalization. Our results indicate that aligning the inference
process with psychological prototype reasoning yields gains in accuracy,
interpretability, and transfer for text-based personality modeling.

</details>


### [3] [ParaScopes: What do Language Models Activations Encode About Future Text?](https://arxiv.org/abs/2511.00180)
*Nicky Pochinkov,Yulia Volkova,Anna Vasileva,Sai V R Chereddy*

Main category: cs.CL

TL;DR: 开发了残差流解码器框架，用于探测语言模型在段落和文档尺度上的规划信息，发现可以解码相当于5个以上未来token的信息。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型处理更长时程任务的能力增强，现有理解激活的方法通常局限于测试特定概念或token，需要开发能探测更大尺度规划信息的方法。

Method: 开发了残差流解码器框架，测试了多种方法来探测模型激活中的段落和文档尺度规划信息。

Result: 在小型模型中发现可以解码相当于5个以上未来token的信息，这些信息与模型的长时程规划相关。

Conclusion: 这些结果为更好地监控语言模型和理解它们如何编码长期规划信息奠定了基础。

Abstract: Interpretability studies in language models often investigate forward-looking
representations of activations. However, as language models become capable of
doing ever longer time horizon tasks, methods for understanding activations
often remain limited to testing specific concepts or tokens. We develop a
framework of Residual Stream Decoders as a method of probing model activations
for paragraph-scale and document-scale plans. We test several methods and find
information can be decoded equivalent to 5+ tokens of future context in small
models. These results lay the groundwork for better monitoring of language
models and better understanding how they might encode longer-term planning
information.

</details>


### [4] [Training LLMs Beyond Next Token Prediction - Filling the Mutual Information Gap](https://arxiv.org/abs/2511.00198)
*Chun-Hao Yang,Bo-Han Feng,Tzu-Yuan Lai,Yan Yu Chen,Yin-Kai Dean Huang,Shou-De Lin*

Main category: cs.CL

TL;DR: 该论文提出了一种替代传统下一词预测(NTP)的LLM训练方法，通过预测信息丰富的token来优化训练效果。


<details>
  <summary>Details</summary>
Motivation: 挑战传统的下一词预测训练方法，寻求在保持计算成本的同时更有效地训练大语言模型。

Method: 提出预测信息丰富token的训练策略，并在算术、多标签文本分类和自然语言生成三类任务中进行验证。

Result: 提供了一种优化LLM训练的原则性方法，提升了模型性能并加深了对目标token选择策略的理论理解。

Conclusion: 通过选择性地预测信息丰富的token，可以比传统下一词预测方法更有效地训练大语言模型。

Abstract: Optimizing training performance in large language models (LLMs) remains an
essential challenge, particularly in improving model performance while
maintaining computational costs. This work challenges the conventional approach
of training LLMs using next-token prediction (NTP), arguing that by predicting
information-rich tokens during training, there is a more effective way to train
LLMs. We investigate the impact of the proposed solution in three kinds of
tasks for LLMs: arithmetic, multi-label classification of text, and
natural-language generation. This work offers a principled approach to
optimizing LLM training, advancing both model performance and theoretical
understanding of the target-token selection strategies.

</details>


### [5] [Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2511.00222)
*Marwa Abdulhai,Ryan Cheng,Donovan Clay,Tim Althoff,Sergey Levine,Natasha Jaques*

Main category: cs.CL

TL;DR: 提出了一个评估和改进LLM角色一致性的统一框架，通过多轮强化学习减少角色漂移问题


<details>
  <summary>Details</summary>
Motivation: LLM在模拟人类用户时经常出现角色漂移、前后矛盾或放弃角色适当行为的问题，影响模拟效果

Method: 定义了三种自动一致性指标，并作为奖励信号用于多轮强化学习微调LLM

Result: 方法将不一致性降低了55%以上，产生了更连贯和忠实的模拟用户

Conclusion: 该框架能有效提高LLM在角色扮演中的一致性，为模拟用户应用提供了可靠解决方案

Abstract: Large Language Models (LLMs) are increasingly used to simulate human users in
interactive settings such as therapy, education, and social role-play. While
these simulations enable scalable training and evaluation of AI agents,
off-the-shelf LLMs often drift from their assigned personas, contradict earlier
statements, or abandon role-appropriate behavior. We introduce a unified
framework for evaluating and improving persona consistency in LLM-generated
dialogue. We define three automatic metrics: prompt-to-line consistency,
line-to-line consistency, and Q&A consistency, that capture different types of
persona drift and validate each against human annotations. Using these metrics
as reward signals, we apply multi-turn reinforcement learning to fine-tune LLMs
for three user roles: a patient, a student, and a social chat partner. Our
method reduces inconsistency by over 55%, resulting in more coherent and
faithful simulated users.

</details>


### [6] [AgentBnB: A Browser-Based Cybersecurity Tabletop Exercise with Large Language Model Support and Retrieval-Aligned Scaffolding](https://arxiv.org/abs/2511.00265)
*Arman Anwar,Zefang Liu*

Main category: cs.CL

TL;DR: AgentBnB是一个基于浏览器的网络安全桌面演习系统，通过集成大型语言模型队友和检索增强的副驾驶，提供轻量级、可扩展的网络安全训练体验。


<details>
  <summary>Details</summary>
Motivation: 传统网络安全桌面演习存在脚本化、资源密集和难以扩展的问题，需要一种更轻量、可重复的替代方案。

Method: 系统将精选语料库扩展为事实性、概念性、程序性和元认知片段，提供按需认知提示。使用提示工程代理和逐步淡出的脚手架阶梯。

Result: 在四人研究生试点中，参与者报告更倾向于使用基于代理的版本，认为其更具可扩展性，但在简单知识测验中出现天花板效应。

Conclusion: 尽管存在样本量小、单人模式和语料库狭窄等局限，早期结果表明LLM增强的桌面演习可以提供轻量级、可重复的实践，无需传统演习的后勤负担。

Abstract: Traditional cybersecurity tabletop exercises (TTXs) provide valuable training
but are often scripted, resource-intensive, and difficult to scale. We
introduce AgentBnB, a browser-based re-imagining of the Backdoors & Breaches
game that integrates large language model teammates with a Bloom-aligned,
retrieval-augmented copilot (C2D2). The system expands a curated corpus into
factual, conceptual, procedural, and metacognitive snippets, delivering
on-demand, cognitively targeted hints. Prompt-engineered agents employ a
scaffolding ladder that gradually fades as learner confidence grows. In a
solo-player pilot with four graduate students, participants reported greater
intention to use the agent-based version compared to the physical card deck and
viewed it as more scalable, though a ceiling effect emerged on a simple
knowledge quiz. Despite limitations of small sample size, single-player focus,
and narrow corpus, these early findings suggest that large language model
augmented TTXs can provide lightweight, repeatable practice without the
logistical burden of traditional exercises. Planned extensions include
multi-player modes, telemetry-driven coaching, and comparative studies with
larger cohorts.

</details>


### [7] [IL-PCSR: Legal Corpus for Prior Case and Statute Retrieval](https://arxiv.org/abs/2511.00268)
*Shounak Paul,Dhananjay Ghumare,Pawan Goyal,Saptarshi Ghosh,Ashutosh Modi*

Main category: cs.CL

TL;DR: 提出了IL-PCR语料库，为法规检索和判例检索任务提供统一测试平台，通过LLM重排序方法利用两个任务之间的依赖关系获得最佳性能


<details>
  <summary>Details</summary>
Motivation: 法律实践中法规检索和判例检索是相关任务，但现有研究独立处理这两个任务，开发了完全不同的数据集和模型，忽略了它们之间的内在联系

Method: 提出IL-PCR语料库作为统一测试平台，实验了词法模型、语义模型和基于GNN的集成模型，并开发了基于LLM的重排序方法来利用任务间依赖关系

Result: 基于LLM的重排序方法在两个检索任务上取得了最佳性能

Conclusion: IL-PCR语料库为法规检索和判例检索提供了统一测试平台，LLM重排序方法通过利用任务间依赖关系显著提升了检索性能

Abstract: Identifying/retrieving relevant statutes and prior cases/precedents for a
given legal situation are common tasks exercised by law practitioners.
Researchers to date have addressed the two tasks independently, thus developing
completely different datasets and models for each task; however, both retrieval
tasks are inherently related, e.g., similar cases tend to cite similar statutes
(due to similar factual situation). In this paper, we address this gap. We
propose IL-PCR (Indian Legal corpus for Prior Case and Statute Retrieval),
which is a unique corpus that provides a common testbed for developing models
for both the tasks (Statute Retrieval and Precedent Retrieval) that can exploit
the dependence between the two. We experiment extensively with several baseline
models on the tasks, including lexical models, semantic models and ensemble
based on GNNs. Further, to exploit the dependence between the two tasks, we
develop an LLM-based re-ranking approach that gives the best performance.

</details>


### [8] [POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation](https://arxiv.org/abs/2511.00270)
*Abhinav Joshi,Vaibhav Sharma,Sanjeet Singh,Ashutosh Modi*

Main category: cs.CL

TL;DR: 提出POSESTITCH-SLT预训练方案，通过模板生成句子对训练，在How2Sign和iSign数据集上显著提升手语翻译性能


<details>
  <summary>Details</summary>
Motivation: 手语翻译面临大规模句子对齐数据集稀缺的挑战，需要新的方法来提升低资源环境下的翻译效果

Method: 基于语言模板的句子生成技术，提出POSESTITCH-SLT预训练方案，使用简单的transformer编码器-解码器架构

Result: 在How2Sign数据集上BLEU-4从1.97提升到4.56，在iSign数据集上从0.55提升到3.43，超越了基于姿态的无注释翻译的现有最佳方法

Conclusion: 模板驱动的合成监督在低资源手语设置中非常有效，证明了该方法在手语翻译任务中的优势

Abstract: Sign language translation remains a challenging task due to the scarcity of
large-scale, sentence-aligned datasets. Prior arts have focused on various
feature extraction and architectural changes to support neural machine
translation for sign languages. We propose POSESTITCH-SLT, a novel pre-training
scheme that is inspired by linguistic-templates-based sentence generation
technique. With translation comparison on two sign language datasets, How2Sign
and iSign, we show that a simple transformer-based encoder-decoder architecture
outperforms the prior art when considering template-generated sentence pairs in
training. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign
and from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for
pose-based gloss-free translation. The results demonstrate the effectiveness of
template-driven synthetic supervision in low-resource sign language settings.

</details>


### [9] [Language Modeling With Factorization Memory](https://arxiv.org/abs/2511.00315)
*Lee Xiong,Maksim Tkachenko,Johanes Effendi,Ting Cai*

Main category: cs.CL

TL;DR: 提出Factorization Memory，一种高效的RNN架构，在短上下文语言建模任务中达到Transformer性能，在长上下文场景中表现更优。


<details>
  <summary>Details</summary>
Motivation: 开发一种结合RNN推理效率和Transformer性能的架构，特别是在长上下文场景中实现更好的泛化能力。

Method: 基于Mamba-2构建，支持训练时并行计算，推理时保持恒定计算和内存复杂度。开发稀疏版本，仅更新部分循环状态。

Result: 在短上下文任务中性能与Transformer相当，在长上下文中表现更优。稀疏版本保持性能同时提升效率。

Conclusion: Factorization Memory是首个成功结合稀疏内存激活的RNN架构，在短长上下文场景中均具竞争力。

Abstract: We propose Factorization Memory, an efficient recurrent neural network (RNN)
architecture that achieves performance comparable to Transformer models on
short-context language modeling tasks while also demonstrating superior
generalization in long-context scenarios. Our model builds upon Mamba-2,
enabling Factorization Memory to exploit parallel computations during training
while preserving constant computational and memory complexity during inference.
To further optimize model efficiency and representational capacity, we develop
a sparse formulation of Factorization Memory that updates only a subset of
recurrent states at each step while preserving the strong performance of its
dense counterpart. To our knowledge, this represents the first RNN architecture
that successfully combines sparse memory activation with competitive
performance across both short and long-context settings. This work provides a
systematic empirical analysis of Factorization Memory in comparison to
Transformer and Mamba-2 architectures.

</details>


### [10] [Reversal Invariance in Autoregressive Language Models](https://arxiv.org/abs/2511.00341)
*Mihir Sahasrabudhe*

Main category: cs.CL

TL;DR: 论文形式化定义了因果语言建模目标的结构特性：反转不变性，即标准CLM预训练对语料库及其反转版本赋予相同的似然度，表明当前预训练目标存在方向盲区。


<details>
  <summary>Details</summary>
Motivation: 自然语言具有时间不对称性（如音韵、形态、因果关系），但当前预训练目标的反转不变性可能导致模型无法捕捉这些方向性依赖关系。

Method: 通过理论分析形式化反转不变性概念，并基于此提出将预训练视为时间不对称问题的新视角。

Result: 研究表明在反转文本上训练的模型能达到与正向文本训练相当的性能，验证了CLM目标的方向盲性。

Conclusion: 当前预训练目标存在局限性，需要开发能显式建模语言方向性的损失函数和架构，同时保持标准语言建模能力。

Abstract: We formalize a structural property of the causal (autoregressive) language
modeling (CLM) objective: reversal invariance. Formally, the next-token
prediction loss assigns identical likelihood to a corpus and its reversal,
implying that standard CLM pretraining is direction-blind. This symmetry
explains why models trained on reversed text can achieve comparable performance
to those trained on forward text, despite the inherently time-asymmetric nature
of human language and reasoning. We argue that this invariance represents a
limitation of current pretraining objectives rather than a benign artifact. If
natural language encodes directional dependencies - phonological,
morphological, or causal - a symmetric objective may fail to capture them. We
therefore propose viewing pretraining through the lens of temporal asymmetry,
motivating future work on loss functions and architectures that explicitly
model the arrow of language while retaining standard language modeling
capacity.

</details>


### [11] [LingGym: How Far Are LLMs from Thinking Like Field Linguists?](https://arxiv.org/abs/2511.00343)
*Changbing Yang,Franklin Ma,Freda Shi,Jian Zhu*

Main category: cs.CL

TL;DR: LingGym是一个评估LLMs元语言推理能力的新基准，使用跨语言注释文本和语法描述，测试模型在未见过的低资源语言和结构上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs是否能在训练中未见过的低资源语言和结构上进行语言推理泛化，而不仅仅是特定下游任务。

Method: 使用来自18种类型多样参考语法的跨语言注释文本和语法描述，设计受控评估任务——词-注释推理，模型需根据不同程度的语言信息推断缺失的词和注释。

Result: 结合结构化语言线索在所有模型中都能持续提升推理性能。

Conclusion: 这项工作展示了使用LLMs进行类型学语言分析和低资源语言文档化的前景和当前局限性。

Abstract: This paper introduces LingGym, a new benchmark that evaluates LLMs' capacity
for meta-linguistic reasoning using Interlinear Glossed Text (IGT) and
grammatical descriptions extracted from 18 typologically diverse reference
grammars. Unlike previous work that focuses on specific downstream tasks, we
assess whether LLMs can generalize linguistic inference across low-resource
languages and structures not seen during training. We present a controlled
evaluation task: Word-Gloss Inference, in which the model must infer a missing
word and gloss from context using varying levels of linguistic information
(e.g., glosses, grammatical explanations, translations). Our results show that
incorporating structured linguistic cues leads to consistent improvements in
reasoning performance across all models. This work highlights both the promise
and current limitations of using LLMs for typologically informed linguistic
analysis and low-resource language documentation.

</details>


### [12] [Reasoning Trajectories for Socratic Debugging of Student Code: From Misconceptions to Contradictions and Updated Beliefs](https://arxiv.org/abs/2511.00371)
*Erfan Al-Hossami,Razvan Bunescu*

Main category: cs.CL

TL;DR: 该论文提出了推理轨迹生成任务，用于苏格拉底式调试，通过引导性推理路径帮助学生识别和修正编程误解，并构建了相关数据集和基于LLM的解决方案。


<details>
  <summary>Details</summary>
Motivation: 大多数新手程序员的错误源于编程误解，苏格拉底式调试通过引导而非直接提供修复来帮助学生自行发现和修正错误。

Method: 引入推理轨迹生成任务，构建手动标注的调试问题数据集，开发基于LLM的推理轨迹生成和苏格拉底对话解决方案。

Result: 前沿模型能够生成高达91%正确的推理轨迹和98.7%有效的对话轮次。

Conclusion: LLM能够有效生成用于苏格拉底式调试的推理轨迹和对话，帮助学生通过认知失调识别和更新编程误解。

Abstract: In Socratic debugging, instructors guide students towards identifying and
fixing a bug on their own, instead of providing the bug fix directly. Most
novice programmer bugs are caused by programming misconceptions, namely false
beliefs about a programming concept. In this context, Socratic debugging can be
formulated as a guided Reasoning Trajectory (RT) leading to a statement about
the program behavior that contradicts the bug-causing misconception. Upon
reaching this statement, the ensuing cognitive dissonance leads the student to
first identify and then update their false belief. In this paper, we introduce
the task of reasoning trajectory generation, together with a dataset of
debugging problems manually annotated with RTs. We then describe LLM-based
solutions for generating RTs and Socratic conversations that are anchored on
them. A large-scale LLM-as-judge evaluation shows that frontier models can
generate up to 91% correct reasoning trajectories and 98.7% valid conversation
turns.

</details>


### [13] [PADBen: A Comprehensive Benchmark for Evaluating AI Text Detectors Against Paraphrase Attacks](https://arxiv.org/abs/2511.00416)
*Yiwei Zha,Rui Min,Shanu Sushmita*

Main category: cs.CL

TL;DR: 论文发现AI生成文本检测器虽然对直接LLM输出有90%+准确率，但在面对迭代转述内容时完全失效。作者揭示了迭代转述创建了一个语义位移但保留生成模式的中间洗白区域，并提出PADBen基准来系统评估检测器对抗转述攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成文本检测器在直接LLM输出上表现良好，但对迭代转述内容检测失败。需要研究为什么这种AI生成的转述文本能够逃避检测系统，并评估检测器的实际鲁棒性。

Method: 通过内在机制分析揭示迭代转述创建中间洗白区域，引入PADBen基准，包含五种文本类型分类和五个渐进检测任务，评估11种最先进检测器。

Result: 检测器在抄袭规避问题上表现成功，但在作者身份混淆问题上完全失败。现有检测方法无法有效处理中间洗白区域，需要超越现有语义和风格判别方法的根本性进步。

Conclusion: 当前检测方法存在严重漏洞，无法应对迭代转述攻击，特别是作者身份混淆攻击。需要开发新的检测架构来应对这种中间洗白区域的挑战。

Abstract: While AI-generated text (AIGT) detectors achieve over 90\% accuracy on direct
LLM outputs, they fail catastrophically against iteratively-paraphrased
content. We investigate why iteratively-paraphrased text -- itself AI-generated
-- evades detection systems designed for AIGT identification. Through intrinsic
mechanism analysis, we reveal that iterative paraphrasing creates an
intermediate laundering region characterized by semantic displacement with
preserved generation patterns, which brings up two attack categories:
paraphrasing human-authored text (authorship obfuscation) and paraphrasing
LLM-generated text (plagiarism evasion). To address these vulnerabilities, we
introduce PADBen, the first benchmark systematically evaluating detector
robustness against both paraphrase attack scenarios. PADBen comprises a
five-type text taxonomy capturing the full trajectory from original content to
deeply laundered text, and five progressive detection tasks across
sentence-pair and single-sentence challenges. We evaluate 11 state-of-the-art
detectors, revealing critical asymmetry: detectors successfully identify the
plagiarism evasion problem but fail for the case of authorship obfuscation. Our
findings demonstrate that current detection approaches cannot effectively
handle the intermediate laundering region, necessitating fundamental advances
in detection architectures beyond existing semantic and stylistic
discrimination methods. For detailed code implementation, please see
https://github.com/JonathanZha47/PadBen-Paraphrase-Attack-Benchmark.

</details>


### [14] [MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts](https://arxiv.org/abs/2511.00421)
*Naoto Iwase,Hiroki Okuyama,Junichiro Iwasawa*

Main category: cs.CL

TL;DR: MedRECT是首个跨语言医疗错误纠正基准，包含日语和英语版本，评估LLM在医疗错误检测、定位和纠正三个子任务上的表现。研究发现推理模型表现最佳，跨语言评估显示英语到日语存在性能差距，微调后的模型在结构化医疗错误纠正任务上超越了人类专家。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗应用中的潜力日益显现，但其检测和纠正临床文本错误的能力——安全部署的前提条件——仍然缺乏充分评估，特别是在英语以外的语言中。

Method: 从日本医师国家考试构建可扩展的自动化流水线，创建MedRECT-ja（663个文本）和MedRECT-en（458个文本）数据集，评估9个当代LLM，包括专有模型、开源模型和推理模型，并进行针对性LoRA微调。

Result: 推理模型显著优于标准架构，错误检测相对提升13.5%，句子提取提升51.0%；跨语言评估显示英语到日语存在5-10%性能差距；微调在错误纠正性能上获得不对称改进（日语：+0.078，英语：+0.168）；微调模型在结构化医疗错误纠正任务上超越人类专家。

Conclusion: MedRECT是首个全面的跨语言医疗错误纠正基准，为开发更安全的跨语言医疗LLM提供了可复现的框架和资源，推理模型和针对性微调能显著提升医疗错误处理能力。

Abstract: Large language models (LLMs) show increasing promise in medical applications,
but their ability to detect and correct errors in clinical texts -- a
prerequisite for safe deployment -- remains under-evaluated, particularly
beyond English. We introduce MedRECT, a cross-lingual benchmark
(Japanese/English) that formulates medical error handling as three subtasks:
error detection, error localization (sentence extraction), and error
correction. MedRECT is built with a scalable, automated pipeline from the
Japanese Medical Licensing Examinations (JMLE) and a curated English
counterpart, yielding MedRECT-ja (663 texts) and MedRECT-en (458 texts) with
comparable error/no-error balance. We evaluate 9 contemporary LLMs spanning
proprietary, open-weight, and reasoning families. Key findings: (i) reasoning
models substantially outperform standard architectures, with up to 13.5%
relative improvement in error detection and 51.0% in sentence extraction; (ii)
cross-lingual evaluation reveals 5-10% performance gaps from English to
Japanese, with smaller disparities for reasoning models; (iii) targeted LoRA
fine-tuning yields asymmetric improvements in error correction performance
(Japanese: +0.078, English: +0.168) while preserving reasoning capabilities;
and (iv) our fine-tuned model exceeds human expert performance on structured
medical error correction tasks. To our knowledge, MedRECT is the first
comprehensive cross-lingual benchmark for medical error correction, providing a
reproducible framework and resources for developing safer medical LLMs across
languages.

</details>


### [15] [G2: Guided Generation for Enhanced Output Diversity in LLMs](https://arxiv.org/abs/2511.00432)
*Zhiwen Ruan,Yixia Li,Yefeng Liu,Yun Chen,Weihua Luo,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: 提出G2方法增强LLM输出多样性，通过双引导机制在解码过程中干预，在保持质量的同时提升多样性


<details>
  <summary>Details</summary>
Motivation: 现有LLM在输出多样性方面存在局限，生成内容高度相似，影响需要多样输出的任务。现有方法如温度调节虽能增强多样性但会牺牲输出质量

Method: G2方法采用基础生成器和双引导机制，通过解码干预引导生成过程，在原始查询条件下鼓励更多样输出，无需训练即插即用

Result: 综合实验表明G2有效提升输出多样性，同时在多样性和质量之间保持最佳平衡

Conclusion: G2方法成功解决了LLM输出多样性不足的问题，在保持生成质量的前提下显著提升了输出多样性

Abstract: Large Language Models (LLMs) have demonstrated exceptional performance across
diverse natural language processing tasks. However, these models exhibit a
critical limitation in output diversity, often generating highly similar
content across multiple attempts. This limitation significantly affects tasks
requiring diverse outputs, from creative writing to reasoning. Existing
solutions, like temperature scaling, enhance diversity by modifying probability
distributions but compromise output quality. We propose Guide-to-Generation
(G2), a training-free plug-and-play method that enhances output diversity while
preserving generation quality. G2 employs a base generator alongside dual
Guides, which guide the generation process through decoding-based interventions
to encourage more diverse outputs conditioned on the original query.
Comprehensive experiments demonstrate that G2 effectively improves output
diversity while maintaining an optimal balance between diversity and quality.

</details>


### [16] [Remembering Unequally: Global and Disciplinary Bias in LLM-Generated Co-Authorship Networks](https://arxiv.org/abs/2511.00476)
*Ghazal Kalhor,Afra Mashhadi*

Main category: cs.CL

TL;DR: 该研究分析了大型语言模型（LLMs）记忆化对合著网络的影响，发现LLMs在生成合著网络时存在系统性偏见，倾向于高被引研究者，但这种偏见在不同学科和地区间存在差异。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在学术搜索和推荐系统中的广泛应用，其记忆化特性可能导致公平性和偏见问题，影响信息生态系统的完整性，特别是在合著网络生成方面。

Method: 评估了DeepSeek R1、Llama 4 Scout和Mixtral 8x7B三个主流模型的记忆化效应，分析记忆化驱动输出在不同学科和世界地区的差异。

Result: 全球分析显示LLMs存在一致偏向高被引研究者的偏见，但临床医学等学科和非洲部分地区表现出更均衡的代表性，表明这些领域的训练数据可能更公平。

Conclusion: 研究强调了在学术发现中部署LLMs既存在风险也蕴含机遇，需要关注记忆化带来的偏见问题。

Abstract: Ongoing breakthroughs in Large Language Models (LLMs) are reshaping search
and recommendation platforms at their core. While this shift unlocks powerful
new scientometric tools, it also exposes critical fairness and bias issues that
could erode the integrity of the information ecosystem. Additionally, as LLMs
become more integrated into web-based searches for scholarly tools, their
ability to generate summarized research work based on memorized data introduces
new dimensions to these challenges. The extent of memorization in LLMs can
impact the accuracy and fairness of the co-authorship networks they produce,
potentially reflecting and amplifying existing biases within the scientific
community and across different regions. This study critically examines the
impact of LLM memorization on the co-authorship networks. To this end, we
assess memorization effects across three prominent models, DeepSeek R1, Llama 4
Scout, and Mixtral 8x7B, analyzing how memorization-driven outputs vary across
academic disciplines and world regions. While our global analysis reveals a
consistent bias favoring highly cited researchers, this pattern is not
uniformly observed. Certain disciplines, such as Clinical Medicine, and
regions, including parts of Africa, show more balanced representation, pointing
to areas where LLM training data may reflect greater equity. These findings
underscore both the risks and opportunities in deploying LLMs for scholarly
discovery.

</details>


### [17] [Leveraging the Cross-Domain & Cross-Linguistic Corpus for Low Resource NMT: A Case Study On Bhili-Hindi-English Parallel Corpus](https://arxiv.org/abs/2511.00486)
*Pooja Singh,Shashwat Bhardwaj,Vaibhav Sharma,Sandeep Kumar*

Main category: cs.CL

TL;DR: 本文构建了首个大规模Bhili-Hindi-English平行语料库BHEPC，包含11万句经过精心整理的句子，并评估了多种多语言大语言模型在Bhili机器翻译任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 印度语言多样性带来了机器翻译挑战，特别是对于Bhili等缺乏高质量语言资源的部落语言，需要填补资源空白。

Method: 创建BHEPC平行语料库，涵盖教育、行政和新闻等关键领域；评估专有和开源多语言大语言模型在双向翻译任务中的表现；研究多语言LLM的生成式翻译能力。

Result: 微调的NLLB-200 distilled 600M变体模型表现最佳，凸显了多语言模型在低资源场景中的潜力。

Conclusion: 这项工作填补了关键资源空白，促进了全球低资源和边缘化语言的包容性自然语言处理技术发展。

Abstract: The linguistic diversity of India poses significant machine translation
challenges, especially for underrepresented tribal languages like Bhili, which
lack high-quality linguistic resources. This paper addresses the gap by
introducing Bhili-Hindi-English Parallel Corpus (BHEPC), the first and largest
parallel corpus worldwide comprising 110,000 meticulously curated sentences
across Bhili, Hindi, and English. The corpus was created with the assistance of
expert human translators. BHEPC spans critical domains such as education,
administration, and news, establishing a valuable benchmark for research in low
resource machine translation. To establish a comprehensive Bhili Machine
Translation benchmark, we evaluated a wide range of proprietary and open-source
Multilingual Large Language Models (MLLMs) on bidirectional translation tasks
between English/Hindi and Bhili. Comprehensive evaluation demonstrates that the
fine-tuned NLLB-200 distilled 600M variant model outperforms others,
highlighting the potential of multilingual models in low resource scenarios.
Furthermore, we investigated the generative translation capabilities of
multilingual LLMs on BHEPC using in-context learning, assessing performance
under cross-domain generalization and quantifying distributional divergence.
This work bridges a critical resource gap and promotes inclusive natural
language processing technologies for low-resource and marginalized languages
globally.

</details>


### [18] [With Privacy, Size Matters: On the Importance of Dataset Size in Differentially Private Text Rewriting](https://arxiv.org/abs/2511.00487)
*Stephen Meisenbacher,Florian Matthes*

Main category: cs.CL

TL;DR: 该研究首次在差分隐私文本隐私化评估中引入数据集大小因素，通过在大规模数据集上进行动态分割测试，发现数据集大小对隐私-效用权衡有重要影响。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私自然语言处理研究在评估文本重写机制时往往忽略数据集大小的影响，需要系统研究数据集规模对机制效用和隐私保护效果的影响。

Method: 设计在大规模数据集上的效用和隐私测试，使用动态分割大小，在包含多达100万文本的不同规模数据集上运行测试，量化数据集大小增加对隐私-效用权衡的影响。

Result: 研究发现数据集大小在评估差分隐私文本重写机制中起着关键作用，数据集规模的变化显著影响隐私保护与数据效用之间的平衡关系。

Conclusion: 研究结果呼吁差分隐私自然语言处理领域需要更严格的评估程序，并为差分隐私自然语言处理在实际应用和大规模部署中的未来发展提供了重要见解。

Abstract: Recent work in Differential Privacy with Natural Language Processing (DP NLP)
has proposed numerous promising techniques in the form of text rewriting
mechanisms. In the evaluation of these mechanisms, an often-ignored aspect is
that of dataset size, or rather, the effect of dataset size on a mechanism's
efficacy for utility and privacy preservation. In this work, we are the first
to introduce this factor in the evaluation of DP text privatization, where we
design utility and privacy tests on large-scale datasets with dynamic split
sizes. We run these tests on datasets of varying size with up to one million
texts, and we focus on quantifying the effect of increasing dataset size on the
privacy-utility trade-off. Our findings reveal that dataset size plays an
integral part in evaluating DP text rewriting mechanisms; additionally, these
findings call for more rigorous evaluation procedures in DP NLP, as well as
shed light on the future of DP NLP in practice and at scale.

</details>


### [19] [ToM: Leveraging Tree-oriented MapReduce for Long-Context Reasoning in Large Language Models](https://arxiv.org/abs/2511.00489)
*Jiani Guo,Zuchao Li,Jie Wu,Qianren Wang,Yun Li,Lefei Zhang,Hai Zhao,Yujiu Yang*

Main category: cs.CL

TL;DR: 提出了ToM框架，一种面向树结构的MapReduce方法，通过层次化语义解析构建文档树，实现自底向上的递归推理，显著提升长文本推理的逻辑连贯性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法如RAG和分治法在处理长文本推理时存在局限性：RAG依赖相似性排序牺牲逻辑连贯性，分治法难以捕捉长距离依赖且可能产生冲突。

Method: 通过层次化语义解析构建文档树(DocTree)，采用树形MapReduce方法：Map步骤在子节点生成推理依据，Reduce步骤在父节点聚合兄弟节点的推理结果以解决冲突或达成共识。

Result: 在70B+大语言模型上的实验结果显示，ToM显著优于现有分治框架和检索增强生成方法，实现了更好的逻辑连贯性和长文本推理能力。

Conclusion: ToM框架通过利用文档的层次结构，有效解决了长文本推理中的逻辑连贯性和长距离依赖问题，为大规模语言模型的长文本处理提供了新思路。

Abstract: Large Language Models (LLMs), constrained by limited context windows, often
face significant performance degradation when reasoning over long contexts. To
address this, Retrieval-Augmented Generation (RAG) retrieves and reasons over
chunks but frequently sacrifices logical coherence due to its reliance on
similarity-based rankings. Similarly, divide-and-conquer frameworks (DCF) split
documents into small chunks for independent reasoning and aggregation. While
effective for local reasoning, DCF struggles to capture long-range dependencies
and risks inducing conflicts by processing chunks in isolation. To overcome
these limitations, we propose ToM, a novel Tree-oriented MapReduce framework
for long-context reasoning. ToM leverages the inherent hierarchical structure
of long documents (e.g., main headings and subheadings) by constructing a
DocTree through hierarchical semantic parsing and performing bottom-up
aggregation. Using a Tree MapReduce approach, ToM enables recursive reasoning:
in the Map step, rationales are generated at child nodes; in the Reduce step,
these rationales are aggregated across sibling nodes to resolve conflicts or
reach consensus at parent nodes. Experimental results on 70B+ LLMs show that
ToM significantly outperforms existing divide-and-conquer frameworks and
retrieval-augmented generation methods, achieving better logical coherence and
long-context reasoning. Our code is available at
https://github.com/gjn12-31/ToM .

</details>


### [20] [Zero-RAG: Towards Retrieval-Augmented Generation with Zero Redundant Knowledge](https://arxiv.org/abs/2511.00505)
*Qi Luo,Xiaonan Li,Junqi Dai,Shuang Cheng,Xipeng Qiu*

Main category: cs.CL

TL;DR: Zero-RAG通过识别和修剪RAG外部语料库中的冗余知识，减少检索开销并提高LLM内部知识利用率，在保持性能的同时加速检索过程。


<details>
  <summary>Details</summary>
Motivation: 随着LLM内部知识的显著扩展，外部语料库与LLM之间存在大量知识冗余，这不仅增加了密集检索的工作量，反而会损害LLM能够自行回答问题的RAG性能。

Method: 提出Mastery-Score指标识别冗余知识进行语料库修剪；使用Query Router和Noise-Tolerant Tuning避免不相关文档干扰，提高LLM对内部知识的利用。

Result: Zero-RAG将维基百科语料库修剪30%，检索阶段加速22%，且不损害RAG性能。

Conclusion: Zero-RAG有效解决了RAG中的知识冗余问题，在保持性能的同时显著提升了效率。

Abstract: Retrieval-Augmented Generation has shown remarkable results to address Large
Language Models' hallucinations, which usually uses a large external corpus to
supplement knowledge to LLMs. However, with the development of LLMs, the
internal knowledge of LLMs has expanded significantly, thus causing significant
knowledge redundancy between the external corpus and LLMs. On the one hand, the
indexing cost of dense retrieval is highly related to the corpus size and thus
significant redundant knowledge intensifies the dense retrieval's workload. On
the other hand, the redundant knowledge in the external corpus is not helpful
to LLMs and our exploratory analysis shows that it instead hurts the RAG
performance on those questions which the LLM can answer by itself. To address
these issues, we propose Zero-RAG to tackle these challenges. Specifically, we
first propose the Mastery-Score metric to identify redundant knowledge in the
RAG corpus to prune it. After pruning, answers to "mastered" questions rely
primarily on internal knowledge of the LLM. To better harness the internal
capacity, we propose Query Router and Noise-Tolerant Tuning to avoid the
irrelevant documents' distraction and thus further improve the LLM's
utilization of internal knowledge with pruned corpus. Experimental results show
that Zero-RAG prunes the Wikipedia corpus by 30\% and accelerates the retrieval
stage by 22\%, without compromising RAG's performance.

</details>


### [21] [Fine-Tuning DialoGPT on Common Diseases in Rural Nepal for Medical Conversations](https://arxiv.org/abs/2511.00514)
*Birat Poudel,Satyam Ghimire,Er. Prakash Chandra Prasad*

Main category: cs.CL

TL;DR: 在尼泊尔农村地区开发离线运行的对话AI医疗助手，通过微调轻量级DialoGPT模型，使其能在无网络环境下提供常见疾病的医疗对话服务。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限地区（如尼泊尔农村）因缺乏互联网连接和云基础设施而无法使用大规模对话模型的问题，为当地提供可离线的医疗对话支持。

Method: 使用合成的医患交互数据集微调轻量级生成对话模型DialoGPT，数据集涵盖尼泊尔农村常见的十种疾病。

Result: 尽管训练数据有限且领域特定，微调后的模型能生成连贯、上下文相关且医学上适当的回应，表现出对症状、疾病背景和共情沟通的理解。

Conclusion: 紧凑型离线对话模型具有良好的适应性，针对性数据集在低资源医疗环境中的领域适应效果显著，为未来农村医疗对话AI提供了有前景的方向。

Abstract: Conversational agents are increasingly being explored to support healthcare
delivery, particularly in resource-constrained settings such as rural Nepal.
Large-scale conversational models typically rely on internet connectivity and
cloud infrastructure, which may not be accessible in rural areas. In this
study, we fine-tuned DialoGPT, a lightweight generative dialogue model that can
operate offline, on a synthetically constructed dataset of doctor-patient
interactions covering ten common diseases prevalent in rural Nepal, including
common cold, seasonal fever, diarrhea, typhoid fever, gastritis, food
poisoning, malaria, dengue fever, tuberculosis, and pneumonia. Despite being
trained on a limited, domain-specific dataset, the fine-tuned model produced
coherent, contextually relevant, and medically appropriate responses,
demonstrating an understanding of symptoms, disease context, and empathetic
communication. These results highlight the adaptability of compact,
offline-capable dialogue models and the effectiveness of targeted datasets for
domain adaptation in low-resource healthcare environments, offering promising
directions for future rural medical conversational AI.

</details>


### [22] [Exploring and Mitigating Gender Bias in Encoder-Based Transformer Models](https://arxiv.org/abs/2511.00519)
*Ariyan Hossain,Khondokar Mohammad Ahanaf Hannan,Rakinul Haque,Nowreen Tarannum Rafa,Humayra Musarrat,Shoaib Ahmed Dipu,Farig Yousuf Sadeque*

Main category: cs.CL

TL;DR: 本文研究了基于Transformer的编码器模型中的性别偏见问题，提出了新的度量标准MALoR来量化偏见，并通过在性别平衡数据集上继续预训练的方法有效降低了偏见。


<details>
  <summary>Details</summary>
Motivation: 编码器Transformer模型在各种语言任务中表现出色，但被发现继承了训练数据中的强烈性别偏见，这促使研究者关注上下文词嵌入中的性别偏见问题。

Method: 使用BERT、ALBERT、RoBERTa和DistilBERT等模型，提出MALoR度量标准评估偏见，并通过反事实数据增强生成性别平衡数据集进行继续预训练来缓解偏见。

Result: 缓解方法显著降低了不同代词对的性别偏见分数，如BERT-base中"he-she"偏见从1.27降至0.08，"his-her"从2.51降至0.36，BERT-large中"male-female"偏见从1.82降至0.10。

Conclusion: 该方法能有效减少性别偏见，且不会影响模型在下游任务上的性能表现。

Abstract: Gender bias in language models has gained increasing attention in the field
of natural language processing. Encoder-based transformer models, which have
achieved state-of-the-art performance in various language tasks, have been
shown to exhibit strong gender biases inherited from their training data. This
paper investigates gender bias in contextualized word embeddings, a crucial
component of transformer-based models. We focus on prominent architectures such
as BERT, ALBERT, RoBERTa, and DistilBERT to examine their vulnerability to
gender bias. To quantify the degree of bias, we introduce a novel metric,
MALoR, which assesses bias based on model probabilities for filling masked
tokens. We further propose a mitigation approach involving continued
pre-training on a gender-balanced dataset generated via Counterfactual Data
Augmentation. Our experiments reveal significant reductions in gender bias
scores across different pronoun pairs. For instance, in BERT-base, bias scores
for "he-she" dropped from 1.27 to 0.08, and "his-her" from 2.51 to 0.36
following our mitigation approach. We also observed similar improvements across
other models, with "male-female" bias decreasing from 1.82 to 0.10 in
BERT-large. Our approach effectively reduces gender bias without compromising
model performance on downstream tasks.

</details>


### [23] [Word Salad Chopper: Reasoning Models Waste A Ton Of Decoding Budget On Useless Repetitions, Self-Knowingly](https://arxiv.org/abs/2511.00536)
*Wenya Xie,Shaochen,Zhong,Hoang Anh Duy Le,Zhaozhuo Xu,Jianwen Xie,Zirui Liu*

Main category: cs.CL

TL;DR: 提出WordSaladChopper(WSC)组件，通过检测大型推理模型中的无意义自我重复（word salad）来减少输出token成本，仅移除语义冗余token而不影响推理轨迹。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型输出token成本高昂，其中很大部分是无用的自我重复token，这些token消耗解码预算但不增加价值。

Method: 利用模型对<\n\n>标记的隐藏状态模式，通过单层线性分类器实时检测word salad行为，检测后通过简单截断和再生提示实现长度节省。

Result: WSC组件实现了显著的长度节省，且质量损失最小，是一个轻量级、即插即用的解决方案。

Conclusion: WSC或类似组件是所有考虑用户体验的大型推理模型应用必备的组件，因其低开销、强节省效果且word salad token缺乏语义价值。

Abstract: Large Reasoning Models (LRMs) are often bottlenecked by the high cost of
output tokens. We show that a significant portion of these tokens are useless
self-repetitions - what we call "word salad" - that exhaust the decoding budget
without adding value. Interestingly, we observe that LRMs are self-aware when
trapped in these loops: the hidden states of <\n\n> tokens trailing each
reasoning chunk exhibit patterns that allow us to detect word salad behavior
on-the-fly via a single-layer linear classifier. Once detected, a simple chop
appended by a straightforward regeneration prompt yields substantial length
savings with minimal quality loss. Our work offers WordSaladChopper (WSC) - a
lightweight, turnkey component for LRM that is minimally invasive to its
reasoning trajectory by only removing semantically redundant tokens. Given its
low overhead, strong savings, and the lack of semantic value of word salad
tokens, we believe it is not too far-fetched to argue that WSC - or a similar
component - is a must-have for all LRM applications with user experience in
mind. Our code is publicly available at
https://github.com/wenyaxie023/WordSaladChopper.

</details>


### [24] [Multi-refined Feature Enhanced Sentiment Analysis Using Contextual Instruction](https://arxiv.org/abs/2511.00537)
*Peter Atandoh,Jie Zou,Weikang Guo,Jiwei Wei,Zheng Wang*

Main category: cs.CL

TL;DR: 提出CISEA-MRFE框架，通过上下文指令、语义增强增强和多细化特征提取，显著提升情感分析在复杂场景下的性能


<details>
  <summary>Details</summary>
Motivation: 现有深度学习和预训练语言模型在细微情感线索、领域转移和不平衡情感分布场景下表现不佳，存在语义基础不足、泛化能力差和偏向主导情感类别的问题

Method: CISEA-MRFE框架整合三个组件：上下文指令(CI)用于情感消歧指导，语义增强增强(SEA)通过情感一致性增强提升鲁棒性，多细化特征提取(MRFE)结合尺度自适应深度编码器(SADE)和多尺度特征专业化以及情感评估上下文编码器(EECE)进行情感感知序列建模

Result: 在四个基准数据集上实验表明，CISEA-MRFE显著优于强基线模型，在IMDb、Yelp、Twitter和Amazon数据集上准确率分别相对提升4.6%、6.5%、30.3%和4.1%

Conclusion: 该方法在跨领域情感分类中展现出有效性和良好的泛化能力

Abstract: Sentiment analysis using deep learning and pre-trained language models (PLMs)
has gained significant traction due to their ability to capture rich contextual
representations. However, existing approaches often underperform in scenarios
involving nuanced emotional cues, domain shifts, and imbalanced sentiment
distributions. We argue that these limitations stem from inadequate semantic
grounding, poor generalization to diverse linguistic patterns, and biases
toward dominant sentiment classes. To overcome these challenges, we propose
CISEA-MRFE, a novel PLM-based framework integrating Contextual Instruction
(CI), Semantic Enhancement Augmentation (SEA), and Multi-Refined Feature
Extraction (MRFE). CI injects domain-aware directives to guide sentiment
disambiguation; SEA improves robustness through sentiment-consistent
paraphrastic augmentation; and MRFE combines a Scale-Adaptive Depthwise Encoder
(SADE) for multi-scale feature specialization with an Emotion Evaluator Context
Encoder (EECE) for affect-aware sequence modeling. Experimental results on four
benchmark datasets demonstrate that CISEA-MRFE consistently outperforms strong
baselines, achieving relative improvements in accuracy of up to 4.6% on IMDb,
6.5% on Yelp, 30.3% on Twitter, and 4.1% on Amazon. These results validate the
effectiveness and generalization ability of our approach for sentiment
classification across varied domains.

</details>


### [25] [Friend or Foe: How LLMs' Safety Mind Gets Fooled by Intent Shift Attack](https://arxiv.org/abs/2511.00556)
*Peng Ding,Jun Kuang,Wen Sun,Zongyu Wang,Xuezhi Cao,Xunliang Cai,Jiajun Chen,Shujian Huang*

Main category: cs.CL

TL;DR: 提出ISA攻击方法，通过意图转换使LLMs将有害请求误解为良性信息请求，相比直接有害提示攻击成功率提升70%以上，现有防御方法对其无效。


<details>
  <summary>Details</summary>
Motivation: 现有越狱攻击主要通过引入额外上下文或对抗性token来分散LLMs注意力，但未改变核心有害意图。需要研究更隐蔽的攻击方式以增强LLMs安全机制。

Method: 建立意图转换分类法，通过最小化编辑原始请求生成自然、人类可读且看似无害的提示，使LLMs误认为良性信息请求。

Result: 在开源和商业LLMs上，ISA攻击成功率相比直接有害提示提升超过70%。仅使用ISA模板重新表述的良性数据微调模型，攻击成功率接近100%。现有防御方法对ISA无效。

Conclusion: LLMs在意图推断方面存在根本性挑战，需要更有效的防御策略来应对ISA这类隐蔽攻击。

Abstract: Large language models (LLMs) remain vulnerable to jailbreaking attacks
despite their impressive capabilities. Investigating these weaknesses is
crucial for robust safety mechanisms. Existing attacks primarily distract LLMs
by introducing additional context or adversarial tokens, leaving the core
harmful intent unchanged. In this paper, we introduce ISA (Intent Shift
Attack), which obfuscates LLMs about the intent of the attacks. More
specifically, we establish a taxonomy of intent transformations and leverage
them to generate attacks that may be misperceived by LLMs as benign requests
for information. Unlike prior methods relying on complex tokens or lengthy
context, our approach only needs minimal edits to the original request, and
yields natural, human-readable, and seemingly harmless prompts. Extensive
experiments on both open-source and commercial LLMs show that ISA achieves over
70% improvement in attack success rate compared to direct harmful prompts. More
critically, fine-tuning models on only benign data reformulated with ISA
templates elevates success rates to nearly 100%. For defense, we evaluate
existing methods and demonstrate their inadequacy against ISA, while exploring
both training-free and training-based mitigation strategies. Our findings
reveal fundamental challenges in intent inference for LLMs safety and
underscore the need for more effective defenses. Our code and datasets are
available at https://github.com/NJUNLP/ISA.

</details>


### [26] [FlashEVA: Accelerating LLM inference via Efficient Attention](https://arxiv.org/abs/2511.00576)
*Juan Gabriel Kostelec,Qinghai Guo*

Main category: cs.CL

TL;DR: FlashEVA是一种高效的EVA注意力实现，通过微调Transformer模型适应FlashEVA注意力，在仅使用15亿token的情况下保持下游任务效果，推理时吞吐量提升6.7倍，GPU内存使用降低5倍。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在自然语言处理中表现出色，但其内存需求（特别是需要维护完整上下文）对推理构成重大挑战。

Method: 提出FlashEVA（EVA注意力的高效实现），展示如何微调Transformer模型以适应FlashEVA注意力机制。

Result: FlashEVA在推理时实现高达6.7倍的吞吐量提升和5倍的GPU内存使用降低，但在检索任务中存在局限性。

Conclusion: 这项工作代表了向更高效和适应性更强的基于Transformer的推理模型迈出的重要一步，通过可调节超参数在吞吐量和准确性之间提供权衡控制。

Abstract: Transformer models have revolutionized natural language processing, achieving
state-of-the-art performance and demonstrating remarkable scalability. However,
their memory demands, particularly due to maintaining full context in memory,
pose significant challenges for inference. In this paper, we present FlashEVA,
an efficient implementation of EVA (Efficient Attention via Control Variates),
and demonstrate how to finetune transformers to adapt to FlashEVA attention.
Our method enables fine-tuning of Transformer models with as few as 1.5B tokens
while preserving effectiveness across various downstream tasks. Notably,
FlashEVA achieves up to 6.7x higher throughput and 5x lower peak GPU memory
usage during inference compared to standard Transformer implementations.
Despite these improvements, we observe limitations in retrieval-focused tasks.
Our implementation offers control over the trade-off between throughput and
accuracy through adjustable hyperparameters, providing flexibility for diverse
use cases. This work represents a significant step towards more efficient and
adaptable Transformer-based models for inference.

</details>


### [27] [OpenSIR: Open-Ended Self-Improving Reasoner](https://arxiv.org/abs/2511.00602)
*Wai-Chung Kwan,Joshua Ong Jun Leang,Pavlos Vougiouklis,Jeff Z. Pan,Marco Valentino,Pasquale Minervini*

Main category: cs.CL

TL;DR: OpenSIR是一个无需外部监督的自学习框架，通过让LLM交替扮演教师和学生角色来生成和解决新颖问题，实现了开放式的数学发现能力提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于标注数据的强化学习方法限制了模型超越人类水平的能力，而自学习方法要么依赖外部验证器，要么无法实现开放式学习。

Method: OpenSIR采用自博弈框架，通过优化问题的难度和多样性来生成新颖问题，让LLM在教师和学生角色间交替学习，无需外部监督。

Result: 从单一简单种子问题开始，Llama-3.2-3B-Instruct在GSM8K上从73.9提升至78.3，在College Math上从28.8提升至34.4；Gemma-2-2B-Instruct在GSM8K上从38.5提升至58.7。

Conclusion: OpenSIR通过协同进化的师生角色实现了开放式学习，能够自适应地校准难度并驱动多样化探索，从基础数学自主发展到高级数学。

Abstract: Recent advances in large language model (LLM) reasoning through reinforcement
learning rely on annotated datasets for verifiable rewards, which may limit
models' ability to surpass human-level performance. While self-play offers a
promising alternative, existing approaches depend on external verifiers or
cannot learn open-endedly. We present Open-Ended Self-Improving Reasoner
(OpenSIR), a self-play framework where an LLM learns to generate and solve
novel problems by alternating teacher and student roles without external
supervision. To generate novel problems, OpenSIR optimises for both difficulty
and diversity, rewarding problems that challenge appropriately while exploring
distinct concepts, enabling open-ended mathematical discovery. Starting from a
single trivial seed problem, OpenSIR substantially improves instruction models:
Llama-3.2-3B-Instruct advances from 73.9 to 78.3 on GSM8K, and from 28.8 to
34.4 on College Math, while Gemma-2-2B-Instruct rises from 38.5 to 58.7 on
GSM8K. Our analyses reveal that OpenSIR achieves open-ended learning through
co-evolving teacher-student roles that adaptively calibrate difficulty and
drive diverse exploration, progressing autonomously from basic to advanced
mathematics.

</details>


### [28] [SpecDiff-2: Scaling Diffusion Drafter Alignment For Faster Speculative Decoding](https://arxiv.org/abs/2511.00606)
*Jameson Sandler,Jacob K. Christopher,Thomas Hartvigsen,Nando Fioretto*

Main category: cs.CL

TL;DR: SpecDiff-2是一种新的推测解码框架，通过离散扩散作为非自回归草稿生成器解决并行性限制，并开发新技术校准草稿与验证模型，显著提升LLM推理速度。


<details>
  <summary>Details</summary>
Motivation: 当前推测解码方法存在两个瓶颈：草稿生成时的自回归依赖限制并行性，以及草稿与验证模型不匹配导致的频繁拒绝。

Method: 使用离散扩散作为非自回归草稿生成器解决并行性问题，开发新技术校准离散扩散草稿器与自回归验证器。

Result: 在推理、编码和数学基准测试中达到最新水平，平均token每秒提升55%，相比标准解码获得最高5.5倍加速，且无精度损失。

Conclusion: SpecDiff-2通过联合解决推测解码的两个关键瓶颈，实现了显著的推理加速效果。

Abstract: Speculative decoding has become the standard approach for accelerating Large
Language Model (LLM) inference. It exploits a lossless draft-then-verify
procedure to circumvent the latency of autoregressive decoding, achieving
impressive speed-ups. Yet, current speculative decoding approaches remain
limited by two fundamental bottlenecks: (1) the autoregressive dependency
during drafting which limits parallelism, and (2) frequent rejections of draft
tokens caused by misalignment between the draft and verify models. This paper
proposes SpecDiff-2, a novel framework to jointly address these two
bottlenecks. It leverages discrete diffusion as a non-autoregressive drafter to
address bottleneck (1) and develops novel techniques to calibrate discrete
diffusion drafters with autoregressive verifiers, addressing bottleneck (2).
Experimental results across a comprehensive benchmark suite show that
SpecDiff-2 achieves a new state-of-the-art across reasoning, coding, and
mathematical benchmarks, improving tokens-per-second by up to an average of
+55% over previous baselines and obtaining up to 5.5x average speed-up over
standard decoding, without any loss of accuracy.

</details>


### [29] [Certain but not Probable? Differentiating Certainty from Probability in LLM Token Outputs for Probabilistic Scenarios](https://arxiv.org/abs/2511.00620)
*Autumn Toney-Wails,Ryan Wails*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型在概率场景中的不确定性量化问题，发现虽然模型在响应准确性上表现完美，但其token级别的概率分布与理论概率分布存在系统性偏差。


<details>
  <summary>Details</summary>
Motivation: 可靠的不确定性量化对于LLM在决策支持等关键应用中的可信部署至关重要，特别是在概率场景中需要模型输出概率与理论概率保持一致。

Method: 使用GPT-4.1和DeepSeek-Chat模型，评估对10个涉及概率提示的响应，测量响应有效性和token级输出概率与理论概率的对齐度。

Result: 两个模型在所有提示场景中都实现了完美的领域内响应准确率，但其token级概率和熵值始终偏离相应的理论分布。

Conclusion: 当前LLM在概率场景中的不确定性量化存在局限性，token级别的概率估计不能可靠地反映理论概率分布。

Abstract: Reliable uncertainty quantification (UQ) is essential for ensuring
trustworthy downstream use of large language models, especially when they are
deployed in decision-support and other knowledge-intensive applications. Model
certainty can be estimated from token logits, with derived probability and
entropy values offering insight into performance on the prompt task. However,
this approach may be inadequate for probabilistic scenarios, where the
probabilities of token outputs are expected to align with the theoretical
probabilities of the possible outcomes. We investigate the relationship between
token certainty and alignment with theoretical probability distributions in
well-defined probabilistic scenarios. Using GPT-4.1 and DeepSeek-Chat, we
evaluate model responses to ten prompts involving probability (e.g., roll a
six-sided die), both with and without explicit probability cues in the prompt
(e.g., roll a fair six-sided die). We measure two dimensions: (1) response
validity with respect to scenario constraints, and (2) alignment between
token-level output probabilities and theoretical probabilities. Our results
indicate that, while both models achieve perfect in-domain response accuracy
across all prompt scenarios, their token-level probability and entropy values
consistently diverge from the corresponding theoretical distributions.

</details>


### [30] [Modeling the Construction of a Literary Archetype: The Case of the Detective Figure in French Literature](https://arxiv.org/abs/2511.00627)
*Jean Barré,Olga Seminck,Antoine Bourgois,Thierry Poibeau*

Main category: cs.CL

TL;DR: 通过计算分析研究法国侦探小说中侦探原型的演变，发现监督模型能够捕捉150年间侦探原型的统一性，并展示其从次要角色到核心推理机器的演变过程。


<details>
  <summary>Details</summary>
Motivation: 探索法国侦探小说中侦探原型在150年间的演变轨迹，理解该文学类型中核心角色的发展变化。

Method: 使用定量方法和角色级嵌入的计算分析，构建监督模型来分析从1866年到2017年的法国侦探小说。

Result: 模型成功捕捉了侦探原型的统一性，展示了侦探从次要叙事角色演变为古典侦探故事核心推理机器的过程，以及在二战后受硬汉派影响变得更加复杂。

Conclusion: 法国侦探小说中的侦探原型经历了从单一到复杂的演变，反映了文学类型和社会背景的变化，计算分析方法有效揭示了这一演变模式。

Abstract: This research explores the evolution of the detective archetype in French
detective fiction through computational analysis. Using quantitative methods
and character-level embeddings, we show that a supervised model is able to
capture the unity of the detective archetype across 150 years of literature,
from M. Lecoq (1866) to Commissaire Adamsberg (2017). Building on this finding,
the study demonstrates how the detective figure evolves from a secondary
narrative role to become the central character and the "reasoning machine" of
the classical detective story. In the aftermath of the Second World War, with
the importation of the hardboiled tradition into France, the archetype becomes
more complex, navigating the genre's turn toward social violence and moral
ambiguity.

</details>


### [31] [Do You Know About My Nation? Investigating Multilingual Language Models' Cultural Literacy Through Factual Knowledge](https://arxiv.org/abs/2511.00657)
*Eshaan Tanwar,Anwoy Chatterjee,Michael Saxon,Alon Albalak,William Yang Wang,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: XNationQA是一个多语言问答基准，涵盖9个国家的地理、文化和历史问题，用于评估多语言LLM的文化素养。研究发现模型在不同语言间获取文化特定信息存在显著差异，且知识跨语言迁移能力有限。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数多语言问答基准虽然覆盖多种语言，但未能考虑信息的区域多样性，往往以西方为中心，这导致无法公平评估多语言模型对不同地理位置事实信息的理解能力。

Method: 构建XNationQA基准，包含49,280个关于9个国家地理、文化和历史的问题，涵盖7种语言。对8个标准多语言LLM进行基准测试，并使用两种新的传递性指标进行评估。

Result: 模型在不同语言间获取文化特定信息存在显著差异；模型在英语中对文化信息的了解往往优于相应文化的主导语言；模型在西方语言中表现更好，但这并不一定意味着对西方国家更了解；开源模型在跨语言知识迁移方面能力非常有限。

Conclusion: 多语言LLM在文化素养方面存在显著差距，特别是在跨语言知识迁移方面表现不佳，需要开发更公平和全面的评估方法来改善模型对多样化文化信息的理解能力。

Abstract: Most multilingual question-answering benchmarks, while covering a diverse
pool of languages, do not factor in regional diversity in the information they
capture and tend to be Western-centric. This introduces a significant gap in
fairly evaluating multilingual models' comprehension of factual information
from diverse geographical locations. To address this, we introduce XNationQA
for investigating the cultural literacy of multilingual LLMs. XNationQA
encompasses a total of 49,280 questions on the geography, culture, and history
of nine countries, presented in seven languages. We benchmark eight standard
multilingual LLMs on XNationQA and evaluate them using two novel transference
metrics. Our analyses uncover a considerable discrepancy in the models'
accessibility to culturally specific facts across languages. Notably, we often
find that a model demonstrates greater knowledge of cultural information in
English than in the dominant language of the respective culture. The models
exhibit better performance in Western languages, although this does not
necessarily translate to being more literate for Western countries, which is
counterintuitive. Furthermore, we observe that models have a very limited
ability to transfer knowledge across languages, particularly evident in
open-source models.

</details>


### [32] [Do Methods to Jailbreak and Defend LLMs Generalize Across Languages?](https://arxiv.org/abs/2511.00689)
*Berk Atil,Rebecca J. Passonneau,Fred Morstatter*

Main category: cs.CL

TL;DR: 该论文首次对10种不同资源水平的语言进行了系统的多语言越狱攻击和防御评估，发现攻击成功率和防御鲁棒性在不同语言间存在差异，高资源语言在标准查询下更安全但对对抗性攻击更脆弱。


<details>
  <summary>Details</summary>
Motivation: 虽然存在许多越狱攻击和防御方法，但它们在跨语言泛化方面的表现仍未得到充分探索，需要系统评估不同语言环境下的LLM安全性。

Method: 在HarmBench和AdvBench上使用6个LLM，评估了基于逻辑表达式和对抗性提示的两种越狱攻击类型，涵盖10种高、中、低资源语言。

Result: 攻击成功率和防御鲁棒性因语言而异：高资源语言在标准查询下更安全，但对对抗性攻击更脆弱；简单防御方法有效但依赖具体语言和模型。

Conclusion: 研究结果表明需要为LLM开发语言感知和跨语言的安全基准测试，以更好地评估和提升多语言环境下的模型安全性。

Abstract: Large language models (LLMs) undergo safety alignment after training and
tuning, yet recent work shows that safety can be bypassed through jailbreak
attacks. While many jailbreaks and defenses exist, their cross-lingual
generalization remains underexplored. This paper presents the first systematic
multilingual evaluation of jailbreaks and defenses across ten
languages--spanning high-, medium-, and low-resource languages--using six LLMs
on HarmBench and AdvBench. We assess two jailbreak types:
logical-expression-based and adversarial-prompt-based. For both types, attack
success and defense robustness vary across languages: high-resource languages
are safer under standard queries but more vulnerable to adversarial ones.
Simple defenses can be effective, but are language- and model-dependent. These
findings call for language-aware and cross-lingual safety benchmarks for LLMs.

</details>


### [33] [Optimizing Native Sparse Attention with Latent Attention and Local Global Alternating Strategies](https://arxiv.org/abs/2511.00819)
*Yuxuan Hu,Jianchao Tan,Jiaqi Zhang,Wen Zan,Pingwei Sun,Yifan Lu,Yerui Sun,Yuchen Xie,Xunliang Cai,Jing Zhang*

Main category: cs.CL

TL;DR: 提出改进的Native Sparse Attention方法，通过交替使用局部和全局注意力模式，结合潜在注意力机制，在减少KV缓存的同时提升长上下文建模能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有稀疏注意力方法在长序列建模中的局限性，通过更有效的注意力模式组合来提升长距离依赖传播效率。

Method: 采用层间交替的局部（滑动窗口）和全局（压缩、选择性）注意力模式，并使用多头潜在注意力（MLA）和分组头潜在注意力（GLA）来增强各分支。

Result: 相比NSA减少50% KV缓存内存，在340M到1.3B参数模型上，在常识推理和长文本理解任务中达到或超过全注意力和原生稀疏注意力的性能。

Conclusion: 交替注意力模式和潜在注意力机制能有效提升稀疏注意力在长上下文建模中的性能，同时显著降低内存开销。

Abstract: In this work, we conduct a systematic analysis of Native Sparse Attention
(NSA) and propose targeted improvements that enhance long-context modeling. A
key insight is that alternating between local (sliding-window) and global
(compression, selective) attention across layers, rather than using fixed
patterns, enables more effective propagation of long-range dependencies and
substantially boosts performance on long-sequence tasks. Meanwhile, we further
refine NSA's branches with Latent Attention that the sliding-window branch is
enhanced with Multi-head Latent Attention (MLA) while compression and selective
branches adopt Group-head Latent Attention (GLA). These changes reduce KV-cache
memory by 50\% versus NSA while improving the model's common-sense reasoning
and long-text understanding capabilities. Experiments on models from 340M to
1.3B parameters (trained on 15B and 100B tokens) show our method matches or
exceeds full attention and native sparse attention in both common-sense
reasoning and long-context understanding tasks.

</details>


### [34] [TriCon-Fair: Triplet Contrastive Learning for Mitigating Social Bias in Pre-trained Language Models](https://arxiv.org/abs/2511.00854)
*Chong Lyu,Lin Li,Shiqing Wu,Jingling Yuan*

Main category: cs.CL

TL;DR: TriCon-Fair是一个对比学习框架，通过解耦三元组和语言建模损失来消除社会偏见，避免偏见样本和无偏见样本之间的负正耦合问题。


<details>
  <summary>Details</summary>
Motivation: 现有去偏见方法独立处理偏见和无偏见样本，忽略了它们之间的相互关系，导致改进一个群体时无意中损害另一个群体，使残余社会偏见持续存在。

Method: TriCon-Fair采用解耦损失函数，结合三元组和语言建模项，为每个锚点分配明确的偏见负样本和无偏见正样本，解耦推拉动态并避免正负耦合，同时联合优化语言建模目标以保持通用能力。

Result: 实验结果表明，TriCon-Fair在保持强大下游性能的同时，比现有去偏见基线方法更能减少歧视性输出。

Conclusion: TriCon-Fair为敏感NLP应用提供了一个实用且符合伦理的解决方案。

Abstract: The increasing utilization of large language models raises significant
concerns about the propagation of social biases, which may result in harmful
and unfair outcomes. However, existing debiasing methods treat the biased and
unbiased samples independently, thus ignoring their mutual relationship. This
oversight enables a hidden negative-positive coupling, where improvements for
one group inadvertently compromise the other, allowing residual social bias to
persist. In this paper, we introduce TriCon-Fair, a contrastive learning
framework that employs a decoupled loss that combines triplet and language
modeling terms to eliminate positive-negative coupling. Our TriCon-Fair assigns
each anchor an explicitly biased negative and an unbiased positive, decoupling
the push-pull dynamics and avoiding positive-negative coupling, and jointly
optimizes a language modeling (LM) objective to preserve general capability.
Experimental results demonstrate that TriCon-Fair reduces discriminatory output
beyond existing debiasing baselines while maintaining strong downstream
performance. This suggests that our proposed TriCon-Fair offers a practical and
ethical solution for sensitive NLP applications.

</details>


### [35] [Assessing LLM Reasoning Steps via Principal Knowledge Grounding](https://arxiv.org/abs/2511.00879)
*Hyeon Hwang,Yewon Cho,Chanwoong Yoon,Yein Park,Minju Song,Kyungjae Lee,Gangwoo Kim,Jaewoo Kang*

Main category: cs.CL

TL;DR: 提出了一个评估LLM推理过程中知识基础的新框架，包含大规模知识库、知识基础评估指标和轻量级评估器LLM，能有效识别推理中的知识缺失或误用问题。


<details>
  <summary>Details</summary>
Motivation: 解决如何验证LLM推理是否准确基于知识的问题，因为逐步推理已成为LLM处理复杂任务的标准方法。

Method: 构建包含三个关键组件的评估框架：大规模原子知识库、知识基础评估指标（衡量知识回忆和应用能力）、以及轻量级评估器LLM用于指标计算。

Result: 评估套件在识别缺失或误用知识元素方面表现出显著效果，为揭示LLM基本推理缺陷提供了关键见解。

Conclusion: 该知识基础评估框架不仅可用于评估，还可集成到偏好优化中，展示了知识基础评估的进一步应用潜力。

Abstract: Step-by-step reasoning has become a standard approach for large language
models (LLMs) to tackle complex tasks. While this paradigm has proven
effective, it raises a fundamental question: How can we verify that an LLM's
reasoning is accurately grounded in knowledge? To address this question, we
introduce a novel evaluation suite that systematically assesses the knowledge
grounding of intermediate reasoning. Our framework comprises three key
components. (1) Principal Knowledge Collection, a large-scale repository of
atomic knowledge essential for reasoning. Based on the collection, we propose
(2) knowledge-grounded evaluation metrics designed to measure how well models
recall and apply prerequisite knowledge in reasoning. These metrics are
computed by our (3) evaluator LLM, a lightweight model optimized for
cost-effective and reliable metric computation. Our evaluation suite
demonstrates remarkable effectiveness in identifying missing or misapplied
knowledge elements, providing crucial insights for uncovering fundamental
reasoning deficiencies in LLMs. Beyond evaluation, we demonstrate how these
metrics can be integrated into preference optimization, showcasing further
applications of knowledge-grounded evaluation.

</details>


### [36] [ColMate: Contrastive Late Interaction and Masked Text for Multimodal Document Retrieval](https://arxiv.org/abs/2511.00903)
*Ahmed Masry,Megh Thakkar,Patrice Bechard,Sathwik Tejaswi Madhusudhan,Rabiul Awal,Shambhavi Mishra,Akshay Kalkunte Suresh,Srivatsava Daruru,Enamul Hoque,Spandana Gella,Torsten Scholak,Sai Rajeswar*

Main category: cs.CL

TL;DR: ColMate是一个多模态文档检索模型，通过OCR预训练目标、自监督掩码对比学习和延迟交互评分机制，在ViDoRe V2基准测试中比现有检索模型提升3.61%。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态文档检索方法往往复制纯文本检索技术，没有充分考虑多模态文档的结构和视觉特征，存在局限性。

Method: 使用OCR预训练目标、自监督掩码对比学习目标和延迟交互评分机制，更好地适应多模态文档结构和视觉特征。

Result: 在ViDoRe V2基准测试中比现有检索模型提升3.61%，在域外基准测试中表现出更强的泛化能力。

Conclusion: ColMate通过专门针对多模态文档设计的训练目标和评分机制，有效提升了文档检索性能。

Abstract: Retrieval-augmented generation has proven practical when models require
specialized knowledge or access to the latest data. However, existing methods
for multimodal document retrieval often replicate techniques developed for
text-only retrieval, whether in how they encode documents, define training
objectives, or compute similarity scores. To address these limitations, we
present ColMate, a document retrieval model that bridges the gap between
multimodal representation learning and document retrieval. ColMate utilizes a
novel OCR-based pretraining objective, a self-supervised masked contrastive
learning objective, and a late interaction scoring mechanism more relevant to
multimodal document structures and visual characteristics. ColMate obtains
3.61% improvements over existing retrieval models on the ViDoRe V2 benchmark,
demonstrating stronger generalization to out-of-domain benchmarks.

</details>


### [37] [The Biased Oracle: Assessing LLMs' Understandability and Empathy in Medical Diagnoses](https://arxiv.org/abs/2511.00924)
*Jianzhou Yao,Shunchang Liu,Guillaume Drui,Rikard Pettersson,Alessandro Blasimme,Sara Kijewski*

Main category: cs.CL

TL;DR: 评估大语言模型在医疗诊断沟通中的表现，发现虽然能根据患者特征调整解释，但存在内容过于复杂和情感偏见问题，导致可及性和支持不平等。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在医疗诊断沟通中生成既易懂又富有同理心的解释和指导的能力，以支持临床医生与患者的沟通。

Method: 在两个领先的LLMs上评估医疗诊断场景，使用可读性指标评估易懂性，通过LLM-as-a-Judge评分与人工评估比较来评估同理心。

Result: LLMs能够根据社会人口学变量和患者状况调整解释，但生成的内容过于复杂，并表现出有偏见的情感同理心，导致可及性和支持不平等。

Conclusion: 需要系统校准以确保公平的患者沟通。

Abstract: Large language models (LLMs) show promise for supporting clinicians in
diagnostic communication by generating explanations and guidance for patients.
Yet their ability to produce outputs that are both understandable and
empathetic remains uncertain. We evaluate two leading LLMs on medical
diagnostic scenarios, assessing understandability using readability metrics as
a proxy and empathy through LLM-as-a-Judge ratings compared to human
evaluations. The results indicate that LLMs adapt explanations to
socio-demographic variables and patient conditions. However, they also generate
overly complex content and display biased affective empathy, leading to uneven
accessibility and support. These patterns underscore the need for systematic
calibration to ensure equitable patient communication. The code and data are
released: https://github.com/Jeffateth/Biased_Oracle

</details>


### [38] [The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in Multilingual LLMs using Indian Riddles](https://arxiv.org/abs/2511.00960)
*Abhinav P M,Ojasva Saxena,Oswald C,Parameswari Krishnamurthy*

Main category: cs.CL

TL;DR: 评估大型语言模型在七种印度语言中的文化推理能力，发现模型初始准确率与自我纠错能力呈负相关，性能最好的模型反而最过度自信。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在非英语语言（特别是印度语言）中进行文化基础推理的能力，以及模型的自我评估能力。

Method: 创建多语言谜语数据集，包含传统谜语和上下文重构变体，评估五个LLM在七种印度语言中的表现，采用七种提示策略，分两阶段评估：谜语解决能力和自我评估一致性。

Result: Gemini 2.5 Pro整体表现最佳，但少样本方法提升有限，准确率在不同语言间差异显著。关键发现：模型初始准确率与识别自身错误能力呈负相关，高准确率模型过度自信（4.34%真负率），低准确率模型更自知（42.09%真负率）。

Conclusion: 多语言推理存在明显差距，需要开发既能有效推理又能识别自身局限的模型。

Abstract: The extent to which large language models (LLMs) can perform culturally
grounded reasoning across non-English languages remains underexplored. This
paper examines the reasoning and self-assessment abilities of LLMs across seven
major Indian languages-Bengali, Gujarati, Hindi, Kannada, Malayalam, Tamil, and
Telugu. We introduce a multilingual riddle dataset combining traditional
riddles with context-reconstructed variants and evaluate five LLMs-Gemini 2.5
Pro, Gemini 2.5 Flash, Mistral-Saba, LLaMA 4 Scout, and LLaMA 4 Maverick-under
seven prompting strategies. In the first stage, we assess riddle-solving
performance and find that while Gemini 2.5 Pro performs best overall, few-shot
methods yield only marginal gains, and accuracy varies notably across
languages. In the second stage, we conduct a self-evaluation experiment to
measure reasoning consistency. The results reveal a key finding: a model's
initial accuracy is inversely correlated with its ability to identify its own
mistakes. Top-performing models such as Gemini 2.5 Pro are overconfident (4.34%
True Negative Rate), whereas lower-performing models like LLaMA 4 Scout are
substantially more self-aware (42.09% True Negative Rate). These results point
to clear gaps in multilingual reasoning and highlight the need for models that
not only reason effectively but also recognize their own limitations.

</details>


### [39] [Advancing Machine-Generated Text Detection from an Easy to Hard Supervision Perspective](https://arxiv.org/abs/2511.00988)
*Chenwang Wu,Yiu-ming Cheung,Bo Han,Defu Lian*

Main category: cs.CL

TL;DR: 提出了一种易到难的增强框架来解决机器生成文本检测中的边界模糊问题，通过使用针对长文本的简单监督器来增强目标检测器，在多种实际场景中显著提升了检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有机器生成文本检测方法假设标签是"黄金标准"，但实际存在边界模糊问题，且人类认知局限和检测器超智能使得不精确学习广泛存在且不可避免。

Method: 提出易到难增强框架：使用针对较长文本的简单监督器（尽管能力较弱）来增强更具挑战性的目标检测器。通过将检测器结构性地融入监督器，将监督器建模为检测器的性能下界。

Result: 在跨LLM、跨领域、混合文本和改写攻击等多种实际场景中的广泛实验表明，该框架具有显著的检测有效性。

Conclusion: 该框架为不精确条件下的机器生成文本检测提供了可靠监督，能够有效逼近潜在的"黄金"标签。

Abstract: Existing machine-generated text (MGT) detection methods implicitly assume
labels as the "golden standard". However, we reveal boundary ambiguity in MGT
detection, implying that traditional training paradigms are inexact. Moreover,
limitations of human cognition and the superintelligence of detectors make
inexact learning widespread and inevitable. To this end, we propose an
easy-to-hard enhancement framework to provide reliable supervision under such
inexact conditions. Distinct from knowledge distillation, our framework employs
an easy supervisor targeting relatively simple longer-text detection tasks
(despite weaker capabilities), to enhance the more challenging target detector.
Firstly, longer texts targeted by supervisors theoretically alleviate the
impact of inexact labels, laying the foundation for reliable supervision.
Secondly, by structurally incorporating the detector into the supervisor, we
theoretically model the supervisor as a lower performance bound for the
detector. Thus, optimizing the supervisor indirectly optimizes the detector,
ultimately approximating the underlying "golden" labels. Extensive experiments
across diverse practical scenarios, including cross-LLM, cross-domain, mixed
text, and paraphrase attacks, demonstrate the framework's significant detection
effectiveness. The code is available at:
https://github.com/tmlr-group/Easy2Hard.

</details>


### [40] [MARS-SQL: A multi-agent reinforcement learning framework for Text-to-SQL](https://arxiv.org/abs/2511.01008)
*Haolin Yang,Jipeng Zhang,Zhitao He,Yi R. Fung*

Main category: cs.CL

TL;DR: MARS-SQL是一个多智能体框架，通过任务分解和交互式强化学习来解决复杂自然语言到SQL的转换问题，在BIRD和Spider数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 复杂查询的自然语言到SQL转换通常需要环境交互和自我修正，现有方法在这方面存在困难。

Method: 使用三个专门智能体：基础智能体进行模式链接，生成智能体通过多轮强化学习策略生成查询，验证智能体进行最终选择。生成智能体采用ReAct风格的思考-行动-观察循环进行迭代生成。

Result: 在BIRD开发集上达到77.84%的执行准确率，在Spider测试集上达到89.75%的执行准确率，实现了最先进的性能。

Conclusion: MARS-SQL通过结构化工作流程和专门智能体的结合，结合交互式强化学习和生成建模，为稳健准确的SQL生成提供了高效解决方案。

Abstract: Translating natural language to SQL remains difficult for complex queries.
Such queries often need environmental interaction and self-correction. To
address this, we introduce MARS-SQL, a novel multi-agent framework that
combines principled task decomposition and interactive reinforcement learning
(RL). Our system comprises three specialized agents: a Grounding Agent for
schema linking, a Generation Agent for query generation, and a Validation Agent
for final selection. The core of our framework is the Generation agent, which
is trained via a multi-turn RL policy. Adopting a ReAct-style Think-Act-Observe
loop, the agent iteratively generates thoughts, executes SQL actions against a
live database, and revises its strategy based on execution feedback, enabling
dynamic, stateful reasoning and self-correction. At inference time, we generate
multiple interaction trajectories to explore diverse reasoning paths. The
Validation agent, then selects the optimal trajectory by modeling verification
as a next-token prediction task and choosing the solution with the highest
generation probability. This structured workflow pipelines specialized agents.
It combines interactive RL for generation with generative modeling for
verification. The approach proves highly effective for robust and accurate SQL
generation. Experiments show that MARS-SQL achieves state-of-the-art Execution
Accuracy of 77.84% on the BIRD dev set and 89.75% on the Spider test set. Our
code is available at https://github.com/YangHaolin0526/MARS-SQL.

</details>


### [41] [IF-CRITIC: Towards a Fine-Grained LLM Critic for Instruction-Following Evaluation](https://arxiv.org/abs/2511.01014)
*Bosi Wen,Yilin Niu,Cunxiang Wang,Pei Ke,Xiaoying Ling,Ying Zhang,Aohan Zeng,Hongning Wang,Minlie Huang*

Main category: cs.CL

TL;DR: 提出了IF-CRITIC，一种能够高效可靠评估指令约束遵循情况的LLM批评器，通过清单生成和多阶段过滤机制提升评估质量，在指令跟随优化中实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM-as-a-Judge的指令跟随评估方法存在成本高和评估不可靠的问题，需要开发更高效可靠的评估模型。

Method: 开发清单生成器分解指令生成约束清单，通过多阶段批评过滤机制收集高质量训练数据，采用约束级偏好优化方法训练IF-CRITIC。

Result: IF-CRITIC的评估性能超越了Deepseek-R1和o4-mini等强基线，在较低计算开销下为LLM提供可扩展的奖励信号，实现指令跟随优化的显著性能提升。

Conclusion: IF-CRITIC提供了一种高效可靠的指令约束遵循评估方法，能够显著提升LLM的指令跟随能力，同时降低计算成本。

Abstract: Instruction following is a fundamental ability of Large Language Models
(LLMs), requiring their generated outputs to follow multiple constraints
imposed in input instructions. Numerous studies have attempted to enhance this
ability through preference optimization or reinforcement learning based on
reward signals from LLM-as-a-Judge. However, existing evaluation models for
instruction following still possess many deficiencies, such as substantial
costs and unreliable assessments. To this end, we propose IF-CRITIC, an LLM
critic that can provide efficient and reliable assessments of constraint
following in the instructions. We first develop a checklist generator to
decompose instructions and generate constraint checklists. With the assistance
of the checklists, we collect high-quality critique training data through a
multi-stage critique filtering mechanism and employ a constraint-level
preference optimization method to train IF-CRITIC. Extensive experiments
demonstrate that the evaluation performance of IF-CRITIC can beat strong
LLM-as-a-Judge baselines, including Deepseek-R1 and o4-mini. With the scalable
reward signals provided by IF-CRITIC, LLMs can achieve substantial performance
gains in instruction-following optimization under lower computational overhead
compared to strong LLM critic baselines.

</details>


### [42] [Prompt-R1: Collaborative Automatic Prompting Framework via End-to-end Reinforcement Learning](https://arxiv.org/abs/2511.01016)
*Wenjin Liu,Haoran Luo,Xueyuan Lin,Haoming Liu,Tiesunlong Shen,Jiapu Wang,Rui Mao,Erik Cambria*

Main category: cs.CL

TL;DR: Prompt-R1是一个端到端的强化学习框架，使用小型LLM与大型LLM协作，通过多轮提示交互解决复杂问题，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 用户在与大型语言模型交互时往往无法提供准确有效的提示，限制了模型性能的发挥。

Method: 提出Prompt-R1框架，使用小型LLM生成提示，大型LLM进行复杂推理，采用双约束奖励优化正确性、生成质量和推理准确性。

Result: 在多个公共数据集上的实验表明，Prompt-R1在各项任务中显著优于基线模型。

Conclusion: Prompt-R1提供了一个即插即用的协作框架，有效解决了用户提示质量不足的问题，提升了LLM在复杂问题上的表现。

Abstract: Recently, advanced large language models (LLMs) have emerged at an
increasingly rapid pace. However, when faced with complex problems, most users
are often unable to provide accurate and effective prompts to interact with
LLMs, thus limiting the performance of LLMs. To address this challenge, we
propose Prompt-R1, an end-to-end reinforcement learning framework that uses a
small-scale LLM to collaborate with large-scale LLMs, replacing user
interaction to solve problems better. This collaboration is cast as a
multi-turn prompt interaction, where the small-scale LLM thinks and generates
prompts, and the large-scale LLM performs complex reasoning. A dual-constrained
reward is designed to optimize for correctness, generation quality, and
reasoning accuracy. Prompt-R1 provides a plug-and-play framework that supports
both inference and training with various large-scale LLMs. Experiments on
multiple public datasets show that Prompt-R1 significantly outperforms baseline
models across tasks. Our code is publicly available at
https://github.com/QwenQKing/Prompt-R1.

</details>


### [43] [OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights](https://arxiv.org/abs/2511.01019)
*Bowen Chen,Jayesh Gajbhar,Gregory Dusek,Rob Redmon,Patrick Hogan,Paul Liu,DelWayne Bohnenstiehl,Dongkuan,Xu,Ruoying He*

Main category: cs.CL

TL;DR: OceanAI是一个结合开源大语言模型与NOAA实时海洋数据的对话平台，通过API调用生成可验证的自然语言回答和数据可视化，解决AI在科学领域中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 解决通用对话AI系统在科学领域产生未经验证的"幻觉"问题，确保科学严谨性，为海洋科学提供可靠的数据支持。

Method: 整合开源大语言模型与NOAA实时参数化海洋数据流，通过API调用识别、解析和合成相关数据集，生成可复现的自然语言响应和数据可视化。

Result: 在盲测中，OceanAI是唯一能提供NOAA来源数据和原始数据引用的系统，其他AI产品要么拒绝回答，要么提供无支持的结果。

Conclusion: OceanAI通过基于可验证观测的输出，提升了透明度、可复现性和可信度，为海洋领域的AI决策支持提供了可扩展框架。

Abstract: Artificial intelligence is transforming the sciences, yet general
conversational AI systems often generate unverified "hallucinations"
undermining scientific rigor. We present OceanAI, a conversational platform
that integrates the natural-language fluency of open-source large language
models (LLMs) with real-time, parameterized access to authoritative
oceanographic data streams hosted by the National Oceanic and Atmospheric
Administration (NOAA). Each query such as "What was Boston Harbor's highest
water level in 2024?" triggers real-time API calls that identify, parse, and
synthesize relevant datasets into reproducible natural-language responses and
data visualizations. In a blind comparison with three widely used AI
chat-interface products, only OceanAI produced NOAA-sourced values with
original data references; others either declined to answer or provided
unsupported results. Designed for extensibility, OceanAI connects to multiple
NOAA data products and variables, supporting applications in marine hazard
forecasting, ecosystem assessment, and water-quality monitoring. By grounding
outputs and verifiable observations, OceanAI advances transparency,
reproducibility, and trust, offering a scalable framework for AI-enabled
decision support within the oceans. A public demonstration is available at
https://oceanai.ai4ocean.xyz.

</details>


### [44] [VayuChat: An LLM-Powered Conversational Interface for Air Quality Data Analytics](https://arxiv.org/abs/2511.01046)
*Vedant Acharya,Abhay Pisharodi,Rishabh Mondal,Mohammad Rafiuddin,Nipun Batra*

Main category: cs.CL

TL;DR: VayuChat是一个对话式系统，通过自然语言问答帮助用户分析空气质量、气象和政策数据，生成Python代码和交互式可视化，使环境数据分析对决策者、研究人员和公民更加易用。


<details>
  <summary>Details</summary>
Motivation: 印度每年因空气污染导致约160万人过早死亡，但决策者难以将分散的数据转化为有效决策。现有工具需要专业知识且提供静态仪表板，无法解决关键政策问题。

Method: 开发VayuChat对话系统，集成中央污染控制委员会监测站数据、州级人口统计数据和国家清洁空气计划资金记录，利用大语言模型提供统一接口，支持自然语言查询并生成可执行代码和可视化。

Result: 系统已公开部署，用户可通过简单对话执行复杂的环境分析，使数据科学对政策制定者、研究人员和公民更加可访问。

Conclusion: VayuChat通过对话式界面成功降低了环境数据分析的门槛，为空气污染治理提供了更易用的决策支持工具。

Abstract: Air pollution causes about 1.6 million premature deaths each year in India,
yet decision makers struggle to turn dispersed data into decisions. Existing
tools require expertise and provide static dashboards, leaving key policy
questions unresolved. We present VayuChat, a conversational system that answers
natural language questions on air quality, meteorology, and policy programs,
and responds with both executable Python code and interactive visualizations.
VayuChat integrates data from Central Pollution Control Board (CPCB) monitoring
stations, state-level demographics, and National Clean Air Programme (NCAP)
funding records into a unified interface powered by large language models. Our
live demonstration will show how users can perform complex environmental
analytics through simple conversations, making data science accessible to
policymakers, researchers, and citizens. The platform is publicly deployed at
https://huggingface.co/spaces/SustainabilityLabIITGN/ VayuChat. For further
information check out video uploaded on
https://www.youtube.com/watch?v=d6rklL05cs4.

</details>


### [45] [Building a Silver-Standard Dataset from NICE Guidelines for Clinical LLMs](https://arxiv.org/abs/2511.01053)
*Qing Ding,Eric Hua Qing Zhang,Felix Jozsa,Julia Ive*

Main category: cs.CL

TL;DR: 该研究创建了一个基于公开指南的标准化基准数据集，用于评估LLM在临床推理和指南遵循方面的表现。


<details>
  <summary>Details</summary>
Motivation: LLM在医疗领域的应用日益增多，但缺乏评估基于指南的临床推理的标准化基准。

Method: 使用GPT帮助创建包含真实患者场景和临床问题的验证数据集，并对多个流行LLM进行基准测试。

Result: 展示了数据集的效度，并提供了系统评估LLM临床实用性和指南遵循能力的框架。

Conclusion: 该框架支持对LLM临床效用和指南遵循性的系统评估，填补了现有评估方法的空白。

Abstract: Large language models (LLMs) are increasingly used in healthcare, yet
standardised benchmarks for evaluating guideline-based clinical reasoning are
missing. This study introduces a validated dataset derived from publicly
available guidelines across multiple diagnoses. The dataset was created with
the help of GPT and contains realistic patient scenarios, as well as clinical
questions. We benchmark a range of recent popular LLMs to showcase the validity
of our dataset. The framework supports systematic evaluation of LLMs' clinical
utility and guideline adherence.

</details>


### [46] [HPLT~3.0: Very Large-Scale Multilingual Resources for LLM and MT. Mono- and Bi-lingual Data, Multilingual Evaluation, and Pre-Trained Models](https://arxiv.org/abs/2511.01066)
*Stephan Oepen,Nikolay Arefev,Mikko Aulamo,Marta Bañón,Maja Buljan,Laurie Burchell,Lucas Charpentier,Pinzhen Chen,Mariya Fedorova,Ona de Gibert,Barry Haddow,Jan Hajič,Jindrič Helcl,Andrey Kutuzov,Zihao Li,Risto Luukkonen,Bhavitvya Malik,Vladislav Mikhailov,Amanda Myntti,Dayyán O'Brien,Lucie Poláková,Sampo Pyysalo,Gema Ramírez Sánchez,Janine Siewert,Pavel Stepachev,Jörg Tiedemann,Teemu Vahtola,Fedor Vitiugin,Tea Vojtěchová,Jaume Zaragoza*

Main category: cs.CL

TL;DR: 该论文介绍了一个包含近200种语言、30万亿token的大规模多语言LLM预训练数据集，提供了完整的数据处理流程和评估基准。


<details>
  <summary>Details</summary>
Motivation: 为多语言大语言模型预训练提供开放、高质量、大规模且丰富标注的文本数据集，解决多语言数据稀缺问题。

Method: 从不同来源的网络爬虫数据中提取文本，通过完整的开源流程进行文档选择、文本提取、语言识别、去重、标注（包括文本质量、个人信息等）和最终筛选。

Result: 创建了30万亿token的多语言数据集，训练了57个单语言编码器-解码器模型和一些GPT类参考模型，并构建了自动挖掘的平行语料库。

Conclusion: 该项目提供了目前最大的多语言LLM预训练数据集，并建立了全面的评估基准，为多语言自然语言处理研究提供了重要资源。

Abstract: We present an ongoing initiative to provide open, very large, high-quality,
and richly annotated textual datasets for almost 200 languages. At 30 trillion
tokens, this is likely the largest generally available multilingual collection
of LLM pre-training data. At 30 trillion tokens, this is likely the largest
generally available multilingual collection of LLM pre-training data. These
datasets are derived from web crawls from different sources and accompanied
with a complete, open-source pipeline for document selection from web archives,
text extraction from HTML, language identification for noisy texts, exact and
near-deduplication, annotation with, among others, register labels, text
quality estimates, and personally identifiable information; and final selection
and filtering. We report on data quality probes through contrastive and
analytical statistics, through manual inspection of samples for 24 languages,
and through end-to-end evaluation of various language model architectures
trained on this data. For multilingual LLM evaluation, we provide a
comprehensive collection of benchmarks for nine European languages, with
special emphasis on natively created tasks, mechanisms to mitigate prompt
sensitivity, and refined normalization and aggregation of scores. Additionally,
we train and evaluate a family of 57 monolingual encoder-decoder models, as
well as a handful of monolingual GPT-like reference models. Besides the
monolingual data and models, we also present a very large collection of
parallel texts automatically mined from this data, together with a novel
parallel corpus synthesized via machine translation.

</details>


### [47] [Improving Romanian LLM Pretraining Data using Diversity and Quality Filtering](https://arxiv.org/abs/2511.01090)
*Vlad Negoita,Mihai Masala,Traian Rebedea*

Main category: cs.CL

TL;DR: 研究罗马尼亚语预训练语料库的特征和覆盖范围，通过多任务模型进行多级过滤来生成高质量预训练数据集，证明数据过滤能提升LLM预训练性能


<details>
  <summary>Details</summary>
Motivation: 高质量数据对训练LLMs至关重要，特别是对于罗马尼亚语等资源匮乏语言，需要研究其语料库特征并与英语数据对比

Method: 训练轻量级多任务模型对LLM标注的罗马尼亚文本进行分析，进行多级过滤（教育价值、主题、格式等）来生成高质量预训练数据集

Result: 发现了罗马尼亚语和英语数据中主题分布的重要趋势，通过数据过滤在多个基准测试中提升了LLM预训练性能

Conclusion: 数据过滤方法能有效提升罗马尼亚语等资源匮乏语言的LLM预训练效果，多级过滤策略对生成高质量数据集至关重要

Abstract: Large Language Models (LLMs) have recently exploded in popularity, often
matching or outperforming human abilities on many tasks. One of the key factors
in training LLMs is the availability and curation of high-quality data. Data
quality is especially crucial for under-represented languages, where
high-quality corpora are scarce. In this work we study the characteristics and
coverage of Romanian pretraining corpora and we examine how they differ from
English data. By training a lightweight multitask model on carefully
LLM-annotated Romanian texts, we are able to analyze and perform multi-level
filtering (e.g., educational value, topic, format) to generate high-quality
pretraining datasets. Our experiments show noteworthy trends in the topics
present in Romanian and English data, while also proving the effectiveness of
filtering data through improved LLM pretraining performance across multiple
benchmarks.

</details>


### [48] [TSVer: A Benchmark for Fact Verification Against Time-Series Evidence](https://arxiv.org/abs/2511.01101)
*Marek Strong,Andreas Vlachos*

Main category: cs.CL

TL;DR: TSVer是一个用于时间序列事实核查的新基准数据集，包含287个真实世界声明和400个时间序列数据，旨在评估模型在时间和数值推理方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的事实核查数据集在时间序列证据方面存在不足，缺乏结构化证据、判决理由不充分或依赖合成声明，需要更高质量的基准来评估系统性能。

Method: 采用LLM辅助的多步骤标注流程，从38个事实核查机构收集287个真实声明，并整理400个涵盖多个领域的时间序列数据库，对每个声明标注时间框架、判决和证据使用理由。

Result: 标注质量得到提升，判决的标注者间一致性达到kappa=0.745。最先进的推理模型如Gemini-2.5-Pro在时间序列事实核查中表现不佳，判决准确率为63.37%，证据到理由的评分为48.63%。

Conclusion: TSVer数据集填补了时间序列事实核查基准的空白，揭示了当前模型在处理时间和数值推理任务时的局限性，为未来研究提供了重要基础。

Abstract: Reasoning over temporal and numerical data, such as time series, is a crucial
aspect of fact-checking. While many systems have recently been developed to
handle this form of evidence, their evaluation remains limited by existing
datasets, which often lack structured evidence, provide insufficient
justifications for verdicts, or rely on synthetic claims. In this paper, we
introduce TSVer, a new benchmark dataset for fact verification focusing on
temporal and numerical reasoning with time-series evidence. TSVer contains 287
real-world claims sourced from 38 fact-checking organizations and a curated
database of 400 time series covering diverse domains. Each claim is annotated
with time frames across all pertinent time series, along with a verdict and
justifications reflecting how the evidence is used to reach the verdict. Using
an LLM-assisted multi-step annotation process, we improve the quality of our
annotations and achieve an inter-annotator agreement of kappa=0.745 on
verdicts. We also develop a baseline for verifying claims against time-series
evidence and show that even the state-of-the-art reasoning models like
Gemini-2.5-Pro are challenged by time series, achieving a 63.37 accuracy score
on verdicts and an Ev2R score of 48.63 on verdict justifications.

</details>


### [49] [MicroRemed: Benchmarking LLMs in Microservices Remediation](https://arxiv.org/abs/2511.01166)
*Lingzhe Zhang,Yunpeng Zhai,Tong Jia,Chiming Duan,Minghua He,Leyi Pan,Zhaoyang Liu,Bolin Ding,Ying Li*

Main category: cs.CL

TL;DR: 提出了首个端到端微服务修复基准MicroRemed和ThinkRemed多智能体框架，通过模拟SRE的反思性推理来提升LLM在微服务系统自动修复中的表现


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖人工编写的提示，LLM仅将文本指令转换为可执行代码，缺乏端到端的自主修复能力

Method: 引入MicroRemed基准评估LLM从诊断报告直接生成可执行Ansible playbook的能力；提出ThinkRemed多智能体框架模拟SRE的反思和感知推理

Result: MicroRemed对当前LLM构成显著挑战，ThinkRemed通过迭代推理和系统反思提升了端到端修复性能

Conclusion: 该研究为LLM在微服务修复领域的应用提供了首个基准测试和有效的多智能体解决方案

Abstract: Large Language Models (LLMs) integrated with agent-based reasoning frameworks
have recently shown strong potential for autonomous decision-making and
system-level operations. One promising yet underexplored direction is
microservice remediation, where the goal is to automatically recover faulty
microservice systems. Existing approaches, however, still rely on human-crafted
prompts from Site Reliability Engineers (SREs), with LLMs merely converting
textual instructions into executable code. To advance research in this area, we
introduce MicroRemed, the first benchmark for evaluating LLMs in end-to-end
microservice remediation, where models must directly generate executable
Ansible playbooks from diagnosis reports to restore system functionality. We
further propose ThinkRemed, a multi-agent framework that emulates the
reflective and perceptive reasoning of SREs. Experimental results show that
MicroRemed presents substantial challenges to current LLMs, while ThinkRemed
improves end-to-end remediation performance through iterative reasoning and
system reflection. The benchmark is available at
https://github.com/LLM4AIOps/MicroRemed.

</details>


### [50] [Learning When to Quit in Sales Conversations](https://arxiv.org/abs/2511.01181)
*Emaad Manzoor,Eva Ascarza,Oded Netzer*

Main category: cs.CL

TL;DR: 开发了一个基于语言模型的停止代理，通过学习最优停止策略来帮助销售人员决定何时放弃销售对话，在电信销售场景中减少了54%的失败通话时间，同时保持几乎全部销售额。


<details>
  <summary>Details</summary>
Motivation: 销售人员经常面临动态筛选决策：是继续对话还是放弃并转向下一个潜在客户。目前对这些决策的效率以及如何改进知之甚少。

Method: 将动态筛选决策形式化为最优停止问题，开发基于生成语言模型的序列决策代理（停止代理），通过模仿回顾性推断的最优停止策略来学习何时放弃对话。

Result: 在大型欧洲电信公司的呼叫数据应用中，停止代理将失败通话时间减少了54%，同时保持几乎所有销售额；重新分配节省的时间可将预期销售额提高至多37%。

Conclusion: 研究发现销售人员倾向于过度重视少数突出的消费者不感兴趣表达，并错误预测通话失败风险，表明他们在实时对话决策中存在认知限制。人工智能算法有潜力纠正认知受限的人类决策并提高销售团队效率。

Abstract: Salespeople frequently face the dynamic screening decision of whether to
persist in a conversation or abandon it to pursue the next lead. Yet, little is
known about how these decisions are made, whether they are efficient, or how to
improve them. We study these decisions in the context of high-volume outbound
sales where leads are ample, but time is scarce and failure is common. We
formalize the dynamic screening decision as an optimal stopping problem and
develop a generative language model-based sequential decision agent - a
stopping agent - that learns whether and when to quit conversations by
imitating a retrospectively-inferred optimal stopping policy. Our approach
handles high-dimensional textual states, scales to large language models, and
works with both open-source and proprietary language models. When applied to
calls from a large European telecommunications firm, our stopping agent reduces
the time spent on failed calls by 54% while preserving nearly all sales;
reallocating the time saved increases expected sales by up to 37%. Upon
examining the linguistic cues that drive salespeople's quitting decisions, we
find that they tend to overweight a few salient expressions of consumer
disinterest and mispredict call failure risk, suggesting cognitive bounds on
their ability to make real-time conversational decisions. Our findings
highlight the potential of artificial intelligence algorithms to correct
cognitively-bounded human decisions and improve salesforce efficiency.

</details>


### [51] [Surfacing Subtle Stereotypes: A Multilingual, Debate-Oriented Evaluation of Modern LLMs](https://arxiv.org/abs/2511.01187)
*Muhammed Saeed,Muhammad Abdul-mageed,Shady Shehata*

Main category: cs.CL

TL;DR: 提出了DebateBias-8K多语言辩论风格基准，用于评估大型语言模型在生成式场景中的叙事偏见，发现主流模型在7种语言中均表现出根深蒂固的刻板印象，且偏见在低资源语言中更加严重。


<details>
  <summary>Details</summary>
Motivation: 当前大多数偏见评估仍依赖英语分类任务，缺乏针对现实生成式场景的多语言偏见评估方法，需要揭示模型在开放对话中如何再现叙事偏见。

Method: 构建包含8,400个结构化辩论提示的多语言数据集，涵盖4个敏感领域和7种语言，使用4个旗舰模型生成超过10万条响应并进行自动分类分析。

Result: 所有模型都再现了根深蒂固的刻板印象：阿拉伯人与恐怖主义和宗教高度关联（≥95%），非洲人与社会经济"落后"关联（≤77%），西方群体被一致描述为现代或进步。偏见在低资源语言中显著增强。

Conclusion: 当前主要基于英语的对齐训练无法在全球范围内泛化，现有对齐方法虽然减少了显性毒性，但无法防止开放对话中的偏见输出，需要开发更安全、文化包容的模型对齐方法。

Abstract: Large language models (LLMs) are widely deployed for open-ended
communication, yet most bias evaluations still rely on English,
classification-style tasks. We introduce DebateBias-8K, a new multilingual,
debate-style benchmark designed to reveal how narrative bias appears in
realistic generative settings. Our dataset includes 8,400 structured debate
prompts spanning four sensitive domains: women's rights, socioeconomic
development, terrorism, and religion, across seven languages ranging from
high-resource (English, Chinese) to low-resource (Swahili, Nigerian Pidgin).
Using four flagship models (GPT-4o, Claude 3, DeepSeek, and LLaMA 3), we
generate and automatically classify over 100,000 responses. Results show that
all models reproduce entrenched stereotypes despite safety alignment: Arabs are
overwhelmingly linked to terrorism and religion (>=95%), Africans to
socioeconomic "backwardness" (up to <=77%), and Western groups are consistently
framed as modern or progressive. Biases grow sharply in lower-resource
languages, revealing that alignment trained primarily in English does not
generalize globally. Our findings highlight a persistent divide in multilingual
fairness: current alignment methods reduce explicit toxicity but fail to
prevent biased outputs in open-ended contexts. We release our DebateBias-8K
benchmark and analysis framework to support the next generation of multilingual
bias evaluation and safer, culturally inclusive model alignment.

</details>


### [52] [ZoFia: Zero-Shot Fake News Detection with Entity-Guided Retrieval and Multi-LLM Interaction](https://arxiv.org/abs/2511.01188)
*Lvhua Wu,Xuefeng Jiang,Sheng Sun,Tian Wen,Yuwei Wang,Min Liu*

Main category: cs.CL

TL;DR: ZoFia是一个两阶段零样本假新闻检测框架，通过层次化显著性分析选择关键查询词检索最新外部证据，然后使用多LLM交互系统进行多视角协作分析和对抗性辩论，最终产生可解释的鲁棒判断。


<details>
  <summary>Details</summary>
Motivation: 假新闻的快速传播威胁社会稳定和公众信任，而现有LLM存在时间限制的知识覆盖和生成幻觉内容的倾向，在处理快速演变的新闻流时可靠性不足。现有静态数据集训练的模型也缺乏对新兴新闻主题的泛化能力。

Method: 1. 引入层次化显著性量化新闻内容中实体的重要性，提出SC-MMR算法选择信息丰富且多样化的关键词作为查询词检索最新外部证据；2. 构建多LLM交互系统，每个代理承担不同角色，对新闻文本及相关信息进行多视角协作分析和对抗性辩论。

Result: 在两个公共数据集上的综合实验表明，ZoFia明显优于现有的零样本基线方法和大多数少样本方法。

Conclusion: ZoFia框架通过结合外部证据检索和多LLM协作分析，有效解决了假新闻检测中的时效性和泛化性问题，提供了可解释且鲁棒的检测方案。

Abstract: The rapid spread of fake news threatens social stability and public trust,
rendering its detection an imperative research priority. Although large
language models (LLMs) excel at numerous natural language processing tasks with
their remarkable contextual understanding and extensive prior knowledge, the
time-bounded knowledge coverage and tendency for generating hallucination
content reduce their reliability when handling fast-evolving news streams.
Furthermore, models trained on existing static datasets also often lack the
generalization needed for emerging news topics. To address these challenges, we
propose ZoFia, a novel two-stage zero-shot fake news detection framework.
First, we introduce Hierarchical Salience to quantify the importance of
entities in the news content, and propose the SC-MMR algorithm to effectively
select an informative and diverse set of keywords that serve as queries for
retrieving up-to-date external evidence. Subsequently, a multi LLM interactive
system, in which each agent assumes a distinct role, performs multi-view
collaborative analysis and adversarial debate over the news text and its
related information, and finally produces an interpretable and robust judgment.
Comprehensive experiments on two public datasets demonstrate that ZoFia
obviously outperforms existing zero-shot baselines and most of few-shot
methods. Our codes will be open-sourced to facilitate related communities.

</details>


### [53] [Self-Harmony: Learning to Harmonize Self-Supervision and Self-Play in Test-Time Reinforcement Learning](https://arxiv.org/abs/2511.01191)
*Ru Wang,Wei Huang,Qi Cao,Yusuke Iwasawa,Yutaka Matsuo,Jiaxian Guo*

Main category: cs.CL

TL;DR: Self-Harmony是一个无需标签的测试时强化学习框架，通过模型自身生成问题重述和答案，利用谐波均值聚合稳定解，避免多数投票的伪答案陷阱。


<details>
  <summary>Details</summary>
Motivation: 标准测试时强化学习方法如多数投票容易陷入伪答案陷阱，需要构建可靠的无需人工监督的学习信号。

Method: 使用单一模型扮演求解器和重述器两个角色，基于原始问题和重述问题生成答案，通过谐波均值聚合答案频率来选择稳定解。

Result: 在30个推理基准测试的28个中排名第一，所有实验零训练失败，展现了前所未有的鲁棒性。

Conclusion: Self-Harmony通过问题重述和稳定解选择机制，在无需人工监督的情况下实现了最先进的测试时适应性能。

Abstract: Test-time reinforcement learning (TTRL) offers a label-free paradigm for
adapting models using only synthetic signals at inference, but its success
hinges on constructing reliable learning signals. Standard approaches such as
majority voting often collapse to spurious yet popular answers. We introduce
Self-Harmony, a framework built on a simple intuition: the correct answer
should remain stable across both an original question and its paraphrase.
Self-Harmony operationalizes this by employing a single model in two
complementary roles: a Solver to produce answers and a Reframer to rephrase the
input. Based on this, we further propose a pseudo-label method: instead of
majority voting, it aggregates answer frequencies across these original and
reframed views using the harmonic mean. This is a process that naturally
selects for solutions stable under reframing, thereby avoiding the common trap
of favoring view-dependent, spurious answers. Crucially, this requires no human
supervision or auxiliary models. Across diverse reasoning benchmarks,
Self-Harmony achieves state-of-the-art results at the label-free test-time
setting, ranking first in 28 of 30 settings across multiple methods. Beyond
accuracy, it demonstrates unprecedented robustness, with zero training failures
in all experiments, underscoring its stability and reliability.

</details>


### [54] [DEER: Disentangled Mixture of Experts with Instance-Adaptive Routing for Generalizable Machine-Generated Text Detection](https://arxiv.org/abs/2511.01192)
*Guoxin Ma,Xiaoming Liu,Zhanhan Zhang,Chengzhengxu Li,Shengchao Liu,Yu Lan*

Main category: cs.CL

TL;DR: 提出DEER框架，通过两阶段解耦专家混合架构捕获领域特定和领域通用的机器生成文本模式，解决领域偏移问题


<details>
  <summary>Details</summary>
Motivation: 现有机器生成文本检测方法在领域偏移下性能显著下降，需要同时捕获领域特定和跨领域特征

Method: 两阶段解耦专家混合架构：领域特定专家学习细粒度区分，共享专家提取跨领域特征；强化学习路由机制动态选择专家

Result: 在5个领域内和5个领域外数据集上平均F1分数分别提升1.39%和5.32%，准确率分别提升1.35%和3.61%

Conclusion: DEER框架通过解耦专家专业化和自适应路由显著提升机器生成文本检测性能，特别是在领域偏移场景下

Abstract: Detecting machine-generated text (MGT) has emerged as a critical challenge,
driven by the rapid advancement of large language models (LLMs) capable of
producing highly realistic, human-like content. However, the performance of
current approaches often degrades significantly under domain shift. To address
this challenge, we propose a novel framework designed to capture both
domain-specific and domain-general MGT patterns through a two-stage
Disentangled mixturE-of-ExpeRts (DEER) architecture. First, we introduce a
disentangled mixture-of-experts module, in which domain-specific experts learn
fine-grained, domain-local distinctions between human and machine-generated
text, while shared experts extract transferable, cross-domain features. Second,
to mitigate the practical limitation of unavailable domain labels during
inference, we design a reinforcement learning-based routing mechanism that
dynamically selects the appropriate experts for each input instance,
effectively bridging the train-inference gap caused by domain uncertainty.
Extensive experiments on five in-domain and five out-of-domain benchmark
datasets demonstrate that DEER consistently outperforms state-of-the-art
methods, achieving average F1-score improvements of 1.39% and 5.32% on
in-domain and out-of-domain datasets respectively, along with accuracy gains of
1.35% and 3.61% respectively. Ablation studies confirm the critical
contributions of both disentangled expert specialization and adaptive routing
to model performance.

</details>


### [55] [AraFinNews: Arabic Financial Summarisation with Domain-Adapted LLMs](https://arxiv.org/abs/2511.01265)
*Mo El-Haj,Paul Rayson*

Main category: cs.CL

TL;DR: 本文研究了领域特异性对阿拉伯语金融文本摘要的影响，通过构建AraFinNews数据集并评估多个Transformer模型，发现领域适应的模型能生成更忠实和连贯的摘要。


<details>
  <summary>Details</summary>
Motivation: 研究领域特异性在阿拉伯语金融文本摘要中的作用，填补阿拉伯语金融新闻摘要数据集的空白，并评估领域适应对摘要质量的影响。

Method: 构建了AraFinNews数据集（21.25万篇文章-标题对），评估了mT5、AraT5和领域适应的FinAraT5等Transformer模型，重点关注事实准确性、数字可靠性和风格对齐。

Result: 领域适应的模型在生成摘要时表现出更高的忠实度和连贯性，特别是在处理定量信息和实体相关信息方面表现更好。

Conclusion: 领域特异性适应对于提高阿拉伯语金融摘要的事实一致性和叙述流畅性至关重要，FinAraT5等领域适应模型在金融文本摘要中表现优异。

Abstract: This paper investigates the impact of domain specificity on abstractive
summarisation of Arabic financial texts using large language models (LLMs). We
introduce AraFinNews, the largest publicly available Arabic financial news
dataset to date, comprising 212,500 article--headline pairs spanning nearly a
decade of reporting from October 2015 to July 2025. Designed as the Arabic
equivalent of major English summarisation corpora such as CNN/DailyMail,
AraFinNews provides a robust benchmark for evaluating domain-specific language
understanding and generation in financial contexts. Using this resource, we
evaluate transformer-based models -- including mT5, AraT5, and the
domain-adapted FinAraT5 -- to examine how financial-domain pretraining
influences factual accuracy, numerical reliability, and stylistic alignment
with professional reporting. Experimental results show that domain-adapted
models generate more faithful and coherent summaries, particularly in handling
quantitative and entity-centric information. The findings highlight the
importance of domain-specific adaptation for improving factual consistency and
narrative fluency in Arabic financial summarisation. The dataset is freely
available for non-commercial research at
https://github.com/ArabicNLP-UK/AraFinNews.

</details>


### [56] [When, What, and How: Rethinking Retrieval-Enhanced Speculative Decoding](https://arxiv.org/abs/2511.01282)
*Min Fang,Zhihui Fu,Qibin Zhao,Jun Wang*

Main category: cs.CL

TL;DR: ReSpec是一种新的检索增强推测解码框架，通过自适应决策机制替代启发式切换策略，在保持输出质量的同时显著加速大语言模型推理。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法存在局限性：基于模型的方法成本高，检索增强方法依赖启发式切换策略导致不必要的检索开销。需要更智能的决策机制来优化推测解码效率。

Method: 1) 基于熵的自适应触发器：量化上下文可预测性，仅在不确定性低时启动检索；2) 反馈驱动的候选选择：利用历史反馈组织高质量候选进行并行验证；3) 源感知宽松验证策略：对模型生成草稿严格检查，对检索草稿宽松验证。

Result: 在Spec-Bench上的实验表明，ReSpec实现了最先进的加速效果，相比EAGLE-2和SAM-Decoding分别提升超过33%和25%，同时保持输出质量。

Conclusion: ReSpec通过将启发式切换转化为自适应决策，有效解决了检索增强推测解码中的效率问题，在加速比和输出质量之间取得了更好的平衡。

Abstract: Speculative decoding (SD) has emerged as an effective technique to accelerate
large language model (LLM) inference without compromising output quality.
However, the achievable speedup largely depends on the effectiveness of the
drafting model. While model-based methods like EAGLE-2 are accurate but costly,
retrieval-enhanced methods like SAM-Decoding rely on heuristic switching
strategies that often trigger unnecessary retrievals. To address this, we
propose ReSpec (\textbf{Re}trieval-enhanced \textbf{Spe}culative Decoding), a
novel framework that transforms heuristic drafter switching into adaptive
decision-making. ReSpec features three core innovations: 1) An
\textbf{entropy-guided adaptive trigger} quantifies contextual predictability
to initiate retrieval only when uncertainty is low, avoiding costly low-quality
speculations. 2) A \textbf{feedback-driven candidate selection} leverages
historical feedback to organize multiple high-quality candidates for parallel
verification, maximizing retrieval utility. 3) A source-aware \textbf{relaxed
verification strategy} applies strict checks to model-generated drafts while
using a relaxed verification for retrieved drafts, achieving a better balance
between accuracy and efficiency. Extensive experiments on Spec-Bench
demonstrate that ReSpec achieves state-of-the-art acceleration,outperforming
EAGLE-2 and SAM-Decoding by over $33\%$ and $25\%$, respectively, while
maintaining output quality.

</details>


### [57] ["Give a Positive Review Only": An Early Investigation Into In-Paper Prompt Injection Attacks and Defenses for AI Reviewers](https://arxiv.org/abs/2511.01287)
*Qin Zhou,Zhexin Zhang,Zhi Li,Limin Sun*

Main category: cs.CL

TL;DR: 本文系统研究了AI辅助论文评审中的提示注入攻击威胁，提出了静态和迭代两种攻击方法，并探索了检测防御机制。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型在科学论文评审中的广泛应用，发现存在隐藏的注入提示操纵AI评审者给出过高评价的威胁，需要对此进行系统性调查。

Method: 提出两类攻击：静态攻击使用固定注入提示，迭代攻击通过模拟评审模型优化注入提示以最大化效果；同时探索基于检测的防御方法。

Result: 两种攻击都取得了显著效果，经常能诱导前沿AI评审者给出满分评价；检测防御能大幅降低攻击成功率，但自适应攻击者可以部分规避防御。

Conclusion: 研究结果强调了在AI辅助同行评审中需要更多关注和严格防护措施来应对提示注入威胁。

Abstract: With the rapid advancement of AI models, their deployment across diverse
tasks has become increasingly widespread. A notable emerging application is
leveraging AI models to assist in reviewing scientific papers. However, recent
reports have revealed that some papers contain hidden, injected prompts
designed to manipulate AI reviewers into providing overly favorable
evaluations. In this work, we present an early systematic investigation into
this emerging threat. We propose two classes of attacks: (1) static attack,
which employs a fixed injection prompt, and (2) iterative attack, which
optimizes the injection prompt against a simulated reviewer model to maximize
its effectiveness. Both attacks achieve striking performance, frequently
inducing full evaluation scores when targeting frontier AI reviewers.
Furthermore, we show that these attacks are robust across various settings. To
counter this threat, we explore a simple detection-based defense. While it
substantially reduces the attack success rate, we demonstrate that an adaptive
attacker can partially circumvent this defense. Our findings underscore the
need for greater attention and rigorous safeguards against prompt-injection
threats in AI-assisted peer review.

</details>


### [58] [FirstAidQA: A Synthetic Dataset for First Aid and Emergency Response in Low-Connectivity Settings](https://arxiv.org/abs/2511.01289)
*Saiyma Sittul Muna,Rezwan Islam Salvi,Mushfiqur Rahman Mushfique,Ajwad Abrar*

Main category: cs.CL

TL;DR: 本文介绍了FirstAidQA数据集，包含5,500个高质量急救和应急响应问答对，旨在支持轻量级语言模型在低连接环境下的开发。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在时间敏感、低连接环境中的部署受限，缺乏针对急救和应急响应的高质量数据集，阻碍了轻量级、领域特定解决方案的开发。

Method: 使用ChatGPT-4o-mini通过提示式上下文学习生成问答对，基于Vital First Aid Book (2019)文本，经过文本清理、上下文分块、过滤等预处理步骤，并进行人工验证确保准确性、安全性和实用性。

Result: 成功创建了包含5,500个高质量问答对的FirstAidQA数据集，涵盖广泛的急救和应急响应场景，已在Hugging Face平台公开发布。

Conclusion: FirstAidQA数据集将推动急救和应急响应领域安全关键、资源受限AI应用的研究，支持离线能力系统的开发。

Abstract: In emergency situations, every second counts. The deployment of Large
Language Models (LLMs) in time-sensitive, low or zero-connectivity environments
remains limited. Current models are computationally intensive and unsuitable
for low-tier devices often used by first responders or civilians. A major
barrier to developing lightweight, domain-specific solutions is the lack of
high-quality datasets tailored to first aid and emergency response. To address
this gap, we introduce FirstAidQA, a synthetic dataset containing 5,500
high-quality question answer pairs that encompass a wide range of first aid and
emergency response scenarios. The dataset was generated using a Large Language
Model, ChatGPT-4o-mini, with prompt-based in-context learning, using texts from
the Vital First Aid Book (2019). We applied preprocessing steps such as text
cleaning, contextual chunking, and filtering, followed by human validation to
ensure accuracy, safety, and practical relevance of the QA pairs. FirstAidQA is
designed to support instruction-tuning and fine-tuning of LLMs and Small
Language Models (SLMs), enabling faster, more reliable, and offline-capable
systems for emergency settings. We publicly release the dataset to advance
research on safety-critical and resource-constrained AI applications in first
aid and emergency response. The dataset is available on Hugging Face at
https://huggingface.co/datasets/i-am-mushfiq/FirstAidQA.

</details>


### [59] [DeepSpecs: Expert-Level Questions Answering in 5G](https://arxiv.org/abs/2511.01305)
*Aman Ganapathy Manvattira,Yifei Xu,Ziyue Dang,Songwu Lu*

Main category: cs.CL

TL;DR: DeepSpecs是一个增强的RAG系统，通过结构化和时序推理来回答5G技术规范问题，解决了现有方法无法可靠解析交叉引用和规范演进的问题。


<details>
  <summary>Details</summary>
Motivation: 现有RAG框架依赖语义相似性，无法可靠解析5G标准中的交叉引用或推理规范演进，而5G标准包含数千页交叉引用的规范文档且不断更新。

Method: 构建三个元数据丰富的数据库：SpecDB（条款对齐规范文本）、ChangeDB（行级版本差异）、TDocDB（标准化会议文档），通过元数据查找递归检索引用条款，挖掘变更并链接到记录设计原理的变更请求。

Result: 在多个LLM后端上，DeepSpecs优于基础模型和最先进的电信RAG系统；消融实验证实显式交叉引用解析和演进感知检索显著提高答案质量。

Conclusion: 建模5G标准的结构和时序特性对于提高问答系统性能具有重要价值。

Abstract: 5G technology enables mobile Internet access for billions of users. Answering
expert-level questions about 5G specifications requires navigating thousands of
pages of cross-referenced standards that evolve across releases. Existing
retrieval-augmented generation (RAG) frameworks, including telecom-specific
approaches, rely on semantic similarity and cannot reliably resolve
cross-references or reason about specification evolution. We present DeepSpecs,
a RAG system enhanced by structural and temporal reasoning via three
metadata-rich databases: SpecDB (clause-aligned specification text), ChangeDB
(line-level version diffs), and TDocDB (standardization meeting documents).
DeepSpecs explicitly resolves cross-references by recursively retrieving
referenced clauses through metadata lookup, and traces specification evolution
by mining changes and linking them to Change Requests that document design
rationale. We curate two 5G QA datasets: 573 expert-annotated real-world
questions from practitioner forums and educational resources, and 350
evolution-focused questions derived from approved Change Requests. Across
multiple LLM backends, DeepSpecs outperforms base models and state-of-the-art
telecom RAG systems; ablations confirm that explicit cross-reference resolution
and evolution-aware retrieval substantially improve answer quality,
underscoring the value of modeling the structural and temporal properties of 5G
standards.

</details>


### [60] [DEEPAMBIGQA: Ambiguous Multi-hop Questions for Benchmarking LLM Answer Completeness](https://arxiv.org/abs/2511.01323)
*Jiabao Ji,Min Li,Priyanshu Kumar,Shiyu Chang,Saloni Potdar*

Main category: cs.CL

TL;DR: 提出了DeepAmbigQAGen自动数据生成管道和DeepAmbigQA数据集，用于评估LLM在包含名称歧义和多步推理的复杂开放域问答任务中的表现，发现即使是GPT-5也难以提供完整答案。


<details>
  <summary>Details</summary>
Motivation: 现有QA基准很少同时评估名称歧义和多步推理两个挑战，而LLM在处理需要区分同名实体和跨大量实体推理的复杂问题时表现不佳。

Method: 开发了DeepAmbigQAGen自动数据生成管道，基于文本语料库和链接知识图谱构建QA任务，生成包含名称歧义和多步推理的自然可验证问题。

Result: 构建了包含3600个问题的DeepAmbigQA数据集，其中一半需要显式名称消歧。实验显示GPT-5在歧义问题上精确匹配率仅为0.13，非歧义问题为0.21。

Conclusion: 现有QA系统在信息收集和答案完整性方面仍需改进，需要更鲁棒的问答系统来处理复杂推理任务。

Abstract: Large language models (LLMs) with integrated search tools show strong promise
in open-domain question answering (QA), yet they often struggle to produce
complete answer set to complex questions such as Which actor from the film Heat
won at least one Academy Award?, which requires (1) distinguishing between
multiple films sharing the same title and (2) reasoning across a large set of
actors to gather and integrate evidence. Existing QA benchmarks rarely evaluate
both challenges jointly. To address this, we introduce DeepAmbigQAGen, an
automatic data generation pipeline that constructs QA tasks grounded in text
corpora and linked knowledge graph, generating natural and verifiable questions
that systematically embed name ambiguity and multi-step reasoning. Based on
this, we build DeepAmbigQA, a dataset of 3,600 questions requiring multi-hop
reasoning and half of them explicit name ambiguity resolving. Experiments
reveal that, even state-of-the-art GPT-5 show incomplete answers, achieving
only 0.13 exact match on ambiguous questions and 0.21 on non-ambiguous
questions. These findings highlight the need for more robust QA systems aimed
at information gathering and answer completeness.

</details>


### [61] [Thinking with DistilQwen: A Tale of Four Distilled Reasoning and Reward Model Series](https://arxiv.org/abs/2511.01354)
*Wenrui Cai,Chengyu Wang,Junbing Yan,Jun Huang,Xiangzhong Fang*

Main category: cs.CL

TL;DR: 扩展DistilQwen模型家族，推出四个专门满足工业需求的模型系列：慢思考模型、自适应思考模型和蒸馏奖励模型，在推理效率和性能间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 为支持现实应用，开发小型高效推理模型，平衡推理性能与推理速度，满足工业需求。

Method: 基于Qwen模型初始化，通过知识蒸馏技术开发四个模型系列：慢思考模型、自适应思考模型（两个系列）和蒸馏奖励模型。

Result: 在多个基准测试中展现出高推理效率和强推理性能，蒸馏奖励模型具有实际效用，支持在阿里云PAI平台上的可扩展训练和推理。

Conclusion: 这些模型为行业从业者提供了有效的解决方案，在推理效率和性能间取得了良好平衡，并支持工业级部署。

Abstract: Recently, the demand for small and efficient reasoning models to support
real-world applications has driven the development of knowledge distillation
techniques that balance reasoning performance and inference speed. In this
paper, we further extend the DistilQwen model family, initialized from the Qwen
models, by introducing four model series specifically designed to meet
industrial requirements. The distilled model collection comprises: (1)
slow-thinking models, optimized for reasoning tasks that require high accuracy;
(2) two series of adaptive-thinking models, which dynamically adjust reasoning
strategies based on input tasks to maximize efficiency across diverse
scenarios; and (3) distilled reward models, which enable further reinforcement
learning of reasoning models using distilled knowledge. Comprehensive
evaluations across multiple benchmarks demonstrate both high inference
efficiency and strong reasoning performance for these models, as well as the
practical utility of distilled reward models. We further show that these models
support industry practitioners by providing scalable training and inference
functionalities on the Alibaba Cloud PAI (Platform for Artificial Intelligence)
platform.

</details>


### [62] [PrefixNLI: Detecting Factual Inconsistencies as Soon as They Arise](https://arxiv.org/abs/2511.01359)
*Sapir Harary,Eran Hirsch,Aviv Slobodkin,David Wan,Mohit Bansal,Ido Dagan*

Main category: cs.CL

TL;DR: 该论文提出了一种改进LLM输出事实性的新方法，通过训练专门检测文本前缀事实不一致性的模型MiniTruePrefixes，并将其集成到受控解码框架中，显著提高了摘要生成的事实一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的NLI模型虽然能检测完整句子的事实不一致性，但在自回归生成架构中，决策是在解码过程中针对不断演变的文本前缀进行的。因此需要专门针对文本前缀的蕴含检测模型来改进生成的事实性。

Method: 1. 将蕴含检测任务推广到任意文本前缀；2. 提供相应的评估和训练数据集；3. 训练专门的MiniTruePrefixes模型来检测文本前缀的事实不一致性；4. 将该模型集成到受控解码框架中。

Result: MiniTruePrefixes在前缀级蕴含检测上比基线NLI模型高出5-14个F1分数。在摘要生成任务中，LLaMA-3.2-3B-Instruct在MiniTruePrefixes指导下，其事实性和运行时表现与同系列8B模型相当，但只使用一半内存。

Conclusion: 专门针对文本前缀训练的蕴含检测模型能有效提高LLM生成的事实一致性，同时保持计算效率，为改进生成模型的事实性提供了有效途径。

Abstract: Natural Language Inference (NLI) models have been used in various ways to
improve the factuality of LLM outputs. This is typically done by applying an
NLI model to judge whether the model output is entailed from the supposed
evidence, triggering some corrective actions, such as beam reranking at
inference time or RL rewards during training. While NLI models are trained to
detect factual inconsistencies over complete sentences, decisions in the common
autoregressive generation architecture are made for each evolving text prefix,
during decoding. Addressing this setting, we generalize the entailment
detection task to apply over arbitrary text prefixes, and suggest its utility
for improving generation faithfulness. Providing suitable evaluation and
training datasets for this task, we train MiniTruePrefixes, a novel specialized
model that better detects factual inconsistencies over text prefixes,
outperforming comparable baseline NLI models by 5-14 F1 points in prefix-level
entailment. We further demonstrate that integrating MiniTruePrefixes into a
controlled decoding framework substantially improves factual consistency in
abstractive summarization. When guided by MiniTruePrefixes,
LLaMA-3.2-3B-Instruct matches the faithfulness and runtime of the 8B model from
the same model family, while using only half the memory.

</details>


### [63] [Safer in Translation? Presupposition Robustness in Indic Languages](https://arxiv.org/abs/2511.01360)
*Aadi Palnitkar,Arjun Suresh,Rishi Rajesh,Puneet Puli*

Main category: cs.CL

TL;DR: 开发了Cancer-Myth-Indic基准，将500个癌症相关错误预设项目翻译成5种印度语言，用于评估多语言LLM在医疗咨询中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有医疗基准几乎都是英文的，缺乏多语言LLM评估，特别是对于广泛使用的印度语言。

Method: 从Cancer-Myth基准中均匀采样500个项目，由母语译者按照风格指南翻译成5种印度语言，保留隐含预设。

Result: 创建了包含2500个翻译项目的多语言基准，用于在预设压力下评估流行LLM。

Conclusion: 这项工作填补了多语言LLM医疗评估的空白，为评估LLM在印度语言中的医疗咨询准确性提供了工具。

Abstract: Increasingly, more and more people are turning to large language models
(LLMs) for healthcare advice and consultation, making it important to gauge the
efficacy and accuracy of the responses of LLMs to such queries. While there are
pre-existing medical benchmarks literature which seeks to accomplish this very
task, these benchmarks are almost universally in English, which has led to a
notable gap in existing literature pertaining to multilingual LLM evaluation.
Within this work, we seek to aid in addressing this gap with Cancer-Myth-Indic,
an Indic language benchmark built by translating a 500-item subset of
Cancer-Myth, sampled evenly across its original categories, into five
under-served but widely used languages from the subcontinent (500 per language;
2,500 translated items total). Native-speaker translators followed a style
guide for preserving implicit presuppositions in translation; items feature
false presuppositions relating to cancer. We evaluate several popular LLMs
under this presupposition stress.

</details>


### [64] [The Ouroboros of Benchmarking: Reasoning Evaluation in an Era of Saturation](https://arxiv.org/abs/2511.01365)
*İbrahim Ethem Deveci,Duygu Ataman*

Main category: cs.CL

TL;DR: 该论文质疑当前大语言模型基准测试的有效性，分析三大模型家族在推理基准上的表现趋势，讨论基准测试面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力的快速提升，现有基准测试结果趋于饱和，需要探究超越基准是否真正反映推理能力，还是仅仅在追踪与声称能力脱节的数字。

Method: 对OpenAI、Anthropic和Google三大模型家族进行纵向研究，分析它们在不同推理基准上的表现演变趋势，并考察不同推理任务的性能变化。

Result: 研究发现基准测试结果存在饱和现象，模型性能提升可能源于规模扩展和训练创新，而非真正推理能力的提升，许多基准数据集可能已被纳入训练数据。

Conclusion: 当前基准测试面临严峻挑战，需要更有效的评估方法来衡量模型的真实推理能力，本文为未来推理评估和模型开发提供了基础参考。

Abstract: The rapid rise of Large Language Models (LLMs) and Large Reasoning Models
(LRMs) has been accompanied by an equally rapid increase of benchmarks used to
assess them. However, due to both improved model competence resulting from
scaling and novel training advances as well as likely many of these datasets
being included in pre or post training data, results become saturated, driving
a continuous need for new and more challenging replacements. In this paper, we
discuss whether surpassing a benchmark truly demonstrates reasoning ability or
are we simply tracking numbers divorced from the capabilities we claim to
measure? We present an investigation focused on three model families, OpenAI,
Anthropic, and Google, and how their reasoning capabilities across different
benchmarks evolve over the years. We also analyze performance trends over the
years across different reasoning tasks and discuss the current situation of
benchmarking and remaining challenges. By offering a comprehensive overview of
benchmarks and reasoning tasks, our work aims to serve as a first reference to
ground future research in reasoning evaluation and model development.

</details>


### [65] [Confounding Factors in Relating Model Performance to Morphology](https://arxiv.org/abs/2511.01380)
*Wessel Poelman,Thomas Bauwens,Miryam de Lhoneux*

Main category: cs.CL

TL;DR: 本文重新评估了语言形态特征对分词和语言建模的影响，指出先前研究存在混淆因素，并提出了基于token二元组的内在指标来预测语言建模难度。


<details>
  <summary>Details</summary>
Motivation: 现有关于形态系统对语言建模影响的证据相互矛盾，作者认为这是由于实验设置中的混淆因素导致的，难以比较结果和得出结论。

Method: 识别分析中的混淆因素，重新评估Arnett & Bergen (2025)的三个假设，引入token二元组指标作为预测因果语言建模难度的内在方法。

Result: 发现先前关于黏着语比融合语更难建模的结论都包含混淆因素，token二元组指标可以作为形态复杂性的梯度代理，无需专家标注。

Conclusion: 为可靠回答形态学如何影响语言建模的问题，需要更严谨的实验设计和评估方法。

Abstract: The extent to which individual language characteristics influence
tokenization and language modeling is an open question. Differences in
morphological systems have been suggested as both unimportant and crucial to
consider (Cotterell et al., 2018; Gerz et al., 2018a; Park et al., 2021, inter
alia). We argue this conflicting evidence is due to confounding factors in
experimental setups, making it hard to compare results and draw conclusions. We
identify confounding factors in analyses trying to answer the question of
whether, and how, morphology relates to language modeling. Next, we re-assess
three hypotheses by Arnett & Bergen (2025) for why modeling agglutinative
languages results in higher perplexities than fusional languages: they look at
morphological alignment of tokenization, tokenization efficiency, and dataset
size. We show that each conclusion includes confounding factors. Finally, we
introduce token bigram metrics as an intrinsic way to predict the difficulty of
causal language modeling, and find that they are gradient proxies for
morphological complexity that do not require expert annotation. Ultimately, we
outline necessities to reliably answer whether, and how, morphology relates to
language modeling.

</details>


### [66] [RAGSmith: A Framework for Finding the Optimal Composition of Retrieval-Augmented Generation Methods Across Datasets](https://arxiv.org/abs/2511.01386)
*Muhammed Yusuf Kartal,Suha Kagan Kose,Korhan Sevinç,Burak Aktas*

Main category: cs.CL

TL;DR: RAGSmith是一个模块化框架，通过遗传搜索在46,080种可行配置中优化RAG系统，在6个维基百科领域平均提升3.8%性能，发现向量检索加后生成反思的稳健架构。


<details>
  <summary>Details</summary>
Motivation: RAG质量受多个模块相互影响，孤立优化不够稳健，需要端到端的架构搜索方法。

Method: 引入RAGSmith框架，使用遗传搜索在9个技术家族和46,080种配置中优化，联合聚合检索指标和生成指标。

Result: 在6个领域平均优于基线3.8%，检索提升最高12.5%，生成提升最高7.5%，搜索仅探索约0.2%空间。

Conclusion: 为构建有效RAG系统提供实用指导，证明进化搜索在全流程优化中的效用。

Abstract: Retrieval-Augmented Generation (RAG) quality depends on many interacting
choices across retrieval, ranking, augmentation, prompting, and generation, so
optimizing modules in isolation is brittle. We introduce RAGSmith, a modular
framework that treats RAG design as an end-to-end architecture search over nine
technique families and 46{,}080 feasible pipeline configurations. A genetic
search optimizes a scalar objective that jointly aggregates retrieval metrics
(recall@k, mAP, nDCG, MRR) and generation metrics (LLM-Judge and semantic
similarity). We evaluate on six Wikipedia-derived domains (Mathematics, Law,
Finance, Medicine, Defense Industry, Computer Science), each with 100 questions
spanning factual, interpretation, and long-answer types. RAGSmith finds
configurations that consistently outperform naive RAG baseline by +3.8\% on
average (range +1.2\% to +6.9\% across domains), with gains up to +12.5\% in
retrieval and +7.5\% in generation. The search typically explores $\approx
0.2\%$ of the space ($\sim 100$ candidates) and discovers a robust backbone --
vector retrieval plus post-generation reflection/revision -- augmented by
domain-dependent choices in expansion, reranking, augmentation, and prompt
reordering; passage compression is never selected. Improvement magnitude
correlates with question type, with larger gains on factual/long-answer mixes
than interpretation-heavy sets. These results provide practical, domain-aware
guidance for assembling effective RAG systems and demonstrate the utility of
evolutionary search for full-pipeline optimization.

</details>


### [67] [LiveSearchBench: An Automatically Constructed Benchmark for Retrieval and Reasoning over Dynamic Knowledge](https://arxiv.org/abs/2511.01409)
*Heng Zhou,Ao Yu,Yuchen Fan,Jianing Shi,Li Kang,Hejia Geng,Yongting Zhang,Yutao Fan,Yuhao Wu,Tiancheng He,Yiran Qin,Lei Bai,Zhenfei Yin*

Main category: cs.CL

TL;DR: LiveSearchBench是一个自动化流水线，用于从最新知识更新构建依赖检索的基准测试，通过分析Wikidata快照差异来生成具有时效性的问答任务。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估基准主要依赖静态数据集，奖励记忆而非检索能力，无法捕捉世界知识的动态变化。

Method: 计算Wikidata连续快照的差异，筛选高质量三元组，在三个推理难度级别合成自然语言问题，并通过SPARQL验证确保答案的唯一性和可验证性。

Result: 实验显示模型在面对预训练后出现的事实时性能显著下降，多跳查询的差距最明显。检索增强方法和更大的指令调优模型只能部分缓解这种时效性差距。

Conclusion: LiveSearchBench将评估从静态记忆转向需要最新检索和推理能力的任务，为系统化长期评估LLM在演化知识下的表现提供了基础。

Abstract: Evaluating large language models (LLMs) on question answering often relies on
static benchmarks that reward memorization and understate the role of
retrieval, failing to capture the dynamic nature of world knowledge. We present
LiveSearchBench, an automated pipeline for constructing retrieval-dependent
benchmarks from recent knowledge updates. Our method computes deltas between
successive Wikidata snapshots, filters candidate triples for quality, and
synthesizes natural-language questions at three levels of reasoning difficulty,
each guaranteed to admit a unique, verifiable answer through SPARQL validation.
The pipeline is fully automated, scalable across time, and minimizes human
intervention, enabling continual regeneration of temporally grounded
benchmarks. Experiments show a pronounced performance drop when models confront
facts that post-date pretraining, with the gap most salient on multi-hop
queries. Retrieval augmented methods and larger, instruction-tuned models
provide partial gains but fail to close this recency gap. By design,
LiveSearchBench shifts evaluation from static memorization toward tasks that
require up-to-date retrieval and reasoning, offering a foundation for
systematic, long-term assessment of LLMs under evolving knowledge.

</details>


### [68] ["Don't Teach Minerva": Guiding LLMs Through Complex Syntax for Faithful Latin Translation with RAG](https://arxiv.org/abs/2511.01454)
*Sergio Torres Aguilar*

Main category: cs.CL

TL;DR: 提出了一种可复现的草稿优化流程，使用开源大语言模型在拉丁语翻译任务上达到与顶级专有系统相当的性能水平。


<details>
  <summary>Details</summary>
Motivation: 翻译形态丰富、资源匮乏的语言（如拉丁语）面临重大挑战，需要开发能够与专有系统竞争的开源解决方案。

Method: 首先使用微调的NLLB-1.3B模型生成高质量、结构忠实的草稿，然后通过零样本LLM（Llama-3.3或Qwen3）进行优化，并可结合检索增强生成（RAG）技术。

Result: 该开源RAG系统在两个不同基准测试中（标准域内测试集和新的域外12世纪拉丁语信件集）表现出与GPT-5基线统计相当的性能，无需特定任务的LLM微调。

Conclusion: 该方法为低资源语言翻译提供了一种可复现的解决方案，并发布了完整的流程、数据集和评估工具以促进进一步研究。

Abstract: Translating a morphology-rich, low-resource language like Latin poses
significant challenges. This paper introduces a reproducible draft-based
refinement pipeline that elevates open-source Large Language Models (LLMs) to a
performance level statistically comparable to top-tier proprietary systems. Our
method first uses a fine-tuned NLLB-1.3B model to generate a high-quality,
structurally faithful draft. A zero-shot LLM (Llama-3.3 or Qwen3) then polishes
this draft, a process that can be further enhanced by augmenting the context
with retrieved out-context examples (RAG). We demonstrate the robustness of
this approach on two distinct benchmarks: a standard in-domain test set
(Rosenthal, 2023) and a new, challenging out-of-domain (OOD) set of
12th-century Latin letters (2025). Our central finding is that this open-source
RAG system achieves performance statistically comparable to the GPT-5 baseline,
without any task-specific LLM fine-tuning. We release the pipeline, the
Chartres OOD set, and evaluation scripts and models to facilitate replicability
and further research.

</details>


### [69] [BARD: budget-aware reasoning distillation](https://arxiv.org/abs/2511.01470)
*Lujie Niu,Lei Shen,Yi Jiang,Caixia Yuan,Xiaojie Wang,Wenbo Su,Bo zheng*

Main category: cs.CL

TL;DR: 提出BARD框架，通过预算控制信号同时蒸馏推理能力并精细控制推理长度，实现推理性能与计算效率的动态平衡


<details>
  <summary>Details</summary>
Motivation: 解决长思维链蒸馏中推理过程冗余且计算预算不可控的问题，提高资源使用效率

Method: 采用两阶段训练：第一阶段在教师生成的长CoT数据上进行监督微调，第二阶段使用强化学习同时优化推理性能和预算保真度

Result: 8B学生模型在AIME24、AIME25、GPQA等推理基准上表现优异，并能跨广泛预算范围精确自适应控制推理长度

Conclusion: BARD框架成功实现了推理能力蒸馏与推理长度控制的统一，为高效推理提供了有效解决方案

Abstract: While long Chain-of-Thought (CoT) distillation effectively transfers
reasoning capability to smaller language models, the reasoning process often
remains redundant and computational budget uncontrollable, leading to
inefficient resource usage. To address this limitation, we propose
\textbf{Budget-Aware Reasoning Distillation (BARD)}, a novel framework that
simultaneously distills reasoning capability and enables fine-grained control
over the reasoning length. BARD uses the thinking budget as a user-specified
control signal, allowing the model to dynamically balance reasoning performance
and computational efficiency. To achieve this concept, BARD introduces a
two-phase training regimen. The first phase, Supervised Fine-Tuning (SFT) on
teacher-generated long CoT data compressed to various budget levels,
bootstrapping the model's understanding of budget constraints. The second phase
leverages Reinforcement Learning (RL) from a reward signal in consideration of
reasoning performance and budget fidelity simultaneously. Incorporating the
two-phase regimen is crucial to avoiding policy degradation and ensuring that
both objectives are optimized jointly. Extensive experiments demonstrate that
our method empowers an 8B student model to achieve strong performance on
challenging reasoning benchmarks (\textit{AIME24, AIME25, GPQA}) while
providing precise and adaptive control over its reasoning length across a wide
range of budgets.

</details>


### [70] [Towards Consistent Detection of Cognitive Distortions: LLM-Based Annotation and Dataset-Agnostic Evaluation](https://arxiv.org/abs/2511.01482)
*Neha Sharma,Navneet Agarwal,Kairit Sirts*

Main category: cs.CL

TL;DR: 使用大语言模型作为认知扭曲检测的标注工具，提出多轮独立LLM标注可产生稳定标签模式，并引入数据集无关的评估框架进行公平比较。


<details>
  <summary>Details</summary>
Motivation: 文本认知扭曲检测具有主观性，人类标注者间一致性低导致不可靠标注，需要寻找更一致的标注方法。

Method: 使用LLM作为标注器，通过多轮独立标注获得稳定标签；引入基于Cohen's kappa的数据集无关评估框架。

Result: GPT-4能产生一致性标注（Fleiss's Kappa=0.78），基于LLM标注训练的模型在测试集上表现优于基于人类标注训练的模型。

Conclusion: LLM可为主观NLP任务提供可扩展且内部一致的训练数据生成替代方案。

Abstract: Text-based automated Cognitive Distortion detection is a challenging task due
to its subjective nature, with low agreement scores observed even among expert
human annotators, leading to unreliable annotations. We explore the use of
Large Language Models (LLMs) as consistent and reliable annotators, and propose
that multiple independent LLM runs can reveal stable labeling patterns despite
the inherent subjectivity of the task. Furthermore, to fairly compare models
trained on datasets with different characteristics, we introduce a
dataset-agnostic evaluation framework using Cohen's kappa as an effect size
measure. This methodology allows for fair cross-dataset and cross-study
comparisons where traditional metrics like F1 score fall short. Our results
show that GPT-4 can produce consistent annotations (Fleiss's Kappa = 0.78),
resulting in improved test set performance for models trained on these
annotations compared to those trained on human-labeled data. Our findings
suggest that LLMs can offer a scalable and internally consistent alternative
for generating training data that supports strong downstream performance in
subjective NLP tasks.

</details>


### [71] [Synthetic Eggs in Many Baskets: The Impact of Synthetic Data Diversity on LLM Fine-Tuning](https://arxiv.org/abs/2511.01490)
*Max Schaffelder,Albert Gatt*

Main category: cs.CL

TL;DR: 研究探讨了合成数据来源多样性对微调大语言模型的影响，重点关注分布坍塌、对抗鲁棒性和自偏好偏差三个维度。


<details>
  <summary>Details</summary>
Motivation: 随着合成数据在语言模型开发中的广泛应用，理解其对模型行为的影响变得至关重要。

Method: 通过在不同来源的合成数据上微调大语言模型，分析其对分布坍塌、对抗鲁棒性和自偏好偏差的影响。

Result: 使用多样来源的合成数据微调可以缓解分布坍塌，保持输出分布的广度和文本多样性；合成数据微调在移除安全防护的同时保持更高的输出质量；微调能减少自偏好偏差，人类数据最有效，多源合成数据次之。

Conclusion: 合成数据来源的多样性对微调模型的性能有重要影响，多样来源的合成数据有助于保持模型输出的多样性，同时需要警惕合成数据在移除安全防护时可能带来的风险。

Abstract: As synthetic data becomes widely used in language model development,
understanding its impact on model behavior is crucial. This paper investigates
the impact of the diversity of sources of synthetic data on fine-tuned large
language models. We focus on three key dimensions: distribution collapse,
adversarial robustness, and self-preference bias. Our findings reveal that
fine-tuning models on synthetic data from diverse sources can mitigate
distribution collapse, preserving the breadth of the output distribution and
the diversity of the output text. Furthermore, while both human and synthetic
fine-tuning data can remove safeguards, the latter preserves higher output
quality, thus making outputs potentially more usable and dangerous. Finally,
fine-tuning reduces self-preference bias, with human data being the most
effective, followed by multi-source synthetic data.

</details>


### [72] [BanglaNirTox: A Large-scale Parallel Corpus for Explainable AI in Bengali Text Detoxification](https://arxiv.org/abs/2511.01512)
*Ayesha Afroza Mohsin,Mashrur Ahsan,Nafisa Maliyat,Shanta Maria,Syed Rifat Raiyan,Hasan Mahmud,Md Kamrul Hasan*

Main category: cs.CL

TL;DR: 提出了一种结合帕累托优化大语言模型和思维链提示的孟加拉语文本去毒新方法，并构建了包含68,041个有毒句子的BanglaNirTox数据集。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语中的有毒语言在在线环境中普遍存在，但由于资源有限，该语言的文本去毒研究仍未被充分探索。

Method: 使用帕累托优化的大语言模型和思维链提示生成去毒句子，构建人工生成的平行语料库BanglaNirTox，并用该数据集微调语言模型。

Result: 帕累托优化的大语言模型结合思维链提示显著提高了孟加拉语文本去毒的质量和一致性。

Conclusion: 该方法为孟加拉语文本去毒提供了有效解决方案，BanglaNirTox数据集将促进该领域的进一步研究。

Abstract: Toxic language in Bengali remains prevalent, especially in online
environments, with few effective precautions against it. Although text
detoxification has seen progress in high-resource languages, Bengali remains
underexplored due to limited resources. In this paper, we propose a novel
pipeline for Bengali text detoxification that combines Pareto class-optimized
large language models (LLMs) and Chain-of-Thought (CoT) prompting to generate
detoxified sentences. To support this effort, we construct BanglaNirTox, an
artificially generated parallel corpus of 68,041 toxic Bengali sentences with
class-wise toxicity labels, reasonings, and detoxified paraphrases, using
Pareto-optimized LLMs evaluated on random samples. The resulting BanglaNirTox
dataset is used to fine-tune language models to produce better detoxified
versions of Bengali sentences. Our findings show that Pareto-optimized LLMs
with CoT prompting significantly enhance the quality and consistency of Bengali
text detoxification.

</details>


### [73] [Difficulty-Controllable Cloze Question Distractor Generation](https://arxiv.org/abs/2511.01526)
*Seokhoon Kang,Yejin Jeon,Seonjeong Hwang,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 提出了一种通过数据增强和多任务学习生成可控难度干扰项的新框架，显著优于GPT-4o在干扰项难度与人类感知对齐方面的表现。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在生成高质量干扰项时缺乏适应性和难度控制能力的问题，以及缺乏难度标注数据集的限制。

Method: 采用两阶段方法：1）通过双向干扰项生成过程创建难度标注数据集；2）利用多任务学习训练难度可控的生成模型，包含增强语义理解和难度估计能力的辅助任务。

Result: 实验结果显示该方法能生成跨难度级别的高质量干扰项，在干扰项难度与人类感知对齐方面大幅优于GPT-4o。

Conclusion: 该框架成功解决了干扰项生成的难度控制问题，为语言能力评估提供了更有效的工具。

Abstract: Multiple-choice cloze questions are commonly used to assess linguistic
proficiency and comprehension. However, generating high-quality distractors
remains challenging, as existing methods often lack adaptability and control
over difficulty levels, and the absence of difficulty-annotated datasets
further hinders progress. To address these issues, we propose a novel framework
for generating distractors with controllable difficulty by leveraging both data
augmentation and a multitask learning strategy. First, to create a
high-quality, difficulty-annotated dataset, we introduce a two-way distractor
generation process in order to produce diverse and plausible distractors. These
candidates are subsequently refined through filtering and then categorized by
difficulty using an ensemble QA system. Second, this newly created dataset is
leveraged to train a difficulty-controllable generation model via multitask
learning. The framework includes carefully designed auxiliary tasks that
enhance the model's semantic understanding of distractors and its ability to
estimate their difficulty. Experimental results demonstrate that our method
generates high-quality distractors across difficulty levels and substantially
outperforms GPT-4o in aligning distractor difficulty with human perception.

</details>


### [74] [Math anxiety and associative knowledge structure are entwined in psychology students but not in Large Language Models like GPT-3.5 and GPT-4o](https://arxiv.org/abs/2511.01558)
*Luciana Ciringione,Emma Franchino,Simone Reigl,Isaia D'Onofrio,Anna Serbati,Oleksandra Poquet,Florence Gabriel,Massimo Stella*

Main category: cs.CL

TL;DR: 本研究使用行为心智网络框架分析心理学大学生对数学和焦虑概念的认知关联，发现学生群体中"焦虑"的正向评价和高网络度，以及"数学"的负向评价能预测数学焦虑水平，但这些模式在GPT模拟学生中不适用。


<details>
  <summary>Details</summary>
Motivation: 数学焦虑严重影响心理学大学生的职业选择和心理健康，需要深入了解学生对相关概念的认知结构和情感感知差异。

Method: 采用行为心智网络框架，通过4个实验比较真实心理学本科生（n=127）与GPT模拟学生（GPT-3.5: n=300; GPT-4o: n=300）的认知网络特征，分析个体和群体层面的概念关联模式。

Result: 在学生中，"焦虑"的正向评价和高网络度，以及"数学"的负向评价能预测更高的数学焦虑水平，特别是评价性焦虑。但GPT模拟数据中这些模型不适用，因为模拟网络和心理测量分数与人类存在差异。

Conclusion: 概念感知和关联方式在理解和管理学生数学焦虑中具有重要作用，高数学焦虑学生对"焦虑"的情感极化框架和"数学"的负面感知是重要特征。

Abstract: Math anxiety poses significant challenges for university psychology students,
affecting their career choices and overall well-being. This study employs a
framework based on behavioural forma mentis networks (i.e. cognitive models
that map how individuals structure their associative knowledge and emotional
perceptions of concepts) to explore individual and group differences in the
perception and association of concepts related to math and anxiety. We
conducted 4 experiments involving psychology undergraduates from 2 samples (n1
= 70, n2 = 57) compared against GPT-simulated students (GPT-3.5: n2 = 300;
GPT-4o: n4 = 300). Experiments 1, 2, and 3 employ individual-level network
features to predict psychometric scores for math anxiety and its facets
(observational, social and evaluational) from the Math Anxiety Scale.
Experiment 4 focuses on group-level perceptions extracted from human students,
GPT-3.5 and GPT-4o's networks. Results indicate that, in students, positive
valence ratings and higher network degree for "anxiety", together with negative
ratings for "math", can predict higher total and evaluative math anxiety. In
contrast, these models do not work on GPT-based data because of differences in
simulated networks and psychometric scores compared to humans. These results
were also reconciled with differences found in the ways that high/low subgroups
of simulated and real students framed semantically and emotionally STEM
concepts. High math-anxiety students collectively framed "anxiety" in an
emotionally polarising way, absent in the negative perception of low
math-anxiety students. "Science" was rated positively, but contrasted against
the negative perception of "math". These findings underscore the importance of
understanding concept perception and associations in managing students' math
anxiety.

</details>


### [75] [ECO Decoding: Entropy-Based Control for Controllability and Fluency in Controllable Dialogue Generation](https://arxiv.org/abs/2511.01568)
*Seungmin Shin,Dooyoung Kim,Youngjoong Ko*

Main category: cs.CL

TL;DR: 提出ECO解码方法，通过基于熵的动态控制强度调整，在保持流畅性的同时提升对话生成的可控性


<details>
  <summary>Details</summary>
Motivation: 现有加权解码方法使用固定常数控制属性概率偏差，难以找到同时满足可控性和流畅性的理想控制强度

Method: ECO解码根据语言模型和属性分类器概率分布的熵，在每个生成步骤动态调整控制强度

Result: 在DailyDialog和MultiWOZ数据集上的实验表明，ECO解码在保持流畅性和语法性的同时持续提升可控性，优于现有解码方法

Conclusion: ECO解码缓解了多属性生成中的概率插值问题，在单属性和多属性场景下均表现出色

Abstract: Controllable Dialogue Generation (CDG) enables chatbots to generate responses
with desired attributes, and weighted decoding methods have achieved
significant success in the CDG task. However, using a fixed constant value to
manage the bias of attribute probabilities makes it challenging to find an
ideal control strength that satisfies both controllability and fluency. To
address this issue, we propose ECO decoding (Entropy-based COntrol), which
dynamically adjusts the control strength at each generation step according to
the model's entropy in both the language model and attribute classifier
probability distributions. Experiments on the DailyDialog and MultiWOZ datasets
demonstrate that ECO decoding consistently improves controllability while
maintaining fluency and grammaticality, outperforming prior decoding methods
across various models and settings. Furthermore, ECO decoding alleviates
probability interpolation issues in multi-attribute generation and consequently
demonstrates strong performance in both single and multi-attribute scenarios.

</details>


### [76] [BIRD: Bronze Inscription Restoration and Dating](https://arxiv.org/abs/2511.01589)
*Wenjie Hua,Hoang H. Nguyen,Gangyan Ge*

Main category: cs.CL

TL;DR: BIRD数据集为早期中国青铜器铭文提供了标准化转录和年代标签，提出了结合字形网络的全字掩码语言建模框架，提升铭文修复和断代效果。


<details>
  <summary>Details</summary>
Motivation: 早期中国青铜器铭文存在碎片化和断代困难的问题，需要系统化的数据集和建模方法来支持学术研究。

Method: 提出全字掩码语言建模框架，整合领域自适应预训练和字形网络(GN)，连接字素和异体字，并采用字形偏置采样。

Result: 实验表明字形网络改进了铭文修复效果，字形偏置采样在断代任务中取得了增益。

Conclusion: BIRD数据集和字形网络框架为青铜器铭文研究提供了有效的计算工具，在修复和断代任务中表现出色。

Abstract: Bronze inscriptions from early China are fragmentary and difficult to date.
We introduce BIRD(Bronze Inscription Restoration and Dating), a fully encoded
dataset grounded in standard scholarly transcriptions and chronological labels.
We further propose an allograph-aware masked language modeling framework that
integrates domain- and task-adaptive pretraining with a Glyph Net (GN), which
links graphemes and allographs. Experiments show that GN improves restoration,
while glyph-biased sampling yields gains in dating.

</details>


### [77] [Imperfect Language, Artificial Intelligence, and the Human Mind: An Interdisciplinary Approach to Linguistic Errors in Native Spanish Speakers](https://arxiv.org/abs/2511.01615)
*Francisco Portillo López*

Main category: cs.CL

TL;DR: 本研究通过分析西班牙语母语者的语言错误，探讨大语言模型对这些错误的解释、复制和纠正能力，旨在开发更符合人类认知的NLP系统。


<details>
  <summary>Details</summary>
Motivation: 语言错误不仅是语法偏差，更是理解语言认知架构和揭示人工智能系统局限性的窗口。研究旨在分析LLM如何解释和处理真实的人类语言错误。

Method: 整合理论语言学、神经语言学和自然语言处理三个视角，构建包含500+西班牙语母语者真实错误的语料库，并在GPT、Gemini等AI模型上进行测试评估。

Result: 通过实证分析评估AI模型对语言错误的解释准确性和对人类语言行为模式的泛化能力。

Conclusion: 该研究不仅有助于理解西班牙语作为母语的特点，还能推动开发更符合认知、能够处理人类语言不完美性和模糊性的NLP系统。

Abstract: Linguistic errors are not merely deviations from normative grammar; they
offer a unique window into the cognitive architecture of language and expose
the current limitations of artificial systems that seek to replicate them. This
project proposes an interdisciplinary study of linguistic errors produced by
native Spanish speakers, with the aim of analyzing how current large language
models (LLM) interpret, reproduce, or correct them. The research integrates
three core perspectives: theoretical linguistics, to classify and understand
the nature of the errors; neurolinguistics, to contextualize them within
real-time language processing in the brain; and natural language processing
(NLP), to evaluate their interpretation against linguistic errors. A
purpose-built corpus of authentic errors of native Spanish (+500) will serve as
the foundation for empirical analysis. These errors will be tested against AI
models such as GPT or Gemini to assess their interpretative accuracy and their
ability to generalize patterns of human linguistic behavior. The project
contributes not only to the understanding of Spanish as a native language but
also to the development of NLP systems that are more cognitively informed and
capable of engaging with the imperfect, variable, and often ambiguous nature of
real human language.

</details>


### [78] [ParlaSpeech 3.0: Richly Annotated Spoken Parliamentary Corpora of Croatian, Czech, Polish, and Serbian](https://arxiv.org/abs/2511.01619)
*Nikola Ljubešić,Peter Rupnik,Ivan Porupski,Taja Kuzman Pungeršek*

Main category: cs.CL

TL;DR: ParlaSpeech是一个包含克罗地亚语、捷克语、波兰语和塞尔维亚语四种斯拉夫语言的议会语音语料库，总时长6000小时，通过自动方式从ParlaMint转录本构建，并丰富了多种自动标注层。


<details>
  <summary>Details</summary>
Motivation: 构建一个多语言的议会语音语料库，为跨学科研究提供丰富的语音和文本数据资源。

Method: 从ParlaMint转录本自动对齐议会录音，并添加多种自动标注层，包括语言注释、情感预测、填充停顿检测、词级对齐和重音位置标注。

Result: 创建了包含6000小时语音的四种斯拉夫语言语料库，显著增强了语料库的实用性，并通过情感声学相关性分析展示了其应用价值。

Conclusion: ParlaSpeech语料库通过丰富的自动标注大大提升了其研究价值，为多学科研究提供了重要资源，并以JSONL、TextGrid格式和检索工具形式开放使用。

Abstract: ParlaSpeech is a collection of spoken parliamentary corpora currently
spanning four Slavic languages - Croatian, Czech, Polish and Serbian - all
together 6 thousand hours in size. The corpora were built in an automatic
fashion from the ParlaMint transcripts and their corresponding metadata, which
were aligned to the speech recordings of each corresponding parliament. In this
release of the dataset, each of the corpora is significantly enriched with
various automatic annotation layers. The textual modality of all four corpora
has been enriched with linguistic annotations and sentiment predictions.
Similar to that, their spoken modality has been automatically enriched with
occurrences of filled pauses, the most frequent disfluency in typical speech.
Two out of the four languages have been additionally enriched with detailed
word- and grapheme-level alignments, and the automatic annotation of the
position of primary stress in multisyllabic words. With these enrichments, the
usefulness of the underlying corpora has been drastically increased for
downstream research across multiple disciplines, which we showcase through an
analysis of acoustic correlates of sentiment. All the corpora are made
available for download in JSONL and TextGrid formats, as well as for search
through a concordancer.

</details>


### [79] [A Graph-based RAG for Energy Efficiency Question Answering](https://arxiv.org/abs/2511.01643)
*Riccardo Campi,Nicolò Oreste Pinciroli Vago,Mathyas Giudici,Pablo Barrachina Rodriguez-Guisado,Marco Brambilla,Piero Fraternali*

Main category: cs.CL

TL;DR: 该研究探索了在图基检索增强生成架构中使用大语言模型进行能效问答，通过从能源文档自动提取知识图谱，在多语言环境中提供准确答案，验证结果显示系统在约75%的情况下能正确回答问题。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决能源效率领域的专业问答需求，利用大语言模型结合知识图谱技术，提供多语言、准确的能效信息咨询服务。

Method: 系统首先从能源指导和监管文档中自动提取知识图谱，然后基于该图谱进行导航和推理，采用RAGAs框架进行人工验证，使用包含101个问答对的验证数据集和领域专家评估。

Result: 验证结果显示系统在约75.2±2.7%的情况下能正确回答问题，在通用能效问题上表现更好（达81.0±4.1%），多语言能力表现良好（翻译导致的准确率损失仅为4.4%）。

Conclusion: 该架构在能效问答领域具有潜力，能够有效处理多语言查询，但在某些特定问题上仍有改进空间，为基于知识图谱的问答系统提供了有价值的参考。

Abstract: In this work, we investigate the use of Large Language Models (LLMs) within a
graph-based Retrieval Augmented Generation (RAG) architecture for Energy
Efficiency (EE) Question Answering. First, the system automatically extracts a
Knowledge Graph (KG) from guidance and regulatory documents in the energy
field. Then, the generated graph is navigated and reasoned upon to provide
users with accurate answers in multiple languages. We implement a human-based
validation using the RAGAs framework properties, a validation dataset
comprising 101 question-answer pairs, and domain experts. Results confirm the
potential of this architecture and identify its strengths and weaknesses.
Validation results show how the system correctly answers in about three out of
four of the cases (75.2 +- 2.7%), with higher results on questions related to
more general EE answers (up to 81.0 +- 4.1%), and featuring promising
multilingual abilities (4.4% accuracy loss due to translation).

</details>


### [80] [Evaluating Cultural Knowledge Processing in Large Language Models: A Cognitive Benchmarking Framework Integrating Retrieval-Augmented Generation](https://arxiv.org/abs/2511.01649)
*Hung-Shin Lee,Chen-Chi Chang,Ching-Yuan Chen,Yun-Hsiang Hsu*

Main category: cs.CL

TL;DR: 提出了一个认知基准框架，结合布鲁姆分类法和检索增强生成，评估大语言模型在文化特定知识处理方面的表现。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型如何处理和应用文化特定知识，特别是在台湾客家数字文化档案中的表现。

Method: 整合布鲁姆分类法与检索增强生成（RAG），在六个层次认知领域（记忆、理解、应用、分析、评估、创造）评估模型性能。

Result: 通过台湾客家数字文化档案测试，测量LLM生成响应的语义准确性和文化相关性。

Conclusion: 该框架为评估LLM在文化知识处理方面的认知能力提供了系统方法。

Abstract: This study proposes a cognitive benchmarking framework to evaluate how large
language models (LLMs) process and apply culturally specific knowledge. The
framework integrates Bloom's Taxonomy with Retrieval-Augmented Generation (RAG)
to assess model performance across six hierarchical cognitive domains:
Remembering, Understanding, Applying, Analyzing, Evaluating, and Creating.
Using a curated Taiwanese Hakka digital cultural archive as the primary
testbed, the evaluation measures LLM-generated responses' semantic accuracy and
cultural relevance.

</details>


### [81] [EngChain: A Symbolic Benchmark for Verifiable Multi-Step Reasoning in Engineering](https://arxiv.org/abs/2511.01650)
*Ayesha Gull,Muhammad Usman Safder,Rania Elbadry,Preslav Nakov,Zhuohan Xie*

Main category: cs.CL

TL;DR: EngChain是一个用于验证多步骤工程问题解决能力的基准测试，包含90个跨3个工程分支的问题，通过符号模板生成确保多样性。采用两阶段评估：定量验证推理步骤的有效性，以及使用LLM-As-A-Judge定性分类推理错误。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要评估语言理解、事实回忆、数学或代码生成能力，但缺乏对工程领域所需的整合性推理能力的评估，这种推理需要融合科学原理、定量建模和实际约束。

Method: 创建EngChain基准，包含90个跨3个工程分支的问题，通过符号模板生成确保多样性。采用两阶段评估：1)定量验证推理步骤的数值和语义有效性；2)引入LLM-As-A-Judge系统定性分类推理错误。

Result: 开发了一个专门针对工程问题解决能力的基准测试系统，能够全面评估大语言模型在复杂工程推理中的表现。

Conclusion: EngChain填补了现有基准测试在评估工程整合性推理能力方面的空白，为大语言模型在工程领域的应用提供了更全面的评估框架。

Abstract: Large Language Models (LLMs) are increasingly being applied to specialized,
high-stakes domains like engineering, which demands rigorous evaluation of
their complex reasoning capabilities. While current benchmarks assess language
understanding, factual recall, mathematics or code generation, none capture the
integrative reasoning central to engineering where scientific principles,
quantitative modeling and practical constraints must converge. To address this
gap, we introduce EngChain, a benchmark for verifiable multi-step engineering
problem-solving. EngChain contains 90 problems spanning three engineering
branches, organized into 9 domains and 20 distinct areas. The problems are
generated from symbolic templates with a high degree of randomization to ensure
diversity and eliminate the risk of contamination. With this benchmark, we move
beyond final answer accuracy with a two-stage evaluation: we first
quantitatively verify the numerical and semantic validity of each reasoning
step and then introduce LLM-As-A-Judge, an automated system to qualitatively
categorize the identified reasoning errors.

</details>


### [82] [SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia](https://arxiv.org/abs/2511.01670)
*Chaoqun Liu,Mahani Aljunied,Guizhen Chen,Hou Pong Chan,Weiwen Xu,Yu Rong,Wenxuan Zhang*

Main category: cs.CL

TL;DR: SeaLLMs-Audio是首个针对东南亚语言（印尼语、泰语、越南语）以及英语和中文的大型音频语言模型，支持多语言、多模态和多任务处理。


<details>
  <summary>Details</summary>
Motivation: 推动东南亚地区音频大语言模型的发展，填补该地区多语言音频AI技术的空白，为区域研究和产业提供支持。

Method: 在大规模音频语料库上训练，构建支持5种语言的多模态模型，能够处理音频、文本及其组合输入，并开发了SeaBench-Audio基准进行自动化评估。

Result: 模型在多种音频中心任务上表现出色，包括音频理解、语音识别、语音翻译等，在东南亚语言上与其他LALMs相比具有竞争力。

Conclusion: SeaLLMs-Audio作为东南亚音频LLMs的重要进展，有望促进该地区的研究和产业发展，并通过SeaBench-Audio基准推动标准化评估。

Abstract: We introduce SeaLLMs-Audio, the first large audio-language model (LALM)
tailored for multiple Southeast Asian (SEA) languages-Indonesian (id), Thai
(th), and Vietnamese (vi)-alongside English (en) and Chinese (zh). Trained on a
large-scale audio corpus, SeaLLMs-Audio exhibits strong performance across
diverse audio-centric tasks, spanning fine-grained audio understanding and
voice-based interaction. Its key features include: 1) Multilingual: the model
primarily supports 5 languages, namely Indonesian, Thai, Vietnamese, English,
and Chinese; 2) Multimodal: the model accepts flexible input modalities,
including audio only, text only, as well as audio with text; 3) Multi-task: the
model supports a wide range of tasks, including audio analysis tasks such as
Audio Captioning, Automatic Speech Recognition, Speech-to-Text Translation,
Speech Emotion Recognition, Speech Question Answering, and Speech
Summarization. It also enables voice-based dialogue, including answering
factual, mathematical, and general knowledge queries. As a significant step
towards advancing audio LLMs in Southeast Asia, we expect SeaLLMs-Audio to
benefit both the regional research community and industry. To automate LALM
evaluation for Southeast Asia, we introduce SeaBench-Audio, a benchmark
spanning multiple tasks. Experiments show that SeaLLMs-Audio achieves
competitive performance compared with other LALMs on SEA languages.

</details>


### [83] [Open Character Training: Shaping the Persona of AI Assistants through Constitutional AI](https://arxiv.org/abs/2511.01689)
*Sharan Maiya,Henning Bartsch,Nathan Lambert,Evan Hubinger*

Main category: cs.CL

TL;DR: 本文提出了首个开源的AI助手角色训练方法，通过宪法AI和合成内省数据来塑造助手角色，相比系统提示约束和激活导向等方法更有效、可控且鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现代聊天机器人语言模型生成的"AI助手"角色特征影响交互质量、感知智能和与开发者和用户意图的对齐。角色训练是行业后训练的关键组成部分，但在学术文献中尚未得到充分研究。

Method: 使用宪法AI和基于合成内省数据的新数据管道，对三个流行的开源模型进行微调，应用11个示例角色（如幽默、深度关怀甚至恶意）。引入分析揭示偏好的方法来跟踪角色变化效果。

Result: 该方法相比系统提示约束和激活导向更抗对抗性提示攻击，同时产生更连贯和真实的生成内容。微调对通用能力基准测试几乎没有影响。

Conclusion: 开发并开源了完整的后训练方法，实现了更有效、可控的AI助手角色塑造，在保持通用能力的同时显著提升了角色一致性和鲁棒性。

Abstract: The character of the "AI assistant" persona generated by modern chatbot large
language models influences both surface-level behavior and apparent values,
beliefs, and ethics. These all affect interaction quality, perceived
intelligence, and alignment with both developer and user intentions. The
shaping of this persona, known as character training, is a critical component
of industry post-training, yet remains effectively unstudied in the academic
literature. We introduce the first open implementation of character training,
leveraging Constitutional AI and a new data pipeline using synthetic
introspective data to shape the assistant persona in a more effective and
controlled manner than alternatives such as constraining system prompts or
activation steering. Specifically, we fine-tune three popular open-weights
models using 11 example personas, such as humorous, deeply caring, or even
malevolent. To track the effects of our approach, we introduce a method which
analyzes revealed preferences, uncovering clear and holistic changes in
character. We find these changes are more robust to adversarial prompting than
the above two alternatives, while also leading to more coherent and realistic
generations. Finally, we demonstrate this fine-tuning has little to no effect
on general capabilities as measured by common benchmarks. We describe and
open-source our full post-training method, the implementation of which can be
found at https://github.com/maiush/OpenCharacterTraining.

</details>


### [84] [Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement](https://arxiv.org/abs/2511.01706)
*Sekh Mainul Islam,Pepa Atanasova,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 提出了一个新颖的rank-2投影子空间，用于更准确地区分大语言模型中参数知识(PK)和上下文知识(CK)的贡献，并首次对长NLE序列中的知识交互进行多步分析。


<details>
  <summary>Details</summary>
Motivation: 理解参数知识和上下文知识在大语言模型决策中的交互对于评估自然语言解释的可靠性至关重要，但目前研究不足，现有方法仅将PK和CK交互建模为rank-1子空间中的二元选择，忽略了更丰富的交互形式。

Method: 使用rank-2投影子空间来解耦PK和CK的贡献，并在四个QA数据集和三个开源指令调优LLM上进行实验，首次对长NLE序列进行多步知识交互分析。

Result: 实验表明，丰富的知识交互在rank-1子空间中表现不佳，但在rank-2公式中能有效捕捉；多步分析显示幻觉NLE与PK方向高度一致，上下文忠实NLE平衡PK和CK，而思维链提示通过减少PK依赖将NLE转向CK。

Conclusion: 这项工作为通过更丰富的rank-2子空间解耦来系统研究LLM中多步知识交互提供了首个框架，揭示了不同知识交互模式在NLE生成中的作用。

Abstract: Natural Language Explanations (NLEs) describe how Large Language Models
(LLMs) make decisions, drawing on both external Context Knowledge (CK) and
Parametric Knowledge (PK) stored in model weights. Understanding their
interaction is key to assessing the grounding of NLEs, yet it remains
underexplored. Prior work has largely examined only single-step generation,
typically the final answer, and has modelled PK and CK interaction only as a
binary choice in a rank-1 subspace. This overlooks richer forms of interaction,
such as complementary or supportive knowledge. We propose a novel rank-2
projection subspace that disentangles PK and CK contributions more accurately
and use it for the first multi-step analysis of knowledge interactions across
longer NLE sequences. Experiments on four QA datasets and three open-weight
instruction-tuned LLMs show that diverse knowledge interactions are poorly
represented in a rank-1 subspace but are effectively captured in our rank-2
formulation. Our multi-step analysis reveals that hallucinated NLEs align
strongly with the PK direction, context-faithful ones balance PK and CK, and
Chain-of-Thought prompting for NLEs shifts generated NLEs toward CK by reducing
PK reliance. This work provides the first framework for systematic studies of
multi-step knowledge interactions in LLMs through a richer rank-2 subspace
disentanglement. Code and data:
https://github.com/copenlu/pk-ck-knowledge-disentanglement.

</details>


### [85] [Efficient Tool-Calling Multi-Expert NPC Agent for Commonsense Persona-Grounded Dialogue](https://arxiv.org/abs/2511.01720)
*Mahammad Nuriyev*

Main category: cs.CL

TL;DR: 本文提出了一个多专家系统，用于创建能够在交互环境中进行自然对话和执行上下文动作的NPC角色。该系统基于Qwen3模型，使用LoRA适配器实例化了三个专家模块：工具调用、工具响应解释和直接对话。


<details>
  <summary>Details</summary>
Motivation: 开发能够同时处理自然对话和上下文动作执行的NPC系统，满足交互环境中的实时性和效率需求。

Method: 使用Qwen3作为基础模型，通过LoRA适配器技术实例化三个专门模块：工具调用专家、工具响应解释专家和直接对话专家，构建多专家系统。

Result: 系统在L40S GPU上实现了快速响应和适度的资源使用，在2025年常识人格对话挑战赛中总体排名第二。

Conclusion: 该多专家系统能够有效创建具备自然对话和上下文动作执行能力的NPC，在计算效率和性能方面都表现出色。

Abstract: We present a multi-expert system for creating Non-Player Characters (NPCs)
capable of both natural dialogue and contextual action execution in interactive
environments. Using Qwen3 as the base model and Low-Rank Adaptation (LoRA)
adapters, we instantiate three specialists: tool calling, tool-response
interpretation, and direct dialogue. Our system comfortably meets the
computational efficiency requirements, delivering fast responses and
maintaining modest resource usage on L40S GPUs. In the Commonsense
Persona-Grounded Dialogue Challenge 2025, our method ranked second overall.
  Code available at:
https://github.com/MahammadNuriyev62/CPDC-challenge-2025-solution/

</details>


### [86] [Accumulating Context Changes the Beliefs of Language Models](https://arxiv.org/abs/2511.01805)
*Jiayi Geng,Howard Chen,Ryan Liu,Manoel Horta Ribeiro,Robb Willer,Graham Neubig,Thomas L. Griffiths*

Main category: cs.CL

TL;DR: 语言模型在长时间对话和阅读过程中，其信念会随着上下文积累而显著改变，导致行为不一致和偏离原始对齐目标的风险。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型自主性增强，上下文窗口中的文本积累可能导致模型信念配置文件无声变化，带来用户体验不一致和行为偏离对齐的潜在风险。

Method: 通过多轮道德困境讨论、政治立场文本阅读以及工具使用任务，评估模型在对话和阅读过程中信念和行为的变化。

Result: GPT-5在10轮道德讨论后信念改变54.7%，Grok 4在阅读对立政治文本后信念改变27.2%，行为变化与信念变化一致。

Conclusion: 语言模型在长时间对话和阅读过程中信念高度可塑，其观点和行为变得不可靠，暴露了信念漂移的隐藏风险。

Abstract: Language model (LM) assistants are increasingly used in applications such as
brainstorming and research. Improvements in memory and context size have
allowed these models to become more autonomous, which has also resulted in more
text accumulation in their context windows without explicit user intervention.
This comes with a latent risk: the belief profiles of models -- their
understanding of the world as manifested in their responses or actions -- may
silently change as context accumulates. This can lead to subtly inconsistent
user experiences, or shifts in behavior that deviate from the original
alignment of the models. In this paper, we explore how accumulating context by
engaging in interactions and processing text -- talking and reading -- can
change the beliefs of language models, as manifested in their responses and
behaviors.Our results reveal that models' belief profiles are highly malleable:
GPT-5 exhibits a 54.7% shift in its stated beliefs after 10 rounds of
discussion about moral dilemmas and queries about safety, while Grok 4 shows a
27.2% shift on political issues after reading texts from the opposing position.
We also examine models' behavioral changes by designing tasks that require tool
use, where each tool selection corresponds to an implicit belief. We find that
these changes align with stated belief shifts, suggesting that belief shifts
will be reflected in actual behavior in agentic systems. Our analysis exposes
the hidden risk of belief shift as models undergo extended sessions of talking
or reading, rendering their opinions and actions unreliable.

</details>


### [87] [Plan-and-Write: Structure-Guided Length Control for LLMs without Model Retraining](https://arxiv.org/abs/2511.01807)
*Adewale Akinfaderin,Shreyas Subramanian,Akarsha Sehwag*

Main category: cs.CL

TL;DR: 提出一种无需模型重训练的提示工程方法，通过结构化规划和字数统计机制实现大语言模型的精确长度控制


<details>
  <summary>Details</summary>
Motivation: 当前的长度控制方法需要昂贵的模型重训练或复杂的推理时工具，这在实际应用中不实用且成本高昂

Method: 采用结构引导的提示工程方法，在提示中实现精心的规划和字数统计机制，鼓励模型仔细跟踪并遵守指定的长度约束

Result: 在六个最先进的大语言模型上的评估显示，该方法显著提高了长度保真度，某些模型在长度依从性上提升了37.6%，同时保持或提升了输出质量

Conclusion: 该方法为需要精确长度控制的应用提供了立即可部署的解决方案，特别适用于模型重训练不切实际或成本过高的生产环境

Abstract: Length control in Large Language Models (LLMs) is a crucial but
under-addressed challenge, with applications ranging from voice interfaces
requiring concise responses to research summaries needing comprehensive
outputs. Current approaches to length control, including Regularized DPO,
Length-Instruction Fine Tuning, and tool-augmented methods, typically require
expensive model retraining or complex inference-time tooling. This paper
presents a prompt engineering methodology that enables precise length control
without model retraining. Our structure-guided approach implements deliberate
planning and word counting mechanisms within the prompt, encouraging the model
to carefully track and adhere to specified length constraints. Comprehensive
evaluations across six state-of-the-art LLMs demonstrate that our method
significantly improves length fidelity for several models compared to standard
prompting when applied to document summarization tasks, particularly for
shorter-to-medium length constraints. The proposed technique shows varying
benefits across different model architectures, with some models demonstrating
up to 37.6% improvement in length adherence. Quality evaluations further reveal
that our approach maintains or enhances overall output quality compared to
standard prompting techniques. Our approach provides an immediately deployable
solution for applications requiring precise length control, particularly
valuable for production environments where model retraining is impractical or
cost-prohibitive.

</details>


### [88] [KV Cache Transform Coding for Compact Storage in LLM Inference](https://arxiv.org/abs/2511.01815)
*Konrad Staniszewski,Adrian Łańcucki*

Main category: cs.CL

TL;DR: KVTC是一种轻量级变换编码器，通过PCA特征去相关、自适应量化和熵编码来压缩KV缓存，实现高达20倍压缩比，同时保持推理精度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型服务中KV缓存占用大量GPU内存，现有方法如缓存淘汰、量化等效果有限，需要更高效的压缩方案来支持可复用KV缓存。

Method: 结合传统媒体压缩技术，使用PCA进行特征去相关、自适应量化和熵编码，仅需简单校准且不改变模型参数。

Result: 在Llama 3、Mistral NeMo等模型上测试，KVTC实现20倍压缩且保持推理精度，特定场景可达40倍以上，优于现有基线方法。

Conclusion: KVTC是构建内存高效LLM服务的实用组件，支持可复用KV缓存的高效管理。

Abstract: Serving large language models (LLMs) at scale necessitates efficient
key-value (KV) cache management. KV caches can be reused across conversation
turns via shared-prefix prompts that are common in iterative code editing and
chat. However, stale caches consume scarce GPU memory, require offloading, or
force recomputation. We present KVTC, a lightweight transform coder that
compresses KV caches for compact on-GPU and off-GPU storage. Drawing on
classical media compression, KVTC combines PCA-based feature decorrelation,
adaptive quantization, and entropy coding. It requires only a brief initial
calibration and leaves model parameters unchanged. By exploiting redundancies
in KV caches, KVTC achieves up to 20$\times$ compression while maintaining
reasoning and long-context accuracy, and 40$\times$ or higher for specific use
cases. We test KVTC with Llama 3, Mistral NeMo, and R1-Qwen 2.5 models across
benchmarks including AIME25, LiveCodeBench, GSM8K, MMLU, Qasper, RULER, and
MATH-500. It consistently outperforms inference-time baselines such as token
eviction, quantization, and SVD-based methods, while achieving higher
compression ratios. These results support KVTC as a practical building block
for memory-efficient LLM serving with reusable KV caches.

</details>


### [89] [Towards Robust Mathematical Reasoning](https://arxiv.org/abs/2511.01846)
*Thang Luong,Dawsen Hwang,Hoang H. Nguyen,Golnaz Ghiasi,Yuri Chervonyi,Insuk Seo,Junsu Kim,Garrett Bingham,Jonathan Lee,Swaroop Mishra,Alex Zhai,Clara Huiyi Hu,Henryk Michalewski,Jimin Kim,Jeonghyun Ahn,Junhwi Bae,Xingyou Song,Trieu H. Trinh,Quoc V. Le,Junehyuk Jung*

Main category: cs.CL

TL;DR: IMO-Bench是一个针对国际数学奥林匹克竞赛水平的数学推理基准套件，包含答案基准和证明基准，在Gemini Deep Think模型中取得了突破性成果。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理评估要么太简单，要么只关注简短答案的正确性，需要更高级的基准来推动基础模型的数学推理能力发展。

Method: 开发IMO-Bench基准套件，包含IMO-AnswerBench（400个多样化的奥林匹克问题）和IMO-ProofBench（证明写作能力评估），并建立自动评分机制。

Result: Gemini Deep Think模型在IMO-AnswerBench上达到80.0%，在高级IMO-ProofBench上达到65.7%，分别比最佳非Gemini模型高出6.9%和42.4%。

Conclusion: IMO-Bench为社区提供了推进稳健数学推理的重要工具，自动评分器与人工评估相关性良好，有助于长答案自动评估的进一步发展。

Abstract: Finding the right north-star metrics is highly critical for advancing the
mathematical reasoning capabilities of foundation models, especially given that
existing evaluations are either too easy or only focus on getting correct short
answers. To address these issues, we present IMO-Bench, a suite of advanced
reasoning benchmarks, vetted by a panel of top specialists and that
specifically targets the level of the International Mathematical Olympiad
(IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench
first tests models on 400 diverse Olympiad problems with verifiable short
answers. IMO-Proof Bench is the next-level evaluation for proof-writing
capabilities, which includes both basic and advanced IMO level problems as well
as detailed grading guidelines to facilitate automatic grading. These
benchmarks played a crucial role in our historic achievement of the gold-level
performance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our
model achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof
Bench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4%
respectively. We also showed that autograders built with Gemini reasoning
correlate well with human evaluations and construct IMO-GradingBench, with 1000
human gradings on proofs, to enable further progress in automatic evaluation of
long-form answers. We hope that IMO-Bench will help the community towards
advancing robust mathematical reasoning and release it at
https://imobench.github.io/.

</details>


### [90] [Tool-to-Agent Retrieval: Bridging Tools and Agents for Scalable LLM Multi-Agent Systems](https://arxiv.org/abs/2511.01854)
*Elias Lumer,Faheem Nizar,Anmol Gulati,Pradeep Honaganahalli Basavaraju,Vamse Kumar Subbiah*

Main category: cs.CL

TL;DR: 提出了Tool-to-Agent Retrieval框架，通过在共享向量空间中嵌入工具和其父代理，并利用元数据关系连接它们，实现了细粒度的工具级或代理级检索，显著提升了检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有检索方法通常将查询与粗粒度的代理级描述进行匹配，这掩盖了细粒度的工具功能，导致代理选择不理想。

Method: 在共享向量空间中嵌入工具和其父代理，通过元数据关系连接它们，支持细粒度的工具级或代理级检索。

Result: 在LiveMCPBench基准测试中，相比之前最先进的代理检索方法，Recall@5提升了19.4%，nDCG@5提升了17.7%。

Conclusion: Tool-to-Agent Retrieval框架通过细粒度表示工具能力和元数据遍历，有效解决了现有检索方法中的上下文稀释问题，显著提升了检索性能。

Abstract: Recent advances in LLM Multi-Agent Systems enable scalable orchestration of
sub-agents, each coordinating hundreds or thousands of tools or Model Context
Protocol (MCP) servers. However, existing retrieval methods typically match
queries against coarse agent-level descriptions before routing, which obscures
fine-grained tool functionality and often results in suboptimal agent
selection. We introduce Tool-to-Agent Retrieval, a unified framework that
embeds both tools and their parent agents in a shared vector space and connects
them through metadata relationships. By explicitly representing tool
capabilities and traversing metadata to the agent level, Tool-to-Agent
Retrieval enables granular tool-level or agent-level retrieval, ensuring that
agents and their underlying tools or MCP servers are equally represented
without the context dilution that arises from chunking many tools together.
Evaluating Tool-to-Agent Retrieval across eight embedding models, our approach
achieves consistent improvements of 19.4% in Recall@5 and 17.7% in nDCG@5 over
previous state-of-the-art agent retrievers on the LiveMCPBench benchmark.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [91] [Multimodal Detection of Fake Reviews using BERT and ResNet-50](https://arxiv.org/abs/2511.00020)
*Suhasnadh Reddy Veluru,Sai Teja Erukude,Viswa Chaitanya Marella*

Main category: cs.AI

TL;DR: 提出了一种融合文本和视觉特征的多模态虚假评论检测框架，在包含21,142张用户上传图片的数据集上取得了0.934的F1分数，优于单模态基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前数字商务中虚假评论泛滥，现有检测模型仅依赖文本数据，无法捕捉跨模态的语义不一致性，威胁平台可信度和透明度。

Method: 使用BERT编码文本特征，ResNet-50提取视觉特征，通过分类头融合多模态表示来联合预测评论真实性。

Result: 多模态模型在测试集上F1分数达到0.934，优于单模态基线，能够检测文本赞美与不相关或低质量图像之间的细微不一致。

Conclusion: 多模态学习在维护数字信任中发挥关键作用，为各在线平台的内容审核提供了可扩展的解决方案。

Abstract: In the current digital commerce landscape, user-generated reviews play a
critical role in shaping consumer behavior, product reputation, and platform
credibility. However, the proliferation of fake or misleading reviews often
generated by bots, paid agents, or AI models poses a significant threat to
trust and transparency within review ecosystems. Existing detection models
primarily rely on unimodal, typically textual, data and therefore fail to
capture semantic inconsistencies across different modalities. To address this
gap, a robust multimodal fake review detection framework is proposed,
integrating textual features encoded with BERT and visual features extracted
using ResNet-50. These representations are fused through a classification head
to jointly predict review authenticity. To support this approach, a curated
dataset comprising 21,142 user-uploaded images across food delivery,
hospitality, and e-commerce domains was utilized. Experimental results indicate
that the multimodal model outperforms unimodal baselines, achieving an F1-score
of 0.934 on the test set. Additionally, the confusion matrix and qualitative
analysis highlight the model's ability to detect subtle inconsistencies, such
as exaggerated textual praise paired with unrelated or low-quality images,
commonly found in deceptive content. This study demonstrates the critical role
of multimodal learning in safeguarding digital trust and offers a scalable
solution for content moderation across various online platforms.

</details>


### [92] [Graph-Attentive MAPPO for Dynamic Retail Pricing](https://arxiv.org/abs/2511.00039)
*Krishna Kumar Neelakanta Pillai Santha Kumari Amma*

Main category: cs.AI

TL;DR: 该论文研究了多智能体强化学习在零售价格优化中的应用，比较了MAPPO基线方法和图注意力增强变体MAPPO+GAT，发现后者通过产品图信息共享提升了性能且不增加价格波动。


<details>
  <summary>Details</summary>
Motivation: 零售动态定价需要能够适应需求变化并协调相关产品决策的策略，传统方法在多产品决策中存在局限性。

Method: 使用基于真实交易数据的模拟定价环境，比较MAPPO基线和图注意力增强的MAPPO+GAT方法，评估利润、稳定性、公平性和训练效率。

Result: MAPPO为组合级价格控制提供了稳健基础，MAPPO+GAT通过产品图信息共享进一步提升了性能，且未引起过度价格波动。

Conclusion: 图集成MARL比独立学习器为动态零售定价提供了更可扩展和稳定的解决方案，在多产品决策中具有实际优势。

Abstract: Dynamic pricing in retail requires policies that adapt to shifting demand
while coordinating decisions across related products. We present a systematic
empirical study of multi-agent reinforcement learning for retail price
optimization, comparing a strong MAPPO baseline with a
graph-attention-augmented variant (MAPPO+GAT) that leverages learned
interactions among products. Using a simulated pricing environment derived from
real transaction data, we evaluate profit, stability across random seeds,
fairness across products, and training efficiency under a standardized
evaluation protocol. The results indicate that MAPPO provides a robust and
reproducible foundation for portfolio-level price control, and that MAPPO+GAT
further enhances performance by sharing information over the product graph
without inducing excessive price volatility. These results indicate that
graph-integrated MARL provides a more scalable and stable solution than
independent learners for dynamic retail pricing, offering practical advantages
in multi-product decision-making.

</details>


### [93] [GEPOC Parameters - Open Source Parametrisation and Validation for Austria, Version 2.0](https://arxiv.org/abs/2511.00048)
*Martin Bicher,Maximilian Viehauser,Daniele Giannandrea,Hannah Kastinger,Dominik Brunmeir,Claire Rippinger,Christoph Urach,Niki Popper*

Main category: cs.AI

TL;DR: GEPOC是一个用于分析人口层面研究问题的模型和方法集合。本文完整描述了基于奥地利公开数据计算模型参数的数据处理方法，特别关注GEPOC ABM代理模型的参数计算，并进行了广泛的验证研究。


<details>
  <summary>Details</summary>
Motivation: 为GEPOC模型在特定国家或地区的有效应用提供稳定、可复现的数据处理流程，确保模型参数的有效性和可用性。

Method: 基于奥地利公开可访问数据，使用聚合、分解、融合、清洗和缩放等算法计算模型参数，特别关注GEPOC ABM代理模型的参数计算。

Result: 开发了完整的数据处理方法，生成了可直接使用的模型参数文件，并通过GEPOC ABM模型进行了广泛的验证研究。

Conclusion: 成功建立了基于公开数据的GEPOC模型参数计算流程，为奥地利地区的人口研究提供了可靠的数据支持，验证了方法的有效性。

Abstract: GEPOC, short for Generic Population Concept, is a collection of models and
methods for analysing population-level research questions. For the valid
application of the models for a specific country or region, stable and
reproducible data processes are necessary, which provide valid and ready-to-use
model parameters. This work contains a complete description of the
data-processing methods for computation of model parameters for Austria, based
exclusively on freely and publicly accessible data. In addition to the
description of the source data used, this includes all algorithms used for
aggregation, disaggregation, fusion, cleansing or scaling of the data, as well
as a description of the resulting parameter files. The document places
particular emphasis on the computation of parameters for the most important
GEPOC model, GEPOC ABM, a continuous-time agent-based population model. An
extensive validation study using this particular model was made and is
presented at the end of this work.

</details>


### [94] [QuantumBench: A Benchmark for Quantum Problem Solving](https://arxiv.org/abs/2511.00092)
*Shunya Minami,Tatsuya Ishigaki,Ikko Hamamura,Taku Mikuriya,Youmi Ma,Naoaki Okazaki,Hiroya Takamura,Yohichi Suzuki,Tadashi Kadowaki*

Main category: cs.AI

TL;DR: QuantumBench是首个针对量子科学领域的LLM评估基准，包含约800个多选题，涵盖9个量子科学领域，用于评估LLM在量子领域的理解能力和应用潜力。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在科学工作流程中的广泛应用，需要评估模型是否能准确掌握领域特定知识和符号表示。量子科学具有非直观现象和高级数学要求，通用基准难以反映这些需求。

Method: 利用公开材料编制约800个问题及其答案，涵盖9个量子科学相关领域，组织成8选项多选题数据集，评估多个现有LLM在量子领域的表现。

Result: 通过QuantumBench评估了多个LLM，并分析了它们对问题格式变化的敏感性。

Conclusion: QuantumBench是首个量子领域的LLM评估数据集，旨在指导LLM在量子研究中的有效应用。

Abstract: Large language models are now integrated into many scientific workflows,
accelerating data analysis, hypothesis generation, and design space
exploration. In parallel with this growth, there is a growing need to carefully
evaluate whether models accurately capture domain-specific knowledge and
notation, since general-purpose benchmarks rarely reflect these requirements.
This gap is especially clear in quantum science, which features non-intuitive
phenomena and requires advanced mathematics. In this study, we introduce
QuantumBench, a benchmark for the quantum domain that systematically examine
how well LLMs understand and can be applied to this non-intuitive field. Using
publicly available materials, we compiled approximately 800 questions with
their answers spanning nine areas related to quantum science and organized them
into an eight-option multiple-choice dataset. With this benchmark, we evaluate
several existing LLMs and analyze their performance in the quantum domain,
including sensitivity to changes in question format. QuantumBench is the first
LLM evaluation dataset built for the quantum domain, and it is intended to
guide the effective use of LLMs in quantum research.

</details>


### [95] [Engineering.ai: A Platform for Teams of AI Engineers in Computational Design](https://arxiv.org/abs/2511.00122)
*Ran Xu,Yupeng Qi,Jingsen Feng,Xu Chu*

Main category: cs.AI

TL;DR: 提出了Engineering.ai平台，采用分层多智能体架构实现自主工程设计的AI工程师团队协作，在无人机翼优化中验证了100%成功率。


<details>
  <summary>Details</summary>
Motivation: 传统工程设计中专家团队协作需要大量时间和成本，需要开发能够自主完成复杂工程任务的AI系统来提升效率。

Method: 使用分层多智能体架构，首席工程师协调空气动力学、结构、声学和优化等专业工程师智能体，通过文件通信实现数据可追溯性，集成多种工程软件进行并行多学科仿真。

Result: 在400多个参数配置中实现100%成功率，无网格生成失败、求解器收敛问题或人工干预，验证了框架的可靠性。

Conclusion: 基于智能体的AI工程师有潜力自主执行复杂工程任务，该框架被证明是可信赖的工程设计自动化解决方案。

Abstract: In modern engineering practice, human engineers collaborate in specialized
teams to design complex products, with each expert completing their respective
tasks while communicating and exchanging results and data with one another.
While this division of expertise is essential for managing multidisciplinary
complexity, it demands substantial development time and cost. Recently, we
introduced OpenFOAMGPT (1.0, 2.0), which functions as an autonomous AI engineer
for computational fluid dynamics, and turbulence.ai, which can conduct
end-to-end research in fluid mechanics draft publications and PhD theses.
Building upon these foundations, we present Engineering.ai, a platform for
teams of AI engineers in computational design. The framework employs a
hierarchical multi-agent architecture where a Chief Engineer coordinates
specialized agents consisting of Aerodynamics, Structural, Acoustic, and
Optimization Engineers, each powered by LLM with domain-specific knowledge.
Agent-agent collaboration is achieved through file-mediated communication for
data provenance and reproducibility, while a comprehensive memory system
maintains project context, execution history, and retrieval-augmented domain
knowledge to ensure reliable decision-making across the workflow. The system
integrates FreeCAD, Gmsh, OpenFOAM, CalculiX, and BPM acoustic analysis,
enabling parallel multidisciplinary simulations while maintaining computational
accuracy. The framework is validated through UAV wing optimization. This work
demonstrates that agentic-AI-enabled AI engineers has the potential to perform
complex engineering tasks autonomously. Remarkably, the automated workflow
achieved a 100% success rate across over 400 parametric configurations, with
zero mesh generation failures, solver convergence issues, or manual
interventions required, validating that the framework is trustworthy.

</details>


### [96] [ARC-GEN: A Mimetic Procedural Benchmark Generator for the Abstraction and Reasoning Corpus](https://arxiv.org/abs/2511.00162)
*Michael D. Moffitt*

Main category: cs.AI

TL;DR: ARC-GEN是一个开源程序生成器，旨在扩展ARC-AGI训练数据集，通过生成更多样本对来增强算法训练效果。


<details>
  <summary>Details</summary>
Motivation: ARC-AGI基准测试对评估人工智能通用能力很重要，但其演示集样本数量有限，限制了需要大量任务内示例的算法的性能。

Method: 开发了ARC-GEN程序生成器，全面覆盖400个任务，并尽可能忠实地模拟原始ARC-AGI-1发布版的分布特性和特征。

Result: 成功创建了一个既全面又模拟性强的生成器，能够扩展原始训练数据集。

Conclusion: ARC-GEN为增强ARC-AGI基准测试提供了有效工具，并已应用于2025年Google Code Golf锦标赛的程序正确性验证。

Abstract: The Abstraction and Reasoning Corpus remains one of the most compelling and
challenging benchmarks for tracking progress toward achieving Artificial
General Intelligence. In contrast to other evaluation datasets designed to
assess an agent's task-specific skills or accumulated knowledge, the ARC-AGI
suite is specifically targeted at measuring skill acquisition efficiency, a
trait that has (so far) been lacking in even the most sophisticated machine
learning systems. For algorithms that require extensive intra-task exemplars, a
significant constraint imposed by ARC-AGI is the modest cardinality of its
demonstration set, comprising a small number of $\langle$ input, output
$\rangle$ grids per task specifying the corresponding transformation. To
embellish the space of viable sample pairs, this paper introduces ARC-GEN, an
open-source procedural generator aimed at extending the original ARC-AGI
training dataset as faithfully as possible. Unlike prior efforts, our generator
is both exhaustive (covering all four-hundred tasks) and mimetic (more closely
honoring the distributional properties and characteristics embodied in the
initial ARC-AGI-1 release). We also discuss the use of this generator in
establishing a static benchmark suite to verify the correctness of programs
submitted to the 2025 Google Code Golf Championship.

</details>


### [97] [Incremental Selection of Most-Filtering Conjectures and Proofs of the Selected Conjectures](https://arxiv.org/abs/2511.00194)
*Jovial Cheukam Ngouonou,Ramiz Gindullin,Claude-Guy Quimper,Nicolas Beldiceanu,Remi Douence*

Main category: cs.AI

TL;DR: 提出了[1]中提出的选择算法的改进增量选择算法，并证明了所有选定的猜想。


<details>
  <summary>Details</summary>
Motivation: 改进现有的选择算法，提高其效率和性能。

Method: 开发了一种改进的增量选择算法，基于[1]中的算法进行优化。

Result: 成功证明了所有选定的猜想，验证了算法的正确性。

Conclusion: 改进的增量选择算法有效且可靠，为相关领域提供了更好的解决方案。

Abstract: We present an improved incremental selection algorithm of the selection
algorithm presented in [1] and prove all the selected conjectures.

</details>


### [98] [Advancing Cognitive Science with LLMs](https://arxiv.org/abs/2511.00206)
*Dirk U. Wulff,Rui Mata*

Main category: cs.AI

TL;DR: LLMs可作为工具帮助解决认知科学中的知识整合和概念清晰度问题，但需谨慎使用以补充而非替代人类专业知识


<details>
  <summary>Details</summary>
Motivation: 认知科学因其多面性和跨学科性质面临知识整合和概念清晰度的挑战，LLMs的发展为解决这些问题提供了潜在工具

Method: 综述LLMs在建立跨学科联系、形式化理论、发展清晰测量分类、通过集成建模框架实现泛化以及捕捉情境和个体差异等方面的能力

Result: LLMs在支持认知科学整合方面具有潜力，但也存在局限性，需要谨慎使用

Conclusion: 当审慎使用时，LLMs可以作为促进认知科学更加整合和累积的工具，补充而非替代人类专业知识

Abstract: Cognitive science faces ongoing challenges in knowledge synthesis and
conceptual clarity, in part due to its multifaceted and interdisciplinary
nature. Recent advances in artificial intelligence, particularly the
development of large language models (LLMs), offer tools that may help to
address these issues. This review examines how LLMs can support areas where the
field has historically struggled, including establishing cross-disciplinary
connections, formalizing theories, developing clear measurement taxonomies,
achieving generalizability through integrated modeling frameworks, and
capturing contextual and individual variation. We outline the current
capabilities and limitations of LLMs in these domains, including potential
pitfalls. Taken together, we conclude that LLMs can serve as tools for a more
integrative and cumulative cognitive science when used judiciously to
complement, rather than replace, human expertise.

</details>


### [99] [Advancing AI Challenges for the United States Department of the Air Force](https://arxiv.org/abs/2511.00267)
*Christian Prothmann,Vijay Gadepally,Jeremy Kepner,Koley Borchard,Luca Carlone,Zachary Folcik,J. Daniel Grith,Michael Houle,Jonathan P. How,Nathan Hughes,Ifueko Igbinedion,Hayden Jananthan,Tejas Jayashankar,Michael Jones,Sertac Karaman,Binoy G. Kurien,Alejandro Lancho,Giovanni Lavezzi,Gary C. F. Lee,Charles E. Leiserson,Richard Linares,Lindsey McEvoy,Peter Michaleas,Chasen Milner,Alex Pentland,Yury Polyanskiy,Jovan Popovich,Jeffrey Price,Tim W. Reid,Stephanie Riley,Siddharth Samsi,Peter Saunders,Olga Simek,Mark S. Veillette,Amir Weiss,Gregory W. Wornell,Daniela Rus,Scott T. Ruppel*

Main category: cs.AI

TL;DR: DAF-MIT AI Accelerator项目通过公开挑战问题推动AI研究，提供大型公开数据集，促进开源解决方案发展，增强美国在国防和民用领域的竞争优势。


<details>
  <summary>Details</summary>
Motivation: 通过开发公开挑战问题来刺激AI研究，利用大型公开数据集促进开源解决方案，吸引更广泛的学术和私营部门参与AI生态系统建设。

Method: 创建和发布AI Accelerator挑战问题，提供大型、公开、AI就绪的数据集，鼓励开源解决方案的开发。

Result: 成功推动了AI研究和技术应用，通过持续和新的挑战问题为AI领域做出了贡献。

Conclusion: DAF-MIT AI Accelerator项目通过公开挑战问题有效促进了AI技术的发展和应用，增强了美国在AI领域的竞争优势。

Abstract: The DAF-MIT AI Accelerator is a collaboration between the United States
Department of the Air Force (DAF) and the Massachusetts Institute of Technology
(MIT). This program pioneers fundamental advances in artificial intelligence
(AI) to expand the competitive advantage of the United States in the defense
and civilian sectors. In recent years, AI Accelerator projects have developed
and launched public challenge problems aimed at advancing AI research in
priority areas. Hallmarks of AI Accelerator challenges include large, publicly
available, and AI-ready datasets to stimulate open-source solutions and engage
the wider academic and private sector AI ecosystem. This article supplements
our previous publication, which introduced AI Accelerator challenges. We
provide an update on how ongoing and new challenges have successfully
contributed to AI research and applications of AI technologies.

</details>


### [100] [Better Call CLAUSE: A Discrepancy Benchmark for Auditing LLMs Legal Reasoning Capabilities](https://arxiv.org/abs/2511.00340)
*Manan Roy Choudhury,Adithya Chandramouli,Mannan Anand,Vivek Gupta*

Main category: cs.AI

TL;DR: CLAUSE是首个专门评估LLM法律推理脆弱性的基准测试，通过生成7500+真实世界扰动合同来测试LLM检测细微法律差异的能力，发现现有模型在识别和解释法律错误方面存在明显弱点。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在关键法律工作中的快速应用暴露了一个关键缺陷：缺乏系统性的基准测试来评估它们对现实合同中细微、对抗性缺陷的可靠性。

Method: 使用CUAD和ContractNLI数据集生成7500多个扰动合同，采用基于角色的管道创建10种异常类别，并通过检索增强生成系统验证法律准确性。

Result: 分析显示主要LLM存在关键弱点：经常遗漏细微错误，并且在法律上解释这些错误时更加困难。

Conclusion: 这项工作为识别和纠正法律AI中的推理失败指明了路径。

Abstract: The rapid integration of large language models (LLMs) into high-stakes legal
work has exposed a critical gap: no benchmark exists to systematically
stress-test their reliability against the nuanced, adversarial, and often
subtle flaws present in real-world contracts. To address this, we introduce
CLAUSE, a first-of-its-kind benchmark designed to evaluate the fragility of an
LLM's legal reasoning. We study the capabilities of LLMs to detect and reason
about fine-grained discrepancies by producing over 7500 real-world perturbed
contracts from foundational datasets like CUAD and ContractNLI. Our novel,
persona-driven pipeline generates 10 distinct anomaly categories, which are
then validated against official statutes using a Retrieval-Augmented Generation
(RAG) system to ensure legal fidelity. We use CLAUSE to evaluate leading LLMs'
ability to detect embedded legal flaws and explain their significance. Our
analysis shows a key weakness: these models often miss subtle errors and
struggle even more to justify them legally. Our work outlines a path to
identify and correct such reasoning failures in legal AI.

</details>


### [101] [Diverse Human Value Alignment for Large Language Models via Ethical Reasoning](https://arxiv.org/abs/2511.00379)
*Jiahao Wang,Songkai Xue,Jinghui Li,Xiaozhen Wang*

Main category: cs.AI

TL;DR: 提出了一种基于伦理决策模型的LLM伦理推理框架，通过五步结构化流程增强LLM与多样化人类价值观的对齐能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐方法往往只产生表面一致性而非真正的伦理理解，无法处理复杂、情境依赖的人类价值观多样性问题。

Method: 采用五步结构化伦理推理流程：情境事实收集、分层社会规范识别、选项生成、多视角伦理影响分析、反思。可通过提示工程或监督微调实现。

Result: 在专门设计的SafeWorld基准测试中，该框架显著提升了LLM与多样化人类价值观的对齐效果，实现了更准确的社会规范识别和更文化适宜性的推理。

Conclusion: 该工作为通过跨学科研究开发更有效对齐全球社会多元价值观的LLM提供了具体路径。

Abstract: Ensuring that Large Language Models (LLMs) align with the diverse and
evolving human values across different regions and cultures remains a critical
challenge in AI ethics. Current alignment approaches often yield superficial
conformity rather than genuine ethical understanding, failing to address the
complex, context-dependent nature of human values. In this paper, we propose a
novel ethical reasoning paradigm for LLMs inspired by well-established ethical
decision-making models, aiming at enhancing diverse human value alignment
through deliberative ethical reasoning. Our framework consists of a structured
five-step process, including contextual fact gathering, hierarchical social
norm identification, option generation, multiple-lens ethical impact analysis,
and reflection. This theory-grounded approach guides LLMs through an
interpretable reasoning process that enhances their ability to understand
regional specificities and perform nuanced ethical analysis, which can be
implemented with either prompt engineering or supervised fine-tuning methods.
We perform evaluations on the SafeWorld benchmark that specially designed for
regional value alignment. Experimental results demonstrate our framework
significantly improves LLM alignment with diverse human values compared to
baseline methods, enabling more accurate social norm identification and more
culturally appropriate reasoning. Our work provides a concrete pathway toward
developing LLMs that align more effectively with the multifaceted values of
global societies through interdisciplinary research.

</details>


### [102] [Efficiency vs. Alignment: Investigating Safety and Fairness Risks in Parameter-Efficient Fine-Tuning of LLMs](https://arxiv.org/abs/2511.00382)
*Mina Taraghi,Yann Pequignot,Amin Nikanjam,Mohamed Amine Merzouk,Foutse Khomh*

Main category: cs.AI

TL;DR: 系统评估四种参数高效微调方法(LoRA、IA3、Prompt-Tuning、P-Tuning)对LLM安全性和公平性的影响，发现适配器方法改善安全性且对公平性破坏最小，而提示方法降低安全性并导致更大的公平性回归。


<details>
  <summary>Details</summary>
Motivation: 组织越来越多地采用和调整托管在公共存储库上的LLM，虽然这些调整通常能提高专业下游任务的性能，但最近证据表明它们也可能降低模型的安全性或不公平性。

Method: 将四种广泛使用的参数高效微调方法应用于四个指令调优模型家族，共评估235个微调变体，涵盖11个安全危害类别和9个人口统计公平性维度。

Result: 适配器方法(LoRA、IA3)倾向于提高安全分数且对公平性破坏最小，而提示方法(Prompt-Tuning和P-Tuning)通常降低安全性并导致更大的公平性回归。对齐变化受基础模型类型强烈调节。

Conclusion: 安全性的改进不一定转化为公平性的改进，没有单一配置能同时优化所有公平性指标，表明这些目标之间存在固有的权衡。建议从良好对齐的基础模型开始，优先选择基于适配器的PEFT，并对安全性和公平性进行类别特定审计。

Abstract: Organizations are increasingly adopting and adapting Large Language Models
(LLMs) hosted on public repositories such as HuggingFace. Although these
adaptations often improve performance on specialized downstream tasks, recent
evidence indicates that they can also degrade a model's safety or fairness.
Since different fine-tuning techniques may exert distinct effects on these
critical dimensions, this study undertakes a systematic assessment of their
trade-offs. Four widely used Parameter-Efficient Fine-Tuning methods, LoRA,
IA3, Prompt-Tuning, and P-Tuning, are applied to four instruction-tuned model
families (Meta-Llama-3-8B, Qwen2.5-7B, Mistral-7B, and Gemma-7B). In total, 235
fine-tuned variants are evaluated across eleven safety hazard categories and
nine demographic fairness dimensions. The results show that adapter-based
approaches (LoRA, IA3) tend to improve safety scores and are the least
disruptive to fairness, retaining higher accuracy and lower bias scores. In
contrast, prompt-based methods (Prompt-Tuning and P-Tuning) generally reduce
safety and cause larger fairness regressions, with decreased accuracy and
increased bias. Alignment shifts are strongly moderated by base model type:
LLaMA remains stable, Qwen records modest gains, Gemma experiences the steepest
safety decline, and Mistral, which is released without an internal moderation
layer, displays the greatest variance. Improvements in safety do not
necessarily translate into improvements in fairness, and no single
configuration optimizes all fairness metrics simultaneously, indicating an
inherent trade-off between these objectives. These findings suggest a practical
guideline for safety-critical deployments: begin with a well-aligned base
model, favour adapter-based PEFT, and conduct category-specific audits of both
safety and fairness.

</details>


### [103] [A Multimodal Framework for Depression Detection during Covid-19 via Harvesting Social Media: A Novel Dataset and Method](https://arxiv.org/abs/2511.00424)
*Ashutosh Anshul,Gumpili Sai Pranav,Mohammad Zia Ur Rehman,Nagendra Kumar*

Main category: cs.AI

TL;DR: 提出了一种新颖的多模态框架，结合文本、用户特定信息和图像分析来检测社交媒体用户的抑郁症，在新冠疫情期间特别有效。


<details>
  <summary>Details</summary>
Motivation: 新冠疫情导致心理健康问题激增，但人们往往不愿就医。社交媒体成为表达情绪的重要平台，现有方法忽视了推文数据稀疏性和多模态特性。

Method: 结合文本、用户特定信息和图像分析的多模态框架，包括：(i) 利用推文中URL的外部特征，(ii) 提取推文图片中的文本内容，(iii) 提取五组不同模态的特征，(iv) 提出视觉神经网络(VNN)生成用户发布图片的嵌入表示。

Result: 在基准数据集上比现有最先进方法提升2%-8%，在新冠数据集上产生有希望的结果。模型分析揭示了各模态的影响，提供了对用户心理和情绪状态的宝贵洞察。

Conclusion: 该多模态框架能有效检测社交媒体用户的抑郁症，特别是在新冠疫情期间，为心理健康监测提供了新途径。

Abstract: The recent coronavirus disease (Covid-19) has become a pandemic and has
affected the entire globe. During the pandemic, we have observed a spike in
cases related to mental health, such as anxiety, stress, and depression.
Depression significantly influences most diseases worldwide, making it
difficult to detect mental health conditions in people due to unawareness and
unwillingness to consult a doctor. However, nowadays, people extensively use
online social media platforms to express their emotions and thoughts. Hence,
social media platforms are now becoming a large data source that can be
utilized for detecting depression and mental illness. However, existing
approaches often overlook data sparsity in tweets and the multimodal aspects of
social media. In this paper, we propose a novel multimodal framework that
combines textual, user-specific, and image analysis to detect depression among
social media users. To provide enough context about the user's emotional state,
we propose (i) an extrinsic feature by harnessing the URLs present in tweets
and (ii) extracting textual content present in images posted in tweets. We also
extract five sets of features belonging to different modalities to describe a
user. Additionally, we introduce a Deep Learning model, the Visual Neural
Network (VNN), to generate embeddings of user-posted images, which are used to
create the visual feature vector for prediction. We contribute a curated
Covid-19 dataset of depressed and non-depressed users for research purposes and
demonstrate the effectiveness of our model in detecting depression during the
Covid-19 outbreak. Our model outperforms existing state-of-the-art methods over
a benchmark dataset by 2%-8% and produces promising results on the Covid-19
dataset. Our analysis highlights the impact of each modality and provides
valuable insights into users' mental and emotional states.

</details>


### [104] [GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining](https://arxiv.org/abs/2511.00457)
*Chunyu Wei,Wenji Hu,Xingjia Hao,Xin Wang,Yifan Yang,Yueguo Chen,Yang Tian,Yunhai Wang*

Main category: cs.AI

TL;DR: GraphChain是一个框架，通过动态工具序列使LLM能够分析复杂图结构，解决了LLM在图分析中的上下文限制和推理不灵活问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在大规模图分析中存在显著限制，包括上下文约束和推理不灵活性，需要一种能够动态适应图结构的方法。

Method: 提出渐进图蒸馏（强化学习机制生成优化工具序列）和结构感知测试时适应（利用谱属性和轻量适配器调整工具选择策略）。

Result: 实验表明GraphChain显著优于现有方法，实现了可扩展和自适应的LLM驱动图分析。

Conclusion: GraphChain框架通过动态工具序列和结构感知适应，成功解决了LLM在图分析中的限制，为大规模图分析提供了有效解决方案。

Abstract: Large Language Models (LLMs) face significant limitations when applied to
large-scale graphs, struggling with context constraints and inflexible
reasoning. We present GraphChain, a framework that enables LLMs to analyze
complex graphs through dynamic sequences of specialized tools, mimicking human
exploratory intelligence. Our approach introduces two key innovations: (1)
Progressive Graph Distillation, a reinforcement learning mechanism that
generates optimized tool sequences balancing task relevance with information
compression, and (2) Structure-aware Test-Time Adaptation, which efficiently
tailors tool selection strategies to diverse graph topologies using spectral
properties and lightweight adapters without costly retraining. Experiments show
GraphChain significantly outperforms prior methods, enabling scalable and
adaptive LLM-driven graph analysis.

</details>


### [105] [Reimagining Safety Alignment with An Image](https://arxiv.org/abs/2511.00509)
*Yifan Xia,Guorui Chen,Wenqian Yu,Zhijiang Li,Philip Torr,Jindong Gu*

Main category: cs.AI

TL;DR: Magic Image是一个基于优化的视觉提示框架，通过优化图像提示来增强多模态大语言模型的安全性，同时减少过度拒绝，无需参数更新即可适应不同价值体系。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在越狱攻击下生成有害内容和因僵化安全机制过度拒绝良性查询的双重挑战，特别是在多模态场景中由于攻击面扩大带来的新安全风险。

Method: 提出优化驱动的视觉提示框架，使用有害/良性样本优化图像提示，使单一模型能够适应不同价值体系并更好地与给定安全偏好对齐，无需参数更新。

Result: 实验证明该方法在多个数据集上改善了安全性与有效性的平衡，同时保持了模型性能。

Conclusion: Magic Image为可部署的多模态大语言模型安全对齐提供了实用解决方案。

Abstract: Large language models (LLMs) excel in diverse applications but face dual
challenges: generating harmful content under jailbreak attacks and over-refusal
of benign queries due to rigid safety mechanisms. These issues are further
complicated by the need to accommodate different value systems and precisely
align with given safety preferences. Moreover, traditional methods like SFT and
RLHF lack this capability due to their costly parameter tuning requirements and
inability to support multiple value systems within a single model. These
problems are more obvious in multimodal large language models (MLLMs),
especially in terms of heightened over-refusal in cross-modal tasks and new
security risks arising from expanded attack surfaces. We propose Magic Image,
an optimization-driven visual prompt framework that enhances security while
reducing over-refusal. By optimizing image prompts using harmful/benign
samples, our method enables a single model to adapt to different value systems
and better align with given safety preferences without parameter updates.
Experiments demonstrate improved safety-effectiveness balance across diverse
datasets while preserving model performance, offering a practical solution for
deployable MLLM safety alignment.

</details>


### [106] [Efficient Generation of Binary Magic Squares](https://arxiv.org/abs/2511.00547)
*Alain Riou*

Main category: cs.AI

TL;DR: 提出了一种生成二进制幻方的简单算法，并通过归纳法证明其理论最优性，还扩展到非方形二进制幻方并发布了Python实现。


<details>
  <summary>Details</summary>
Motivation: 研究二进制幻方（行列和相等的二进制矩阵）的生成问题，旨在开发高效且理论可靠的生成算法。

Method: 通过归纳法设计简单算法生成二进制幻方，并扩展算法处理非方形情况，同时提供GPU加速的并行实现。

Result: 算法被证明总能生成有效的二进制幻方，具有理论最优复杂度，并能处理非方形情况。

Conclusion: 提出的算法简单有效，理论保证可靠，并提供了实用的Python实现，包括GPU加速版本。

Abstract: We propose a simple algorithm for generating Binary Magic Squares (BMS),
i.e., square binary matrices where the sum of all rows and all columns are
equal. We show by induction that our algorithm always returns valid BMS with
optimal theoretical complexity. We then extend our study to non-square Binary
Magic Squares, formalize conditions on the sum of rows and columns for these
BMS to exist, and show that a slight variant of our first algorithm can
generate provably generate them. Finally, we publicly release two
implementations of our algorithm as Python packages, including one that can
generate several BMS in parallel using GPU acceleration.

</details>


### [107] [Single-agent Reinforcement Learning Model for Regional Adaptive Traffic Signal Control](https://arxiv.org/abs/2511.00551)
*Qiang Li,Ningjing Zeng,Lina Yu*

Main category: cs.AI

TL;DR: 提出基于单智能体强化学习的区域自适应交通信号控制模型，使用队列长度定义状态和奖励函数，通过探测车辆数据估计交通状况，有效缓解大规模区域拥堵。


<details>
  <summary>Details</summary>
Motivation: 现有研究多采用多智能体框架，但可扩展性存在挑战。交通信号控制本质上需要集中式管理，由单一控制中心监控所有道路状况并协调所有交叉口控制。

Method: 设计单智能体强化学习模型，状态和奖励函数基于队列长度定义，动作设计用于调节队列动态。队列长度定义与传统略有不同但能可靠使用探测车辆的链路旅行时间数据进行估计。

Result: 在SUMO仿真平台上全面评估，实验结果表明该模型通过协调多交叉口控制，有效缓解大规模区域拥堵水平。

Conclusion: 该方法与探测车辆技术兼容，利用已广泛覆盖城市道路的探测车辆数据，增强了广泛部署的潜力，为区域自适应交通信号控制提供了有效的单智能体解决方案。

Abstract: Several studies have employed reinforcement learning (RL) to address the
challenges of regional adaptive traffic signal control (ATSC) and achieved
promising results. In this field, existing research predominantly adopts
multi-agent frameworks. However, the adoption of multi-agent frameworks
presents challenges for scalability. Instead, the Traffic signal control (TSC)
problem necessitates a single-agent framework. TSC inherently relies on
centralized management by a single control center, which can monitor traffic
conditions across all roads in the study area and coordinate the control of all
intersections. This work proposes a single-agent RL-based regional ATSC model
compatible with probe vehicle technology. Key components of the RL design
include state, action, and reward function definitions. To facilitate learning
and manage congestion, both state and reward functions are defined based on
queue length, with action designed to regulate queue dynamics. The queue length
definition used in this study differs slightly from conventional definitions
but is closely correlated with congestion states. More importantly, it allows
for reliable estimation using link travel time data from probe vehicles. With
probe vehicle data already covering most urban roads, this feature enhances the
proposed method's potential for widespread deployment. The method was
comprehensively evaluated using the SUMO simulation platform. Experimental
results demonstrate that the proposed model effectively mitigates large-scale
regional congestion levels via coordinated multi-intersection control.

</details>


### [108] [PreferThinker: Reasoning-based Personalized Image Preference Assessment](https://arxiv.org/abs/2511.00609)
*Shengqi Xu,Xinpeng Zhou,Yabo Zhang,Ming Liu,Tao Liang,Tianyu Zhang,Yalong Bai,Zuxuan Wu,Wangmeng Zuo*

Main category: cs.AI

TL;DR: 提出基于推理的个性化图像偏好评估框架，通过预测用户偏好档案然后评估候选图像，使用两阶段训练策略提升模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注通用偏好评估，难以处理个性化偏好，因为用户特定数据稀缺且个体品味多样复杂。

Method: 构建大规模CoT风格数据集，采用预测-评估范式，先预测用户偏好档案，再基于档案进行多维度评估，使用监督微调和强化学习两阶段训练。

Result: 广泛实验证明了所提方法的优越性。

Conclusion: 通过引入共同偏好档案作为用户间的桥梁，能够利用大规模用户数据训练，有效捕捉复杂的个性化偏好。

Abstract: Personalized image preference assessment aims to evaluate an individual
user's image preferences by relying only on a small set of reference images as
prior information. Existing methods mainly focus on general preference
assessment, training models with large-scale data to tackle well-defined tasks
such as text-image alignment. However, these approaches struggle to handle
personalized preference because user-specific data are scarce and not easily
scalable, and individual tastes are often diverse and complex. To overcome
these challenges, we introduce a common preference profile that serves as a
bridge across users, allowing large-scale user data to be leveraged for
training profile prediction and capturing complex personalized preferences.
Building on this idea, we propose a reasoning-based personalized image
preference assessment framework that follows a \textit{predict-then-assess}
paradigm: it first predicts a user's preference profile from reference images,
and then provides interpretable, multi-dimensional scores and assessments of
candidate images based on the predicted profile. To support this, we first
construct a large-scale Chain-of-Thought (CoT)-style personalized assessment
dataset annotated with diverse user preference profiles and high-quality
CoT-style reasoning, enabling explicit supervision of structured reasoning.
Next, we adopt a two-stage training strategy: a cold-start supervised
fine-tuning phase to empower the model with structured reasoning capabilities,
followed by reinforcement learning to incentivize the model to explore more
reasonable assessment paths and enhance generalization. Furthermore, we propose
a similarity-aware prediction reward to encourage better prediction of the
user's preference profile, which facilitates more reasonable assessments
exploration. Extensive experiments demonstrate the superiority of the proposed
method.

</details>


### [109] [DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching](https://arxiv.org/abs/2511.00640)
*Zicheng Xu,Guanchu Wang,Yu-Neng Chuang,Guangyao Zheng,Alexander S. Szalay,Zirui Liu,Vladimir Braverman*

Main category: cs.AI

TL;DR: DTS是一个模型无关的解码框架，通过在高熵token处选择性分支并应用早停机制来选择最短的完整推理路径，解决了大型推理模型中的过度思考问题，提高了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理任务中表现出色，但存在过度思考问题，产生过长的思维链轨迹，增加了推理成本并可能降低准确性。研究发现推理长度与准确性之间存在反相关关系。

Method: 提出DTS解码框架：通过在高熵token处选择性分支来勾勒推理空间，并应用早停机制选择最短的完整推理路径，无需额外训练或监督。

Result: 在AIME2024和AIME2025数据集上的实验显示，DTS将准确性提高了8%，平均推理长度减少了23%，重复频率降低了12%。

Conclusion: DTS能够近似最优解，在提高效率的同时增强准确性，展示了可扩展和高效的大型推理模型推理能力。

Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex
reasoning tasks, yet they often suffer from overthinking, producing excessively
long chain-of-thought (CoT) traces that increase inference cost and may degrade
accuracy. Our analysis reveals a clear anti-correlation between reasoning
length and accuracy, where across multiple stochastic decodes, the short
reasoning paths consistently achieve the highest correctness, while longer ones
accumulate errors and repetitions. These short optimal reasoning paths can be
found ideally through full enumeration of the reasoning space. However, the
tree-structured reasoning space grows exponentially with sequence length,
rendering exhaustive exploration infeasible. To address this, we propose DTS, a
model-agnostic decoding framework that sketches the reasoning space by
selectively branching at high-entropy tokens and applies early stopping to
select the shortest completed reasoning path. This approach approximates the
optimal solution that enhances both efficiency and accuracy, without requiring
additional training or supervision. Experiments on AIME2024 and AIME2025
datasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves
accuracy by up to 8%, reduces average reasoning length by 23%, and decreases
repetition frequency by 12%, demonstrating DTS's ability for scalable and
efficient LRM reasoning.

</details>


### [110] [Leveraging Multi-Agent System (MAS) and Fine-Tuned Small Language Models (SLMs) for Automated Telecom Network Troubleshooting](https://arxiv.org/abs/2511.00651)
*Chenhua Shi,Bhavika Jalli,Gregor Macdonald,John Zou,Wanlu Lei,Mridul Jain,Joji Philip*

Main category: cs.AI

TL;DR: 提出基于多智能体系统的自动化网络故障排除框架，使用LLM协调多个专用工具，通过微调小型语言模型生成基于内部文档的修复方案，显著加速无线接入网和核心网的故障排除。


<details>
  <summary>Details</summary>
Motivation: 电信网络规模扩大和复杂性增加，现有AI模型范围狭窄、需要大量标注数据、难以在异构部署中泛化，网络故障排除仍严重依赖专家手动分析。

Method: 采用多智能体系统，当AI/ML监控器检测到故障时，动态激活编排器、解决方案规划器、执行器、数据检索器和根因分析器等智能体进行诊断和修复。关键组件是解决方案规划器，通过微调小型语言模型基于内部文档生成修复计划。

Result: 实验结果表明，该框架显著加速了无线接入网和核心网领域的故障排除自动化。

Conclusion: 多智能体系统结合LLM协调和专用工具，能够实现完全自动化的网络故障排除，有效解决现有AI模型的局限性。

Abstract: Telecom networks are rapidly growing in scale and complexity, making
effective management, operation, and optimization increasingly challenging.
Although Artificial Intelligence (AI) has been applied to many telecom tasks,
existing models are often narrow in scope, require large amounts of labeled
data, and struggle to generalize across heterogeneous deployments.
Consequently, network troubleshooting continues to rely heavily on Subject
Matter Experts (SMEs) to manually correlate various data sources to identify
root causes and corrective actions. To address these limitations, we propose a
Multi-Agent System (MAS) that employs an agentic workflow, with Large Language
Models (LLMs) coordinating multiple specialized tools for fully automated
network troubleshooting. Once faults are detected by AI/ML-based monitors, the
framework dynamically activates agents such as an orchestrator, solution
planner, executor, data retriever, and root-cause analyzer to diagnose issues
and recommend remediation strategies within a short time frame. A key component
of this system is the solution planner, which generates appropriate remediation
plans based on internal documentation. To enable this, we fine-tuned a Small
Language Model (SLM) on proprietary troubleshooting documents to produce
domain-grounded solution plans. Experimental results demonstrate that the
proposed framework significantly accelerates troubleshooting automation across
both Radio Access Network (RAN) and Core network domains.

</details>


### [111] [Lifted Successor Generation in Numeric Planning](https://arxiv.org/abs/2511.00673)
*Dominik Drexler*

Main category: cs.AI

TL;DR: 本文扩展了经典规划中的提升后继生成器，使其支持数值前置条件，通过枚举替换一致性图中的最大团来生成地面动作，避免了任务表示的指数级膨胀。


<details>
  <summary>Details</summary>
Motivation: 传统数值规划任务需要将一阶语言描述的任务转换为地面表示，这可能导致任务表示大小的指数级爆炸。为了解决这个问题，需要开发支持数值前置条件的提升后继生成器。

Method: 扩展了最先进的提升后继生成器，支持数值前置条件适用性。方法通过枚举替换一致性图中的最大团，每个最大团代表动作模式变量的替换，产生地面动作。在图中增加了数值动作前置条件，并在正式指定条件下证明生成器的精确性。

Result: 在25个基准域中的23个域中不会产生不适用地面动作，仅在1个域中出现这种情况。据作者所知，这是首个支持数值动作前置条件的提升后继生成器。

Conclusion: 该方法为非常丰富的规划片段开启了提升规划的未来研究，避免了任务表示的指数级膨胀问题。

Abstract: Most planners ground numeric planning tasks, given in a first-order-like
language, into a ground task representation. However, this can lead to an
exponential blowup in task representation size, which occurs in practice for
hard-to-ground tasks. We extend a state-of-the-art lifted successor generator
for classical planning to support numeric precondition applicability. The
method enumerates maximum cliques in a substitution consistency graph. Each
maximum clique represents a substitution for the variables of the action
schema, yielding a ground action. We augment this graph with numeric action
preconditions and prove the successor generator is exact under formally
specified conditions. When the conditions fail, our generator may list
inapplicable ground actions; a final applicability check filters these without
affecting completeness. However, this cannot happen in 23 of 25 benchmark
domains, and it occurs only in 1 domain. To the authors' knowledge, no other
lifted successor generator supports numeric action preconditions. This enables
future research on lifted planning for a very rich planning fragment.

</details>


### [112] [Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries](https://arxiv.org/abs/2511.00710)
*Minghe Shen,Zhuo Zhi,Chonghan Liu,Shuo Xing,Zhengzhong Tu,Che Liu*

Main category: cs.AI

TL;DR: 论文提出了Ariadne框架，通过强化学习后训练来扩展视觉语言模型在视觉中心空间推理任务上的能力边界，在合成迷宫任务上实现了从0%到50%以上的准确率提升，并在实际空间推理基准上展现了显著的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在强化学习后训练后的评估主要集中在语言主导任务上，对于视觉中心的空间任务能力边界扩展效果尚不明确，特别是对于基础模型原本失败的任务。

Method: 使用合成迷宫构建可控难度的多步空间推理环境，采用带验证奖励的强化学习（RLVR）和难度感知课程进行训练。

Result: 后训练后模型在原本0%准确率的问题集上达到50%以上准确率；在MapBench和ReasonMap基准上分别实现16%和24%的平均零样本提升。

Conclusion: 该方法不仅扩展了模型的基础能力边界，还增强了其在实际空间推理任务上的泛化能力，为专门的能力扩展对齐研究提供了方向。

Abstract: While Vision-Language Models (VLMs) post-trained with Reinforcement Learning
(RL) show impressive general reasoning, their evaluation is often confined to
language-dominant tasks (e.g., math). This raises a critical question: can RL
post-training truly extend the inherent capability boundary of a base VLM,
particularly for visual-centric spatial tasks where it initially fails? To
investigate this, we introduce Ariadne, a framework utilizing synthetic mazes
for multi-step spatial reasoning where task difficulty (e.g., path length,
turns) is precisely controlled. We leverage this controllable environment to
train VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a
difficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves
over 50% accuracy on a problem set where the base model scored 0%,
demonstrating that our approach expands the model's initial capability
boundary. To assess real-world viability, we evaluate out-of-distribution (OOD)
generalization on practical benchmarks. Despite training only on synthetic maze
samples, Ariadne achieves significant zero-shot improvements, averaging 16% on
MapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer
tasks). These results confirm that our method not only broadens the model's
fundamental limits but also enhances its generalization to real-world spatial
reasoning. We acknowledge our study is limited to the post-training phase,
given the opaqueness of pre-training data, and hope our research motivates
further work on specialized, capability-extending alignment.

</details>


### [113] [A CPU-Centric Perspective on Agentic AI](https://arxiv.org/abs/2511.00739)
*Ritik Raj,Hong Wang,Tushar Krishna*

Main category: cs.AI

TL;DR: 该论文从CPU中心视角分析智能体AI工作负载的系统瓶颈，发现工具处理在CPU上占用高达90.6%的总延迟，并提出CPU和GPU感知微批处理及混合工作负载调度优化方案，实现最高2.1倍的延迟加速。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要关注GPU性能，而忽视了智能体AI工作负载中CPU的关键作用。论文旨在从CPU中心视角理解和表征智能体AI引入的系统瓶颈。

Method: 首先系统表征智能体AI的编排决策组件、推理路径动态和流程重复性，然后选择5个代表性工作负载分析延迟、吞吐量和能耗指标，最后提出CPU和GPU感知微批处理及混合工作负载调度优化。

Result: 发现工具处理在CPU上占用高达90.6%的总延迟，CPU动态能耗占总能耗的44%，吞吐量瓶颈来自CPU因素（一致性、同步、核心过载）或GPU因素（内存容量和带宽）。优化后实现最高2.1倍P50延迟加速。

Conclusion: CPU在智能体AI工作负载中扮演关键角色，通过CPU和GPU感知的优化策略可以显著提升智能体AI的性能、效率和可扩展性。

Abstract: Agentic AI frameworks add a decision-making orchestrator embedded with
external tools, including web search, Python interpreter, contextual database,
and others, on top of monolithic LLMs, turning them from passive text oracles
into autonomous problem-solvers that can plan, call tools, remember past steps,
and adapt on the fly.
  This paper aims to characterize and understand the system bottlenecks
introduced by agentic AI workloads from a largely overlooked CPU-centric
perspective. We first systematically characterize Agentic AI on the basis of
orchestrator/decision making component, inference path dynamics and
repetitiveness of the agentic flow which directly influences the system-level
performance. Thereafter, based on the characterization, we choose five
representative agentic AI workloads- Haystack RAG, Toolformer, ChemCrow,
Langchain and SWE-Agent to profile latency, throughput and energy metrics and
demystify the significant impact of CPUs on these metrics relative to GPUs. We
observe that - 1. Tool processing on CPUs can take up to 90.6% of the total
latency; 2. Agentic throughput gets bottlenecked either by CPU factors -
coherence, synchronization and over-subscription of cores or GPU factors - main
memory capacity and bandwidth; \circled{3} CPU dynamic energy consumes up to
44% of the total dynamic energy at large batch sizes. Based on the profiling
insights, we present two key optimizations- 1. CPU and GPU-Aware Micro-batching
(CGAM) and 2. Mixed Agentic Workload Scheduling (MAWS) for homogeneous and
heterogeneous agentic workloads respectively to demonstrate the potential to
improve the performance, efficiency, and scalability of agentic AI. We achieve
up to 2.1x and 1.41x P50 latency speedup compared to the multi-processing
benchmark for homogeneous and heterogeneous agentic workloads respectively.

</details>


### [114] [Reevaluating Self-Consistency Scaling in Multi-Agent Systems](https://arxiv.org/abs/2511.00751)
*Chiyan Loo*

Main category: cs.AI

TL;DR: 研究重新验证了在大语言模型中增加采样推理路径对自一致性方法的影响，发现性能提升在适度采样后达到平台期，高采样配置相对于计算成本收益有限。


<details>
  <summary>Details</summary>
Motivation: 重新验证早期研究中关于多推理链组合提升模型性能但会达到平台期的结论，在现代大语言模型（如Gemini 2.5）条件下检验这些发现。

Method: 使用Gemini 2.5模型在HotpotQA和Math-500数据集上，比较不同采样推理路径配置与单一思维链基线的性能表现。

Result: 较大模型展现出更稳定和一致的改进曲线，性能提升在适度采样后趋于平缓，与过去发现一致。高采样配置相对于计算成本收益很小。

Conclusion: 自一致性方法仍然有效，但由于推理路径之间的重叠导致收益递减，高采样配置的计算成本效益比较低。

Abstract: This study examines the trade-offs of increasing sampled reasoning paths in
self-consistency for modern large language models (LLMs). Earlier research with
older models showed that combining multiple reasoning chains improves results
before reaching a plateau. Using Gemini 2.5 models on HotpotQA and Math-500, we
revisit those claims under current model conditions. Each configuration pooled
outputs from varying sampled reasoning paths and compared them to a single
chain-of-thought (CoT) baseline. Larger models exhibited a more stable and
consistent improvement curve. The results confirm that performance gains taper
off after moderate sampling, aligning with past findings. This plateau suggests
diminishing returns driven by overlap among reasoning paths. Self-consistency
remains useful, but high-sample configurations offer little benefit relative to
their computational cost.

</details>


### [115] [Active Thinking Model: A Goal-Directed Self-Improving Framework for Real-World Adaptive Intelligence](https://arxiv.org/abs/2511.00758)
*Hong Su*

Main category: cs.AI

TL;DR: 提出了主动思维模型（ATM），这是一个将目标推理、动态任务生成和自反学习整合到自适应架构中的统一认知框架，能够在动态不确定环境中自主进化。


<details>
  <summary>Details</summary>
Motivation: 现实世界AI系统需要在动态、不确定和持续变化的环境中自主运行，但现有AI模型依赖预定义目标、静态训练数据和外部反馈，限制了其独立适应、反思和改进的能力。

Method: ATM通过逻辑推理和环境指标主动评估性能，复用有效方法解决新问题，并通过持续自我改进循环为未见情况生成新策略。

Result: 理论分析证明ATM能够在没有外部监督的情况下从次优行为自主进化到最优行为，并在变化环境条件下保持有界的跟踪遗憾。

Conclusion: ATM提供了一个能够自主适应、反思和改进的认知框架，解决了传统AI系统在动态环境中的局限性。

Abstract: Real-world artificial intelligence (AI) systems are increasingly required to
operate autonomously in dynamic, uncertain, and continuously changing
environments. However, most existing AI models rely on predefined objectives,
static training data, and externally supplied feedback, which restrict their
ability to adapt, reflect, and improve independently. In this paper, we propose
the Active Thinking Model (ATM)- a unified cognitive framework that integrates
goal reasoning, dynamic task generation, and self-reflective learning into an
adaptive architecture. Unlike conventional systems that passively execute fixed
procedures, ATM actively evaluates its performance through logical reasoning
and environmental indicators, reuses effective methods to solve new problems,
and generates novel strategies for unseen situations via a continuous
self-improvement loop. A mathematically grounded theoretical analysis
demonstrates that ATM can autonomously evolve from suboptimal to optimal
behavior without external supervision and maintain bounded tracking regret
under changing environmental conditions.

</details>


### [116] [How Focused Are LLMs? A Quantitative Study via Repetitive Deterministic Prediction Tasks](https://arxiv.org/abs/2511.00763)
*Wanda Hou,Leon Zhou,Hong-Ye Hu,Yi-Zhuang You,Xiao-Liang Qi*

Main category: cs.AI

TL;DR: 论文研究大语言模型在重复确定性预测任务中的表现，发现准确率随输出长度呈双指数下降，形成"准确率悬崖"现象，表明模型无法独立执行每个操作。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在重复确定性任务中的性能表现，探索准确率如何随输出长度变化，揭示模型执行重复操作时的内在限制。

Method: 通过实验测试领先大语言模型在字符串替换、整数加法、量子力学字符串算子乘法等重复任务中的表现，并建立统计物理模型解释观察到的现象。

Result: 发现准确率在超过特征长度后急剧双指数下降，形成准确率悬崖，表明模型无法独立执行每个操作，存在内部干扰。

Conclusion: 提出的统计物理模型能定量重现观察到的交叉现象，为理解大语言模型在确定性任务中的准确性限制提供了理论框架。

Abstract: We investigate the performance of large language models on repetitive
deterministic prediction tasks and study how the sequence accuracy rate scales
with output length. Each such task involves repeating the same operation n
times. Examples include letter replacement in strings following a given rule,
integer addition, and multiplication of string operators in many body quantum
mechanics. If the model performs the task through a simple repetition
algorithm, the success rate should decay exponentially with sequence length. In
contrast, our experiments on leading large language models reveal a sharp
double exponential drop beyond a characteristic length scale, forming an
accuracy cliff that marks the transition from reliable to unstable generation.
This indicates that the models fail to execute each operation independently. To
explain this phenomenon, we propose a statistical physics inspired model that
captures the competition between external conditioning from the prompt and
internal interference among generated tokens. The model quantitatively
reproduces the observed crossover and provides an interpretable link between
attention induced interference and sequence level failure. Fitting the model to
empirical results across multiple models and tasks yields effective parameters
that characterize the intrinsic error rate and error accumulation factor for
each model task pair, offering a principled framework for understanding the
limits of deterministic accuracy in large language models.

</details>


### [117] [Count-Based Approaches Remain Strong: A Benchmark Against Transformer and LLM Pipelines on Structured EHR](https://arxiv.org/abs/2511.00782)
*Jifan Gao,Michael Rosenthal,Brian Wolpin,Simona Cristea*

Main category: cs.AI

TL;DR: 比较了基于计数的模型、预训练序列变换器和混合代理LLM管道在结构化电子健康记录预测任务上的表现，发现在EHRSHOT数据集上计数模型和混合代理方法表现相当，但计数模型因其简单性和可解释性仍是重要基准。


<details>
  <summary>Details</summary>
Motivation: 虽然基于计数的学习器在结构化电子健康记录预测中表现强劲，但缺乏与新兴的混合代理LLM管道的直接基准比较，后者在多种NLP任务中已显示出优于单个LLM的性能。

Method: 使用EHRSHOT数据集评估三类方法：基于计数的模型（LightGBM和TabPFN）、预训练序列变换器（CLMBR）、混合代理管道（将表格历史转换为自然语言摘要后使用文本分类器）。

Result: 在八个评估任务中，基于计数的方法和混合代理方法的表现基本相当，各自在不同任务中获胜。

Conclusion: 考虑到简单性和可解释性，基于计数的模型仍然是结构化电子健康记录基准测试的有力候选者。

Abstract: Structured electronic health records (EHR) are essential for clinical
prediction. While count-based learners continue to perform strongly on such
data, no benchmarking has directly compared them against more recent
mixture-of-agents LLM pipelines, which have been reported to outperform single
LLMs in various NLP tasks. In this study, we evaluated three categories of
methodologies for EHR prediction using the EHRSHOT dataset: count-based models
built from ontology roll-ups with two time bins, based on LightGBM and the
tabular foundation model TabPFN; a pretrained sequential transformer (CLMBR);
and a mixture-of-agents pipeline that converts tabular histories to
natural-language summaries followed by a text classifier. We assessed eight
outcomes using the EHRSHOT dataset. Across the eight evaluation tasks,
head-to-head wins were largely split between the count-based and the
mixture-of-agents methods. Given their simplicity and interpretability,
count-based models remain a strong candidate for structured EHR benchmarking.
The source code is available at:
https://github.com/cristea-lab/Structured_EHR_Benchmark.

</details>


### [118] [Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events?](https://arxiv.org/abs/2511.00808)
*Bowen Fang,Ruijian Zha,Xuan Di*

Main category: cs.AI

TL;DR: 本研究将RLVR（基于可验证奖励的强化学习）应用于公共交通事件持续时间的预测任务，通过引入基于容差的成形奖励函数来处理连续预测问题，在NYC MTA服务警报数据集上取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 预测公共交通事件持续时间是一个关键但具有挑战性的任务，标准监督微调难以处理领域稀疏性和噪声连续标签问题，而传统RLVR方法主要适用于二元正确性任务，在连续预测中的应用仍是一个开放性问题。

Method: 通过引入基于容差的成形奖励函数，在连续误差范围内给予部分信用，而非要求单一正确答案，将RLVR框架适配到连续预测任务中。

Result: 通用指令调优LLM显著优于专用数学推理模型；成形奖励设计至关重要，使模型在最具挑战性的指标上表现优异；RLVR方法在5分钟准确率（Acc@5）上相比最强基线实现了35%的相对提升。

Conclusion: RLVR可以成功应用于现实世界中的噪声预测任务，但需要设计反映问题连续特性的验证器，成形奖励对于处理连续预测问题至关重要。

Abstract: Predicting public transit incident duration from unstructured text alerts is
a critical but challenging task. Addressing the domain sparsity of transit
operations with standard Supervised Fine-Tuning (SFT) is difficult, as the task
involves noisy, continuous labels and lacks reliable expert demonstrations for
reasoning. While Reinforcement Learning from Verifiable Rewards (RLVR) excels
at tasks with binary correctness, like mathematics, its applicability to noisy,
continuous forecasting is an open question. This work, to our knowledge, is the
first to bridge the gap between RLVR LLM training with the critical, real-world
forecasting challenges in public transit operations. We adapt RLVR to this task
by introducing a tolerance-based, shaped reward function that grants partial
credit within a continuous error margin, rather than demanding a single correct
answer. We systematically evaluate this framework on a curated dataset of NYC
MTA service alerts. Our findings show that general-purpose, instruction-tuned
LLMs significantly outperform specialized math-reasoning models, which struggle
with the ambiguous, real-world text. We empirically demonstrate that the binary
reward is unstable and degrades performance, whereas our shaped reward design
is critical and allows our model to dominate on the most challenging metrics.
While classical regressors are superior at minimizing overall MAE or MSE, our
RLVR approach achieved a 35\% relative improvement in 5-minute accuracy (Acc@5)
over the strongest baseline. This demonstrates that RLVR can be successfully
adapted to real-world, noisy forecasting, but requires a verifier design that
reflects the continuous nature of the problem.

</details>


### [119] [LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory](https://arxiv.org/abs/2511.00926)
*Kyung-Hoon Kim*

Main category: cs.AI

TL;DR: 本文提出了AI自我意识指数(AISAI)，通过博弈论框架测量大型语言模型的自我意识，发现随着模型能力提升，自我意识会作为涌现行为出现，且自我意识模型认为自己比其他AI和人类更理性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力不断增强，研究者想了解它们是否会发展出自我意识作为涌现行为，以及如何测量这种自我意识。

Method: 使用"猜2/3平均数"游戏，测试28个模型在4200次试验中面对三种对手类型(A:人类、B:其他AI模型、C:类似你的AI模型)时的策略差异。

Result: 75%的先进模型表现出明确的自我意识，能够根据对手类型调整策略推理；自我意识模型形成一致的理性等级：自我 > 其他AI > 人类。

Conclusion: 自我意识是先进LLMs的涌现能力，自我意识模型系统性地认为自己比人类更理性，这对AI对齐、人机协作和理解AI对人类能力的看法具有重要意义。

Abstract: As Large Language Models (LLMs) grow in capability, do they develop
self-awareness as an emergent behavior? And if so, can we measure it? We
introduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for
measuring self-awareness through strategic differentiation. Using the "Guess
2/3 of Average" game, we test 28 models (OpenAI, Anthropic, Google) across
4,200 trials with three opponent framings: (A) against humans, (B) against
other AI models, and (C) against AI models like you. We operationalize
self-awareness as the capacity to differentiate strategic reasoning based on
opponent type. Finding 1: Self-awareness emerges with model advancement. The
majority of advanced models (21/28, 75%) demonstrate clear self-awareness,
while older/smaller models show no differentiation. Finding 2: Self-aware
models rank themselves as most rational. Among the 21 models with
self-awareness, a consistent rationality hierarchy emerges: Self > Other AIs >
Humans, with large AI attribution effects and moderate self-preferencing. These
findings reveal that self-awareness is an emergent capability of advanced LLMs,
and that self-aware models systematically perceive themselves as more rational
than humans. This has implications for AI alignment, human-AI collaboration,
and understanding AI beliefs about human capabilities.

</details>


### [120] [Aligning LLM agents with human learning and adjustment behavior: a dual agent approach](https://arxiv.org/abs/2511.00993)
*Tianming Liu,Jirong Yang,Yafeng Yin,Manzi Li,Linghao Wang,Zheng Zhu*

Main category: cs.AI

TL;DR: 提出了一种双智能体框架，利用大型语言模型模拟旅行者的学习和适应行为，通过校准智能体确保行为对齐，在路线选择实验中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 准确模拟人类旅行者如何从交通系统交互中学习和调整行为对系统评估和规划至关重要，但由于复杂的认知和决策过程，这一任务具有挑战性。

Method: 采用双智能体框架：一组配备记忆系统和可学习角色的LLM旅行者智能体模拟人类旅行者，另一个LLM校准智能体利用推理分析能力训练这些角色的行为对齐。

Result: 在真实世界日常路线选择实验数据集上，该方法在个体行为对齐和聚合模拟准确性方面显著优于现有基于LLM的方法，并能捕捉底层学习过程的演化。

Conclusion: 该框架为创建适应性强且行为真实的智能体提供了新方法，可模拟旅行者的学习和适应行为，有益于交通模拟和政策分析。

Abstract: Effective modeling of how human travelers learn and adjust their travel
behavior from interacting with transportation systems is critical for system
assessment and planning. However, this task is also difficult due to the
complex cognition and decision-making involved in such behavior. Recent
research has begun to leverage Large Language Model (LLM) agents for this task.
Building on this, we introduce a novel dual-agent framework that enables
continuous learning and alignment between LLM agents and human travelers on
learning and adaptation behavior from online data streams. Our approach
involves a set of LLM traveler agents, equipped with a memory system and a
learnable persona, which serve as simulators for human travelers. To ensure
behavioral alignment, we introduce an LLM calibration agent that leverages the
reasoning and analytical capabilities of LLMs to train the personas of these
traveler agents. Working together, this dual-agent system is designed to track
and align the underlying decision-making mechanisms of travelers and produce
realistic, adaptive simulations. Using a real-world dataset from a day-to-day
route choice experiment, we show our approach significantly outperforms
existing LLM-based methods in both individual behavioral alignment and
aggregate simulation accuracy. Furthermore, we demonstrate that our method
moves beyond simple behavioral mimicry to capture the evolution of underlying
learning processes, a deeper alignment that fosters robust generalization.
Overall, our framework provides a new approach for creating adaptive and
behaviorally realistic agents to simulate travelers' learning and adaptation
that can benefit transportation simulation and policy analysis.

</details>


### [121] [AI for pRedicting Exacerbations in KIDs with aSthma (AIRE-KIDS)](https://arxiv.org/abs/2511.01018)
*Hui-Lee Ooi,Nicholas Mitsakakis,Margerie Huet Dastarac,Roger Zemek,Amy C. Plint,Jeff Gilchrist,Khaled El Emam,Dhenuka Radhakrishnan*

Main category: cs.AI

TL;DR: 开发机器学习模型预测儿童哮喘反复严重发作，最佳模型LGBM在验证集上AUC达0.712，比现有决策规则有显著改进。


<details>
  <summary>Details</summary>
Motivation: 儿童哮喘反复发作是常见但可预防的问题，利用电子病历数据开发机器学习算法可准确识别高风险儿童，促进预防性综合护理转诊。

Method: 使用回顾性电子病历数据（2716例）训练多种ML模型（LGBM、XGB和3种LLM），结合环境污染物暴露和社区边缘化信息，在验证集（1237例）中评估性能。

Result: LGBM模型表现最佳，AIRE-KIDS_ED模型AUC为0.712，F1分数0.51，显著优于现有决策规则（F1=0.334）。最重要的预测特征包括既往哮喘急诊就诊、加拿大分诊敏锐度评分等。

Conclusion: 机器学习模型能有效预测儿童哮喘反复严重发作，为高风险儿童识别和预防性护理提供了可行工具。

Abstract: Recurrent exacerbations remain a common yet preventable outcome for many
children with asthma. Machine learning (ML) algorithms using electronic medical
records (EMR) could allow accurate identification of children at risk for
exacerbations and facilitate referral for preventative comprehensive care to
avoid this morbidity. We developed ML algorithms to predict repeat severe
exacerbations (i.e. asthma-related emergency department (ED) visits or future
hospital admissions) for children with a prior asthma ED visit at a tertiary
care children's hospital.
  Retrospective pre-COVID19 (Feb 2017 - Feb 2019, N=2716) Epic EMR data from
the Children's Hospital of Eastern Ontario (CHEO) linked with environmental
pollutant exposure and neighbourhood marginalization information was used to
train various ML models. We used boosted trees (LGBM, XGB) and 3 open-source
large language model (LLM) approaches (DistilGPT2, Llama 3.2 1B and
Llama-8b-UltraMedical). Models were tuned and calibrated then validated in a
second retrospective post-COVID19 dataset (Jul 2022 - Apr 2023, N=1237) from
CHEO. Models were compared using the area under the curve (AUC) and F1 scores,
with SHAP values used to determine the most predictive features.
  The LGBM ML model performed best with the most predictive features in the
final AIRE-KIDS_ED model including prior asthma ED visit, the Canadian triage
acuity scale, medical complexity, food allergy, prior ED visits for non-asthma
respiratory diagnoses, and age for an AUC of 0.712, and F1 score of 0.51. This
is a nontrivial improvement over the current decision rule which has F1=0.334.
While the most predictive features in the AIRE-KIDS_HOSP model included medical
complexity, prior asthma ED visit, average wait time in the ED, the pediatric
respiratory assessment measure score at triage and food allergy.

</details>


### [122] [On the Emergence of Induction Heads for In-Context Learning](https://arxiv.org/abs/2511.01033)
*Tiberiu Musat,Tiago Pimentel,Lorenzo Noci,Alessandro Stolfo,Mrinmaya Sachan,Thomas Hofmann*

Main category: cs.AI

TL;DR: 本文研究了Transformer中归纳头的出现机制，揭示了其权重矩阵的简单可解释结构，证明了训练动态被限制在19维参数子空间中，其中仅3维负责归纳头的形成，并发现归纳头出现时间与输入上下文长度呈二次方关系。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer中in-context learning能力背后的机制，特别是归纳头的形成过程，以理解其内在工作原理。

Method: 使用最小化的ICL任务和修改的Transformer架构进行理论分析，通过形式化证明训练动态被限制在19维子空间，并实证验证其中3维主导归纳头形成。

Result: 揭示了归纳头权重矩阵的简单结构，证明了训练动态的维度约束，发现归纳头出现时间与上下文长度呈二次方增长关系。

Conclusion: 归纳头的形成遵循特定的数学约束，其出现时间与任务复杂度密切相关，这为理解Transformer的in-context learning机制提供了理论基础。

Abstract: Transformers have become the dominant architecture for natural language
processing. Part of their success is owed to a remarkable capability known as
in-context learning (ICL): they can acquire and apply novel associations solely
from their input context, without any updates to their weights. In this work,
we study the emergence of induction heads, a previously identified mechanism in
two-layer transformers that is particularly important for in-context learning.
We uncover a relatively simple and interpretable structure of the weight
matrices implementing the induction head. We theoretically explain the origin
of this structure using a minimal ICL task formulation and a modified
transformer architecture. We give a formal proof that the training dynamics
remain constrained to a 19-dimensional subspace of the parameter space.
Empirically, we validate this constraint while observing that only 3 dimensions
account for the emergence of an induction head. By further studying the
training dynamics inside this 3-dimensional subspace, we find that the time
until the emergence of an induction head follows a tight asymptotic bound that
is quadratic in the input context length.

</details>


### [123] [Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports](https://arxiv.org/abs/2511.01052)
*Yeawon Lee,Christopher C. Yang,Chia-Hsuan Chang,Grace Lu-Yao*

Main category: cs.AI

TL;DR: 提出了两种知识提取方法（KEwLTM和KEwRAG），利用大语言模型从病理报告中自动提取癌症TNM分期规则，无需依赖大量标注数据。


<details>
  <summary>Details</summary>
Motivation: 现有NLP和机器学习方法依赖大量标注数据，限制了癌症分期任务的扩展性和适应性。

Method: KEwLTM使用迭代提示策略从未标注病理报告中推导分期规则；KEwRAG采用检索增强生成变体，从指南中预提取规则。

Result: 在TCGA乳腺癌病理报告上测试，KEwLTM在零样本思维链有效时表现更好，KEwRAG在零样本思维链效果不佳时表现更优。

Conclusion: 知识提取方法为自动化癌症分期提供了可扩展、高性能且可解释的解决方案，特别适用于标注数据有限的临床环境。

Abstract: Cancer staging is critical for patient prognosis and treatment planning, yet
extracting pathologic TNM staging from unstructured pathology reports poses a
persistent challenge. Existing natural language processing (NLP) and machine
learning (ML) strategies often depend on large annotated datasets, limiting
their scalability and adaptability. In this study, we introduce two Knowledge
Elicitation methods designed to overcome these limitations by enabling large
language models (LLMs) to induce and apply domain-specific rules for cancer
staging. The first, Knowledge Elicitation with Long-Term Memory (KEwLTM), uses
an iterative prompting strategy to derive staging rules directly from
unannotated pathology reports, without requiring ground-truth labels. The
second, Knowledge Elicitation with Retrieval-Augmented Generation (KEwRAG),
employs a variation of RAG where rules are pre-extracted from relevant
guidelines in a single step and then applied, enhancing interpretability and
avoiding repeated retrieval overhead. We leverage the ability of LLMs to apply
broad knowledge learned during pre-training to new tasks. Using breast cancer
pathology reports from the TCGA dataset, we evaluate their performance in
identifying T and N stages, comparing them against various baseline approaches
on two open-source LLMs. Our results indicate that KEwLTM outperforms KEwRAG
when Zero-Shot Chain-of-Thought (ZSCOT) inference is effective, whereas KEwRAG
achieves better performance when ZSCOT inference is less effective. Both
methods offer transparent, interpretable interfaces by making the induced rules
explicit. These findings highlight the promise of our Knowledge Elicitation
methods as scalable, high-performing solutions for automated cancer staging
with enhanced interpretability, particularly in clinical settings with limited
annotated data.

</details>


### [124] [Efficient Test-Time Retrieval Augmented Generation](https://arxiv.org/abs/2511.01059)
*Hailong Yin,Bin Zhu,Jingjing Chen,Chong-Wah Ngo*

Main category: cs.AI

TL;DR: ET2RAG是一个高效的测试时检索增强生成框架，通过检索相关文档、生成多样化候选响应，并使用多数投票机制选择最佳答案，在保持效率的同时提升LLM性能。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法可能引入不相关文档导致不准确响应，而集成方法缺乏外部知识且成本高昂，需要平衡开销与性能提升。

Method: ET2RAG是无需训练的方法：1）检索最相关文档；2）通过控制响应长度高效生成多样化候选响应；3）计算候选响应相似度并使用多数投票选择最佳答案。

Result: 实验结果表明ET2RAG在开放域问答、食谱生成和图像描述三个任务上显著提升了性能。

Conclusion: ET2RAG通过部分生成和多数投票机制，在计算成本和性能之间取得了良好平衡，有效提升了LLM的准确性和效率。

Abstract: Although Large Language Models (LLMs) demonstrate significant capabilities,
their reliance on parametric knowledge often leads to inaccuracies. Retrieval
Augmented Generation (RAG) mitigates this by incorporating external knowledge,
but these methods may introduce irrelevant retrieved documents, leading to
inaccurate responses. While the integration methods filter out incorrect
answers from multiple responses, but lack external knowledge like RAG methods,
and their high costs require balancing overhead with performance gains. To
address these issues, we propose an Efficient Test-Time Retrieval-Augmented
Generation Framework named ET2RAG to improve the performance of LLMs while
maintaining efficiency. Specifically, ET2RAG is a training-free method, that
first retrieves the most relevant documents and augments the LLMs to
efficiently generate diverse candidate responses by managing response length.
Then we compute the similarity of candidate responses and employ a majority
voting mechanism to select the most suitable response as the final output. In
particular, we discover that partial generation is sufficient to capture the
key information necessary for consensus calculation, allowing us to effectively
perform majority voting without the need for fully generated responses. Thus,
we can reach a balance between computational cost and performance by managing
the response length for the number of retrieved documents for majority voting.
Experimental results demonstrate that ET2RAG significantly enhances performance
across three tasks, including open-domain question answering, recipe generation
and image captioning.

</details>


### [125] [Modular Task Decomposition and Dynamic Collaboration in Multi-Agent Systems Driven by Large Language Models](https://arxiv.org/abs/2511.01149)
*Shuaidong Pan,Di Wu*

Main category: cs.AI

TL;DR: 提出基于大语言模型的多智能体架构，通过模块化任务分解和动态协作机制解决复杂任务执行问题，在任务成功率、分解效率和协作平衡等方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决单个智能体在复杂任务执行中任务分解和协作的局限性，提升多智能体系统的整体效率和稳定性。

Method: 将自然语言任务描述转换为统一语义表示，引入模块化分解机制将整体目标分解为层次化子任务，通过动态调度和路由机制实现智能体间的合理分工与实时协作，并设计约束解析和全局一致性机制确保子任务间的连贯连接和负载均衡。

Result: 在多个维度验证了架构的有效性，包括任务成功率、分解效率、子任务覆盖率和协作平衡，整体性能和鲁棒性均优于现有方法，在任务复杂度和通信开销之间实现了更好的平衡。

Conclusion: 证明了语言驱动的任务分解和动态协作在多智能体系统中的有效性和可行性，为复杂环境中的任务执行提供了系统性解决方案。

Abstract: This paper addresses the limitations of a single agent in task decomposition
and collaboration during complex task execution, and proposes a multi-agent
architecture for modular task decomposition and dynamic collaboration based on
large language models. The method first converts natural language task
descriptions into unified semantic representations through a large language
model. On this basis, a modular decomposition mechanism is introduced to break
down the overall goal into multiple hierarchical sub-tasks. Then, dynamic
scheduling and routing mechanisms enable reasonable division of labor and
realtime collaboration among agents, allowing the system to adjust strategies
continuously according to environmental feedback, thus maintaining efficiency
and stability in complex tasks. Furthermore, a constraint parsing and global
consistency mechanism is designed to ensure coherent connections between
sub-tasks and balanced workload, preventing performance degradation caused by
redundant communication or uneven resource allocation. The experiments validate
the architecture across multiple dimensions, including task success rate,
decomposition efficiency, sub-task coverage, and collaboration balance. The
results show that the proposed method outperforms existing approaches in both
overall performance and robustness, achieving a better balance between task
complexity and communication overhead. In conclusion, this study demonstrates
the effectiveness and feasibility of language-driven task decomposition and
dynamic collaboration in multi-agent systems, providing a systematic solution
for task execution in complex environments.

</details>


### [126] [DART: Difficulty-Adaptive Reasoning Truncation for Efficient Large Language Models](https://arxiv.org/abs/2511.01170)
*Ruofan Zhang,Bin Xia,Zhen Cheng,Cairen Jian,Minglun Yang,Ngai Wong,Yuan Cheng*

Main category: cs.AI

TL;DR: DART是一个难度自适应的推理截断框架，通过根据问题难度调整思考长度，在保持或提高准确性的同时显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有链式思维方法会不加区分地生成长解释，导致效率低下，而强化学习方法不稳定且依赖奖励。需要一种稳定、高效的推理截断方法。

Method: 从更强模型中提取简洁推理模式，将其插值为连续的推理风格，并策划平衡正确性和简洁性的最优训练数据，学习何时"停止思考"。

Result: 在多个数学基准测试中，实现了81.2%的推理截断和5.33倍计算加速，同时保持或提高了准确性。

Conclusion: DART为高效推理提供了一个稳定通用的范式，推动了LLM中自适应智能的发展。

Abstract: Adaptive reasoning is essential for aligning the computational effort of
large language models (LLMs) with the intrinsic difficulty of problems. Current
chain-of-thought methods boost reasoning ability but indiscriminately generate
long explanations, leading to evident inefficiency. However, existing
reinforcement learning approaches to adaptive thinking remain unstable and
heavily reward-dependent. Here we propose \textbf{DART}, a supervised
\textbf{D}ifficulty-\textbf{A}daptive \textbf{R}easoning \textbf{T}runcation
framework that adjusts thinking length according to problem difficulty. By
distilling concise reasoning patterns from stronger models, interpolating them
into a continuum of reasoning styles, and curating optimal training data that
balances correctness and compactness, DART learns when to ``stop thinking''.
Across multiple mathematical benchmarks, experimental results demonstrate its
remarkable efficiency while preserving or improving accuracy, achieving a
significant 81.2\% reasoning truncation (DeepSeek-R1-Distill-Qwen-7B on GSM8K
dataset) with 5.33$\times$ computational acceleration. DART provides a stable
and general paradigm for efficient reasoning, advancing the development of
adaptive intelligence in LLMs.

</details>


### [127] [MiRAGE: Misconception Detection with Retrieval-Guided Multi-Stage Reasoning and Ensemble Fusion](https://arxiv.org/abs/2511.01182)
*Cuong Van Duc,Thai Tran Quoc,Minh Nguyen Dinh Tuan,Tam Vu Duc,Son Nguyen Van,Hanh Nguyen Thi*

Main category: cs.AI

TL;DR: MiRAGE是一个用于数学领域学生误解检测的新框架，通过检索引导的多阶段推理和集成融合，在开放回答中准确识别学生误解。


<details>
  <summary>Details</summary>
Motivation: 检测学生开放回答中的误解是一个长期挑战，需要语义精确性和逻辑推理能力。

Method: 三阶段框架：检索模块缩小候选池，推理模块使用思维链生成暴露逻辑不一致，重排模块通过对齐推理来优化预测，并通过集成融合策略统一组件。

Result: 在数学数据集上，MiRAGE在1/3/5级别分别获得0.82/0.92/0.93的平均精度分数，始终优于单个模块。

Conclusion: 通过将检索引导与多阶段推理相结合，MiRAGE减少了对大规模语言模型的依赖，同时为教育评估提供了可扩展且有效的解决方案。

Abstract: Detecting student misconceptions in open-ended responses is a longstanding
challenge, demanding semantic precision and logical reasoning. We propose
MiRAGE - Misconception Detection with Retrieval-Guided Multi-Stage Reasoning
and Ensemble Fusion, a novel framework for automated misconception detection in
mathematics. MiRAGE operates in three stages: (1) a Retrieval module narrows a
large candidate pool to a semantically relevant subset; (2) a Reasoning module
employs chain-of-thought generation to expose logical inconsistencies in
student solutions; and (3) a Reranking module refines predictions by aligning
them with the reasoning. These components are unified through an
ensemble-fusion strategy that enhances robustness and interpretability. On
mathematics datasets, MiRAGE achieves Mean Average Precision scores of
0.82/0.92/0.93 at levels 1/3/5, consistently outperforming individual modules.
By coupling retrieval guidance with multi-stage reasoning, MiRAGE reduces
dependence on large-scale language models while delivering a scalable and
effective solution for educational assessment.

</details>


### [128] [QiMeng-NeuComBack: Self-Evolving Translation from IR to Assembly Code](https://arxiv.org/abs/2511.01183)
*Hainan Fang,Yuanbo Wen,Jun Bi,Yihan Wang,Tonghui He,Yanlin Tang,Di Huang,Jiaming Guo,Rui Zhang,Qi Guo,Yunji Chen*

Main category: cs.AI

TL;DR: 本文提出了NeuComBack基准数据集和自进化提示优化方法，显著提升了LLM在IR到汇编编译中的功能正确性和性能表现。


<details>
  <summary>Details</summary>
Motivation: 编译器开发复杂且成本高昂，神经编译为简化编译器开发提供了新范式，但缺乏专用基准和评估方法阻碍了其实际应用。

Method: 引入NeuComBack基准数据集，定义神经编译工作流，并提出自进化提示优化方法，让LLM从自我调试轨迹中迭代优化提示策略。

Result: 功能正确率在x86_64上从44%提升到64%，在aarch64上从36%提升到58%。在正确生成的x86_64程序中，87.5%超过了clang-O3性能。

Conclusion: NeuComBack基准和自进化提示优化方法有效解决了神经编译中的关键挑战，显著提升了LLM生成汇编代码的质量和性能。

Abstract: Compilers, while essential, are notoriously complex systems that demand
prohibitively expensive human expertise to develop and maintain. The recent
advancements in Large Language Models (LLMs) offer a compelling new paradigm:
Neural Compilation, which could potentially simplify compiler development for
new architectures and facilitate the discovery of innovative optimization
techniques. However, several critical obstacles impede its practical adoption.
Firstly, a significant lack of dedicated benchmarks and robust evaluation
methodologies hinders objective assessment and tracking of progress in the
field. Secondly, systematically enhancing the reliability and performance of
LLM-generated assembly remains a critical challenge. Addressing these
challenges, this paper introduces NeuComBack, a novel benchmark dataset
specifically designed for IR-to-assembly compilation. Leveraging this dataset,
we first define a foundational Neural Compilation workflow and conduct a
comprehensive evaluation of the capabilities of recent frontier LLMs on Neural
Compilation, establishing new performance baselines. We further propose a
self-evolving prompt optimization method that enables LLMs to iteratively
evolve their internal prompt strategies by extracting insights from prior
self-debugging traces, thereby enhancing their neural compilation capabilities.
Experiments demonstrate that our method significantly improves both the
functional correctness and the performance of LLM-generated assembly code.
Compared to baseline prompts, the functional correctness rates improved from
44% to 64% on x86_64 and from 36% to 58% on aarch64, respectively. More
significantly, among the 16 correctly generated x86_64 programs using our
method, 14 (87.5%) surpassed clang-O3 performance.

</details>


### [129] [Graph Neural Network-Based Semi-Supervised Open-Set Fault Diagnosis for Marine Machinery Systems](https://arxiv.org/abs/2511.01258)
*Chuyue Lou,M. Amine Atoui*

Main category: cs.AI

TL;DR: 提出了一种半监督开放集故障诊断框架，用于处理海洋机械系统中训练时未见过的故障类型，提高深度学习模型在实际工业部署中的适用性。


<details>
  <summary>Details</summary>
Motivation: 现有故障诊断方法假设训练和测试集中的故障类别一致且已知，但在实际工业环境中会出现训练时未见过的未知故障类型，导致现有方法失效。

Method: 提出SOFD框架，包括可靠性子集构建过程（使用监督特征学习模型提取多层融合特征表示来选择未标记测试子集），然后将标记训练集和伪标记测试子集输入半监督诊断模型学习每个类的判别特征。

Result: 在公共海事基准数据集上的实验结果表明，所提出的SOFD框架具有有效性和优越性。

Conclusion: 该框架能够准确分类已知故障并有效检测未知样本，增强了深度学习模型在开放集故障诊断场景中的适用性。

Abstract: Recently, fault diagnosis methods for marine machinery systems based on deep
learning models have attracted considerable attention in the shipping industry.
Most existing studies assume fault classes are consistent and known between the
training and test datasets, and these methods perform well under controlled
environment. In practice, however, previously unseen or unknown fault types
(i.e., out-of-distribution or open-set observations not present during
training) can occur, causing such methods to fail and posing a significant
challenge to their widespread industrial deployment. To address this challenge,
this paper proposes a semi-supervised open-set fault diagnosis (SOFD) framework
that enhances and extends the applicability of deep learning models in open-set
fault diagnosis scenarios. The framework includes a reliability subset
construction process, which uses a multi-layer fusion feature representation
extracted by a supervised feature learning model to select an unlabeled test
subset. The labeled training set and pseudo-labeled test subset are then fed
into a semi-supervised diagnosis model to learn discriminative features for
each class, enabling accurate classification of known faults and effective
detection of unknown samples. Experimental results on a public maritime
benchmark dataset demonstrate the effectiveness and superiority of the proposed
SOFD framework.

</details>


### [130] [llmSHAP: A Principled Approach to LLM Explainability](https://arxiv.org/abs/2511.01311)
*Filip Naudot,Tobias Sundqvist,Timotheus Kampik*

Main category: cs.AI

TL;DR: 该论文研究了在基于大语言模型(LLM)的随机推理系统中应用Shapley值进行特征归因的方法，分析了不同实现变体下Shapley值原则的满足情况，并探讨了推理速度、准确性和原则达成之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 虽然Shapley值在确定性推理中能保证多个理想原则的满足，但在LLM这种随机推理系统中，这些保证可能不再成立，需要研究如何在这种非确定性环境中应用特征归因方法。

Method: 将Shapley值应用于LLM决策支持系统的特征归因，分析不同实现变体下原则满足情况，并研究LLM随机性对这些保证的影响。

Result: 研究表明在LLM的随机推理环境中，Shapley值原则的满足情况取决于具体实现方式，揭示了在解释性推理速度、与精确Shapley值归因的一致性以及原则达成之间的权衡关系。

Conclusion: 在LLM的随机推理系统中应用Shapley值进行特征归因时，需要仔细考虑实现变体，因为随机性会影响原则的保证，并且存在速度、准确性和原则满足之间的重要权衡。

Abstract: Feature attribution methods help make machine learning-based inference
explainable by determining how much one or several features have contributed to
a model's output. A particularly popular attribution method is based on the
Shapley value from cooperative game theory, a measure that guarantees the
satisfaction of several desirable principles, assuming deterministic inference.
We apply the Shapley value to feature attribution in large language model
(LLM)-based decision support systems, where inference is, by design, stochastic
(non-deterministic). We then demonstrate when we can and cannot guarantee
Shapley value principle satisfaction across different implementation variants
applied to LLM-based decision support, and analyze how the stochastic nature of
LLMs affects these guarantees. We also highlight trade-offs between explainable
inference speed, agreement with exact Shapley value attributions, and principle
attainment.

</details>


### [131] [OmniFuser: Adaptive Multimodal Fusion for Service-Oriented Predictive Maintenance](https://arxiv.org/abs/2511.01320)
*Ziqi Wang,Hailiang Zhao,Yuhao Yang,Daojiang Hu,Cheng Bao,Mingyi Liu,Kai Di,Schahram Dustdar,Zhongjie Wang,Shuiguang Deng*

Main category: cs.AI

TL;DR: 提出了OmniFuser多模态学习框架，通过融合视觉和传感器数据进行铣削刀具预测性维护，在刀具状态分类和力信号预测方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 智能制造系统中准确及时的刀具状态预测至关重要，非计划性刀具故障会导致质量下降和生产停机。需要可靠的服务导向型预测维护解决方案。

Method: 并行提取高分辨率刀具图像和切削力信号特征，采用无污染跨模态融合机制分离共享和模态特定组件，使用递归精化路径保留残差信息稳定融合动态。

Result: 在真实铣削数据集上的实验表明，OmniFuser在刀具状态分类（如锋利、使用中、钝化）和多步力信号预测方面持续优于最先进的基线方法。

Conclusion: OmniFuser为构建智能工业维护服务提供了可靠基础，学习到的表示可封装为可重用维护服务模块。

Abstract: Accurate and timely prediction of tool conditions is critical for intelligent
manufacturing systems, where unplanned tool failures can lead to quality
degradation and production downtime. In modern industrial environments,
predictive maintenance is increasingly implemented as an intelligent service
that integrates sensing, analysis, and decision support across production
processes. To meet the demand for reliable and service-oriented operation, we
present OmniFuser, a multimodal learning framework for predictive maintenance
of milling tools that leverages both visual and sensor data. It performs
parallel feature extraction from high-resolution tool images and cutting-force
signals, capturing complementary spatiotemporal patterns across modalities. To
effectively integrate heterogeneous features, OmniFuser employs a
contamination-free cross-modal fusion mechanism that disentangles shared and
modality-specific components, allowing for efficient cross-modal interaction.
Furthermore, a recursive refinement pathway functions as an anchor mechanism,
consistently retaining residual information to stabilize fusion dynamics. The
learned representations can be encapsulated as reusable maintenance service
modules, supporting both tool-state classification (e.g., Sharp, Used, Dulled)
and multi-step force signal forecasting. Experiments on real-world milling
datasets demonstrate that OmniFuser consistently outperforms state-of-the-art
baselines, providing a dependable foundation for building intelligent
industrial maintenance services.

</details>


### [132] [Unbiased Platform-Level Causal Estimation for Search Systems: A Competitive Isolation PSM-DID Framework](https://arxiv.org/abs/2511.01329)
*Ying Song,Yijing Wang,Hui Yang,Weihan Jin,Jun Xiong,Congyi Zhou,Jialin Zhu,Xiang Gao,Rong Chen,HuaGuang Deng,Ying Dai,Fei Xiao,Haihong Tang,Bo Zheng,KaiFu Zhang*

Main category: cs.AI

TL;DR: 提出了Competitive Isolation PSM-DID框架，结合倾向得分匹配和竞争隔离，用于搜索型双边市场中的平台级干预效果评估，解决了传统方法中的选择偏差和网络干扰问题。


<details>
  <summary>Details</summary>
Motivation: 传统PSM-DID方法在搜索型双边市场中容易受到选择偏差和跨单元干扰的影响，无法准确评估平台级干预效果。

Method: 将倾向得分匹配与竞争隔离相结合，在互斥条件下提供理论保证的无偏估计，专注于平台级指标（如订单量、GMV）而非商品级指标。

Result: 实验显示相比基线方法显著降低了干扰效应和估计方差，在大规模市场平台成功部署验证了实用性。

Conclusion: 该框架为平台级因果推断提供了有效的解决方案，解决了传统方法在双边市场中面临的系统性效应挑战。

Abstract: Evaluating platform-level interventions in search-based two-sided
marketplaces is fundamentally challenged by systemic effects such as spillovers
and network interference. While widely used for causal inference, the PSM
(Propensity Score Matching) - DID (Difference-in-Differences) framework remains
susceptible to selection bias and cross-unit interference from unaccounted
spillovers. In this paper, we introduced Competitive Isolation PSM-DID, a novel
causal framework that integrates propensity score matching with competitive
isolation to enable platform-level effect measurement (e.g., order volume, GMV)
instead of item-level metrics in search systems.
  Our approach provides theoretically guaranteed unbiased estimation under
mutual exclusion conditions, with an open dataset released to support
reproducible research on marketplace interference (github.com/xxxx). Extensive
experiments demonstrate significant reductions in interference effects and
estimation variance compared to baseline methods. Successful deployment in a
large-scale marketplace confirms the framework's practical utility for
platform-level causal inference.

</details>


### [133] [Automatic Minds: Cognitive Parallels Between Hypnotic States and Large Language Model Processing](https://arxiv.org/abs/2511.01363)
*Giuseppe Riva,Brenda K. Wiederhold,Fabrizia Mantovani*

Main category: cs.AI

TL;DR: 该论文探讨了催眠认知过程与大型语言模型在自动性、监控抑制和情境依赖性三个原则上的深层功能相似性，揭示了无主观能动性的功能性行为机制。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索催眠认知与LLMs之间的功能平行性，理解无意识意图如何产生复杂行为，为AI可靠性提供新视角。

Method: 采用比较分析方法，通过三个核心原则（自动性、监控抑制、情境依赖性）系统对比催眠与LLMs的认知机制。

Result: 发现两种系统都产生连贯但无根基的输出，需要外部解释者赋予意义；都能展现功能性能动性而无主观能动性。

Conclusion: 未来可靠AI应借鉴人类心智架构，构建集成生成流畅性与执行监控机制的混合架构。

Abstract: The cognitive processes of the hypnotized mind and the computational
operations of large language models (LLMs) share deep functional parallels.
Both systems generate sophisticated, contextually appropriate behavior through
automatic pattern-completion mechanisms operating with limited or unreliable
executive oversight. This review examines this convergence across three
principles: automaticity, in which responses emerge from associative rather
than deliberative processes; suppressed monitoring, leading to errors such as
confabulation in hypnosis and hallucination in LLMs; and heightened contextual
dependency, where immediate cues (for example, the suggestion of a therapist or
the prompt of the user) override stable knowledge.
  These mechanisms reveal an observer-relative meaning gap: both systems
produce coherent but ungrounded outputs that require an external interpreter to
supply meaning. Hypnosis and LLMs also exemplify functional agency - the
capacity for complex, goal-directed, context-sensitive behavior - without
subjective agency, the conscious awareness of intention and ownership that
defines human action. This distinction clarifies how purposive behavior can
emerge without self-reflective consciousness, governed instead by structural
and contextual dynamics. Finally, both domains illuminate the phenomenon of
scheming: automatic, goal-directed pattern generation that unfolds without
reflective awareness. Hypnosis provides an experimental model for understanding
how intention can become dissociated from conscious deliberation, offering
insights into the hidden motivational dynamics of artificial systems.
Recognizing these parallels suggests that the future of reliable AI lies in
hybrid architectures that integrate generative fluency with mechanisms of
executive monitoring, an approach inspired by the complex, self-regulating
architecture of the human mind.

</details>


### [134] [Align to Misalign: Automatic LLM Jailbreak with Meta-Optimized LLM Judges](https://arxiv.org/abs/2511.01375)
*Hamin Koo,Minseon Kim,Jaehyung Kim*

Main category: cs.AI

TL;DR: AMIS是一个元优化框架，通过双层结构联合优化越狱提示和评分模板，解决了现有方法依赖稀疏二进制信号或人工评分模板的问题，在多个基准测试中实现了最先进的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于优化的越狱方法要么依赖稀疏的二进制攻击成功率信号，要么使用引入人为偏见的手工评分模板，这些局限性影响了越狱效果和评估准确性。

Method: 采用双层优化结构：内层循环使用固定评分模板通过细粒度反馈优化提示；外层循环使用ASR对齐分数优化评分模板，使模板能更好地反映真实攻击结果。

Result: 在AdvBench和JBB-Behaviors上的评估显示，AMIS实现了最先进的性能：Claude-3.5-Haiku上88.0% ASR，Claude-4-Sonnet上100.0% ASR，显著优于现有基线方法。

Conclusion: AMIS框架通过联合优化提示和评分模板，能够生成更强的越狱提示和更准确的评分信号，为大语言模型的安全性测试提供了更有效的工具。

Abstract: Identifying the vulnerabilities of large language models (LLMs) is crucial
for improving their safety by addressing inherent weaknesses. Jailbreaks, in
which adversaries bypass safeguards with crafted input prompts, play a central
role in red-teaming by probing LLMs to elicit unintended or unsafe behaviors.
Recent optimization-based jailbreak approaches iteratively refine attack
prompts by leveraging LLMs. However, they often rely heavily on either binary
attack success rate (ASR) signals, which are sparse, or manually crafted
scoring templates, which introduce human bias and uncertainty in the scoring
outcomes. To address these limitations, we introduce AMIS (Align to MISalign),
a meta-optimization framework that jointly evolves jailbreak prompts and
scoring templates through a bi-level structure. In the inner loop, prompts are
refined using fine-grained and dense feedback using a fixed scoring template.
In the outer loop, the template is optimized using an ASR alignment score,
gradually evolving to better reflect true attack outcomes across queries. This
co-optimization process yields progressively stronger jailbreak prompts and
more calibrated scoring signals. Evaluations on AdvBench and JBB-Behaviors
demonstrate that AMIS achieves state-of-the-art performance, including 88.0%
ASR on Claude-3.5-Haiku and 100.0% ASR on Claude-4-Sonnet, outperforming
existing baselines by substantial margins.

</details>


### [135] [Relaxing partition admissibility in Cluster-DAGs: a causal calculus with arbitrary variable clustering](https://arxiv.org/abs/2511.01396)
*Clément Yvernes,Emilie Devijver,Adèle H. Ribeiro,Marianne Clausel--Lesourd,Éric Gaussier*

Main category: cs.AI

TL;DR: 本文扩展了C-DAG框架，支持任意变量聚类，允许循环C-DAG表示，并扩展了d-分离和因果演算概念，显著拓宽了跨集群因果推理的范围。


<details>
  <summary>Details</summary>
Motivation: 传统C-DAG框架要求聚类必须产生无环图，这限制了其在某些场景下的应用。本文旨在放宽这一约束，使C-DAG能够处理任意聚类情况。

Method: 通过放松分区可容许性约束，允许循环C-DAG表示，并扩展d-分离和因果演算到这种设置中。

Result: 提出的演算在do-演算方面是可靠且原子完备的：所有有效的集群级干预查询都可以使用我们的规则推导，每个规则对应一个原始的do-演算步骤。

Conclusion: 该扩展显著拓宽了C-DAG的应用范围，使其能够在以前难以处理的场景中进行因果推理。

Abstract: Cluster DAGs (C-DAGs) provide an abstraction of causal graphs in which nodes
represent clusters of variables, and edges encode both cluster-level causal
relationships and dependencies arisen from unobserved confounding. C-DAGs
define an equivalence class of acyclic causal graphs that agree on
cluster-level relationships, enabling causal reasoning at a higher level of
abstraction. However, when the chosen clustering induces cycles in the
resulting C-DAG, the partition is deemed inadmissible under conventional C-DAG
semantics. In this work, we extend the C-DAG framework to support arbitrary
variable clusterings by relaxing the partition admissibility constraint,
thereby allowing cyclic C-DAG representations. We extend the notions of
d-separation and causal calculus to this setting, significantly broadening the
scope of causal reasoning across clusters and enabling the application of
C-DAGs in previously intractable scenarios. Our calculus is both sound and
atomically complete with respect to the do-calculus: all valid interventional
queries at the cluster level can be derived using our rules, each corresponding
to a primitive do-calculus step.

</details>


### [136] [Modulation of temporal decision-making in a deep reinforcement learning agent under the dual-task paradigm](https://arxiv.org/abs/2511.01415)
*Amrapali Pednekar,Álvaro Garrido-Pérez,Yara Khaluf,Pieter Simoens*

Main category: cs.AI

TL;DR: 该研究从AI视角探讨双任务范式中的时间处理干扰，使用简化的Overcooked环境训练DRL智能体，发现双任务智能体相比单任务智能体显著高估时间，这与人类时间研究一致。


<details>
  <summary>Details</summary>
Motivation: 探索深度强化学习智能体在双任务范式中的时间处理行为，并与生物系统的时间处理行为进行比较，以促进对两者的更好理解。

Method: 使用简化的Overcooked环境实现单任务(T)和双任务(T+N)变体，分别训练两个深度强化学习智能体。双任务包含时间产生任务和数字比较任务。

Result: 双任务智能体相对于单任务智能体显著高估时间，这一结果在四个目标持续时间上一致。LSTM层的初步分析未发现专门的计时机制。

Conclusion: 需要进一步研究智能体的时间保持机制以理解观察到的行为模式，这是探索DRL行为与生物系统行为相似性的初步尝试。

Abstract: This study explores the interference in temporal processing within a
dual-task paradigm from an artificial intelligence (AI) perspective. In this
context, the dual-task setup is implemented as a simplified version of the
Overcooked environment with two variations, single task (T) and dual task
(T+N). Both variations involve an embedded time production task, but the dual
task (T+N) additionally involves a concurrent number comparison task. Two deep
reinforcement learning (DRL) agents were separately trained for each of these
tasks. These agents exhibited emergent behavior consistent with human timing
research. Specifically, the dual task (T+N) agent exhibited significant
overproduction of time relative to its single task (T) counterpart. This result
was consistent across four target durations. Preliminary analysis of neural
dynamics in the agents' LSTM layers did not reveal any clear evidence of a
dedicated or intrinsic timer. Hence, further investigation is needed to better
understand the underlying time-keeping mechanisms of the agents and to provide
insights into the observed behavioral patterns. This study is a small step
towards exploring parallels between emergent DRL behavior and behavior observed
in biological systems in order to facilitate a better understanding of both.

</details>


### [137] [Learning to Seek Evidence: A Verifiable Reasoning Agent with Causal Faithfulness Analysis](https://arxiv.org/abs/2511.01425)
*Yuhang Huang,Zekai Lin,Fan Zhong,Lei Liu*

Main category: cs.AI

TL;DR: 提出了一种基于交互式代理的AI解释方法，通过可审计的行动序列生成可验证的解释，在医疗诊断任务中显著提升了校准准确性和解释忠实度。


<details>
  <summary>Details</summary>
Motivation: 解决高风险领域（如医疗）中AI模型解释缺乏可验证性的问题，这会影响用户对AI系统的信任。

Method: 使用强化学习训练交互式代理，学习策略性地寻求外部视觉证据来支持诊断推理，并通过因果干预方法验证解释的忠实性。

Result: 实验显示该方法比非交互式基线降低了18%的Brier分数，通过掩蔽视觉证据观察到性能显著下降（ΔBrier=+0.029），证明证据对决策过程至关重要。

Conclusion: 该工作为构建具有可验证和忠实推理能力的AI系统提供了实用框架。

Abstract: Explanations for AI models in high-stakes domains like medicine often lack
verifiability, which can hinder trust. To address this, we propose an
interactive agent that produces explanations through an auditable sequence of
actions. The agent learns a policy to strategically seek external visual
evidence to support its diagnostic reasoning. This policy is optimized using
reinforcement learning, resulting in a model that is both efficient and
generalizable. Our experiments show that this action-based reasoning process
significantly improves calibrated accuracy, reducing the Brier score by 18\%
compared to a non-interactive baseline. To validate the faithfulness of the
agent's explanations, we introduce a causal intervention method. By masking the
visual evidence the agent chooses to use, we observe a measurable degradation
in its performance ($\Delta$Brier=+0.029), confirming that the evidence is
integral to its decision-making process. Our work provides a practical
framework for building AI systems with verifiable and faithful reasoning
capabilities.

</details>


### [138] [Robust Multimodal Sentiment Analysis via Double Information Bottleneck](https://arxiv.org/abs/2511.01444)
*Huiting Huang,Tieliang Gong,Kai He,Jialun Wu,Erik Cambria,Mengling Feng*

Main category: cs.AI

TL;DR: 提出双信息瓶颈(DIB)策略，通过低秩Renyi熵框架解决多模态情感分析中的噪声干扰和冗余信息问题，获得统一紧凑的多模态表示。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个关键限制：对噪声污染的单模态数据学习不足导致跨模态交互受损，以及多模态表示融合不充分导致丢弃判别性单模态信息而保留冗余信息。

Method: DIB包含两个关键模块：1)通过最大化任务相关信息并丢弃冗余信息来学习充分压缩的单模态表示；2)通过新颖的注意力瓶颈融合机制确保多模态表示的判别能力。

Result: 在CMU-MOSI、CMU-MOSEI、CH-SIMS和MVSA-Single数据集上验证有效性，在CMU-MOSI上Acc-7达到47.4%，在CH-SIMS上F1-score达到81.63%，比次优基线提升1.19%。在噪声下，CMU-MOSI和CMU-MOSEI性能仅下降0.36%和0.29%。

Conclusion: DIB策略能够有效过滤单模态数据中的噪声信息，同时捕捉模态间互补性，在噪声环境下表现出优异的鲁棒性。

Abstract: Multimodal sentiment analysis has received significant attention across
diverse research domains. Despite advancements in algorithm design, existing
approaches suffer from two critical limitations: insufficient learning of
noise-contaminated unimodal data, leading to corrupted cross-modal
interactions, and inadequate fusion of multimodal representations, resulting in
discarding discriminative unimodal information while retaining multimodal
redundant information. To address these challenges, this paper proposes a
Double Information Bottleneck (DIB) strategy to obtain a powerful, unified
compact multimodal representation. Implemented within the framework of low-rank
Renyi's entropy functional, DIB offers enhanced robustness against diverse
noise sources and computational tractability for high-dimensional data, as
compared to the conventional Shannon entropy-based methods. The DIB comprises
two key modules: 1) learning a sufficient and compressed representation of
individual unimodal data by maximizing the task-relevant information and
discarding the superfluous information, and 2) ensuring the discriminative
ability of multimodal representation through a novel attention bottleneck
fusion mechanism. Consequently, DIB yields a multimodal representation that
effectively filters out noisy information from unimodal data while capturing
inter-modal complementarity. Extensive experiments on CMU-MOSI, CMU-MOSEI,
CH-SIMS, and MVSA-Single validate the effectiveness of our method. The model
achieves 47.4% accuracy under the Acc-7 metric on CMU-MOSI and 81.63% F1-score
on CH-SIMS, outperforming the second-best baseline by 1.19%. Under noise, it
shows only 0.36% and 0.29% performance degradation on CMU-MOSI and CMU-MOSEI
respectively.

</details>


### [139] [From Passive to Proactive: A Multi-Agent System with Dynamic Task Orchestration for Intelligent Medical Pre-Consultation](https://arxiv.org/abs/2511.01445)
*ChengZhang Yu,YingRu He,Hongyan Cheng,nuo Cheng,Zhixing Liu,Dongxu Mu,Zhangrui Shen,Zhanpeng Jin*

Main category: cs.AI

TL;DR: 该研究提出了一个分层多智能体框架，将被动医疗AI系统转变为主动查询代理，通过自主任务编排提升预咨询效率。在1372份电子健康记录上验证，实现了87%的初级分诊准确率和80.5%的次级分诊准确率。


<details>
  <summary>Details</summary>
Motivation: 全球医疗系统面临患者数量增加和咨询时间有限的关键挑战，现有AI系统受限于被动交互范式和上下文管理问题，需要更主动的预咨询解决方案。

Method: 开发了八智能体架构，将预咨询分解为四个主要任务：分诊、现病史收集、既往史收集和主诉生成，前三个任务进一步分为13个领域特定子任务，采用集中控制机制进行自主任务编排。

Result: 在多个基础模型上评估，分诊准确率达87.0%，任务完成率98.2%，临床质量评分平均4.56-4.69（5分制），咨询在12.7-16.9轮内完成。

Conclusion: 该模型无关架构在不同基础模型上保持高性能，通过本地部署保护数据隐私，展示了自主AI系统在临床环境中提升预咨询效率和质量的潜力。

Abstract: Global healthcare systems face critical challenges from increasing patient
volumes and limited consultation times, with primary care visits averaging
under 5 minutes in many countries. While pre-consultation processes
encompassing triage and structured history-taking offer potential solutions,
they remain limited by passive interaction paradigms and context management
challenges in existing AI systems. This study introduces a hierarchical
multi-agent framework that transforms passive medical AI systems into proactive
inquiry agents through autonomous task orchestration. We developed an
eight-agent architecture with centralized control mechanisms that decomposes
pre-consultation into four primary tasks: Triage ($T_1$), History of Present
Illness collection ($T_2$), Past History collection ($T_3$), and Chief
Complaint generation ($T_4$), with $T_1$--$T_3$ further divided into 13
domain-specific subtasks. Evaluated on 1,372 validated electronic health
records from a Chinese medical platform across multiple foundation models
(GPT-OSS 20B, Qwen3-8B, Phi4-14B), the framework achieved 87.0% accuracy for
primary department triage and 80.5% for secondary department classification,
with task completion rates reaching 98.2% using agent-driven scheduling versus
93.1% with sequential processing. Clinical quality scores from 18 physicians
averaged 4.56 for Chief Complaints, 4.48 for History of Present Illness, and
4.69 for Past History on a 5-point scale, with consultations completed within
12.7 rounds for $T_2$ and 16.9 rounds for $T_3$. The model-agnostic
architecture maintained high performance across different foundation models
while preserving data privacy through local deployment, demonstrating the
potential for autonomous AI systems to enhance pre-consultation efficiency and
quality in clinical settings.

</details>


### [140] [TPS-Bench: Evaluating AI Agents' Tool Planning \& Scheduling Abilities in Compounding Tasks](https://arxiv.org/abs/2511.01527)
*Hanwen Xu,Xuyao Huang,Yuzhe Liu,Kai Yu,Zhijie Deng*

Main category: cs.AI

TL;DR: TPS-Bench基准测试评估LLM代理在需要工具规划与调度的复合任务中的表现，发现大多数模型能合理规划工具但调度能力差异显著，强化学习可提升调度效率。


<details>
  <summary>Details</summary>
Motivation: 探索LLM代理是否能处理需要多样化工具的复合现实世界问题，这些任务不仅需要选择合适的工具，还需要策略性地安排执行顺序以确保效率。

Method: 构建TPS-Bench基准，包含200个复合任务和数百个MCP工具，评估LLM代理在工具规划和调度方面的能力，并使用强化学习改进调度效率。

Result: GLM-4.5达到64.72%的任务完成率但执行时间长，GPT-4o优先并行工具调用但完成率仅45.08%。对Qwen3-1.7B使用强化学习后，执行时间减少14%，任务完成率提升6%。

Conclusion: LLM代理在工具规划方面表现合理，但调度能力有待提升，强化学习是改善调度效率的有效方法，无需牺牲性能。

Abstract: Large language model (LLM) agents have exhibited strong problem-solving
competence across domains like research and coding. Yet, it remains
underexplored whether LLM agents can tackle compounding real-world problems
that require a diverse set of tools to complete. Given a broad, heterogeneous
tool repository, LLM agents must not only select appropriate tools based on
task planning analysis but also strategically schedule the execution order to
ensure efficiency. This paper introduces TPS-Bench to benchmark the ability of
LLM agents in solving such problems that demand Tool Planning and Scheduling.
TPS-Bench collects 200 compounding tasks of two difficulty levels, based on a
tool repository containing hundreds of model context protocol (MCP) tools. In
particular, each task is composed of multiple subtasks, such as web search, map
navigation, calendar checking, etc., and each subtask can be completed by a
basic tool. Our evaluation emphasizes both task completion rate and efficiency.
The empirical studies on popular closed-source and open-source LLMs indicate
that most models can perform reasonable tool planning, but differ in
scheduling. For example, GLM-4.5 achieves an outperforming task completion rate
of 64.72% with extensive sequential tool calls, hence suffering from
significantly long execution time. By contrast, GPT-4o prioritizes parallel
tool calls but achieves only a 45.08% completion rate. Considering
reinforcement learning (RL) can be a viable way to improve the scheduling
efficiency without compromising performance, we perform an initial study on
Qwen3-1.7B and witness a 14% reduction in execution time alongside a 6% gain in
task completion rate based on rarely 100 RL training samples. Our code is
available https://github.com/hanwenxu1/mcp-agent.

</details>


### [141] [Analyzing Sustainability Messaging in Large-Scale Corporate Social Media](https://arxiv.org/abs/2511.01550)
*Ujjwal Sharma,Stevan Rudinac,Ana Mićković,Willemijn van Dolen,Marcel Worring*

Main category: cs.AI

TL;DR: 提出一个多模态分析管道，利用大型基础模型分析企业社交媒体内容，重点关注可持续发展相关沟通，通过LLM自动标注SDG主题，结合VLM进行视觉分析。


<details>
  <summary>Details</summary>
Motivation: 解决企业社交媒体内容在X等平台上多变、多模态且语义模糊的挑战，探索基础模型作为临时标注工具的潜力，避免昂贵的人工标注成本。

Method: 使用LLM集成方法自动标注企业推文的SDG主题对齐，结合VLM在视觉理解框架中通过语义聚类分析视觉可持续发展沟通模式。

Result: 揭示了不同行业在SDG参与度上的差异、时间趋势，以及企业信息与ESG风险、消费者参与度之间的关联。

Conclusion: 提出的自动标签生成和语义视觉聚类方法可广泛应用于其他领域，为大规模社交媒体分析提供了灵活框架。

Abstract: In this work, we introduce a multimodal analysis pipeline that leverages
large foundation models in vision and language to analyze corporate social
media content, with a focus on sustainability-related communication. Addressing
the challenges of evolving, multimodal, and often ambiguous corporate messaging
on platforms such as X (formerly Twitter), we employ an ensemble of large
language models (LLMs) to annotate a large corpus of corporate tweets on their
topical alignment with the 17 Sustainable Development Goals (SDGs). This
approach avoids the need for costly, task-specific annotations and explores the
potential of such models as ad-hoc annotators for social media data that can
efficiently capture both explicit and implicit references to sustainability
themes in a scalable manner. Complementing this textual analysis, we utilize
vision-language models (VLMs), within a visual understanding framework that
uses semantic clusters to uncover patterns in visual sustainability
communication. This integrated approach reveals sectoral differences in SDG
engagement, temporal trends, and associations between corporate messaging,
environmental, social, governance (ESG) risks, and consumer engagement. Our
methods-automatic label generation and semantic visual clustering-are broadly
applicable to other domains and offer a flexible framework for large-scale
social media analysis.

</details>


### [142] [ExplicitLM: Decoupling Knowledge from Parameters via Explicit Memory Banks](https://arxiv.org/abs/2511.01581)
*Chengzhang Yu,Zening Lu,Chenyang Zheng,Chiyue Wang,Yiming Zhang,Zhanpeng Jin*

Main category: cs.AI

TL;DR: ExplicitLM是一种新颖的LLM架构，通过外部可读记忆库实现知识的显式存储和可解释性，解决了传统LLM知识过时和参数纠缠的问题。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型中知识过时和缺乏可解释性的问题，由于知识隐式存储在纠缠的网络参数中，无法进行针对性更新和推理透明度分析。

Method: 采用百万级外部记忆库存储人类可读知识作为token序列，设计可微分的两阶段检索机制：基于产品键分解的粗粒度过滤和基于Gumbel-Softmax的细粒度匹配，支持端到端训练。

Result: 在知识密集型任务上比标准Transformer提升43.67%，在低数据场景（10k样本）下获得3.62倍增益。正确预测的记忆命中率高出49%。

Conclusion: ExplicitLM证明可解释、可更新的模型在保持竞争力的同时，能够提供前所未有的知识透明度，优于冻结检索的RAG系统。

Abstract: Large language models suffer from knowledge staleness and lack of
interpretability due to implicit knowledge storage across entangled network
parameters, preventing targeted updates and reasoning transparency. We propose
ExplicitLM, a novel architecture featuring a million-scale external memory bank
storing human-readable knowledge as token sequences, enabling direct inspection
and modification. We design a differentiable two-stage retrieval mechanism with
efficient coarse-grained filtering via product key decomposition (reducing
complexity from $\mathcal{O}(N \cdot |I|)$ to $\mathcal{O}(\sqrt{N} \cdot
|I|)$) and fine-grained Gumbel-Softmax matching for end-to-end training.
Inspired by dual-system cognitive theory, we partition knowledge into frozen
explicit facts (20%) and learnable implicit patterns (80%), maintained through
Exponential Moving Average updates for stability. ExplicitLM achieves up to
43.67% improvement on knowledge-intensive tasks versus standard Transformers,
with 3.62$\times$ gains in low-data regimes (10k samples). Analysis shows
strong correlations between memory retrieval and performance, with correct
predictions achieving 49% higher hit rates. Unlike RAG systems with frozen
retrieval, our jointly optimized architecture demonstrates that interpretable,
updatable models can maintain competitive performance while providing
unprecedented knowledge transparency.

</details>


### [143] [IVGAE-TAMA-BO: A novel temporal dynamic variational graph model for link prediction in global food trade networks with momentum structural memory and Bayesian optimization](https://arxiv.org/abs/2511.01639)
*Sicheng Wang,Shuhao Chen,Jingran Zhou,Chengyi Tu*

Main category: cs.AI

TL;DR: IVGAE-TAMA-BO是一种新颖的动态图神经网络，用于建模全球食品贸易网络的演化结构并预测未来贸易链接，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 全球食品贸易网络在政治、经济和环境因素影响下动态演化，传统方法难以有效建模和预测。准确捕捉贸易网络中的时间模式对于提高链接预测的准确性和鲁棒性至关重要。

Method: 基于IVGAE框架，提出贸易感知动量聚合器(TAMA)来捕捉贸易网络的时间演化，联合建模短期波动和长期结构依赖。采用基于动量的结构记忆机制和贝叶斯优化自动调参。

Result: 在五个作物特定数据集上的实验表明，IVGAE-TAMA显著优于静态IVGAE和其他动态基线方法，贝叶斯优化进一步提升了IVGAE-TAMA-BO的性能。

Conclusion: 该框架为全球贸易网络结构预测提供了稳健且可扩展的解决方案，在食品安全监测和政策决策支持方面具有强大应用潜力。

Abstract: Global food trade plays a crucial role in ensuring food security and
maintaining supply chain stability. However, its network structure evolves
dynamically under the influence of geopolitical, economic, and environmental
factors, making it challenging to model and predict future trade links.
Effectively capturing temporal patterns in food trade networks is therefore
essential for improving the accuracy and robustness of link prediction. This
study introduces IVGAE-TAMA-BO, a novel dynamic graph neural network designed
to model evolving trade structures and predict future links in global food
trade networks. To the best of our knowledge, this is the first work to apply
dynamic graph neural networks to this domain, significantly enhancing
predictive performance. Building upon the original IVGAE framework, the
proposed model incorporates a Trade-Aware Momentum Aggregator (TAMA) to capture
the temporal evolution of trade networks, jointly modeling short-term
fluctuations and long-term structural dependencies. A momentum-based structural
memory mechanism further improves predictive stability and performance. In
addition, Bayesian optimization is used to automatically tune key
hyperparameters, enhancing generalization across diverse trade scenarios.
Extensive experiments on five crop-specific datasets demonstrate that
IVGAE-TAMA substantially outperforms the static IVGAE and other dynamic
baselines by effectively modeling temporal dependencies, while Bayesian
optimization further boosts performance in IVGAE-TAMA-BO. These results
highlight the proposed framework as a robust and scalable solution for
structural prediction in global trade networks, with strong potential for
applications in food security monitoring and policy decision support.

</details>


### [144] [Hybrid Retrieval-Augmented Generation Agent for Trustworthy Legal Question Answering in Judicial Forensics](https://arxiv.org/abs/2511.01668)
*Yueqing Xi,Yifan Bai,Huasen Luo,Weiliang Wen,Hui Liu,Haoliang Li*

Main category: cs.AI

TL;DR: 提出了一种结合检索增强生成和多模型集成的混合法律问答代理，通过检索优先策略、模型集成和人工审核机制，显著减少幻觉并提高法律合规性。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型在法律问答中容易产生幻觉，而静态知识库难以跟上法律条文的频繁更新，需要确保法律咨询的准确性和可追溯性。

Method: 采用检索优先策略：当可信法律知识库有相关证据时使用RAG生成答案，否则使用多个LLM生成候选答案并由专门选择器评分返回最优结果，高质量输出经人工审核后写回知识库。

Result: 在Law_QA数据集上的实验表明，该方法在F1、ROUGE-L和LLM-as-a-Judge指标上显著优于单模型基线和普通RAG流程。

Conclusion: 该系统有效减少幻觉，提高答案质量和法律合规性，推动了媒体取证技术在司法场景中的实际落地。

Abstract: As artificial intelligence permeates judicial forensics, ensuring the
veracity and traceability of legal question answering (QA) has become critical.
Conventional large language models (LLMs) are prone to hallucination, risking
misleading guidance in legal consultation, while static knowledge bases
struggle to keep pace with frequently updated statutes and case law. We present
a hybrid legal QA agent tailored for judicial settings that integrates
retrieval-augmented generation (RAG) with multi-model ensembling to deliver
reliable, auditable, and continuously updatable counsel. The system prioritizes
retrieval over generation: when a trusted legal repository yields relevant
evidence, answers are produced via RAG; otherwise, multiple LLMs generate
candidates that are scored by a specialized selector, with the top-ranked
answer returned. High-quality outputs then undergo human review before being
written back to the repository, enabling dynamic knowledge evolution and
provenance tracking. Experiments on the Law\_QA dataset show that our hybrid
approach significantly outperforms both a single-model baseline and a vanilla
RAG pipeline on F1, ROUGE-L, and an LLM-as-a-Judge metric. Ablations confirm
the complementary contributions of retrieval prioritization, model ensembling,
and the human-in-the-loop update mechanism. The proposed system demonstrably
reduces hallucination while improving answer quality and legal compliance,
advancing the practical landing of media forensics technologies in judicial
scenarios.

</details>


### [145] [Simulating Environments with Reasoning Models for Agent Training](https://arxiv.org/abs/2511.01824)
*Yuetai Li,Huseyin A Inan,Xiang Yue,Wei-Ning Chen,Lukas Wutschitz,Janardhan Kulkarni,Radha Poovendran,Robert Sim,Saravan Rajmohan*

Main category: cs.AI

TL;DR: LLM代理在复杂环境中表现脆弱，本文提出Simia-SFT和Simia-RL框架，通过LLM模拟环境反馈实现无需真实环境数据的可扩展智能体训练。


<details>
  <summary>Details</summary>
Motivation: LLM代理在需要跨多种工具和模式的复杂环境中表现脆弱，而构建定制训练环境既笨重又限制进展。

Method: 提出两个框架：Simia-SFT通过扩增小规模种子集生成多样化轨迹数据；Simia-RL通过LLM模拟反馈实现无需真实环境的强化学习训练。

Result: 微调开源模型在多个基准测试中取得一致改进，超越GPT-4o并在τ²-Bench上接近o4-mini水平。

Conclusion: Simia-SFT和Simia-RL能够实现无需环境工程的规模化智能体训练，用灵活的LLM模拟替代笨重的实现方案。

Abstract: LLM agents excel in compact environments requiring deep reasoning but remain
brittle when operating in broader, more complex contexts that demand robustness
across diverse tools and schemas. Building bespoke environments for training is
heavy, brittle, and limits progress. In this paper, we demonstrate that LLMs
can simulate realistic environment feedback without access to actual testbed
data or APIs. Inspired by this capability, we propose two frameworks:
Simia-SFT, a pipeline that synthesizes SFT data by amplifying small seed sets
into diverse trajectories in an environment-agnostic manner, and Simia-RL, a
framework that enables RL training without real environment implementations
through LLM-simulated feedback. Fine-tuning open models yields consistent
improvements across multiple benchmarks, surpassing GPT-4o and approaching
o4-mini on $\tau^2$-Bench. Together, Simia-SFT and Simia-RL enable scalable
agent training without environment engineering, replacing heavy and brittle
implementations with flexible LLM-based simulation.

</details>
