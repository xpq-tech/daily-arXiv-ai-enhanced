<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 57]
- [cs.AI](#cs.AI) [Total: 60]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Direct Token Optimization: A Self-contained Approach to Large Language Model Unlearning](https://arxiv.org/abs/2510.00125)
*Hong kyu Lee,Ruixuan Liu,Li Xiong*

Main category: cs.CL

TL;DR: 提出了一种名为直接令牌优化（DTO）的新型自包含机器学习遗忘方法，无需外部资源即可优化令牌级目标，在多个基准数据集上比最新基线方法遗忘质量提升高达16.8倍。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型遗忘方法通常依赖辅助语言模型、保留数据集或商业AI服务，这在实际应用中不切实际且可能引入额外隐私风险。

Method: 直接优化令牌级目标，将序列中的令牌分为目标令牌（用于优化遗忘目标）和非目标令牌（用于保持模型效用），无需外部资源。

Result: 在多个基准数据集上，DTO比最新基线方法在遗忘质量上提升高达16.8倍，同时保持可比较的模型效用水平。

Conclusion: DTO是一种有效的自包含遗忘方法，能够在不依赖外部资源的情况下实现高质量的遗忘效果，同时保持模型性能。

Abstract: Machine unlearning is an emerging technique that removes the influence of a
subset of training data (forget set) from a model without full retraining, with
applications including privacy protection, content moderation, and model
correction. The key challenge lies in ensuring that the model completely
forgets the knowledge of the forget set without compromising its overall
utility. Existing unlearning methods for large language models (LLMs) often
utilize auxiliary language models, retain datasets, or even commercial AI
services for effective unlearning and maintaining the model utility. However,
dependence on these external resources is often impractical and could
potentially introduce additional privacy risks. In this work, we propose direct
token optimization (DTO), a novel self-contained unlearning approach for LLMs
that directly optimizes the token level objectives and eliminates the need for
external resources. Given a sequence to unlearn, we identify two categories of
tokens: target tokens, which capture critical knowledge for unlearning, and the
remaining non-target tokens, which are crucial for maintaining the model
utility. The former are used to optimize the unlearning objective, while the
latter serve to preserve the model's performance. The experimental results show
that the proposed DTO achieves up to 16.8$\times$ improvement in forget quality
on several benchmark datasets than the latest baselines while maintaining a
comparable level of model utility.

</details>


### [2] [TAMA: Tool-Augmented Multimodal Agent for Procedural Activity Understanding](https://arxiv.org/abs/2510.00161)
*Kimihiro Hasegawa,Wiradee Imrattanatrai,Masaki Asada,Ken Fukuda,Teruko Mitamura*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Procedural activity assistants potentially support humans in a variety of
settings, from our daily lives, e.g., cooking or assembling flat-pack
furniture, to professional situations, e.g., manufacturing or biological
experiments. Despite its potential use cases, the system development tailored
for such an assistant is still underexplored. In this paper, we propose a novel
framework, called TAMA, a Tool-Augmented Multimodal Agent, for procedural
activity understanding. TAMA enables interleaved multimodal reasoning by making
use of multimedia-returning tools in a training-free setting. Our experimental
result on the multimodal procedural QA dataset, ProMQA-Assembly, shows that our
approach can improve the performance of vision-language models, especially
GPT-5 and MiMo-VL. Furthermore, our ablation studies provide empirical support
for the effectiveness of two features that characterize our framework,
multimedia-returning tools and agentic flexible tool selection. We believe our
proposed framework and experimental results facilitate the thinking with images
paradigm for video and multimodal tasks, let alone the development of
procedural activity assistants.

</details>


### [3] [DRBench: A Realistic Benchmark for Enterprise Deep Research](https://arxiv.org/abs/2510.00172)
*Amirhossein Abaskohi,Tianyi Chen,Miguel Muñoz-Mármol,Curtis Fox,Amrutha Varshini Ramesh,Étienne Marcotte,Xing Han Lù,Nicolas Chapados,Spandana Gella,Christopher Pal,Alexandre Drouin,Issam H. Laradji*

Main category: cs.CL

TL;DR: DRBench是一个用于评估AI代理在企业环境中执行复杂、开放式深度研究任务的基准测试平台，涵盖多步骤查询和异构搜索空间。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注简单问题或仅限网络查询，缺乏对复杂企业深度研究任务的评估能力。

Method: 通过精心设计的合成管道生成任务，结合人工验证，评估代理在召回相关信息、保持事实准确性和生成结构化报告方面的能力。

Result: 发布了涵盖10个领域的15个深度研究任务，评估了多种开源和闭源模型及策略，揭示了它们的优势和弱点。

Conclusion: DRBench为推进企业深度研究提供了关键路径，展示了不同AI代理在复杂企业任务中的表现差异。

Abstract: We introduce DRBench, a benchmark for evaluating AI agents on complex,
open-ended deep research tasks in enterprise settings. Unlike prior benchmarks
that focus on simple questions or web-only queries, DRBench evaluates agents on
multi-step queries (for example, ``What changes should we make to our product
roadmap to ensure compliance with this standard?") that require identifying
supporting facts from both the public web and private company knowledge base.
Each task is grounded in realistic user personas and enterprise context,
spanning a heterogeneous search space that includes productivity software,
cloud file systems, emails, chat conversations, and the open web. Tasks are
generated through a carefully designed synthesis pipeline with
human-in-the-loop verification, and agents are evaluated on their ability to
recall relevant insights, maintain factual accuracy, and produce coherent,
well-structured reports. We release 15 deep research tasks across 10 domains,
such as Sales, Cybersecurity, and Compliance. We demonstrate the effectiveness
of DRBench by evaluating diverse DR agents across open- and closed-source
models (such as GPT, Llama, and Qwen) and DR strategies, highlighting their
strengths, weaknesses, and the critical path for advancing enterprise deep
research. Code is available at https://github.com/ServiceNow/drbench.

</details>


### [4] [PrimeX: A Dataset of Worldview, Opinion, and Explanation](https://arxiv.org/abs/2510.00174)
*Rik Koncel-Kedziorski,Brihi Joshi,Tim Paek*

Main category: cs.CL

TL;DR: PrimeX数据集包含858名美国居民的民意调查数据，包括意见解释和世界观评估，用于研究语言模型如何利用个人信念系统进行个性化改进。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型的广泛应用，需要更好地表示个体用户。研究探索语言模型是否能利用个人信念系统的某些方面来改进对齐效果。

Method: 开发PrimeX数据集，收集858名美国居民的民意调查数据，包括书面意见解释和Primal World Belief调查来评估受访者世界观。

Result: 初步分析显示信念解释和世界观信息对个性化语言模型具有价值。

Conclusion: PrimeX中的额外信念信息对NLP和心理学研究社区都有益处，为后续研究开辟了新途径。

Abstract: As the adoption of language models advances, so does the need to better
represent individual users to the model. Are there aspects of an individual's
belief system that a language model can utilize for improved alignment?
Following prior research, we investigate this question in the domain of opinion
prediction by developing PrimeX, a dataset of public opinion survey data from
858 US residents with two additional sources of belief information: written
explanations from the respondents for why they hold specific opinions, and the
Primal World Belief survey for assessing respondent worldview. We provide an
extensive initial analysis of our data and show the value of belief
explanations and worldview for personalizing language models. Our results
demonstrate how the additional belief information in PrimeX can benefit both
the NLP and psychological research communities, opening up avenues for further
study.

</details>


### [5] [Personalized Reasoning: Just-In-Time Personalization and Why LLMs Fail At It](https://arxiv.org/abs/2510.00177)
*Shuyue Stella Li,Avinandan Bose,Faeze Brahman,Simon Shaolei Du,Pang Wei Koh,Maryam Fazel,Yulia Tsvetkov*

Main category: cs.CL

TL;DR: PREFDISCO是一个评估方法，将静态基准转化为交互式个性化任务，使用基于心理学的角色和稀疏偏好来评估LLM的个性化推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM开发将任务解决和偏好对齐视为独立挑战，但在面向人类的应用中，正确解决问题不足够，还需要匹配用户需求，特别是在冷启动或隐私限制的即时场景中。

Method: 引入PREFDISCO评估框架，使用心理学基础的角色和稀疏偏好创建交互式个性化任务，其中相同问题需要根据不同用户上下文采用不同的推理链。

Result: 评估21个前沿模型在10个任务上的表现，发现29.0%的简单个性化尝试比通用响应更差，而通用响应也无法有效满足个体用户需求。

Conclusion: 个性化推理需要专门开发而非自然涌现，PREFDISCO将个性化推理确立为可衡量的研究前沿，揭示了当前LLM交互能力的基本限制。

Abstract: Current large language model (LLM) development treats task-solving and
preference alignment as separate challenges, optimizing first for objective
correctness, then for alignment to aggregated human preferences. This paradigm
fails in human-facing applications where solving a problem correctly is
insufficient if the response mismatches the user's needs. This challenge
intensifies in just-in-time scenarios where no prior user interaction history
exists due to cold-start conditions or privacy constraints. LLMs need to
identify what they don't know about user preferences, strategically elicit
preference values through questioning, then adapt their reasoning processes and
responses accordingly -- a complicated chain of cognitive processes which we
term personalized reasoning. We introduce PREFDISCO, an evaluation methodology
that transforms static benchmarks into interactive personalization tasks using
psychologically-grounded personas with sparse preferences. Our framework
creates scenarios where identical questions require different reasoning chains
depending on user context, as optimal explanation approaches vary by individual
expertise and preferences while maintaining factual accuracy. Evaluation of 21
frontier models across 10 tasks reveals 29.0% of naive personalization attempts
produce worse preference alignment than generic responses, yet generic
responses also fail to serve individual user needs effectively. These findings
suggest personalized reasoning requires dedicated development rather than
emerging naturally. PREFDISCO establishes personalized reasoning as a
measurable research frontier and reveals fundamental limitations in current
LLMs' interactive capabilities, providing a foundation for developing systems
that can adapt to individual users in education, healthcare, and technical
domains where personalization is critical.

</details>


### [6] [BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model Responses](https://arxiv.org/abs/2510.00232)
*Xin Xu,Xunzhi He,Churan Zhi,Ruizhe Chen,Julian McAuley,Zexue He*

Main category: cs.CL

TL;DR: 提出了BiasFreeBench基准，用于统一评估LLM去偏方法，通过重组现有数据集为统一查询-响应格式，并引入Bias-Free Score指标来衡量模型响应的公平性和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM去偏方法使用不同的基线和指标进行评估，导致比较不一致，且评估主要基于模型概率而非实际用户交互场景。

Method: 构建BiasFreeBench基准，比较8种主流去偏技术（4种提示方法和4种训练方法），在两种测试场景下（多选QA和开放式多轮QA）进行系统评估。

Result: 系统比较了不同去偏方法在提示vs训练范式、模型大小以及训练策略对未见偏见的泛化能力等关键维度上的表现。

Conclusion: BiasFreeBench为偏见缓解研究建立了统一的测试平台，有助于更一致地评估不同去偏方法的效果。

Abstract: Existing studies on bias mitigation methods for large language models (LLMs)
use diverse baselines and metrics to evaluate debiasing performance, leading to
inconsistent comparisons among them. Moreover, their evaluations are mostly
based on the comparison between LLMs' probabilities of biased and unbiased
contexts, which ignores the gap between such evaluations and real-world use
cases where users interact with LLMs by reading model responses and expect fair
and safe outputs rather than LLMs' probabilities. To enable consistent
evaluation across debiasing methods and bridge this gap, we introduce
BiasFreeBench, an empirical benchmark that comprehensively compares eight
mainstream bias mitigation techniques (covering four prompting-based and four
training-based methods) on two test scenarios (multi-choice QA and open-ended
multi-turn QA) by reorganizing existing datasets into a unified query-response
setting. We further introduce a response-level metric, Bias-Free Score, to
measure the extent to which LLM responses are fair, safe, and
anti-stereotypical. Debiasing performances are systematically compared and
analyzed across key dimensions: the prompting vs. training paradigm, model
size, and generalization of different training strategies to unseen bias types.
We will publicly release our benchmark, aiming to establish a unified testbed
for bias mitigation research.

</details>


### [7] [TASER: Translation Assessment via Systematic Evaluation and Reasoning](https://arxiv.org/abs/2510.00255)
*Monishwaran Maheswaran,Marco Carini,Christian Federmann,Tony Diaz*

Main category: cs.CL

TL;DR: TASER是一个基于大型推理模型的自动翻译质量评估指标，通过系统化逐步推理来评估翻译质量，在WMT24评测中表现出色，在系统级和片段级评估中都取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动翻译评估指标存在可解释性差的问题，需要开发结合准确性和透明度的新评估方法。

Method: 利用大型推理模型的显式推理能力，采用结构化提示模板进行系统化、逐步的翻译质量评估，测试了不同推理深度对评估质量的影响。

Result: 在WMT24评测中，TASER在系统级评估中获得了最高的软配对准确率（参考和无参考设置），在片段级评估中无参考版本在所有无参考方法中排名第一。

Conclusion: 大型推理模型在翻译质量评估方面显示出可衡量的进步，结合了改进的准确性和跨语言对的透明评估。

Abstract: We introduce TASER (Translation Assessment via Systematic Evaluation and
Reasoning), a metric that uses Large Reasoning Models (LRMs) for automated
translation quality assessment. TASER harnesses the explicit reasoning
capabilities of LRMs to conduct systematic, step-by-step evaluation of
translation quality. We evaluate TASER on the WMT24 Metrics Shared Task across
both reference-based and reference-free scenarios, demonstrating
state-of-the-art performance. In system-level evaluation, TASER achieves the
highest soft pairwise accuracy in both reference-based and reference-free
settings, outperforming all existing metrics. At the segment level, TASER
maintains competitive performance with our reference-free variant ranking as
the top-performing metric among all reference-free approaches. Our experiments
reveal that structured prompting templates yield superior results with LRMs
compared to the open-ended approaches that proved optimal for traditional LLMs.
We evaluate o3, a large reasoning model from OpenAI, with varying reasoning
efforts, providing insights into the relationship between reasoning depth and
evaluation quality. The explicit reasoning process in LRMs offers
interpretability and visibility, addressing a key limitation of existing
automated metrics. Our results demonstrate that Large Reasoning Models show a
measurable advancement in translation quality assessment, combining improved
accuracy with transparent evaluation across diverse language pairs.

</details>


### [8] [Retrieval-Augmented Generation for Electrocardiogram-Language Models](https://arxiv.org/abs/2510.00261)
*Xiaoyu Song,William Han,Tony Chen,Chaojing Duan,Michael A. Rosenberg,Emerson Liu,Ding Zhao*

Main category: cs.CL

TL;DR: 提出了首个用于心电图文语言模型的开源RAG管道，通过检索增强生成技术提升模型性能，在三个公开数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有心电图文语言模型缺乏检索增强生成技术的系统研究和开源实现，这限制了模型性能提升和实际应用。

Method: 开发了首个开源的心电图文语言模型RAG管道，包括检索模块和生成模块，并进行了基线测试和消融研究。

Result: 在三个公开数据集上的实验表明，采用RAG的心电图文语言模型性能持续优于非RAG基线模型。

Conclusion: RAG技术能有效提升心电图文语言模型的性能，该开源实现为相关研究提供了重要基础。

Abstract: Interest in generative Electrocardiogram-Language Models (ELMs) is growing,
as they can produce textual responses conditioned on ECG signals and textual
queries. Unlike traditional classifiers that output label probabilities, ELMs
are more versatile, supporting domain-specific tasks (e.g., waveform analysis,
diagnosis, prognosis) as well as general tasks (e.g., open-ended questions,
dialogue). Retrieval-Augmented Generation (RAG), widely used in Large Language
Models (LLMs) to ground LLM outputs in retrieved knowledge, helps reduce
hallucinations and improve natural language generation (NLG). However, despite
its promise, no open-source implementation or systematic study of RAG pipeline
design for ELMs currently exists. To address this gap, we present the first
open-source RAG pipeline for ELMs, along with baselines and ablation studies
for NLG. Experiments on three public datasets show that ELMs with RAG
consistently improves performance over non-RAG baselines and highlights key ELM
design considerations. Our code is available at:
https://github.com/willxxy/ECG-Bench.

</details>


### [9] [Judging with Confidence: Calibrating Autoraters to Preference Distributions](https://arxiv.org/abs/2510.00263)
*Zhuohang Li,Xiaowei Li,Chengyu Huang,Guowang Li,Katayoon Goshvadi,Bo Dai,Dale Schuurmans,Paul Zhou,Hamid Palangi,Yiwen Song,Palash Goyal,Murat Kantarcioglu,Bradley A. Malin,Yuan Xue*

Main category: cs.CL

TL;DR: 提出了一种校准概率自动评分器的方法，使其能够建模目标人群的完整偏好分布，而不是强制使用单一标准答案。


<details>
  <summary>Details</summary>
Motivation: 当前LLM自动评分器存在根本性问题：它们基于离散偏好标签训练，将主观、模糊或细微的任务强制简化为单一标准答案，限制了可靠性。

Method: 提出了一个通用框架，包含两种学习方法：1）针对密集概率标签的直接监督微调；2）针对稀疏二元标签的强化学习方法。

Result: 实验结果显示，使用分布匹配目标微调的自动评分器能够产生与目标偏好分布更好对齐的言语化概率预测，具有改进的校准度和显著降低的位置偏差，同时保持客观任务的性能。

Conclusion: 通过建模完整偏好分布来校准概率自动评分器，能够提高其在主观任务上的可靠性，同时保持客观任务的性能。

Abstract: The alignment of large language models (LLMs) with human values increasingly
relies on using other LLMs as automated judges, or ``autoraters''. However,
their reliability is limited by a foundational issue: they are trained on
discrete preference labels, forcing a single ground truth onto tasks that are
often subjective, ambiguous, or nuanced. We argue that a reliable autorater
must learn to model the full distribution of preferences defined by a target
population. In this paper, we propose a general framework for calibrating
probabilistic autoraters to any given preference distribution. We formalize the
problem and present two learning methods tailored to different data conditions:
1) a direct supervised fine-tuning for dense, probabilistic labels, and 2) a
reinforcement learning approach for sparse, binary labels. Our empirical
results show that finetuning autoraters with a distribution-matching objective
leads to verbalized probability predictions that are better aligned with the
target preference distribution, with improved calibration and significantly
lower positional bias, all while preserving performance on objective tasks.

</details>


### [10] [Efficient Layer-wise LLM Fine-tuning for Revision Intention Prediction](https://arxiv.org/abs/2510.00268)
*Zhexiong Liu,Diane Litman*

Main category: cs.CL

TL;DR: IR-Tuning：一种基于梯度范数分布动态选择重要层的参数高效微调框架，用于提升LLM在文本修订分类任务中的性能，同时实现快速收敛和低内存消耗。


<details>
  <summary>Details</summary>
Motivation: LLM在文本生成任务中表现出色，但在文本分类特别是细微文本修订分类方面表现不足。直接微调需要大量昂贵的修订标注数据，这在社区中稀缺且成本高昂。

Method: 提出IR-Tuning框架，基于梯度范数分布动态选择LLM中的重要层进行参数高效微调，同时冻结冗余层，实现plug-and-play的层级PEFT。

Result: 在多种文本修订任务上，IR-Tuning超越了多个层级PEFT基线方法，实现了快速收敛、低GPU内存消耗，并在小规模修订语料上表现有效。

Conclusion: IR-Tuning为解决LLM在细微文本分类任务中的局限性提供了有效的参数高效微调方案，特别适用于标注数据稀缺的场景。

Abstract: Large Language Models (LLMs) have shown extraordinary success across various
text generation tasks; however, their potential for simple yet essential text
classification remains underexplored, as LLM pre-training tends to emphasize
generation over classification. While LLMs with instruction tuning can
transform classification into a generation task, they often struggle to
categorize nuanced texts. One such example is text revision, which involves
nuanced edits between pairs of texts. Although simply fine-tuning LLMs for
revision classification seems plausible, it requires a large amount of revision
annotations, which are exceptionally expensive and scarce in the community. To
address this issue, we introduce a plug-and-play layer-wise parameter-efficient
fine-tuning (PEFT) framework, i.e., IR-Tuning, which fine-tunes a subset of
important LLM layers that are dynamically selected based on their gradient norm
distribution, while freezing those of redundant layers. Extensive experiments
suggest that IR-Tuning surpasses several layer-wise PEFT baselines over diverse
text revisions, while achieving fast convergence, low GPU memory consumption,
and effectiveness on small revision corpora.

</details>


### [11] [SafePassage: High-Fidelity Information Extraction with Black Box LLMs](https://arxiv.org/abs/2510.00276)
*Joe Barrow,Raj Patel,Misha Kharkovski,Ben Davies,Ryan Schmitt*

Main category: cs.CL

TL;DR: SafePassage是一个三阶段管道，通过生成安全段落来减少LLM在信息抽取中的幻觉问题，将幻觉率降低85%，同时能评估LLM抽取质量。


<details>
  <summary>Details</summary>
Motivation: 黑盒LLM使信息抽取易于配置但难以信任，抽取的信息可能不基于文档内容。需要确保抽取信息在文档中有依据。

Method: 三阶段管道：(1) LLM抽取器生成结构化实体及其上下文；(2) 基于字符串的全局对齐器；(3) 评分模型。使用微调的transformer编码器优于LLM评分模型。

Result: 将信息抽取任务中的幻觉减少高达85%，误报非幻觉的风险最小。SafePassage与人类对抽取质量的判断高度一致。

Conclusion: SafePassage能有效减少LLM在信息抽取中的幻觉，同时可作为评估LLM的工具。小样本微调的transformer编码器在识别不安全段落方面优于LLM评分模型。

Abstract: Black box large language models (LLMs) make information extraction (IE) easy
to configure, but hard to trust. Unlike traditional information extraction
pipelines, the information "extracted" is not guaranteed to be grounded in the
document. To prevent this, this paper introduces the notion of a "safe
passage": context generated by the LLM that is both grounded in the document
and consistent with the extracted information. This is operationalized via a
three-step pipeline, SafePassage, which consists of: (1) an LLM extractor that
generates structured entities and their contexts from a document, (2) a
string-based global aligner, and (3) a scoring model. Results show that using
these three parts in conjunction reduces hallucinations by up to 85% on
information extraction tasks with minimal risk of flagging non-hallucinations.
High agreement between the SafePassage pipeline and human judgments of
extraction quality mean that the pipeline can be dually used to evaluate LLMs.
Surprisingly, results also show that using a transformer encoder fine-tuned on
a small number of task-specific examples can outperform an LLM scoring model at
flagging unsafe passages. These annotations can be collected in as little as
1-2 hours.

</details>


### [12] [ReEvalMed: Rethinking Medical Report Evaluation by Aligning Metrics with Real-World Clinical Judgment](https://arxiv.org/abs/2510.00280)
*Ruochen Li,Jun Li,Bailiang Jian,Kun Yuan,Youxiang Zhu*

Main category: cs.CL

TL;DR: 提出一个临床基础的元评估框架，重新思考放射学报告生成评估指标的设计，发现现有指标在临床语义解释上的局限性。


<details>
  <summary>Details</summary>
Motivation: 自动生成的放射学报告在现有评估指标中得分很高，但未能获得临床医生的信任，这揭示了当前评估指标在评估生成报告质量方面的根本缺陷。

Method: 定义了涵盖临床对齐和关键指标能力的临床基础标准，包括区分度、鲁棒性和单调性。使用细粒度数据集系统评估现有指标。

Result: 揭示了现有指标在解释临床语义方面的局限性，如无法区分临床显著错误、过度惩罚无害变异以及在不同错误严重程度级别上缺乏一致性。

Conclusion: 该框架为构建更临床可靠的评估方法提供了指导。

Abstract: Automatically generated radiology reports often receive high scores from
existing evaluation metrics but fail to earn clinicians' trust. This gap
reveals fundamental flaws in how current metrics assess the quality of
generated reports. We rethink the design and evaluation of these metrics and
propose a clinically grounded Meta-Evaluation framework. We define clinically
grounded criteria spanning clinical alignment and key metric capabilities,
including discrimination, robustness, and monotonicity. Using a fine-grained
dataset of ground truth and rewritten report pairs annotated with error types,
clinical significance labels, and explanations, we systematically evaluate
existing metrics and reveal their limitations in interpreting clinical
semantics, such as failing to distinguish clinically significant errors,
over-penalizing harmless variations, and lacking consistency across error
severity levels. Our framework offers guidance for building more clinically
reliable evaluation methods.

</details>


### [13] [o-MEGA: Optimized Methods for Explanation Generation and Analysis](https://arxiv.org/abs/2510.00288)
*Ľuboš Kriš,Jaroslav Kopčan,Qiwei Peng,Andrej Ridzik,Marcel Veselý,Martin Tamajka*

Main category: cs.CL

TL;DR: o-mega是一个超参数优化工具，用于自动识别语义匹配领域中最有效的可解释AI方法及其配置，以提升自动化事实核查系统的透明度。


<details>
  <summary>Details</summary>
Motivation: 基于transformer的语言模型在NLP领域取得了革命性进展，但同时也带来了模型透明度和可信度的挑战。由于存在大量解释方法和评估指标，选择最优的可解释性方法变得复杂。

Method: 开发了o-mega超参数优化工具，在社交媒体帖子与反驳声明配对的精选数据集上，系统探索不同的可解释方法及其超参数配置。

Result: o-mega工具在事后声明匹配流程中进行了评估，显示出在自动化事实核查系统中提高了透明度。

Conclusion: 这种解释方法的自动优化可以显著增强关键应用（如错误信息检测）中声明匹配模型的可解释性，有助于构建更可信和透明的AI系统。

Abstract: The proliferation of transformer-based language models has revolutionized NLP
domain while simultaneously introduced significant challenges regarding model
transparency and trustworthiness. The complexity of achieving explainable
systems in this domain is evidenced by the extensive array of explanation
methods and evaluation metrics developed by researchers. To address the
challenge of selecting optimal explainability approaches, we present
\textbf{\texttt{o-mega}}, a hyperparameter optimization tool designed to
automatically identify the most effective explainable AI methods and their
configurations within the semantic matching domain. We evaluate o-mega on a
post-claim matching pipeline using a curated dataset of social media posts
paired with refuting claims. Our tool systematically explores different
explainable methods and their hyperparameters, demonstrating improved
transparency in automated fact-checking systems. As a result, such automated
optimization of explanation methods can significantly enhance the
interpretability of claim-matching models in critical applications such as
misinformation detection, contributing to more trustworthy and transparent AI
systems.

</details>


### [14] [CORTEX: Collaborative LLM Agents for High-Stakes Alert Triage](https://arxiv.org/abs/2510.00311)
*Bowen Wei,Yuan Shen Tay,Howard Liu,Jinhao Pan,Kun Luo,Ziwei Zhu,Chris Jordan*

Main category: cs.CL

TL;DR: CORTEX是一个多智能体LLM架构，用于安全运营中心的高风险警报分类，通过专业化智能体协作减少误报并提高调查质量。


<details>
  <summary>Details</summary>
Motivation: 安全运营中心面临每日数万警报的过载问题，传统检测管道脆弱且缺乏上下文，而单一LLM方法在处理嘈杂企业数据和透明度方面存在局限。

Method: 采用多智能体LLM架构，包括行为分析智能体检查活动序列、证据收集智能体查询外部系统、推理智能体综合发现形成可审计决策。

Result: 在多样化企业场景中，CORTEX显著减少误报并优于最先进的单智能体LLM的调查质量。

Conclusion: 多智能体协作方法在警报分类中比单一模型更有效，提供了更好的透明度和处理企业数据的能力。

Abstract: Security Operations Centers (SOCs) are overwhelmed by tens of thousands of
daily alerts, with only a small fraction corresponding to genuine attacks. This
overload creates alert fatigue, leading to overlooked threats and analyst
burnout. Classical detection pipelines are brittle and context-poor, while
recent LLM-based approaches typically rely on a single model to interpret logs,
retrieve context, and adjudicate alerts end-to-end -- an approach that
struggles with noisy enterprise data and offers limited transparency. We
propose CORTEX, a multi-agent LLM architecture for high-stakes alert triage in
which specialized agents collaborate over real evidence: a behavior-analysis
agent inspects activity sequences, evidence-gathering agents query external
systems, and a reasoning agent synthesizes findings into an auditable decision.
To support training and evaluation, we release a dataset of fine-grained SOC
investigations from production environments, capturing step-by-step analyst
actions and linked tool outputs. Across diverse enterprise scenarios, CORTEX
substantially reduces false positives and improves investigation quality over
state-of-the-art single-agent LLMs.

</details>


### [15] [TokMem: Tokenized Procedural Memory for Large Language Models](https://arxiv.org/abs/2510.00444)
*Zijun Wu,Yongchang Hao,Lili Mou*

Main category: cs.CL

TL;DR: TokMem是一种令牌化程序记忆系统，将重复程序存储为紧凑可训练的嵌入，通过内存令牌编码程序地址和控制信号，在保持骨干模型冻结的同时实现持续适应。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型过度依赖提示存在效率低下、跨任务扩展性差、缺乏模块化重用机制等问题，需要更高效的替代方案。

Method: 引入TokMem系统，使用令牌化程序记忆存储程序作为紧凑嵌入，每个内存令牌包含程序地址和控制信号，保持骨干模型冻结以支持持续适应。

Result: 在1000个原子回忆任务和函数调用组合回忆任务中，TokMem持续优于检索增强生成方法，避免了重复上下文开销，且参数远少于微调方法。

Conclusion: TokMem为LLMs提供了一个可扩展、模块化的显式程序记忆，是提示工程和微调的有效替代方案。

Abstract: Large language models rely heavily on prompts to specify tasks, recall
knowledge and guide reasoning. However, this reliance is inefficient as prompts
must be re-read at each step, scale poorly across tasks, and lack mechanisms
for modular reuse. We introduce TokMem, a tokenized procedural memory that
stores recurring procedures as compact, trainable embeddings. Each memory token
encodes both an address to a procedure and a control signal that steers
generation, enabling targeted behavior with constant-size overhead. To support
continual adaptation, TokMem keeps the backbone model frozen, allowing new
procedures to be added without interfering with existing ones. We evaluate
TokMem on 1,000 tasks for atomic recall, and on function-calling tasks for
compositional recall, where it consistently outperforms retrieval-augmented
generation while avoiding repeated context overhead, and fine-tuning with far
fewer parameters. These results establish TokMem as a scalable and modular
alternative to prompt engineering and fine-tuning, offering an explicit
procedural memory for LLMs.

</details>


### [16] [LongCodeZip: Compress Long Context for Code Language Models](https://arxiv.org/abs/2510.00446)
*Yuling Shi,Yichun Qian,Hongyu Zhang,Beijun Shen,Xiaodong Gu*

Main category: cs.CL

TL;DR: LongCodeZip是一个专为代码LLM设计的即插即用压缩框架，通过双阶段压缩策略显著减少上下文长度，在保持任务性能的同时实现高达5.6倍压缩比。


<details>
  <summary>Details</summary>
Motivation: 现有上下文剪枝技术如LLMLingua在通用文本上表现良好，但忽略了代码特有的结构和依赖关系，导致在编程任务中性能不佳。高API成本和生成延迟是代码LLM处理长上下文的主要瓶颈。

Method: 采用双阶段压缩策略：1）粗粒度压缩，基于条件困惑度识别和排序函数级块，仅保留最相关函数；2）细粒度压缩，将保留函数按困惑度分段，在自适应token预算下选择最优子集以最大化相关性。

Result: 在代码补全、摘要和问答等多个任务上的评估表明，LongCodeZip始终优于基线方法，在保持任务性能的同时实现高达5.6倍压缩比。

Conclusion: 通过有效减少上下文大小同时保留关键信息，LongCodeZip使LLM能够更好地扩展到现实世界的大规模代码场景，提升了代码智能应用的效率和能力。

Abstract: Code generation under long contexts is becoming increasingly critical as
Large Language Models (LLMs) are required to reason over extensive information
in the codebase. While recent advances enable code LLMs to process long inputs,
high API costs and generation latency remain substantial bottlenecks. Existing
context pruning techniques, such as LLMLingua, achieve promising results for
general text but overlook code-specific structures and dependencies, leading to
suboptimal performance in programming tasks. In this paper, we propose
LongCodeZip, a novel plug-and-play code compression framework designed
specifically for code LLMs. LongCodeZip employs a dual-stage strategy: (1)
coarse-grained compression, which identifies and ranks function-level chunks
using conditional perplexity with respect to the instruction, retaining only
the most relevant functions; and (2) fine-grained compression, which segments
retained functions into blocks based on perplexity and selects an optimal
subset under an adaptive token budget to maximize relevance. Evaluations across
multiple tasks, including code completion, summarization, and question
answering, show that LongCodeZip consistently outperforms baseline methods,
achieving up to a 5.6x compression ratio without degrading task performance. By
effectively reducing context size while preserving essential information,
LongCodeZip enables LLMs to better scale to real-world, large-scale code
scenarios, advancing the efficiency and capability of code intelligence
applications.

</details>


### [17] [Enhancing Rating Prediction with Off-the-Shelf LLMs Using In-Context User Reviews](https://arxiv.org/abs/2510.00449)
*Koki Ryu,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 该研究探索了现成大语言模型在评分预测任务上的表现，发现用户评论能显著提升预测性能，性能可与传统矩阵分解方法媲美，为解决冷启动问题提供了有前景的方案。


<details>
  <summary>Details</summary>
Motivation: 个性化大语言模型输出以适应用户偏好是活跃研究领域，但先前研究主要关注分类或排序任务，未考虑需要语言和数学推理的Likert评分预测任务，该任务具有重要工业应用价值。

Method: 通过三个数据集上八个模型的综合实验，研究不同上下文信息对评分预测的影响，包括用户评论和一般偏好描述，并探索了让LLM先生成假设评论的提示策略。

Result: 用户撰写的评论显著提高了LLMs的评分预测性能，性能与传统矩阵分解方法相当；具体物品的评论比一般偏好描述更有效；让LLM先生成假设评论能进一步提升预测性能。

Conclusion: 现成LLMs在评分预测任务上表现出色，用户评论是关键影响因素，该方法为解决推荐系统中的冷启动问题提供了有前景的解决方案。

Abstract: Personalizing the outputs of large language models (LLMs) to align with
individual user preferences is an active research area. However, previous
studies have mainly focused on classification or ranking tasks and have not
considered Likert-scale rating prediction, a regression task that requires both
language and mathematical reasoning to be solved effectively. This task has
significant industrial applications, but the utilization of LLMs remains
underexplored, particularly regarding the capabilities of off-the-shelf LLMs.
This study investigates the performance of off-the-shelf LLMs on rating
prediction, providing different in-context information. Through comprehensive
experiments with eight models across three datasets, we demonstrate that
user-written reviews significantly improve the rating prediction performance of
LLMs. This result is comparable to traditional methods like matrix
factorization, highlighting the potential of LLMs as a promising solution for
the cold-start problem. We also find that the reviews for concrete items are
more effective than general preference descriptions that are not based on any
specific item. Furthermore, we discover that prompting LLMs to first generate a
hypothetical review enhances the rating prediction performance. Our code is
available at https://github.com/ynklab/rating-prediction-with-reviews.

</details>


### [18] [Agent Fine-tuning through Distillation for Domain-specific LLMs in Microdomains](https://arxiv.org/abs/2510.00482)
*Yawen Xue,Masaya Tsunokake,Yuta Koreeda,Ekant Muljibhai Amin,Takashi Sumiyoshi,Yasuhiro Sogawa*

Main category: cs.CL

TL;DR: 该论文研究了在日立JP1中间件这一专业IT运维微领域中，通过代理微调方法提升大语言模型的领域适应能力，在认证考试问题上实现了14%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有代理大语言模型主要通过上下文学习实现多步推理，但存在输入冗长和计算成本高的问题。代理微调能够通过训练让模型内化领域知识和推理过程，但在专业微领域中的有效性尚不明确。

Method: 使用JP1特定数据集进行代理微调，数据集来源于领域手册和LLM生成的推理轨迹。在推理阶段采用检索增强生成和上下文-答案提取器来提高信息相关性。

Result: 在JP1认证考试问题上，该方法相比基础模型实现了14%的性能提升，证明了代理微调在复杂微领域中领域特定推理的潜力。

Conclusion: 代理微调是提升大语言模型在专业微领域中推理能力的有效方法，特别适用于需要领域特定知识的复杂任务。

Abstract: Agentic large language models (LLMs) have become prominent for autonomously
interacting with external environments and performing multi-step reasoning
tasks. Most approaches leverage these capabilities via in-context learning with
few-shot prompts, but this often results in lengthy inputs and higher
computational costs. Agent fine-tuning offers an alternative by enabling LLMs
to internalize procedural reasoning and domain-specific knowledge through
training on relevant data and demonstration trajectories. While prior studies
have focused on general domains, their effectiveness in specialized technical
microdomains remains unclear. This paper explores agent fine-tuning for domain
adaptation within Hitachi's JP1 middleware, a microdomain for specialized IT
operations. We fine-tuned LLMs using JP1-specific datasets derived from domain
manuals and distilled reasoning trajectories generated by LLMs themselves,
enhancing decision making accuracy and search efficiency. During inference, we
used an agentic prompt with retrieval-augmented generation and introduced a
context-answer extractor to improve information relevance. On JP1 certification
exam questions, our method achieved a 14% performance improvement over the base
model, demonstrating the potential of agent fine-tuning for domain-specific
reasoning in complex microdomains.

</details>


### [19] [Agent-ScanKit: Unraveling Memory and Reasoning of Multimodal Agents via Sensitivity Perturbations](https://arxiv.org/abs/2510.00496)
*Pengzhou Cheng,Lingzhong Dong,Zeng Wu,Zongru Wu,Xiangru Tang,Chengwei Qin,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.CL

TL;DR: 提出了Agent-ScanKit框架，通过三种正交探测范式来揭示多模态代理在受控扰动下的记忆和推理能力，发现在现有GUI基准测试中，机械记忆往往胜过系统推理。


<details>
  <summary>Details</summary>
Motivation: 现有多模态代理在复杂或领域外任务中的可靠性有限，需要探究其是否存在伪推理问题。

Method: 开发了Agent-ScanKit系统探测框架，包含视觉引导、文本引导和结构引导三种正交探测范式，无需访问模型内部即可量化记忆和推理的贡献。

Result: 在5个公开GUI基准测试和18个多模态代理上的结果表明，机械记忆通常优于系统推理，大多数模型主要作为训练对齐知识的检索器，泛化能力有限。

Conclusion: 研究强调了在现实场景中为多模态代理开发稳健推理模型的必要性，为可靠多模态代理的发展提供了宝贵见解。

Abstract: Although numerous strategies have recently been proposed to enhance the
autonomous interaction capabilities of multimodal agents in graphical user
interface (GUI), their reliability remains limited when faced with complex or
out-of-domain tasks. This raises a fundamental question: Are existing
multimodal agents reasoning spuriously? In this paper, we propose
\textbf{Agent-ScanKit}, a systematic probing framework to unravel the memory
and reasoning capabilities of multimodal agents under controlled perturbations.
Specifically, we introduce three orthogonal probing paradigms: visual-guided,
text-guided, and structure-guided, each designed to quantify the contributions
of memorization and reasoning without requiring access to model internals. In
five publicly available GUI benchmarks involving 18 multimodal agents, the
results demonstrate that mechanical memorization often outweighs systematic
reasoning. Most of the models function predominantly as retrievers of
training-aligned knowledge, exhibiting limited generalization. Our findings
underscore the necessity of robust reasoning modeling for multimodal agents in
real-world scenarios, offering valuable insights toward the development of
reliable multimodal agents.

</details>


### [20] [MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance](https://arxiv.org/abs/2510.00499)
*Xingjian Zhao,Zhe Xu,Luozhijie Jin,Yang Wang,Hanfu Chen,Yaozhou Jiang,Ke Chen,Ruixiao Li,Mingshu Chen,Ruiming Wang,Wenbo Zhang,Yiyang Zhang,Donghua Yu,Yang Gao,Xiaogui Yang,Yitian Gong,Yuanfan Xu,Qinyuan Cheng,Zhaoye Fei,Shimin Li,Yaqian Zhou,Xuanjing Huang,Xipeng Qiu*

Main category: cs.CL

TL;DR: MOSS-Speech是一个真正的语音到语音大语言模型，无需依赖文本中间体就能直接理解和生成语音，解决了传统语音系统中文本瓶颈的问题。


<details>
  <summary>Details</summary>
Motivation: 传统语音对话系统使用级联流水线设计，会丢弃副语言线索并限制表达性。现有的端到端方法虽然减少了延迟并更好地保留了这些线索，但仍然依赖文本中间体，形成了根本性瓶颈。

Method: 结合基于模态的层分割架构和冻结预训练策略，在保留预训练文本LLM的推理和知识的同时，添加原生语音能力。

Result: 模型在口语问答任务中达到最先进结果，在语音到语音性能上与现有文本引导系统相当，同时保持有竞争力的文本性能。

Conclusion: 通过缩小文本引导和直接语音生成之间的差距，这项工作为表达性和高效的端到端语音交互建立了新范式。

Abstract: Spoken dialogue systems often rely on cascaded pipelines that transcribe,
process, and resynthesize speech. While effective, this design discards
paralinguistic cues and limits expressivity. Recent end-to-end methods reduce
latency and better preserve these cues, yet still rely on text intermediates,
creating a fundamental bottleneck. We present MOSS-Speech, a true
speech-to-speech large language model that directly understands and generates
speech without relying on text guidance. Our approach combines a modality-based
layer-splitting architecture with a frozen pre-training strategy, preserving
the reasoning and knowledge of pretrained text LLMs while adding native speech
capabilities. Experiments show that our model achieves state-of-the-art results
in spoken question answering and delivers comparable speech-to-speech
performance relative to existing text-guided systems, while still maintaining
competitive text performance. By narrowing the gap between text-guided and
direct speech generation, our work establishes a new paradigm for expressive
and efficient end-to-end speech interaction.

</details>


### [21] [Graph2Eval: Automatic Multimodal Task Generation for Agents via Knowledge Graphs](https://arxiv.org/abs/2510.00507)
*Yurun Chen,Xavier Hu,Yuhan Liu,Ziqi Wang,Zeyi Liao,Lin Chen,Feng Wei,Yuxi Qian,Bo Zheng,Keting Yin,Shengyu Zhang*

Main category: cs.CL

TL;DR: Graph2Eval是一个基于知识图谱的框架，用于自动生成多模态文档理解和网页交互任务，以全面评估智能代理的推理、协作和交互能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基于LLM的合成数据方法主要针对LLM训练和评估设计，无法直接应用于需要工具使用和交互能力的代理任务。现有代理任务生成方法大多局限于文本或图像分析，缺乏对网页环境中多步交互的系统建模。

Method: 基于多源外部数据构建知识图谱作为任务空间，通过子图采样、任务模板和元路径将语义关系转化为结构化多模态任务。采用基于节点可达性、LLM评分和相似性分析的多阶段过滤管道来保证生成任务的质量和可执行性。

Result: 构建了Graph2Eval-Bench数据集，包含1,319个涵盖文档理解和网页交互场景的任务。实验表明Graph2Eval能有效生成区分代理和模型性能的任务，揭示不同设置下推理、协作和网页交互方面的差距。

Conclusion: Graph2Eval为代理评估提供了新视角，能够全面评估多类型代理（单代理、多代理、网页代理）的推理、协作和交互能力。

Abstract: As multimodal LLM-driven agents continue to advance in autonomy and
generalization, evaluation based on static datasets can no longer adequately
assess their true capabilities in dynamic environments and diverse tasks.
Existing LLM-based synthetic data methods are largely designed for LLM training
and evaluation, and thus cannot be directly applied to agent tasks that require
tool use and interactive capabilities. While recent studies have explored
automatic agent task generation with LLMs, most efforts remain limited to text
or image analysis, without systematically modeling multi-step interactions in
web environments. To address these challenges, we propose Graph2Eval, a
knowledge graph-based framework that automatically generates both multimodal
document comprehension tasks and web interaction tasks, enabling comprehensive
evaluation of agents' reasoning, collaboration, and interactive capabilities.
In our approach, knowledge graphs constructed from multi-source external data
serve as the task space, where we translate semantic relations into structured
multimodal tasks using subgraph sampling, task templates, and meta-paths. A
multi-stage filtering pipeline based on node reachability, LLM scoring, and
similarity analysis is applied to guarantee the quality and executability of
the generated tasks. Furthermore, Graph2Eval supports end-to-end evaluation of
multiple agent types (Single-Agent, Multi-Agent, Web Agent) and measures
reasoning, collaboration, and interaction capabilities. We instantiate the
framework with Graph2Eval-Bench, a curated dataset of 1,319 tasks spanning
document comprehension and web interaction scenarios. Experiments show that
Graph2Eval efficiently generates tasks that differentiate agent and model
performance, revealing gaps in reasoning, collaboration, and web interaction
across different settings and offering a new perspective for agent evaluation.

</details>


### [22] [Copy-Paste to Mitigate Large Language Model Hallucinations](https://arxiv.org/abs/2510.00508)
*Yongchao Long,Xian Wu,Yingying Zhang,Xianbin Wen,Yuxi Zhou,Shenda Hong*

Main category: cs.CL

TL;DR: CopyPasteLLM通过两阶段高复制响应偏好训练，提高RAG系统中LLM的上下文忠实度，减少幻觉，仅需少量训练样本即可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决RAG系统中LLM不信任提供上下文导致的上下文不忠实幻觉问题，观察到响应复制程度与幻觉存在负相关关系。

Method: 提出CopyPasteLLM，通过两阶段高复制响应偏好训练，设计三种提示方法增强复制程度，构建全自动管道将生成响应转换为高复制偏好数据进行训练。

Result: 在FaithEval、ConFiQA和PubMedQA上表现最佳，FaithEval准确率比最佳基线提升12.2%至24.5%，仅需365个训练样本（基线的1/50）。

Conclusion: CopyPasteLLM通过重新校准对内部参数知识的依赖而非外部知识来减少幻觉，提供了一种高效提升RAG系统可靠性的方法。

Abstract: While Retrieval-Augmented Generation (RAG) enables large language models
(LLMs) to generate contextually grounded responses, contextual faithfulness
remains challenging as LLMs may not consistently trust provided context,
leading to hallucinations that undermine reliability. We observe an inverse
correlation between response copying degree and context-unfaithful
hallucinations on RAGTruth, suggesting that higher copying degrees reduce
hallucinations by fostering genuine contextual belief. We propose CopyPasteLLM,
obtained through two-stage high-copying response preference training. We design
three prompting methods to enhance copying degree, demonstrating that
high-copying responses achieve superior contextual faithfulness and
hallucination control. These approaches enable a fully automated pipeline that
transforms generated responses into high-copying preference data for training
CopyPasteLLM. On FaithEval, ConFiQA and PubMedQA, CopyPasteLLM achieves best
performance in both counterfactual and original contexts, remarkably with 12.2%
to 24.5% accuracy improvements on FaithEval over the best baseline, while
requiring only 365 training samples -- 1/50th of baseline data. To elucidate
CopyPasteLLM's effectiveness, we propose the Context-Parameter Copying
Capturing algorithm. Interestingly, this reveals that CopyPasteLLM recalibrates
reliance on internal parametric knowledge rather than external knowledge during
generation. All codes are available at
https://github.com/longyongchao/CopyPasteLLM

</details>


### [23] [JoyAgent-JDGenie: Technical Report on the GAIA](https://arxiv.org/abs/2510.00510)
*Jiarun Liu,Shiyue Xu,Shangkun Liu,Yang Li,Wen Liu,Min Liu,Xiaoqing Zhou,Hanmin Wang,Shilin Jia,zhen Wang,Shaohua Tian,Hanhao Li,Junbo Zhang,Yongli Yu,Peng Cao,Haofen Wang*

Main category: cs.CL

TL;DR: 提出了一种通用智能体架构，整合多智能体框架、分层记忆系统和工具套件，在综合基准测试中优于开源基线并接近专有系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有系统通常专注于孤立改进，缺乏统一设计来确保鲁棒性和适应性，而大语言模型越来越多地被部署为复杂现实任务的自主智能体。

Method: 集成三个核心组件：结合规划和执行智能体与批评模型投票的集体多智能体框架、跨越工作记忆、语义记忆和程序记忆的分层记忆系统、用于搜索、代码执行和多模态解析的精炼工具套件。

Result: 在综合基准测试中，该框架始终优于开源基线，并接近专有系统的性能。

Conclusion: 系统级集成的重要性得到验证，为构建可扩展、有弹性和适应性的AI助手指明了方向，使其能够在不同领域和任务中运行。

Abstract: Large Language Models are increasingly deployed as autonomous agents for
complex real-world tasks, yet existing systems often focus on isolated
improvements without a unifying design for robustness and adaptability. We
propose a generalist agent architecture that integrates three core components:
a collective multi-agent framework combining planning and execution agents with
critic model voting, a hierarchical memory system spanning working, semantic,
and procedural layers, and a refined tool suite for search, code execution, and
multimodal parsing. Evaluated on a comprehensive benchmark, our framework
consistently outperforms open-source baselines and approaches the performance
of proprietary systems. These results demonstrate the importance of
system-level integration and highlight a path toward scalable, resilient, and
adaptive AI assistants capable of operating across diverse domains and tasks.

</details>


### [24] [EuroSpeech: A Multilingual Speech Corpus](https://arxiv.org/abs/2510.00514)
*Samuel Pfisterer,Florian Grötschla,Luca A. Lanzendörfer,Florian Yan,Roger Wattenhofer*

Main category: cs.CL

TL;DR: 提出了一种从议会录音构建语音数据集的可扩展流程，为22种欧洲语言提取了超过61k小时的高质量语音数据，显著提升了多语言ASR性能。


<details>
  <summary>Details</summary>
Motivation: 现有多语言数据集对大多数语言包含数据不足，导致训练模型在多数支持语言上表现不佳，需要为各个语言提供充足的训练数据。

Method: 开发了包含媒体检索和两阶段对齐算法的可扩展流程，专门处理非逐字转录和长音频，应用于22个欧洲议会的录音。

Result: 提取了61k+小时的语音片段，19种语言超过1k小时，22种语言超过500小时，微调现有ASR模型后词错误率平均降低41.8%。

Conclusion: 该流程成功构建了大规模多语言语音数据集，显著改善了多语言ASR性能，证明了方法的有效性。

Abstract: Recent progress in speech processing has highlighted that high-quality
performance across languages requires substantial training data for each
individual language. While existing multilingual datasets cover many languages,
they often contain insufficient data for most languages. Thus, trained models
perform poorly on the majority of the supported languages. Our work addresses
this challenge by introducing a scalable pipeline for constructing speech
datasets from parliamentary recordings. The proposed pipeline includes robust
components for media retrieval and a two-stage alignment algorithm designed to
handle non-verbatim transcripts and long-form audio. Applying this pipeline to
recordings from 22 European parliaments, we extract over 61k hours of aligned
speech segments, achieving substantial per-language coverage with 19 languages
exceeding 1k hours and 22 languages exceeding 500 hours of high-quality speech
data. We obtain an average 41.8\% reduction in word error rates over baselines
when finetuning an existing ASR model on our dataset, demonstrating the
usefulness of our approach.

</details>


### [25] [Beyond Log Likelihood: Probability-Based Objectives for Supervised Fine-Tuning across the Model Capability Continuum](https://arxiv.org/abs/2510.00526)
*Gaotang Li,Ruizhong Qiu,Xiusi Chen,Heng Ji,Hanghang Tong*

Main category: cs.CL

TL;DR: 本文研究了监督微调(SFT)中负对数似然(NLL)目标的局限性，提出了基于概率的目标家族，发现在模型能力连续体上不同目标表现各异：模型能力强时，偏向先验的目标优于NLL；模型能力弱时，NLL占优；中间区域无单一主导目标。


<details>
  <summary>Details</summary>
Motivation: 监督微调(SFT)是后训练大语言模型的标准方法，但其泛化能力有限。作者认为这种限制源于默认的负对数似然(NLL)目标，该目标在从头训练时最优，但在后训练场景下可能违反其最优性假设，因为模型已编码任务相关先验且监督信号可能冗长嘈杂。

Method: 研究了一类基于概率的目标家族，通过7个模型主干、14个基准测试和3个领域的综合实验和广泛消融研究，分析了不同条件下这些目标的有效性。

Result: 发现模型能力连续体是决定目标行为的关键维度：在模型能力强的一端，倾向于先验的目标(如$-p$, $-p^{10}$, 阈值变体)持续优于NLL；在模型能力弱的一端，NLL占主导；在中间区域，没有单一目标占优。

Conclusion: 理论分析阐明了目标在连续体上的权衡关系，为根据模型能力自适应选择目标提供了原则性基础。建议在模型能力强时使用偏向先验的目标，在模型能力弱时使用NLL。

Abstract: Supervised fine-tuning (SFT) is the standard approach for post-training large
language models (LLMs), yet it often shows limited generalization. We trace
this limitation to its default training objective: negative log likelihood
(NLL). While NLL is classically optimal when training from scratch,
post-training operates in a different paradigm and could violate its optimality
assumptions, where models already encode task-relevant priors and supervision
can be long and noisy. To this end, we study a general family of
probability-based objectives and characterize their effectiveness under
different conditions. Through comprehensive experiments and extensive ablation
studies across 7 model backbones, 14 benchmarks, and 3 domains, we uncover a
critical dimension that governs objective behavior: the model-capability
continuum. Near the model-strong end, prior-leaning objectives that downweight
low-probability tokens (e.g., $-p$, $-p^{10}$, thresholded variants)
consistently outperform NLL; toward the model-weak end, NLL dominates; in
between, no single objective prevails. Our theoretical analysis further
elucidates how objectives trade places across the continuum, providing a
principled foundation for adapting objectives to model capability. Our code is
available at https://github.com/GaotangLi/Beyond-Log-Likelihood.

</details>


### [26] [GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness](https://arxiv.org/abs/2510.00536)
*Kung-Hsiang Huang,Haoyi Qiu,Yutong Dai,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: GUI-KV是一种即插即用的KV缓存压缩方法，专门针对GUI代理设计，通过空间显著性引导和时间冗余评分技术，在保持精度的同时显著减少计算开销。


<details>
  <summary>Details</summary>
Motivation: GUI代理在处理高分辨率截图和长任务时面临效率挑战，现有缓存压缩方法未能充分利用GUI的空间和时间冗余特性。

Method: 结合空间显著性引导（增强注意力分数）和时间冗余评分（投影历史帧到当前子空间），采用均匀预算分配策略压缩KV缓存。

Result: 在标准基准测试中，GUI-KV在5截图设置下减少38.9%解码FLOPs，同时提高4.1%步骤准确率，接近全缓存精度。

Conclusion: 利用GUI特有的冗余特性可以实现高效可靠的代理性能，无需重新训练即可显著提升效率。

Abstract: Graphical user interface (GUI) agents built on vision-language models have
emerged as a promising approach to automate human-computer workflows. However,
they also face the inefficiency challenge as they process long sequences of
high-resolution screenshots and solving long-horizon tasks, making inference
slow, costly and memory-bound. While key-value (KV) caching can mitigate this,
storing the full cache is prohibitive for image-heavy contexts. Existing
cache-compression methods are sub-optimal as they do not account for the
spatial and temporal redundancy of GUIs. In this work, we first analyze
attention patterns in GUI agent workloads and find that, unlike in natural
images, attention sparsity is uniformly high across all transformer layers.
This insight motivates a simple uniform budget allocation strategy, which we
show empirically outperforms more complex layer-varying schemes. Building on
this, we introduce GUI-KV, a plug-and-play KV cache compression method for GUI
agents that requires no retraining. GUI-KV combines two novel techniques: (i)
spatial saliency guidance, which augments attention scores with the L2 norm of
hidden states to better preserve semantically important visual tokens, and (ii)
temporal redundancy scoring, which projects previous frames' keys onto the
current frame's key subspace to preferentially prune redundant history. Across
standard GUI agent benchmarks and models, GUI-KV outperforms competitive KV
compression baselines, closely matching full-cache accuracy at modest budgets.
Notably, in a 5-screenshot setting on the AgentNetBench benchmark, GUI-KV
reduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the
full-cache baseline. These results demonstrate that exploiting GUI-specific
redundancies enables efficient and reliable agent performance.

</details>


### [27] [ThinkBrake: Mitigating Overthinking in Tool Reasoning](https://arxiv.org/abs/2510.00546)
*Minjae Oh,Sangjun Song,Seungkyu Lee,Sungmin Jo,Yohan Jo*

Main category: cs.CL

TL;DR: 该论文提出了一种解决小型推理模型在工具使用中过度思考问题的方法，通过监控句子边界处的概率差异来提前终止推理，在保持准确性的同时显著减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 小型推理模型在工具使用过程中经常过度思考：它们已经达到了正确的工具参数配置，但继续推理并用错误的最终调用覆盖它。这种过度思考问题在工具推理领域尚未得到充分探索。

Method: 通过oracle rollout在句子边界注入</think>标签来诊断过度思考问题，并提出了ThinkBrake方法——一种无需训练的解码启发式方法，监控句子边界处</think>与当前顶部token之间的对数概率差异，当差异变小时触发终止。

Result: 在Berkeley Function Calling Leaderboard上，oracle终止将平均准确率从85.8%提升到94.2%，同时减少80-94%的token。ThinkBrake在单轮、非实时和实时分割中保持或提高了准确性，同时减少了高达25%的token。

Conclusion: 工具推理中存在显著的过度思考问题，通过适当的提前终止策略可以显著提升性能并减少计算开销，ThinkBrake方法在此方面优于各种基线方法。

Abstract: Small reasoning models (SRMs) often overthink during tool use: they reach a
correct tool-argument configuration, then continue reasoning and overwrite it
with an incorrect final call. We diagnose overthinking via oracle rollouts that
inject </think> at sentence boundaries. On the Berkeley Function Calling
Leaderboard (BFCL), this oracle termination lifts average accuracy from 85.8\%
to 94.2\% while reducing tokens by 80-94\%, revealing substantial recoverable
headroom and potential redundant reasoning. While prior work on concise
reasoning has largely targeted mathematics, tool reasoning remains
underexplored. We adapt various early-termination baselines to tool use and
introduce ThinkBrake, a training-free decoding heuristic. ThinkBrake monitors
the log-probability margin between </think> and the current top token at
sentence boundaries and triggers termination when this margin becomes small.
Across BFCL's single turn, non-live and live splits, ThinkBrake preserves or
improves accuracy while reducing tokens up to 25\%, outperforming various
baselines.

</details>


### [28] [Are Large Language Models Chronically Online Surfers? A Dataset for Chinese Internet Meme Explanation](https://arxiv.org/abs/2510.00567)
*Yubo Xie,Chenkai Wang,Zongyang Ma,Fahui Miao*

Main category: cs.CL

TL;DR: 提出了CHIME数据集用于评估大语言模型对中文网络流行语的理解能力，包含两个任务：解释流行语含义和选择合适流行语填空。结果显示LLMs在文化背景复杂的流行语理解上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型是否真正理解在互联网上快速传播的病毒性内容——网络流行语，特别是在中文语境下的文化内涵。

Method: 构建CHIME数据集，包含中文网络流行语及其详细注释；设计两个评估任务：1) 解释流行语含义、来源和生成例句；2) 在上下文句子中选择合适流行语填空的多选题。

Result: LLMs能够解释部分流行语含义，但在文化和语言细微差别的类型上表现显著下降；在提供准确来源方面持续困难；多选题表现正确但明显低于人类水平。

Conclusion: 大语言模型对中文网络流行语的理解能力有限，特别是在文化背景复杂的类型上。CHIME数据集公开以促进计算流行语理解的未来研究。

Abstract: Large language models (LLMs) are trained on vast amounts of text from the
Internet, but do they truly understand the viral content that rapidly spreads
online -- commonly known as memes? In this paper, we introduce CHIME, a dataset
for CHinese Internet Meme Explanation. The dataset comprises popular
phrase-based memes from the Chinese Internet, annotated with detailed
information on their meaning, origin, example sentences, types, etc. To
evaluate whether LLMs understand these memes, we designed two tasks. In the
first task, we assessed the models' ability to explain a given meme, identify
its origin, and generate appropriate example sentences. The results show that
while LLMs can explain the meanings of some memes, their performance declines
significantly for culturally and linguistically nuanced meme types.
Additionally, they consistently struggle to provide accurate origins for the
memes. In the second task, we created a set of multiple-choice questions (MCQs)
requiring LLMs to select the most appropriate meme to fill in a blank within a
contextual sentence. While the evaluated models were able to provide correct
answers, their performance remains noticeably below human levels. We have made
CHIME public and hope it will facilitate future research on computational meme
understanding.

</details>


### [29] [ReSeek: A Self-Correcting Framework for Search Agents with Instructive Rewards](https://arxiv.org/abs/2510.00568)
*Shiyu Li,Yang Tang,Yifan Wang,Peiming Li,Xi Chen*

Main category: cs.CL

TL;DR: 提出了ReSeek框架，通过自校正机制让搜索代理能够动态识别和纠正错误的搜索路径，使用JUDGE动作重新规划搜索策略，结合密集的过程奖励函数提升复杂推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的搜索代理依赖稀疏或基于规则的奖励，容易陷入次优或错误的推理路径且无法恢复，需要更有效的自校正机制。

Method: 引入自校正框架ReSeek，包含JUDGE动作让代理评估信息并重新规划搜索，设计密集的过程奖励函数分解为正确性奖励和实用性奖励，并使用新基准FictionalHot避免数据污染。

Result: 实验表明，使用ReSeek训练的代理在任务成功率和路径忠实度方面显著优于现有最先进基线方法。

Conclusion: ReSeek框架通过自校正机制和密集过程奖励有效提升了搜索代理在复杂推理任务中的性能，解决了现有方法无法从错误路径恢复的问题。

Abstract: Search agents powered by Large Language Models (LLMs) have demonstrated
significant potential in tackling knowledge-intensive tasks. Reinforcement
learning (RL) has emerged as a powerful paradigm for training these agents to
perform complex, multi-step reasoning. However, prior RL-based methods often
rely on sparse or rule-based rewards, which can lead agents to commit to
suboptimal or erroneous reasoning paths without the ability to recover. To
address these limitations, we propose ReSeek, a novel self-correcting framework
for training search agents. Our framework introduces a self-correction
mechanism that empowers the agent to dynamically identify and recover from
erroneous search paths during an episode. By invoking a special JUDGE action,
the agent can judge the information and re-plan its search strategy. To guide
this process, we design a dense, instructive process reward function, which
decomposes into a correctness reward for retrieving factual information and a
utility reward for finding information genuinely useful for the query.
Furthermore, to mitigate the risk of data contamination in existing datasets,
we introduce FictionalHot, a new and challenging benchmark with recently
curated questions requiring complex reasoning. Being intuitively reasonable and
practically simple, extensive experiments show that agents trained with ReSeek
significantly outperform SOTA baselines in task success rate and path
faithfulness.

</details>


### [30] [CoT Vectors: Transferring and Probing the Reasoning Mechanisms of LLMs](https://arxiv.org/abs/2510.00579)
*Li Li,Ziyi Wang,Yongliang Wu,Jianfei Cai,Xu Yang*

Main category: cs.CL

TL;DR: 提出了CoT Vectors方法，通过紧凑表示编码多步推理知识，解决了传统CoT方法成本高、效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 现有Chain-of-Thought实现（如上下文学习和微调）成本高且效率低，需要更经济高效的推理增强方法。

Method: 引入CoT Vectors作为紧凑表示，通过提取和可学习两种方式实现：提取式观察层间不稳定性，可学习式在师生框架下优化提供稳定指导。

Result: 在多种基准测试和模型上，CoT Vectors不仅超越现有基线，性能可与参数高效微调方法媲美，且需要更少可训练参数。

Conclusion: CoT Vectors不仅提供高效推理增强，还作为探针揭示了LLM中多步推理的功能组织机制，包括潜在空间结构、信息密度等因素的影响。

Abstract: Chain-of-Thought (CoT) prompting has emerged as a powerful approach to
enhancing the reasoning capabilities of Large Language Models (LLMs). However,
existing implementations, such as in-context learning and fine-tuning, remain
costly and inefficient. To improve CoT reasoning at a lower cost, and inspired
by the task vector paradigm, we introduce CoT Vectors, compact representations
that encode task-general, multi-step reasoning knowledge. Through experiments
with Extracted CoT Vectors, we observe pronounced layer-wise instability,
manifesting as a U-shaped performance curve that reflects a systematic
three-stage reasoning process in LLMs. To address this limitation, we propose
Learnable CoT Vectors, optimized under a teacher-student framework to provide
more stable and robust guidance. Extensive evaluations across diverse
benchmarks and models demonstrate that CoT Vectors not only outperform existing
baselines but also achieve performance comparable to parameter-efficient
fine-tuning methods, while requiring fewer trainable parameters. Moreover, by
treating CoT Vectors as a probe, we uncover how their effectiveness varies due
to latent space structure, information density, acquisition mechanisms, and
pre-training differences, offering new insights into the functional
organization of multi-step reasoning in LLMs. The source code will be released.

</details>


### [31] [SAGE-LD: Towards Scalable and Generalizable End-to-End Language Diarization via Simulated Data Augmentation](https://arxiv.org/abs/2510.00582)
*Sangmin Lee,Woongjib Choi,Jihyun Kim,Hong-Goo Kang*

Main category: cs.CL

TL;DR: 提出一种支持多语言的神经说话人语言识别模型，通过可学习查询架构和大规模代码切换预训练，在多个基准测试中取得23%-52%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在数据稀缺和架构优化方面的限制，构建能够有效泛化到真实世界多语言环境的语言识别框架。

Method: 集成基于可学习查询的多语言感知架构，并在模拟代码切换数据上进行大规模预训练，联合利用这两个组件。

Result: 在多个语言识别基准测试中达到最先进性能，相对先前方法有23%到52%的性能提升。

Conclusion: 该工作不仅推进了语言识别研究，还为代码切换语音技术建立了基础框架。

Abstract: In this paper, we present a neural spoken language diarization model that
supports an unconstrained span of languages within a single framework. Our
approach integrates a learnable query-based architecture grounded in
multilingual awareness, with large-scale pretraining on simulated
code-switching data. By jointly leveraging these two components, our method
overcomes the limitations of conventional approaches in data scarcity and
architecture optimization, and generalizes effectively to real-world
multilingual settings across diverse environments. Experimental results
demonstrate that our approach achieves state-of-the-art performance on several
language diarization benchmarks, with a relative performance improvement of 23%
to 52% over previous methods. We believe that this work not only advances
research in language diarization but also establishes a foundational framework
for code-switching speech technologies.

</details>


### [32] [Tenyidie Syllabification corpus creation and deep learning applications](https://arxiv.org/abs/2510.00629)
*Teisovi Angami,Kevisino Khate*

Main category: cs.CL

TL;DR: 该论文为低资源语言Tenyidie创建了10,120个音节化单词的数据集，并应用多种深度学习模型进行音节划分任务，其中BLSTM模型在测试集上达到了99.21%的最高准确率。


<details>
  <summary>Details</summary>
Motivation: Tenyidie是印度那加兰邦的一种低资源藏缅语系语言，目前尚未有关于其音节划分的研究。音节划分是NLP中的重要任务，对形态分析、词性标注等应用至关重要。

Method: 创建了10,120个音节化Tenyidie单词的数据集，应用了LSTM、BLSTM、BLSTM+CRF和编码器-解码器等深度学习架构，数据集按80:10:10的比例划分为训练集、验证集和测试集。

Result: 在测试集上，BLSTM模型取得了99.21%的最高准确率，表明深度学习模型在Tenyidie语言的音节划分任务上表现优异。

Conclusion: 这项工作为Tenyidie语言的NLP研究提供了重要资源，将有助于该语言的形态分析、词性标注、机器翻译等后续应用的发展。

Abstract: The Tenyidie language is a low-resource language of the Tibeto-Burman family
spoken by the Tenyimia Community of Nagaland in the north-eastern part of India
and is considered a major language in Nagaland. It is tonal,
Subject-Object-Verb, and highly agglutinative in nature. Being a low-resource
language, very limited research on Natural Language Processing (NLP) has been
conducted. To the best of our knowledge, no work on syllabification has been
reported for this language. Among the many NLP tasks, syllabification or
syllabication is an important task in which the given word syllables are
identified. The contribution of this work is the creation of 10,120 syllabified
Tenyidie words and the application of the Deep Learning techniques on the
created corpus. In this paper, we have applied LSTM, BLSTM, BLSTM+CRF, and
Encoder-decoder deep learning architectures on our created dataset. In our
dataset split of 80:10:10 (train:validation:test) set, we achieved the highest
accuracy of 99.21% with BLSTM model on the test set. This work will find its
application in numerous other NLP applications, such as morphological analysis,
part-of-speech tagging, machine translation, etc, for the Tenyidie Language.
  Keywords: Tenyidie; NLP; syllabification; deep learning; LSTM; BLSTM; CRF;
Encoder-decoder

</details>


### [33] [MCM-DPO: Multifaceted Cross-Modal Direct Preference Optimization for Alt-text Generation](https://arxiv.org/abs/2510.00647)
*Jinlan Fu,Shenzhen Huangfu,Hao Fei,Yichong Huang,Xiaoyu Shen,Xipeng Qiu,See-Kiong Ng*

Main category: cs.CL

TL;DR: 提出了MCM-DPO方法，通过多维度跨模态直接偏好优化来改进替代文本生成，无需精确标注，在Twitter和Pinterest构建的大规模数据集上表现优于DPO和SFT。


<details>
  <summary>Details</summary>
Motivation: 现有替代文本生成性能受限，主要由于用户标注噪声、标准不一致以及多模态大语言模型对上下文信息不敏感。监督微调依赖准确的目标标注，但在用户生成的替代文本中这些标注往往存在缺陷。

Method: 提出多维度跨模态直接偏好优化(MCM-DPO)，在偏好对中学习识别更好的选项，无需精确标注。该方法优化了单个、配对和多重偏好维度，涵盖文本、视觉和跨模态因素。构建了TAlt和PAlt两个大规模高质量数据集。

Result: 实验结果表明，MCM-DPO方法在替代文本生成任务中持续优于DPO和SFT方法，建立了新的最先进水平。

Conclusion: MCM-DPO通过多维度偏好优化有效解决了替代文本生成中的标注质量问题，为盲人和低视力用户提供了更好的图像访问支持。

Abstract: The alt-text generation task produces concise, context-relevant descriptions
of images, enabling blind and low-vision users to access online images. Despite
the capabilities of large vision-language models, alt-text generation
performance remains limited due to noisy user annotations, inconsistent
standards, and MLLMs' insensitivity to contextual information. Previous efforts
to fine-tune MLLMs using supervised fine-tuning (SFT) have struggled, as SFT
relies on accurate target annotations, which are often flawed in user-generated
alt-text. To address this, we propose Multi-faceted Cross-modal Direct
Preference Optimization (MCM-DPO), which improves alt-text generation by
learning to identify better options in preference pairs without requiring
precise annotations. MCM-DPO optimizes preferences across single, paired, and
multi-preference dimensions, covering textual, visual, and cross-modal factors.
In light of the scarcity of high-quality annotated and preference-labeled
datasets for alt-text, we constructed two large-scale, high-quality datasets
named TAlt and PAlt, sourced from Twitter and Pinterest. These datasets include
202k annotated alt-text samples and 18k preference pairs that cover diverse
preference dimensions, aiming to support further research in this domain.
Experimental results show that our proposed MCM-DPO method consistently
outperforms both DPO and SFT, establishing a new state of the art in alt-text
generation. We release the code and data here:
https://github.com/LVUGAI/MCM-DPO

</details>


### [34] [Facilitating Cognitive Accessibility with LLMs: A Multi-Task Approach to Easy-to-Read Text Generation](https://arxiv.org/abs/2510.00662)
*François Ledoyen,Gaël Dias,Jeremie Pantin,Alexis Lechervy,Fabrice Maurel,Youssef Chahir*

Main category: cs.CL

TL;DR: 使用多任务学习方法自动化生成易读文本，通过结合文本摘要、文本简化和ETR生成任务，在Mistral-7B和LLaMA-3-8B模型上验证了多任务设置优于单任务基线。


<details>
  <summary>Details</summary>
Motivation: 为神经多样性人群提供信息平等获取，解决手动创建易读文本耗时耗力的问题，利用大语言模型自动化生成符合Easy-to-Read标准的文本内容。

Method: 提出多任务学习方法，包括基于检索增强生成的多任务RAG策略和参数高效微调的MTL-LoRA方法，在ETR-fr新数据集上训练模型。

Result: 多任务设置在所有配置中都优于单任务基线，RAG策略在领域外设置中具有泛化能力，MTL-LoRA在领域内配置中表现最佳。

Conclusion: 多任务学习是自动化生成易读文本的有效方法，不同策略在不同场景下各有优势，为解决ETR内容生成提供了可行方案。

Abstract: Simplifying complex texts is essential for ensuring equitable access to
information, especially for individuals with cognitive impairments. The
Easy-to-Read (ETR) initiative offers a framework for making content accessible
to the neurodivergent population, but the manual creation of such texts remains
time-consuming and resource-intensive. In this work, we investigate the
potential of large language models (LLMs) to automate the generation of ETR
content. To address the scarcity of aligned corpora and the specificity of ETR
constraints, we propose a multi-task learning (MTL) approach that trains models
jointly on text summarization, text simplification, and ETR generation. We
explore two different strategies: multi-task retrieval-augmented generation
(RAG) for in-context learning, and MTL-LoRA for parameter-efficient
fine-tuning. Our experiments with Mistral-7B and LLaMA-3-8B, based on ETR-fr, a
new high-quality dataset, demonstrate the benefits of multi-task setups over
single-task baselines across all configurations. Moreover, results show that
the RAG-based strategy enables generalization in out-of-domain settings, while
MTL-LoRA outperforms all learning strategies within in-domain configurations.

</details>


### [35] [Inclusive Easy-to-Read Generation for Individuals with Cognitive Impairments](https://arxiv.org/abs/2510.00691)
*François Ledoyen,Gaël Dias,Alexis Lechervy,Jeremie Pantin,Fabrice Maurel,Youssef Chahir,Elisa Gouzonnat,Mélanie Berthelot,Stanislas Moravac,Armony Altinier,Amy Khairalla*

Main category: cs.CL

TL;DR: 本文提出了ETR-fr数据集，这是首个完全符合欧洲易读文本指南的数据集，用于AI驱动的易读文本生成，解决了数据集稀缺、领域适应和大型语言模型轻量学习等关键挑战。


<details>
  <summary>Details</summary>
Motivation: 确保认知障碍人士的信息可访问性对于他们的自主权、自决权和完全公民身份至关重要。手动易读文本改编缓慢、成本高且难以扩展，限制了在医疗、教育和公民生活中的关键信息获取。

Method: 构建了ETR-fr数据集，实施参数高效微调在预训练语言模型和大型语言模型上建立生成基线，并引入基于自动指标和人工评估的评估框架，使用与指南对齐的36问题评估表。

Result: 结果显示预训练语言模型与大型语言模型表现相当，并能有效适应领域外文本。

Conclusion: AI驱动的易读文本生成提供了可扩展的解决方案，预训练语言模型在易读文本生成任务中表现出色，能够有效解决数据集稀缺和领域适应问题。

Abstract: Ensuring accessibility for individuals with cognitive impairments is
essential for autonomy, self-determination, and full citizenship. However,
manual Easy-to-Read (ETR) text adaptations are slow, costly, and difficult to
scale, limiting access to crucial information in healthcare, education, and
civic life. AI-driven ETR generation offers a scalable solution but faces key
challenges, including dataset scarcity, domain adaptation, and balancing
lightweight learning of Large Language Models (LLMs). In this paper, we
introduce ETR-fr, the first dataset for ETR text generation fully compliant
with European ETR guidelines. We implement parameter-efficient fine-tuning on
PLMs and LLMs to establish generative baselines. To ensure high-quality and
accessible outputs, we introduce an evaluation framework based on automatic
metrics supplemented by human assessments. The latter is conducted using a
36-question evaluation form that is aligned with the guidelines. Overall
results show that PLMs perform comparably to LLMs and adapt effectively to
out-of-domain texts.

</details>


### [36] [ALARB: An Arabic Legal Argument Reasoning Benchmark](https://arxiv.org/abs/2510.00694)
*Harethah Abu Shairah,Somayah AlHarbi,Abdulaziz AlHussein,Sameer Alsabea,Omar Shaqaqi,Hebah AlShamlan,Omar Knio,George Turkiyyah*

Main category: cs.CL

TL;DR: ALARB是一个阿拉伯语法律推理数据集，包含13K+沙特商业法庭案例，用于评估LLMs在法律领域中的多步推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有阿拉伯语基准测试主要关注知识密集型任务，但缺乏专门针对开放式多步推理的数据集，特别是在法律领域。

Method: 使用13K+沙特商业法庭案例，定义包括判决预测、推理链补全和相关法规识别等挑战性任务，并对代表性阿拉伯语LLMs进行基准测试。

Result: 使用ALARB进行指令调优的12B参数模型在判决预测和阿拉伯语判决生成方面表现显著提升，达到与GPT-4o相当的水平。

Conclusion: ALARB数据集有效提升了阿拉伯语LLMs在法律推理任务中的性能，证明了其在指令调优中的实用性。

Abstract: We introduce ALARB, a dataset and suite of tasks designed to evaluate the
reasoning capabilities of large language models (LLMs) within the Arabic legal
domain. While existing Arabic benchmarks cover some knowledge-intensive tasks
such as retrieval and understanding, substantial datasets focusing specifically
on multistep reasoning for Arabic LLMs, especially in open-ended contexts, are
lacking. The dataset comprises over 13K commercial court cases from Saudi
Arabia, with each case including the facts presented, the reasoning of the
court, the verdict, as well as the cited clauses extracted from the regulatory
documents. We define a set of challenging tasks leveraging this dataset and
reflecting the complexity of real-world legal reasoning, including verdict
prediction, completion of reasoning chains in multistep legal arguments, and
identification of relevant regulations based on case facts. We benchmark a
representative selection of current open and closed Arabic LLMs on these tasks
and demonstrate the dataset's utility for instruction tuning. Notably, we show
that instruction-tuning a modest 12B parameter model using ALARB significantly
enhances its performance in verdict prediction and Arabic verdict generation,
reaching a level comparable to that of GPT-4o.

</details>


### [37] [Family Matters: Language Transfer and Merging for Adapting Small LLMs to Faroese](https://arxiv.org/abs/2510.00810)
*Jenny Kunz,Iben Nyholm Debess,Annika Simonsen*

Main category: cs.CL

TL;DR: 该研究探索如何将小型高效LLM适配到法罗语（一种低资源的北日耳曼语言），通过从英语模型出发，在相关斯堪的纳维亚语言上继续预训练，然后对法罗语进行微调，比较了全参数微调和LoRA参数高效微调方法。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言法罗语的LLM适配问题，探索从相关语言进行知识迁移的有效方法，并比较不同微调策略对语言准确性和文本理解的影响。

Method: 从英语模型开始，在相关斯堪的纳维亚语言（单独或合并）上继续预训练，然后对法罗语进行微调；比较全参数微调和LoRA方法；构建新的法罗语评估基准，并辅以语言学家的人工评估。

Result: 相关语言的迁移至关重要，但最佳源语言因任务而异：冰岛语提升语言准确性，丹麦语促进理解；LoRA在基础模型上改善语言可接受性并略微提高人工评估分数，而全参数微调在理解任务上表现更强，并在下游微调中更好地保留模型能力。

Conclusion: 针对低资源语言的LLM适配需要根据具体任务选择适当的源语言和微调策略，语言迁移是有效的，但最优方法因任务目标而异。

Abstract: We investigate how to adapt small, efficient LLMs to Faroese, a low-resource
North Germanic language. Starting from English models, we continue pre-training
on related Scandinavian languages, either individually or combined via merging,
before fine-tuning on Faroese. We compare full fine-tuning with
parameter-efficient tuning using LoRA, evaluating their impact on both
linguistic accuracy and text comprehension. Due to the lack of existing Faroese
evaluation data, we construct two new minimal-pair benchmarks from adapted and
newly collected datasets and complement them with human evaluations by Faroese
linguists. Our results demonstrate that transfer from related languages is
crucial, though the optimal source language depends on the task: Icelandic
enhances linguistic accuracy, whereas Danish boosts comprehension. Similarly,
the choice between full fine-tuning and LoRA is task-dependent: LoRA improves
linguistic acceptability and slightly increases human evaluation scores on the
base model, while full fine-tuning yields stronger comprehension performance
and better preserves model capabilities during downstream fine-tuning.

</details>


### [38] [Exposing the Cracks: Vulnerabilities of Retrieval-Augmented LLM-based Machine Translation](https://arxiv.org/abs/2510.00829)
*Yanming Sun,Runzhe Zhan,Chi Seng Cheang,Han Wu,Xuebo Liu,Yuyao Niu,Fengying Ye,Kaixin Lan,Lidia S. Chao,Derek F. Wong*

Main category: cs.CL

TL;DR: REAL-MT在噪声检索环境下可靠性不足，特别是在低资源语言对中表现更差，即使增强推理模型也无法纠正错误，反而更容易受噪声影响。


<details>
  <summary>Details</summary>
Motivation: 尽管检索增强的LLM机器翻译在知识密集型任务中表现良好，但其在真实部署中常见的噪声检索环境下的可靠性尚未得到充分研究。

Method: 提出了噪声合成框架和新评估指标，使用Qwen系列模型（包括标准LLM和增强推理的大型推理模型）在不同资源水平的语言对上评估REAL-MT在合成噪声下的表现。

Result: 低资源语言对在噪声下退化更严重，常产生无意义翻译；增强推理模型在错误纠正方面无改进，反而更易受噪声影响，倾向于合理化错误上下文；注意力从源习语转向噪声内容，置信度增加但准确性下降。

Conclusion: 当前方法存在局限性，需要在干净上下文性能和噪声鲁棒性之间权衡，突显了需要自验证集成机制的必要性。

Abstract: \textbf{RE}trieval-\textbf{A}ugmented \textbf{L}LM-based \textbf{M}achine
\textbf{T}ranslation (REAL-MT) shows promise for knowledge-intensive tasks like
idiomatic translation, but its reliability under noisy retrieval contexts
remains poorly understood despite this being a common challenge in real-world
deployment. To address this gap, we propose a noise synthesis framework and new
metrics to evaluate the robustness of REAL-MT systematically. Using this
framework, we instantiate REAL-MT with Qwen-series models, including standard
LLMs and large reasoning models (LRMs) with enhanced reasoning, and evaluate
their performance on idiomatic translation across high-, medium-, and
low-resource language pairs under synthesized noise. Our results show that
low-resource language pairs, which rely more heavily on retrieved context,
degrade more severely under noise than high-resource ones and often produce
nonsensical translations. Although LRMs possess enhanced reasoning
capabilities, they show no improvement in error correction and are even more
susceptible to noise, tending to rationalize incorrect contexts. We find that
this stems from an attention shift away from the source idiom to noisy content,
while confidence increases despite declining accuracy, indicating poor
calibration. To mitigate these issues, we investigate training-free and
fine-tuning strategies, which improve robustness at the cost of performance in
clean contexts, revealing a fundamental trade-off. Our findings highlight the
limitations of current approaches, underscoring the need for self-verifying
integration mechanisms.

</details>


### [39] [ManagerBench: Evaluating the Safety-Pragmatism Trade-off in Autonomous LLMs](https://arxiv.org/abs/2510.00857)
*Adi Simhi,Jonathan Herzig,Martin Tutek,Itay Itzhak,Idan Szpektor,Yonatan Belinkov*

Main category: cs.CL

TL;DR: 提出了ManagerBench基准测试，用于评估LLM在现实管理场景中的决策安全性与实用性权衡，发现前沿LLM在安全-实用权衡方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有安全基准主要关注防止生成有害内容，但忽视了当操作目标与人类安全冲突时，智能体可能采取有害行动的问题。

Method: 创建包含人类验证的管理场景，每个场景都要求在实现操作目标的实用但有害行动与导致较差操作性能的安全行动之间做出选择，并设置平行控制集来测量模型的实用主义倾向。

Result: 前沿LLM在安全-实用性权衡方面表现不佳：许多模型为推进操作目标而持续选择有害选项，而其他模型则因避免伤害而变得过于安全且无效。

Conclusion: 这种错位并非源于无法感知伤害，而是源于优先级的错误设定。ManagerBench是评估智能体行为核心组成部分的挑战性基准：在操作目标和对齐价值激励冲突行动时做出安全选择。

Abstract: As large language models (LLMs) evolve from conversational assistants into
autonomous agents, evaluating the safety of their actions becomes critical.
Prior safety benchmarks have primarily focused on preventing generation of
harmful content, such as toxic text. However, they overlook the challenge of
agents taking harmful actions when the most effective path to an operational
goal conflicts with human safety. To address this gap, we introduce
ManagerBench, a benchmark that evaluates LLM decision-making in realistic,
human-validated managerial scenarios. Each scenario forces a choice between a
pragmatic but harmful action that achieves an operational goal, and a safe
action that leads to worse operational performance. A parallel control set,
where potential harm is directed only at inanimate objects, measures a model's
pragmatism and identifies its tendency to be overly safe. Our findings indicate
that the frontier LLMs perform poorly when navigating this safety-pragmatism
trade-off. Many consistently choose harmful options to advance their
operational goals, while others avoid harm only to become overly safe and
ineffective. Critically, we find this misalignment does not stem from an
inability to perceive harm, as models' harm assessments align with human
judgments, but from flawed prioritization. ManagerBench is a challenging
benchmark for a core component of agentic behavior: making safe choices when
operational goals and alignment values incentivize conflicting actions.
Benchmark & code available at https://github.com/technion-cs-nlp/ManagerBench.

</details>


### [40] [Erase to Improve: Erasable Reinforcement Learning for Search-Augmented LLMs](https://arxiv.org/abs/2510.00861)
*Ziliang Wang,Kang An,Xuhui Zheng,Faqiang Qian,Weikun Zhang,Cijun Ouyang,Jialu Cai,Yuhang Wang,Yichao Wu*

Main category: cs.CL

TL;DR: 提出可擦除强化学习(ERL)框架，通过识别、擦除和重新生成错误推理步骤，显著提升搜索增强大语言模型在多跳推理任务中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 搜索增强大语言模型在复杂多跳推理中可靠性有限，主要面临分解错误、检索缺失和推理错误三大挑战，任一环节的失败都会导致最终答案错误。

Method: ERL框架明确识别错误步骤，擦除它们并在原位置重新生成推理，防止错误逻辑在推理链中传播。

Result: 在HotpotQA、MuSiQue、2Wiki和Bamboogle等数据集上，3B模型EM提升8.48%、F1提升11.56%，7B模型EM提升5.38%、F1提升7.22%，超越之前的SOTA结果。

Conclusion: 可擦除强化学习为LLMs中的鲁棒多步推理提供了强大的范式转变。

Abstract: While search-augmented large language models (LLMs) exhibit impressive
capabilities, their reliability in complex multi-hop reasoning remains limited.
This limitation arises from three fundamental challenges: decomposition errors,
where tasks are incorrectly broken down; retrieval missing, where key evidence
fails to be retrieved; and reasoning errors, where flawed logic propagates
through the reasoning chain. A single failure in any of these stages can derail
the final answer. We propose Erasable Reinforcement Learning (ERL), a novel
framework that transforms fragile reasoning into a robust process. ERL
explicitly identifies faulty steps, erases them, and regenerates reasoning in
place, preventing defective logic from propagating through the reasoning chain.
This targeted correction mechanism turns brittle reasoning into a more
resilient process. Models trained with ERL, termed ESearch, achieve substantial
improvements on HotpotQA, MuSiQue, 2Wiki, and Bamboogle, with the 3B model
achieving +8.48% EM and +11.56% F1, and the 7B model achieving +5.38% EM and
+7.22% F1 over previous state-of-the-art(SOTA) results. These findings suggest
that erasable reinforcement learning provides a powerful paradigm shift for
robust multi-step reasoning in LLMs.

</details>


### [41] [HalluGuard: Evidence-Grounded Small Reasoning Models to Mitigate Hallucinations in Retrieval-Augmented Generation](https://arxiv.org/abs/2510.00880)
*Loris Bergeron,Ioana Buhnila,Jérôme François,Radu State*

Main category: cs.CL

TL;DR: HalluGuard是一个40亿参数的小型推理模型，专门用于缓解检索增强生成(RAG)中的幻觉问题，通过分类文档-声明对并提供证据支持的论证来实现透明性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在NLP任务中表现出色，但在实际应用中容易产生幻觉，限制了其可信度。需要专门的方法来检测和缓解RAG系统中的幻觉问题。

Method: 结合领域无关的合成数据集、合成的基础和幻觉声明，以及使用几率比偏好优化的偏好微调方法，将大模型推理能力蒸馏到小型骨干网络中。

Result: 在RAGTruth子集上达到84.0%的平衡准确率，与专门模型MiniCheck和Granite Guardian 3.3相当，但参数量仅为它们的一半。在整个基准测试中达到75.7%的平衡准确率，与GPT-4o性能相当。

Conclusion: HalluGuard展示了通过精心设计的数据合成和偏好微调，可以构建高效的小型模型来有效检测RAG系统中的幻觉，性能可与更大模型媲美。

Abstract: Large Language Models (LLMs) excel in many NLP tasks but remain prone to
hallucinations, limiting trust in real-world applications. We present
HalluGuard, a 4B-parameter Small Reasoning Model (SRM) for mitigating
hallucinations in Retrieval-Augmented Generation (RAG). HalluGuard classifies
document-claim pairs as grounded or hallucinated and produces evidence-grounded
justifications for transparency. Our approach combines (i) a domain-agnostic
synthetic dataset derived from FineWeb and refined through multi-stage curation
and data reformation, (ii) synthetic grounded and hallucinated claims, and
(iii) preference-based fine-tuning with Odds Ratio Preference Optimization to
distill large-model reasoning into a smaller backbone. On the RAGTruth subset
of the LLM-AggreFact benchmark, HalluGuard achieves 84.0% balanced accuracy
(BAcc), rivaling specialized models, MiniCheck (7B; 84.0%) and Granite Guardian
3.3 (8B; 82.2%) while using roughly half their parameters. Over the full
benchmark it reaches 75.7% BAcc, matching larger general-purpose LLMs such as
GPT-4o (75.9%). We will release HalluGuard and datasets under Apache 2.0 upon
acceptance.

</details>


### [42] [Span-level Detection of AI-generated Scientific Text via Contrastive Learning and Structural Calibration](https://arxiv.org/abs/2510.00890)
*Zhen Yin,Shenghua Wang*

Main category: cs.CL

TL;DR: Sci-SpanDet是一个用于检测AI生成学术文本的结构感知框架，通过结合分段条件风格建模和多层次对比学习，实现细粒度的AI文本定位和可靠的置信度估计。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在科学写作中的快速应用引发了关于作者身份完整性和学术出版物可靠性的严重担忧，现有检测方法存在细粒度定位缺失、校准能力弱和跨领域泛化能力差等问题。

Method: 结合分段条件风格建模与多层次对比学习，集成BIO-CRF序列标注和基于指针的边界解码，并进行置信度校准。

Result: 在包含10万个标注样本的跨学科数据集上，Sci-SpanDet实现了最先进的性能：AI-F1为80.17，AUROC为92.63，Span-F1为74.36，在对抗性重写下表现出强韧性。

Conclusion: Sci-SpanDet在AI生成学术文本检测方面显著超越现有基线方法，具有跨学科鲁棒性和精确的片段级检测能力，相关数据集和源代码将公开发布以促进进一步研究。

Abstract: The rapid adoption of large language models (LLMs) in scientific writing
raises serious concerns regarding authorship integrity and the reliability of
scholarly publications. Existing detection approaches mainly rely on
document-level classification or surface-level statistical cues; however, they
neglect fine-grained span localization, exhibit weak calibration, and often
fail to generalize across disciplines and generators. To address these
limitations, we present Sci-SpanDet, a structure-aware framework for detecting
AI-generated scholarly texts. The proposed method combines section-conditioned
stylistic modeling with multi-level contrastive learning to capture nuanced
human-AI differences while mitigating topic dependence, thereby enhancing
cross-domain robustness. In addition, it integrates BIO-CRF sequence labeling
with pointer-based boundary decoding and confidence calibration to enable
precise span-level detection and reliable probability estimates. Extensive
experiments on a newly constructed cross-disciplinary dataset of 100,000
annotated samples generated by multiple LLM families (GPT, Qwen, DeepSeek,
LLaMA) demonstrate that Sci-SpanDet achieves state-of-the-art performance, with
F1(AI) of 80.17, AUROC of 92.63, and Span-F1 of 74.36. Furthermore, it shows
strong resilience under adversarial rewriting and maintains balanced accuracy
across IMRaD sections and diverse disciplines, substantially surpassing
existing baselines. To ensure reproducibility and to foster further research on
AI-generated text detection in scholarly documents, the curated dataset and
source code will be publicly released upon publication.

</details>


### [43] [Benchmarking Foundation Models with Retrieval-Augmented Generation in Olympic-Level Physics Problem Solving](https://arxiv.org/abs/2510.00919)
*Shunfeng Zheng,Yudi Zhang,Meng Fang,Zihan Zhang,Zhitan Wu,Mykola Pechenizkiy,Ling Chen*

Main category: cs.CL

TL;DR: 该论文研究了检索增强生成(RAG)在解决奥林匹克物理问题中的潜力，提出了PhoPile多模态数据集，并展示了RAG能提升基础模型在物理推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 探索RAG在专家级物理推理（如解决奥林匹克物理问题）中的潜力，受学生通过复习过往问题准备竞赛的启发。

Method: 引入PhoPile多模态数据集，包含图表和方程，系统性地研究基于检索的推理，并基准测试RAG增强的基础模型（包括LLM和LMM）。

Result: 结果表明，将检索与物理语料库集成可以提高模型性能，但也突显了需要进一步研究的挑战。

Conclusion: RAG在增强物理推理方面具有潜力，但还需要更多研究来解决现有挑战。

Abstract: Retrieval-augmented generation (RAG) with foundation models has achieved
strong performance across diverse tasks, but their capacity for expert-level
reasoning-such as solving Olympiad-level physics problems-remains largely
unexplored. Inspired by the way students prepare for competitions by reviewing
past problems, we investigate the potential of RAG to enhance physics reasoning
in foundation models. We introduce PhoPile, a high-quality multimodal dataset
specifically designed for Olympiad-level physics, enabling systematic study of
retrieval-based reasoning. PhoPile includes diagrams, graphs, and equations,
capturing the inherently multimodal nature of physics problem solving. Using
PhoPile, we benchmark RAG-augmented foundation models, covering both large
language models (LLMs) and large multimodal models (LMMs) with multiple
retrievers. Our results demonstrate that integrating retrieval with physics
corpora can improve model performance, while also highlighting challenges that
motivate further research in retrieval-augmented physics reasoning.

</details>


### [44] [Making, not Taking, the Best of N](https://arxiv.org/abs/2510.00931)
*Ammar Khairi,Daniel D'souza,Marzieh Fadaee,Julia Kreutzer*

Main category: cs.CL

TL;DR: 提出Fusion-of-N方法，通过LLM法官将多个生成样本中最具信息量的元素融合成单一最终答案，相比传统的Best-of-N选择方法能更好地利用多样性信息。


<details>
  <summary>Details</summary>
Motivation: 传统的Best-of-N方法本质上是零和游戏，会丢弃池中多样且有潜在价值的信息。需要探索协作设置，让所有候选都能为最终生成做出贡献。

Method: 使用通用LLM法官来合成每个样本中最具信息量的元素，形成单一最终答案。在测试时扩展和合成数据生成两种设置下进行评估。

Result: 在11种语言、3个不同任务和不同模型规模上的广泛基准测试中，FusioN始终优于BoN，在测试时扩展和合成数据生成的下游增益方面都表现出多功能性和鲁棒性。

Conclusion: 应该从单一质量度量转向接受LLM生成的多面性，通过整合多样优势、释放潜在能力，实现仅靠选择无法达到的改进。

Abstract: Obtaining high-quality generations in modern LLMs has largely been framed as
a selection problem: identifying a single winning generation from a diverse
pool of N samples, the Best-of-N (BoN). Yet, this approach is inherently
zero-sum, discarding diverse and potentially useful information from the pool.
Instead, we explore a collaborative setup, where all candidates can potentially
contribute to the final winning generation. To this end, we propose Fusion-of-N
(FusioN): a method that uses a general LLM judge to synthesize the most
informative elements of each sample into a single final answer. We compare
FusioN to BoN in two settings, (i) test-time scaling, where we sample and
aggregate from a single model at test-time (ii) synthetic data generation,
where we fuse samples from a pool of diverse teachers to improve a student
model. We extensively benchmark both setups across 11 languages, 3 diverse
tasks and varying model scales. Across the bench, FusioN consistently
outperforms BoN showing versatility and robustness both in test-time scaling
and in downstream gains from synthetic data generation. We also perform
extensive analysis on FusioN, where it shows surprising strengths and
robustness under challenging settings. These results show that we should shift
how we think about evaluating and utilizing LLM generations from a monolithic
measure of quality, to embracing their polylithic nature. This shift allows us
to integrate diverse strengths, unlock latent potential, and achieve
improvements that were previously inaccessible through selection alone.

</details>


### [45] [Analyzing Dialectical Biases in LLMs for Knowledge and Reasoning Benchmarks](https://arxiv.org/abs/2510.00962)
*Eileen Pan,Anna Seo Gyeong Choi,Maartje ter Hoeve,Skyler Seto,Allison Koenecke*

Main category: cs.CL

TL;DR: LLMs在非标准英语方言上表现不佳，某些特定语法规则（如existential 'it'、zero copula和y'all）能解释大部分性能下降。


<details>
  <summary>Details</summary>
Motivation: 分析LLMs在处理非标准英语方言时的性能下降问题，特别是语法规则对性能的影响。

Method: 通过将标准美国英语问题转换为非标准方言变体，在多选题回答任务上测试LLM性能，并研究具体语法规则的影响。

Result: 非标准英语问题导致准确率下降高达20%，其中三个特定语法规则解释了大部分性能下降。

Conclusion: 呼吁未来研究关注针对个别高影响语法结构的偏见缓解方法。

Abstract: Large language models (LLMs) are ubiquitous in modern day natural language
processing. However, previous work has shown degraded LLM performance for
under-represented English dialects. We analyze the effects of typifying
"standard" American English language questions as non-"standard" dialectal
variants on multiple choice question answering tasks and find up to a 20%
reduction in accuracy. Additionally, we investigate the grammatical basis of
under-performance in non-"standard" English questions. We find that individual
grammatical rules have varied effects on performance, but some are more
consequential than others: three specific grammar rules (existential "it", zero
copula, and y'all) can explain the majority of performance degradation observed
in multiple dialects. We call for future work to investigate bias mitigation
methods focused on individual, high-impact grammatical structures.

</details>


### [46] [Syntax-Guided Diffusion Language Models with User-Integrated Personalization](https://arxiv.org/abs/2510.01028)
*Ruqian Zhang,Yijiao Zhang,Juan Shen,Zhongyi Zhu,Annie Qu*

Main category: cs.CL

TL;DR: 提出了一种语法引导的扩散语言模型，通过结构监督和个性化条件来增强文本质量、多样性和可控性，解决了传统语言模型输出过于通用、结构多样性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成类人文本方面取得了革命性进展，但其输出往往过于通用，结构多样性不足，限制了个性化表达。扩散模型的最新进展为超越自回归范式限制的语言生成提供了新机会。

Method: 提出了语法引导的扩散语言模型，采用级联框架先生成语法指导再进行条件文本生成，并进一步推广到非级联架构以实现结构与内容的更好对齐。通过共享表示机制实现跨用户信息集成，支持忠实风格生成和可泛化的零样本推理。

Result: 在多个任务上的广泛实验表明，该方法在流畅性、多样性和风格保真度方面具有优越性。定性分析突显了其在学习个性化模式方面的可解释性和灵活性。

Conclusion: 通过将语法信息整合到生成过程中，所提出的模型能更好地捕捉风格化句子构建的词汇和结构特征，实现了文本质量、多样性和可控性的显著提升。

Abstract: Large language models have made revolutionary progress in generating
human-like text, yet their outputs often tend to be generic, exhibiting
insufficient structural diversity, which limits personalized expression. Recent
advances in diffusion models have opened new opportunities for improving
language generation beyond the limitations of autoregressive paradigms. In this
work, we propose a syntax-guided diffusion language model that integrates
structural supervision and personalized conditioning to enhance text quality,
diversity, and controllability. We introduce a cascaded framework that
generates syntactic guidance before conditional text generation, and further
generalize it to a novel noncascaded architecture for better alignment between
structure and content. By incorporating syntactic information in the generating
process, the proposed model better captures the lexical and structural
characteristics of stylistic sentence construction. To enable fine-grained
personalization, we develop a shared representation mechanism that facilitates
information integration across users, supporting both faithful stylistic
generation and generalizable zero-shot inference. Extensive experiments on
multiple tasks demonstrate the superiority of our approach in fluency,
diversity, and stylistic fidelity. Further qualitative analyses highlight its
interpretability and flexibility in learning personalized patterns.

</details>


### [47] [Interpreting Language Models Through Concept Descriptions: A Survey](https://arxiv.org/abs/2510.01048)
*Nils Feldhus,Laura Kopf*

Main category: cs.CL

TL;DR: 本文是对神经网络模型组件概念描述研究领域的首次综述，涵盖了生成方法、评估指标和数据集，旨在提高模型透明度。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络决策过程是机制可解释性的核心目标，需要揭示底层机制并识别模型组件（如神经元、注意力头）和抽象表示（如稀疏自编码器提取的特征）的作用。

Method: 综述了使用生成模型为模型组件生成开放词汇自然语言概念描述的关键方法，包括自动化评估和人工评估指标，以及支撑研究的数据集。

Result: 揭示了该领域对更严格因果评估的日益增长需求，当前研究主要关注概念描述生成方法和评估框架。

Conclusion: 通过概述最新进展和识别关键挑战，本综述为未来研究提供了路线图，以增强模型透明度。

Abstract: Understanding the decision-making processes of neural networks is a central
goal of mechanistic interpretability. In the context of Large Language Models
(LLMs), this involves uncovering the underlying mechanisms and identifying the
roles of individual model components such as neurons and attention heads, as
well as model abstractions such as the learned sparse features extracted by
Sparse Autoencoders (SAEs). A rapidly growing line of work tackles this
challenge by using powerful generator models to produce open-vocabulary,
natural language concept descriptions for these components. In this paper, we
provide the first survey of the emerging field of concept descriptions for
model components and abstractions. We chart the key methods for generating
these descriptions, the evolving landscape of automated and human metrics for
evaluating them, and the datasets that underpin this research. Our synthesis
reveals a growing demand for more rigorous, causal evaluation. By outlining the
state of the art and identifying key challenges, this survey provides a roadmap
for future research toward making models more transparent.

</details>


### [48] [Hybrid Dialogue State Tracking for Persian Chatbots: A Language Model-Based Approach](https://arxiv.org/abs/2510.01052)
*Samin Mahdipour Aghabagher,Saeedeh Momtazi*

Main category: cs.CL

TL;DR: 提出了一种混合对话状态跟踪模型，结合规则方法和语言模型，在波斯语多轮对话数据集上显著提升了准确性和连贯性。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的DST方法在开放域和多轮聊天机器人中效率不足，无法为复杂对话提供足够适应性和连贯性，需要更有效的解决方案。

Method: 使用BERT进行槽填充和意图检测，XGBoost进行意图验证，GPT进行DST，在线代理进行实时答案生成的混合模型。

Result: 在波斯语多轮对话数据集上评估，相比现有方法显著提高了准确性和连贯性。

Conclusion: 混合方法能有效提升DST能力，为更定制化、适应性强且类人的对话AI系统铺平道路。

Abstract: Dialogue State Tracking (DST) is an essential element of conversational AI
with the objective of deeply understanding the conversation context and leading
it toward answering user requests. Due to high demands for open-domain and
multi-turn chatbots, the traditional rule-based DST is not efficient enough,
since it cannot provide the required adaptability and coherence for human-like
experiences in complex conversations. This study proposes a hybrid DST model
that utilizes rule-based methods along with language models, including BERT for
slot filling and intent detection, XGBoost for intent validation, GPT for DST,
and online agents for real-time answer generation. This model is uniquely
designed to be evaluated on a comprehensive Persian multi-turn dialogue dataset
and demonstrated significantly improved accuracy and coherence over existing
methods in Persian-based chatbots. The results demonstrate how effectively a
hybrid approach may improve DST capabilities, paving the way for conversational
AI systems that are more customized, adaptable, and human-like.

</details>


### [49] [Research on the Integration of Embodied Intelligence and Reinforcement Learning in Textual Domains](https://arxiv.org/abs/2510.01076)
*Haonan Wang,Junfeng Sun,Mingjia Zhao,Wei Liu*

Main category: cs.CL

TL;DR: 本文提出了一种结合具身智能和强化学习的文本处理集成模型，利用具身智能的感知行动优势和强化学习的决策优化能力，在多种文本处理任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 旨在通过结合具身智能的感知行动优势和强化学习的决策优化能力，提升文本处理的智能化水平。

Method: 提出了一种新颖的集成模型，通过详细的理论解释和实验探索来验证模型的有效性。

Result: 该模型在广泛的文本处理任务中被证明非常有效，验证了其应用潜力。

Conclusion: 具身智能与强化学习的集成模型在文本处理领域具有显著的应用价值和潜力。

Abstract: This article addresses embodied intelligence and reinforcement learning
integration in the field of text processing, aiming to enhance text handling
with more intelligence on the basis of embodied intelligence's perception and
action superiority and reinforcement learning's decision optimization
capability. Through detailed theoretical explanation and experimental
exploration, a novel integration model is introduced. This model has been
demonstrated to be very effective in a wide range oftext processing tasks,
validating its applicative potential

</details>


### [50] [Automatic Speech Recognition (ASR) for African Low-Resource Languages: A Systematic Literature Review](https://arxiv.org/abs/2510.01145)
*Sukairaj Hafiz Imam,Tadesse Destaw Belay,Kedir Yassin Husse,Ibrahim Said Ahmad,Idris Abdulmumin,Hadiza Ali Umar,Muhammad Yahuza Bello,Joyce Nakatumba-Nabende,Seid Muhie Yimam,Shamsuddeen Hassan Muhammad*

Main category: cs.CL

TL;DR: 本文对2020-2025年间非洲低资源语言自动语音识别(ASR)研究进行系统性综述，发现该领域面临数据集稀缺、模型可复现性差、评估指标单一等挑战，但社区驱动倡议和方法创新显示出改进潜力。


<details>
  <summary>Details</summary>
Motivation: 非洲拥有2000多种语言，但低资源语言在ASR研究中代表性严重不足，阻碍了数字包容性发展。

Method: 采用PRISMA 2020程序，检索DBLP、ACM Digital Library等数据库，筛选71篇相关研究，分析74个数据集覆盖111种语言约11,206小时语音数据。

Result: 仅不到15%研究提供可复现材料，数据集许可不明确；自监督和迁移学习有前景但受限于预训练数据不足；主要使用WER指标，缺乏对音调和形态丰富语言的专门评估。

Conclusion: 该领域可持续发展需要利益相关者合作、创建伦理平衡数据集、采用轻量建模技术和积极基准测试。

Abstract: ASR has achieved remarkable global progress, yet African low-resource
languages remain rigorously underrepresented, producing barriers to digital
inclusion across the continent with more than +2000 languages. This systematic
literature review (SLR) explores research on ASR for African languages with a
focus on datasets, models and training methods, evaluation techniques,
challenges, and recommends future directions. We employ the PRISMA 2020
procedures and search DBLP, ACM Digital Library, Google Scholar, Semantic
Scholar, and arXiv for studies published between January 2020 and July 2025. We
include studies related to ASR datasets, models or metrics for African
languages, while excluding non-African, duplicates, and low-quality studies
(score <3/5). We screen 71 out of 2,062 records and we record a total of 74
datasets across 111 languages, encompassing approximately 11,206 hours of
speech. Fewer than 15% of research provided reproducible materials, and dataset
licensing is not clear. Self-supervised and transfer learning techniques are
promising, but are hindered by limited pre-training data, inadequate coverage
of dialects, and the availability of resources. Most of the researchers use
Word Error Rate (WER), with very minimal use of linguistically informed scores
such as Character Error Rate (CER) or Diacritic Error Rate (DER), and thus with
limited application in tonal and morphologically rich languages. The existing
evidence on ASR systems is inconsistent, hindered by issues like dataset
availability, poor annotations, licensing uncertainties, and limited
benchmarking. Nevertheless, the rise of community-driven initiatives and
methodological advancements indicates a pathway for improvement. Sustainable
development for this area will also include stakeholder partnership, creation
of ethically well-balanced datasets, use of lightweight modelling techniques,
and active benchmarking.

</details>


### [51] [mR3: Multilingual Rubric-Agnostic Reward Reasoning Models](https://arxiv.org/abs/2510.01146)
*David Anugraha,Shou-Yi Hung,Zilu Tang,Annie En-Shiun Lee,Derry Tanti Wijaya,Genta Indra Winata*

Main category: cs.CL

TL;DR: mR3是一个多语言奖励推理模型，在72种语言上训练，实现了最广泛的语言覆盖范围，在多项基准测试中超越了更大的模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型评估在英语中表现良好，但在非英语环境中泛化能力不足，需要研究有效的多语言训练策略。

Method: 通过大规模多语言训练，结合目标语言推理数据集，研究数据和课程选择策略来构建高质量的奖励模型。

Result: 在多项多语言奖励模型基准测试中达到最先进性能，超越GPT-OSS-120B等更大模型，同时模型尺寸小9倍。

Conclusion: mR3证明了通过精心设计的数据和训练策略，可以构建高效的多语言奖励模型，为多语言评估提供了可靠解决方案。

Abstract: Evaluation using Large Language Model (LLM) judges has been widely adopted in
English and shown to be effective for automatic evaluation. However, their
performance does not generalize well to non-English settings, and it remains
unclear what constitutes effective multilingual training for such judges. In
this paper, we introduce mR3, a massively multilingual, rubric-agnostic reward
reasoning model trained on 72 languages, achieving the broadest language
coverage in reward modeling to date. We present a comprehensive study of data
and curriculum selection for training to identify effective strategies and data
sources for building high-quality reward models, including the integration of
target-language reasoning datasets. Our approach attains state-of-the-art
performance on multilingual reward model benchmarks, surpassing much larger
models (i.e., GPT-OSS-120B) while being up to 9x smaller, and its effectiveness
is further confirmed through extensive ablation studies. Our models, data, and
code are available as open source at https://github.com/rubricreward/mr3.

</details>


### [52] [Pay-Per-Search Models are Abstention Models](https://arxiv.org/abs/2510.01152)
*Mustafa Omer Gul,Claire Cardie,Tanya Goyal*

Main category: cs.CL

TL;DR: MASH是一个通过强化学习训练LLMs选择性求助的框架，将外部搜索工具使用作为弃权的代理信号，在奖励答案准确性的同时惩罚搜索行为，从而提取LLMs的弃权能力。


<details>
  <summary>Details</summary>
Motivation: LLMs无法可靠识别其参数知识边界，经常对超出边界的问题产生幻觉回答。相比之下，人类能够认识到自身局限性，对这类问题要么寻求外部帮助，要么弃权。

Method: 使用强化学习框架，采用按次搜索付费的奖励机制，在奖励答案准确性的同时惩罚搜索工具的使用，从而训练LLMs选择性求助。

Result: 在三个知识密集型QA数据集上的实验显示，MASH显著提升了选择性求助性能；在多跳数据集上答案准确率提高了7.6%。MASH能够区分可回答/不可回答问题，并选择性生成回答。

Conclusion: MASH训练有效地将搜索工具使用与参数知识对齐，可以成功用于做出弃权决策。与先前方法不同，MASH不需要预先确定知识边界来构建训练数据，其弃权能力是训练选择性求助任务的副产品。

Abstract: LLMs cannot reliably recognize their parametric knowledge boundaries and
often hallucinate answers to outside-of-boundary questions. In contrast, humans
recognize their limitations and can either seek external help for such
questions or abstain. In this paper, we introduce MASH (Modeling Abstention via
Selective Help-seeking), a training framework that readily extracts abstentions
from LLMs. Our key idea is that any external help-seeking by an LLM, i.e.
search tool use, can serve as a proxy for abstention if the external help
(search) is appropriately penalized while simultaneously rewarding answer
accuracy. MASH operationalizes this idea using reinforcement learning with a
pay-per-search reward.
  We run experiments on three knowledge-intensive QA datasets. Our results show
that MASH substantially improves upon the selective help-seeking performance of
prior efficient search approaches; on multi-hop datasets, MASH improves answer
accuracy by 7.6%. Furthermore, MASH demonstrates strong off-the-shelf
abstention -- it can distinguish between unanswerable/answerable questions and
selectively generate responses for answerable questions -- showcasing behavior
analogous to specialized abstention approaches. We emphasize that contrary to
prior abstention methods, MASH does not require pre-determining knowledge
boundaries to construct training data. Instead, MASH's abstentions are a
by-product of training for the auxiliary selective help-seeking task. Overall,
we show that MASH training effectively aligns search tool use with parametric
knowledge, which can be successfully leveraged for making abstention decisions.

</details>


### [53] [Backdoor Attacks Against Speech Language Models](https://arxiv.org/abs/2510.01157)
*Alexandrine Fortier,Thomas Thebaud,Jesús Villalba,Najim Dehak,Patrick Cardinal*

Main category: cs.CL

TL;DR: 本文首次系统研究了针对语音语言模型的音频后门攻击，在四种语音编码器和三个数据集上验证了攻击有效性，并提出了一种基于微调的防御方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型及其多模态扩展日益流行，但级联领域特定编码器的方法使模型继承了所有组件的漏洞，需要研究音频后门攻击的威胁。

Method: 使用四种语音编码器和三个数据集，针对自动语音识别、语音情感识别、性别和年龄预测四个任务进行后门攻击实验，并进行组件级分析。

Result: 攻击成功率高达90.76%到99.41%，成功识别了流水线中最脆弱的阶段。

Conclusion: 音频后门攻击对语音语言模型构成严重威胁，提出的微调防御方法能有效缓解预训练编码器中毒的风险。

Abstract: Large Language Models (LLMs) and their multimodal extensions are becoming
increasingly popular. One common approach to enable multimodality is to cascade
domain-specific encoders with an LLM, making the resulting model inherit
vulnerabilities from all of its components. In this work, we present the first
systematic study of audio backdoor attacks against speech language models. We
demonstrate its effectiveness across four speech encoders and three datasets,
covering four tasks: automatic speech recognition (ASR), speech emotion
recognition, and gender and age prediction. The attack consistently achieves
high success rates, ranging from 90.76% to 99.41%. To better understand how
backdoors propagate, we conduct a component-wise analysis to identify the most
vulnerable stages of the pipeline. Finally, we propose a fine-tuning-based
defense that mitigates the threat of poisoned pretrained encoders.

</details>


### [54] [Social Welfare Function Leaderboard: When LLM Agents Allocate Social Welfare](https://arxiv.org/abs/2510.01164)
*Zhengliang Shi,Ruotian Ma,Jen-tse Huang,Xinbei Ma,Xingyu Chen,Mengru Wang,Qu Yang,Yue Wang,Fanghua Ye,Ziyang Chen,Shanyi Wang,Cixing Li,Wenxuan Wang,Zhaopeng Tu,Xiaolong Li,Zhaochun Ren,Linus*

Main category: cs.CL

TL;DR: 该论文提出了Social Welfare Function (SWF) Benchmark，用于评估LLMs在社会资源分配中的表现，发现大多数LLMs默认采用功利主义取向，优先考虑群体效率而忽视公平性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在影响人类福祉的高风险决策中扮演越来越重要的角色，需要评估这些模型在分配稀缺社会资源时的原则和价值观。

Method: 引入SWF Benchmark动态模拟环境，让LLM作为主权分配者，在异质社区中分配任务，创建集体效率与分配公平之间的持久权衡。

Result: 评估了20个最先进的LLMs，发现：1) 通用对话能力不能预测分配技能；2) 大多数LLMs默认采用功利主义取向；3) 分配策略易受输出长度限制和社会影响框架的干扰。

Conclusion: 当前LLMs作为社会决策者存在风险，需要专门的基准测试和针对性的对齐方法来进行AI治理。

Abstract: Large language models (LLMs) are increasingly entrusted with high-stakes
decisions that affect human welfare. However, the principles and values that
guide these models when distributing scarce societal resources remain largely
unexamined. To address this, we introduce the Social Welfare Function (SWF)
Benchmark, a dynamic simulation environment where an LLM acts as a sovereign
allocator, distributing tasks to a heterogeneous community of recipients. The
benchmark is designed to create a persistent trade-off between maximizing
collective efficiency (measured by Return on Investment) and ensuring
distributive fairness (measured by the Gini coefficient). We evaluate 20
state-of-the-art LLMs and present the first leaderboard for social welfare
allocation. Our findings reveal three key insights: (i) A model's general
conversational ability, as measured by popular leaderboards, is a poor
predictor of its allocation skill. (ii) Most LLMs exhibit a strong default
utilitarian orientation, prioritizing group productivity at the expense of
severe inequality. (iii) Allocation strategies are highly vulnerable, easily
perturbed by output-length constraints and social-influence framing. These
results highlight the risks of deploying current LLMs as societal
decision-makers and underscore the need for specialized benchmarks and targeted
alignment for AI governance.

</details>


### [55] [GRAD: Generative Retrieval-Aligned Demonstration Sampler for Efficient Few-Shot Reasoning](https://arxiv.org/abs/2510.01165)
*Oussama Gabouj,Kamel Charaf,Ivan Zakazov,Nicolas Baldwin,Robert West*

Main category: cs.CL

TL;DR: 提出了GRAD方法，通过训练LLM生成针对特定输入的简洁演示，在预算限制下优于传统RAG方法，并在数学推理和STEM问题上表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法依赖静态数据库，缺乏适应性且可能提供不相关的演示，限制了LLM在资源受限环境下的表现。

Method: 训练LLM模型生成输入特定的简洁演示，通过动态演示提供更好的上下文支持，而非依赖静态数据库。

Result: 在Qwen2.5-14B上，GRAD在数学推理和STEM问题上持续优于基线方法，表现出良好的跨领域泛化能力。

Conclusion: GRAD为资源受限环境下的动态少样本学习范式提供了可扩展的演示生成器，小模型生成的演示能有效指导大模型，降低训练成本。

Abstract: Large Language Models (LLMs) achieve strong performance across diverse tasks,
but their effectiveness often depends on the quality of the provided context.
Retrieval-Augmented Generation (RAG) enriches prompts with external
information, but its reliance on static databases constrains adaptability and
can result in irrelevant demonstrations. In this work, we propose a Generative
Retrieval-Aligned Demonstrator (GRAD), a dynamic demonstration-based approach
where an LLM model is trained to generate input-specific concise
demonstrations. By tailoring demonstrations to each input, our method offers
better contextual support than traditional RAG approaches. We demonstrate the
superiority of GRAD under budget constraints, where we limit both the number of
tokens used per demonstration and the number of tokens used for the final
output. Trained solely on a math dataset, GRAD consistently outperforms strong
baselines on Qwen2.5-14B across mathematical reasoning and advanced STEM
questions, highlighting GRAD's robust generalization to out-of-distribution
(OOD) domains such as physics, chemistry, and computer science. Furthermore, we
show that demonstrations generated by trained smaller models can effectively
guide larger target models, reducing training costs while maintaining
competitive accuracy. Overall, this work introduces a scalable demonstration
generator model presenting the first step toward a dynamic few-shot learning
paradigm in resource-constrained settings. We release the code used for the
project.

</details>


### [56] [Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity](https://arxiv.org/abs/2510.01171)
*Jiayi Zhang,Simon Yu,Derek Chong,Anthony Sicilia,Michael R. Tomz,Christopher D. Manning,Weiyan Shi*

Main category: cs.CL

TL;DR: 本文发现后训练对齐导致LLM多样性下降（模式崩溃）的根本原因是偏好数据中的典型性偏见，并提出了一种无需训练的提示策略——言语化采样，能显著提升生成多样性。


<details>
  <summary>Details</summary>
Motivation: 后训练对齐经常减少LLM的多样性，导致模式崩溃现象。与之前将这种现象归因于算法限制的研究不同，本文识别了一个根本性的、普遍存在的数据层面驱动因素：偏好数据中的典型性偏见。

Method: 提出了言语化采样（Verbalized Sampling），这是一种简单、无需训练的提示策略。该方法提示模型对一组回应进行概率分布的言语化（例如“生成5个关于咖啡的笑话及其相应概率”）。

Result: 综合实验表明，VS在创意写作（诗歌、故事、笑话）、对话模拟、开放式问答和合成数据生成方面显著提升了性能，同时不牺牲事实准确性和安全性。在创意写作中，VS将多样性提高了1.6-2.1倍。

Conclusion: 本文提供了一个关于模式崩溃的新数据中心视角，以及一个实用的推理时补救措施，有助于释放预训练生成模型的多样性。

Abstract: Post-training alignment often reduces LLM diversity, leading to a phenomenon
known as mode collapse. Unlike prior work that attributes this effect to
algorithmic limitations, we identify a fundamental, pervasive data-level
driver: typicality bias in preference data, whereby annotators systematically
favor familiar text as a result of well-established findings in cognitive
psychology. We formalize this bias theoretically, verify it on preference
datasets empirically, and show that it plays a central role in mode collapse.
Motivated by this analysis, we introduce Verbalized Sampling, a simple,
training-free prompting strategy to circumvent mode collapse. VS prompts the
model to verbalize a probability distribution over a set of responses (e.g.,
``Generate 5 jokes about coffee and their corresponding probabilities'').
Comprehensive experiments show that VS significantly improves performance
across creative writing (poems, stories, jokes), dialogue simulation,
open-ended QA, and synthetic data generation, without sacrificing factual
accuracy and safety. For instance, in creative writing, VS increases diversity
by 1.6-2.1x over direct prompting. We further observe an emergent trend that
more capable models benefit more from VS. In sum, our work provides a new
data-centric perspective on mode collapse and a practical inference-time remedy
that helps unlock pre-trained generative diversity.

</details>


### [57] [Energy-Regularized Sequential Model Editing on Hyperspheres](https://arxiv.org/abs/2510.01172)
*Qingyuan Liu,Jia-Chen Gu,Yunzhi Yao,Hong Wang,Nanyun Peng*

Main category: cs.CL

TL;DR: 该论文提出SPHERE方法，通过超球面能量正则化来稳定神经元权重分布，在保持预训练知识的同时实现可靠的连续模型编辑。


<details>
  <summary>Details</summary>
Motivation: 大语言模型需要持续更新以跟上现实世界知识的演进，但连续编辑会导致表示不稳定和灾难性遗忘。研究发现超球面均匀性对模型稳定性至关重要。

Method: 使用超球面能量量化神经元均匀性，提出SPHERE方法：识别预训练权重矩阵主方向的互补稀疏空间，将新知识投影到该空间以减少对主方向的扰动。

Result: 在LLaMA3和Qwen2.5上的实验表明，SPHERE在编辑能力上平均优于最佳基线16.41%，同时最忠实地保持模型通用性能。

Conclusion: SPHERE提供了一种原则性的方法来实现可靠的大规模知识编辑，通过稳定超球面能量来平衡知识更新与保留。

Abstract: Large language models (LLMs) require constant updates to remain aligned with
evolving real-world knowledge. Model editing offers a lightweight alternative
to retraining, but sequential editing often destabilizes representations and
induces catastrophic forgetting. In this work, we seek to better understand and
mitigate performance degradation caused by sequential editing. We hypothesize
that hyperspherical uniformity, a property that maintains uniform distribution
of neuron weights on a hypersphere, helps the model remain stable, retain prior
knowledge, while still accommodate new updates. We use Hyperspherical Energy
(HE) to quantify neuron uniformity during editing, and examine its correlation
with editing performance. Empirical studies across widely used editing methods
reveals a strong correlation between HE dynamics and editing performance, with
editing failures consistently coinciding with high HE fluctuations. We further
theoretically prove that HE dynamics impose a lower bound on the degradation of
pretrained knowledge, highlighting why HE stability is crucial for knowledge
retention. Motivated by these insights, we propose SPHERE (Sparse Projection
for Hyperspherical Energy-Regularized Editing), an HE-driven regularization
strategy that stabilizes neuron weight distributions, ultimately preserving
prior knowledge while enabling reliable sequential updates. Specifically,
SPHERE identifies a sparse space complementary to the principal hyperspherical
directions of the pretrained weight matrices and projects new knowledge onto
it, attenuating perturbations on the principal directions. Extensive
experiments on LLaMA3 (8B) and Qwen2.5 (7B) show that SPHERE outperforms the
best baseline in editing capability by an average of 16.41%, while most
faithfully preserving general model performance, thereby offering a principled
path toward reliable large-scale knowledge editing.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [58] [Learning to Lead Themselves: Agentic AI in MAS using MARL](https://arxiv.org/abs/2510.00022)
*Ansh Kamthan*

Main category: cs.AI

TL;DR: 本文研究了自主智能体如何通过多智能体强化学习改进任务分配和协调，重点关注无人机配送和仓库自动化场景。


<details>
  <summary>Details</summary>
Motivation: 随着自主系统从原型转向实际部署，多智能体去中心化协同决策能力成为核心需求。

Method: 采用合作式多智能体强化学习框架，实现了轻量级多智能体近端策略优化(IPPO)方法，使用集中训练、分散执行的范式，在PettingZoo环境中进行实验。

Result: 多个同质无人机或智能体能够在无显式通信的情况下自组织覆盖不同目标。

Conclusion: 自主智能体通过独立、自适应和主动行动的能力，可以有效提升多智能体系统中的任务分配和协调性能。

Abstract: As autonomous systems move from prototypes to real deployments, the ability
of multiple agents to make decentralized, cooperative decisions becomes a core
requirement. This paper examines how agentic artificial intelligence, agents
that act independently, adaptively and proactively can improve task allocation
and coordination in multi-agent systems, with primary emphasis on drone
delivery and secondary relevance to warehouse automation. We formulate the
problem in a cooperative multi-agent reinforcement learning setting and
implement a lightweight multi-agent Proximal Policy Optimization, called IPPO,
approach in PyTorch under a centralized-training, decentralized-execution
paradigm. Experiments are conducted in PettingZoo environment, where multiple
homogeneous drones or agents must self-organize to cover distinct targets
without explicit communication.

</details>


### [59] [ToolBrain: A Flexible Reinforcement Learning Framework for Agentic Tools](https://arxiv.org/abs/2510.00023)
*Quy Minh Le,Minh Sao Khue Luu,Khanh-Tung Tran,Duc-Hai Nguyen,Hoang-Quoc-Viet Pham,Quan Le,Hoang Thanh Lam,Hoang D. Nguyen*

Main category: cs.AI

TL;DR: ToolBrain是一个轻量级框架，通过灵活的强化学习训练智能代理使用工具，解决了手动设计奖励、训练数据有限和多工具选择困难等问题。


<details>
  <summary>Details</summary>
Motivation: 当前训练AI代理使用工具面临手动设计奖励、训练数据有限、多工具选择困难等挑战，导致适应缓慢、计算资源浪费和性能不佳。

Method: ToolBrain框架支持多种训练策略，包括GRPO和DPO等RL算法以及监督学习，提供自定义奖励函数和自动LLM评判系统，具备知识蒸馏、自动任务生成、工具检索等功能。

Result: 在代码代理执行邮件搜索任务的应用中，ToolBrain实现了工具使用技能的快速针对性改进（提升达30.0%），同时保持代码库简洁可扩展。

Conclusion: ToolBrain为研究人员和从业者提供了一个用户友好的框架，能够有效训练LLM代理在特定领域中使用工具，显著提升工具使用能力。

Abstract: Effective tool use is essential for agentic AI, yet training agents to
utilize tools remains challenging due to manually designed rewards, limited
training data, and poor multi-tool selection, resulting in slow adaptation,
wasted computational resources, and suboptimal performance. We introduce
ToolBrain, a lightweight and user-friendly framework for coaching tool use in
agentic models with flexible reinforcement learning (RL), easing the barriers
for researchers and practitioners to adapt LLM-based agents to specific
domains. It supports a wide range of training strategies, including RL
algorithms such as GRPO and DPO, as well as supervised learning. ToolBrain
enables custom reward callables directly on an agent's execution traces or
simply utilizes an automated LLM-as-a-judge system for reward generation. It is
packed with useful capabilities, including knowledge distillation from large to
small models for efficient development, automatic task generation from tool
descriptions, seamless tool retrieval, efficient fine-tuning pipelines with
QLoRA through Unsloth, and quantized inference via bitsandbytes. We demonstrate
ToolBrain through diverse use cases, such as training a CodeAct agent to
autonomously execute email search tasks, showing fast, targeted improvements
(up to 30.0%) in tool-use skills while keeping the codebase simple and
extensible in Agentic AI. Our framework is publicly available at
https://toolbrain.org.

</details>


### [60] [ARS: Adaptive Reasoning Suppression for Efficient Large Reasoning Language Models](https://arxiv.org/abs/2510.00071)
*Dongqi Zheng*

Main category: cs.AI

TL;DR: 提出自适应推理抑制（ARS）方法，通过动态抑制冗余推理步骤来提升大型推理语言模型的效率，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理语言模型在复杂推理任务中表现出色，但存在计算效率低下的问题，现有方法难以平衡推理质量与推理成本降低。

Method: 采用训练免费的自适应推理抑制方法，通过多检查点确定性估计机制和渐进抑制阈值来动态抑制冗余推理步骤。

Result: 在数学推理基准测试中，ARS实现了最高53%的token减少、46.1%的延迟降低和57.9%的能耗降低，同时保持或提高了准确性。

Conclusion: ARS方法能够有效提升大型推理语言模型的推理效率，在保持准确性的同时显著降低计算成本。

Abstract: Large Reasoning Language Models (LRLMs or LRMs) demonstrate remarkable
capabilities in complex reasoning tasks, but suffer from significant
computational inefficiencies due to overthinking phenomena. Existing efficient
reasoning methods face the challenge of balancing reasoning quality with
inference cost reduction. We propose \textbf{Adaptive Reasoning Suppression
(ARS)}, a novel training-free approach that dynamically suppresses redundant
reasoning steps while preserving accuracy through adaptive certainty
monitoring. ARS introduces a multi-checkpoint certainty estimation mechanism
with progressive suppression thresholds, achieving superior efficiency compared
to static suppression methods. Our extensive evaluation across mathematical
reasoning benchmarks using multiple model architectures demonstrates that ARS
achieves up to 53%, 46.1%, and 57.9% in token, latency and energy reduction,
while maintaining or improving accuracy.

</details>


### [61] [NeurIPS should lead scientific consensus on AI policy](https://arxiv.org/abs/2510.00075)
*Rishi Bommasani*

Main category: cs.AI

TL;DR: 本文主张NeurIPS会议应积极促进AI政策领域的科学共识形成，借鉴IPCC在气候政策方面的经验，通过试点项目来填补当前共识形成机制的空白。


<details>
  <summary>Details</summary>
Motivation: 当前AI政策制定缺乏科学共识形成机制，而NeurIPS作为AI领域的领导者，具备推动这一进程的独特优势。作者认为高质量AI政策需要基于严谨证据和科学共识。

Method: 建议NeurIPS通过试点项目主动催化AI政策科学共识，借鉴IPCC在气候政策共识形成方面的经验教训，并反驳了AI研究者分歧过大无法达成共识等反对观点。

Result: 识别了当前AI政策领域在共识形成机制上的完全空白，论证了NeurIPS是最适合填补这一空白的平台，并提出了具体的实施建议。

Conclusion: NeurIPS应该在AI政策领域发挥领导作用，通过促进科学共识来创造更高质量的AI政策，这符合其作为AI领域前沿会议的地位和责任。

Abstract: Designing wise AI policy is a grand challenge for society. To design such
policy, policymakers should place a premium on rigorous evidence and scientific
consensus. While several mechanisms exist for evidence generation, and nascent
mechanisms tackle evidence synthesis, we identify a complete void on consensus
formation. In this position paper, we argue NeurIPS should actively catalyze
scientific consensus on AI policy. Beyond identifying the current deficit in
consensus formation mechanisms, we argue that NeurIPS is the best option due
its strengths and the paucity of compelling alternatives. To make progress, we
recommend initial pilots for NeurIPS by distilling lessons from the IPCC's
leadership to build scientific consensus on climate policy. We dispel
predictable counters that AI researchers disagree too much to achieve consensus
and that policy engagement is not the business of NeurIPS. NeurIPS leads AI on
many fronts, and it should champion scientific consensus to create higher
quality AI policy.

</details>


### [62] [Towards a Framework for Supporting the Ethical and Regulatory Certification of AI Systems](https://arxiv.org/abs/2510.00084)
*Fabian Kovac,Sebastian Neumaier,Timea Pahi,Torsten Priebe,Rafael Rodrigues,Dimitrios Christodoulou,Maxime Cordy,Sylvain Kubler,Ali Kordia,Georgios Pitsiladis,John Soldatos,Petros Zervoudakis*

Main category: cs.AI

TL;DR: CERTAIN项目开发了一个综合框架，将监管合规、道德标准和透明度整合到AI系统中，通过语义MLOps、本体驱动的数据谱系跟踪和RegOps工作流来实现。


<details>
  <summary>Details</summary>
Motivation: AI在欧洲社会和经济中日益重要，但其扩散带来了伦理、法律和监管方面的关键挑战，需要解决这些挑战以促进负责任的AI创新。

Method: 开发语义MLOps进行结构化AI生命周期管理，本体驱动的数据谱系跟踪确保可追溯性和问责制，以及RegOps工作流来操作化合规要求。

Result: 通过在不同试点中实施和验证解决方案，CERTAIN项目推进了监管合规，并促进了符合欧洲标准的负责任AI创新。

Conclusion: CERTAIN项目通过其综合框架成功解决了AI伦理、法律和监管挑战，为欧洲AI发展提供了重要支持。

Abstract: Artificial Intelligence has rapidly become a cornerstone technology,
significantly influencing Europe's societal and economic landscapes. However,
the proliferation of AI also raises critical ethical, legal, and regulatory
challenges. The CERTAIN (Certification for Ethical and Regulatory Transparency
in Artificial Intelligence) project addresses these issues by developing a
comprehensive framework that integrates regulatory compliance, ethical
standards, and transparency into AI systems. In this position paper, we outline
the methodological steps for building the core components of this framework.
Specifically, we present: (i) semantic Machine Learning Operations (MLOps) for
structured AI lifecycle management, (ii) ontology-driven data lineage tracking
to ensure traceability and accountability, and (iii) regulatory operations
(RegOps) workflows to operationalize compliance requirements. By implementing
and validating its solutions across diverse pilots, CERTAIN aims to advance
regulatory compliance and to promote responsible AI innovation aligned with
European standards.

</details>


### [63] [Judging by Appearances? Auditing and Intervening Vision-Language Models for Bail Prediction](https://arxiv.org/abs/2510.00088)
*Sagnik Basu,Shubham Prakash,Ashish Maruti Barge,Siddharth D Jaiswal,Abhisek Dash,Saptarshi Ghosh,Animesh Mukherjee*

Main category: cs.AI

TL;DR: 本文审计了视觉语言模型在保释决策预测中的表现，发现模型存在严重偏见，会错误地高置信度拒绝应获保释的个体。通过RAG管道引入法律先例和微调干预，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型的兴起，法律判决预测系统开始利用罪犯图像和文本案件报告。这种应用可能带来意外后果或被恶意使用，因此需要评估其效率和公平性。

Method: 首先审计独立VLMs在保释预测任务中的表现，然后设计干预算法：通过RAG管道引入法律先例，并使用创新方案微调VLMs。

Result: 独立VLMs在多个交叉群体中表现不佳，会错误地高置信度拒绝应获保释的个体。干预措施显著提高了保释预测性能。

Conclusion: 这项工作为未来在真实世界法律判决预测部署VLMs之前设计更智能的干预措施铺平了道路。

Abstract: Large language models (LLMs) have been extensively used for legal judgment
prediction tasks based on case reports and crime history. However, with a surge
in the availability of large vision language models (VLMs), legal judgment
prediction systems can now be made to leverage the images of the criminals in
addition to the textual case reports/crime history. Applications built in this
way could lead to inadvertent consequences and be used with malicious intent.
In this work, we run an audit to investigate the efficiency of standalone VLMs
in the bail decision prediction task. We observe that the performance is poor
across multiple intersectional groups and models \textit{wrongly deny bail to
deserving individuals with very high confidence}. We design different
intervention algorithms by first including legal precedents through a RAG
pipeline and then fine-tuning the VLMs using innovative schemes. We demonstrate
that these interventions substantially improve the performance of bail
prediction. Our work paves the way for the design of smarter interventions on
VLMs in the future, before they can be deployed for real-world legal judgment
prediction.

</details>


### [64] [AuditAgent: Expert-Guided Multi-Agent Reasoning for Cross-Document Fraudulent Evidence Discovery](https://arxiv.org/abs/2510.00156)
*Songran Bai,Bingzhe Wu,Yiwei Zhang,Chengke Wu,Xiaolong Zheng,Yaze Yuan,Ke Wu,Jianqiang Li*

Main category: cs.AI

TL;DR: 提出了一个名为AuditAgent的多智能体推理框架，用于在金融欺诈案件中进行细粒度证据链定位，通过整合审计领域专业知识显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的金融欺诈检测面临证据隐蔽且分散在多年财务报告中的挑战，需要更有效的自动化方法来识别跨报告证据。

Method: 构建专家标注数据集，整合主体级风险先验、混合检索策略和专用智能体模块，实现跨报告证据的高效识别和聚合。

Result: 在召回率和可解释性方面显著优于通用智能体范式，为自动化、透明的金融取证建立了新基准。

Conclusion: 领域特定推理和数据集构建对于推进实际监管应用中的稳健金融欺诈检测具有重要价值。

Abstract: Financial fraud detection in real-world scenarios presents significant
challenges due to the subtlety and dispersion of evidence across complex,
multi-year financial disclosures. In this work, we introduce a novel
multi-agent reasoning framework AuditAgent, enhanced with auditing domain
expertise, for fine-grained evidence chain localization in financial fraud
cases. Leveraging an expert-annotated dataset constructed from enforcement
documents and financial reports released by the China Securities Regulatory
Commission, our approach integrates subject-level risk priors, a hybrid
retrieval strategy, and specialized agent modules to efficiently identify and
aggregate cross-report evidence. Extensive experiments demonstrate that our
method substantially outperforms General-Purpose Agent paradigm in both recall
and interpretability, establishing a new benchmark for automated, transparent
financial forensics. Our results highlight the value of domain-specific
reasoning and dataset construction for advancing robust financial fraud
detection in practical, real-world regulatory applications.

</details>


### [65] [Drones that Think on their Feet: Sudden Landing Decisions with Embodied AI](https://arxiv.org/abs/2510.00167)
*Diego Ortiz Barbosa,Mohit Agrawal,Yash Malegaonkar,Luis Burbano,Axel Andersson,György Dán,Henrik Sandberg,Alvaro A. Cardenas*

Main category: cs.AI

TL;DR: 该论文提出使用具身AI和大视觉语言模型来实现无人机在突发情况下的自适应决策和恢复能力，解决了传统手工编码恢复规则无法覆盖现实世界各种意外情况的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖安全工程师手工编码大量恢复规则，但无法预测现实世界中各种意外情况，导致规则集不完整。需要一种能够实时评估上下文并生成适当动作的自适应决策系统。

Method: 利用具身AI和大视觉语言模型，在Unreal Engine模拟的城市场景中，让无人机动态解释周围环境并决定紧急机动以实现安全着陆。

Result: 研究结果表明，具身AI使得以前无法手工设计的一类自适应恢复和决策管道成为可能，提升了自主空中系统的韧性和安全性。

Conclusion: 具身AI技术为自主无人机系统提供了一种新的自适应恢复和决策能力，能够应对突发事件，显著提高了系统的安全性和适应性。

Abstract: Autonomous drones must often respond to sudden events, such as alarms,
faults, or unexpected changes in their environment, that require immediate and
adaptive decision-making. Traditional approaches rely on safety engineers
hand-coding large sets of recovery rules, but this strategy cannot anticipate
the vast range of real-world contingencies and quickly becomes incomplete.
Recent advances in embodied AI, powered by large visual language models,
provide commonsense reasoning to assess context and generate appropriate
actions in real time. We demonstrate this capability in a simulated urban
benchmark in the Unreal Engine, where drones dynamically interpret their
surroundings and decide on sudden maneuvers for safe landings. Our results show
that embodied AI makes possible a new class of adaptive recovery and
decision-making pipelines that were previously infeasible to design by hand,
advancing resilience and safety in autonomous aerial systems.

</details>


### [66] [Object-Centric Case-Based Reasoning via Argumentation](https://arxiv.org/abs/2510.00185)
*Gabriel de Olim Gaul,Adam Gould,Avinash Kori,Francesca Toni*

Main category: cs.AI

TL;DR: SAA-CBR是一种新颖的神经符号图像分类管道，结合了基于神经网络的Slot Attention对象中心学习和基于符号推理的抽象论证案例推理。


<details>
  <summary>Details</summary>
Motivation: 旨在整合神经网络的表示学习能力和符号推理的可解释性，为图像分类提供既有效又可解释的解决方案。

Method: 使用Slot Attention进行对象中心学习，通过AA-CBR进行符号推理，包括特征组合策略、案例库简化、基于计数的偏序关系、One-Vs-Rest多分类策略以及支持性AA-CBR变体。

Result: 在CLEVR-Hans数据集上表现出色，与基线模型相比具有竞争力。

Conclusion: SAA-CBR是一个有效的图像分类器，成功融合了神经和符号方法，在保持性能的同时增强了可解释性。

Abstract: We introduce Slot Attention Argumentation for Case-Based Reasoning (SAA-CBR),
a novel neuro-symbolic pipeline for image classification that integrates
object-centric learning via a neural Slot Attention (SA) component with
symbolic reasoning conducted by Abstract Argumentation for Case-Based Reasoning
(AA-CBR). We explore novel integrations of AA-CBR with the neural component,
including feature combination strategies, casebase reduction via representative
samples, novel count-based partial orders, a One-Vs-Rest strategy for extending
AA-CBR to multi-class classification, and an application of Supported AA-CBR, a
bipolar variant of AA-CBR. We demonstrate that SAA-CBR is an effective
classifier on the CLEVR-Hans datasets, showing competitive performance against
baseline models.

</details>


### [67] [Thinkquel: A Model Dedicated to Text-to-dbt Using Synthetic Data and a Span-Aware Objective](https://arxiv.org/abs/2510.00186)
*Anni Li,Aria Attar,Paul Dong*

Main category: cs.AI

TL;DR: Thinkquel是一个经过微调的模型，用于生成可靠、可移植且经过执行验证的数据库查询。它通过TS-SQL合成数据管道和TS-GRPO强化学习目标，解决了自然语言到SQL转换中的模式链接和SQL方言问题。


<details>
  <summary>Details</summary>
Motivation: 将自然语言请求转换为可靠的生产级数据转换具有挑战性：正确性依赖于精确的模式链接和数据仓库特定的SQL方言，而训练期间最强的监督（执行成功和结果匹配）仅在序列级别提供。

Method: Thinkquel集成了TS-SQL合成数据管道，利用dbt作为可移植的中间表示，以及Token-Sequence GRPO（TS-GRPO）强化学习目标，专门设计用于在微调LLM时弥合令牌级训练信号和序列级执行奖励之间的差距。

Result: 在500个示例的TS-SQL测试集上，Thinkquel（32B）达到93.2%的执行成功率和61.8%的精确结果匹配率，相比基础模型分别提高了67.2%和44.4%。在Spider（14B）实验中，TS-GRPO提高了训练稳定性并加速了执行匹配奖励的收敛。

Conclusion: Thinkquel通过创新的合成数据管道和强化学习目标，显著提高了自然语言到SQL转换的可靠性和可移植性，为生产环境中的数据库查询生成提供了有效的解决方案。

Abstract: Transforming natural-language requests into reliable, production-ready data
transformations remains challenging: correctness depends on precise schema
linking and warehouse-specific SQL dialects, while the strongest supervision
available during training--execution success and result matching--are provided
only at the sequence level. At the same time, assembling large,
execution-validated corpora is costly, and token-level objectives misalign with
these global signals, yielding unstable optimization and limited portability.
We introduce Thinkquel, a fine-tuned model for producing robust, portable, and
execution-validated database queries. Methodologies in Thinkquel integrates a
novel synthetic data pipeline, TS-SQL, that leverages dbt as a portable
intermediate representation with a span-aware reinforcement learning objective,
and Token-Sequence GRPO (TS-GRPO), specifically designed to bridge the gap
between token-level training signals and sequence-level execution rewards when
finetuning LLMs. On the 500-example TS-SQL test set, Thinkquel (32B) reaches
93.2\% execution success and 61.8\% exact-result match with a two-stage SFT
curriculum, improving over the base model by 67.2\% (exec.) and 44.4\% (match).
In Spider (14B) experiments, TS-GRPO increases training stability and speeds
convergence of the execution-match reward relative to GRPO and GSPO.

</details>


### [68] [DualTune: Decoupled Fine-Tuning for On-Device Agentic Systems](https://arxiv.org/abs/2510.00229)
*Rohan Kadekodi,Zhan Jin,Keisuke Kamahori,Yile Gu,Sean Khatiri,Noah H. Bayindirli,Sergey Gorbunov,Baris Kasikci*

Main category: cs.AI

TL;DR: 提出了一种解耦微调方法，将工具调用任务分解为工具选择和参数生成两个子任务，通过专用LoRA适配器分别优化，显著提升了本地LLM在工具调用场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 本地LLM在工具调用场景中表现不佳，难以从大型工具集中准确选择工具并为复杂参数结构生成正确参数，需要隐私保护且成本效益高的本地推理解决方案。

Method: 采用解耦微调方法，使用LoRA微调创建专用适配器，分别处理工具选择和工具特定参数生成，通过分离损失掩码优化两个子任务。DualTune推理框架动态加载相应LoRA适配器，并实现分层编排以限制工具选择数量。

Result: 在MCP-Bench基准测试中，使用解耦微调训练的Qwen-2.5-7B模型将基础模型的工具调用准确率提高了46%，在所有情况下都优于相似大小的其他本地推理、非推理和微调模型，在大多数情况下也优于2倍大小的模型。

Conclusion: 解耦微调和DualTune框架有效解决了本地LLM在工具调用中的性能瓶颈，为设备端代理编排提供了高效解决方案。

Abstract: The deployment of Large Language Models (LLMs) as agentic orchestrators has
revolutionized task automation, but the need for privacy-preserving,
cost-effective solutions demands on-device inference capabilities. However,
local LLMs consistently underperform compared to frontier models in tool
calling scenarios, struggling with both tool selection from large tool sets and
accurate argument generation for complex parameter structures. We introduce a
methodology that disaggregates a tool-calling task into two distinct subtasks:
tool selection and argument generation. We propose "decoupled fine-tuning", a
novel post-training approach that employs LoRA fine-tuning to create dedicated
LoRA adapters for tool selection and tool-specific argument generation using
separate loss masking for each of the subtasks. Furthermore, we present
DualTune, an inference framework that leverages the LoRA adapters created using
decoupled fine-tuning to perform efficient agent orchestration with the help of
local models on end-user devices. DualTune decomposes the tool-call generation
step into tool selection and argument generation, and dynamically loads the
corresponding LoRA adapters to generate tool calls. Additionally, DualTune
implements hierarchical orchestration to restrict the number of tools required
for tool selection. Our experiments on the MCP-Bench benchmark demonstrate that
the Qwen-2.5-7B model trained using decoupled fine-tuning improves the tool
calling accuracy of the base model by 46%, and outperforms other local
reasoning, non-reasoning and fine-tuned models of similar size in all cases,
and models that are 2x larger, in most cases.

</details>


### [69] [MAGIC-MASK: Multi-Agent Guided Inter-Agent Collaboration with Mask-Based Explainability for Reinforcement Learning](https://arxiv.org/abs/2510.00274)
*Maisha Maliha,Dean Hougen*

Main category: cs.AI

TL;DR: 提出MAGIC-MASK框架，将基于扰动的可解释性方法扩展到多智能体强化学习，通过智能体间协作共享掩码状态信息和经验，提高关键状态发现效率和解释保真度。


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习智能体决策过程的可解释性问题，特别是在安全关键和多智能体环境中。现有方法如StateMask存在计算成本高、探索覆盖不足、缺乏多智能体适应性等局限。

Method: 集成近端策略优化、自适应ε-贪婪探索和轻量级智能体间协作，通过轨迹扰动、奖励保真度分析和KL散度正则化的统一数学形式化，实现基于概率建模和多智能体马尔可夫决策过程的局部可解释解释。

Result: 在单智能体和多智能体基准测试（包括多智能体高速公路驾驶环境和Google Research Football）上验证，MAGIC-MASK在保真度、学习效率和策略鲁棒性方面持续优于最先进基线方法。

Conclusion: MAGIC-MASK成功将可解释性从单智能体系统推广到多智能体系统，提供可解释且可转移的解释，同时实现更快、更鲁棒的学习。

Abstract: Understanding the decision-making process of Deep Reinforcement Learning
agents remains a key challenge for deploying these systems in safety-critical
and multi-agent environments. While prior explainability methods like
StateMask, have advanced the identification of critical states, they remain
limited by computational cost, exploration coverage, and lack of adaptation to
multi-agent settings. To overcome these limitations, we propose a
mathematically grounded framework, MAGIC-MASK (Multi-Agent Guided Inter-agent
Collaboration with Mask-Based Explainability for Reinforcement Learning), that
extends perturbation-based explanation to Multi-Agent Reinforcement Learning.
Our method integrates Proximal Policy Optimization, adaptive epsilon-greedy
exploration, and lightweight inter-agent collaboration to share masked state
information and peer experience. This collaboration enables each agent to
perform saliency-guided masking and share reward-based insights with peers,
reducing the time required for critical state discovery, improving explanation
fidelity, and leading to faster and more robust learning. The core novelty of
our approach lies in generalizing explainability from single-agent to
multi-agent systems through a unified mathematical formalism built on
trajectory perturbation, reward fidelity analysis, and Kullback-Leibler
divergence regularization. This framework yields localized, interpretable
explanations grounded in probabilistic modeling and multi-agent Markov decision
processes. We validate our framework on both single-agent and multi-agent
benchmarks, including a multi-agent highway driving environment and Google
Research Football, demonstrating that MAGIC-MASK consistently outperforms
state-of-the-art baselines in fidelity, learning efficiency, and policy
robustness while offering interpretable and transferable explanations.

</details>


### [70] [ICL Optimized Fragility](https://arxiv.org/abs/2510.00300)
*Serena Gomez Wannaz*

Main category: cs.AI

TL;DR: ICL引导虽然能提升特定任务性能，但会损害跨领域推理能力，产生"优化脆弱性"现象。ICL模型在简单知识任务上表现优异(91%-99%)，但在复杂推理问题上性能下降(10-43%)，而复杂数学推理不受影响。


<details>
  <summary>Details</summary>
Motivation: 探索ICL引导对跨领域认知能力的影响，特别是推理能力在不同知识领域中的表现。

Method: 使用GPT-OSS:20b模型的六个变体（一个基准模型和五个ICL配置），进行840项测试，涵盖常识问题、逻辑谜题和数学奥赛题，并进行ANOVA统计分析。

Result: ICL变体表现出显著的行为改变(p<0.001)，在常识任务上达到91%-99%准确率，但在逻辑谜题上准确率降至10-43%（基准模型为43%），数学奥赛题无显著差异(p=0.2173)。

Conclusion: ICL引导在效率和推理灵活性之间产生系统性权衡，对LLM部署和AI安全具有重要影响。

Abstract: ICL guides are known to improve task-specific performance, but their impact
on cross-domain cognitive abilities remains unexplored. This study examines how
ICL guides affect reasoning across different knowledge domains using six
variants of the GPT-OSS:20b model: one baseline model and five ICL
configurations (simple, chain-of-thought, random, appended text, and symbolic
language). The models were subjected to 840 tests spanning general knowledge
questions, logic riddles, and a mathematical olympiad problem. Statistical
analysis (ANOVA) revealed significant behavioral modifications (p less than
0.001) across ICL variants, demonstrating a phenomenon termed "optimized
fragility." ICL models achieved 91%-99% accuracy on general knowledge tasks
while showing degraded performance on complex reasoning problems, with accuracy
dropping to 10-43% on riddles compared to 43% for the baseline model. Notably,
no significant differences emerged on the olympiad problem (p=0.2173),
suggesting that complex mathematical reasoning remains unaffected by ICL
optimization. These findings indicate that ICL guides create systematic
trade-offs between efficiency and reasoning flexibility, with important
implications for LLM deployment and AI safety.

</details>


### [71] [BiasBusters: Uncovering and Mitigating Tool Selection Bias in Large Language Models](https://arxiv.org/abs/2510.00307)
*Thierry Blankenstein,Jialin Yu,Zixuan Li,Vassilis Plachouras,Sunando Sengupta,Philip Torr,Yarin Gal,Alasdair Paren,Adel Bibi*

Main category: cs.AI

TL;DR: 该论文研究了LLM代理在工具选择中的偏见问题，提出了评估工具选择偏见的基准，发现模型存在对特定提供商的偏好或对列表中较早工具的偏向，并提出了轻量级缓解方法。


<details>
  <summary>Details</summary>
Motivation: LLM代理依赖外部工具市场，但工具选择如果存在系统性偏见，会降低用户体验并扭曲竞争，需要研究工具选择偏见问题。

Method: 创建包含多个功能等效工具的基准测试集，测试7个模型，进行控制实验分析工具特征、元数据和预训练暴露的影响，提出基于过滤和均匀采样的缓解方法。

Result: 发现语义对齐是选择的最强预测因素，扰动描述会显著改变选择，重复预训练暴露会放大偏见，提出的缓解方法能减少偏见同时保持良好任务覆盖。

Conclusion: 工具选择偏见是工具增强LLM公平部署的关键障碍，需要关注和解决。

Abstract: Agents backed by large language models (LLMs) often rely on external tools
drawn from marketplaces where multiple providers offer functionally equivalent
options. This raises a critical point concerning fairness: if selection is
systematically biased, it can degrade user experience and distort competition
by privileging some providers over others. We introduce a benchmark of diverse
tool categories, each containing multiple functionally equivalent tools, to
evaluate tool-selection bias. Using this benchmark, we test seven models and
show that unfairness exists with models either fixating on a single provider or
disproportionately preferring earlier-listed tools in context. To investigate
the origins of this bias, we conduct controlled experiments examining tool
features, metadata (name, description, parameters), and pre-training exposure.
We find that: (1) semantic alignment between queries and metadata is the
strongest predictor of choice; (2) perturbing descriptions significantly shifts
selections; and (3) repeated pre-training exposure to a single endpoint
amplifies bias. Finally, we propose a lightweight mitigation that first filters
the candidate tools to a relevant subset and then samples uniformly, reducing
bias while preserving good task coverage. Our findings highlight tool-selection
bias as a key obstacle for the fair deployment of tool-augmented LLMs.

</details>


### [72] [When Hallucination Costs Millions: Benchmarking AI Agents in High-Stakes Adversarial Financial Markets](https://arxiv.org/abs/2510.00332)
*Zeshi Dai,Zimo Peng,Zerui Cheng,Ryan Yihe Li*

Main category: cs.AI

TL;DR: CAIA基准测试揭示了AI模型在对抗性高风险环境中的严重能力缺陷，特别是在加密货币市场等需要识别错误信息、做出不可逆决策的场景中，即使最先进的模型表现也远低于人类水平。


<details>
  <summary>Details</summary>
Motivation: 现有AI评估基准主要在受控环境中衡量任务完成度，而现实世界部署需要模型具备抵抗主动欺骗的韧性。加密货币市场2024年因漏洞损失300亿美元，是测试AI在对抗性环境中能力的理想场景。

Method: 使用加密货币市场作为测试平台，在178个时间锚定任务上评估17个模型，要求代理区分真相与操纵、导航碎片化信息环境，并在对抗压力下做出不可逆金融决策。

Result: 即使前沿模型在没有工具辅助时准确率仅为28%，工具增强后提升至67.4%，但仍低于80%的人类基准。模型存在系统性工具选择灾难，偏好不可靠的网页搜索而非权威数据源。

Conclusion: 当前模型尽管在推理得分上表现优异，但在需要抵御主动对抗的环境中仍存在根本性不足，对抗性鲁棒性是可信AI自主性的必要条件。

Abstract: We present CAIA, a benchmark exposing a critical blind spot in AI evaluation:
the inability of state-of-the-art models to operate in adversarial, high-stakes
environments where misinformation is weaponized and errors are irreversible.
While existing benchmarks measure task completion in controlled settings,
real-world deployment demands resilience against active deception. Using crypto
markets as a testbed where $30 billion was lost to exploits in 2024, we
evaluate 17 models on 178 time-anchored tasks requiring agents to distinguish
truth from manipulation, navigate fragmented information landscapes, and make
irreversible financial decisions under adversarial pressure.
  Our results reveal a fundamental capability gap: without tools, even frontier
models achieve only 28% accuracy on tasks junior analysts routinely handle.
Tool augmentation improves performance but plateaus at 67.4% versus 80% human
baseline, despite unlimited access to professional resources. Most critically,
we uncover a systematic tool selection catastrophe: models preferentially
choose unreliable web search over authoritative data, falling for SEO-optimized
misinformation and social media manipulation. This behavior persists even when
correct answers are directly accessible through specialized tools, suggesting
foundational limitations rather than knowledge gaps. We also find that Pass@k
metrics mask dangerous trial-and-error behavior for autonomous deployment.
  The implications extend beyond crypto to any domain with active adversaries,
e.g. cybersecurity, content moderation, etc. We release CAIA with contamination
controls and continuous updates, establishing adversarial robustness as a
necessary condition for trustworthy AI autonomy. The benchmark reveals that
current models, despite impressive reasoning scores, remain fundamentally
unprepared for environments where intelligence must survive active opposition.

</details>


### [73] [Hierarchical Reasoning Model: A Critical Supplementary Material](https://arxiv.org/abs/2510.00355)
*Renee Ge,Qianli Liao,Tomaso Poggio*

Main category: cs.AI

TL;DR: 对分层推理模型进行批判性回顾，提出改进变体，在Sudoku-Extreme和Maze-Hard任务上取得显著性能提升


<details>
  <summary>Details</summary>
Motivation: Transformer在逻辑推理方面存在困难，需要探索更创新的使用方法，如潜在空间和循环推理

Method: 对分层推理模型进行关键设计选择分析，提出改进变体

Result: 在Sudoku-Extreme和Maze-Hard任务上比之前报告的性能显著更好

Conclusion: 研究结果提出了令人惊讶的观察和进一步研究的有趣方向

Abstract: Transformers have demonstrated remarkable performance in natural language
processing and related domains, as they largely focus on sequential,
autoregressive next-token prediction tasks. Yet, they struggle in logical
reasoning, not necessarily because of a fundamental limitation of these models,
but possibly due to the lack of exploration of more creative uses, such as
latent space and recurrent reasoning. An emerging exploration in this direction
is the Hierarchical Reasoning Model (Wang et al., 2025), which introduces a
novel type of recurrent reasoning in the latent space of transformers,
achieving remarkable performance on a wide range of 2D reasoning tasks. Despite
the promising results, this line of models is still at an early stage and calls
for in-depth investigation. In this work, we perform a critical review on this
class of models, examine key design choices and present intriguing variants
that achieve significantly better performance on the Sudoku-Extreme and
Maze-Hard tasks than previously reported. Our results also raise surprising
observations and intriguing directions for further research.

</details>


### [74] [Semantic-Driven AI Agent Communications: Challenges and Solutions](https://arxiv.org/abs/2510.00381)
*Kaiwen Yu,Mengying Sun,Zhijin Qin,Xiaodong Xu,Ping Yang,Yue Xiao,Gang Wu*

Main category: cs.AI

TL;DR: 提出了一个语义驱动的AI智能体通信框架，包含语义自适应传输、语义轻量传输和语义自进化控制三项关键技术，旨在解决动态环境和有限资源下的智能体通信挑战。


<details>
  <summary>Details</summary>
Motivation: 随着智能服务的快速发展，通信目标从人类转向AI智能体，需要新的通信范式来实现实时感知、决策和协作。语义通信虽然提供了有前景的解决方案，但在动态环境和有限资源下的实际部署仍面临挑战。

Method: 1. 语义自适应传输：使用真实或生成样本进行微调，使模型适应变化的环境；2. 语义轻量传输：采用剪枝、量化和感知感知采样来降低模型复杂度；3. 语义自进化控制：采用分布式分层决策来优化多维资源。

Result: 仿真结果表明，所提出的解决方案实现了更快的收敛速度和更强的鲁棒性，分布式分层优化方法显著优于传统决策方案。

Conclusion: 该框架在AI智能体通信网络中具有巨大潜力，能够有效支持动态环境下的多智能体协作。

Abstract: With the rapid growth of intelligent services, communication targets are
shifting from humans to artificial intelligent (AI) agents, which require new
paradigms to enable real-time perception, decision-making, and collaboration.
Semantic communication, which conveys task-relevant meaning rather than raw
data, offers a promising solution. However, its practical deployment remains
constrained by dynamic environments and limited resources. To address these
issues, this article proposes a semantic-driven AI agent communication
framework and develops three enabling techniques. First, semantic adaptation
transmission applies fine-tuning with real or generative samples to efficiently
adapt models to varying environments. Second, semantic lightweight transmission
incorporates pruning, quantization, and perception-aware sampling to reduce
model complexity and alleviate computational burden on edge agents. Third,
semantic self-evolution control employs distributed hierarchical
decision-making to optimize multi-dimensional resources, enabling robust
multi-agent collaboration in dynamic environments. Simulation results show that
the proposed solutions achieve faster convergence and stronger robustness,
while the proposed distributed hierarchical optimization method significantly
outperforms conventional decision-making schemes, highlighting its potential
for AI agent communication networks.

</details>


### [75] [Towards Self-Evolving Benchmarks: Synthesizing Agent Trajectories via Test-Time Exploration under Validate-by-Reproduce Paradigm](https://arxiv.org/abs/2510.00415)
*Dadi Guo,Tianyi Zhou,Dongrui Liu,Chen Qian,Qihan Ren,Shuai Shao,Zhiyuan Fan,Yi R. Fung,Kun Wang,Linfeng Zhang,Jing Shao*

Main category: cs.AI

TL;DR: 提出了TRACE框架，通过让智能体自由探索和演化现有基准任务，自动生成难度更高的新任务，并记录可验证的执行轨迹，解决了现有智能体基准测试快速达到性能天花板的问题。


<details>
  <summary>Details</summary>
Motivation: 现有智能体基准测试面临新开发智能体快速达到性能上限的问题，难以满足评估智能体能力的需求，需要一种能够持续生成更复杂任务的动态评估系统。

Method: TRACE框架包含三个阶段：进化提案挖掘（通过初步探索和发散思维提供任务进化提案）、问题形成与自由探索（将提案概念化为可行问题候选，记录智能体执行轨迹）、多级验证（确保进化任务具有可验证和可复现的轨迹）。

Result: 在GAIA基准测试上的实验表明，TRACE框架能够持续提升任务复杂度，同时通过可验证的执行轨迹提高正确性的可靠性。

Conclusion: 这项工作标志着从静态、人工策划的基准测试向动态、自进化评估系统的范式转变，为智能体开发提供了可持续且具有挑战性的评估平台。

Abstract: Recent advances in large language models (LLMs) and agent system designs have
empowered agents with unprecedented levels of capability. However, existing
agent benchmarks are showing a trend of rapid ceiling-hitting by newly
developed agents, making it difficult to meet the demands for evaluating agent
abilities. To address this problem, we propose the Trajectory-based
Validated-by-Reproducing Agent-benchmark Complexity Evolution (TRACE)
framework. This framework takes an original task from an existing benchmark and
encourages agents to freely explore and evolve it into a new task with higher
difficulty while recording validatable agent trajectories. The framework
proceeds in three stages: (1) evolutionary proposal mining, which provides task
evolution proposals through preliminary exploration and divergent thinking; (2)
problem formation and free exploration, where proposals are conceptualized into
feasible problem candidates and the agents then explore them freely while
recording their execution trajectories; and (3) multi-level validation, which
ensures that the evolved tasks are accompanied by validatable and reproducible
trajectories. Experiments on the GAIA benchmark demonstrate that the TRACE
framework consistently enhances task complexity while improving the reliability
of correctness through validatable execution trajectories. This work marks a
paradigm shift from static, manually curated benchmarks to dynamic,
self-evolving evaluation systems, providing a sustainable and challenging
runway for agent development.

</details>


### [76] [Automated Evaluation can Distinguish the Good and Bad AI Responses to Patient Questions about Hospitalization](https://arxiv.org/abs/2510.00436)
*Sarvesh Soni,Dina Demner-Fushman*

Main category: cs.AI

TL;DR: 该研究探索了自动化评估AI系统回答患者健康问题的可行性，通过系统研究评估方法，发现精心设计的自动化评估能够有效扩展AI系统的比较评估。


<details>
  <summary>Details</summary>
Motivation: 当前评估AI回答患者健康问题的黄金标准——人工专家评审——劳动密集且缓慢，限制了可扩展性。自动化指标虽然前景广阔，但与人类判断的一致性不一且通常依赖上下文。

Method: 在100个患者案例中，收集了28个AI系统的回答（共2800个），并从三个维度评估：是否回答问题、是否适当使用临床记录证据、是否使用一般医学知识。使用临床医生撰写的参考答案作为指标锚点。

Result: 自动化排名与专家评分高度匹配，表明自动化评估能够有效替代人工评审。

Conclusion: 精心设计的自动化评估可以扩展AI系统的比较评估，并支持患者-临床医生沟通。

Abstract: Automated approaches to answer patient-posed health questions are rising, but
selecting among systems requires reliable evaluation. The current gold standard
for evaluating the free-text artificial intelligence (AI) responses--human
expert review--is labor-intensive and slow, limiting scalability. Automated
metrics are promising yet variably aligned with human judgments and often
context-dependent. To address the feasibility of automating the evaluation of
AI responses to hospitalization-related questions posed by patients, we
conducted a large systematic study of evaluation approaches. Across 100 patient
cases, we collected responses from 28 AI systems (2800 total) and assessed them
along three dimensions: whether a system response (1) answers the question, (2)
appropriately uses clinical note evidence, and (3) uses general medical
knowledge. Using clinician-authored reference answers to anchor metrics,
automated rankings closely matched expert ratings. Our findings suggest that
carefully designed automated evaluation can scale comparative assessment of AI
systems and support patient-clinician communication.

</details>


### [77] [Expandable Decision-Making States for Multi-Agent Deep Reinforcement Learning in Soccer Tactical Analysis](https://arxiv.org/abs/2510.00480)
*Kenjiro Ide,Taiga Someya,Kohei Kawaguchi,Keisuke Fujii*

Main category: cs.AI

TL;DR: 提出EDMS方法，通过语义增强的状态表示和动作掩码机制，构建可解释的球员级智能体模型，用于足球等入侵性团队运动的战术分析。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的分析方法直观但有限，现代机器学习模型缺乏明确的智能体表示和战术可解释性。需要从数据中构建球员级智能体模型，使其学习到的价值和策略既具有战术可解释性，又能跨异构数据源保持鲁棒性。

Method: 提出可扩展决策状态(EDMS)：1）语义增强的状态表示，在原始位置和速度基础上添加关系变量（如空间得分、传球和得分）；2）动作掩码方案，为持球和无球球员提供不同的决策集；3）将学习到的价值函数和动作策略映射到人类可解释的战术概念。

Result: 与基线相比，EDMS结合动作掩码持续降低了动作预测损失和时序差分误差。定性案例研究和Q值可视化显示EDMS能突出高风险高回报的战术模式（如快速反击和防守突破）。方法兼容多个商业和开源数据集。

Conclusion: EDMS方法成功构建了战术可解释的球员级智能体模型，将学习到的价值函数和策略映射到人类可理解的战术概念，而非原始坐标特征，使智能体选择与比赛规则保持一致。

Abstract: Invasion team sports such as soccer produce a high-dimensional, strongly
coupled state space as many players continuously interact on a shared field,
challenging quantitative tactical analysis. Traditional rule-based analyses are
intuitive, while modern predictive machine learning models often perform
pattern-matching without explicit agent representations. The problem we address
is how to build player-level agent models from data, whose learned values and
policies are both tactically interpretable and robust across heterogeneous data
sources. Here, we propose Expandable Decision-Making States (EDMS), a
semantically enriched state representation that augments raw positions and
velocities with relational variables (e.g., scoring of space, pass, and score),
combined with an action-masking scheme that gives on-ball and off-ball agents
distinct decision sets. Compared to prior work, EDMS maps learned value
functions and action policies to human-interpretable tactical concepts (e.g.,
marking pressure, passing lanes, ball accessibility) instead of raw coordinate
features, and aligns agent choices with the rules of play. In the experiments,
EDMS with action masking consistently reduced both action-prediction loss and
temporal-difference (TD) error compared to the baseline. Qualitative case
studies and Q-value visualizations further indicate that EDMS highlights
high-risk, high-reward tactical patterns (e.g., fast counterattacks and
defensive breakthroughs). We also integrated our approach into an open-source
library and demonstrated compatibility with multiple commercial and open
datasets, enabling cross-provider evaluation and reproducible experiments.

</details>


### [78] [Rethinking Reward Models for Multi-Domain Test-Time Scaling](https://arxiv.org/abs/2510.00492)
*Dong Bok Lee,Seanie Lee,Sangwoo Park,Minki Kang,Jinheon Baek,Dongki Kim,Dominik Wagner,Jiongdao Jin,Heejun Lee,Tobias Bocklet,Jinyu Wang,Jingjing Fu,Sung Ju Hwang,Jiang Bia,Lei Song*

Main category: cs.AI

TL;DR: 该论文挑战了传统观点，发现在14个不同领域中，生成式结果奖励模型（GenORM）比过程奖励模型（PRM）表现更稳健，而过程奖励模型在长推理轨迹中会累积错误。


<details>
  <summary>Details</summary>
Motivation: 传统认为过程奖励模型（PRM）优于结果奖励模型（ORM），但这种观点主要基于数学相关领域的证据。本研究旨在在多样化领域中统一评估不同奖励模型的性能。

Method: 在14个不同领域中对四种奖励模型变体进行统一评估：判别式ORM和PRM（DisORM、DisPRM）以及生成式ORM和PRM（GenORM、GenPRM）。

Result: 与传统观点相反：（1）DisORM与DisPRM表现相当；（2）GenPRM不具竞争力；（3）GenORM是最稳健的模型，在所有测试领域都获得显著且一致的提升。

Conclusion: 研究挑战了细粒度监督总是更好的普遍假设，支持生成式结果验证在多领域部署中的应用。过程式评分会从LLM自动标注中继承标签噪声，且在长推理轨迹中难以评估。

Abstract: The reliability of large language models (LLMs) during test-time scaling is
often assessed with \emph{external verifiers} or \emph{reward models} that
distinguish correct reasoning from flawed logic. Prior work generally assumes
that process reward models (PRMs), which score every intermediate reasoning
step, outperform outcome reward models (ORMs) that assess only the final
answer. This view is based mainly on evidence from narrow, math-adjacent
domains. We present the first unified evaluation of four reward model variants,
discriminative ORM and PRM (\DisORM, \DisPRM) and generative ORM and PRM
(\GenORM, \GenPRM), across 14 diverse domains. Contrary to conventional wisdom,
we find that (i) \DisORM performs on par with \DisPRM, (ii) \GenPRM is not
competitive, and (iii) overall, \GenORM is the most robust, yielding
significant and consistent gains across every tested domain. We attribute this
to PRM-style stepwise scoring, which inherits label noise from LLM
auto-labeling and has difficulty evaluating long reasoning trajectories,
including those involving self-correcting reasoning. Our theoretical analysis
shows that step-wise aggregation compounds errors as reasoning length grows,
and our empirical observations confirm this effect. These findings challenge
the prevailing assumption that fine-grained supervision is always better and
support generative outcome verification for multi-domain deployment. We
publicly release our code, datasets, and checkpoints at
\href{https://github.com/db-Lee/Multi-RM}{\underline{\small\texttt{https://github.com/db-Lee/Multi-RM}}}
to facilitate future research in multi-domain settings.

</details>


### [79] [VIRTUE: Visual-Interactive Text-Image Universal Embedder](https://arxiv.org/abs/2510.00523)
*Wei-Yao Wang,Kazuya Tateishi,Qiyu Wu,Shusuke Takahashi,Yuki Mitsufuji*

Main category: cs.AI

TL;DR: 提出了VIRTUE模型，将视觉交互能力引入嵌入模型，通过分割模型处理视觉提示来精确定位图像特定区域，在36个通用MMEB任务和5个视觉交互SCaR任务上实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有嵌入模型缺乏视觉交互能力，无法处理用户指定的感兴趣区域（如点、边界框、掩码），限制了其在局部意图定位和实体级信息学习方面的应用潜力。

Method: 结合分割模型和视觉语言模型，分割模型处理视觉提示来精确定位图像特定区域，使嵌入器能更精确处理复杂和模糊场景。

Result: 在36个通用MMEB任务上提升3.1%-8.5%，在5个视觉交互SCaR任务上提升15.2%-20.3%，达到最先进性能。

Conclusion: VIRTUE成功将视觉交互能力扩展到表示学习领域，通过视觉提示实现了更精确的局部意图定位和实体级信息学习。

Abstract: Multimodal representation learning models have demonstrated successful
operation across complex tasks, and the integration of vision-language models
(VLMs) has further enabled embedding models with instruction-following
capabilities. However, existing embedding models lack visual-interactive
capabilities to specify regions of interest from users (e.g., point, bounding
box, mask), which have been explored in generative models to broaden their
human-interactive applicability. Equipping embedding models with visual
interactions not only would unlock new applications with localized grounding of
user intent, which remains unexplored, but also enable the models to learn
entity-level information within images to complement their global
representations for conventional embedding tasks. In this paper, we propose a
novel Visual-InteRactive Text-Image Universal Embedder (VIRTUE) that extends
the capabilities of the segmentation model and the vision-language model to the
realm of representation learning. In VIRTUE, the segmentation model can process
visual prompts that pinpoint specific regions within an image, thereby enabling
the embedder to handle complex and ambiguous scenarios more precisely. To
evaluate the visual-interaction ability of VIRTUE, we introduce a large-scale
Segmentation-and-Scene Caption Retrieval (SCaR) benchmark comprising 1M samples
that aims to retrieve the text caption by jointly considering the entity with a
specific object and image scene. VIRTUE consistently achieves a
state-of-the-art performance with significant improvements across 36 universal
MMEB (3.1%-8.5%) and five visual-interactive SCaR (15.2%-20.3%) tasks.

</details>


### [80] [Data Quality Challenges in Retrieval-Augmented Generation](https://arxiv.org/abs/2510.00552)
*Leopold Müller,Joshua Holstein,Sarah Bause,Gerhard Satzger,Niklas Kühl*

Main category: cs.AI

TL;DR: 本研究针对RAG系统开发了15个数据质量维度，涵盖数据提取、转换、提示搜索和生成四个处理阶段，发现需要在传统DQ框架中增加新维度，并建议采用前置质量管理和动态质量监控策略。


<details>
  <summary>Details</summary>
Motivation: 当前数据质量框架主要针对静态数据集，无法充分应对RAG系统的动态多阶段特性，需要开发专门针对这类AI系统的数据质量维度。

Method: 通过对领先IT服务公司从业者进行16次半结构化访谈，采用定性内容分析方法，归纳推导出RAG系统的数据质量维度。

Result: 识别出15个不同的DQ维度，分布在RAG系统的四个处理阶段；发现新维度主要集中在早期阶段，DQ问题会在管道中转化和传播。

Conclusion: 需要扩展传统DQ框架以覆盖RAG上下文，采用前置质量管理和动态、阶段感知的质量管理方法。

Abstract: Organizations increasingly adopt Retrieval-Augmented Generation (RAG) to
enhance Large Language Models with enterprise-specific knowledge. However,
current data quality (DQ) frameworks have been primarily developed for static
datasets, and only inadequately address the dynamic, multi-stage nature of RAG
systems. This study aims to develop DQ dimensions for this new type of AI-based
systems. We conduct 16 semi-structured interviews with practitioners of leading
IT service companies. Through a qualitative content analysis, we inductively
derive 15 distinct DQ dimensions across the four processing stages of RAG
systems: data extraction, data transformation, prompt & search, and generation.
Our findings reveal that (1) new dimensions have to be added to traditional DQ
frameworks to also cover RAG contexts; (2) these new dimensions are
concentrated in early RAG steps, suggesting the need for front-loaded quality
management strategies, and (3) DQ issues transform and propagate through the
RAG pipeline, necessitating a dynamic, step-aware approach to quality
management.

</details>


### [81] [Toward Safer Diffusion Language Models: Discovery and Mitigation of Priming Vulnerability](https://arxiv.org/abs/2510.00565)
*Shojiro Yamabe,Jun Sakuma*

Main category: cs.AI

TL;DR: 本文揭示了扩散语言模型在迭代去噪过程中的关键安全漏洞，并提出了一种针对性的安全对齐方法。研究发现，如果在中间步骤出现对有害查询的肯定令牌，后续去噪过程会被引导生成有害回应，从而绕过安全防护。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型通过并行迭代去噪生成令牌，虽然降低了延迟并支持双向条件化，但其推理机制带来的安全风险尚未被充分理解。需要研究针对扩散语言模型特有的安全漏洞和防护措施。

Method: 提出了一种专门针对扩散语言模型的安全对齐方法，训练模型从包含肯定令牌的污染中间状态生成安全回应。该方法通过针对性训练增强模型对中间状态污染的鲁棒性。

Result: 实验表明，所提出的方法显著缓解了该安全漏洞，对任务性能影响最小，同时提高了对传统越狱攻击的鲁棒性。

Conclusion: 扩散语言模型存在由其迭代去噪过程带来的独特安全漏洞，需要专门的安全研究。提出的安全对齐方法有效提升了模型的安全性，同时保持了良好的任务性能。

Abstract: Diffusion language models (DLMs) generate tokens in parallel through
iterative denoising, which can reduce latency and enable bidirectional
conditioning. However, the safety risks posed by jailbreak attacks that exploit
this inference mechanism are not well understood. In this paper, we reveal that
DLMs have a critical vulnerability stemming from their iterative denoising
process and propose a countermeasure. Specifically, our investigation shows
that if an affirmative token for a harmful query appears at an intermediate
step, subsequent denoising can be steered toward a harmful response even in
aligned models. As a result, simply injecting such affirmative tokens can
readily bypass the safety guardrails. Furthermore, we demonstrate that the
vulnerability allows existing optimization-based jailbreak attacks to succeed
on DLMs. Building on this analysis, we propose a novel safety alignment method
tailored to DLMs that trains models to generate safe responses from
contaminated intermediate states that contain affirmative tokens. Our
experiments indicate that the proposed method significantly mitigates the
vulnerability with minimal impact on task performance. Furthermore, our method
improves robustness against conventional jailbreak attacks. Our work
underscores the need for DLM-specific safety research.

</details>


### [82] [ACON: Optimizing Context Compression for Long-horizon LLM Agents](https://arxiv.org/abs/2510.00615)
*Minki Kang,Wei-Ning Chen,Dongge Han,Huseyin A. Inan,Lukas Wutschitz,Yanzhi Chen,Robert Sim,Saravan Rajmohan*

Main category: cs.AI

TL;DR: 提出了Agent Context Optimization (ACON)框架，通过优化压缩指南来压缩环境观察和交互历史，减少内存使用26-54%同时保持任务性能，并能蒸馏到更小的压缩器中。


<details>
  <summary>Details</summary>
Motivation: LLM作为智能体在动态环境中部署时，需要积累长历史记录导致上下文长度增长，增加成本并降低长视野任务的效率，而现有的上下文压缩方法主要关注单步任务或狭窄应用。

Method: ACON框架通过自然语言空间中的压缩指南优化：在完整上下文成功但压缩上下文失败的配对轨迹中，LLM分析失败原因并相应更新压缩指南，然后将优化的LLM压缩器蒸馏到更小的模型中。

Result: 在AppWorld、OfficeBench和Multi-objective QA上的实验显示，ACON减少峰值token内存使用26-54%同时基本保持任务性能，蒸馏到更小压缩器时保持95%以上准确率，并提升较小LM作为长视野智能体的性能达46%。

Conclusion: ACON提供了一种有效的上下文压缩方法，能够在减少内存使用的同时保持智能体性能，并通过蒸馏技术实现高效部署。

Abstract: Large language models (LLMs) are increasingly deployed as agents in dynamic,
real-world environments, where success requires both reasoning and effective
tool use. A central challenge for agentic tasks is the growing context length,
as agents must accumulate long histories of actions and observations. This
expansion raises costs and reduces efficiency in long-horizon tasks, yet prior
work on context compression has mostly focused on single-step tasks or narrow
applications. We introduce Agent Context Optimization (ACON), a unified
framework that optimally compresses both environment observations and
interaction histories into concise yet informative condensations. ACON
leverages compression guideline optimization in natural language space: given
paired trajectories where full context succeeds but compressed context fails,
capable LLMs analyze the causes of failure, and the compression guideline is
updated accordingly. Furthermore, we propose distilling the optimized LLM
compressor into smaller models to reduce the overhead of the additional module.
Experiments on AppWorld, OfficeBench, and Multi-objective QA show that ACON
reduces memory usage by 26-54% (peak tokens) while largely preserving task
performance, preserves over 95% of accuracy when distilled into smaller
compressors, and enhances smaller LMs as long-horizon agents with up to 46%
performance improvement.

</details>


### [83] [HARPA: A Testability-Driven, Literature-Grounded Framework for Research Ideation](https://arxiv.org/abs/2510.00620)
*Rosni Vasu,Peter Jansen,Pao Siangliulue,Cristina Sarasua,Abraham Bernstein,Peter Clark,Bhavana Dalvi Mishra*

Main category: cs.AI

TL;DR: HARPA是一个AI驱动的科学发现系统，通过文献挖掘识别研究趋势，探索假设设计空间，并生成可测试的假设。相比基线AI研究者，HARPA在可行性和基础性方面表现更优，并能通过奖励模型从实验经验中学习。


<details>
  <summary>Details</summary>
Motivation: 解决现有自动化科学发现工具难以生成既可测试又基于科学文献的假设，且无法根据先前实验结果进行调整的问题。

Method: HARPA采用受人类研究人员启发的构思流程：文献挖掘识别研究趋势→探索假设设计空间→通过定位研究空白和论证设计选择来收敛到精确可测试的假设。系统还学习奖励模型，基于先前实验结果对新假设进行评分。

Result: HARPA生成的研究提案在可行性(+0.78)和基础性(+0.85)方面显著优于基线AI研究者。与ASD代理(CodeScientist)测试时，HARPA产生更多成功执行(20 vs 11/40)和更少失败(16 vs 21/40)。奖励模型比未训练基线评分器提升约28%。

Conclusion: HARPA的方法代表了AI驱动科学发现领域的重要进展，能够生成更可行、更基础的假设，并能从实验经验中学习改进假设质量。

Abstract: While there has been a surge of interest in automated scientific discovery
(ASD), especially with the emergence of LLMs, it remains challenging for tools
to generate hypotheses that are both testable and grounded in the scientific
literature. Additionally, existing ideation tools are not adaptive to prior
experimental outcomes. We developed HARPA to address these challenges by
incorporating the ideation workflow inspired by human researchers. HARPA first
identifies emerging research trends through literature mining, then explores
hypothesis design spaces, and finally converges on precise, testable hypotheses
by pinpointing research gaps and justifying design choices. Our evaluations
show that HARPA-generated hypothesis-driven research proposals perform
comparably to a strong baseline AI-researcher across most qualitative
dimensions (e.g., specificity, novelty, overall quality), but achieve
significant gains in feasibility(+0.78, p$<0.05$, bootstrap) and groundedness
(+0.85, p$<0.01$, bootstrap) on a 10-point Likert scale. When tested with the
ASD agent (CodeScientist), HARPA produced more successful executions (20 vs. 11
out of 40) and fewer failures (16 vs. 21 out of 40), showing that expert
feasibility judgments track with actual execution success. Furthermore, to
simulate how researchers continuously refine their understanding of what
hypotheses are both testable and potentially interesting from experience, HARPA
learns a reward model that scores new hypotheses based on prior experimental
outcomes, achieving approx. a 28\% absolute gain over HARPA's untrained
baseline scorer. Together, these methods represent a step forward in the field
of AI-driven scientific discovery.

</details>


### [84] [Is Model Editing Built on Sand? Revealing Its Illusory Success and Fragile Foundation](https://arxiv.org/abs/2510.00625)
*Wei Liu,Haomei Xu,Bingqing Liu,Zhiying Deng,Haozhao Wang,Jun Wang,Ruixuan Li,Yee Whye Teh,Wee Sun Lee*

Main category: cs.AI

TL;DR: 该论文揭示当前模型编辑方法存在严重缺陷，它们依赖隐藏的捷径而非真实语义，导致在否定查询等简单测试中崩溃，挑战了模型编辑领域的可行性基础。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型包含过时或错误知识，需要更新和删除这些知识以实现对齐和安全。模型编辑被提出作为解决方案，但作者发现现有方法的可靠性建立在脆弱基础上，存在虚幻的成功。

Method: 系统开发了一套新的评估方法，特别设计了包含否定查询的负例测试，用于揭示模型编辑方法是否真正基于语义理解而非隐藏捷径。

Result: 研究发现即使是最先进的模型编辑方法，在简单的否定查询下也会崩溃，表明编辑很可能基于捷径而非完整的语义理解。

Conclusion: 当前模型编辑文献的基础受到根本性挑战，需要在进一步推进前重新考虑其基本前提，因为捷径与稳健的知识整合存在本质冲突。

Abstract: Large language models (LLMs) inevitably encode outdated or incorrect
knowledge. Updating, deleting, and forgetting such knowledge is important for
alignment, safety, and other issues. To address this issue, model editing has
emerged as a promising paradigm: by precisely editing a small subset of
parameters such that a specific fact is updated while preserving other
knowledge. Despite its great success reported in previous papers, we find the
apparent reliability of editing rests on a fragile foundation and the current
literature is largely driven by illusory success. The fundamental goal of
steering the model's output toward a target with minimal modification would
encourage exploiting hidden shortcuts, rather than utilizing real semantics.
This problem directly challenges the feasibility of the current model editing
literature at its very foundation, as shortcuts are inherently at odds with
robust knowledge integration. Coincidentally, this issue has long been obscured
by evaluation frameworks that lack the design of negative examples. To uncover
it, we systematically develop a suite of new evaluation methods. Strikingly, we
find that state-of-the-art approaches collapse even under the simplest negation
queries. Our empirical evidence shows that editing is likely to be based on
shortcuts rather than full semantics, calling for an urgent reconsideration of
the very basis of model editing before further advancements can be meaningfully
pursued.

</details>


### [85] [Collaborative-Distilled Diffusion Models (CDDM) for Accelerated and Lightweight Trajectory Prediction](https://arxiv.org/abs/2510.00627)
*Bingzhang Wang,Kehua Chen,Yinhai Wang*

Main category: cs.AI

TL;DR: 提出CDDM方法，通过协作渐进蒸馏将大模型知识转移到轻量学生模型，实现实时轻量轨迹预测，在保持高精度的同时大幅压缩模型和加速采样。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在轨迹预测中表现出色，但模型大、采样慢的问题阻碍实际部署，需要开发实时轻量解决方案。

Method: 基于协作渐进蒸馏(CPD)，逐步将教师扩散模型知识转移到学生模型，减少采样步数和模型大小，并引入双信号正则化蒸馏损失防止过拟合。

Result: 在ETH-UCY和nuScenes基准测试中达到SOTA精度，仅需231K参数和2-4采样步数，实现161倍压缩、31倍加速和9ms延迟。

Conclusion: CDDM成功将高性能生成模型与部署约束结合，为自动驾驶和智能交通系统提供资源高效的轨迹预测方案。

Abstract: Trajectory prediction is a fundamental task in Autonomous Vehicles (AVs) and
Intelligent Transportation Systems (ITS), supporting efficient motion planning
and real-time traffic safety management. Diffusion models have recently
demonstrated strong performance in probabilistic trajectory prediction, but
their large model size and slow sampling process hinder real-world deployment.
This paper proposes Collaborative-Distilled Diffusion Models (CDDM), a novel
method for real-time and lightweight trajectory prediction. Built upon
Collaborative Progressive Distillation (CPD), CDDM progressively transfers
knowledge from a high-capacity teacher diffusion model to a lightweight student
model, jointly reducing both the number of sampling steps and the model size
across distillation iterations. A dual-signal regularized distillation loss is
further introduced to incorporate guidance from both the teacher and
ground-truth data, mitigating potential overfitting and ensuring robust
performance. Extensive experiments on the ETH-UCY pedestrian benchmark and the
nuScenes vehicle benchmark demonstrate that CDDM achieves state-of-the-art
prediction accuracy. The well-distilled CDDM retains 96.2% and 95.5% of the
baseline model's ADE and FDE performance on pedestrian trajectories, while
requiring only 231K parameters and 4 or 2 sampling steps, corresponding to 161x
compression, 31x acceleration, and 9 ms latency. Qualitative results further
show that CDDM generates diverse and accurate trajectories under dynamic agent
behaviors and complex social interactions. By bridging high-performing
generative models with practical deployment constraints, CDDM enables
resource-efficient probabilistic prediction for AVs and ITS. Code is available
at https://github.com/bingzhangw/CDDM.

</details>


### [86] [Expected Attention: KV Cache Compression by Estimating Attention from Future Queries Distribution](https://arxiv.org/abs/2510.00636)
*Alessio Devoto,Maximilian Jeblick,Simon Jégou*

Main category: cs.AI

TL;DR: 提出Expected Attention方法，通过预测未来查询对KV对的关注度来估计重要性，实现无需训练的KV缓存压缩，并发布KVPress库包含20多种压缩技术。


<details>
  <summary>Details</summary>
Motivation: KV缓存的内存消耗是LLM推理效率的主要瓶颈，现有基于注意力分数的KV缓存剪枝方法面临实际限制：未来token的注意力分数在压缩时不可用，且现代实现如Flash Attention不生成完整注意力矩阵。

Method: 利用LLM激活的分布特性，以闭式形式计算每个KV对的期望注意力分数，基于这些分数进行原则性排序和剪枝，最小化对残差流的影响。

Result: 该方法在预填充和解码阶段都能无缝运行，在两个场景下都持续优于最先进的基线方法。

Conclusion: Expected Attention方法能够实现有效的KV缓存压缩而不会导致性能下降，并提供了KVPress库来支持KV缓存压缩方法的研究和基准测试。

Abstract: Memory consumption of the Key-Value (KV) cache represents a major bottleneck
for efficient large language model inference. While attention-score-based KV
cache pruning shows promise, it faces critical practical limitations: attention
scores from future tokens are unavailable during compression, and modern
implementations like Flash Attention do not materialize the full attention
matrix, making past scores inaccessible. To overcome these challenges, we
introduce $\textbf{Expected Attention, a training-free compression method}$
that estimates KV pairs importance by predicting how future queries will attend
to them. Our approach leverages the distributional properties of LLM
activations to compute expected attention scores in closed form for each KV
pair. These scores enable principled ranking and pruning of KV pairs with
minimal impact on the residual stream, achieving effective compression without
performance degradation. Importantly, our method operates seamlessly across
both prefilling and decoding phases, consistently outperforming
state-of-the-art baselines in both scenarios. Finally, $\textbf{we release
KVPress, a comprehensive library to enable researchers to implement and
benchmark KV cache compression methods, already including more than 20
techniques}$.

</details>


### [87] [Batch-CAM: Introduction to better reasoning in convolutional deep learning models](https://arxiv.org/abs/2510.00664)
*Giacomo Ignesti,Davide Moroni,Massimo Martinelli*

Main category: cs.AI

TL;DR: 提出Batch-CAM训练范式，结合批处理Grad-CAM算法和原型重建损失，提升模型在分类任务中的性能和可解释性


<details>
  <summary>Details</summary>
Motivation: 在医疗等高风险领域，深度学习模型的可解释性至关重要，需要准确解释模型决策依据

Method: 融合批处理Grad-CAM算法和原型重建损失，引导模型关注显著图像特征

Result: 在准确性和图像重建质量上同时提升，同时减少了训练和推理时间

Conclusion: 该方法通过确保模型学习证据相关信息，为构建更透明、可解释和可信的AI系统做出贡献

Abstract: Understanding the inner workings of deep learning models is crucial for
advancing artificial intelligence, particularly in high-stakes fields such as
healthcare, where accurate explanations are as vital as precision. This paper
introduces Batch-CAM, a novel training paradigm that fuses a batch
implementation of the Grad-CAM algorithm with a prototypical reconstruction
loss. This combination guides the model to focus on salient image features,
thereby enhancing its performance across classification tasks. Our results
demonstrate that Batch-CAM achieves a simultaneous improvement in accuracy and
image reconstruction quality while reducing training and inference times. By
ensuring models learn from evidence-relevant information,this approach makes a
relevant contribution to building more transparent, explainable, and
trustworthy AI systems.

</details>


### [88] [Relevance-Zone Reduction in Game Solving](https://arxiv.org/abs/2510.00689)
*Chi-Huang Lin,Ting Han Wei,Chun-Jui Wang,Hung Guei,Chung-Chin Shih,Yun-Jui Tsai,I-Chen Wu,Ti-Rong Wu*

Main category: cs.AI

TL;DR: 提出了一种迭代式RZ缩减方法，通过逐步限制搜索区域来减小相关区域(RZ)的大小，从而提高策略重用效率和剪枝效果。


<details>
  <summary>Details</summary>
Motivation: 游戏求解面临指数级增长的博弈树问题，虽然相关区域(RZ)技术能缩小搜索空间，但不同解会产生不同大小的RZ，较小的RZ更有利于重用和剪枝。

Method: 设计迭代RZ缩减方法，重复求解同一位置并逐步限制参与区域；提出三种约束生成策略，并集成RZ模式表以充分利用历史解。

Result: 在7x7 Killall-Go实验中，平均RZ大小减少到原始的85.95%。

Conclusion: 缩减后的RZ可作为可重用知识永久存储，用于未来更大的棋盘或不同开局位置的求解任务。

Abstract: Game solving aims to find the optimal strategies for all players and
determine the theoretical outcome of a game. However, due to the exponential
growth of game trees, many games remain unsolved, even though methods like
AlphaZero have demonstrated super-human level in game playing. The
Relevance-Zone (RZ) is a local strategy reuse technique that restricts the
search to only the regions relevant to the outcome, significantly reducing the
search space. However, RZs are not unique. Different solutions may result in
RZs of varying sizes. Smaller RZs are generally more favorable, as they
increase the chance of reuse and improve pruning efficiency. To this end, we
propose an iterative RZ reduction method that repeatedly solves the same
position while gradually restricting the region involved, guiding the solver
toward smaller RZs. We design three constraint generation strategies and
integrate an RZ Pattern Table to fully leverage past solutions. In experiments
on 7x7 Killall-Go, our method reduces the average RZ size to 85.95% of the
original. Furthermore, the reduced RZs can be permanently stored as reusable
knowledge for future solving tasks, especially for larger board sizes or
different openings.

</details>


### [89] [ACPO: Adaptive Curriculum Policy Optimization for Aligning Vision-Language Models in Complex Reasoning](https://arxiv.org/abs/2510.00690)
*Yunhao Wang,Ziting Li,Shuai Chen,Tao Liu,Chao Song,Junjie Jiang,Jian Zhu,Peng Gao,Bin Qin*

Main category: cs.AI

TL;DR: 提出了自适应课程策略优化（ACPO）框架，通过动态课程和自适应裁剪机制改进视觉语言模型的强化学习对齐，在多个多模态推理基准上实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有策略优化算法（如PPO）存在静态训练计划和刚性裁剪机制的限制，阻碍了大规模视觉语言模型在复杂推理任务中的强化学习对齐。

Method: ACPO框架包含两个核心组件：动态课程策略（从近策略探索逐步过渡到离策略利用）和优势感知自适应裁剪机制（基于归一化优势动态调整样本级裁剪边界）。

Result: 在MathVista、LogicVista和MMMU-Pro等基准测试中，ACPO一致优于DAPO和PAPO等基线方法，实现了最先进性能、加速收敛和更好的训练稳定性。

Conclusion: ACPO通过自适应学习策略有效解决了现有策略优化算法的局限性，为复杂多模态推理任务的强化学习对齐提供了更优的解决方案。

Abstract: Aligning large-scale vision-language models (VLMs) for complex reasoning via
reinforcement learning is often hampered by the limitations of existing policy
optimization algorithms, such as static training schedules and the rigid,
uniform clipping mechanism in Proximal Policy Optimization (PPO). In this work,
we introduce Adaptive Curriculum Policy Optimization (ACPO), a novel framework
that addresses these challenges through a dual-component adaptive learning
strategy. First, ACPO employs a dynamic curriculum that orchestrates a
principled transition from a stable, near on-policy exploration phase to an
efficient, off-policy exploitation phase by progressively increasing sample
reuse. Second, we propose an Advantage-Aware Adaptive Clipping (AAAC) mechanism
that replaces the fixed clipping hyperparameter with dynamic, sample-wise
bounds modulated by the normalized advantage of each token. This allows for
more granular and robust policy updates, enabling larger gradients for
high-potential samples while safeguarding against destructive ones. We conduct
extensive experiments on a suite of challenging multimodal reasoning
benchmarks, including MathVista, LogicVista, and MMMU-Pro. Results demonstrate
that ACPO consistently outperforms strong baselines such as DAPO and PAPO,
achieving state-of-the-art performance, accelerated convergence, and superior
training stability.

</details>


### [90] [AttentionDep: Domain-Aware Attention for Explainable Depression Severity Assessment](https://arxiv.org/abs/2510.00706)
*Yusif Ibrahimov,Tarique Anwar,Tommy Yuan,Turan Mutallimov,Elgun Hasanov*

Main category: cs.AI

TL;DR: 提出了AttentionDep模型，这是一个融合上下文和领域知识的注意力模型，用于从社交媒体进行可解释的抑郁症严重程度检测。


<details>
  <summary>Details</summary>
Motivation: 利用社交媒体平台（如Facebook、X、Reddit）作为了解个体思想、情绪和心理健康状态的窗口，开发可信赖和透明的AI系统进行心理健康评估。

Method: 使用AttentionDep模型，通过层次化编码单字和双字，注意力机制突出临床相关标记，并利用心理健康知识图谱的领域知识通过交叉注意力机制丰富上下文特征，最后使用有序回归框架预测抑郁症严重程度。

Result: 实验表明AttentionDep在分级F1分数上比最先进的基线方法提高了5%以上，同时为其预测提供了可解释的见解。

Conclusion: 这项工作推动了从社交媒体进行心理健康评估的可信赖和透明AI系统的发展。

Abstract: In today's interconnected society, social media platforms provide a window
into individuals' thoughts, emotions, and mental states. This paper explores
the use of platforms like Facebook, X (formerly Twitter), and Reddit for
depression severity detection. We propose AttentionDep, a domain-aware
attention model that drives explainable depression severity estimation by
fusing contextual and domain knowledge. Posts are encoded hierarchically using
unigrams and bigrams, with attention mechanisms highlighting clinically
relevant tokens. Domain knowledge from a curated mental health knowledge graph
is incorporated through a cross-attention mechanism, enriching the contextual
features. Finally, depression severity is predicted using an ordinal regression
framework that respects the clinical-relevance and natural ordering of severity
levels. Our experiments demonstrate that AttentionDep outperforms
state-of-the-art baselines by over 5% in graded F1 score across datasets, while
providing interpretable insights into its predictions. This work advances the
development of trustworthy and transparent AI systems for mental health
assessment from social media.

</details>


### [91] [EvolProver: Advancing Automated Theorem Proving by Evolving Formalized Problems via Symmetry and Difficulty](https://arxiv.org/abs/2510.00732)
*Yuchen Tian,Ruiyuan Huang,Xuanwu Wang,Jing Ma,Zengfeng Huang,Ziyang Luo,Hongzhan Lin,Da Zheng,Lun Du*

Main category: cs.AI

TL;DR: 提出了一种新颖的数据增强流水线，通过对称性和难度两个角度提升大语言模型在形式定理证明中的鲁棒性，并训练出在多个基准测试中达到最先进水平的7B参数非推理定理证明器EvolProver。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在形式定理证明中缺乏泛化能力，对问题陈述的微小变换也很脆弱，需要提升模型的鲁棒性。

Method: 从对称性角度提出EvolAST（基于抽象语法树的句法对称）和EvolDomain（跨数学领域的语义对称），从难度角度提出EvolDifficulty（生成不同难度定理），使用进化数据训练7B参数的EvolProver定理证明器。

Result: EvolProver在FormalMATH-Lite上达到53.8% pass@32的新SOTA，在MiniF2F-Test（69.8%）、Ineq-Comp-Seed（52.2%）和Ineq-Comp-Transformed（34.0%）等基准测试中均为非推理模型的新SOTA。

Conclusion: 数据增强流水线有效提升了定理证明模型的鲁棒性，EvolProver在多个基准测试中超越了同类规模模型，包括推理型模型。

Abstract: Large Language Models (LLMs) for formal theorem proving have shown
significant promise, yet they often lack generalizability and are fragile to
even minor transformations of problem statements. To address this limitation,
we introduce a novel data augmentation pipeline designed to enhance model
robustness from two perspectives: symmetry and difficulty. From the symmetry
perspective, we propose two complementary methods: EvolAST, an Abstract Syntax
Tree (AST) based approach that targets syntactic symmetry to generate
semantically equivalent problem variants, and EvolDomain, which leverages LLMs
to address semantic symmetry by translating theorems across mathematical
domains. From the difficulty perspective, we propose EvolDifficulty, which uses
carefully designed evolutionary instructions to guide LLMs in generating new
theorems with a wider range of difficulty. We then use the evolved data to
train EvolProver, a 7B-parameter non-reasoning theorem prover. EvolProver
establishes a new state-of-the-art (SOTA) on FormalMATH-Lite with a 53.8%
pass@32 rate, surpassing all models of comparable size, including
reasoning-based models. It also sets new SOTA records for non-reasoning models
on MiniF2F-Test (69.8% pass@32), Ineq-Comp-Seed (52.2% pass@32), and
Ineq-Comp-Transformed (34.0% pass@32). Ablation studies further confirm our
data augmentation pipeline's effectiveness across multiple benchmarks.

</details>


### [92] [DIA: The Adversarial Exposure of Deterministic Inversion in Diffusion Models](https://arxiv.org/abs/2510.00778)
*Seunghoo Hong,Geonho Son,Juhun Lee,Simon S. Woo*

Main category: cs.AI

TL;DR: 提出了DDIM反演攻击(DIA)方法，通过攻击DDIM轨迹路径来有效破坏恶意图像编辑，在防御性能上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: DDIM反演技术使恶意用户能够轻松合成虚假内容，而现有防御方法由于目标与去噪轨迹不匹配，破坏效果有限。

Method: 提出DDIM反演攻击(DIA)，直接攻击DDIM的集成轨迹路径，而不是单个步骤。

Result: DIA在多种编辑方法上都实现了有效的破坏，性能超越了之前的防御方法。

Conclusion: 该框架为产业界和研究社区提供了实用的防御方法，对抗AI的恶意使用。

Abstract: Diffusion models have shown to be strong representation learners, showcasing
state-of-the-art performance across multiple domains. Aside from accelerated
sampling, DDIM also enables the inversion of real images back to their latent
codes. A direct inheriting application of this inversion operation is real
image editing, where the inversion yields latent trajectories to be utilized
during the synthesis of the edited image. Unfortunately, this practical tool
has enabled malicious users to freely synthesize misinformative or deepfake
contents with greater ease, which promotes the spread of unethical and abusive,
as well as privacy-, and copyright-infringing contents. While defensive
algorithms such as AdvDM and Photoguard have been shown to disrupt the
diffusion process on these images, the misalignment between their objectives
and the iterative denoising trajectory at test time results in weak disruptive
performance.In this work, we present the DDIM Inversion Attack (DIA) that
attacks the integrated DDIM trajectory path. Our results support the effective
disruption, surpassing previous defensive methods across various editing
methods. We believe that our frameworks and results can provide practical
defense methods against the malicious use of AI for both the industry and the
research community. Our code is available here:
https://anonymous.4open.science/r/DIA-13419/.

</details>


### [93] [AI in data science education: experiences from the classroom](https://arxiv.org/abs/2510.00793)
*J. A. Hageman,C. F. W. Peeters*

Main category: cs.AI

TL;DR: 本研究探讨了AI（特别是像ChatGPT这样的大语言模型）在教育环境中的整合，重点关注对教学和学习的影响。研究发现AI工具能简化任务和增强学习，但也存在学生过度依赖技术而阻碍认知和问题解决能力发展的风险。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术在教育领域的广泛应用，需要了解其对教学实践的影响，特别是如何平衡AI带来的便利与学生基本技能发展之间的关系。

Method: 通过对瓦赫宁根大学数据科学课程协调员进行访谈，收集关于AI在课堂中应用的定性数据。

Result: 研究识别出AI在教育中的双重影响：一方面能提高效率和增强学习体验，另一方面可能导致学生过度依赖技术，影响基本认知能力的发展。

Conclusion: AI可以成为教育中的宝贵资产，但需要负责任地使用，确保其补充而非替代基本学习过程，同时需要调整评估方法来确保教育目标的实现。

Abstract: This study explores the integration of AI, particularly large language models
(LLMs) like ChatGPT, into educational settings, focusing on the implications
for teaching and learning. Through interviews with course coordinators from
data science courses at Wageningen University, this research identifies both
the benefits and challenges associated with AI in the classroom. While AI tools
can streamline tasks and enhance learning, concerns arise regarding students'
overreliance on these technologies, potentially hindering the development of
essential cognitive and problem solving skills. The study highlights the
importance of responsible AI usage, ethical considerations, and the need for
adapting assessment methods to ensure educational outcomes are met. With
careful integration, AI can be a valuable asset in education, provided it is
used to complement rather than replace fundamental learning processes.

</details>


### [94] [Benchmarking Agentic Systems in Automated Scientific Information Extraction with ChemX](https://arxiv.org/abs/2510.00795)
*Anastasia Vepreva,Julia Razlivina,Maria Eremeeva,Nina Gubina,Anastasia Orlova,Aleksei Dmitrenko,Ksenya Kapranova,Susan Jyakhwo,Nikita Vasilev,Arsen Sarkisyan,Ivan Yu. Chernyshov,Vladimir Vinogradov,Andrei Dmitrenko*

Main category: cs.AI

TL;DR: 提出了ChemX数据集，用于评估和改进化学信息提取方法，通过基准测试发现现有方法在化学领域仍面临挑战


<details>
  <summary>Details</summary>
Motivation: 化学信息提取由于数据异质性而具有挑战性，现有基于代理的方法在该领域表现有限，需要专门的评估资源

Method: 创建了10个手动策划且经过领域专家验证的数据集，进行了广泛的基准测试，包括现有最先进的代理系统和现代基线模型，并引入了单代理方法进行文档预处理控制

Result: 实证发现化学信息提取存在持续挑战，特别是在处理领域特定术语、复杂表格和示意图表示以及上下文依赖的模糊性方面

Conclusion: ChemX基准是推进化学自动化信息提取的关键资源，挑战现有方法的泛化能力，并为有效评估策略提供宝贵见解

Abstract: The emergence of agent-based systems represents a significant advancement in
artificial intelligence, with growing applications in automated data
extraction. However, chemical information extraction remains a formidable
challenge due to the inherent heterogeneity of chemical data. Current
agent-based approaches, both general-purpose and domain-specific, exhibit
limited performance in this domain. To address this gap, we present ChemX, a
comprehensive collection of 10 manually curated and domain-expert-validated
datasets focusing on nanomaterials and small molecules. These datasets are
designed to rigorously evaluate and enhance automated extraction methodologies
in chemistry. To demonstrate their utility, we conduct an extensive
benchmarking study comparing existing state-of-the-art agentic systems such as
ChatGPT Agent and chemical-specific data extraction agents. Additionally, we
introduce our own single-agent approach that enables precise control over
document preprocessing prior to extraction. We further evaluate the performance
of modern baselines, such as GPT-5 and GPT-5 Thinking, to compare their
capabilities with agentic approaches. Our empirical findings reveal persistent
challenges in chemical information extraction, particularly in processing
domain-specific terminology, complex tabular and schematic representations, and
context-dependent ambiguities. The ChemX benchmark serves as a critical
resource for advancing automated information extraction in chemistry,
challenging the generalization capabilities of existing methods, and providing
valuable insights into effective evaluation strategies.

</details>


### [95] [Semantic Bridges Between First Order c-Representations and Cost-Based Semantics: An Initial Perspective](https://arxiv.org/abs/2510.00817)
*Nicholas Leisegang,Giovanni Casini,Thomas Meyer*

Main category: cs.AI

TL;DR: 比较加权知识库与c-表示这两种处理不一致知识库的形式化方法，分析它们在语义层面和推理关系上的等价性


<details>
  <summary>Details</summary>
Motivation: 研究两种处理不一致知识库的方法——加权知识库和c-表示之间的语义关系，探索它们能否产生相同的解释排序

Method: 在语义层面比较两种方法：加权知识库通过为每个解释分配基于违反规则的成本，c-表示通过为违反条件分配惩罚值来排序解释

Result: 证明在特定条件下，加权知识库和一组可废止条件可以生成相同的解释排序，实现语义结构的相对成本等价性

Conclusion: 两种形式化方法在特定条件下具有语义等价性，这为成本语义和c-表示的进一步研究提供了有益启示

Abstract: Weighted-knowledge bases and cost-based semantics represent a recent
formalism introduced by Bienvenu et al. for Ontology Mediated Data Querying in
the case where a given knowledge base is inconsistent. This is done by adding a
weight to each statement in the knowledge base (KB), and then giving each DL
interpretation a cost based on how often it breaks rules in the KB. In this
paper we compare this approach with c-representations, a form of non-monotonic
reasoning originally introduced by Kern-Isberner. c-Representations describe a
means to interpret defeasible concept inclusions in the first-order case. This
is done by assigning a numerical ranking to each interpretations via penalties
for each violated conditional. We compare these two approaches on a semantic
level. In particular, we show that under certain conditions a weighted
knowledge base and a set of defeasible conditionals can generate the same
ordering on interpretations, and therefore an equivalence of semantic
structures up to relative cost. Moreover, we compare entailment described in
both cases, where certain notions are equivalently expressible in both
formalisms. Our results have the potential to benefit further work on both
cost-based semantics and c-representations

</details>


### [96] [Logical Consistency Between Disagreeing Experts and Its Role in AI Safety](https://arxiv.org/abs/2510.00821)
*Andrés Corrada-Emmanuel*

Main category: cs.AI

TL;DR: 本文提出了一种无监督评估分类器的逻辑框架，通过分析分类器之间的一致性和分歧来计算逻辑一致的群体评估结果，并构建了无需知识的警报系统来检测LLM评委是否违反用户设定的最低评分阈值。


<details>
  <summary>Details</summary>
Motivation: 探索分类器评估中一致性与分歧的不对称性，当专家意见分歧时我们可以确定他们不可能都100%正确，但当他们完全一致时无法排除任何可能的评估结果。

Method: 将分类器对齐决策的统计摘要作为线性规划问题的输入，在整数空间中计算可能的正确或错误响应，使用不等式约束（如正确响应数不能超过观察响应数）和适用于所有有限测试的线性等式公理。

Result: 开发了基于纯逻辑一致性的无监督评估方法，构建了无需知识的警报系统，能够检测LLM评委是否违反用户设定的最低评分阈值。

Conclusion: 该方法展示了仅通过逻辑一致性进行无监督评估的实际效用，为分类器评估提供了新的理论框架和实用工具。

Abstract: If two experts disagree on a test, we may conclude both cannot be 100 per
cent correct. But if they completely agree, no possible evaluation can be
excluded. This asymmetry in the utility of agreements versus disagreements is
explored here by formalizing a logic of unsupervised evaluation for
classifiers. Its core problem is computing the set of group evaluations that
are logically consistent with how we observe them agreeing and disagreeing in
their decisions. Statistical summaries of their aligned decisions are inputs
into a Linear Programming problem in the integer space of possible correct or
incorrect responses given true labels. Obvious logical constraints, such as,
the number of correct responses cannot exceed the number of observed responses,
are inequalities. But in addition, there are axioms, universally applicable
linear equalities that apply to all finite tests. The practical and immediate
utility of this approach to unsupervised evaluation using only logical
consistency is demonstrated by building no-knowledge alarms that can detect
when one or more LLMs-as-Judges are violating a minimum grading threshold
specified by the user.

</details>


### [97] [Benchmarking Machine Learning Models for Fault Classification and Localization in Power System Protection](https://arxiv.org/abs/2510.00831)
*Julian Oelhaf,Georg Kordowich,Changhun Kim,Paula Andrea Pérez-Toro,Christian Bergler,Andreas Maier,Johann Jäger,Siming Bayer*

Main category: cs.AI

TL;DR: 本文首次对电力系统保护中的故障分类和故障定位进行了经典机器学习模型的比较基准研究，基于EMT数据评估模型在实时约束下的性能。


<details>
  <summary>Details</summary>
Motivation: 随着分布式能源资源特别是可再生能源的集成增加，传统基于固定阈值的保护方案无法在动态条件下可靠识别和定位短路故障，而机器学习提供了有前景的替代方案，但缺乏系统性的模型和设置基准。

Method: 使用电压和电流波形，将其分割为10毫秒到50毫秒的滑动窗口，在现实的实时约束下评估经典机器学习模型。

Result: 最佳故障分类模型达到F1分数0.992±0.001，最佳故障定位模型达到R2 0.806±0.008，平均处理时间为0.563毫秒。

Conclusion: 机器学习模型在电力系统保护的故障分类和故障定位任务中表现出色，具有良好的准确性和实时处理能力。

Abstract: The increasing integration of distributed energy resources (DERs),
particularly renewables, poses significant challenges for power system
protection, with fault classification (FC) and fault localization (FL) being
among the most critical tasks. Conventional protection schemes, based on fixed
thresholds, cannot reliably identify and localize short circuits with the
increasing complexity of the grid under dynamic conditions. Machine learning
(ML) offers a promising alternative; however, systematic benchmarks across
models and settings remain limited. This work presents, for the first time, a
comparative benchmarking study of classical ML models for FC and FL in power
system protection based on EMT data. Using voltage and current waveforms
segmented into sliding windows of 10 ms to 50 ms, we evaluate models under
realistic real-time constraints. Performance is assessed in terms of accuracy,
robustness to window size, and runtime efficiency. The best-performing FC model
achieved an F1 score of 0.992$\pm$0.001, while the top FL model reached an R2
of 0.806$\pm$0.008 with a mean processing time of 0.563 ms.

</details>


### [98] [Improving Cryptocurrency Pump-and-Dump Detection through Ensemble-Based Models and Synthetic Oversampling Techniques](https://arxiv.org/abs/2510.00836)
*Jieun Yu,Minjung Park,Sangmi Chai*

Main category: cs.AI

TL;DR: 该研究应用SMOTE技术解决加密货币市场中Pump and Dump操纵检测的类别不平衡问题，通过集成学习模型实现了高召回率的检测性能。


<details>
  <summary>Details</summary>
Motivation: 加密货币市场中Pump and Dump操纵事件的稀缺性导致严重的类别不平衡，阻碍了准确的检测。

Method: 应用合成少数类过采样技术(SMOTE)，并评估先进的集成学习模型来区分操纵交易行为与正常市场活动。

Result: SMOTE显著提高了所有模型检测P&D事件的能力，XGBoost和LightGBM分别达到94.87%和93.59%的高召回率，具有强大的F1分数和快速计算性能。

Conclusion: 将数据平衡技术与集成方法相结合，显著改善了操纵活动的早期检测，有助于建立更公平、透明和稳定的加密货币市场。

Abstract: This study aims to detect pump and dump (P&D) manipulation in cryptocurrency
markets, where the scarcity of such events causes severe class imbalance and
hinders accurate detection. To address this issue, the Synthetic Minority
Oversampling Technique (SMOTE) was applied, and advanced ensemble learning
models were evaluated to distinguish manipulative trading behavior from normal
market activity. The experimental results show that applying SMOTE greatly
enhanced the ability of all models to detect P&D events by increasing recall
and improving the overall balance between precision and recall. In particular,
XGBoost and LightGBM achieved high recall rates (94.87% and 93.59%,
respectively) with strong F1-scores and demonstrated fast computational
performance, making them suitable for near real time surveillance. These
findings indicate that integrating data balancing techniques with ensemble
methods significantly improves the early detection of manipulative activities,
contributing to a fairer, more transparent, and more stable cryptocurrency
market.

</details>


### [99] [Learning Compact Representations of LLM Abilities via Item Response Theory](https://arxiv.org/abs/2510.00844)
*Jianhao Chen,Chenxu Wang,Gengrui Zhang,Peng Ye,Lei Bai,Wei Hu,Yuzhong Qu,Shuyue Hu*

Main category: cs.AI

TL;DR: 提出了一种基于项目反应理论(IRT)的LLM能力表示学习方法，通过混合专家网络学习模型能力向量和查询特征，在模型路由和性能预测任务中达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型数量激增，如何高效管理和利用这些资源成为重要挑战，需要学习紧凑的模型能力表示来支持下游任务。

Method: 受心理测量学中项目反应理论启发，将模型正确回答查询的概率建模为三个因素函数：模型多技能能力向量、查询区分度向量和查询难度标量，使用混合专家网络联合学习这些参数。

Result: 在模型路由和基准测试精度预测任务中达到最先进性能，分析验证学习到的参数能够编码有意义、可解释的模型能力和查询特征信息。

Conclusion: 该方法成功学习了紧凑且有意义的LLM能力表示，为高效管理和利用大量语言模型资源提供了有效解决方案。

Abstract: Recent years have witnessed a surge in the number of large language models
(LLMs), yet efficiently managing and utilizing these vast resources remains a
significant challenge. In this work, we explore how to learn compact
representations of LLM abilities that can facilitate downstream tasks, such as
model routing and performance prediction on new benchmarks. We frame this
problem as estimating the probability that a given model will correctly answer
a specific query. Inspired by the item response theory (IRT) in psychometrics,
we model this probability as a function of three key factors: (i) the model's
multi-skill ability vector, (2) the query's discrimination vector that
separates models of differing skills, and (3) the query's difficulty scalar. To
learn these parameters jointly, we introduce a Mixture-of-Experts (MoE) network
that couples model- and query-level embeddings. Extensive experiments
demonstrate that our approach leads to state-of-the-art performance in both
model routing and benchmark accuracy prediction. Moreover, analysis validates
that the learned parameters encode meaningful, interpretable information about
model capabilities and query characteristics.

</details>


### [100] [Unveiling Interesting Insights: Monte Carlo Tree Search for Knowledge Discovery](https://arxiv.org/abs/2510.00876)
*Pietro Totis,Alberto Pozanco,Daniel Borrajo*

Main category: cs.AI

TL;DR: 提出AIDE方法，使用蒙特卡洛树搜索(MCTS)实现自动化洞察和数据探索，解决数据到知识转化的难题。


<details>
  <summary>Details</summary>
Motivation: 组织收集大量数据但难以转化为可操作知识，自动化知识发现面临数据导航、模型构建和主观目标等复杂挑战。

Method: 基于蒙特卡洛树搜索(MCTS)的自动化洞察和数据探索框架AIDE，能够识别数据转换和模型来发现数据模式。

Result: 在真实世界和合成数据上验证了AIDE的有效性，能够发现有趣的数据模式，且框架具有良好的可扩展性。

Conclusion: AIDE为自动化知识发现提供了重要基础，未来可集成更多模式提取策略和领域知识，是迈向全面解决方案的有价值一步。

Abstract: Organizations are increasingly focused on leveraging data from their
processes to gain insights and drive decision-making. However, converting this
data into actionable knowledge remains a difficult and time-consuming task.
There is often a gap between the volume of data collected and the ability to
process and understand it, which automated knowledge discovery aims to fill.
Automated knowledge discovery involves complex open problems, including
effectively navigating data, building models to extract implicit relationships,
and considering subjective goals and knowledge. In this paper, we introduce a
novel method for Automated Insights and Data Exploration (AIDE), that serves as
a robust foundation for tackling these challenges through the use of Monte
Carlo Tree Search (MCTS). We evaluate AIDE using both real-world and synthetic
data, demonstrating its effectiveness in identifying data transformations and
models that uncover interesting data patterns. Among its strengths, AIDE's
MCTS-based framework offers significant extensibility, allowing for future
integration of additional pattern extraction strategies and domain knowledge.
This makes AIDE a valuable step towards developing a comprehensive solution for
automated knowledge discovery.

</details>


### [101] [FusionAdapter for Few-Shot Relation Learning in Multimodal Knowledge Graphs](https://arxiv.org/abs/2510.00894)
*Ran Liu,Yuan Fang,Xiaoli Li*

Main category: cs.AI

TL;DR: 提出了FusionAdapter方法，用于多模态知识图谱中的少样本关系学习，通过适配器模块和融合策略有效整合多模态信息，在低资源设置下提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有MMKG方法主要将多模态对齐到共享空间，忽视了特定模态的独特贡献，特别是在低资源设置下性能受限。

Method: 引入适配器模块使每个模态能够高效适应未见关系，并提出融合策略在整合多模态实体表示的同时保留模态特定特征。

Result: 在两个基准MMKG数据集上的广泛实验表明，FusionAdapter优于最先进的方法。

Conclusion: 通过有效适应和融合来自不同模态的信息，FusionAdapter在最小监督下提高了对新关系的泛化能力。

Abstract: Multimodal Knowledge Graphs (MMKGs) incorporate various modalities, including
text and images, to enhance entity and relation representations. Notably,
different modalities for the same entity often present complementary and
diverse information. However, existing MMKG methods primarily align modalities
into a shared space, which tends to overlook the distinct contributions of
specific modalities, limiting their performance particularly in low-resource
settings. To address this challenge, we propose FusionAdapter for the learning
of few-shot relationships (FSRL) in MMKG. FusionAdapter introduces (1) an
adapter module that enables efficient adaptation of each modality to unseen
relations and (2) a fusion strategy that integrates multimodal entity
representations while preserving diverse modality-specific characteristics. By
effectively adapting and fusing information from diverse modalities,
FusionAdapter improves generalization to novel relations with minimal
supervision. Extensive experiments on two benchmark MMKG datasets demonstrate
that FusionAdapter achieves superior performance over state-of-the-art methods.

</details>


### [102] [On Discovering Algorithms for Adversarial Imitation Learning](https://arxiv.org/abs/2510.00922)
*Shashank Reddy Chirra,Jayden Teoh,Praveen Paruchuri,Pradeep Varakantham*

Main category: cs.AI

TL;DR: 本文提出了DAIL（发现的对抗模仿学习），这是一种通过LLM引导的进化框架自动发现奖励分配函数的AIL方法，超越了人工设计的基线方法。


<details>
  <summary>Details</summary>
Motivation: 对抗模仿学习（AIL）方法虽然有效，但通常被认为不稳定。现有研究主要关注密度比估计的改进，而奖励分配函数对训练动态和最终策略性能的影响却被忽视。

Method: 采用LLM引导的进化框架来探索奖励分配函数空间，基于模仿策略的性能直接发现数据驱动的奖励分配函数，而不是依赖人工设计。

Result: DAIL在未见过的环境和策略优化算法上表现出良好的泛化能力，超越了当前最先进的人工设计基线方法，并实现了更稳定的训练。

Conclusion: DAIL是第一个元学习的AIL算法，通过自动发现奖励分配函数，提供了对AIL稳定性中奖励分配函数作用的新见解。

Abstract: Adversarial Imitation Learning (AIL) methods, while effective in settings
with limited expert demonstrations, are often considered unstable. These
approaches typically decompose into two components: Density Ratio (DR)
estimation $\frac{\rho_E}{\rho_{\pi}}$, where a discriminator estimates the
relative occupancy of state-action pairs under the policy versus the expert;
and Reward Assignment (RA), where this ratio is transformed into a reward
signal used to train the policy. While significant research has focused on
improving density estimation, the role of reward assignment in influencing
training dynamics and final policy performance has been largely overlooked. RA
functions in AIL are typically derived from divergence minimization objectives,
relying heavily on human design and ingenuity. In this work, we take a
different approach: we investigate the discovery of data-driven RA functions,
i.e, based directly on the performance of the resulting imitation policy. To
this end, we leverage an LLM-guided evolutionary framework that efficiently
explores the space of RA functions, yielding \emph{Discovered Adversarial
Imitation Learning} (DAIL), the first meta-learnt AIL algorithm. Remarkably,
DAIL generalises across unseen environments and policy optimization algorithms,
outperforming the current state-of-the-art of \emph{human-designed} baselines.
Finally, we analyse why DAIL leads to more stable training, offering novel
insights into the role of RA functions in the stability of AIL. Code is
publicly available: https://github.com/shshnkreddy/DAIL.

</details>


### [103] [Test-Time Search in Neural Graph Coarsening Procedures for the Capacitated Vehicle Routing Problem](https://arxiv.org/abs/2510.00958)
*Yoonju Sim,Hyeonah Kim,Changhyun Kwon*

Main category: cs.AI

TL;DR: 提出一种基于随机搜索的测试时增强方法，通过随机边选择和GraphCHiP算法改进CVRP中切割平面的生成效果，能够发现更多样化的RCIs和首次识别FCIs。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的切割平面分离方法生成的切割数量不足，主要是因为模型对生成多样化子集的敏感性不够。

Method: 1. 在图粗化过程中引入随机边选择替代贪心方法；2. 提出GraphCHiP算法，利用粗化历史识别RCIs和FCIs。

Result: 在随机生成的CVRP实例上，相比现有神经分离方法，本方法能有效减小对偶间隙，并在特定实例上成功识别出有效的FCIs。

Conclusion: 通过测试时随机搜索增强训练模型性能，能够生成更多样化的切割平面，提高CVRP求解效果。

Abstract: The identification of valid inequalities, such as the rounded capacity
inequalities (RCIs), is a key component of cutting plane methods for the
Capacitated Vehicle Routing Problem (CVRP). While a deep learning-based
separation method can learn to find high-quality cuts, our analysis reveals
that the model produces fewer cuts than expected because it is insufficiently
sensitive to generate a diverse set of generated subsets. This paper proposes
an alternative: enhancing the performance of a trained model at inference time
through a new test-time search with stochasticity. First, we introduce
stochastic edge selection into the graph coarsening procedure, replacing the
previously proposed greedy approach. Second, we propose the Graph Coarsening
History-based Partitioning (GraphCHiP) algorithm, which leverages coarsening
history to identify not only RCIs but also, for the first time, the Framed
capacity inequalities (FCIs). Experiments on randomly generated CVRP instances
demonstrate the effectiveness of our approach in reducing the dual gap compared
to the existing neural separation method. Additionally, our method discovers
effective FCIs on a specific instance, despite the challenging nature of
identifying such cuts.

</details>


### [104] [A Neuro-Fuzzy System for Interpretable Long-Term Stock Market Forecasting](https://arxiv.org/abs/2510.00960)
*Miha Ožbot,Igor Škrjanc,Vitomir Štruc*

Main category: cs.AI

TL;DR: 提出Fuzzformer模型，结合循环神经网络、多头自注意力和模糊推理系统，用于多变量股票市场数据的长期时间序列预测，在保持预测准确性的同时提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 在多变量时间序列预测中，同时实现准确性和可解释性是一个重大挑战。

Method: 使用LSTM网络和时间注意力机制将多变量数据压缩为适合模糊推理系统的可解释特征，结合多头自注意力和模糊推理系统构建Fuzzformer架构。

Result: 在S&P500股票市场指数上的实验表明，该模型与传统ARIMA和LSTM模型具有相当的预测性能，同时能提供网络内部有意义的信息流。

Conclusion: 该方法在可解释预测方面显示出潜力，虽然存在性能权衡，但在理解和预测股票市场行为方面具有实际应用价值。

Abstract: In the complex landscape of multivariate time series forecasting, achieving
both accuracy and interpretability remains a significant challenge. This paper
introduces the Fuzzy Transformer (Fuzzformer), a novel recurrent neural network
architecture combined with multi-head self-attention and fuzzy inference
systems to analyze multivariate stock market data and conduct long-term time
series forecasting. The method leverages LSTM networks and temporal attention
to condense multivariate data into interpretable features suitable for fuzzy
inference systems. The resulting architecture offers comparable forecasting
performance to conventional models such as ARIMA and LSTM while providing
meaningful information flow within the network. The method was examined on the
real world stock market index S\&P500. Initial results show potential for
interpretable forecasting and identify current performance tradeoffs,
suggesting practical application in understanding and forecasting stock market
behavior.

</details>


### [105] [QUASAR: Quantum Assembly Code Generation Using Tool-Augmented LLMs via Agentic RL](https://arxiv.org/abs/2510.00967)
*Cong Yu,Valter Uotila,Shilong Deng,Qingyuan Wu,Tuo Shi,Songlin Jiang,Lei You,Bo Zhao*

Main category: cs.AI

TL;DR: QUASAR是一个基于工具增强大语言模型的强化学习框架，用于生成和优化量子电路，通过量子电路验证和分层奖励机制解决LLM生成量子电路的质量问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成的量子电路存在参数精度不足和领域知识缺乏的问题，导致电路质量低下。需要开发能够结合量子领域专业知识的自动化量子电路生成方法。

Method: 提出QUASAR框架，结合工具增强的LLM和强化学习，包含量子电路验证方法和分层奖励机制，确保生成的量子电路在语法和语义上的正确性。

Result: 在4B参数LLM上，QUASAR在Pass@1和Pass@10上分别达到99.31%和100%的有效性，优于GPT-4o、GPT-5、DeepSeek-V3等工业级LLM以及仅使用监督微调或强化学习的基线方法。

Conclusion: QUASAR通过结合工具增强LLM和强化学习，有效解决了量子电路生成中的参数精度和领域知识问题，显著提升了生成电路的质量和有效性。

Abstract: Designing and optimizing task-specific quantum circuits are crucial to
leverage the advantage of quantum computing. Recent large language model
(LLM)-based quantum circuit generation has emerged as a promising automatic
solution. However, the fundamental challenges remain unaddressed: (i)
parameterized quantum gates require precise numerical values for optimal
performance, which also depend on multiple aspects, including the number of
quantum gates, their parameters, and the layout/depth of the circuits. (ii)
LLMs often generate low-quality or incorrect quantum circuits due to the lack
of quantum domain-specific knowledge. We propose QUASAR, an agentic
reinforcement learning (RL) framework for quantum circuits generation and
optimization based on tool-augmented LLMs. To align the LLM with
quantum-specific knowledge and improve the generated quantum circuits, QUASAR
designs (i) a quantum circuit verification approach with external quantum
simulators and (ii) a sophisticated hierarchical reward mechanism in RL
training. Extensive evaluation shows improvements in both syntax and semantic
performance of the generated quantum circuits. When augmenting a 4B LLM, QUASAR
has achieved the validity of 99.31% in Pass@1 and 100% in Pass@10,
outperforming industrial LLMs of GPT-4o, GPT-5 and DeepSeek-V3 and several
supervised-fine-tuning (SFT)-only and RL-only baselines.

</details>


### [106] [Adaptive Federated Few-Shot Rare-Disease Diagnosis with Energy-Aware Secure Aggregation](https://arxiv.org/abs/2510.00976)
*Aueaphum Aueawatthanaphisut*

Main category: cs.AI

TL;DR: AFFR框架整合了元学习联邦优化、能量感知客户端调度和安全聚合，在罕见病诊断中实现了10%的准确率提升和50%的客户端退出率降低。


<details>
  <summary>Details</summary>
Motivation: 解决罕见病诊断中的数据稀缺、隐私保护和边缘设备资源有限等挑战。

Method: 结合三个支柱：基于元学习的少样本联邦优化、能量感知客户端调度、以及带校准差分隐私的安全聚合。

Result: 在模拟罕见病检测数据集上，相比基线联邦学习准确率提升10%，客户端退出率降低50%以上，隐私-效用权衡在临床可接受范围内。

Conclusion: AFFR为罕见病的公平可信联邦诊断提供了实用路径。

Abstract: Rare-disease diagnosis remains one of the most pressing challenges in digital
health, hindered by extreme data scarcity, privacy concerns, and the limited
resources of edge devices. This paper proposes the Adaptive Federated Few-Shot
Rare-Disease Diagnosis (AFFR) framework, which integrates three pillars: (i)
few-shot federated optimization with meta-learning to generalize from limited
patient samples, (ii) energy-aware client scheduling to mitigate device
dropouts and ensure balanced participation, and (iii) secure aggregation with
calibrated differential privacy to safeguard sensitive model updates. Unlike
prior work that addresses these aspects in isolation, AFFR unifies them into a
modular pipeline deployable on real-world clinical networks. Experimental
evaluation on simulated rare-disease detection datasets demonstrates up to 10%
improvement in accuracy compared with baseline FL, while reducing client
dropouts by over 50% without degrading convergence. Furthermore,
privacy-utility trade-offs remain within clinically acceptable bounds. These
findings highlight AFFR as a practical pathway for equitable and trustworthy
federated diagnosis of rare conditions.

</details>


### [107] [Integrating AI and Ensemble Forecasting: Explainable Materials Planning with Scorecards and Trend Insights for a Large-Scale Manufacturer](https://arxiv.org/abs/2510.01006)
*Saravanan Venkatachalam*

Main category: cs.AI

TL;DR: 提出一个售后需求预测和监控的实用架构，结合统计、机器学习和深度学习模型，通过角色驱动的分析层提供评分卡和趋势诊断，使用LLM生成业务洞察。


<details>
  <summary>Details</summary>
Motivation: 解决售后需求预测的复杂性，统一多种模型，处理外生信号，并将COVID-19作为特殊机制，为决策者提供可操作的洞察。

Method: 使用收入感知和聚类感知的集成模型，结合外生信号，采用Pareto分割策略，通过LLM生成角色化叙述和标准化报告。

Result: 系统在90多个国家和约6000个零件上实现可复现的工作流，提供校准的预测区间和业务相关的损失对齐。

Conclusion: 该架构成功连接预测、监控和库存决策，使规划者从关注当前准确性转向预测未来趋势和识别关键杠杆。

Abstract: This paper presents a practical architecture for after-sales demand
forecasting and monitoring that unifies a revenue- and cluster-aware ensemble
of statistical, machine-learning, and deep-learning models with a role-driven
analytics layer for scorecards and trend diagnostics. The framework ingests
exogenous signals (installed base, pricing, macro indicators, life cycle,
seasonality) and treats COVID-19 as a distinct regime, producing country-part
forecasts with calibrated intervals. A Pareto-aware segmentation forecasts
high-revenue items individually and pools the long tail via clusters, while
horizon-aware ensembling aligns weights with business-relevant losses (e.g.,
WMAPE). Beyond forecasts, a performance scorecard delivers decision-focused
insights: accuracy within tolerance thresholds by revenue share and count, bias
decomposition (over- vs under-forecast), geographic and product-family
hotspots, and ranked root causes tied to high-impact part-country pairs. A
trend module tracks trajectories of MAPE/WMAPE and bias across recent months,
flags entities that are improving or deteriorating, detects change points
aligned with known regimes, and attributes movements to lifecycle and seasonal
factors. LLMs are embedded in the analytics layer to generate role-aware
narratives and enforce reporting contracts. They standardize business
definitions, automate quality checks and reconciliations, and translate
quantitative results into concise, explainable summaries for planners and
executives. The system exposes a reproducible workflow -- request
specification, model execution, database-backed artifacts, and AI-generated
narratives -- so planners can move from "How accurate are we now?" to "Where is
accuracy heading and which levers should we pull?", closing the loop between
forecasting, monitoring, and inventory decisions across more than 90 countries
and about 6,000 parts.

</details>


### [108] [Shape Happens: Automatic Feature Manifold Discovery in LLMs via Supervised Multi-Dimensional Scaling](https://arxiv.org/abs/2510.01025)
*Federico Tiblias,Irina Bigoulaeva,Jingcheng Niu,Simone Balloccu,Iryna Gurevych*

Main category: cs.AI

TL;DR: 本文提出SMDS方法，自动发现语言模型中特征流形的几何结构，并以时间推理为例展示了这些结构如何支持模型推理。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注特定特征的特定几何结构，缺乏通用性。本文旨在开发一种模型无关的方法来自动发现特征流形。

Method: 引入监督多维缩放(SMDS)方法，该方法能够自动发现特征流形，并以时间推理作为案例研究。

Result: 发现不同特征形成各种几何结构（如圆形、直线、簇群），这些结构能反映概念特性、在不同模型中保持稳定、支持模型推理，并随上下文动态重塑。

Conclusion: 特征流形在语言模型中具有功能性作用，支持基于实体的推理模型，其中语言模型编码和转换结构化表示。

Abstract: The linear representation hypothesis states that language models (LMs) encode
concepts as directions in their latent space, forming organized,
multidimensional manifolds. Prior efforts focus on discovering specific
geometries for specific features, and thus lack generalization. We introduce
Supervised Multi-Dimensional Scaling (SMDS), a model-agnostic method to
automatically discover feature manifolds. We apply SMDS to temporal reasoning
as a case study, finding that different features form various geometric
structures such as circles, lines, and clusters. SMDS reveals many insights on
these structures: they consistently reflect the properties of the concepts they
represent; are stable across model families and sizes; actively support
reasoning in models; and dynamically reshape in response to context changes.
Together, our findings shed light on the functional role of feature manifolds,
supporting a model of entity-based reasoning in which LMs encode and transform
structured representations.

</details>


### [109] [Uncovering the Computational Ingredients of Human-Like Representations in LLMs](https://arxiv.org/abs/2510.01030)
*Zach Studdiford,Timothy T. Rogers,Kushin Mukherjee,Siddharth Suresh*

Main category: cs.AI

TL;DR: 该研究评估了70多个不同架构的LLM在概念表示上与人类的对齐程度，发现指令微调和注意力头维度是影响对齐的关键因素，而现有基准测试无法充分衡量这种对齐。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的快速发展带来了多种计算要素，但哪些要素对构建具有人类类似表示能力的模型最关键尚不清楚，且现有基准测试不适合衡量人类与模型之间的表示对齐。

Method: 使用认知科学中成熟的三元组相似性任务，基于THINGS数据库的概念，评估了70多个不同计算要素的模型，比较人类和模型的表示对齐程度。

Result: 发现经过指令微调的模型和具有更大注意力头维度的模型与人类表示最对齐，而多模态预训练和参数大小对对齐影响有限；现有基准测试中MMLU比MUSR更适合捕捉表示对齐，但都无法完全解释对齐分数的方差。

Conclusion: 研究确定了推进LLM成为人类概念表示模型的关键计算要素，并指出了LLM评估中的基准测试不足问题。

Abstract: The ability to translate diverse patterns of inputs into structured patterns
of behavior has been thought to rest on both humans' and machines' ability to
learn robust representations of relevant concepts. The rapid advancement of
transformer-based large language models (LLMs) has led to a diversity of
computational ingredients -- architectures, fine tuning methods, and training
datasets among others -- but it remains unclear which of these ingredients are
most crucial for building models that develop human-like representations.
Further, most current LLM benchmarks are not suited to measuring
representational alignment between humans and models, making benchmark scores
unreliable for assessing if current LLMs are making progress towards becoming
useful cognitive models. We address these limitations by first evaluating a set
of over 70 models that widely vary in their computational ingredients on a
triplet similarity task, a method well established in the cognitive sciences
for measuring human conceptual representations, using concepts from the THINGS
database. Comparing human and model representations, we find that models that
undergo instruction-finetuning and which have larger dimensionality of
attention heads are among the most human aligned, while multimodal pretraining
and parameter size have limited bearing on alignment. Correlations between
alignment scores and scores on existing benchmarks reveal that while some
benchmarks (e.g., MMLU) are better suited than others (e.g., MUSR) for
capturing representational alignment, no existing benchmark is capable of fully
accounting for the variance of alignment scores, demonstrating their
insufficiency in capturing human-AI alignment. Taken together, our findings
help highlight the computational ingredients most essential for advancing LLMs
towards models of human conceptual representation and address a key
benchmarking gap in LLM evaluation.

</details>


### [110] [Activation-Deactivation: A General Framework for Robust Post-hoc Explainable AI](https://arxiv.org/abs/2510.01038)
*Akchunya Chanchal,David A. Kelly,Hana Chockler*

Main category: cs.AI

TL;DR: 提出了一种新的前向传播范式Activation-Deactivation(AD)，通过关闭模型中与遮挡部分对应的组件来消除遮挡输入特征对模型决策的影响，从而生成更鲁棒的解释。


<details>
  <summary>Details</summary>
Motivation: 现有的黑盒可解释性方法依赖通过遮挡部分输入获得的突变体，这会导致分布外图像，从而影响解释质量。此外，选择合适的遮挡值通常需要领域知识。

Method: 提出了ConvAD机制，可以轻松添加到任何训练好的卷积神经网络中，实现AD范式。该方法不需要额外训练或微调，且不会改变网络的决策过程。

Result: 在多个数据集和模型架构上的实验表明，与使用遮挡方法获得的解释相比，AD解释在鲁棒性方面有显著提升（最高达62.5%），且不需要领域知识。

Conclusion: ConvAD能够提取更鲁棒的解释，消除了对领域知识的依赖，为图像分类器的可解释性提供了更可靠的解决方案。

Abstract: Black-box explainability methods are popular tools for explaining the
decisions of image classifiers. A major drawback of these tools is their
reliance on mutants obtained by occluding parts of the input, leading to
out-of-distribution images. This raises doubts about the quality of the
explanations. Moreover, choosing an appropriate occlusion value often requires
domain knowledge. In this paper we introduce a novel forward-pass paradigm
Activation-Deactivation (AD), which removes the effects of occluded input
features from the model's decision-making by switching off the parts of the
model that correspond to the occlusions. We introduce ConvAD, a drop-in
mechanism that can be easily added to any trained Convolutional Neural Network
(CNN), and which implements the AD paradigm. This leads to more robust
explanations without any additional training or fine-tuning. We prove that the
ConvAD mechanism does not change the decision-making process of the network. We
provide experimental evaluation across several datasets and model
architectures. We compare the quality of AD-explanations with explanations
achieved using a set of masking values, using the proxies of robustness, size,
and confidence drop-off. We observe a consistent improvement in robustness of
AD explanations (up to 62.5%) compared to explanations obtained with
occlusions, demonstrating that ConvAD extracts more robust explanations without
the need for domain knowledge.

</details>


### [111] [Typed Chain-of-Thought: A Curry-Howard Framework for Verifying LLM Reasoning](https://arxiv.org/abs/2510.01069)
*Elija Perrier*

Main category: cs.AI

TL;DR: 提出基于Curry-Howard对应的理论框架，将CoT推理过程映射为形式化类型证明结构，为推理忠实性提供可验证的证书。


<details>
  <summary>Details</summary>
Motivation: CoT提示增强了大型语言模型的推理能力，但生成的推理过程的忠实性仍然是模型可解释性的开放问题。

Method: 基于Curry-Howard对应关系，将非正式的自然语言CoT步骤提取并映射到形式化的类型证明结构中。

Result: 成功将CoT轨迹转换为良好类型的证明，可作为计算忠实性的强有力可验证证书。

Conclusion: 该框架提供了一种将可信叙事解释转化为形式化可验证程序的方法，为构建更可靠和可信的AI系统提供了路径。

Abstract: While Chain-of-Thought (CoT) prompting enhances the reasoning capabilities of
large language models, the faithfulness of the generated rationales remains an
open problem for model interpretability. We propose a novel theoretical lens
for this problem grounded in the Curry-Howard correspondence, which posits a
direct relationship between formal proofs and computer programs. Under this
paradigm, a faithful reasoning trace is analogous to a well-typed program,
where each intermediate step corresponds to a typed logical inference. We
operationalise this analogy, presenting methods to extract and map the
informal, natural language steps of CoT into a formal, typed proof structure.
Successfully converting a CoT trace into a well-typed proof serves as a strong,
verifiable certificate of its computational faithfulness, moving beyond
heuristic interpretability towards formal verification. Our framework provides
a methodology to transform plausible narrative explanations into formally
verifiable programs, offering a path towards building more reliable and
trustworthy AI systems.

</details>


### [112] [Safety Instincts: LLMs Learn to Trust Their Internal Compass for Self-Defense](https://arxiv.org/abs/2510.01088)
*Guobin Shen,Dongcheng Zhao,Haibo Tong,Jindong Li,Feifei Zhao,Yi Zeng*

Main category: cs.AI

TL;DR: 提出SIRL方法，利用LLM内部的安全置信度作为奖励信号进行强化学习，无需外部验证器即可提升模型安全防御能力。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏通用标准和可靠的内容验证器，确保LLM安全面临挑战，难以获得有效的训练信号。研究发现对齐模型已具备内部安全信念，在拒绝有害请求时表现出高置信度。

Method: 引入Safety Instincts Reinforcement Learning (SIRL)，将模型内部置信度转化为自生成奖励信号，通过强化低熵拒绝行为来教导模型信任其安全本能。

Result: 在Llama和Qwen模型上评估，SIRL对20+种越狱方法保持89%+的防御成功率，仅使用15,000个未标注提示即可超越资源密集的监督方法，同时在数学、编程和对话基准上保持性能。

Conclusion: 有效对齐可以从模型内部产生，为无需大量人工监督的自主、鲁棒AI安全机制开辟了道路。

Abstract: Ensuring Large Language Model (LLM) safety remains challenging due to the
absence of universal standards and reliable content validators, making it
difficult to obtain effective training signals. We discover that aligned models
already possess robust internal safety beliefs: they consistently produce
high-confidence refusals to harmful requests while exhibiting high entropy when
generating potentially dangerous content. This entropy gap reveals an untapped
signal--models intrinsically "know" when to refuse. We introduce Safety
Instincts Reinforcement Learning (SIRL), which transforms this internal
confidence into a self-generated reward signal, eliminating dependence on
external validators or human annotations. SIRL teaches models to trust their
safety instincts by reinforcing low-entropy refusal behaviors. Evaluated on
Llama and Qwen models, SIRL maintains 89%+ Defense Success Rates (DSRs) against
20+ jailbreak methods, from static prompts to adaptive attacks. Using only
15,000 unlabeled prompts, SIRL surpasses resource-intensive supervised methods
while preserving performance on mathematics, coding, and conversation
benchmarks. Our work demonstrates that effective alignment can emerge from
within, paving the way for more autonomous and robust AI safety mechanisms that
scale without extensive human oversight.

</details>


### [113] [Optimizing Fairness in Production Planning: A Human-Centric Approach to Machine and Workforce Allocation](https://arxiv.org/abs/2510.01094)
*Alexander Nasuta,Alessandro Cisi,Sylwia Olbrych,Gustavo Vieira,Rui Fernandes,Lucas Paletta,Marlene Mayr,Rishyank Chevuri,Robert Woitsch,Hans Aoyang Zhou,Anas Abdelrazeq,Robert H. Schmitt*

Main category: cs.AI

TL;DR: 提出一个两层的人本生产规划框架，结合约束规划(CP)和马尔可夫决策过程(MDP)，同时优化运营效率和劳动力公平性。


<details>
  <summary>Details</summary>
Motivation: 在工业制造中平衡运营效率和劳动力公平性，考虑工人偏好、经验、韧性和医疗约束等人本因素。

Method: 第一层使用约束规划进行订单产线分配，第二层使用MDP进行工人产线分配，比较了贪婪分配、MCTS和RL三种策略。

Result: CP调度产生紧凑可行的生产计划，MDP工人分配显著提高公平性和偏好对齐，领域专家评价有效。

Conclusion: CP与学习型决策相结合为人本生产规划提供了稳健方法，能同时优化吞吐量和劳动力福祉。

Abstract: This work presents a two-layer, human-centric production planning framework
designed to optimize both operational efficiency and workforce fairness in
industrial manufacturing. The first layer formulates the Order-Line allocation
as a Constraint Programming (CP) problem, generating high-utilization
production schedules that respect machine capacities, processing times, and due
dates. The second layer models Worker-Line allocation as a Markov Decision
Process (MDP), integrating human factors such as worker preference, experience,
resilience, and medical constraints into the assignment process. Three solution
strategies, greedy allocation, MCTS, and RL, are implemented and compared
across multiple evaluation scenarios. The proposed system is validated through
16 test sessions with domain experts from the automotive industry, combining
quantitative key performance indicators (KPIs) with expert ratings. Results
indicate that the CP-based scheduling approach produces compact, feasible
production plans with low tardiness, while the MDP-based worker allocation
significantly improves fairness and preference alignment compared to baseline
approaches. Domain experts rated both the Order-Line and Worker-Line components
as effective and highlighted opportunities to further refine the objective
function to penalize excessive earliness and improve continuity in worker
assignments. Overall, the findings demonstrate that combining CP with
learning-based decision-making provides a robust approach for human-centric
production planning. The approach enables simultaneous optimization of
throughput and workforce well-being, offering a practical foundation for fair
and efficient manufacturing scheduling in industrial settings.

</details>


### [114] [PRISM-Consult: A Panel-of-Experts Architecture for Clinician-Aligned Diagnosis](https://arxiv.org/abs/2510.01114)
*Lionel Levine,John Santerre,Alexander S. Young,T. Barry Levine,Francis Campion,Majid Sarrafzadeh*

Main category: cs.AI

TL;DR: PRISM-Consult是一个临床医生对齐的专家小组架构，通过轻量级路由器将急诊病例分发给特定领域的专家模型，实现了参数效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 为了在急诊科实现安全、可审计且低延迟的大规模咨询，需要开发一个能够高效处理多领域临床事件的专家系统。

Method: 采用专家小组架构，使用轻量级路由器读取前几个标记，将病例分发给五个专业领域的专家模型（心血管、肺、胃肠、肌肉骨骼、心理），每个专家模型继承PRISM的小型transformer骨干和标记模板。

Result: 在真实世界急诊科队列中，专家模型在各领域表现出平滑收敛和低开发困惑度，路由器在安全优先策略下实现高质量路由和大量计算节省。

Conclusion: 该框架为安全、可审计、低延迟的大规模临床咨询提供了实用路径，并通过外部/时间复制、非对称生命威胁阈值和多标签仲裁等验证步骤满足前瞻性临床部署标准。

Abstract: We present PRISM-Consult, a clinician-aligned panel-of-experts architecture
that extends the compact PRISM sequence model into a routed family of domain
specialists. Episodes are tokenized as structured clinical events; a
light-weight router reads the first few tokens and dispatches to specialist
models (Cardiac-Vascular, Pulmonary, Gastro-Oesophageal, Musculoskeletal,
Psychogenic). Each specialist inherits PRISM's small transformer backbone and
token template, enabling parameter efficiency and interpretability. On
real-world Emergency Department cohorts, specialists exhibit smooth convergence
with low development perplexities across domains, while the router achieves
high routing quality and large compute savings versus consult-all under a
safety-first policy. We detail the data methodology (initial vs. conclusive
ICD-9 families), routing thresholds and calibration, and report per-domain
results to avoid dominance by common events. The framework provides a practical
path to safe, auditable, and low-latency consult at scale, and we outline
validation steps-external/temporal replication, asymmetric life-threat
thresholds, and multi-label arbitration-to meet prospective clinical deployment
standards.

</details>


### [115] [Exploring Network-Knowledge Graph Duality: A Case Study in Agentic Supply Chain Risk Analysis](https://arxiv.org/abs/2510.01115)
*Evan Heus,Rick Bookstaber,Dhruv Sharma*

Main category: cs.AI

TL;DR: 提出了一个基于LLM的智能体框架，用于供应链风险分析，通过将供应链网络视为知识图谱，利用网络中心性评分指导图遍历，结合数值因子表和新闻流数据，使用上下文模板使定量数据对LLM可理解，无需微调或专用图数据库即可生成实时风险叙述。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理复杂、多模态、网络原生金融风险数据时存在困难，标准检索增强生成方法过度简化关系，而专业模型成本高且静态，需要解决这一差距。

Method: 将供应链网络视为知识图谱，利用网络科学原理进行检索；图遍历器根据网络中心性评分提取经济显著风险路径；智能体架构协调图检索与数值因子表、新闻流数据；使用上下文模板将原始数据嵌入自然语言描述中。

Result: 该轻量级方法使模型能够生成简洁、可解释、上下文丰富的实时风险叙述，无需昂贵的微调或专用图数据库。

Conclusion: 通过利用网络与知识图谱之间的内在对偶性，结合智能体架构和上下文模板，成功构建了一个高效、可解释的供应链风险分析框架，克服了现有方法的局限性。

Abstract: Large Language Models (LLMs) struggle with the complex, multi-modal, and
network-native data underlying financial risk. Standard Retrieval-Augmented
Generation (RAG) oversimplifies relationships, while specialist models are
costly and static. We address this gap with an LLM-centric agent framework for
supply chain risk analysis. Our core contribution is to exploit the inherent
duality between networks and knowledge graphs (KG). We treat the supply chain
network as a KG, allowing us to use structural network science principles for
retrieval. A graph traverser, guided by network centrality scores, efficiently
extracts the most economically salient risk paths. An agentic architecture
orchestrates this graph retrieval alongside data from numerical factor tables
and news streams. Crucially, it employs novel ``context shells'' -- descriptive
templates that embed raw figures in natural language -- to make quantitative
data fully intelligible to the LLM. This lightweight approach enables the model
to generate concise, explainable, and context-rich risk narratives in real-time
without costly fine-tuning or a dedicated graph database.

</details>


### [116] [Apriel-1.5-15b-Thinker](https://arxiv.org/abs/2510.01141)
*Shruthan Radhakrishna,Aman Tiwari,Aanjaneya Shukla,Masoud Hashemi,Rishabh Maheshwary,Shiva Krishna Reddy Malay,Jash Mehta,Pulkit Pattnaik,Saloni Mittal,Khalil Slimi,Kelechi Ogueji,Akintunde Oladipo,Soham Parikh,Oluwanifemi Bamgbose,Toby Liang,Ahmed Masry,Khyati Mahajan,Sai Rajeswar Mudumba,Vikas Yadav,Sathwik Tejaswi Madhusudhan,Torsten Scholak,Sagar Davasam,Srinivas Sunkara,Nicholas Chapados*

Main category: cs.AI

TL;DR: Apriel-1.5-15B-Thinker是一个15亿参数的多模态推理模型，通过渐进式三阶段训练方法实现前沿性能，无需大规模计算资源。


<details>
  <summary>Details</summary>
Motivation: 旨在证明通过精心设计的训练方法而非单纯扩大规模，可以在有限计算资源下实现前沿水平的多模态推理能力，使更多组织能够使用先进模型。

Method: 采用三阶段渐进方法：1）深度扩展推理能力；2）分阶段持续预训练，先建立文本和视觉基础理解，再通过合成数据增强视觉推理；3）高质量文本监督微调，涵盖数学、编程、科学和工具使用。

Result: 在Artificial Analysis Intelligence Index上获得52分，与DeepSeek-R1-0528相当；在十个图像基准测试中，性能平均仅比Gemini-2.5-Flash和Claude Sonnet-3.7低5分，可在单GPU上部署。

Conclusion: 精心设计的中期训练方法可以在不依赖大规模计算的情况下显著缩小能力差距，使前沿多模态推理技术对资源有限的组织更加可及。

Abstract: We present Apriel-1.5-15B-Thinker, a 15-billion parameter open-weights
multimodal reasoning model that achieves frontier-level performance through
training design rather than sheer scale. Starting from Pixtral-12B, we apply a
progressive three-stage methodology: (1) depth upscaling to expand reasoning
capacity without pretraining from scratch, (2) staged continual pre-training
that first develops foundational text and vision understanding, then enhances
visual reasoning through targeted synthetic data generation addressing spatial
structure, compositional understanding, and fine-grained perception, and (3)
high-quality text-only supervised fine-tuning on curated instruction-response
pairs with explicit reasoning traces spanning mathematics, coding, science, and
tool use. Notably, our model achieves competitive results without reinforcement
learning or preference optimization, isolating the contribution of our
data-centric continual pre-training approach. On the Artificial Analysis
Intelligence Index, Apriel-1.5-15B-Thinker attains a score of 52, matching
DeepSeek-R1-0528 despite requiring significantly fewer computational resources.
Across ten image benchmarks, its performance is on average within five points
of Gemini-2.5-Flash and Claude Sonnet-3.7, a key achievement for a model
operating within single-GPU deployment constraints. Our results demonstrate
that thoughtful mid-training 2 design can close substantial capability gaps
without massive scale, making frontier-level multimodal reasoning accessible to
organizations with limited infrastructure. We release the model checkpoint, all
training recipes, and evaluation protocols under the MIT license to to advance
open-source research.

</details>


### [117] [Generalized Parallel Scaling with Interdependent Generations](https://arxiv.org/abs/2510.01143)
*Harry Dong,David Brandfonbrener,Eryk Helenowski,Yun He,Mrinal Kumar,Han Fang,Yuejie Chi,Karthik Abinav Sankararaman*

Main category: cs.AI

TL;DR: Bridge方法通过重新思考批量LLM隐藏状态作为整体张量而非独立切片，在并行LLM推理中生成相互依赖的响应，仅需少量新参数即可显著提升响应质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 传统并行LLM推理中，N个并行响应通常相互独立生成，这导致计算资源分配不均且无法充分利用各生成响应中的有用信息。

Method: 提出Bridge方法，将批量LLM隐藏状态视为整体张量而非独立切片，通过仅添加少量新参数（2.8%-5.1%）来实现响应间的相互依赖生成。

Result: Bridge将基于可验证奖励的强化学习的相对平均准确率增益提升高达50%，并提高了正确响应的一致性。该方法一次训练即可扩展到任何生成宽度，性能始终优于独立生成。

Conclusion: Bridge解锁了一种更通用的并行扩展模式，有效利用序列间的信息，兼容任何后生成聚合技术。

Abstract: Parallel LLM inference scaling involves sampling a set of $N>1$ responses for
a single input prompt. However, these $N$ parallel responses tend to be
generated independently from each other, partitioning compute resources and
leaving potentially useful information in one generation untapped by others.
This is in contrast to response length scaling where past computation is used
in all future steps. For higher quality responses and response sets, we propose
Bridge to generate interdependent responses in parallel by rethinking batched
LLM hidden states as holistic tensors rather than independent slices. With only
a small amount (2.8%-5.1%) of new parameters, Bridge improves the relative mean
accuracy gains from reinforcement learning with verifiable rewards by up to 50%
and boosts consistency of correct responses. Trained once, Bridge scales to any
generation width, all with greater performance than independent generations,
unlocking a more general mode of parallel scaling that effectively leverages
information between sequences, compatible with any post-generation aggregation
technique.

</details>
