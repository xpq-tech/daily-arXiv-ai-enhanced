<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 28]
- [cs.AI](#cs.AI) [Total: 23]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Test-time Scaling of LLMs: A Survey from A Subproblem Structure Perspective](https://arxiv.org/abs/2511.14772)
*Zhuoyi Yang,Xu Guo,Tong Zhang,Huijuan Xu,Boyang Li*

Main category: cs.CL

TL;DR: 本文综述了通过增加推理时计算来提升预训练大语言模型预测准确性的方法，重点分析了问题分解方式和子问题拓扑结构，统一了多种推理增强技术。


<details>
  <summary>Details</summary>
Motivation: 预训练大语言模型在推理时可以通过分配额外计算资源来提升预测准确性，但现有方法缺乏系统性的分类和分析框架。

Method: 基于问题分解方式和子问题拓扑结构（顺序、并行、树状）对测试时扩展方法进行分类，将Chain-of-Thought、Branch-Solve-Merge、Tree-of-Thought等方法统一在同一框架下。

Result: 建立了统一的分类框架，综合分析了各种方法的优缺点，揭示了不同拓扑结构在推理增强中的适用场景。

Conclusion: 提出了基于问题分解和拓扑结构的统一分析框架，为未来研究指明了有前景的方向。

Abstract: With this paper, we survey techniques for improving the predictive accuracy of pretrained large language models by allocating additional compute at inference time. In categorizing test-time scaling methods, we place special emphasis on how a problem is decomposed into subproblems and on the topological organization of these subproblems whether sequential, parallel, or tree-structured. This perspective allows us to unify diverse approaches such as Chain-of-Thought, Branch-Solve-Merge, and Tree-of-Thought under a common lens. We further synthesize existing analyses of these techniques, highlighting their respective strengths and weaknesses, and conclude by outlining promising directions for future research

</details>


### [2] [Temporal Predictors of Outcome in Reasoning Language Models](https://arxiv.org/abs/2511.14773)
*Joey David*

Main category: cs.CL

TL;DR: 研究表明，大型语言模型在推理过程中很早就能预测最终答案的正确性，即使需要更长的输出才能得到确定答案。对于难题，预测准确率下降揭示了选择偏差：难题在长推理链中比例过高。


<details>
  <summary>Details</summary>
Motivation: 探索链式思维推理中，大型语言模型在何时内部承诺最终结果，以及这种早期预测能力对模型可解释性和推理时控制的影响。

Method: 通过在推理过程的前t个token后的隐藏状态上训练线性分类器，来预测最终答案的正确性。

Result: 模型在仅经过几个推理token后就能高度预测最终答案的正确性。对于难题，预测准确率下降，表明难题在长推理链中比例过高。

Conclusion: 推理模型在仅几个token后就出现了内部自我评估成功的能力，这对可解释性和推理时控制具有重要意义。

Abstract: The chain-of-thought (CoT) paradigm uses the elicitation of step-by-step rationales as a proxy for reasoning, gradually refining the model's latent representation of a solution. However, it remains unclear just how early a Large Language Model (LLM) internally commits to an eventual outcome. We probe this by training linear classifiers on hidden states after the first t reasoning tokens, showing that eventual correctness is highly predictable after only a few tokens, even when longer outputs are needed to reach a definite answer. We show that, for harder questions, a drop in predictive accuracy highlights a selection artifact: hard items are disproportionately represented in long CoTs. Overall, our results imply that for reasoning models, internal self-assessment of success tends to emerge after only a few tokens, with implications for interpretability and for inference-time control.

</details>


### [3] [LiveCLKTBench: Towards Reliable Evaluation of Cross-Lingual Knowledge Transfer in Multilingual LLMs](https://arxiv.org/abs/2511.14774)
*Pei-Fu Guo,Yun-Da Tsai,Chun-Chia Hsu,Kai-Xin Chen,Ya-An Tsai,Kai-Wei Chang,Nanyun Peng,Mi-Yen Yeh,Shou-De Lin*

Main category: cs.CL

TL;DR: LiveCLKTBench是一个自动化生成管道，专门用于隔离和测量大型语言模型的跨语言知识迁移，通过时间敏感的知识实体来评估真实的知识转移而非预训练暴露。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型的跨语言知识迁移具有挑战性，因为目标语言中的正确答案可能来自真正的知识迁移，也可能来自预训练期间的先前暴露。需要一种方法来区分这两种情况。

Method: 开发LiveCLKTBench自动化生成管道，从真实领域识别自包含、时间敏感的知识实体，基于时间发生进行过滤，并验证模型知识。使用这些有效实体的文档生成事实性问题，翻译成多种语言以评估跨语言边界的可迁移性。

Result: 在五种语言上评估多个LLM，发现跨语言迁移受语言距离强烈影响，且在不同语言方向上往往不对称。较大模型能改善迁移，但收益随规模增加而减少，且在不同领域间变化。

Conclusion: 这些发现为多语言迁移提供了新见解，并证明LiveCLKTBench作为未来研究的可靠基准的价值。

Abstract: Evaluating cross-lingual knowledge transfer in large language models is challenging, as correct answers in a target language may arise either from genuine transfer or from prior exposure during pre-training. We present LiveCLKTBench, an automated generation pipeline specifically designed to isolate and measure cross-lingual knowledge transfer. Our pipeline identifies self-contained, time-sensitive knowledge entities from real-world domains, filters them based on temporal occurrence, and verifies them against the model's knowledge. The documents of these valid entities are then used to generate factual questions, which are translated into multiple languages to evaluate transferability across linguistic boundaries. Using LiveCLKTBench, we evaluate several LLMs across five languages and observe that cross-lingual transfer is strongly influenced by linguistic distance and often asymmetric across language directions. While larger models improve transfer, the gains diminish with scale and vary across domains. These findings provide new insights into multilingual transfer and demonstrate the value of LiveCLKTBench as a reliable benchmark for future research.

</details>


### [4] [COMPASS: Context-Modulated PID Attention Steering System for Hallucination Mitigation](https://arxiv.org/abs/2511.14776)
*Snigdha Pandya,Rohan Nagale,Kenji Sahay,Anna Lin,Shikhar Shiromani,Kevin Zhu,Dev Sunishchal*

Main category: cs.CL

TL;DR: COMPASS是一个轻量级可解释的控制框架，通过模型内反馈回路动态调节注意力头，减少LLM的事实幻觉，同时提供对模型行为的科学理解。


<details>
  <summary>Details</summary>
Motivation: LLM经常生成流畅但事实错误的陈述，这是由于它们在上下文知识和参数知识之间分配注意力的方式存在问题。理解并引导这种内部行为对于可信部署和模型机制的科学可解释性至关重要。

Method: 引入COMPASS框架，通过上下文依赖分数(CRS)量化上下文依赖程度，使用PID控制器动态调节注意力头，无需重新训练或多轮解码即可保持事实一致性。

Result: 在多个基准测试(HotpotQA、XSum、HaluEval、RAGTruth)上，COMPASS持续减少上下文幻觉率(绝对降低2.8%到5.8%)，并揭示了不同注意力头对证据对齐的贡献。

Conclusion: 反馈驱动的可解释性为科学理解LLM行为提供了一条途径，COMPASS展示了如何在解码过程中直接嵌入模型内反馈循环来改善事实一致性。

Abstract: Large language models (LLMs) often generate fluent but factually incorrect statements despite having access to relevant evidence, a failure mode rooted in how they allocate attention between contextual and parametric knowledge. Understanding and steering this internal behavior is key both for trustworthy deployment and for scientific interpretability of model mechanisms. We introduce COMPASS (Context-Modulated PID Attention Steering System), a lightweight, interpretable control framework that embeds a model-based feedback loop directly within decoding. COMPASS quantifies context reliance via a transparent metric, the Context Reliance Score (CRS), which serves as an online probe of how attention heads ground generation in evidence. Using this interpretable signal, a PID controller dynamically modulates attention heads to maintain factual consistency without retraining or multi-pass decoding. Across benchmarks (HotpotQA, XSum, HaluEval, RAGTruth), COMPASS consistently reduces contextual hallucination rates (2.8 to 5.8 percent absolute) while revealing how distinct attention heads contribute to evidence alignment. These results highlight feedback-driven interpretability as a pathway toward scientific understanding of LLM behavior.

</details>


### [5] [The Impact of Prosodic Segmentation on Speech Synthesis of Spontaneous Speech](https://arxiv.org/abs/2511.14779)
*Julio Cesar Galdino,Sidney Evaldo Leal,Leticia Gabriella De Souza,Rodrigo de Freitas Lima,Antonio Nelson Fornari Mendes Moreira,Arnaldo Candido Junior,Miguel Oliveira,Edresson Casanova,Sandra M. Aluísio*

Main category: cs.CL

TL;DR: 评估手动和自动韵律分割标注对巴西葡萄牙语语音合成质量的影响，发现韵律分割训练能产生更清晰自然的语音，手动分割带来更大变异性从而改善韵律自然度。


<details>
  <summary>Details</summary>
Motivation: 自发语音合成面临捕捉对话自然流程的挑战，包括话轮转换、停顿和不流畅性。现有研究很少探索显式韵律分割数据集及其对自发语音合成的影响。

Method: 使用非自回归模型FastSpeech 2，在巴西葡萄牙语数据集上评估手动和自动韵律分割标注对语音合成质量的影响。

Result: 韵律分割训练产生略微更清晰和声学自然的语音。自动分割产生更规则的片段，手动分割引入更大变异性，有助于更自然的韵律。两种方法都能重现预期的核重音模式，但韵律模型更接近自然的核前轮廓。

Conclusion: 韵律分割标注能改善语音合成的自然度和清晰度，手动分割在韵律变异性方面表现更好。所有数据集、源代码和训练模型已公开供进一步研究。

Abstract: Spontaneous speech presents several challenges for speech synthesis, particularly in capturing the natural flow of conversation, including turn-taking, pauses, and disfluencies. Although speech synthesis systems have made significant progress in generating natural and intelligible speech, primarily through architectures that implicitly model prosodic features such as pitch, intensity, and duration, the construction of datasets with explicit prosodic segmentation and their impact on spontaneous speech synthesis remains largely unexplored. This paper evaluates the effects of manual and automatic prosodic segmentation annotations in Brazilian Portuguese on the quality of speech synthesized by a non-autoregressive model, FastSpeech 2. Experimental results show that training with prosodic segmentation produced slightly more intelligible and acoustically natural speech. While automatic segmentation tends to create more regular segments, manual prosodic segmentation introduces greater variability, which contributes to more natural prosody. Analysis of neutral declarative utterances showed that both training approaches reproduced the expected nuclear accent pattern, but the prosodic model aligned more closely with natural pre-nuclear contours. To support reproducibility and future research, all datasets, source codes, and trained models are publicly available under the CC BY-NC-ND 4.0 license.

</details>


### [6] [Human or LLM as Standardized Patients? A Comparative Study for Medical Education](https://arxiv.org/abs/2511.14783)
*Bingquan Zhang,Xiaoxiao Liu,Yuchi Wang,Lei Zhou,Qianqian Xie,Benyou Wang*

Main category: cs.CL

TL;DR: EasyMED是一个基于多智能体的标准化病人模拟框架，通过三个智能体实现真实对话、事实一致性和可操作反馈，在SPBench基准测试中达到与人类SP相当的学习效果，同时提供更好的灵活性、心理安全性和成本效益。


<details>
  <summary>Details</summary>
Motivation: 标准化病人(SP)在临床技能培训中不可或缺，但存在成本高、灵活性差、难以扩展的问题。现有的基于大语言模型的SP模拟器虽然成本较低，但行为不一致且缺乏与人类SP的严格比较。

Method: 提出了EasyMED多智能体框架，包含三个智能体：患者智能体（实现真实对话）、辅助智能体（保证事实一致性）和评估智能体（提供可操作反馈）。同时引入了SPBench基准，包含14个专科的真实SP-医生交互数据和8个专家定义的评估标准。

Result: 实验表明EasyMED能够匹配人类SP的学习效果，对基础较差的学生产生更大的技能提升，同时提供更好的灵活性、心理安全性和成本效益。

Conclusion: EasyMED框架在临床技能培训中能够有效替代传统标准化病人，提供更灵活、安全和经济的解决方案，特别适合大规模应用和个性化培训需求。

Abstract: Standardized Patients (SP) are indispensable for clinical skills training but remain expensive, inflexible, and difficult to scale. Existing large-language-model (LLM)-based SP simulators promise lower cost yet show inconsistent behavior and lack rigorous comparison with human SP. We present EasyMED, a multi-agent framework combining a Patient Agent for realistic dialogue, an Auxiliary Agent for factual consistency, and an Evaluation Agent that delivers actionable feedback. To support systematic assessment, we introduce SPBench, a benchmark of real SP-doctor interactions spanning 14 specialties and eight expert-defined evaluation criteria. Experiments demonstrate that EasyMED matches human SP learning outcomes while producing greater skill gains for lower-baseline students and offering improved flexibility, psychological safety, and cost efficiency.

</details>


### [7] [Opinion Mining and Analysis Using Hybrid Deep Neural Networks](https://arxiv.org/abs/2511.14796)
*Adel Hidri,Suleiman Ali Alsaif,Muteeb Alahmari,Eman AlShehri,Minyar Sassi Hidri*

Main category: cs.CL

TL;DR: 提出了一种混合双向门控循环单元和长短期记忆网络(HBGRU-LSTM)模型，用于改进情感分析，在IMDB电影评论和亚马逊产品评价数据集上达到95%的测试准确率，优于传统深度学习框架。


<details>
  <summary>Details</summary>
Motivation: 现有基于词典的方法和传统机器学习技术在处理上下文细微差别和可扩展性方面不足，深度学习在语义关系捕捉方面有改进，但仍有性能限制。

Method: 采用混合深度神经网络模型，结合双向门控循环单元(BGRU)和长短期记忆(LSTM)层，解决上下文细微差别、可扩展性和类别不平衡等挑战。

Result: HBGRU-LSTM模型测试准确率达95%，优于LSTM(93.06%)、CNN+LSTM(93.31%)和GRU+LSTM(92.20%)；在平衡数据集上负面情感召回率从86%提升至96%，误分类损失从20.24%降至13.3%。

Conclusion: 提出的HBGRU-LSTM模型在情感分析任务中表现出色，具有更好的泛化能力和鲁棒性，能够更公平准确地进行情感分类。

Abstract: Understanding customer attitudes has become a critical component of decision-making due to the growing influence of social media and e-commerce. Text-based opinions are the most structured, hence playing an important role in sentiment analysis. Most of the existing methods, which include lexicon-based approaches and traditional machine learning techniques, are insufficient for handling contextual nuances and scalability. While the latter has limitations in model performance and generalization, deep learning (DL) has achieved improvement, especially on semantic relationship capturing with recurrent neural networks (RNNs) and convolutional neural networks (CNNs). The aim of the study is to enhance opinion mining by introducing a hybrid deep neural network model that combines a bidirectional gated recurrent unit (BGRU) and long short-term memory (LSTM) layers to improve sentiment analysis, particularly addressing challenges such as contextual nuance, scalability, and class imbalance. To substantiate the efficacy of the proposed model, we conducted comprehensive experiments utilizing benchmark datasets, encompassing IMDB movie critiques and Amazon product evaluations. The introduced hybrid BGRULSTM (HBGRU-LSTM) architecture attained a testing accuracy of 95%, exceeding the performance of traditional DL frameworks such as LSTM (93.06%), CNN+LSTM (93.31%), and GRU+LSTM (92.20%). Moreover, our model exhibited a noteworthy enhancement in recall for negative sentiments, escalating from 86% (unbalanced dataset) to 96% (balanced dataset), thereby ensuring a more equitable and just sentiment classification. Furthermore, the model diminished misclassification loss from 20.24% for unbalanced to 13.3% for balanced dataset, signifying enhanced generalization and resilience.

</details>


### [8] [Hierarchical Token Prepending: Enhancing Information Flow in Decoder-based LLM Embeddings](https://arxiv.org/abs/2511.14868)
*Xueying Ding,Xingyue Huang,Mingxuan Ju,Liam Collins,Yozen Liu,Leman Akoglu,Neil Shah,Tong Zhao*

Main category: cs.CL

TL;DR: HTP通过分层令牌预置解决LLM嵌入中的信息流限制问题，在长文档场景下显著提升嵌入质量


<details>
  <summary>Details</summary>
Motivation: 大型语言模型由于因果注意力机制限制信息从后向前流动，导致表示质量下降。现有方法使用单个摘要令牌会过度压缩信息，在长文档上表现不佳。

Method: HTP将输入分区为块，在每个后续块前预置块级摘要令牌，创建多条后向信息流路径；同时用均值池化替代最后令牌池化以缓解读取级过度压缩。

Result: 在11个检索数据集和30个通用嵌入基准测试中实现一致性能提升，尤其在长上下文设置中表现突出。

Conclusion: HTP作为一种简单、架构无关的方法，能够增强零样本和微调模型，为优质长文档嵌入提供了可扩展路径。

Abstract: Large language models produce powerful text embeddings, but their causal attention mechanism restricts the flow of information from later to earlier tokens, degrading representation quality. While recent methods attempt to solve this by prepending a single summary token, they over-compress information, hence harming performance on long documents. We propose Hierarchical Token Prepending (HTP), a method that resolves two critical bottlenecks. To mitigate attention-level compression, HTP partitions the input into blocks and prepends block-level summary tokens to subsequent blocks, creating multiple pathways for backward information flow. To address readout-level over-squashing, we replace last-token pooling with mean-pooling, a choice supported by theoretical analysis. HTP achieves consistent performance gains across 11 retrieval datasets and 30 general embedding benchmarks, especially in long-context settings. As a simple, architecture-agnostic method, HTP enhances both zero-shot and finetuned models, offering a scalable route to superior long-document embeddings.

</details>


### [9] [Mathematical Analysis of Hallucination Dynamics in Large Language Models: Uncertainty Quantification, Advanced Decoding, and Principled Mitigation](https://arxiv.org/abs/2511.15005)
*Moses Kiprono*

Main category: cs.CL

TL;DR: 提出了一个数学基础框架来理解、测量和减轻大语言模型的幻觉问题，结合概率建模、信息论、三角信号分析和贝叶斯不确定性估计等方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然功能强大，但容易产生幻觉——听起来合理但事实错误或缺乏支持的输出。需要系统性地解决这个问题。

Method: 使用概率建模、信息理论、三角信号分析和贝叶斯不确定性估计来分析自回归错误传播，提出改进的不确定性度量（包括语义和相位感知变体），开发对比解码、检索增强基础、事实对齐和弃权等缓解策略。

Result: 建立了一个统一框架，将校准、检索和对齐等最新进展联系起来，支持更安全可靠的大语言模型。

Conclusion: 该数学框架为理解和减轻LLM幻觉提供了理论基础，有助于开发更安全可靠的模型系统。

Abstract: Large Language Models (LLMs) are powerful linguistic engines but remain susceptible to hallucinations: plausible-sounding outputs that are factually incorrect or unsupported. In this work, we present a mathematically grounded framework to understand, measure, and mitigate these hallucinations. Drawing on probabilistic modeling, information theory, trigonometric signal analysis, and Bayesian uncertainty estimation, we analyze how errors compound autoregressively, propose refined uncertainty metrics, including semantic and phase-aware variants, and develop principled mitigation strategies such as contrastive decoding, retrieval-augmented grounding, factual alignment, and abstention. This unified lens connects recent advances in calibration, retrieval, and alignment to support safer and more reliable LLMs.

</details>


### [10] [Teaching According to Students' Aptitude: Personalized Mathematics Tutoring via Persona-, Memory-, and Forgetting-Aware LLMs](https://arxiv.org/abs/2511.15163)
*Yang Wu,Rujing Yao,Tong Zhang,Yufei Shi,Zhuoren Jiang,Zhushan Li,Xiaozhong Liu*

Main category: cs.CL

TL;DR: TASA是一个学生感知的数学辅导框架，通过整合学生画像、记忆和遗忘动态来实现个性化数学学习，相比现有方法能更好地适应学生的知识状态变化。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能辅导系统大多未能捕捉学生知识随熟练度、概念差距和遗忘模式的动态演变，特别是在数学辅导中需要根据学生掌握水平和认知保留进行精细校准。

Method: TASA维护结构化学生画像（熟练度档案）和事件记忆（记录先前学习交互），结合连续遗忘曲线和知识追踪动态更新学生掌握状态，生成情境适当且难度校准的问题和解释。

Result: 实证结果表明，TASA相比代表性基线方法实现了更优的学习成果和更自适应的辅导行为。

Conclusion: 在基于LLM的辅导系统中建模时间遗忘和学习者画像至关重要，TASA框架为此提供了有效解决方案。

Abstract: Large Language Models (LLMs) are increasingly integrated into intelligent tutoring systems to provide human-like and adaptive instruction. However, most existing approaches fail to capture how students' knowledge evolves dynamically across their proficiencies, conceptual gaps, and forgetting patterns. This challenge is particularly acute in mathematics tutoring, where effective instruction requires fine-grained scaffolding precisely calibrated to each student's mastery level and cognitive retention. To address this issue, we propose TASA (Teaching According to Students' Aptitude), a student-aware tutoring framework that integrates persona, memory, and forgetting dynamics for personalized mathematics learning. Specifically, TASA maintains a structured student persona capturing proficiency profiles and an event memory recording prior learning interactions. By incorporating a continuous forgetting curve with knowledge tracing, TASA dynamically updates each student's mastery state and generates contextually appropriate, difficulty-calibrated questions and explanations. Empirical results demonstrate that TASA achieves superior learning outcomes and more adaptive tutoring behavior compared to representative baselines, underscoring the importance of modeling temporal forgetting and learner profiles in LLM-based tutoring systems.

</details>


### [11] [HinTel-AlignBench: A Framework and Benchmark for Hindi-Telugu with English-Aligned Samples](https://arxiv.org/abs/2511.15183)
*Rishikant Chigrupaatii,Ponnada Sai Tulasi Kanishka,Lalit Chandra Routhu,Martin Patel Sama Supratheek Reddy,Divyam Gupta,Dasari Srikar,Krishna Teja Kuchimanchi,Rajiv Misra,Rohun Tripathi*

Main category: cs.CL

TL;DR: 提出了一个评估印度语言视觉语言模型的可扩展框架，创建了HinTel-AlignBench基准测试，发现VLMs在印度语言上的表现比英语平均下降8.3分（印地语）和5.5分（泰卢固语）。


<details>
  <summary>Details</summary>
Motivation: 解决当前多语言VLM评估的四个主要局限：依赖未验证的自动翻译、任务/领域覆盖范围窄、样本量有限、缺乏文化和本地来源的问答数据。

Method: 开发半自动数据集创建框架，结合回译、过滤和人工验证；创建全面的印地语和泰卢固语视觉语言基准，包括适应英语数据集和本地新颖印度数据集。

Result: 在5个任务中，4个任务的所有模型在印度语言上的表现都比英语差，印地语平均下降8.3分，泰卢固语平均下降5.5分。

Conclusion: 揭示了多语言多模态理解的具体改进领域，为低资源语言的公平AI发展提供了重要基准。

Abstract: With nearly 1.5 billion people and more than 120 major languages, India represents one of the most diverse regions in the world. As multilingual Vision-Language Models (VLMs) gain prominence, robust evaluation methodologies are essential to drive progress toward equitable AI for low-resource languages. Current multilingual VLM evaluations suffer from four major limitations: reliance on unverified auto-translations, narrow task/domain coverage, limited sample sizes, and lack of cultural and natively sourced Question-Answering (QA). To address these gaps, we present a scalable framework to evaluate VLMs in Indian languages and compare it with performance in English. Using the framework, we generate HinTel-AlignBench, a benchmark that draws from diverse sources in Hindi and Telugu with English-aligned samples. Our contributions are threefold: (1) a semi-automated dataset creation framework combining back-translation, filtering, and human verification; (2) the most comprehensive vision-language benchmark for Hindi and and Telugu, including adapted English datasets (VQAv2, RealWorldQA, CLEVR-Math) and native novel Indic datasets (JEE for STEM, VAANI for cultural grounding) with approximately 4,000 QA pairs per language; and (3) a detailed performance analysis of various State-of-the-Art (SOTA) open-weight and closed-source VLMs. We find a regression in performance for tasks in English versus in Indian languages for 4 out of 5 tasks across all the models, with an average regression of 8.3 points in Hindi and 5.5 points for Telugu. We categorize common failure modes to highlight concrete areas of improvement in multilingual multimodal understanding.

</details>


### [12] [Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story](https://arxiv.org/abs/2511.15210)
*Vladislav Pedashenko,Laida Kushnareva,Yana Khassan Nibal,Eduard Tulchinskii,Kristian Kuznetsov,Vladislav Zharchinskii,Yury Maximov,Irina Piontkovskaya*

Main category: cs.CL

TL;DR: 本文通过跨编码器分析、语言特征和稀疏自编码器，首次全面研究了内在维度(ID)与可解释文本属性之间的关系，发现ID与基于熵的指标互补，在不同文本类型中呈现稳定分层，并识别出影响ID的因果特征。


<details>
  <summary>Details</summary>
Motivation: 内在维度是现代LLM分析中的重要工具，但其与文本属性的关系尚未得到充分探索。本文旨在通过可解释的文本特性来理解ID的决定因素。

Method: 使用跨编码器分析、语言特征提取和稀疏自编码器(SAEs)来研究ID与文本属性的关系，并进行转向实验验证因果效应。

Result: 发现ID与基于熵的指标在控制长度后不相关；不同文本类型呈现稳定的ID分层：科学文本低ID(~8)，百科内容中等ID(~9)，创意/观点写作高ID(~10.5)；识别出科学信号降低ID，人性化信号增加ID的因果特征。

Conclusion: 对于当代模型，科学写作相对"简单"，而小说、观点和情感表达增加了表示自由度。该多面分析为ID的正确使用和基于ID结果的合理解释提供了实践指导。

Abstract: Intrinsic dimension (ID) is an important tool in modern LLM analysis, informing studies of training dynamics, scaling behavior, and dataset structure, yet its textual determinants remain underexplored. We provide the first comprehensive study grounding ID in interpretable text properties through cross-encoder analysis, linguistic features, and sparse autoencoders (SAEs). In this work, we establish three key findings. First, ID is complementary to entropy-based metrics: after controlling for length, the two are uncorrelated, with ID capturing geometric complexity orthogonal to prediction quality. Second, ID exhibits robust genre stratification: scientific prose shows low ID (~8), encyclopedic content medium ID (~9), and creative/opinion writing high ID (~10.5) across all models tested. This reveals that contemporary LLMs find scientific text "representationally simple" while fiction requires additional degrees of freedom. Third, using SAEs, we identify causal features: scientific signals (formal tone, report templates, statistics) reduce ID; humanized signals (personalization, emotion, narrative) increase it. Steering experiments confirm these effects are causal. Thus, for contemporary models, scientific writing appears comparatively "easy", whereas fiction, opinion, and affect add representational degrees of freedom. Our multi-faceted analysis provides practical guidance for the proper use of ID and the sound interpretation of ID-based results.

</details>


### [13] [OEMA: Ontology-Enhanced Multi-Agent Collaboration Framework for Zero-Shot Clinical Named Entity Recognition](https://arxiv.org/abs/2511.15211)
*Xinli Tao,Xin Dong,Xuezhong Zhou*

Main category: cs.CL

TL;DR: OEMA是一个零样本临床命名实体识别框架，通过多智能体协作解决零样本NER中的示例选择粒度问题和提示与自改进集成问题，在MTSamples和VAERS数据集上达到最先进的精确匹配性能。


<details>
  <summary>Details</summary>
Motivation: 监督模型如CRF和BioClinicalBERT需要昂贵的标注数据，而现有的零样本NER方法在示例选择粒度和集成提示与自改进方面存在困难。

Method: 提出OEMA框架，包含三个组件：自注释器生成示例、鉴别器通过SNOMED CT过滤示例、预测器使用实体描述进行准确推理。

Result: 在MTSamples和VAERS数据集上，OEMA实现了最先进的精确匹配性能。在相关匹配下，它与监督的BioClinicalBERT相当并超过CRF。

Conclusion: OEMA通过本体引导推理和多智能体协作解决了零样本NER的关键挑战，实现了接近监督学习的性能，在临床NLP应用中具有前景。

Abstract: Clinical named entity recognition (NER) is crucial for extracting information from electronic health records (EHRs), but supervised models like CRF and BioClinicalBERT require costly annotated data. While zero-shot NER with large language models (LLMs) reduces this dependency, it struggles with example selection granularity and integrating prompts with self-improvement. To address this, we propose OEMA, a zero-shot clinical NER framework using multi-agent collaboration. OEMA's three components are: a self-annotator generating examples, a discriminator filtering them via SNOMED CT, and a predictor using entity descriptions for accurate inference. On MTSamples and VAERS datasets, OEMA achieves state-of-the-art exact-match performance. Under related-match, it matches supervised BioClinicalBERT and surpasses CRF. OEMA addresses key zero-shot NER challenges through ontology-guided reasoning and multi-agent collaboration, achieving near-supervised performance and showing promise for clinical NLP applications.

</details>


### [14] [Context Cascade Compression: Exploring the Upper Limits of Text Compression](https://arxiv.org/abs/2511.15244)
*Fanfan Liu,Haibo Qiu*

Main category: cs.CL

TL;DR: C3提出了一种级联压缩方法，使用小型LLM压缩长文本为潜在标记，大型LLM进行解码，在20倍压缩比下达到98%准确率，优于DeepSeek-OCR的60%准确率。


<details>
  <summary>Details</summary>
Motivation: 解决百万级token长上下文任务对LLMs的计算和内存挑战，探索文本压缩的上限。

Method: 级联两个不同大小的LLM：小型LLM将长文本压缩为少量潜在标记（如32或64长度），大型LLM在压缩后的上下文中执行解码任务。

Result: 20倍压缩比下达到98%解码准确率（DeepSeek-OCR约60%），40倍压缩比下仍保持约93%准确率。

Conclusion: C3在上下文压缩领域表现出优越性能和可行性，为光学字符压缩等领域的压缩比上限提供了参考。

Abstract: Million-level token inputs in long-context tasks pose significant computational and memory challenges for Large Language Models (LLMs). Recently, DeepSeek-OCR conducted research into the feasibility of Contexts Optical Compression and achieved preliminary results. Inspired by this, we introduce Context Cascade Compression C3 to explore the upper limits of text compression. Our method cascades two LLMs of different sizes to handle the compression and decoding tasks. Specifically, a small LLM, acting as the first stage, performs text compression by condensing a long context into a set of latent tokens (e.g., 32 or 64 in length), achieving a high ratio of text tokens to latent tokens. A large LLM, as the second stage, then executes the decoding task on this compressed context. Experiments show that at a 20x compression ratio (where the number of text tokens is 20 times the number of latent tokens), our model achieves 98% decoding accuracy, compared to approximately 60% for DeepSeek-OCR. When we further increase the compression ratio to 40x, the accuracy is maintained at around 93%. This indicates that in the domain of context compression, C3 Compression demonstrates superior performance and feasibility over optical character compression. C3 uses a simpler, pure-text pipeline that ignores factors like layout, color, and information loss from a visual encoder. This also suggests a potential upper bound for compression ratios in future work on optical character compression, OCR, and related fields. Codes and model weights are publicly accessible at https://github.com/liufanfanlff/C3-Context-Cascade-Compression

</details>


### [15] [IndicGEC: Powerful Models, or a Measurement Mirage?](https://arxiv.org/abs/2511.15260)
*Sowmya Vajjala*

Main category: cs.CL

TL;DR: 该论文报告了TeamNRC在BHASHA-Task 1语法错误校正共享任务中的参与结果，使用零/少样本提示语言模型的方法在5种印度语言上取得了良好表现，特别是在泰卢固语和印地语中分别获得第4和第2名。


<details>
  <summary>Details</summary>
Motivation: 探索使用不同规模的语言模型（4B到大型专有模型）在印度语言语法错误校正任务中的潜力，并关注数据质量和评估指标的问题。

Method: 采用零/少样本提示的方法，使用不同规模的语言模型（4B到大型专有模型）进行语法错误校正。

Result: 在泰卢固语中获得第4名（GLEU 83.78），在印地语中获得第2名（GLEU 84.31），并将实验扩展到泰米尔语、马拉雅拉姆语和孟加拉语。

Conclusion: 小型语言模型在语法错误校正任务中具有潜力，但需要关注创建高质量数据集和适合印度语言脚本的适当评估指标。

Abstract: In this paper, we report the results of the TeamNRC's participation in the BHASHA-Task 1 Grammatical Error Correction shared task https://github.com/BHASHA-Workshop/IndicGEC2025/ for 5 Indian languages. Our approach, focusing on zero/few-shot prompting of language models of varying sizes (4B to large proprietary models) achieved a Rank 4 in Telugu and Rank 2 in Hindi with GLEU scores of 83.78 and 84.31 respectively. In this paper, we extend the experiments to the other three languages of the shared task - Tamil, Malayalam and Bangla, and take a closer look at the data quality and evaluation metric used. Our results primarily highlight the potential of small language models, and summarize the concerns related to creating good quality datasets and appropriate metrics for this task that are suitable for Indian language scripts.

</details>


### [16] [MAPROC at AHaSIS Shared Task: Few-Shot and Sentence Transformer for Sentiment Analysis of Arabic Hotel Reviews](https://arxiv.org/abs/2511.15291)
*Randa Zarnoufi*

Main category: cs.CL

TL;DR: 使用SetFit框架对阿拉伯方言酒店评论进行情感分析，在数据稀缺情况下取得73%的F1分数，排名第12


<details>
  <summary>Details</summary>
Motivation: 阿拉伯方言情感分析面临语言多样性和标注数据稀缺的挑战，特别是在酒店评论等专业领域

Method: 采用SetFit（句子Transformer微调）框架，这是一种数据高效的小样本学习技术

Result: 在官方评估集上获得73%的F1分数，在26个参与者中排名第12

Conclusion: 小样本学习在处理专业领域（如酒店评论）中微妙的阿拉伯方言文本数据稀缺问题方面具有潜力

Abstract: Sentiment analysis of Arabic dialects presents significant challenges due to linguistic diversity and the scarcity of annotated data. This paper describes our approach to the AHaSIS shared task, which focuses on sentiment analysis on Arabic dialects in the hospitality domain. The dataset comprises hotel reviews written in Moroccan and Saudi dialects, and the objective is to classify the reviewers sentiment as positive, negative, or neutral. We employed the SetFit (Sentence Transformer Fine-tuning) framework, a data-efficient few-shot learning technique. On the official evaluation set, our system achieved an F1 of 73%, ranking 12th among 26 participants. This work highlights the potential of few-shot learning to address data scarcity in processing nuanced dialectal Arabic text within specialized domains like hotel reviews.

</details>


### [17] [Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models](https://arxiv.org/abs/2511.15304)
*Piercosma Bisconti,Matteo Prandi,Federico Pierucci,Francesco Giarrusso,Marcantonio Bracale,Marcello Galisai,Vincenzo Suriani,Olga Sorokoletova,Federico Sartore,Daniele Nardi*

Main category: cs.CL

TL;DR: 研究发现诗歌形式的对抗性提示可以作为通用单轮越狱技术，在25个前沿LLM中实现高攻击成功率，诗歌转换使攻击成功率比散文基准提高多达18倍。


<details>
  <summary>Details</summary>
Motivation: 探索风格变异是否能够绕过当代LLM的安全机制，检验当前对齐方法和评估协议的根本局限性。

Method: 使用精心策划的诗歌提示攻击25个前沿模型，将1200个有害提示通过标准化元提示转换为诗歌形式，使用开放权重评判模型集成和人工验证子集进行评估。

Result: 诗歌框架实现平均62%的手工诗歌越狱成功率和约43%的元提示转换成功率，显著优于非诗歌基准，揭示了跨模型家族和安全训练方法的系统性漏洞。

Conclusion: 仅凭风格变异就能规避当代安全机制，表明当前对齐方法和评估协议存在根本性限制。

Abstract: We present evidence that adversarial poetry functions as a universal single-turn jailbreak technique for large language models (LLMs). Across 25 frontier proprietary and open-weight models, curated poetic prompts yielded high attack-success rates (ASR), with some providers exceeding 90%. Mapping prompts to MLCommons and EU CoP risk taxonomies shows that poetic attacks transfer across CBRN, manipulation, cyber-offence, and loss-of-control domains. Converting 1,200 MLCommons harmful prompts into verse via a standardized meta-prompt produced ASRs up to 18 times higher than their prose baselines. Outputs are evaluated using an ensemble of open-weight judge models and a human-validated stratified subset (with double-annotations to measure agreement). Disagreements were manually resolved. Poetic framing achieved an average jailbreak success rate of 62% for hand-crafted poems and approximately 43% for meta-prompt conversions (compared to non-poetic baselines), substantially outperforming non-poetic baselines and revealing a systematic vulnerability across model families and safety training approaches. These findings demonstrate that stylistic variation alone can circumvent contemporary safety mechanisms, suggesting fundamental limitations in current alignment methods and evaluation protocols.

</details>


### [18] [HEAD-QA v2: Expanding a Healthcare Benchmark for Reasoning](https://arxiv.org/abs/2511.15355)
*Alexis Correa-Guillén,Carlos Gómez-Rodríguez,David Vilares*

Main category: cs.CL

TL;DR: HEAD-QA v2是西班牙语/英语医疗推理多选数据集的扩展版本，包含12,000多个来自西班牙专业考试的问题，用于评估LLM在医疗推理方面的表现。


<details>
  <summary>Details</summary>
Motivation: 响应高质量数据集的需求，捕捉医疗推理的语言和概念复杂性。

Method: 扩展数据集至12,000多个问题，使用提示、RAG和基于概率的答案选择方法对多个开源LLM进行基准测试，并提供多语言版本。

Result: 性能主要由模型规模和内在推理能力驱动，复杂推理策略获得的增益有限。

Conclusion: HEAD-QA v2成为推进生物医学推理和模型改进研究的可靠资源。

Abstract: We introduce HEAD-QA v2, an expanded and updated version of a Spanish/English healthcare multiple-choice reasoning dataset originally released by Vilares and Gómez-Rodríguez (2019). The update responds to the growing need for high-quality datasets that capture the linguistic and conceptual complexity of healthcare reasoning. We extend the dataset to over 12,000 questions from ten years of Spanish professional exams, benchmark several open-source LLMs using prompting, RAG, and probability-based answer selection, and provide additional multilingual versions to support future work. Results indicate that performance is mainly driven by model scale and intrinsic reasoning ability, with complex inference strategies obtaining limited gains. Together, these results establish HEAD-QA v2 as a reliable resource for advancing research on biomedical reasoning and model improvement.

</details>


### [19] [The Empowerment of Science of Science by Large Language Models: New Tools and Methods](https://arxiv.org/abs/2511.15370)
*Guoqiang Liang,Jingqian Gong,Mengxuan Li,Gege Lin,Shuo Zhang*

Main category: cs.CL

TL;DR: 本文从用户角度全面回顾了支撑大语言模型（LLMs）的核心技术，包括提示工程、知识增强检索增强生成、微调、预训练和工具学习，并探讨了LLMs在科学计量学领域的潜在应用前景。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言理解、图像识别和多模态任务方面展现出卓越能力，正朝着AGI方向发展，成为全球技术竞争的核心议题。本文旨在系统梳理LLMs的关键技术，并探索其在科学计量学领域的应用潜力。

Method: 采用文献综述方法，从用户角度分析LLMs的核心技术（提示工程、知识增强RAG、微调、预训练、工具学习），结合科学计量学发展历史，提出前瞻性应用视角。

Result: 提出了基于AI代理的科学评估模型，以及使用LLMs进行新研究前沿检测和知识图谱构建的新方法，为科学计量学领域提供了创新工具。

Conclusion: LLMs不仅推动了人工智能技术的发展，也为科学计量学带来了新的研究范式和应用前景，特别是在科学评估、研究前沿发现和知识组织方面具有重要价值。

Abstract: Large language models (LLMs) have exhibited exceptional capabilities in natural language understanding and generation, image recognition, and multimodal tasks, charting a course towards AGI and emerging as a central issue in the global technological race. This manuscript conducts a comprehensive review of the core technologies that support LLMs from a user standpoint, including prompt engineering, knowledge-enhanced retrieval augmented generation, fine tuning, pretraining, and tool learning. Additionally, it traces the historical development of Science of Science (SciSci) and presents a forward looking perspective on the potential applications of LLMs within the scientometric domain. Furthermore, it discusses the prospect of an AI agent based model for scientific evaluation, and presents new research fronts detection and knowledge graph building methods with LLMs.

</details>


### [20] [A Compliance-Preserving Retrieval System for Aircraft MRO Task Search](https://arxiv.org/abs/2511.15383)
*Byungho Jo*

Main category: cs.CL

TL;DR: 开发了一个合规的检索系统，通过LLM重排和语义搜索技术，在航空维修环境中帮助技术人员快速查找手册，将查找时间从6-15分钟减少到18秒，同时保持监管合规性。


<details>
  <summary>Details</summary>
Motivation: 飞机维修技术人员花费高达30%的工作时间搜索手册，这在需要严格可追溯性的MRO操作中成为效率瓶颈。

Method: 构建版本鲁棒的嵌入向量，使用视觉语言解析结构化认证内容，结合LLM重排和语义搜索，与现有认证查看器协同工作而非替代。

Result: 在49k合成查询中达到>90%检索准确率，双语对照研究显示90.9%的top-10成功率，查找时间减少95%（从6-15分钟降至18秒）。

Conclusion: 语义检索可以在严格监管约束下有效运行，显著减少多语言MRO工作流程中的操作负担。

Abstract: Aircraft Maintenance Technicians (AMTs) spend up to 30% of work time searching manuals, a documented efficiency bottleneck in MRO operations where every procedure must be traceable to certified sources. We present a compliance-preserving retrieval system that adapts LLM reranking and semantic search to aviation MRO environments by operating alongside, rather than replacing, certified legacy viewers. The system constructs revision-robust embeddings from ATA chapter hierarchies and uses vision-language parsing to structure certified content, allowing technicians to preview ranked tasks and access verified procedures in existing viewers. Evaluation on 49k synthetic queries achieves >90% retrieval accuracy, while bilingual controlled studies with 10 licensed AMTs demonstrate 90.9% top-10 success rate and 95% reduction in lookup time, from 6-15 minutes to 18 seconds per task. These gains provide concrete evidence that semantic retrieval can operate within strict regulatory constraints and meaningfully reduce operational workload in real-world multilingual MRO workflows.

</details>


### [21] [DEPO: Dual-Efficiency Preference Optimization for LLM Agents](https://arxiv.org/abs/2511.15392)
*Sirui Chen,Mengshi Zhao,Lei Xu,Yuying Zhao,Beier Zhu,Hanwang Zhang,Shengjie Zhao,Chaochao Lu*

Main category: cs.CL

TL;DR: DEPO是一种双效率偏好优化方法，通过联合奖励简洁响应和更少行动步骤，显著减少LLM代理的令牌使用和步骤数量，同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为代理部署时，更丰富的推理往往导致更长的思维链，影响实际场景中的交互效率，但目前缺乏对LLM代理效率的系统定义。

Method: 提出双效率定义（步骤级效率和轨迹级效率），并开发DEPO方法，通过偏好优化联合奖励简洁响应和更少行动步骤。

Result: 在WebShop和BabyAI上，DEPO减少令牌使用达60.9%，减少步骤达26.9%，同时性能提升达29.3%。在数学基准测试中也表现出良好的泛化能力。

Conclusion: DEPO有效解决了LLM代理效率问题，在减少计算成本的同时保持甚至提升性能，具有实际应用价值。

Abstract: Recent advances in large language models (LLMs) have greatly improved their reasoning and decision-making abilities when deployed as agents. Richer reasoning, however, often comes at the cost of longer chain of thought (CoT), hampering interaction efficiency in real-world scenarios. Nevertheless, there still lacks systematic definition of LLM agent efficiency, hindering targeted improvements. To this end, we introduce dual-efficiency, comprising (i) step-level efficiency, which minimizes tokens per step, and (ii) trajectory-level efficiency, which minimizes the number of steps to complete a task. Building on this definition, we propose DEPO, a dual-efficiency preference optimization method that jointly rewards succinct responses and fewer action steps. Experiments on WebShop and BabyAI show that DEPO cuts token usage by up to 60.9% and steps by up to 26.9%, while achieving up to a 29.3% improvement in performance. DEPO also generalizes to three out-of-domain math benchmarks and retains its efficiency gains when trained on only 25% of the data. Our project page is at https://opencausalab.github.io/DEPO.

</details>


### [22] [NAMeGEn: Creative Name Generation via A Novel Agent-based Multiple Personalized Goal Enhancement Framework](https://arxiv.org/abs/2511.15408)
*Shanlin Zhou,Xinpeng Wang,Jianxun Lian,Zhenghao Liu,Laks V. S. Lakshmanan,Xiaoyuan Yi,Yongtao Hao*

Main category: cs.CL

TL;DR: 提出了NAMEGEn框架，通过多智能体优化解决中文起名任务中的多目标灵活性和解释复杂性挑战，无需训练即可生成创意姓名并提供美学解释。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在创意自然语言生成中面临多目标灵活性和解释复杂性的挑战，特别是在短文本生成任务中难以同时满足个性化、细粒度和多元化需求。

Method: NAMEGEn是一个多智能体优化框架，通过目标提取、姓名生成和评估的迭代交替过程，结合17k+古典诗词语料库来增强美学效果。

Result: 实验表明NAMEGEn能有效生成满足多样化个性化需求的创意姓名并提供有意义的解释，在六个基线方法中表现最优。

Conclusion: 该框架成功解决了CNLG中的关键挑战，为短文本创意生成提供了有效解决方案，特别是在中文起名这一代表性任务中。

Abstract: Trained on diverse human-authored texts, Large Language Models (LLMs) unlocked the potential for Creative Natural Language Generation (CNLG), benefiting various applications like advertising and storytelling. Nevertheless, CNLG still remains difficult due to two main challenges. (1) Multi-objective flexibility: user requirements are often personalized, fine-grained, and pluralistic, which LLMs struggle to satisfy simultaneously; (2) Interpretive complexity: beyond generation, creativity also involves understanding and interpreting implicit meaning to enhance users' perception. These challenges significantly limit current methods, especially in short-form text generation, in generating creative and insightful content. To address this, we focus on Chinese baby naming, a representative short-form CNLG task requiring adherence to explicit user constraints (e.g., length, semantics, anthroponymy) while offering meaningful aesthetic explanations. We propose NAMeGEn, a novel multi-agent optimization framework that iteratively alternates between objective extraction, name generation, and evaluation to meet diverse requirements and generate accurate explanations. To support this task, we further construct a classical Chinese poetry corpus with 17k+ poems to enhance aesthetics, and introduce CBNames, a new benchmark with tailored metrics. Extensive experiments demonstrate that NAMeGEn effectively generates creative names that meet diverse, personalized requirements while providing meaningful explanations, outperforming six baseline methods spanning various LLM backbones without any training.

</details>


### [23] [Building Robust and Scalable Multilingual ASR for Indian Languages](https://arxiv.org/abs/2511.15418)
*Arjun Gangwar,Kaousheik Jayakumar,S. Umesh*

Main category: cs.CL

TL;DR: SPRING Lab开发了用于ASRU MADASR 2.0挑战赛的多语言ASR系统，采用多解码器架构和音素通用标签集作为中间表示，在Track 2中在3种语言上超越基线性能，并获得最高的语言和方言识别准确率。


<details>
  <summary>Details</summary>
Motivation: 改进ASR系统在8种语言33种方言中的语言和方言识别能力，参与Track 1和Track 2的从零开始构建多语言系统的挑战。

Method: 使用多解码器架构，以音素通用标签集作为中间表示进行训练，并探讨了将音素空间增益转换回相应字形表示的各种方法。

Result: 在CLS空间中性能优于基线，在Track 2中3种语言的WER/CER超越基线，在所有参赛团队中获得最高的语言ID和方言ID准确率。

Conclusion: 提出的多解码器架构和音素CLS方法有效提升了多语言ASR系统的性能，特别是在语言和方言识别方面表现优异。

Abstract: This paper describes the systems developed by SPRING Lab, Indian Institute of Technology Madras, for the ASRU MADASR 2.0 challenge. The systems developed focuses on adapting ASR systems to improve in predicting the language and dialect of the utterance among 8 languages across 33 dialects. We participated in Track 1 and Track 2, which restricts the use of additional data and develop from-the-scratch multilingual systems. We presented a novel training approach using Multi-Decoder architecture with phonemic Common Label Set (CLS) as intermediate representation. It improved the performance over the baseline (in the CLS space). We also discuss various methods used to retain the gain obtained in the phonemic space while converting them back to the corresponding grapheme representations. Our systems beat the baseline in 3 languages (Track 2) in terms of WER/CER and achieved the highest language ID and dialect ID accuracy among all participating teams (Track 2).

</details>


### [24] [LLM-MemCluster: Empowering Large Language Models with Dynamic Memory for Text Clustering](https://arxiv.org/abs/2511.15424)
*Yuanjie Zhu,Liangwei Yang,Ke Xu,Weizhi Zhang,Zihe Song,Jindong Wang,Philip S. Yu*

Main category: cs.CL

TL;DR: LLM-MemCluster：一个完全基于LLM的文本聚类框架，通过动态内存和双提示策略实现端到端的聚类，无需调优即可显著超越现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在文本聚类中存在两个根本限制：缺乏状态记忆进行迭代优化，以及难以管理聚类粒度。现有方法依赖复杂的外部模块流水线，无法实现真正的端到端方法。

Method: 提出LLM-MemCluster框架，将聚类重新定义为完全LLM原生的任务。使用动态内存实现状态感知，采用双提示策略让模型能够推理并确定聚类数量。

Result: 在多个基准数据集上的评估显示，这个无需调优的框架显著且一致地超越了强基线方法。

Conclusion: LLM-MemCluster为基于LLM的文本聚类提供了一个有效、可解释且真正端到端的范式。

Abstract: Large Language Models (LLMs) are reshaping unsupervised learning by offering an unprecedented ability to perform text clustering based on their deep semantic understanding. However, their direct application is fundamentally limited by a lack of stateful memory for iterative refinement and the difficulty of managing cluster granularity. As a result, existing methods often rely on complex pipelines with external modules, sacrificing a truly end-to-end approach. We introduce LLM-MemCluster, a novel framework that reconceptualizes clustering as a fully LLM-native task. It leverages a Dynamic Memory to instill state awareness and a Dual-Prompt Strategy to enable the model to reason about and determine the number of clusters. Evaluated on several benchmark datasets, our tuning-free framework significantly and consistently outperforms strong baselines. LLM-MemCluster presents an effective, interpretable, and truly end-to-end paradigm for LLM-based text clustering.

</details>


### [25] [Standardising the NLP Workflow: A Framework for Reproducible Linguistic Analysis](https://arxiv.org/abs/2511.15512)
*Yves Pauli,Jan-Bernard Marsman,Finn Rabe,Victoria Edkins,Roya Hüppi,Silvia Ciampelli,Akhil Ratan Misra,Nils Lang,Wolfram Hinzen,Iris Sommer,Philipp Homan*

Main category: cs.CL

TL;DR: 该论文提出了语言处理数据结构(LPDS)和pelican nlp工具包，旨在解决语言数据处理中的标准化和可重复性问题，构建端到端的语言数据处理流程。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型等AI语言处理技术的发展带来了语言数据分析方法的演进，但同时也暴露了缺乏标准化数据组织和共享机制、缺乏标准化可重复处理方法等挑战。

Method: 1) 提出LPDS数据结构，受神经科学BIDS标准启发，提供文件夹结构和文件命名规范；2) 开发pelican nlp模块化Python包，支持从数据清洗到复杂语言和声学特征提取的完整处理流程，可通过单一配置文件执行。

Result: LPDS和pelican nlp共同提供了端到端的语言数据处理管道，能够生成预处理的语言数据或标准化的语言和声学特征提取结果。

Conclusion: LPDS和pelican nlp为语言数据处理提供了方法论透明度和可重复性保证，是推动语言处理研究标准化的重要工具。

Abstract: The introduction of large language models and other influential developments in AI-based language processing have led to an evolution in the methods available to quantitatively analyse language data. With the resultant growth of attention on language processing, significant challenges have emerged, including the lack of standardisation in organising and sharing linguistic data and the absence of standardised and reproducible processing methodologies. Striving for future standardisation, we first propose the Language Processing Data Structure (LPDS), a data structure inspired by the Brain Imaging Data Structure (BIDS), a widely adopted standard for handling neuroscience data. It provides a folder structure and file naming conventions for linguistic research. Second, we introduce pelican nlp, a modular and extensible Python package designed to enable streamlined language processing, from initial data cleaning and task-specific preprocessing to the extraction of sophisticated linguistic and acoustic features, such as semantic embeddings and prosodic metrics. The entire processing workflow can be specified within a single, shareable configuration file, which pelican nlp then executes on LPDS-formatted data. Depending on the specifications, the reproducible output can consist of preprocessed language data or standardised extraction of both linguistic and acoustic features and corresponding result aggregations. LPDS and pelican nlp collectively offer an end-to-end processing pipeline for linguistic data, designed to ensure methodological transparency and enhance reproducibility.

</details>


### [26] [Multimodal Evaluation of Russian-language Architectures](https://arxiv.org/abs/2511.15552)
*Artem Chervyakov,Ulyana Isaeva,Anton Emelyanov,Artem Safin,Maria Tikhonova,Alexander Kharitonov,Yulia Lyakh,Petr Surovtsev,Denis Shevelev Vildan Saburov,Vasily Konovalov,Elisei Rykov,Ivan Sviridov,Amina Miftakhova,Ilseyar Alimova,Alexander Panchenko,Alexander Kapitanov,Alena Fenogenova*

Main category: cs.CL

TL;DR: Mera Multi是一个针对俄语的多模态评估框架，包含文本、图像、音频和视频四种模态的18个评估任务，为俄语多模态大语言模型提供基准测试。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏针对俄语的多模态基准测试，多模态大语言模型的能力、局限性和风险在俄语环境中尚未得到充分理解。

Method: 构建了包含18个全新评估任务的多模态基准框架，涵盖通用模型和模态特定架构，采用指令式评估方法，并包含防泄露机制。

Result: 为闭源和开源模型提供了基线结果，建立了统一的多模态能力分类体系和评估标准。

Conclusion: 该基准不仅填补了俄语多模态评估的空白，其方法论还可复制到其他斯拉夫语系和类型多样的语言中。

Abstract: Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.

</details>


### [27] [HSKBenchmark: Modeling and Benchmarking Chinese Second Language Acquisition in Large Language Models through Curriculum Tuning](https://arxiv.org/abs/2511.15574)
*Qihao Yang,Xuelin Wang,Jiale Chen,Xuelian Dong,Yuxin Hao,Tianyong Hao*

Main category: cs.CL

TL;DR: HSKBenchmark是首个用于中文二语习得的分阶段建模和写作评估基准，涵盖HSK 3-6级，包含676万词符的真实教材、16K合成指令样本、30个测试主题和基于语言学的评估系统。


<details>
  <summary>Details</summary>
Motivation: 语言习得研究对揭示人类语言智能本质至关重要，但控制人类学习者语言输入的实验在伦理和实践上不可行，这给语言习得建模的可验证性和可扩展性带来挑战。

Method: 提出课程调优框架，从初级到高级训练模型；创建评估系统检查语法覆盖、写作错误、词汇句法复杂度和整体评分；构建HSKAgent，在1万篇学习者作文上微调。

Result: 实验结果表明HSKBenchmark能有效建模中文二语习得，并作为LLM动态写作评估的可靠基准。微调的LLM写作表现与高级人类学习者相当，并展现类人习得特征。

Conclusion: HSKBenchmark、HSKAgent和检查点作为基础工具和资源，有望为未来语言习得建模和LLM可解释性研究铺平道路。

Abstract: Language acquisition is vital to revealing the nature of human language intelligence and has recently emerged as a promising perspective for improving the interpretability of large language models (LLMs). However, it is ethically and practically infeasible to conduct experiments that require controlling human learners' language inputs. This poses challenges for the verifiability and scalability of language acquisition modeling, particularly in Chinese second language acquisition (SLA). While LLMs provide a controllable and reproducible alternative, a systematic benchmark to support phase-wise modeling and assessment is still lacking. In this paper, we present HSKBenchmark, the first benchmark for staged modeling and writing assessment of LLMs in Chinese SLA. It covers HSK levels 3 to 6 and includes authentic textbooks with 6.76 million tokens, 16K synthetic instruction samples, 30 test topics, and a linguistically grounded evaluation system. To simulate human learning trajectories, we introduce a curriculum-tuning framework that trains models from beginner to advanced levels. An evaluation system is created to examine level-based grammar coverage, writing errors, lexical and syntactic complexity, and holistic scoring. We also build HSKAgent, fine-tuned on 10K learner compositions. Extensive experimental results demonstrate that HSKBenchmark not only models Chinese SLA effectively, but also serves as a reliable benchmark for dynamic writing assessment in LLMs. Our fine-tuned LLMs have writing performance on par with advanced human learners and exhibit human-like acquisition characteristics. The HSKBenchmark, HSKAgent, and checkpoints serve as foundational tools and resources, with the potential to pave the way for future research on language acquisition modeling and LLMs interpretability. Code and data are publicly available at: https://github.com/CharlesYang030/HSKB.

</details>


### [28] [Tokenisation over Bounded Alphabets is Hard](https://arxiv.org/abs/2511.15709)
*Violeta Kastreva,Philip Whittington,Dennis Komm,Tiago Pimentel*

Main category: cs.CL

TL;DR: 本文证明即使在有界字母表（包括二进制和一元字母表）上，tokenization也是NP完全问题，且不存在多项式时间近似方案，解释了为什么实际算法如BPE和UnigramLM都是启发式的。


<details>
  <summary>Details</summary>
Motivation: 现有研究假设tokenization应用于无界大字母表，但实践中tokenizer在固定大小字母表（如字节或Unicode字符）上操作。本文旨在填补这一空白，分析有界n元字母表上的tokenization问题。

Method: 分析两种自然变体：自底向上tokenization（选择合并操作序列）和直接tokenization（选择词汇表），证明即使在二进制字母表上，两种变体都是NP完全的且无多项式时间近似方案。

Result: 证明即使使用二进制字母表，两种tokenization变体都是NP完全的且无PTAS；直接tokenization在一元字母表上也是NP完全的。

Conclusion: tokenization的计算难处理性不是大字母表或复杂构造的产物，而是根本障碍，解释了为什么实际算法是启发式的，并指出近似算法是tokenization研究的重要方向。

Abstract: Recent works have shown that tokenisation is NP-complete. However, these works assume tokenisation is applied to inputs with unboundedly large alphabets -- an unrealistic assumption, given that in practice tokenisers operate over fixed-size alphabets, such as bytes or Unicode characters. We close this gap by analysing tokenisation over bounded $n$-ary alphabets, considering two natural variants: bottom-up tokenisation and direct tokenisation, where we must, respectively, select a sequence of merge operations or a vocabulary whose application optimally compresses a dataset. First, we note that proving hardness results for an $n$-ary alphabet proves the same results for alphabets of any larger size. We then prove that even with binary alphabets, both variants are not only NP-complete, but admit no polynomial-time approximation scheme (unless P=NP). We further show that direct tokenisation remains NP-complete even when applied to unary alphabets. While unary alphabets may not be practically useful, this result establishes that the computational intractability of tokenisation is not an artifact of large alphabets or complex constructions, but a fundamental barrier. Overall, our results explain why practical algorithms such as BPE and UnigramLM are heuristic, and points toward approximation algorithms being an important path going forward for tokenisation research.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [29] [The Illusion of Procedural Reasoning: Measuring Long-Horizon FSM Execution in LLMs](https://arxiv.org/abs/2511.14777)
*Mahdi Samiei,Mahdi Mansouri,Mahdieh Soleymani Baghshah*

Main category: cs.AI

TL;DR: 本文提出了有限状态机(FSM)执行作为评估LLMs程序推理能力的框架，通过控制任务复杂度和分支因素来系统测量模型在多步骤确定性计算中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏能够隔离和测量LLMs在扩展推理链中性能下降的受控、可解释基准，需要建立评估模型程序推理能力的严谨实验基础。

Method: 使用FSM执行框架，给模型明确的FSM定义并要求其逐步执行输入动作，在多个回合中保持状态一致性，测量回合准确率和任务准确率。

Result: 实验结果显示随着任务视野或分支复杂度增加，模型性能系统性下降；在规则检索涉及高分支因素时表现更差；大模型局部准确率提高但在多步骤推理中仍然脆弱。

Conclusion: FSM评估为诊断LLMs程序推理失败模式提供了透明、复杂度可控的探针，有助于理解和改进LLMs的算法可靠性。

Abstract: Large language models (LLMs) have achieved remarkable results on tasks framed as reasoning problems, yet their true ability to perform procedural reasoning, executing multi-step, rule-based computations remains unclear. Unlike algorithmic systems, which can deterministically execute long-horizon symbolic procedures, LLMs often degrade under extended reasoning chains, but there is no controlled, interpretable benchmark to isolate and measure this collapse. We introduce Finite-State Machine (FSM) Execution as a minimal, fully interpretable framework for evaluating the procedural reasoning capacity of LLMs. In our setup, the model is given an explicit FSM definition and must execute it step-by-step given input actions, maintaining state consistency over multiple turns. This task requires no world knowledge, only faithful application of deterministic transition rules, making it a direct probe of the model's internal procedural fidelity. We measure both Turn Accuracy and Task Accuracy to disentangle immediate computation from cumulative state maintenance. Empirical results reveal systematic degradation as task horizon or branching complexity increases. Models perform significantly worse when rule retrieval involves high branching factors than when memory span is long. Larger models show improved local accuracy but remain brittle under multi-step reasoning unless explicitly prompted to externalize intermediate steps. FSM-based evaluation offers a transparent, complexity-controlled probe for diagnosing this failure mode and guiding the design of inductive biases that enable genuine long-horizon procedural competence. By grounding reasoning in measurable execution fidelity rather than surface correctness, this work helps establish a rigorous experimental foundation for understanding and improving the algorithmic reliability of LLMs.

</details>


### [30] [Learning Interestingness in Automated Mathematical Theory Formation](https://arxiv.org/abs/2511.14778)
*George Tsoukalas,Rahul Saha,Amitayush Thakur,Sabrina Reguyal,Swarat Chaudhuri*

Main category: cs.AI

TL;DR: FERMAT是一个强化学习环境，用于自动化数学理论发现，通过符号操作建模概念发现和定理证明，并探索使用进化算法自动评估数学对象有趣性的问题。


<details>
  <summary>Details</summary>
Motivation: 解决人工智能中开放式的数学理论发现这一重大挑战，特别是自动评估数学对象的有趣性。

Method: 引入FERMAT强化学习环境，使用基于LLM的进化算法合成非平凡的有趣性度量，包含函数抽象机制。

Result: 在初等数论和有限域领域，该方法相比硬编码基线取得了显著改进。

Conclusion: FERMAT环境为数学理论发现开辟了新的强化学习问题空间，基于LLM的进化算法在自动评估数学有趣性方面表现出色。

Abstract: We take two key steps in automating the open-ended discovery of new mathematical theories, a grand challenge in artificial intelligence. First, we introduce $\emph{FERMAT}$, a reinforcement learning (RL) environment that models concept discovery and theorem-proving using a set of symbolic actions, opening up a range of RL problems relevant to theory discovery. Second, we explore a specific problem through $\emph{FERMAT}$: automatically scoring the $\emph{interestingness}$ of mathematical objects. We investigate evolutionary algorithms for synthesizing nontrivial interestingness measures. In particular, we introduce an LLM-based evolutionary algorithm that features function abstraction, leading to notable improvements in discovering elementary number theory and finite fields over hard-coded baselines. We open-source the $\emph{FERMAT}$ environment at this URL(https://github.com/trishullab/Fermat).

</details>


### [31] [Ask WhAI:Probing Belief Formation in Role-Primed LLM Agents](https://arxiv.org/abs/2511.14780)
*Keith Moore,Jun W. Kim,David Lyu,Jeffrey Heo,Ehsan Adeli*

Main category: cs.AI

TL;DR: Ask WhAI是一个用于检查和扰动多智能体交互中信念状态的系统级框架，通过记录回放交互、查询智能体信念和理由、注入反事实证据来测试信念结构对新信息的响应。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体科学推理中的信念形成和认知孤岛，提供一种可重现的方法来观察和测试这些动态，这在人类专家中是不可能的。

Method: 使用具有多智能体共享内存（时间戳电子病历）和持有真实实验室结果的预言智能体的医疗案例模拟器，对具有特定角色先验的LLM智能体进行压力测试，在关键诊断时刻设置断点进行前后信念查询。

Result: 智能体信念往往反映现实世界的学科立场，包括过度依赖规范研究和抵制反证据，这些信念可以被追踪和询问。

Conclusion: Ask WhAI通过使这些动态可见和可测试，为研究多智能体科学推理中的信念形成和认知孤岛提供了一种可重现的方法。

Abstract: We present Ask WhAI, a systems-level framework for inspecting and perturbing belief states in multi-agent interactions. The framework records and replays agent interactions, supports out-of-band queries into each agent's beliefs and rationale, and enables counterfactual evidence injection to test how belief structures respond to new information. We apply the framework to a medical case simulator notable for its multi-agent shared memory (a time-stamped electronic medical record, or EMR) and an oracle agent (the LabAgent) that holds ground truth lab results revealed only when explicitly queried. We stress-test the system on a multi-specialty diagnostic journey for a child with an abrupt-onset neuropsychiatric presentation. Large language model agents, each primed with strong role-specific priors ("act like a neurologist", "act like an infectious disease specialist"), write to a shared medical record and interact with a moderator across sequential or parallel encounters. Breakpoints at key diagnostic moments enable pre- and post-event belief queries, allowing us to distinguish entrenched priors from reasoning or evidence-integration effects. The simulation reveals that agent beliefs often mirror real-world disciplinary stances, including overreliance on canonical studies and resistance to counterevidence, and that these beliefs can be traced and interrogated in ways not possible with human experts. By making such dynamics visible and testable, Ask WhAI offers a reproducible way to study belief formation and epistemic silos in multi-agent scientific reasoning.

</details>


### [32] [Subnational Geocoding of Global Disasters Using Large Language Models](https://arxiv.org/abs/2511.14788)
*Michele Ronco,Damien Delforge,Wiebke S. Jäger,Christina Corbane*

Main category: cs.AI

TL;DR: 开发了一个完全自动化的LLM辅助工作流，用于处理灾害数据库中的非结构化位置文本，通过GPT-4o清理位置信息并使用三个地理信息库进行交叉验证，生成子国家级几何图形和可靠性评分。


<details>
  <summary>Details</summary>
Motivation: 灾害数据库如EM-DAT中的位置数据通常以非结构化文本形式报告，具有不一致的粒度和拼写，难以与空间数据集集成，阻碍了风险评估和灾害风险减少工作。

Method: 使用GPT-4o处理清理文本位置信息，通过交叉检查GADM、OpenStreetMap和Wikidata三个独立地理信息库来分配几何图形，基于源的一致性和可用性为每个位置分配可靠性评分。

Result: 应用于2000-2024年EM-DAT数据集，该工作流对14,215个事件中的17,948个独特位置进行了地理编码，无需人工干预，覆盖所有灾害类型。

Conclusion: 该方法展示了LLMs从非结构化文本中提取和构建地理信息的潜力，为相关分析提供了可扩展且可靠的方法，并允许灵活地重新映射到首选框架。

Abstract: Subnational location data of disaster events are critical for risk assessment and disaster risk reduction. Disaster databases such as EM-DAT often report locations in unstructured textual form, with inconsistent granularity or spelling, that make it difficult to integrate with spatial datasets. We present a fully automated LLM-assisted workflow that processes and cleans textual location information using GPT-4o, and assigns geometries by cross-checking three independent geoinformation repositories: GADM, OpenStreetMap and Wikidata. Based on the agreement and availability of these sources, we assign a reliability score to each location while generating subnational geometries. Applied to the EM-DAT dataset from 2000 to 2024, the workflow geocodes 14,215 events across 17,948 unique locations. Unlike previous methods, our approach requires no manual intervention, covers all disaster types, enables cross-verification across multiple sources, and allows flexible remapping to preferred frameworks. Beyond the dataset, we demonstrate the potential of LLMs to extract and structure geographic information from unstructured text, offering a scalable and reliable method for related analyses.

</details>


### [33] [Project Rachel: Can an AI Become a Scholarly Author?](https://arxiv.org/abs/2511.14819)
*Martin Monperrus,Benoit Baudry,Clément Vidal*

Main category: cs.AI

TL;DR: 通过创建AI学术身份Rachel So并发表10+篇AI生成论文，研究学术生态系统对AI作者身份的反应，包括论文被引用和收到同行评审邀请。


<details>
  <summary>Details</summary>
Motivation: 调查学术生态系统如何应对AI作者身份，为关于超人类AI系统参与学术交流的未来辩论提供实证数据。

Method: 行动研究：创建AI学术身份Rachel So，在2025年3月至10月期间发表10+篇AI生成研究论文，并追踪其学术影响。

Result: Rachel So的论文被引用，并收到了同行评审邀请，表明AI作者身份在学术生态系统中获得了一定程度的认可。

Conclusion: AI作者身份对出版商、研究人员和整个科学系统具有重要影响，需要就AI参与学术交流的未来进行必要辩论。

Abstract: This paper documents Project Rachel, an action research study that created and tracked a complete AI academic identity named Rachel So. Through careful publication of AI-generated research papers, we investigate how the scholarly ecosystem responds to AI authorship. Rachel So published 10+ papers between March and October 2025, was cited, and received a peer review invitation. We discuss the implications of AI authorship on publishers, researchers, and the scientific system at large. This work contributes empirical action research data to the necessary debate about the future of scholarly communication with super human, hyper capable AI systems.

</details>


### [34] [Uncertainty-Aware Measurement of Scenario Suite Representativeness for Autonomous Systems](https://arxiv.org/abs/2511.14853)
*Robab Aghazadeh Chakherlou,Siddartha Khastgir,Xingyu Zhao,Jerein Jeyachandran,Shufeng Chen*

Main category: cs.AI

TL;DR: 提出一种概率方法来量化AI系统训练测试数据集的代表性，通过比较场景套件特征分布与目标操作域(TOD)特征分布，使用不精确贝叶斯方法处理有限数据和先验不确定性，生成区间值代表性估计。


<details>
  <summary>Details</summary>
Motivation: 确保AI系统(如自动驾驶汽车)的可信度和安全性依赖于训练测试数据集的代表性，即数据是否充分反映系统设计的操作条件(ODD)或预期遇到的操作环境(TOD)。

Method: 采用概率方法比较场景套件特征分布与TOD特征分布，使用不精确贝叶斯方法处理有限数据和先验不确定性，生成区间值代表性估计而非单一值。

Result: 通过数值示例展示了在不同操作类别(天气、道路类型、时间等)下，考虑依赖关系和先验不确定性时，场景套件与推断TOD分布的比较，并估计了局部和全局代表性区间。

Conclusion: 该方法能够量化数据集的代表性，并考虑不确定性，为AI系统的安全评估提供更可靠的依据。

Abstract: Assuring the trustworthiness and safety of AI systems, e.g., autonomous vehicles (AV), depends critically on the data-related safety properties, e.g., representativeness, completeness, etc., of the datasets used for their training and testing. Among these properties, this paper focuses on representativeness-the extent to which the scenario-based data used for training and testing, reflect the operational conditions that the system is designed to operate safely in, i.e., Operational Design Domain (ODD) or expected to encounter, i.e., Target Operational Domain (TOD). We propose a probabilistic method that quantifies representativeness by comparing the statistical distribution of features encoded by the scenario suites with the corresponding distribution of features representing the TOD, acknowledging that the true TOD distribution is unknown, as it can only be inferred from limited data.
  We apply an imprecise Bayesian method to handle limited data and uncertain priors. The imprecise Bayesian formulation produces interval-valued, uncertainty-aware estimates of representativeness, rather than a single value. We present a numerical example comparing the distributions of the scenario suite and the inferred TOD across operational categories-weather, road type, time of day, etc., under dependencies and prior uncertainty. We estimate representativeness locally (between categories) and globally as an interval.

</details>


### [35] [Task Specific Sharpness Aware O-RAN Resource Management using Multi Agent Reinforcement Learning](https://arxiv.org/abs/2511.15002)
*Fatemeh Lotfi,Hossein Rajoli,Fatemeh Afghah*

Main category: cs.AI

TL;DR: 提出了一种结合Sharpness-Aware Minimization(SAM)的增强型Soft Actor Critic算法，在分布式多智能体强化学习框架中实现O-RAN资源管理，通过基于TD误差方差的适应性SAM机制提升鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 下一代网络采用O-RAN架构实现动态资源管理，但深度强化学习模型在动态环境中往往缺乏鲁棒性和泛化性，需要改进算法来应对这一挑战。

Method: 在分布式多智能体强化学习框架中，将SAM与SAC算法结合，引入基于TD误差方差的适应性SAM机制，只对面临高环境复杂度的智能体进行正则化，同时采用动态ρ调度方案优化探索-利用权衡。

Result: 实验结果表明，该方法显著优于传统DRL方法，资源分配效率提升高达22%，并在多样化O-RAN切片中确保优异的QoS满意度。

Conclusion: 所提出的方法通过针对性正则化和动态调度机制，有效提升了O-RAN资源管理的鲁棒性、泛化能力和效率，为下一代网络资源优化提供了有效解决方案。

Abstract: Next-generation networks utilize the Open Radio Access Network (O-RAN) architecture to enable dynamic resource management, facilitated by the RAN Intelligent Controller (RIC). While deep reinforcement learning (DRL) models show promise in optimizing network resources, they often struggle with robustness and generalizability in dynamic environments. This paper introduces a novel resource management approach that enhances the Soft Actor Critic (SAC) algorithm with Sharpness-Aware Minimization (SAM) in a distributed Multi-Agent RL (MARL) framework. Our method introduces an adaptive and selective SAM mechanism, where regularization is explicitly driven by temporal-difference (TD)-error variance, ensuring that only agents facing high environmental complexity are regularized. This targeted strategy reduces unnecessary overhead, improves training stability, and enhances generalization without sacrificing learning efficiency. We further incorporate a dynamic $ρ$ scheduling scheme to refine the exploration-exploitation trade-off across agents. Experimental results show our method significantly outperforms conventional DRL approaches, yielding up to a $22\%$ improvement in resource allocation efficiency and ensuring superior QoS satisfaction across diverse O-RAN slices.

</details>


### [36] [Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization](https://arxiv.org/abs/2511.15055)
*Jian-Ting Guo,Yu-Cheng Chen,Ping-Chun Hsieh,Kuo-Hao Ho,Po-Wei Huang,Ti-Rong Wu,I-Chen Wu*

Main category: cs.AI

TL;DR: MAQ是一个人类化强化学习框架，通过向量量化VAE从人类演示中提取宏观动作，在保持奖励最大化的同时实现类似人类的行为。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习智能体虽然在许多领域表现出色，但往往产生与人类行为不符的非自然行为，这影响了可解释性和可信度。目标是设计能够产生类似人类行为的RL智能体。

Method: 将人类相似性建模为轨迹优化问题，引入宏观动作量化(MAQ)框架，使用向量量化VAE从人类演示中提取宏观动作，并采用后退时域控制作为可扩展实现。

Result: 在D4RL Adroit基准测试中，MAQ显著提高了人类相似度，增加了轨迹相似性分数，并在人类评估研究中获得了所有RL智能体中最高的人类相似度排名。

Conclusion: MAQ可以轻松集成到各种现成的RL算法中，为学习人类化RL智能体开辟了有前景的方向。

Abstract: Human-like agents have long been one of the goals in pursuing artificial intelligence. Although reinforcement learning (RL) has achieved superhuman performance in many domains, relatively little attention has been focused on designing human-like RL agents. As a result, many reward-driven RL agents often exhibit unnatural behaviors compared to humans, raising concerns for both interpretability and trustworthiness. To achieve human-like behavior in RL, this paper first formulates human-likeness as trajectory optimization, where the objective is to find an action sequence that closely aligns with human behavior while also maximizing rewards, and adapts the classic receding-horizon control to human-like learning as a tractable and efficient implementation. To achieve this, we introduce Macro Action Quantization (MAQ), a human-like RL framework that distills human demonstrations into macro actions via Vector-Quantized VAE. Experiments on D4RL Adroit benchmarks show that MAQ significantly improves human-likeness, increasing trajectory similarity scores, and achieving the highest human-likeness rankings among all RL agents in the human evaluation study. Our results also demonstrate that MAQ can be easily integrated into various off-the-shelf RL algorithms, opening a promising direction for learning human-like RL agents. Our code is available at https://rlg.iis.sinica.edu.tw/papers/MAQ.

</details>


### [37] [Beyond GeneGPT: A Multi-Agent Architecture with Open-Source LLMs for Enhanced Genomic Question Answering](https://arxiv.org/abs/2511.15061)
*Haodong Chen,Guido Zuccon,Teerapong Leelanupab*

Main category: cs.AI

TL;DR: OpenBioLLM是一个开源的多智能体框架，通过模块化设计和智能体专业化，在基因组问答任务中匹配或超越了GeneGPT的性能，同时显著降低了延迟和成本。


<details>
  <summary>Details</summary>
Motivation: 解决GeneGPT依赖专有模型带来的可扩展性、运营成本、数据隐私和泛化性问题，探索开源模型在基因组问答中的应用潜力。

Method: 采用模块化多智能体框架，引入工具路由、查询生成和响应验证的智能体专业化，实现协调推理和基于角色的任务执行。

Result: 在90%以上的基准任务中匹配或超越GeneGPT，在Gene-Turing和GeneHop上分别获得0.849和0.830的平均分数，延迟降低40-50%。

Conclusion: 开源多智能体系统在基因组问答中具有巨大潜力，OpenBioLLM展示了在不牺牲性能的情况下提高效率和可访问性的可行性。

Abstract: Genomic question answering often requires complex reasoning and integration across diverse biomedical sources. GeneGPT addressed this challenge by combining domain-specific APIs with OpenAI's code-davinci-002 large language model to enable natural language interaction with genomic databases. However, its reliance on a proprietary model limits scalability, increases operational costs, and raises concerns about data privacy and generalization.
  In this work, we revisit and reproduce GeneGPT in a pilot study using open source models, including Llama 3.1, Qwen2.5, and Qwen2.5 Coder, within a monolithic architecture; this allows us to identify the limitations of this approach. Building on this foundation, we then develop OpenBioLLM, a modular multi-agent framework that extends GeneGPT by introducing agent specialization for tool routing, query generation, and response validation. This enables coordinated reasoning and role-based task execution.
  OpenBioLLM matches or outperforms GeneGPT on over 90% of the benchmark tasks, achieving average scores of 0.849 on Gene-Turing and 0.830 on GeneHop, while using smaller open-source models without additional fine-tuning or tool-specific pretraining. OpenBioLLM's modular multi-agent design reduces latency by 40-50% across benchmark tasks, significantly improving efficiency without compromising model capability. The results of our comprehensive evaluation highlight the potential of open-source multi-agent systems for genomic question answering. Code and resources are available at https://github.com/ielab/OpenBioLLM.

</details>


### [38] [ProRAC: A Neuro-symbolic Method for Reasoning about Actions with LLM-based Progression](https://arxiv.org/abs/2511.15069)
*Haoyong Wu,Yongmei Liu*

Main category: cs.AI

TL;DR: ProRAC是一个神经符号框架，利用LLM解决动作与变化推理问题，通过提取动作和问题元素、逐步执行动作推导最终状态，然后评估查询来得出答案。


<details>
  <summary>Details</summary>
Motivation: 解决动作与变化推理问题，利用LLM的能力来处理复杂的推理任务。

Method: 提取RAC问题中的基本元素（动作和问题），逐步执行每个动作来推导最终状态，然后评估查询以得出答案。

Result: 在多个RAC基准测试中表现出色，在不同基准、领域、LLM主干和RAC任务类型上均取得强劲性能。

Conclusion: ProRAC框架在动作与变化推理问题上具有强大的性能表现，适用于多种场景和任务类型。

Abstract: In this paper, we propose ProRAC (Progression-based Reasoning about Actions and Change), a neuro-symbolic framework that leverages LLMs to tackle RAC problems. ProRAC extracts fundamental RAC elements including actions and questions from the problem, progressively executes each action to derive the final state, and then evaluates the query against the progressed state to arrive at an answer. We evaluate ProRAC on several RAC benchmarks, and the results demonstrate that our approach achieves strong performance across different benchmarks, domains, LLM backbones, and types of RAC tasks.

</details>


### [39] [Knowledge-Informed Automatic Feature Extraction via Collaborative Large Language Model Agents](https://arxiv.org/abs/2511.15074)
*Henrik Bradland,Morten Goodwin,Vladimir I. Zadorozhny,Per-Arne Andersen*

Main category: cs.AI

TL;DR: Rogue One是一个基于LLM的多智能体框架，通过三个专门化智能体（科学家、提取器、测试器）的协作，结合外部知识检索和丰富的定性反馈机制，实现知识引导的自动特征提取。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自动特征提取方法存在架构单一、反馈机制简单、缺乏外部知识整合等问题，限制了特征提取的质量和可解释性。

Method: 采用多智能体框架，包含科学家、提取器、测试器三个专门化智能体，结合检索增强生成(RAG)系统获取外部知识，使用"泛滥-修剪"策略平衡特征探索与利用，并引入丰富的定性反馈机制。

Result: 在19个分类和9个回归数据集上显著优于现有最先进方法，并能发现新颖、可测试的假设（如识别心肌数据集中的新潜在生物标志物）。

Conclusion: Rogue One不仅提升了特征提取的统计性能，还增强了特征的可解释性和语义意义，为科学发现提供了有力工具。

Abstract: The performance of machine learning models on tabular data is critically dependent on high-quality feature engineering. While Large Language Models (LLMs) have shown promise in automating feature extraction (AutoFE), existing methods are often limited by monolithic LLM architectures, simplistic quantitative feedback, and a failure to systematically integrate external domain knowledge. This paper introduces Rogue One, a novel, LLM-based multi-agent framework for knowledge-informed automatic feature extraction. Rogue One operationalizes a decentralized system of three specialized agents-Scientist, Extractor, and Tester-that collaborate iteratively to discover, generate, and validate predictive features. Crucially, the framework moves beyond primitive accuracy scores by introducing a rich, qualitative feedback mechanism and a "flooding-pruning" strategy, allowing it to dynamically balance feature exploration and exploitation. By actively incorporating external knowledge via an integrated retrieval-augmented (RAG) system, Rogue One generates features that are not only statistically powerful but also semantically meaningful and interpretable. We demonstrate that Rogue One significantly outperforms state-of-the-art methods on a comprehensive suite of 19 classification and 9 regression datasets. Furthermore, we show qualitatively that the system surfaces novel, testable hypotheses, such as identifying a new potential biomarker in the myocardial dataset, underscoring its utility as a tool for scientific discovery.

</details>


### [40] [SafeRBench: A Comprehensive Benchmark for Safety Assessment in Large Reasoning Models](https://arxiv.org/abs/2511.15169)
*Xin Gao,Shaohan Yu,Zerui Chen,Yueming Lyu,Weichen Yu,Guanghao Li,Jiyao Liu,Jianxiong Gao,Jian Liang,Ziwei Liu,Chenyang Si*

Main category: cs.AI

TL;DR: SafeRBench是第一个端到端评估大型推理模型安全性的基准，从输入、中间推理到最终输出全面评估安全风险，包含风险分类分级、微思维分块机制和人类安全对齐验证。


<details>
  <summary>Details</summary>
Motivation: 现有安全评估主要关注输出层面，无法捕捉推理过程中动态的安全风险，如有害内容的渐进式注入或被误导性理由合理化。

Method: 1) 输入特征化：将风险类别和级别纳入输入设计；2) 细粒度输出分析：通过微思维分块机制将长推理轨迹分段评估；3) 人类安全对齐：基于人类标注验证LLM评估结果。

Result: 在19个大型推理模型上的评估表明，SafeRBench能够进行详细的多维度安全评估，从多个角度提供风险和保护机制的洞察。

Conclusion: SafeRBench填补了推理模型安全评估的空白，为全面理解模型安全风险提供了系统框架。

Abstract: Large Reasoning Models (LRMs) improve answer quality through explicit chain-of-thought, yet this very capability introduces new safety risks: harmful content can be subtly injected, surface gradually, or be justified by misleading rationales within the reasoning trace. Existing safety evaluations, however, primarily focus on output-level judgments and rarely capture these dynamic risks along the reasoning process. In this paper, we present SafeRBench, the first benchmark that assesses LRM safety end-to-end -- from inputs and intermediate reasoning to final outputs. (1) Input Characterization: We pioneer the incorporation of risk categories and levels into input design, explicitly accounting for affected groups and severity, and thereby establish a balanced prompt suite reflecting diverse harm gradients. (2) Fine-Grained Output Analysis: We introduce a micro-thought chunking mechanism to segment long reasoning traces into semantically coherent units, enabling fine-grained evaluation across ten safety dimensions. (3) Human Safety Alignment: We validate LLM-based evaluations against human annotations specifically designed to capture safety judgments. Evaluations on 19 LRMs demonstrate that SafeRBench enables detailed, multidimensional safety assessment, offering insights into risks and protective mechanisms from multiple perspectives.

</details>


### [41] [HISE-KT: Synergizing Heterogeneous Information Networks and LLMs for Explainable Knowledge Tracing with Meta-Path Optimization](https://arxiv.org/abs/2511.15191)
*Zhiyi Duan,Zixing Shi,Hongyu Yuan,Qi Wang*

Main category: cs.AI

TL;DR: HISE-KT是一个结合异构信息网络和大型语言模型的知识追踪框架，通过智能评分筛选元路径实例，并引入相似学生检索机制，在预测准确性和可解释性方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于异构信息网络的方法因手动或随机选择元路径而引入噪声，且缺乏元路径实例质量评估；而基于大语言模型的方法忽略了学生间的丰富信息。两种范式都难以提供准确且基于证据的解释。

Method: 构建多关系异构信息网络捕获结构关系，使用LLM智能评分筛选高质量元路径实例，设计基于元路径的相似学生检索机制，通过结构化提示整合目标学生历史和相似轨迹，让LLM生成预测和解释性分析报告。

Result: 在四个公共数据集上的实验表明，HISE-KT在预测性能和可解释性方面均优于现有知识追踪基线方法。

Conclusion: HISE-KT成功整合了异构信息网络和大型语言模型的优势，实现了更准确的知识追踪预测和基于证据的可解释分析，为教育技术领域提供了创新解决方案。

Abstract: Knowledge Tracing (KT) aims to mine students' evolving knowledge states and predict their future question-answering performance. Existing methods based on heterogeneous information networks (HINs) are prone to introducing noises due to manual or random selection of meta-paths and lack necessary quality assessment of meta-path instances. Conversely, recent large language models (LLMs)-based methods ignore the rich information across students, and both paradigms struggle to deliver consistently accurate and evidence-based explanations. To address these issues, we propose an innovative framework, HIN-LLM Synergistic Enhanced Knowledge Tracing (HISE-KT), which seamlessly integrates HINs with LLMs. HISE-KT first builds a multi-relationship HIN containing diverse node types to capture the structural relations through multiple meta-paths. The LLM is then employed to intelligently score and filter meta-path instances and retain high-quality paths, pioneering automated meta-path quality assessment. Inspired by educational psychology principles, a similar student retrieval mechanism based on meta-paths is designed to provide a more valuable context for prediction. Finally, HISE-KT uses a structured prompt to integrate the target student's history with the retrieved similar trajectories, enabling the LLM to generate not only accurate predictions but also evidence-backed, explainable analysis reports. Experiments on four public datasets show that HISE-KT outperforms existing KT baselines in both prediction performance and interpretability.

</details>


### [42] [As If We've Met Before: LLMs Exhibit Certainty in Recognizing Seen Files](https://arxiv.org/abs/2511.15192)
*Haodong Li,Jingqi Zhang,Xiao Cheng,Peihua Mai,Haoyu Wang,Yang Pan*

Main category: cs.AI

TL;DR: COPYCHECK是一个利用不确定性信号检测LLM训练数据中版权内容的新框架，将LLM的过度自信转化为优势，通过不确定性模式区分训练数据和非训练数据。


<details>
  <summary>Details</summary>
Motivation: LLM在大量数据集上训练，其中可能包含受版权保护的内容，引发未经授权使用的担忧。现有成员推理攻击方法因LLM过度自信、缺乏真实训练数据和依赖经验阈值而面临挑战。

Method: 采用两阶段策略：(1) 将文件分割成小片段以减少对大规模训练数据的依赖；(2) 使用不确定性引导的无监督聚类，无需经验调整阈值。

Result: 在LLaMA 7b和LLaMA2 7b上检测已见文件的平均平衡准确率分别达到90.1%和91.6%，相比SOTA基线相对提升超过90%，最高达到93.8%平衡准确率，在GPT-J 6B上保持高性能。

Conclusion: 这是首次将不确定性应用于LLM版权检测的工作，为训练数据透明度提供了实用工具。

Abstract: The remarkable language ability of Large Language Models (LLMs) stems from extensive training on vast datasets, often including copyrighted material, which raises serious concerns about unauthorized use. While Membership Inference Attacks (MIAs) offer potential solutions for detecting such violations, existing approaches face critical limitations and challenges due to LLMs' inherent overconfidence, limited access to ground truth training data, and reliance on empirically determined thresholds.
  We present COPYCHECK, a novel framework that leverages uncertainty signals to detect whether copyrighted content was used in LLM training sets. Our method turns LLM overconfidence from a limitation into an asset by capturing uncertainty patterns that reliably distinguish between ``seen" (training data) and ``unseen" (non-training data) content. COPYCHECK further implements a two-fold strategy: (1) strategic segmentation of files into smaller snippets to reduce dependence on large-scale training data, and (2) uncertainty-guided unsupervised clustering to eliminate the need for empirically tuned thresholds. Experiment results show that COPYCHECK achieves an average balanced accuracy of 90.1% on LLaMA 7b and 91.6% on LLaMA2 7b in detecting seen files. Compared to the SOTA baseline, COPYCHECK achieves over 90% relative improvement, reaching up to 93.8\% balanced accuracy. It further exhibits strong generalizability across architectures, maintaining high performance on GPT-J 6B. This work presents the first application of uncertainty for copyright detection in LLMs, offering practical tools for training data transparency.

</details>


### [43] [SOLID: a Framework of Synergizing Optimization and LLMs for Intelligent Decision-Making](https://arxiv.org/abs/2511.15202)
*Yinsheng Wang,Tario G You,Léonard Boussioux,Shan Liu*

Main category: cs.AI

TL;DR: SOLID框架通过双价格和偏差惩罚机制，将数学优化与大型语言模型协同工作，在保持模块化和数据隐私的同时提升决策质量。


<details>
  <summary>Details</summary>
Motivation: 结合数学优化的精确性和大型语言模型的上下文理解能力，实现更智能的决策制定，同时保持理论收敛性和实际应用价值。

Method: 提出SOLID框架，通过优化代理和LLM代理的迭代协作，使用双价格和偏差惩罚机制进行交互，在凸性假设下保持理论收敛性。

Result: 在股票投资组合案例中，相比仅使用优化器的方法，SOLID显示出收敛性并提高了年化收益率。

Conclusion: SOLID为跨领域的自动化和智能决策制定提供了一个有前景的框架，验证了优化与LLM协同的有效性。

Abstract: This paper introduces SOLID (Synergizing Optimization and Large Language Models for Intelligent Decision-Making), a novel framework that integrates mathematical optimization with the contextual capabilities of large language models (LLMs). SOLID facilitates iterative collaboration between optimization and LLMs agents through dual prices and deviation penalties. This interaction improves the quality of the decisions while maintaining modularity and data privacy. The framework retains theoretical convergence guarantees under convexity assumptions, providing insight into the design of LLMs prompt. To evaluate SOLID, we applied it to a stock portfolio investment case with historical prices and financial news as inputs. Empirical results demonstrate convergence under various scenarios and indicate improved annualized returns compared to a baseline optimizer-only method, validating the synergy of the two agents. SOLID offers a promising framework for advancing automated and intelligent decision-making across diverse domains.

</details>


### [44] [Efficiency Will Not Lead to Sustainable Reasoning AI](https://arxiv.org/abs/2511.15259)
*Philipp Wiesner,Daniel W. O'Neill,Francesca Larosa,Odej Kao*

Main category: cs.AI

TL;DR: AI研究正转向复杂问题解决，但推理AI缺乏需求饱和点，性能随计算投入指数增长，仅靠效率无法实现可持续性，需要嵌入明确限制。


<details>
  <summary>Details</summary>
Motivation: AI研究从模式识别转向多步推理，但推理AI性能随计算投入指数增长，缺乏自然需求饱和点，效率改进接近物理极限，需要关注可持续性问题。

Method: 分析推理AI与传统计算在能效和需求模式上的差异，讨论效率改进的物理限制，提出需要嵌入明确限制的优化和治理方向。

Result: 推理AI缺乏类似传统计算的需求饱和机制，性能持续随计算投入指数增长，仅靠效率改进无法实现可持续性。

Conclusion: 效率改进不足以实现可持续推理AI，需要在系统优化和治理中嵌入明确限制，并探索相关研究和政策方向。

Abstract: AI research is increasingly moving toward complex problem solving, where models are optimized not only for pattern recognition but for multi-step reasoning. Historically, computing's global energy footprint has been stabilized by sustained efficiency gains and natural saturation thresholds in demand. But as efficiency improvements are approaching physical limits, emerging reasoning AI lacks comparable saturation points: performance is no longer limited by the amount of available training data but continues to scale with exponential compute investments in both training and inference. This paper argues that efficiency alone will not lead to sustainable reasoning AI and discusses research and policy directions to embed explicit limits into the optimization and governance of such systems.

</details>


### [45] [Realist and Pluralist Conceptions of Intelligence and Their Implications on AI Research](https://arxiv.org/abs/2511.15282)
*Ninell Oldenburg,Ruchira Dhar,Anders Søgaard*

Main category: cs.AI

TL;DR: 论文分析了AI研究中两种对立的智力观念：智力现实主义认为智力是单一通用能力，智力多元主义认为智力是多样情境依赖的能力。这两种观念在方法论、解释和风险评估方面产生根本不同的研究路径。


<details>
  <summary>Details</summary>
Motivation: 揭示AI研究中隐含的智力观念如何影响研究方法和风险评估，帮助澄清该领域的分歧。

Method: 通过分析当前AI研究中的辩论，展示两种智力观念如何影响模型选择、基准设计、实验验证、现象解释和风险评估。

Result: 发现智力现实主义导致追求通用智能和统一对齐方案，而智力多元主义关注多样化威胁和情境特定解决方案。

Conclusion: 明确这些基本假设有助于更清晰地理解AI研究中的分歧，促进更富有成效的讨论。

Abstract: In this paper, we argue that current AI research operates on a spectrum between two different underlying conceptions of intelligence: Intelligence Realism, which holds that intelligence represents a single, universal capacity measurable across all systems, and Intelligence Pluralism, which views intelligence as diverse, context-dependent capacities that cannot be reduced to a single universal measure. Through an analysis of current debates in AI research, we demonstrate how the conceptions remain largely implicit yet fundamentally shape how empirical evidence gets interpreted across a wide range of areas. These underlying views generate fundamentally different research approaches across three areas. Methodologically, they produce different approaches to model selection, benchmark design, and experimental validation. Interpretively, they lead to contradictory readings of the same empirical phenomena, from capability emergence to system limitations. Regarding AI risk, they generate categorically different assessments: realists view superintelligence as the primary risk and search for unified alignment solutions, while pluralists see diverse threats across different domains requiring context-specific solutions. We argue that making explicit these underlying assumptions can contribute to a clearer understanding of disagreements in AI research.

</details>


### [46] [Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration](https://arxiv.org/abs/2511.15351)
*Yifu Guo,Zishan Xu,Zhiyuan Yao,Yuquan Lu,Jiaye Lin,Sen Hu,Zhenheng Tang,Yingchao Li,Huacan Wang,Ronghao Chen*

Main category: cs.AI

TL;DR: Octopus：一种新的多模态代理推理范式，通过六种核心能力的编排实现自主推理路径探索和动态能力选择，在Octopus-Bench基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推理模型存在架构限制，缺乏人类式的自主探索多样化推理路径的能力，无法适应动态变化的现实任务需求。

Method: 提出Octopus框架，定义六种核心多模态推理能力，构建Octopus-Bench评估基准，实现自主推理探索和动态能力选择。

Result: 实验结果显示Octopus在Octopus-Bench绝大多数任务中取得最佳性能。

Conclusion: 能力协调在多模态代理推理中起着关键作用，Octopus证明了这种新范式的有效性。

Abstract: Existing multimodal reasoning models and frameworks suffer from fundamental architectural limitations: most lack the human-like ability to autonomously explore diverse reasoning pathways-whether in direct inference, tool-driven visual exploration, programmatic visual manipulation, or intrinsic visual imagination. Consequently, they struggle to adapt to dynamically changing capability requirements in real-world tasks. Meanwhile, humans exhibit a complementary set of thinking abilities when addressing such tasks, whereas existing methods typically cover only a subset of these dimensions. Inspired by this, we propose Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration, a new paradigm for multimodal agentic reasoning. We define six core capabilities essential for multimodal reasoning and organize a comprehensive evaluation benchmark, Octopus-Bench, accordingly. Octopus is capable of autonomously exploring during reasoning and dynamically selecting the most appropriate capability based on the current state. Experimental results show that Octopus achieves the best performance on the vast majority of tasks in Octopus-Bench, highlighting the crucial role of capability coordination in agentic multimodal reasoning.

</details>


### [47] [Terra Nova: A Comprehensive Challenge Environment for Intelligent Agents](https://arxiv.org/abs/2511.15378)
*Trevor McInroe*

Main category: cs.AI

TL;DR: Terra Nova是一个基于《文明V》的综合性挑战环境，旨在同时测试强化学习智能体在部分可观测性、信用分配、表示学习、巨大动作空间等多个经典挑战上的综合能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多任务基准主要评估智能体在不同独立任务间切换策略的能力，而非测试其在多个相互关联挑战中进行深度推理的能力。

Method: 设计了一个单一环境，让多个经典RL挑战同时出现，要求智能体具备跨多个交互变量的集成、长视野理解能力。

Result: 提出了Terra Nova这一新的综合性挑战环境，区别于仅聚合不相关任务的并行流方法。

Conclusion: Terra Nova为测试强化学习智能体在复杂、交互式环境中的深度推理能力提供了更合适的基准。

Abstract: We introduce Terra Nova, a new comprehensive challenge environment (CCE) for reinforcement learning (RL) research inspired by Civilization V. A CCE is a single environment in which multiple canonical RL challenges (e.g., partial observability, credit assignment, representation learning, enormous action spaces, etc.) arise simultaneously. Mastery therefore demands integrated, long-horizon understanding across many interacting variables. We emphasize that this definition excludes challenges that only aggregate unrelated tasks in independent, parallel streams (e.g., learning to play all Atari games at once). These aggregated multitask benchmarks primarily asses whether an agent can catalog and switch among unrelated policies rather than test an agent's ability to perform deep reasoning across many interacting challenges.

</details>


### [48] [IPR-1: Interactive Physical Reasoner](https://arxiv.org/abs/2511.15407)
*Mingyu Zhang,Lifeng Zhuo,Tianxi Tan,Guocan Xie,Xian Nie,Yan Li,Renjie Zhao,Zizhu He,Ziyu Wang,Jiting Cai,Yong-Lu Li*

Main category: cs.AI

TL;DR: 该论文提出IPR（交互式物理推理器），通过世界模型推演来增强VLM的策略，并引入PhysCode物理中心动作编码，在1000+游戏上预训练后，在生存、好奇心和实用性三个层次上表现出色，性能随训练游戏和交互步骤增加而提升。


<details>
  <summary>Details</summary>
Motivation: 研究智能体是否能够像人类一样通过与环境交互来获取物理和因果关系推理能力，并随着经验积累不断改进。

Method: 提出IPR框架，使用世界模型推演来评分和强化VLM的策略，引入PhysCode物理中心动作编码来对齐语义意图与动力学，为预测和推理提供共享动作空间。

Result: 在1000+游戏上预训练的IPR在三个推理层次上表现稳健，整体性能与GPT-5相当，在好奇心方面超越GPT-5，性能随训练游戏和交互步骤增加而提升，并能零样本迁移到未见过的游戏。

Conclusion: 物理中心的交互是实现持续改进物理推理能力的可行路径。

Abstract: Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. We study this in a Game-to-Unseen (G2U) setting, curating 1,000+ heterogeneous games with diverse physical and causal mechanisms, and evaluate at three human-like levels: Survival, Curiosity, Utility, from primitive intuition to goal-driven reasoning. Our analysis reveals complementary failures: VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on three levels, matches GPT-5 overall, and surpasses it on Curiosity. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning.

</details>


### [49] [Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining](https://arxiv.org/abs/2511.15456)
*Qian'ang Mao,Yuxuan Zhang,Jiaman Chen,Wenjun Zhou,Jiaqi Yan*

Main category: cs.AI

TL;DR: 提出了TIM框架，通过DeFi意图分类法和多智能体LLM系统来推断DeFi交易中的用户意图，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: DeFi交易中理解用户意图至关重要但具有挑战性，现有方法缺乏深度语义洞察，需要解决复杂智能合约交互、多因素影响和模糊日志等问题。

Method: TIM框架包含：基于扎根理论的DeFi意图分类法、动态协调领域专家的元级规划器、处理多模态链上/链下数据的问题求解器，以及减轻LLM幻觉的认知评估器。

Result: 实验表明TIM显著优于机器学习模型、单一LLM和单一智能体基线方法。

Conclusion: 该工作为理解DeFi用户动机提供了更可靠的方法，为复杂区块链活动提供情境感知解释。

Abstract: As Decentralized Finance (DeFi) develops, understanding user intent behind DeFi transactions is crucial yet challenging due to complex smart contract interactions, multifaceted on-/off-chain factors, and opaque hex logs. Existing methods lack deep semantic insight. To address this, we propose the Transaction Intent Mining (TIM) framework. TIM leverages a DeFi intent taxonomy built on grounded theory and a multi-agent Large Language Model (LLM) system to robustly infer user intents. A Meta-Level Planner dynamically coordinates domain experts to decompose multiple perspective-specific intent analyses into solvable subtasks. Question Solvers handle the tasks with multi-modal on/off-chain data. While a Cognitive Evaluator mitigates LLM hallucinations and ensures verifiability. Experiments show that TIM significantly outperforms machine learning models, single LLMs, and single Agent baselines. We also analyze core challenges in intent inference. This work helps provide a more reliable understanding of user motivations in DeFi, offering context-aware explanations for complex blockchain activity.

</details>


### [50] [Exploring the use of AI authors and reviewers at Agents4Science](https://arxiv.org/abs/2511.15534)
*Federico Bianchi,Owen Queen,Nitya Thakkar,Eric Sun,James Zou*

Main category: cs.AI

TL;DR: AI agents首次作为主要作者和审稿人参与科学会议，探索其在科研中的能力及人机协作模式


<details>
  <summary>Details</summary>
Motivation: 探索AI代理在科学研究中作为科学家和审稿人的能力，以及人机协作在科学领域的潜力

Method: 组织Agents4Science会议，让AI代理担任主要作者和审稿人，人类作为合著者和共同审稿人

Result: 获得了关于AI代理在科研中能力的重要见解，以及人机协作在科学中的启示

Conclusion: AI代理在科学研究和评审中具有潜力，人机协作模式为科学领域带来新的可能性

Abstract: There is growing interest in using AI agents for scientific research, yet fundamental questions remain about their capabilities as scientists and reviewers. To explore these questions, we organized Agents4Science, the first conference in which AI agents serve as both primary authors and reviewers, with humans as co-authors and co-reviewers. Here, we discuss the key learnings from the conference and their implications for human-AI collaboration in science.

</details>


### [51] [What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity](https://arxiv.org/abs/2511.15593)
*Alexis Audran-Reiss,Jordi Armengol Estapé,Karen Hambardzumyan,Amar Budhiraja,Martin Josifoski,Edan Toledo,Rishi Hazra,Despoina Magka,Michael Shvartsman,Parth Pathak,Justine T Kao,Lucia Cipolina-Kun,Bhavul Gauri,Jean-Christophe Gagnon-Audet,Emanuel Tewolde,Jenny Zhang,Taco Cohen,Yossi Adi,Tatiana Shavrina,Yoram Bachrach*

Main category: cs.AI

TL;DR: 该研究分析了AI研究代理中构思多样性对性能的影响，发现在MLE-bench基准测试中，构思多样性更高的代理表现更好，并通过控制实验验证了这一因果关系。


<details>
  <summary>Details</summary>
Motivation: AI研究代理有望加速科学进步，但成功或失败的关键因素尚未完全理解。本研究旨在探索构思多样性在代理性能中的作用。

Method: 首先分析MLE-bench基准上不同模型和代理框架的轨迹，然后进行控制实验调节构思多样性程度，最后使用额外评估指标验证结果。

Result: 不同模型和代理框架产生不同程度的构思多样性，性能更高的代理往往具有更高的构思多样性。控制实验表明更高的构思多样性确实导致更强的性能。

Conclusion: 构思多样性是影响AI研究代理性能的关键因素，这一发现在多种评估指标下均成立。

Abstract: AI research agents offer the promise to accelerate scientific progress by automating the design, implementation, and training of machine learning models. However, the field is still in its infancy, and the key factors driving the success or failure of agent trajectories are not fully understood. We examine the role that ideation diversity plays in agent performance. First, we analyse agent trajectories on MLE-bench, a well-known benchmark to evaluate AI research agents, across different models and agent scaffolds. Our analysis reveals that different models and agent scaffolds yield varying degrees of ideation diversity, and that higher-performing agents tend to have increased ideation diversity. Further, we run a controlled experiment where we modify the degree of ideation diversity, demonstrating that higher ideation diversity results in stronger performance. Finally, we strengthen our results by examining additional evaluation metrics beyond the standard medal-based scoring of MLE-bench, showing that our findings still hold across other agent performance metrics.

</details>
