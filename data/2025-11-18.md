<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 88]
- [cs.AI](#cs.AI) [Total: 82]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [TimeStampEval: A Simple LLM Eval and a Little Fuzzy Matching Trick to Improve Search Accuracy](https://arxiv.org/abs/2511.11594)
*James McCammon*

Main category: cs.CL

TL;DR: 提出了TimeStampEval基准，用于从长文本记录中检索非逐字引用的精确时间戳。开发的两阶段方法显著提高检索准确性，同时降低90%以上的推理成本。


<details>
  <summary>Details</summary>
Motivation: 解决传统模糊匹配在处理语义相同但语法不同的引用时的失败问题，特别是在对齐官方书面记录和语音转文字记录时。应用场景是自动化的长篇播客，将国会记录片段组装成AI主持的叙述。

Method: 采用两阶段方法：首先使用RapidFuzz进行预过滤，然后使用LLM在短片段上进行验证。提示设计将查询放在转录文本之前并使用紧凑格式。

Result: 该方法将模糊匹配准确率提高了50个百分点，同时将延迟减半，每个正确结果的成本降低了96%。在10个转录文本（50k-900k tokens）的扩展测试中，对转录长度、词汇漂移和领域变化保持稳健，对缺失目标的拒绝准确率保持在95-100%。

Conclusion: 提示设计比模型选择更重要，适度的推理预算可以显著提高准确性，辅助模糊方法在保持高准确性的同时大幅降低成本。

Abstract: Traditional fuzzy matching often fails when searching for quotes that are semantically identical but syntactically different across documents-a common issue when aligning official written records with speech-to-text transcripts. We introduce TimeStampEval, a benchmark for retrieving precise millisecond timestamps from long transcripts given non-verbatim quotes. Our simple two-stage method dramatically improves retrieval accuracy while cutting inference costs by over 90%. The motivating use case is an automated long-form podcast that assembles Congressional Record clips into AI-hosted narration. The technical challenge: given a sentence-timestamped transcript and a target quote that may differ due to transcription or editorial drift, return exact start and end boundaries. Standard algorithms handle verbatim text but break under fuzzier variants. Evaluating six modern LLMs on a 2,800-sentence (120k-token) transcript revealed four key findings. (1) Prompt design matters more than model choice: placing the query before the transcript and using compact formatting improved accuracy by 3-20 points while reducing token count by 30-40%. (2) Off-by-one errors form a distinct category, showing models understand the task but misplace boundaries. (3) A modest reasoning budget (600-850 tokens) raises accuracy from 37% to 77% for weak setups and to above 90% for strong ones. (4) Our "Assisted Fuzzy" approach-RapidFuzz pre-filtering followed by LLM verification on short snippets-improves fuzzy match accuracy by up to 50 points while halving latency and reducing cost per correct result by up to 96%. Extended tests on ten transcripts (50k-900k tokens, 1989-2025) confirm robustness to transcript length, vocabulary drift, and domain change, maintaining 95-100% rejection accuracy for absent targets.

</details>


### [2] [MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling](https://arxiv.org/abs/2511.11793)
*MiroMind Team,Song Bai,Lidong Bing,Carson Chen,Guanzheng Chen,Yuntao Chen,Zhe Chen,Ziyi Chen,Jifeng Dai,Xuan Dong,Yue Deng,Yunjie Fu,Junqi Ge,Chenxia Han,Tammy Huang,Zhenhang Huang,Jerry Jiao,Shilei Jiang,Tianyu Jiao,Xiaoqi Jian,Lei Lei,Ruilin Li,Ryan Luo,Tiantong Li,Xiang Lin,Ziyuan Liu,Zhiqi Li,Jie Ni,Qiang Ren,Pax Sun,Shiqian Su,Chenxin Tao,Bin Wang,Hellen Wang,Haonan Wang,James Wang,Jin Wang,Jojo Wang,Letian Wang,Shizun Wang,Weizhi Wang,Zixuan Wang,Jinfan Xu,Sen Xing,Chenyu Yang,Hai Ye,Jiaheng Yu,Yue Yu,Muyan Zhong,Tianchen Zhao,Xizhou Zhu,Yanpeng Zhou,Yifan Zhang,Zhi Zhu*

Main category: cs.CL

TL;DR: MiroThinker v1.0是一个开源研究代理，通过交互扩展作为模型规模和上下文长度之外的第三个性能提升维度，能够进行深度多轮推理和复杂研究任务。


<details>
  <summary>Details</summary>
Motivation: 现有代理主要关注模型规模或上下文长度的扩展，但缺乏对模型与环境交互深度的系统性探索。传统LLM测试时扩展在长推理链中可能退化，而交互扩展可以利用环境反馈纠正错误。

Method: 通过强化学习实现交互扩展，模型在256K上下文窗口中每任务可执行多达600次工具调用，利用环境反馈和外部信息获取来修正错误和优化轨迹。

Result: 在GAIA、HLE、BrowseComp和BrowseComp-ZH四个基准测试中，72B变体分别达到81.9%、37.7%、47.1%和55.6%的准确率，超越先前开源代理并接近商业对应物。

Conclusion: 交互扩展与模型容量和上下文窗口一样，是构建下一代开源研究代理的第三个关键维度，交互深度展现出与模型规模和上下文长度类似的扩展行为。

Abstract: We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.

</details>


### [3] [On the Notion that Language Models Reason](https://arxiv.org/abs/2511.11810)
*Bertram Højer*

Main category: cs.CL

TL;DR: 该论文质疑语言模型是否真正具备推理能力，认为LM只是实现隐式有限阶马尔可夫核的统计模式匹配器，而非真正的推理者。


<details>
  <summary>Details</summary>
Motivation: 评估语言模型推理能力的定义，澄清LM生成类似推理输出的本质是统计规律而非逻辑机制。

Method: 假设基于Transformer的LM实现隐式有限阶马尔可夫核，将上下文映射到条件标记分布，分析推理类输出对应的统计规律性。

Result: 论证LM是"统计模式匹配器"而非真正推理者，其推理类输出源于学习到的统计不变性，缺乏逻辑一致性保证。

Conclusion: 需要重新讨论NLP研究中如何描述计算过程，这对评估LM的认知不确定性至关重要。

Abstract: Language models (LMs) are said to be exhibiting reasoning, but what does this entail? We assess definitions of reasoning and how key papers in the field of natural language processing (NLP) use the notion and argue that the definitions provided are not consistent with how LMs are trained, process information, and generate new tokens. To illustrate this incommensurability we assume the view that transformer-based LMs implement an \textit{implicit} finite-order Markov kernel mapping contexts to conditional token distributions. In this view, reasoning-like outputs correspond to statistical regularities and approximate statistical invariances in the learned kernel rather than the implementation of explicit logical mechanisms. This view is illustrative of the claim that LMs are "statistical pattern matchers"" and not genuine reasoners and provides a perspective that clarifies why reasoning-like outputs arise in LMs without any guarantees of logical consistency. This distinction is fundamental to how epistemic uncertainty is evaluated in LMs. We invite a discussion on the importance of how the computational processes of the systems we build and analyze in NLP research are described.

</details>


### [4] [Scaling Open-Weight Large Language Models for Hydropower Regulatory Information Extraction: A Systematic Analysis](https://arxiv.org/abs/2511.11821)
*Hong-Jun Yoon,Faisal Ashraf,Thomas A. Ruggles,Debjani Singh*

Main category: cs.CL

TL;DR: 评估了7个开放权重模型（0.6B-70B参数）在水电许可文档信息提取中的性能与计算资源权衡，发现14B参数是性能转折点，为监管文档信息提取提供了首个全面的资源-性能映射。


<details>
  <summary>Details</summary>
Motivation: 监管文档信息提取在使用大型语言模型时面临性能与计算资源的关键权衡，需要为实际部署提供实证指导。

Method: 在水电许可文档上评估了7个不同规模的开放权重模型（0.6B-70B参数），分析验证方法的有效性。

Result: 发现14B参数阈值：低于此阈值时验证方法无效（F1 < 0.15），高于此阈值时验证方法可行（F1 = 0.64）。消费级可部署模型通过适当验证达到64% F1，小型模型在51%处饱和，大规模模型接近77% F1但需企业基础设施。

Conclusion: 建立了监管背景下开放权重信息提取的首个全面资源-性能映射，为基于证据的模型选择提供指导，这些发现对水电合规具有直接价值，且对信息提取任务的参数缩放效应具有普适性。

Abstract: Information extraction from regulatory documents using large language models presents critical trade-offs between performance and computational resources. We evaluated seven open-weight models (0.6B-70B parameters) on hydropower licensing documentation to provide empirical deployment guidance.
  Our analysis identified a pronounced 14B parameter threshold where validation methods transition from ineffective (F1 $<$ 0.15) to viable (F1 = 0.64). Consumer-deployable models achieve 64\% F1 through appropriate validation, while smaller models plateau at 51\%. Large-scale models approach 77\% F1 but require enterprise infrastructure.
  We identified systematic hallucination patterns where perfect recall indicates extraction failure rather than success in smaller models. Our findings establish the first comprehensive resource-performance mapping for open-weight information extraction in regulatory contexts, enabling evidence-based model selection.
  These results provide immediate value for hydropower compliance while contributing insights into parameter scaling effects that generalize across information extraction tasks.

</details>


### [5] [Towards Autoformalization of LLM-generated Outputs for Requirement Verification](https://arxiv.org/abs/2511.11829)
*Mihir Gupte,Ramesh S*

Main category: cs.CL

TL;DR: 探索使用基于LLM的自动形式化方法来验证LLM生成的输出与自然语言需求的一致性，通过两个实验展示了该方法在一致性检查和形式验证方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在从自然语言生成结构化输出方面表现出潜力，但目前缺乏正式方法来验证这些输出的准确性。本文旨在填补这一空白，探索自动形式化在验证LLM生成输出方面的应用。

Method: 使用简单的基于LLM的自动形式化器，通过两个实验：1）验证不同表述的自然语言需求在逻辑上是否等价；2）检测自然语言需求与LLM生成输出之间的逻辑不一致性。

Result: 在第一个实验中，自动形式化器成功识别出两个不同表述的自然语言需求在逻辑上是等价的；在第二个实验中，成功识别出给定自然语言需求与LLM生成输出之间的逻辑不一致性。

Conclusion: 虽然研究有限，但自动形式化在确保LLM生成输出的保真度和逻辑一致性方面具有显著潜力，为未来更广泛的研究奠定了基础。

Abstract: Autoformalization, the process of translating informal statements into formal logic, has gained renewed interest with the emergence of powerful Large Language Models (LLMs). While LLMs show promise in generating structured outputs from natural language (NL), such as Gherkin Scenarios from NL feature requirements, there's currently no formal method to verify if these outputs are accurate. This paper takes a preliminary step toward addressing this gap by exploring the use of a simple LLM-based autoformalizer to verify LLM-generated outputs against a small set of natural language requirements. We conducted two distinct experiments. In the first one, the autoformalizer successfully identified that two differently-worded NL requirements were logically equivalent, demonstrating the pipeline's potential for consistency checks. In the second, the autoformalizer was used to identify a logical inconsistency between a given NL requirement and an LLM-generated output, highlighting its utility as a formal verification tool. Our findings, while limited, suggest that autoformalization holds significant potential for ensuring the fidelity and logical consistency of LLM-generated outputs, laying a crucial foundation for future, more extensive studies into this novel application.

</details>


### [6] [Three Stage Narrative Analysis; Plot-Sentiment Breakdown, Structure Learning and Concept Detection](https://arxiv.org/abs/2511.11857)
*Taimur Khan,Ramoza Ahsan,Mohib Hameed*

Main category: cs.CL

TL;DR: 提出了一个分析电影剧本情感弧线的框架，通过自定义情感词典和层次聚类技术，能够提取叙事中的高低层次概念，帮助消费者选择故事。


<details>
  <summary>Details</summary>
Motivation: 故事理解和分析是自然语言理解中的挑战领域，需要深度计算语义表示和句法处理。大量叙事数据需要自动化语义分析而非手动方法。

Method: 使用基于NRC-VAD数据集的Valence、Arousal和Dominance分数构建自定义情感词典，应用LabMTsimple storylab模块进行基于词典的情感分析，并使用Wards层次聚类技术对相似情感情节进行聚类。

Result: 在电影数据集上的实验评估表明，该分析框架对消费者和读者在选择叙事或故事时很有帮助。

Conclusion: 该框架能够有效分析电影剧本的情感弧线并进行扩展分析，为叙事选择提供有价值的参考。

Abstract: Story understanding and analysis have long been challenging areas within Natural Language Understanding. Automated narrative analysis requires deep computational semantic representations along with syntactic processing. Moreover, the large volume of narrative data demands automated semantic analysis and computational learning rather than manual analytical approaches. In this paper, we propose a framework that analyzes the sentiment arcs of movie scripts and performs extended analysis related to the context of the characters involved. The framework enables the extraction of high-level and low-level concepts conveyed through the narrative. Using dictionary-based sentiment analysis, our approach applies a custom lexicon built with the LabMTsimple storylab module. The custom lexicon is based on the Valence, Arousal, and Dominance scores from the NRC-VAD dataset. Furthermore, the framework advances the analysis by clustering similar sentiment plots using Wards hierarchical clustering technique. Experimental evaluation on a movie dataset shows that the resulting analysis is helpful to consumers and readers when selecting a narrative or story.

</details>


### [7] [Identifying Imaging Follow-Up in Radiology Reports: A Comparative Analysis of Traditional ML and LLM Approaches](https://arxiv.org/abs/2511.11867)
*Namu Park,Giridhar Kaushik Ramachandran,Kevin Lybarger,Fei Xia,Ozlem Uzuner,Meliha Yetisgen,Martin Gunn*

Main category: cs.CL

TL;DR: 该研究引入了一个包含6,393份放射学报告的标注语料库，用于系统比较传统机器学习分类器与生成式大语言模型在随访依从性检测任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门用于评估大语言模型在放射学任务性能的领域特定数据集，需要构建一个标注语料库来支持随访依从性检测系统的开发和基准测试。

Method: 使用6,393份放射学报告标注语料库，比较了逻辑回归、支持向量机、Longformer、微调Llama3-8B-Instruct与GPT-4o、GPT-OSS-20B等生成式LLM。对生成式LLM测试了基础配置和任务优化配置。

Result: GPT-4o（高级配置）表现最佳（F1=0.832），GPT-OSS-20B（高级配置）紧随其后（F1=0.828）。逻辑回归和支持向量机也表现良好（F1分别为0.776和0.775）。标注者间一致性高（F1=0.846）。

Conclusion: 虽然通过提示优化，大语言模型能够接近人类水平的一致性，但可解释且资源效率高的传统模型仍然是重要的基准方法。

Abstract: Large language models (LLMs) have shown considerable promise in clinical natural language processing, yet few domain-specific datasets exist to rigorously evaluate their performance on radiology tasks. In this work, we introduce an annotated corpus of 6,393 radiology reports from 586 patients, each labeled for follow-up imaging status, to support the development and benchmarking of follow-up adherence detection systems. Using this corpus, we systematically compared traditional machine-learning classifiers, including logistic regression (LR), support vector machines (SVM), Longformer, and a fully fine-tuned Llama3-8B-Instruct, with recent generative LLMs. To evaluate generative LLMs, we tested GPT-4o and the open-source GPT-OSS-20B under two configurations: a baseline (Base) and a task-optimized (Advanced) setting that focused inputs on metadata, recommendation sentences, and their surrounding context. A refined prompt for GPT-OSS-20B further improved reasoning accuracy. Performance was assessed using precision, recall, and F1 scores with 95% confidence intervals estimated via non-parametric bootstrapping. Inter-annotator agreement was high (F1 = 0.846). GPT-4o (Advanced) achieved the best performance (F1 = 0.832), followed closely by GPT-OSS-20B (Advanced; F1 = 0.828). LR and SVM also performed strongly (F1 = 0.776 and 0.775), underscoring that while LLMs approach human-level agreement through prompt optimization, interpretable and resource-efficient models remain valuable baselines.

</details>


### [8] [MedPT: A Massive Medical Question Answering Dataset for Brazilian-Portuguese Speakers](https://arxiv.org/abs/2511.11878)
*Fernanda Bufon Färber,Iago Alves Brito,Julia Soares Dollis,Pedro Schindler Freire Brasil Ribeiro,Rafael Teixeira Sousa,Arlindo Rodrigues Galvão Filho*

Main category: cs.CL

TL;DR: MedPT是首个针对巴西葡萄牙语的大规模真实世界医疗语料库，包含384,095个医患问答对，通过多阶段筛选和LLM标注增强了数据质量，在医疗专科分类任务上达到94% F1分数。


<details>
  <summary>Details</summary>
Motivation: 现有LLM开发主要关注高资源语言，而简单翻译无法捕捉特定临床和文化细微差别（如地方性疾病），这为其他语言群体创造了关键障碍。

Method: 构建了巴西葡萄牙语医患交互语料库，采用混合定量-定性分析方法过滤噪音并丰富模糊查询，通过LLM驱动标注将问题分类为七种语义类型，并用于医疗专科路由任务的基准测试。

Result: 语料库涵盖3,200个主题，展示了独特的语言特性（如医患沟通的自然不对称性）。在20类医疗专科分类任务中，微调1.7B参数模型达到94% F1分数，错误分析显示误分类反映了真实的临床模糊性。

Conclusion: MedPT的发布将促进为葡萄牙语世界开发更公平、准确和文化意识的医疗技术。

Abstract: While large language models (LLMs) show transformative potential in healthcare, their development remains focused on high-resource languages, creating a critical barrier for others as simple translation fails to capture unique clinical and cultural nuances, such as endemic diseases. To address this, we introduce MedPT, the first large-scale, real-world corpus for Brazilian Portuguese, comprising 384,095 authentic question-answer pairs from patient-doctor interactions. The dataset underwent a meticulous multi-stage curation protocol, using a hybrid quantitative-qualitative analysis to filter noise and contextually enrich thousands of ambiguous queries. We further augmented the corpus via LLM-driven annotation, classifying questions into seven semantic types to capture user intent. Our analysis reveals its thematic breadth (3,200 topics) and unique linguistic properties, like the natural asymmetry in patient-doctor communication. To validate its utility, we benchmark a medical specialty routing task: fine-tuning a 1.7B parameter model achieves an outstanding 94\% F1-score on a 20-class setup. Furthermore, our qualitative error analysis shows misclassifications are not random but reflect genuine clinical ambiguities (e.g., between comorbid conditions), proving the dataset's deep semantic richness. We publicly release MedPT to foster the development of more equitable, accurate, and culturally-aware medical technologies for the Portuguese-speaking world.

</details>


### [9] [ClinStructor: AI-Powered Structuring of Unstructured Clinical Texts](https://arxiv.org/abs/2511.11883)
*Karthikeyan K,Raghuveer Thirukovalluru,David Carlson*

Main category: cs.CL

TL;DR: ClinStructor是一个利用大语言模型将临床自由文本转换为结构化问答对的管道，旨在解决临床笔记中的偏见、泛化性和可解释性问题，在ICU死亡率预测任务中仅导致AUC轻微下降2-3%。


<details>
  <summary>Details</summary>
Motivation: 临床笔记包含丰富信息但格式非结构化，存在无意偏见（如性别或种族偏见）、跨临床环境泛化性差（不同EHR系统间模型性能差异大）和可解释性差的问题。

Method: 使用大语言模型将临床自由文本转换为结构化的、任务特定的问答对，作为预测建模的前置步骤。

Result: 在ICU死亡率预测任务中，与直接微调相比，该方法仅导致预测性能轻微下降（AUC下降2-3%），但显著提高了透明度和可控性。

Conclusion: ClinStructor为在临床环境中构建可靠、可解释和可泛化的机器学习模型奠定了坚实基础。

Abstract: Clinical notes contain valuable, context-rich information, but their unstructured format introduces several challenges, including unintended biases (e.g., gender or racial bias), and poor generalization across clinical settings (e.g., models trained on one EHR system may perform poorly on another due to format differences) and poor interpretability. To address these issues, we present ClinStructor, a pipeline that leverages large language models (LLMs) to convert clinical free-text into structured, task-specific question-answer pairs prior to predictive modeling. Our method substantially enhances transparency and controllability and only leads to a modest reduction in predictive performance (a 2-3% drop in AUC), compared to direct fine-tuning, on the ICU mortality prediction task. ClinStructor lays a strong foundation for building reliable, interpretable, and generalizable machine learning models in clinical environments.

</details>


### [10] [Context-Emotion Aware Therapeutic Dialogue Generation: A Multi-component Reinforcement Learning Approach to Language Models for Mental Health Support](https://arxiv.org/abs/2511.11884)
*Eric Hua Qing Zhang,Julia Ive*

Main category: cs.CL

TL;DR: 通过监督微调(SFT)和强化学习(RL)技术增强GPT-2的心理治疗对话生成能力，在多个评估指标上取得显著提升，特别是情感准确率达到99.34%。


<details>
  <summary>Details</summary>
Motivation: COVID-19加剧了心理健康服务的可及性挑战，虽然大型语言模型(LLMs)提供24/7可用性和非评判性互动，但预训练模型缺乏情境和情感意识，无法提供适当的治疗响应。

Method: 重构输入格式以同时处理情境信息和情感状态，采用多组件奖励函数使模型输出与专业治疗师响应和标注情感对齐，应用监督微调和强化学习技术。

Result: 强化学习在多个评估指标上优于基线GPT-2：BLEU(0.0111)、ROUGE-1(0.1397)、ROUGE-2(0.0213)、ROUGE-L(0.1317)、METEOR(0.0581)，情感准确率达到99.34%(基线为66.96%)，LLM评估确认高情境相关性和专业性。

Conclusion: 强化学习在开发治疗对话系统方面具有有效性，可作为治疗师的有价值辅助工具，同时保持必要的人类临床监督。

Abstract: Mental health illness represents a substantial global socioeconomic burden, with COVID-19 further exacerbating accessibility challenges and driving increased demand for telehealth mental health support. While large language models (LLMs) offer promising solutions through 24/7 availability and non-judgmental interactions, pre-trained models often lack the contextual and emotional awareness necessary for appropriate therapeutic responses. This paper investigated the application of supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance GPT-2's capacity for therapeutic dialogue generation. The methodology restructured input formats to enable simultaneous processing of contextual information and emotional states alongside user input, employing a multi-component reward function that aligned model outputs with professional therapist responses and annotated emotions. Results demonstrated improvements through reinforcement learning over baseline GPT-2 across multiple evaluation metrics: BLEU (0.0111), ROUGE-1 (0.1397), ROUGE-2 (0.0213), ROUGE-L (0.1317), and METEOR (0.0581). LLM evaluation confirmed high contextual relevance and professionalism, while reinforcement learning achieved 99.34% emotion accuracy compared to 66.96% for baseline GPT-2. These findings demonstrate reinforcement learning's effectiveness in developing therapeutic dialogue systems that can serve as valuable assistive tools for therapists while maintaining essential human clinical oversight.

</details>


### [11] [Additive Large Language Models for Semi-Structured Text](https://arxiv.org/abs/2511.11922)
*Karthikeyan K,Raghuveer Thirukovalluru,David Carlson*

Main category: cs.CL

TL;DR: CALM是一个可解释的临床文本分类框架，通过将预测分解为各语义组件的加性贡献，提供透明的风险解释。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在临床文本分类中预测不透明的问题，满足研究人员和医生需要理解患者记录中哪些部分驱动风险信号的需求。

Method: 使用加性大语言模型框架，将半结构化文本输入分解为语义组件，预测结果为各组件贡献的加性总和，实现忠实解释。

Result: CALM在保持与传统LLM分类器相当性能的同时，提高了模型可信度，支持质量保证检查，并在模型开发和审计中揭示了临床有意义的模式。

Conclusion: CALM框架为临床文本分类提供了可解释的解决方案，通过加性结构实现了透明预测，有助于在研究和临床环境中推广应用。

Abstract: Large Language Models have advanced clinical text classification, but their opaque predictions remain a critical barrier to practical adoption in research and clinical settings where investigators and physicians need to understand which parts of a patient's record drive risk signals. To address this challenge, we introduce \textbf{CALM}, short for \textbf{Classification with Additive Large Language Models}, an interpretable framework for semi-structured text where inputs are composed of semantically meaningful components, such as sections of an admission note or question-answer fields from an intake form. CALM predicts outcomes as the additive sum of each component's contribution, making these contributions part of the forward computation itself and enabling faithful explanations at both the patient and population level. The additive structure also enables clear visualizations, such as component-level risk curves similar to those used in generalized additive models, making the learned relationships easier to inspect and communicate. Although CALM expects semi-structured inputs, many clinical documents already have this form, and similar structure can often be automatically extracted from free-text notes. CALM achieves performance comparable to conventional LLM classifiers while improving trust, supporting quality-assurance checks, and revealing clinically meaningful patterns during model development and auditing.

</details>


### [12] [InData: Towards Secure Multi-Step, Tool-Based Data Analysis](https://arxiv.org/abs/2511.11933)
*Karthikeyan K,Raghuveer Thirukovalluru,Bhuwan Dhingra,David Edwin Carlson*

Main category: cs.CL

TL;DR: 提出了InData数据集，用于评估LLMs在多步骤工具推理方面的能力，发现当前LLMs在复杂任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 传统LLM数据代理直接生成和执行代码存在安全风险，需要限制LLMs只能通过预定义的安全工具与数据交互。

Method: 构建InData数据集，包含三个难度级别的数据分析问题，评估15个开源LLMs在多步骤工具推理中的表现。

Result: 大型模型在简单任务上准确率高（97.3%），但在困难任务上表现显著下降（69.6%），显示当前LLMs缺乏稳健的多步骤工具推理能力。

Conclusion: InData数据集为开发和评估具有更强多步骤工具使用能力的LLMs迈出了重要一步，将公开发布数据集和代码。

Abstract: Large language model agents for data analysis typically generate and execute code directly on databases. However, when applied to sensitive data, this approach poses significant security risks. To address this issue, we propose a security-motivated alternative: restrict LLMs from direct code generation and data access, and require them to interact with data exclusively through a predefined set of secure, verified tools. Although recent tool-use benchmarks exist, they primarily target tool selection and simple execution rather than the compositional, multi-step reasoning needed for complex data analysis. To reduce this gap, we introduce Indirect Data Engagement (InData), a dataset designed to assess LLMs' multi-step tool-based reasoning ability. InData includes data analysis questions at three difficulty levels--Easy, Medium, and Hard--capturing increasing reasoning complexity. We benchmark 15 open-source LLMs on InData and find that while large models (e.g., gpt-oss-120b) achieve high accuracy on Easy tasks (97.3%), performance drops sharply on Hard tasks (69.6%). These results show that current LLMs still lack robust multi-step tool-based reasoning ability. With InData, we take a step toward enabling the development and evaluation of LLMs with stronger multi-step tool-use capabilities. We will publicly release the dataset and code.

</details>


### [13] [Improving LLM's Attachment to External Knowledge In Dialogue Generation Tasks Through Entity Anonymization](https://arxiv.org/abs/2511.11946)
*Hadi Sheikhi,Chenyang Huang,Osmar R. Zaïane*

Main category: cs.CL

TL;DR: LLM-KAT评估程序衡量知识图谱对话生成中的知识附着度，并提出实体匿名化技术来提升LLM使用外部知识的能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在知识图谱对话生成任务中过度依赖内部知识，即使提供了准确的知识图谱也难以有效利用外部知识。

Method: 提出了LLM-KAT评估程序来衡量知识附着度，并采用实体匿名化技术来鼓励LLM更好地利用外部知识。

Result: 在OpenDialKG数据集上的实验表明，该方法提高了LLM对外部知识的附着度。

Conclusion: 实体匿名化是一种简单有效的技术，能够改善LLM在知识图谱对话生成中对外部知识的利用能力。

Abstract: Knowledge graph-based dialogue generation (KG-DG) is a challenging task requiring models to effectively incorporate external knowledge into conversational responses. While large language models (LLMs) have achieved impressive results across various NLP tasks, their ability to utilize external knowledge in KG-DG remains under-explored. We observe that LLMs often rely on internal knowledge, leading to detachment from provided knowledge graphs, even when they are given a flawlessly retrieved knowledge graph. First, we introduce LLM-KAT, an evaluation procedure for measuring knowledge attachment in generated responses. Second, we propose a simple yet effective entity anonymization technique to encourage LLMs to better leverage external knowledge. Experiments on the OpenDialKG dataset demonstrate that our approach improves LLMs' attachment on external knowledge.

</details>


### [14] [On the Entropy Calibration of Language Models](https://arxiv.org/abs/2511.11966)
*Steven Cao,Gregory Valiant,Percy Liang*

Main category: cs.CL

TL;DR: 研究语言模型的熵校准问题，发现模型存在校准错误，随着生成文本变长，每步熵增加且文本质量下降。理论分析表明校准错误随规模改善缓慢，实证验证从0.5B到70B参数模型都显示相似的错误累积率。


<details>
  <summary>Details</summary>
Motivation: 解决自回归模型中错误累积的根本问题，探索校准错误是否随模型规模改善，以及理论上是否能在不牺牲对数损失的情况下进行校准。

Method: 首先在简化理论设置中分析校准错误的缩放行为与数据分布幂律指数的关系，然后实证测量从0.5B到70B参数语言模型的校准错误。

Result: 发现校准错误的缩放指数接近0，意味着大模型与小模型以相似速率累积错误，这解释了为什么即使大模型质量更高，我们仍使用相似的截断采样。

Conclusion: 理论上证明如果假设存在能预测文本未来熵的黑盒，则可以在保持对数损失的同时减少熵，但截断方法会牺牲对数损失。

Abstract: We study the problem of entropy calibration, which asks whether a language model's entropy over generations matches its log loss on human text. Past work found that models are miscalibrated, with entropy per step increasing (and text quality decreasing) as generations grow longer. This error accumulation is a fundamental problem in autoregressive models, and the standard solution is to truncate the distribution, which improves text quality at the cost of diversity. In this paper, we ask: is miscalibration likely to improve with scale, and is it theoretically possible to calibrate without tradeoffs? To build intuition, we first study a simplified theoretical setting to characterize the scaling behavior of miscalibration with respect to dataset size. We find that the scaling behavior depends on the power law exponent of the data distribution -- in particular, for a power law exponent close to 1, the scaling exponent is close to 0, meaning that miscalibration improves very slowly with scale. Next, we measure miscalibration empirically in language models ranging from 0.5B to 70B parameters. We find that the observed scaling behavior is similar to what is predicted by the simplified setting: our fitted scaling exponents for text are close to 0, meaning that larger models accumulate error at a similar rate as smaller ones. This scaling (or, lack thereof) provides one explanation for why we sample from larger models with similar amounts of truncation as smaller models, even though the larger models are of higher quality. However, truncation is not a satisfying solution because it comes at the cost of increased log loss. In theory, is it even possible to reduce entropy while preserving log loss? We prove that it is possible, if we assume access to a black box which can fit models to predict the future entropy of text.

</details>


### [15] [A Reasoning Paradigm for Named Entity Recognition](https://arxiv.org/abs/2511.11978)
*Hui Huang,Yanping Chen,Ruizhang Huang,Chuan Lin,Yongbin Qin*

Main category: cs.CL

TL;DR: 提出了一个名为ReasoningNER的推理框架，将NER从隐式模式匹配转向显式推理，在零样本设置下F1分数比GPT-4高出12.3个百分点。


<details>
  <summary>Details</summary>
Motivation: 生成式LLM通常通过指令调优改进NER性能，但缺乏显式可验证的推理机制，导致次优性能和脆弱的泛化能力，特别是在零样本和低资源场景中。

Method: 提出三阶段推理框架：1) 生成带有NER导向CoT的数据集；2) 使用CoT调优NER模型；3) 推理增强阶段使用综合奖励信号优化推理过程。

Result: 在零样本设置下达到SOTA性能，F1分数比GPT-4高出12.3个百分点。实验显示ReasoningNER在NER任务中展现出令人印象深刻的认知能力。

Conclusion: ReasoningNER在推理导向的信息提取研究中具有巨大潜力，证明了显式推理机制对提升NER性能的重要性。

Abstract: Generative LLMs typically improve Named Entity Recognition (NER) performance through instruction tuning. They excel at generating entities by semantic pattern matching but lack an explicit, verifiable reasoning mechanism. This "cognitive shortcutting" leads to suboptimal performance and brittle generalization, especially in zero-shot and lowresource scenarios where reasoning from limited contextual cues is crucial. To address this issue, a reasoning framework is proposed for NER, which shifts the extraction paradigm from implicit pattern matching to explicit reasoning. This framework consists of three stages: Chain of Thought (CoT) generation, CoT tuning, and reasoning enhancement. First, a dataset annotated with NER-oriented CoTs is generated, which contain task-relevant reasoning chains. Then, they are used to tune the NER model to generate coherent rationales before deriving the final answer. Finally, a reasoning enhancement stage is implemented to optimize the reasoning process using a comprehensive reward signal. This stage ensures explicit and verifiable extractions. Experiments show that ReasoningNER demonstrates impressive cognitive ability in the NER task, achieving competitive performance. In zero-shot settings, it achieves state-of-the-art (SOTA) performance, outperforming GPT-4 by 12.3 percentage points on the F1 score. Analytical results also demonstrate its great potential to advance research in reasoningoriented information extraction. Our codes are available at https://github.com/HuiResearch/ReasoningIE.

</details>


### [16] [Critical or Compliant? The Double-Edged Sword of Reasoning in Chain-of-Thought Explanations](https://arxiv.org/abs/2511.12001)
*Eunkyu Park,Wesley Hanwen Deng,Vasudha Varadarajan,Mingxi Yan,Gunhee Kim,Maarten Sap,Motahhare Eslami*

Main category: cs.CL

TL;DR: 本文研究了思维链解释在道德场景中的双重作用：既能提高透明度，也可能因确认偏见导致用户忽视推理错误，特别是当输出看似合理或语气自信时。


<details>
  <summary>Details</summary>
Motivation: 探讨解释机制在促进透明度与引发确认偏见之间的平衡，特别是在多模态道德场景中，理解思维链解释如何影响用户信任和错误检测能力。

Method: 通过系统性地扰动推理链和操纵表达语气，分析视觉语言模型中的推理错误及其对用户信任和错误检测的影响。

Result: 发现用户常将信任等同于结果一致性，即使推理有缺陷仍保持依赖；自信语气会抑制错误检测但维持依赖，表明表达风格能凌驾于正确性之上。

Conclusion: 思维链解释既能澄清也能误导，NLP系统需要提供鼓励审查和批判性思维而非盲目信任的解释机制。

Abstract: Explanations are often promoted as tools for transparency, but they can also foster confirmation bias; users may assume reasoning is correct whenever outputs appear acceptable. We study this double-edged role of Chain-of-Thought (CoT) explanations in multimodal moral scenarios by systematically perturbing reasoning chains and manipulating delivery tones. Specifically, we analyze reasoning errors in vision language models (VLMs) and how they impact user trust and the ability to detect errors. Our findings reveal two key effects: (1) users often equate trust with outcome agreement, sustaining reliance even when reasoning is flawed, and (2) the confident tone suppresses error detection while maintaining reliance, showing that delivery styles can override correctness. These results highlight how CoT explanations can simultaneously clarify and mislead, underscoring the need for NLP systems to provide explanations that encourage scrutiny and critical thinking rather than blind trust. All code will be released publicly.

</details>


### [17] [CURE: Cultural Understanding and Reasoning Evaluation - A Framework for "Thick" Culture Alignment Evaluation in LLMs](https://arxiv.org/abs/2511.12014)
*Truong Vo,Sanmi Koyejo*

Main category: cs.CL

TL;DR: 本文提出了一个用于评估大语言模型文化能力的新基准，通过情境化评估和多个补充指标来更准确地衡量模型的文化理解和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型文化能力评估方法过于关注去情境化的正确性或强制选择判断，忽视了文化理解和推理的需求，无法准确评估模型在多元文化环境中的表现。

Method: 引入基于现实情境的评估基准，除了标准精确匹配指标外，还提出了覆盖率、特异性、内涵性和连贯性四个补充指标，进行厚评估方法。

Result: 经验分析显示，传统薄评估系统性地高估了文化能力且评估结果不稳定，而厚评估能够揭示推理深度的差异，减少方差，提供更稳定、可解释的文化理解信号。

Conclusion: 厚评估方法比传统薄评估能更准确地衡量大语言模型的文化能力，为模型在多元文化环境中的部署提供了更可靠的评估框架。

Abstract: Large language models (LLMs) are increasingly deployed in culturally diverse environments, yet existing evaluations of cultural competence remain limited. Existing methods focus on de-contextualized correctness or forced-choice judgments, overlooking the need for cultural understanding and reasoning required for appropriate responses. To address this gap, we introduce a set of benchmarks that, instead of directly probing abstract norms or isolated statements, present models with realistic situational contexts that require culturally grounded reasoning. In addition to the standard Exact Match metric, we introduce four complementary metrics (Coverage, Specificity, Connotation, and Coherence) to capture different dimensions of model's response quality. Empirical analysis across frontier models reveals that thin evaluation systematically overestimates cultural competence and produces unstable assessments with high variance. In contrast, thick evaluation exposes differences in reasoning depth, reduces variance, and provides more stable, interpretable signals of cultural understanding.

</details>


### [18] [Exploring Parameter-Efficient Fine-Tuning and Backtranslation for the WMT 25 General Translation Task](https://arxiv.org/abs/2511.12109)
*Felipe Fujita,Hideyuki Takada*

Main category: cs.CL

TL;DR: 结合回译和微调在小规模日语语料上显著提升神经机器翻译质量，COMET分数从0.460提升至0.597。


<details>
  <summary>Details</summary>
Motivation: 研究如何在有限训练数据下提升低资源语言对的翻译质量，探索回译和微调的协同效应。

Method: 使用回译生成合成数据，然后在真实小规模平行语料上进行微调，最后结合两种方法：先用回译增强数据再进行微调。

Result: 单独回译COMET=0.468，单独微调COMET=0.589，结合两者COMET=0.597，显著优于单独使用任一方法。

Conclusion: 回译和针对性微调的协同使用能显著提升低资源语言对的翻译质量，提供了一种轻量级但强大的改进策略。

Abstract: In this paper, we explore the effectiveness of combining fine-tuning and backtranslation on a small Japanese corpus for neural machine translation. Starting from a baseline English{\textrightarrow}Japanese model (COMET = 0.460), we first apply backtranslation (BT) using synthetic data generated from monolingual Japanese corpora, yielding a modest increase (COMET = 0.468). Next, we fine-tune (FT) the model on a genuine small parallel dataset drawn from diverse Japanese news and literary corpora, achieving a substantial jump to COMET = 0.589 when using Mistral 7B. Finally, we integrate both backtranslation and fine-tuning{ -- }first augmenting the small dataset with BT generated examples, then adapting via FT{ -- }which further boosts performance to COMET = 0.597. These results demonstrate that, even with limited training data, the synergistic use of backtranslation and targeted fine-tuning on Japanese corpora can significantly enhance translation quality, outperforming each technique in isolation. This approach offers a lightweight yet powerful strategy for improving low-resource language pairs.

</details>


### [19] [LLMLagBench: Identifying Temporal Training Boundaries in Large Language Models](https://arxiv.org/abs/2511.12116)
*Piotr Pęzik,Konrad Kaczyński,Maria Szymańska,Filip Żarnecki,Zuzanna Deckert,Jakub Kwiatkowski,Wojciech Janowski*

Main category: cs.CL

TL;DR: LLMLagBench是一个评估大型语言模型训练数据时间边界的基准测试，通过检测模型对近期事件的了解来确定其知识新鲜度。


<details>
  <summary>Details</summary>
Motivation: LLMs在特定时间点前的文本数据上预训练，形成了严格的知识边界。当这个限制未知或被忽视时，模型可能在推理任务中无意间混合过时的时效性信息与通用知识，从而影响回答准确性。

Method: 引入LLMLagBench作为系统性方法，通过评估模型对近期事件的知识来识别其训练数据的最早可能时间边界。该方法应用于大量LLMs，包括有明确声明和未声明训练截止时间的模型。

Result: 通过手动验证和与公开发布的LLM预训练信息进行比较，评估了基准测试的可靠性。

Conclusion: LLMLagBench提供了一个系统性的方法来识别LLMs的知识时间边界，有助于理解模型的知识新鲜度限制。

Abstract: Large Language Models (LLMs) are pretrained on textual data up to a specific temporal cutoff. This creates a strict knowledge boundary beyond which models cannot provide accurate information without querying external sources. More subtly, when this limitation is unknown or ignored, LLMs may inadvertently blend outdated time-sensitive information with general knowledge during reasoning tasks, potentially compromising response accuracy. We introduce LLMLagBench, an LLM freshness benchmark, as a systematic approach for identifying the earliest probable temporal boundaries of an LLM's training data by evaluating its knowledge of recent events. We then apply this benchmark to evaluate a large set of LLMs, including models with both explicitly declared and undeclared training cutoffs. The reliability of the benchmark is assessed by manual validation and comparison with publicly released information about LLM pretraining.

</details>


### [20] [PRISM of Opinions: A Persona-Reasoned Multimodal Framework for User-centric Conversational Stance Detection](https://arxiv.org/abs/2511.12130)
*Bingbing Wang,Zhixin Bai,Zhengda Jin,Zihan Wang,Xintong Song,Jingjie Lin,Sixuan Li,Jing Li,Ruifeng Xu*

Main category: cs.CL

TL;DR: 提出了U-MStance数据集和PRISM模型，解决了多模态对话立场检测中的伪多模态和用户同质性问题，通过用户画像和多模态对齐显著提升了立场检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态对话立场检测研究存在两个主要局限：1）伪多模态问题，源帖子包含视觉线索而评论仅被视为文本；2）用户同质性问题，忽视了个体特征对立场表达的影响。

Method: 提出PRISM模型：1）从历史帖子和评论中提取纵向用户画像；2）通过思维链对齐对话上下文中的文本和视觉线索；3）采用相互任务强化机制联合优化立场检测和立场感知响应生成。

Result: 在U-MStance数据集上的实验表明，PRISM相比强基线模型取得了显著性能提升，验证了用户中心和上下文基础的多模态推理的有效性。

Conclusion: 用户中心的多模态推理方法能够更真实地理解对话立场，用户画像和多模态对齐对于现实世界的立场理解至关重要。

Abstract: The rapid proliferation of multimodal social media content has driven research in Multimodal Conversational Stance Detection (MCSD), which aims to interpret users' attitudes toward specific targets within complex discussions. However, existing studies remain limited by: **1) pseudo-multimodality**, where visual cues appear only in source posts while comments are treated as text-only, misaligning with real-world multimodal interactions; and **2) user homogeneity**, where diverse users are treated uniformly, neglecting personal traits that shape stance expression. To address these issues, we introduce **U-MStance**, the first user-centric MCSD dataset, containing over 40k annotated comments across six real-world targets. We further propose **PRISM**, a **P**ersona-**R**easoned mult**I**modal **S**tance **M**odel for MCSD. PRISM first derives longitudinal user personas from historical posts and comments to capture individual traits, then aligns textual and visual cues within conversational context via Chain-of-Thought to bridge semantic and pragmatic gaps across modalities. Finally, a mutual task reinforcement mechanism is employed to jointly optimize stance detection and stance-aware response generation for bidirectional knowledge transfer. Experiments on U-MStance demonstrate that PRISM yields significant gains over strong baselines, underscoring the effectiveness of user-centric and context-grounded multimodal reasoning for realistic stance understanding.

</details>


### [21] [AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing](https://arxiv.org/abs/2511.12133)
*Qingyu Zhang,Chunlei Xin,Xuanang Chen,Yaojie Lu,Hongyu Lin,Xianpei Han,Le Sun,Qing Ye,Qianlong Xie,Xingxing Wang*

Main category: cs.CL

TL;DR: 提出了AI-Salesman框架，通过双阶段架构解决目标驱动说服对话中的策略脆弱性和事实幻觉问题，在真实销售数据集上显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 目标驱动说服对话（如电话销售）需要复杂的多轮规划和严格的事实忠实性，现有LLMs存在策略脆弱性和事实幻觉问题，且缺乏任务特定数据。

Method: 构建TeleSalesCorpus数据集，提出AI-Salesman框架：训练阶段使用贝叶斯监督强化学习从噪声对话中学习鲁棒销售策略；推理阶段使用动态大纲引导代理(DOGA)结合预建脚本库提供逐轮策略指导。

Result: 实验结果表明，AI-Salesman在自动指标和综合人工评估中均显著优于基线模型，在复杂说服场景中表现出色。

Conclusion: 提出的AI-Salesman框架有效解决了目标驱动说服对话中的关键挑战，通过双阶段架构和动态策略指导实现了更好的性能。

Abstract: Goal-driven persuasive dialogue, exemplified by applications like telemarketing, requires sophisticated multi-turn planning and strict factual faithfulness, which remains a significant challenge for even state-of-the-art Large Language Models (LLMs). A lack of task-specific data often limits previous works, and direct LLM application suffers from strategic brittleness and factual hallucination. In this paper, we first construct and release TeleSalesCorpus, the first real-world-grounded dialogue dataset for this domain. We then propose AI-Salesman, a novel framework featuring a dual-stage architecture. For the training stage, we design a Bayesian-supervised reinforcement learning algorithm that learns robust sales strategies from noisy dialogues. For the inference stage, we introduce the Dynamic Outline-Guided Agent (DOGA), which leverages a pre-built script library to provide dynamic, turn-by-turn strategic guidance. Moreover, we design a comprehensive evaluation framework that combines fine-grained metrics for key sales skills with the LLM-as-a-Judge paradigm. Experimental results demonstrate that our proposed AI-Salesman significantly outperforms baseline models in both automatic metrics and comprehensive human evaluations, showcasing its effectiveness in complex persuasive scenarios.

</details>


### [22] [Seeing is Believing: Rich-Context Hallucination Detection for MLLMs via Backward Visual Grounding](https://arxiv.org/abs/2511.12140)
*Pinxue Guo,Chongruo Wu,Xinyu Zhou,Lingyi Hong,Zhaoyu Chen,Jinglun Li,Kaixun Jiang,Sen-ching Samson Cheung,Wei Zhang,Wenqiang Zhang*

Main category: cs.CL

TL;DR: VBackChecker是一个无需参考的幻觉检测框架，通过像素级Grounding LLM验证MLLM生成响应与视觉输入的一致性，在R^2-HalBench基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在严重幻觉问题，需要准确检测以确保实际应用的可靠性。

Method: 基于"眼见为实"原则，使用具备推理和分割能力的像素级Grounding LLM，设计了R-Instruct指令调优数据生成流程，包含丰富上下文描述、定位掩码和困难负样本。

Result: 在R^2-HalBench基准上超越先前复杂框架，性能媲美GPT-4o，在像素级定位任务中提升超过10%。

Conclusion: VBackChecker为MLLM幻觉检测提供了有效的参考无关解决方案，具有良好可解释性并在真实场景中表现优异。

Abstract: Multimodal Large Language Models (MLLMs) have unlocked powerful cross-modal capabilities, but still significantly suffer from hallucinations. As such, accurate detection of hallucinations in MLLMs is imperative for ensuring their reliability in practical applications. To this end, guided by the principle of "Seeing is Believing", we introduce VBackChecker, a novel reference-free hallucination detection framework that verifies the consistency of MLLMgenerated responses with visual inputs, by leveraging a pixellevel Grounding LLM equipped with reasoning and referring segmentation capabilities. This reference-free framework not only effectively handles rich-context scenarios, but also offers interpretability. To facilitate this, an innovative pipeline is accordingly designed for generating instruction-tuning data (R-Instruct), featuring rich-context descriptions, grounding masks, and hard negative samples. We further establish R^2 -HalBench, a new hallucination benchmark for MLLMs, which, unlike previous benchmarks, encompasses real-world, rich-context descriptions from 18 MLLMs with high-quality annotations, spanning diverse object-, attribute, and relationship-level details. VBackChecker outperforms prior complex frameworks and achieves state-of-the-art performance on R^2 -HalBench, even rivaling GPT-4o's capabilities in hallucination detection. It also surpasses prior methods in the pixel-level grounding task, achieving over a 10% improvement. All codes, data, and models are available at https://github.com/PinxueGuo/VBackChecker.

</details>


### [23] [CriticSearch: Fine-Grained Credit Assignment for Search Agents via a Retrospective Critic](https://arxiv.org/abs/2511.12159)
*Yaocheng Zhang,Haohuan Huang,Zijun Song,Yuanheng Zhu,Qichao Zhang,Zijie Zhao,Dongbin Zhao*

Main category: cs.CL

TL;DR: CriticSearch是一个细粒度信用分配框架，通过回顾性批评机制提供密集的回合级反馈，解决搜索代理中稀疏奖励导致的低效探索和不稳定训练问题。


<details>
  <summary>Details</summary>
Motivation: 现有搜索代理管道依赖基于强化学习的优化，但存在稀疏结果奖励问题，导致探索效率低下和训练不稳定。

Method: 使用冻结的非对称批评LLM，利用完整轨迹和黄金答案的特权信息回顾性评估每个回合，将这些评估转化为稳定的密集奖励来指导策略改进。

Result: 在多样化多跳推理基准测试中，CriticSearch始终优于现有基线，实现了更快的收敛、更好的训练稳定性和更高的性能。

Conclusion: CriticSearch通过密集的回合级反馈机制，有效提升了搜索代理在复杂问答任务中的训练效率和性能表现。

Abstract: Tool-Integrated Reasoning (TIR) with search engines enables large language models to iteratively retrieve up-to-date external knowledge, enhancing adaptability and generalization in complex question-answering tasks. However, existing search agent pipelines typically depend on reinforcement learning based optimization, which often suffers from sparse outcome rewards, leading to inefficient exploration and unstable training. We introduce CriticSearch, a fine-grained credit-assignment framework that supplies dense, turn-level feedback via a retrospective critic mechanism. During training, a frozen, asymmetric critique LLM retrospectively evaluates each turn using privileged information from the full trajectory and gold answers, converting these assessments into stable, dense rewards that guide policy improvement. Experimental results across diverse multi-hop reasoning benchmarks demonstrate that CriticSearch consistently outperforms existing baselines, achieving faster convergence, improved training stability, and higher performance.

</details>


### [24] [MME-RAG: Multi-Manager-Expert Retrieval-Augmented Generation for Fine-Grained Entity Recognition in Task-Oriented Dialogues](https://arxiv.org/abs/2511.12213)
*Liang Xue,Haoyu Liu,Yajun Tian,Xinyu Zhong,Yang Liu*

Main category: cs.CL

TL;DR: MME-RAG是一个多管理器-专家检索增强生成框架，将实体识别分解为类型级判断和跨度级提取两个协调阶段，通过轻量级管理器和专业专家实现精确的领域自适应提取。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在面向任务的对话中面临领域适应性和检索可控性的挑战，特别是在细粒度实体识别方面。

Method: 采用分层分解方法：轻量级管理器进行类型级判断，专业专家进行跨度级提取，每个专家配备KeyInfo检索器注入语义对齐的少样本示例。

Result: 在CrossNER、MIT-Movie、MIT-Restaurant和新构建的多领域客服数据集上，MME-RAG在大多数领域表现优于现有基线方法。

Conclusion: MME-RAG通过分层分解和KeyInfo引导的检索实现了鲁棒性和跨领域泛化能力，为自适应对话理解提供了可扩展且可解释的解决方案。

Abstract: Fine-grained entity recognition is crucial for reasoning and decision-making in task-oriented dialogues, yet current large language models (LLMs) continue to face challenges in domain adaptation and retrieval controllability. We introduce MME-RAG, a Multi-Manager-Expert Retrieval-Augmented Generation framework that decomposes entity recognition into two coordinated stages: type-level judgment by lightweight managers and span-level extraction by specialized experts. Each expert is supported by a KeyInfo retriever that injects semantically aligned, few-shot exemplars during inference, enabling precise and domain-adaptive extraction without additional training. Experiments on CrossNER, MIT-Movie, MIT-Restaurant, and our newly constructed multi-domain customer-service dataset demonstrate that MME-RAG performs better than recent baselines in most domains. Ablation studies further show that both the hierarchical decomposition and KeyInfo-guided retrieval are key drivers of robustness and cross-domain generalization, establishing MME-RAG as a scalable and interpretable solution for adaptive dialogue understanding.

</details>


### [25] [Consistency Is the Key: Detecting Hallucinations in LLM Generated Text By Checking Inconsistencies About Key Facts](https://arxiv.org/abs/2511.12236)
*Raavi Gupta,Pranav Hari Panicker,Sumit Bhatia,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: CONFACTCHECK是一种高效的幻觉检测方法，通过检查生成文本中事实探针的一致性来检测幻觉，无需外部知识库，在资源受限环境下优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常产生事实错误的幻觉文本，在医疗、金融等领域存在严重风险。现有检测方法在模型访问受限时需要多次API调用，增加了延迟和成本。

Method: 基于直觉设计的方法：同一LLM内部和不同LLM之间对生成文本中事实探针的回答应该保持一致，通过检查这种一致性来检测幻觉，无需外部知识库。

Result: 在多个数据集上的严格评估表明，CONFACTCHECK能够使用更少资源高效检测幻觉事实，在相似条件下比现有基线方法获得更高的准确率分数。

Conclusion: CONFACTCHECK提供了一种高效、资源友好的幻觉检测解决方案，特别适用于模型访问受限或资源受限的环境。

Abstract: Large language models (LLMs), despite their remarkable text generation capabilities, often hallucinate and generate text that is factually incorrect and not grounded in real-world knowledge. This poses serious risks in domains like healthcare, finance, and customer support. A typical way to use LLMs is via the APIs provided by LLM vendors where there is no access to model weights or options to fine-tune the model. Existing methods to detect hallucinations in such settings where the model access is restricted or constrained by resources typically require making multiple LLM API calls, increasing latency and API cost. We introduce CONFACTCHECK, an efficient hallucination detection approach that does not leverage any external knowledge base and works on the simple intuition that responses to factual probes within the generated text should be consistent within a single LLM and across different LLMs. Rigorous empirical evaluation on multiple datasets that cover both the generation of factual texts and the open generation shows that CONFACTCHECK can detect hallucinated facts efficiently using fewer resources and achieves higher accuracy scores compared to existing baselines that operate under similar conditions. Our code is available here.

</details>


### [26] [ViConBERT: Context-Gloss Aligned Vietnamese Word Embedding for Polysemous and Sense-Aware Representations](https://arxiv.org/abs/2511.12249)
*Khang T. Huynh,Dung H. Nguyen,Binh T. Nguyen*

Main category: cs.CL

TL;DR: 提出ViConBERT框架用于学习越南语上下文嵌入，结合对比学习和基于词义解释的蒸馏方法，并创建了首个大规模越南语语义理解评估数据集ViConWSD。


<details>
  <summary>Details</summary>
Motivation: 越南语缺乏强大的语义理解模型和评估资源，而现有进展主要集中在英语等高资源语言上。

Method: 使用对比学习(SimCLR)和基于词义解释的蒸馏方法学习越南语上下文嵌入，并构建大规模合成数据集ViConWSD进行评估。

Result: ViConBERT在WSD任务上F1达到0.87，在ViCon和ViSim-400数据集上分别达到AP=0.88和Spearman's rho=0.60的竞争性表现。

Conclusion: ViConBERT在建模离散词义和分级语义关系方面表现出色，为越南语语义理解提供了有效解决方案。

Abstract: Recent advances in contextualized word embeddings have greatly improved semantic tasks such as Word Sense Disambiguation (WSD) and contextual similarity, but most progress has been limited to high-resource languages like English. Vietnamese, in contrast, still lacks robust models and evaluation resources for fine-grained semantic understanding. In this paper, we present ViConBERT, a novel framework for learning Vietnamese contextualized embeddings that integrates contrastive learning (SimCLR) and gloss-based distillation to better capture word meaning. We also introduce ViConWSD, the first large-scale synthetic dataset for evaluating semantic understanding in Vietnamese, covering both WSD and contextual similarity. Experimental results show that ViConBERT outperforms strong baselines on WSD (F1 = 0.87) and achieves competitive performance on ViCon (AP = 0.88) and ViSim-400 (Spearman's rho = 0.60), demonstrating its effectiveness in modeling both discrete senses and graded semantic relations. Our code, models, and data are available at https://github.com/tkhangg0910/ViConBERT

</details>


### [27] [Cmprsr: Abstractive Token-Level Question-Agnostic Prompt Compressor](https://arxiv.org/abs/2511.12281)
*Ivan Zakazov,Alexander Sharipov,Berke Argin,Oussama Gabouj,Kamel Charaf,Alexi Semiz,Lorenzo Drudi,Nicolas Baldwin,Robert West*

Main category: cs.CL

TL;DR: 提出使用小型LLM压缩大型LLM输入的新范式，开发了Cmprsr模型，在多种压缩率和输入长度下优于现有压缩方法，并能精确控制压缩率。


<details>
  <summary>Details</summary>
Motivation: 降低使用黑盒大型语言模型的高成本，通过压缩输入来减少计算开销。

Method: 使用小型LLM压缩大型LLM输入；通过Textgrad优化压缩元提示；对Qwen3-4B进行监督微调和GRPO训练，追求压缩率遵从和下游任务性能最大化。

Result: Cmprsr在MeetingBank、LongBench和GSM8k数据集上，在整个压缩率范围内优于提取式和普通抽象压缩方法，并能紧密遵循请求的压缩率。

Conclusion: Cmprsr模型在压缩性能和压缩率控制方面表现优异，具有跨不同输入长度和领域的泛化能力，为成本-质量权衡提供了精细控制。

Abstract: Motivated by the high costs of using black-box Large Language Models (LLMs), we introduce a novel prompt compression paradigm, under which we use smaller LLMs to compress inputs for the larger ones. We present the first comprehensive LLM-as-a-compressor benchmark spanning 25 open- and closed-source models, which reveals significant disparity in models' compression ability in terms of (i) preserving semantically important information (ii) following the user-provided compression rate (CR). We further improve the performance of gpt-4.1-mini, the best overall vanilla compressor, with Textgrad-based compression meta-prompt optimization. We also identify the most promising open-source vanilla LLM - Qwen3-4B - and post-train it with a combination of supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), pursuing the dual objective of CR adherence and maximizing the downstream task performance. We call the resulting model Cmprsr and demonstrate its superiority over both extractive and vanilla abstractive compression across the entire range of compression rates on lengthy inputs from MeetingBank and LongBench as well as short prompts from GSM8k. The latter highlights Cmprsr's generalizability across varying input lengths and domains. Moreover, Cmprsr closely follows the requested compression rate, offering fine control over the cost-quality trade-off.

</details>


### [28] [AugAbEx : Way Forward for Extractive Case Summarization](https://arxiv.org/abs/2511.12290)
*Purnima Bindal,Vikas Kumar,Sagar Rathore,Vasudha Bhatnagar*

Main category: cs.CL

TL;DR: 本文提出了一种利用现有抽象摘要生成对应提取式摘要的轻量级方法，旨在为法律案例摘要研究社区创建增强的数据资源。


<details>
  <summary>Details</summary>
Motivation: 法律判决摘要对法律从业者构成沉重认知负担，而深度神经方法生成的抽象摘要容易误判法律术语或忽略关键细节，因此需要提取式摘要方法。

Method: 设计了一个轻量透明的流程，利用现有的抽象标准摘要生成对应的提取式标准摘要版本，确保专家意见从原始抽象摘要传递到转换后的提取式摘要中。

Result: 计划增强七个现有案例摘要数据集，通过添加对应的提取式摘要来创建丰富的数据资源，并进行结构、词汇和语义维度的广泛比较评估。

Conclusion: 承诺公开发布增强的数据集，相信该资源将推动法律文档自动摘要领域的发展。

Abstract: Summarization of legal judgments poses a heavy cognitive burden on law practitioners due to the complexity of the language, context-sensitive legal jargon, and the length of the document. Therefore, the automatic summarization of legal documents has attracted serious attention from natural language processing researchers. Since the abstractive summaries of legal documents generated by deep neural methods remain prone to the risk of misrepresenting nuanced legal jargon or overlooking key contextual details, we envisage a rising trend toward the use of extractive case summarizers.
  Given the high cost of human annotation for gold standard extractive summaries, we engineer a light and transparent pipeline that leverages existing abstractive gold standard summaries to create the corresponding extractive gold standard versions. The approach ensures that the experts` opinions ensconced in the original gold standard abstractive summaries are carried over to the transformed extractive summaries. We aim to augment seven existing case summarization datasets, which include abstractive summaries, by incorporating corresponding extractive summaries and create an enriched data resource for case summarization research community. To ensure the quality of the augmented extractive summaries, we perform an extensive comparative evaluation with the original abstractive gold standard summaries covering structural, lexical, and semantic dimensions. We also compare the domain-level information of the two summaries. We commit to release the augmented datasets in the public domain for use by the research community and believe that the resource will offer opportunities to advance the field of automatic summarization of legal documents.

</details>


### [29] [Do LLMs and Humans Find the Same Questions Difficult? A Case Study on Japanese Quiz Answering](https://arxiv.org/abs/2511.12300)
*Naoya Sugiura,Kosuke Yamada,Yasuhiro Ogawa,Katsuhiko Toyama,Ryohei Sasano*

Main category: cs.CL

TL;DR: 研究比较了LLMs和人类在抢答式问答中的表现差异，发现LLMs在维基百科未覆盖的问题和需要数值答案的问题上表现较差


<details>
  <summary>Details</summary>
Motivation: 探究对人类困难的问题是否对LLMs同样困难，比较LLMs和人类在抢答式问答中的表现差异

Method: 收集日本问答数据（包含问题、答案和人类正确率），在多种设置下让LLMs回答问题，从两个分析视角比较LLMs与人类的正确率

Result: 相比人类，LLMs在维基百科未覆盖答案的问题上表现更差，在需要数值答案的问题上也存在困难

Conclusion: LLMs与人类在问题难度上存在差异，LLMs在特定类型问题上表现不如人类

Abstract: LLMs have achieved performance that surpasses humans in many NLP tasks. However, it remains unclear whether problems that are difficult for humans are also difficult for LLMs. This study investigates how the difficulty of quizzes in a buzzer setting differs between LLMs and humans. Specifically, we first collect Japanese quiz data including questions, answers, and correct response rate of humans, then prompted LLMs to answer the quizzes under several settings, and compare their correct answer rate to that of humans from two analytical perspectives. The experimental results showed that, compared to humans, LLMs struggle more with quizzes whose correct answers are not covered by Wikipedia entries, and also have difficulty with questions that require numerical answers.

</details>


### [30] [Don't Think of the White Bear: Ironic Negation in Transformer Models Under Cognitive Load](https://arxiv.org/abs/2511.12381)
*Logan Mann,Nayan Saxena,Sarah Tandon,Chenhao Sun,Savar Toteja,Kevin Zhu*

Main category: cs.CL

TL;DR: 研究探讨了否定指令在大型语言模型中引发的讽刺性反弹现象，发现否定反而会增加被禁止概念的可及性，这与人类认知中的讽刺反弹现象相似。


<details>
  <summary>Details</summary>
Motivation: 否定指令如'不要提及X'在人类认知中会引发讽刺反弹，即被禁止的概念反而更容易被激活。本研究旨在探究大型语言模型是否也存在类似现象，以及影响反弹强度的因素。

Method: 通过两个实验：(1) 在否定指令后引入不同类型的干扰文本（语义、句法、重复），测量反弹强度；(2) 测试模型是否能区分同一概念的中性和负面表述，以及这种区分是否预测反弹的持续性。同时进行电路追踪分析，识别负责放大被禁止标记的注意力头。

Result: 结果显示否定后立即出现反弹，且随着干扰文本长度或语义相关性的增加而增强，而重复干扰有助于抑制。更强的极性区分与更持久的反弹相关。电路分析发现中间层稀疏注意力头放大被禁止标记，而早期层进行抑制。

Conclusion: 研究将认知科学中的讽刺反弹预测与长上下文干扰的机制性理解联系起来，并发布了ReboundBench数据集以支持未来研究。

Abstract: Negation instructions such as 'do not mention $X$' can paradoxically increase the accessibility of $X$ in human thought, a phenomenon known as ironic rebound. Large language models (LLMs) face the same challenge: suppressing a concept requires internally activating it, which may prime rebound instead of avoidance. We investigated this tension with two experiments. \textbf{(1) Load \& content}: after a negation instruction, we vary distractor text (semantic, syntactic, repetition) and measure rebound strength. \textbf{(2) Polarity separation}: We test whether models distinguish neutral from negative framings of the same concept and whether this separation predicts rebound persistence. Results show that rebound consistently arises immediately after negation and intensifies with longer or semantic distractors, while repetition supports suppression. Stronger polarity separation correlates with more persistent rebound. Together, these findings, complemented by a circuit tracing analysis that identifies sparse middle-layer attention heads amplifying forbidden tokens while early layers suppress, link cognitive predictions of ironic rebound with mechanistic insights into long-context interference. To support future work, we release ReboundBench, a dataset of $5,000$ systematically varied negation prompts designed to probe rebound in LLMs.

</details>


### [31] [From Phonemes to Meaning: Evaluating Large Language Models on Tamil](https://arxiv.org/abs/2511.12387)
*Jeyarajalingam Varsha,Menan Velayuthan,Sumirtha Karunakaran,Rasan Nivethiga,Kengatharaiyer Sarveswaran*

Main category: cs.CL

TL;DR: ILAKKANAM是首个泰米尔语专用语言评估基准，包含820道来自斯里兰卡学校考试的问题，评估显示LLMs在泰米尔语语言能力上存在显著差距，特别是随着语言复杂性增加性能下降。


<details>
  <summary>Details</summary>
Motivation: 现有多语言基准主要依赖英语翻译数据集，无法捕捉泰米尔语等低资源、形态丰富语言的语言和文化细微差别，需要专门评估LLMs在泰米尔语中的语言能力。

Method: 使用820道斯里兰卡学校泰米尔语考试问题构建基准，由训练有素的语言学家按照五个语言类别和一个事实知识类别进行标注，涵盖1-13年级以确保广泛的语言覆盖。

Result: Gemini 2.5表现最佳，开源模型落后；所有模型在低年级问题上表现良好，但随着语言复杂性增加性能明显下降；模型整体性能与识别语言类别能力之间无强相关性。

Conclusion: LLMs在泰米尔语中的表现可能更多基于曝光而非真正理解，需要改进对低资源、形态丰富语言的语言基础能力。

Abstract: Large Language Models (LLMs) have shown strong generalization across tasks in high-resource languages; however, their linguistic competence in low-resource and morphologically rich languages such as Tamil remains largely unexplored. Existing multilingual benchmarks often rely on translated English datasets, failing to capture the linguistic and cultural nuances of the target language. To address this gap, we introduce ILAKKANAM, the first Tamil-specific linguistic evaluation benchmark manually curated using 820 questions from Sri Lankan school-level Tamil subject examination papers. Each question is annotated by trained linguists under five linguistic categories and a factual knowledge category, spanning Grades 1--13 to ensure broad linguistic coverage. We evaluate both closed-source and open-source LLMs using a standardized evaluation framework. Our results show that Gemini 2.5 achieves the highest overall performance, while open-source models lag behind, highlighting the gap in linguistic grounding. Category- and grade-wise analyses reveal that all models perform well on lower-grade questions but show a clear decline as linguistic complexity increases. Further, no strong correlation is observed between a model's overall performance and its ability to identify linguistic categories, suggesting that performance may be driven by exposure rather than genuine understanding.

</details>


### [32] [Probing Preference Representations: A Multi-Dimensional Evaluation and Analysis Method for Reward Models](https://arxiv.org/abs/2511.12464)
*Chenglong Wang,Yifu Huo,Yang Gan,Yongyu Mu,Qiaozhi He,Murun Yang,Bei Li,Chunliang Zhang,Tongran Liu,Anxiang Ma,Zhengtao Yu,Jingbo Zhu,Tong Xiao*

Main category: cs.CL

TL;DR: 提出了MRMBench基准和推理时探测方法，用于评估奖励模型在多维度偏好上的表现，发现该方法与LLM对齐性能强相关。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型评估方法通常在固定成对排序测试集上进行，但无法提供各偏好维度的性能信息，需要更细粒度的评估方法。

Method: 构建了MRMBench基准（包含6个探测任务），并提出推理时探测方法分析奖励预测时使用的维度，增强可解释性。

Result: 实验表明MRMBench与LLM对齐性能强相关；奖励模型在多维度偏好捕捉上存在困难；推理时探测方法能可靠评估奖励预测置信度。

Conclusion: MRMBench是开发先进奖励模型的可靠参考，多目标优化在奖励建模中具有潜力，推理时探测方法能提升LLM对齐效果。

Abstract: Previous methods evaluate reward models by testing them on a fixed pairwise ranking test set, but they typically do not provide performance information on each preference dimension. In this work, we address the evaluation challenge of reward models by probing preference representations. To confirm the effectiveness of this evaluation method, we construct a Multi-dimensional Reward Model Benchmark (MRMBench), a collection of six probing tasks for different preference dimensions. We design it to favor and encourage reward models that better capture preferences across different dimensions. Furthermore, we introduce an analysis method, inference-time probing, which identifies the dimensions used during the reward prediction and enhances its interpretability. Through extensive experiments, we find that MRMBench strongly correlates with the alignment performance of large language models (LLMs), making it a reliable reference for developing advanced reward models. Our analysis of MRMBench evaluation results reveals that reward models often struggle to capture preferences across multiple dimensions, highlighting the potential of multi-objective optimization in reward modeling. Additionally, our findings show that the proposed inference-time probing method offers a reliable metric for assessing the confidence of reward predictions, which ultimately improves the alignment of LLMs.

</details>


### [33] [Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing](https://arxiv.org/abs/2511.12472)
*Mengying Wang,Chenhui Ma,Ao Jiao,Tuo Liang,Pengjun Lu,Shrinidhi Hegde,Yu Yin,Evren Gurkan-Cavusoglu,Yinghui Wu*

Main category: cs.CL

TL;DR: 提出了SerenQA框架来评估LLM在科学知识图谱问答中发现意外洞察的能力，重点关注药物重定位领域，包含严谨的意外性指标和专家标注基准。


<details>
  <summary>Details</summary>
Motivation: 现有KGQA系统通常返回高度相关但可预测的答案，缺乏发现意外和新颖答案的能力，需要评估LLM发现科学洞察的能力。

Method: 提出SerenQA框架，包含基于相关性、新颖性和意外性的严谨意外性指标，以及从临床知识图谱构建的专家标注基准，采用包含知识检索、子图推理和意外性探索的三阶段评估流程。

Result: 实验表明，最先进的LLM在检索方面表现良好，但在识别真正令人意外和有价值的发现方面仍有困难。

Conclusion: 当前LLM在发现意外洞察方面仍有显著改进空间，SerenQA为评估和改进这一能力提供了资源和基准。

Abstract: Large Language Models (LLMs) have greatly advanced knowledge graph question answering (KGQA), yet existing systems are typically optimized for returning highly relevant but predictable answers. A missing yet desired capacity is to exploit LLMs to suggest surprise and novel ("serendipitious") answers. In this paper, we formally define the serendipity-aware KGQA task and propose the SerenQA framework to evaluate LLMs' ability to uncover unexpected insights in scientific KGQA tasks. SerenQA includes a rigorous serendipity metric based on relevance, novelty, and surprise, along with an expert-annotated benchmark derived from the Clinical Knowledge Graph, focused on drug repurposing. Additionally, it features a structured evaluation pipeline encompassing three subtasks: knowledge retrieval, subgraph reasoning, and serendipity exploration. Our experiments reveal that while state-of-the-art LLMs perform well on retrieval, they still struggle to identify genuinely surprising and valuable discoveries, underscoring a significant room for future improvements. Our curated resources and extended version are released at: https://cwru-db-group.github.io/serenQA.

</details>


### [34] [SGuard-v1: Safety Guardrail for Large Language Models](https://arxiv.org/abs/2511.12497)
*JoonHo Lee,HyeonMin Cho,Jaewoong Yun,Hyunjae Lee,JunKyu Lee,Juree Seok*

Main category: cs.CL

TL;DR: SGuard-v1是一个轻量级的大语言模型安全护栏系统，包含ContentFilter和JailbreakFilter两个专用模型，用于检测有害内容和筛选对抗性提示，支持12种语言，在保持轻量化的同时实现最先进的安全性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决大语言模型在人类-AI对话场景中的安全风险，包括有害内容检测和对抗性提示攻击防护，同时降低部署开销。

Method: 基于2B参数的Granite-3.3-2B-Instruct模型构建，通过指令调优训练两个专用组件：ContentFilter用于识别安全风险，JailbreakFilter用于防御60种主要攻击类型，使用约140万训练实例。

Result: 在公共和专有安全基准测试中实现最先进的安全性能，保持轻量化，减少部署开销，并提供多类安全预测和二元置信度评分以提高可解释性。

Conclusion: SGuard-v1是一个高效、轻量级的安全护栏系统，在Apache-2.0许可下发布，支持进一步研究和实际部署。

Abstract: We present SGuard-v1, a lightweight safety guardrail for Large Language Models (LLMs), which comprises two specialized models to detect harmful content and screen adversarial prompts in human-AI conversational settings. The first component, ContentFilter, is trained to identify safety risks in LLM prompts and responses in accordance with the MLCommons hazard taxonomy, a comprehensive framework for trust and safety assessment of AI. The second component, JailbreakFilter, is trained with a carefully designed curriculum over integrated datasets and findings from prior work on adversarial prompting, covering 60 major attack types while mitigating false-unsafe classification. SGuard-v1 is built on the 2B-parameter Granite-3.3-2B-Instruct model that supports 12 languages. We curate approximately 1.4 million training instances from both collected and synthesized data and perform instruction tuning on the base model, distributing the curated data across the two component according to their designated functions. Through extensive evaluation on public and proprietary safety benchmarks, SGuard-v1 achieves state-of-the-art safety performance while remaining lightweight, thereby reducing deployment overhead. SGuard-v1 also improves interpretability for downstream use by providing multi-class safety predictions and their binary confidence scores. We release the SGuard-v1 under the Apache-2.0 License to enable further research and practical deployment in AI safety.

</details>


### [35] [QA-Noun: Representing Nominal Semantics via Natural Language Question-Answer Pairs](https://arxiv.org/abs/2511.12504)
*Maria Tseytlin,Paul Roit,Omri Abend,Ido Dagan,Ayal Klein*

Main category: cs.CL

TL;DR: QA-Noun是一个基于问答的框架，专门用于捕捉名词为中心的语义关系，通过9个问题模板覆盖名词的显式句法和隐式上下文角色，与QA-SRL结合实现句子意义的统一分解。


<details>
  <summary>Details</summary>
Motivation: 现有的基于QA的语义方法主要关注谓词-论元关系，但名词为中心的语义关系在很大程度上未被解决，需要一种能够全面捕捉名词语义角色的框架。

Method: 定义9个问题模板来覆盖名词的显式句法和隐式上下文角色，创建可解释的QA对，并与QA-SRL集成，形成统一的句子意义分解方法。

Result: QA-Noun几乎完全覆盖了AMR的名词论元，同时揭示了额外的上下文隐含关系，与QA-SRL结合后比FactScore和DecompScore等方法的粒度提高了130%以上。

Conclusion: QA-Noun补充了更广泛的基于QA的语义框架，形成了全面且可扩展的细粒度语义分解方法，适用于跨文本对齐任务。

Abstract: Decomposing sentences into fine-grained meaning units is increasingly used to model semantic alignment. While QA-based semantic approaches have shown effectiveness for representing predicate-argument relations, they have so far left noun-centered semantics largely unaddressed. We introduce QA-Noun, a QA-based framework for capturing noun-centered semantic relations. QA-Noun defines nine question templates that cover both explicit syntactical and implicit contextual roles for nouns, producing interpretable QA pairs that complement verbal QA-SRL. We release detailed guidelines, a dataset of over 2,000 annotated noun mentions, and a trained model integrated with QA-SRL to yield a unified decomposition of sentence meaning into individual, highly fine-grained, facts. Evaluation shows that QA-Noun achieves near-complete coverage of AMR's noun arguments while surfacing additional contextually implied relations, and that combining QA-Noun with QA-SRL yields over 130\% higher granularity than recent fact-based decomposition methods such as FactScore and DecompScore. QA-Noun thus complements the broader QA-based semantic framework, forming a comprehensive and scalable approach to fine-grained semantic decomposition for cross-text alignment.

</details>


### [36] [TAdaRAG: Task Adaptive Retrieval-Augmented Generation via On-the-Fly Knowledge Graph Construction](https://arxiv.org/abs/2511.12520)
*Jie Zhang,Bo Tang,Wanzi Shao,Wenqiang Wei,Jihao Zhao,Jianqing Zhu,Zhiyu li,Wen Xi,Zehao Lin,Feiyu Xiong,Yanchao Tan*

Main category: cs.CL

TL;DR: TAdaRAG是一个新颖的检索增强生成框架，通过动态构建任务自适应的知识图谱来解决传统RAG中信息截断和信息冗余的问题，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法将外部知识截断成小块导致信息丢失，引起回答幻觉和推理链断裂，同时检索的非结构化知识包含无关细节阻碍准确推理。

Method: 设计意图驱动的路由机制到领域特定提取模板，结合监督微调和基于强化学习的隐式提取机制，确保简洁、连贯且非冗余的知识整合。

Result: 在六个公共基准测试和一个真实商业基准测试(NowNewsQA)上，使用三个骨干模型进行评估，TAdaRAG在多样化领域和长文本任务中均优于现有方法。

Conclusion: TAdaRAG展示了强大的泛化能力和实际有效性，能够有效解决传统RAG的信息截断和冗余问题。

Abstract: Retrieval-Augmented Generation (RAG) improves large language models by retrieving external knowledge, often truncated into smaller chunks due to the input context window, which leads to information loss, resulting in response hallucinations and broken reasoning chains. Moreover, traditional RAG retrieves unstructured knowledge, introducing irrelevant details that hinder accurate reasoning. To address these issues, we propose TAdaRAG, a novel RAG framework for on-the-fly task-adaptive knowledge graph construction from external sources. Specifically, we design an intent-driven routing mechanism to a domain-specific extraction template, followed by supervised fine-tuning and a reinforcement learning-based implicit extraction mechanism, ensuring concise, coherent, and non-redundant knowledge integration. Evaluations on six public benchmarks and a real-world business benchmark (NowNewsQA) across three backbone models demonstrate that TAdaRAG outperforms existing methods across diverse domains and long-text tasks, highlighting its strong generalization and practical effectiveness.

</details>


### [37] [Mitigating Length Bias in RLHF through a Causal Lens](https://arxiv.org/abs/2511.12573)
*Hyeonji Kim,Sujeong Oh,Sanghack Lee*

Main category: cs.CL

TL;DR: 提出一种因果框架和反事实数据增强方法来减轻RLHF奖励模型中的长度偏差问题，通过构造长度不同但内容相似的响应对以及长度相似但内容不同的响应对来训练奖励模型。


<details>
  <summary>Details</summary>
Motivation: RLHF训练的奖励模型存在长度偏差，倾向于将冗长与质量混淆，偏爱更长的响应，这影响了模型评估的准确性。

Method: 使用反事实数据增强方法，构造两种类型的响应对：(1)长度不同但内容相似的响应对；(2)长度相似但内容不同的响应对，用于训练奖励模型。

Result: 实证评估表明该方法减少了奖励分配中的长度偏差，并使得策略模型产生更简洁、内容聚焦的输出。

Conclusion: 所提出的方法有效减轻了长度偏差，提高了RLHF流程中奖励建模的鲁棒性和内容敏感性。

Abstract: Reinforcement learning from human feedback (RLHF) is widely used to align large language models (LLMs) with human preferences. However, RLHF-trained reward models often exhibit length bias -- a systematic tendency to favor longer responses by conflating verbosity with quality. We propose a causal framework for analyzing and mitigating length bias in RLHF reward modeling. Central to our approach is a counterfactual data augmentation method that generates response pairs designed to isolate content quality from verbosity. These counterfactual examples are then used to train the reward model, enabling it to assess responses based on content quality independently of verbosity. Specifically, we construct (1) length-divergent pairs with similar content and (2) content-divergent pairs of similar length. Empirical evaluations show that our method reduces length bias in reward assignment and leads to more concise, content-focused outputs from the policy model. These findings demonstrate that the proposed approach effectively reduces length bias and improves the robustness and content sensitivity of reward modeling in RLHF pipelines.

</details>


### [38] [MMWOZ: Building Multimodal Agent for Task-oriented Dialogue](https://arxiv.org/abs/2511.12586)
*Pu-Hai Yang,Heyan Huang,Heng-Da Xu,Fanshu Sun,Xian-Ling Mao,Chaoxu Mu*

Main category: cs.CL

TL;DR: MMWOZ是一个从MultiWOZ 2.3扩展的多模态对话数据集，通过开发网页GUI和自动化脚本将对话状态转换为操作指令，旨在弥合传统任务导向对话系统在现实应用中的差距。


<details>
  <summary>Details</summary>
Motivation: 传统任务导向对话系统依赖定制后端API，而现实场景中普遍存在前端GUI且缺乏定制API，这造成了实际应用中的显著差距。

Method: 开发网页GUI作为前端，设计自动化脚本将原始数据集中的对话状态和系统动作转换为GUI操作指令，收集网页快照及对应操作指令，并提出了MATE多模态模型作为基线。

Result: 创建了MMWOZ多模态对话数据集，并提出了MATE模型作为该数据集的基线模型。

Conclusion: 通过MMWOZ数据集和MATE模型，为构建实用的多模态任务导向对话代理提供了基础，并进行了全面的实验分析。

Abstract: Task-oriented dialogue systems have garnered significant attention due to their conversational ability to accomplish goals, such as booking airline tickets for users. Traditionally, task-oriented dialogue systems are conceptualized as intelligent agents that interact with users using natural language and have access to customized back-end APIs. However, in real-world scenarios, the widespread presence of front-end Graphical User Interfaces (GUIs) and the absence of customized back-end APIs create a significant gap for traditional task-oriented dialogue systems in practical applications. In this paper, to bridge the gap, we collect MMWOZ, a new multimodal dialogue dataset that is extended from MultiWOZ 2.3 dataset. Specifically, we begin by developing a web-style GUI to serve as the front-end. Next, we devise an automated script to convert the dialogue states and system actions from the original dataset into operation instructions for the GUI. Lastly, we collect snapshots of the web pages along with their corresponding operation instructions. In addition, we propose a novel multimodal model called MATE (Multimodal Agent for Task-oriEnted dialogue) as the baseline model for the MMWOZ dataset. Furthermore, we conduct comprehensive experimental analysis using MATE to investigate the construction of a practical multimodal agent for task-oriented dialogue.

</details>


### [39] [Group-Aware Reinforcement Learning for Output Diversity in Large Language Models](https://arxiv.org/abs/2511.12596)
*Oron Anschel,Alon Shoshan,Adam Botach,Shunit Haviv Hakimi,Asaf Gendler,Emanuel Ben Baruch,Nadav Bhonker,Igor Kviatkovsky,Manoj Aggarwal,Gerard Medioni*

Main category: cs.CL

TL;DR: GAPO是一种基于GRPO的扩展方法，通过计算群体层面的奖励来鼓励LLM生成更多样化的响应，解决了模式崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常出现模式崩溃问题，即使存在多个有效答案，也会重复生成相同的几个补全结果，这限制了模型在各种任务中的多样性。

Method: GAPO是GRPO的简单扩展，计算群体层面的奖励，使用频率感知的奖励函数来鼓励在有效补全中均匀采样。

Result: GAPO训练的模型能够产生有效且更多样化的响应，在开放提示下也能提高响应多样性，同时不影响标准LLM基准测试的准确性。

Conclusion: GAPO能够有效解决LLM的模式崩溃问题，提高响应多样性，同时保持模型性能，具有良好的通用性。

Abstract: Large Language Models (LLMs) often suffer from mode collapse, repeatedly generating the same few completions even when many valid answers exist, limiting their diversity across a wide range of tasks. We introduce Group-Aware Policy Optimization (GAPO), a simple extension of the recent and popular Group Relative Policy Optimization (GRPO) that computes rewards over the group as a whole. GAPO enables learning from the group-level properties such as diversity and coverage. We demonstrate GAPO using a frequency-aware reward function that encourages uniform sampling over valid LLM completions, and show that GAPO-trained models produce valid and more diverse model responses. Beyond this setup, GAPO generalizes to open-ended prompts and improves response diversity without compromising accuracy on standard LLM benchmarks (GSM8K, MATH, HumanEval, MMLU-Pro). Our code will be made publicly available.

</details>


### [40] [Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data](https://arxiv.org/abs/2511.12609)
*Yunxin Li,Xinyu Chen,Shenyuan Jiang,Haoyuan Shi,Zhenyu Liu,Xuanyu Zhang,Nanhao Deng,Zhenran Xu,Yicheng Ma,Meishan Zhang,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: Uni-MoE 2.0是一个全开源的跨模态大模型，基于Qwen2.5-7B架构构建，采用动态容量的MoE设计、渐进式训练策略和精心策划的多模态数据匹配技术，在85个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 推动Lychee Uni-MoE系列在语言中心的多模态理解、推理和生成方面的进展，构建一个能够理解所有模态并生成图像、文本和语音的全能模型。

Method: 采用动态容量MoE框架，包含共享、路由和空专家；使用Omni-Modality 3D RoPE确保跨模态对齐；采用渐进式监督微调策略，结合GSPO-DPO方法稳定强化学习训练。

Result: 在85个基准测试中达到SOTA或高度竞争力，在76个基准中超过50个优于Qwen2.5-Omni；视频理解提升7%，全模态理解提升7%，视听推理提升4%；长语音处理WER降低4.2%。

Conclusion: Uni-MoE 2.0证明了其架构和训练策略的有效性，在多个跨模态任务中表现出色，为全开源跨模态大模型的发展提供了重要参考。

Abstract: We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee's Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the Qwen2.5-7B dense architecture, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.

</details>


### [41] [Knots: A Large-Scale Multi-Agent Enhanced Expert-Annotated Dataset and LLM Prompt Optimization for NOTAM Semantic Parsing](https://arxiv.org/abs/2511.12630)
*Maoqi Liu,Quan Fang,Yang Yang,Can Zhao,Kaiquan Cai*

Main category: cs.CL

TL;DR: 本文提出了NOTAM语义解析任务，构建了Knots数据集，并通过多智能体协作框架和多种提示工程策略显著提升了航空文本的理解能力。


<details>
  <summary>Details</summary>
Motivation: NOTAMs作为飞行安全信息的关键渠道，其复杂的语言结构和隐含推理给自动化解析带来挑战。现有研究主要关注分类和命名实体识别等表层任务，缺乏深度语义理解。

Method: 提出NOTAM语义解析任务，构建包含12,347条专家标注NOTAMs的Knots数据集，采用多智能体协作框架进行全面的字段发现，并系统评估多种提示工程策略和模型适配技术。

Result: 实验结果表明，所提方法在航空文本理解和处理方面取得了显著改进，为自动化NOTAM分析系统提供了有价值的见解。

Conclusion: 该方法有效解决了NOTAMs的语义解析挑战，通过结合领域知识和推理能力，能够生成结构化的、富含推理信息的输出。

Abstract: Notice to Air Missions (NOTAMs) serve as a critical channel for disseminating key flight safety information, yet their complex linguistic structures and implicit reasoning pose significant challenges for automated parsing. Existing research mainly focuses on surface-level tasks such as classification and named entity recognition, lacking deep semantic understanding. To address this gap, we propose NOTAM semantic parsing, a task emphasizing semantic inference and the integration of aviation domain knowledge to produce structured, inference-rich outputs. To support this task, we construct Knots (Knowledge and NOTAM Semantics), a high-quality dataset of 12,347 expert-annotated NOTAMs covering 194 Flight Information Regions, enhanced through a multi-agent collaborative framework for comprehensive field discovery. We systematically evaluate a wide range of prompt-engineering strategies and model-adaptation techniques, achieving substantial improvements in aviation text understanding and processing. Our experimental results demonstrate the effectiveness of the proposed approach and offer valuable insights for automated NOTAM analysis systems. Our code is available at: https://github.com/Estrellajer/Knots.

</details>


### [42] [Reason-KE++: Aligning the Process, Not Just the Outcome, for Faithful LLM Knowledge Editing](https://arxiv.org/abs/2511.12661)
*Yuchen Wu,Liang Ding,Li Shen,Dacheng Tao*

Main category: cs.CL

TL;DR: Reason-KE++是一个SFT+RL框架，通过过程级忠实性对齐来解决LLM在复杂多跳推理任务中的事实幻觉问题，在MQUAKE-CF-3k上达到95.48%的新SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在复杂多跳推理任务中对新知识的忠实性问题，现有SFT方法存在"忠实性差距"，导致模型参数先验覆盖上下文事实，产生关键事实幻觉。

Method: 提出Reason-KE++框架，结合SFT和RL，核心是阶段感知奖励机制，为中间推理步骤提供密集监督，包括分解和子答案正确性等过程级评估。

Result: 在MQUAKE-CF-3k数据集上达到95.48%的准确率，相比之前方法提升5.28%，同时避免了仅基于结果的RL方法导致推理完整性崩溃的问题。

Conclusion: 对于复杂任务，对齐推理过程对于构建可信赖的LLM至关重要，过程感知框架能有效解决LLM对齐的核心问题。

Abstract: Aligning Large Language Models (LLMs) to be faithful to new knowledge in complex, multi-hop reasoning tasks is a critical, yet unsolved, challenge. We find that SFT-based methods, e.g., Reason-KE, while state-of-the-art, suffer from a "faithfulness gap": they optimize for format mimicry rather than sound reasoning. This gap enables the LLM's powerful parametric priors to override new contextual facts, resulting in critical factual hallucinations (e.g., incorrectly reasoning "Houston" from "NASA" despite an explicit edit). To solve this core LLM alignment problem, we propose Reason-KE++, an SFT+RL framework that instills process-level faithfulness. Its core is a Stage-aware Reward mechanism that provides dense supervision for intermediate reasoning steps (e.g., Decomposition, Sub-answer Correctness). Crucially, we identify that naive outcome-only RL is a deceptive trap for LLM alignment: it collapses reasoning integrity (e.g., 19.00% Hop acc) while superficially boosting final accuracy. Our process-aware framework sets a new SOTA of 95.48% on MQUAKE-CF-3k (+5.28%), demonstrating that for complex tasks, aligning the reasoning process is essential for building trustworthy LLMs.

</details>


### [43] [Improving Direct Persian-English Speech-to-Speech Translation with Discrete Units and Synthetic Parallel Data](https://arxiv.org/abs/2511.12690)
*Sina Rashidi,Hossein Sameti*

Main category: cs.CL

TL;DR: 本文提出了一种波斯语到英语的直接语音翻译系统，通过自监督预训练、离散语音单元和合成平行数据来解决低资源语言对的语音翻译问题。


<details>
  <summary>Details</summary>
Motivation: 直接语音翻译系统需要大量的平行语音数据，但对于波斯语等低资源语言来说，这种数据非常稀缺。本文旨在解决波斯语-英语语言对的数据稀缺问题。

Method: 系统包含三个组件：基于conformer的编码器、因果transformer解码器和基于单元的神经声码器。通过使用大语言模型翻译波斯语语音转录并合成英语语音，构建了合成平行语音语料库。

Result: 在CVSS语料库的波斯语-英语部分，使用合成数据比直接基线提高了4.6 ASR BLEU。合成语料库使可用平行语音数据增加了约6倍。

Conclusion: 结合自监督预训练、离散语音单元和合成平行数据对于改善波斯语-英语等低资源语言对的直接语音翻译是有效的。

Abstract: Direct speech-to-speech translation (S2ST), in which all components are trained jointly, is an attractive alternative to cascaded systems because it offers a simpler pipeline and lower inference latency. However, direct S2ST models require large amounts of parallel speech data in the source and target languages, which are rarely available for low-resource languages such as Persian. This paper presents a direct S2ST system for translating Persian speech into English speech, as well as a pipeline for synthetic parallel Persian-English speech generation. The model comprises three components: (1) a conformer-based encoder, initialized from self-supervised pre-training, maps source speech to high-level acoustic representations; (2) a causal transformer decoder with relative position multi-head attention translates these representations into discrete target speech units; (3) a unit-based neural vocoder generates waveforms from the predicted discrete units. To mitigate the data scarcity problem, we construct a new Persian-English parallel speech corpus by translating Persian speech transcriptions into English using a large language model and then synthesizing the corresponding English speech with a state-of-the-art zero-shot text-to-speech system. The resulting corpus increases the amount of available parallel speech by roughly a factor of six. On the Persian-English portion of the CVSS corpus, the proposed model achieves improvement of 4.6 ASR BLEU with the synthetic data over direct baselines. These results indicate that combining self-supervised pre-training, discrete speech units, and synthetic parallel data is effective for improving direct S2ST in low-resource language pairs such as Persian-English

</details>


### [44] [Evolve the Method, Not the Prompts: Evolutionary Synthesis of Jailbreak Attacks on LLMs](https://arxiv.org/abs/2511.12710)
*Yunhao Chen,Xin Wang,Juncheng Li,Yixu Wang,Jie Li,Yan Teng,Yingchun Wang,Xingjun Ma*

Main category: cs.CL

TL;DR: EvoSynth是一个自主框架，通过进化合成代码级攻击算法来实现对LLM的越狱攻击，相比现有方法能自主发明全新攻击机制，在Claude-Sonnet-4.5上达到85.5%的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有自动化红队框架的越狱逻辑局限于选择、组合或改进现有攻击策略，无法自主发明全新攻击机制，限制了其创造力。

Method: 采用多智能体系统自主设计、进化和执行基于代码的新型攻击算法，包含代码级自校正循环，能够根据失败情况迭代重写攻击逻辑。

Result: 在Claude-Sonnet-4.5等强健模型上达到85.5%的攻击成功率，生成比现有方法更多样化的攻击方式，建立了新的最先进水平。

Conclusion: EvoSynth通过将范式从攻击规划转向进化合成，成功克服了现有框架的局限性，为越狱方法的进化合成研究开辟了新方向。

Abstract: Automated red teaming frameworks for Large Language Models (LLMs) have become increasingly sophisticated, yet they share a fundamental limitation: their jailbreak logic is confined to selecting, combining, or refining pre-existing attack strategies. This binds their creativity and leaves them unable to autonomously invent entirely new attack mechanisms. To overcome this gap, we introduce \textbf{EvoSynth}, an autonomous framework that shifts the paradigm from attack planning to the evolutionary synthesis of jailbreak methods. Instead of refining prompts, EvoSynth employs a multi-agent system to autonomously engineer, evolve, and execute novel, code-based attack algorithms. Crucially, it features a code-level self-correction loop, allowing it to iteratively rewrite its own attack logic in response to failure. Through extensive experiments, we demonstrate that EvoSynth not only establishes a new state-of-the-art by achieving an 85.5\% Attack Success Rate (ASR) against highly robust models like Claude-Sonnet-4.5, but also generates attacks that are significantly more diverse than those from existing methods. We release our framework to facilitate future research in this new direction of evolutionary synthesis of jailbreak methods. Code is available at: https://github.com/dongdongunique/EvoSynth.

</details>


### [45] [Adaptive Focus Memory for Language Models](https://arxiv.org/abs/2511.12712)
*Christopher Cruz*

Main category: cs.CL

TL;DR: AFM是一种动态上下文管理器，通过语义相似度、时间衰减和重要性分类为历史消息分配三种保真度级别，在严格token预算下显著减少推理成本同时保持安全性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在多轮对话中固定上下文窗口和朴素内存策略的瓶颈问题，避免静态摘要或仅基于新近度的启发式方法丢失关键安全细节。

Method: AFM为每个历史消息分配FULL、COMPRESSED或PLACEHOLDER保真度级别，基于语义相似度、半衰期新近度加权和重要性分类，在token预算内按时间顺序打包消息。

Result: 在涉及严重花生过敏用户规划泰国旅行的安全导向基准测试中，AFM在短中长度对话中保持过敏信息，安全性能与朴素重放相当，平均token使用量比重放基线减少66%。

Conclusion: AFM提供了一种模块化实现，可在不牺牲安全性或事实连续性的前提下显著降低推理成本，适用于OpenAI兼容API和离线操作。

Abstract: Large language models (LLMs) are increasingly deployed in multi-turn dialogue settings, but their behavior is still bottlenecked by fixed context windows and naive memory strategies. Replaying the full conversation at every turn is simple but expensive, while static summarization or recency-only heuristics often erase safety-critical user details. We present Adaptive Focus Memory (AFM), a dynamic context manager that assigns each past message one of three fidelity levels -- FULL, COMPRESSED, or PLACEHOLDER -- based on semantic similarity to the current query, half-life recency weighting, and importance classification. AFM packs messages chronologically under a strict token budget, preferring high fidelity for the most relevant turns while aiming to preserve a cheap trace of the dialogue. In a safety-oriented benchmark involving a user with a severe peanut allergy planning a trip to Thailand, AFM retains the allergy across both short and medium-length conversations, matches the safety performance of naive replay, and cuts average token usage by 66% relative to a replay baseline. We release a modular Python implementation of AFM designed for OpenAI-compatible APIs and offline operation, enabling practitioners to reduce inference cost without sacrificing safety or factual continuity in the evaluated scenario.

</details>


### [46] [On the Brittleness of LLMs: A Journey around Set Membership](https://arxiv.org/abs/2511.12728)
*Lea Hergert,Gábor Berend,Mario Szegedy,Gyorgy Turan,Márk Jelasity*

Main category: cs.CL

TL;DR: LLMs在复杂推理任务上表现超人类，但在简单集合成员查询任务中却频繁失败，揭示了其推理能力的脆弱性和不可预测性。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在复杂任务表现优异却在简单任务上失败的矛盾现象，通过简单但可扩展的实验设计来揭示基本失败模式。

Method: 使用集合成员查询作为基础推理任务，系统评估提示措辞、语义结构、元素排序和模型选择等维度，进行大规模实证分析。

Result: LLMs在这一基础任务上的表现持续脆弱且在所有维度上都不可预测，表明模型对集合概念的理解是碎片化和复杂的。

Conclusion: 通过简单问题的大规模实验能够全面映射和分析失败模式，这种方法为LLM评估提供了有价值的方法论。

Abstract: Large language models (LLMs) achieve superhuman performance on complex reasoning tasks, yet often fail on much simpler problems, raising concerns about their reliability and interpretability. We investigate this paradox through a focused study with two key design features: simplicity, to expose basic failure modes, and scale, to enable comprehensive controlled experiments. We focus on set membership queries -- among the most fundamental forms of reasoning -- using tasks like ``Is apple an element of the set \{pear, plum, apple, raspberry\}?''. We conduct a systematic empirical evaluation across prompt phrasing, semantic structure, element ordering, and model choice. Our large-scale analysis reveals that LLM performance on this elementary task is consistently brittle, and unpredictable across all dimensions, suggesting that the models' ``understanding'' of the set concept is fragmented and convoluted at best. Our work demonstrates that the large-scale experiments enabled by the simplicity of the problem allow us to map and analyze the failure modes comprehensively, making this approach a valuable methodology for LLM evaluation in general.

</details>


### [47] [Evidence of Phase Transitions in Small Transformer-Based Language Models](https://arxiv.org/abs/2511.12768)
*Noah Hong,Tao Hong*

Main category: cs.CL

TL;DR: 研究发现语言模型训练中存在相变现象，即使在小规模transformer模型中也能观察到，这些相变在词汇使用和统计特征上表现明显，但在标准损失曲线中不可见。


<details>
  <summary>Details</summary>
Motivation: 探索相变现象是否仅限于大型语言模型，能否在线性训练空间中直接检测到，以及这些相变是否在训练早期就出现。

Method: 训练小型GPT风格transformer模型，分析词汇使用演变，包括平均词长、正确与错误词汇数量、词汇多样性变化，并应用泊松和次泊松统计来量化词汇连接和重组。

Result: 在训练过程中发现明显的相变点，这些相变在标准损失或验证曲线中不可见，但通过词汇和统计探针变得可见。

Conclusion: 相变重组是语言模型训练的普遍特征，可在小型模型中观察到，能在线性训练空间中直接检测，并在训练早期就出现，为理解语言模型训练的非线性动态提供了新视角。

Abstract: Phase transitions have been proposed as the origin of emergent abilities in large language models (LLMs), where new capabilities appear abruptly once models surpass critical thresholds of scale. Prior work, such as that of Wei et al., demonstrated these phenomena under model and data scaling, with transitions revealed after applying a log scale to training compute. In this work, we ask three complementary questions: (1) Are phase transitions unique to large models, or can they also be observed in small transformer-based language models? (2) Can such transitions be detected directly in linear training space, rather than only after log rescaling? and (3) Can these transitions emerge at early stages of training? To investigate, we train a small GPT-style transformer on a character-level corpus and analyze the evolution of vocabulary usage throughout training. We track the average word length, the number of correct versus incorrect words, and shifts in vocabulary diversity. Building on these measures, we apply Poisson and sub-Poisson statistics to quantify how words connect and reorganize. This combined analysis reveals a distinct transition point during training. Notably, these transitions are not apparent in standard loss or validation curves, but become visible through our vocabulary- and statistics-based probes. Our findings suggest that phase-transition reorganizations are a general feature of language model training, observable even in modest models, detectable directly in linear training space, and occurring surprisingly early as coherence emerges. This perspective provides new insight into the nonlinear dynamics of language model training and underscores the importance of tailored metrics for uncovering phase transition behaviors

</details>


### [48] [LLM Reinforcement in Context](https://arxiv.org/abs/2511.12782)
*Thomas Rivasseau*

Main category: cs.CL

TL;DR: 提出使用中断机制来增强大语言模型的对齐性，通过在用户输入中定期插入控制语句来防止越狱行为。


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐研究主要关注通过训练和提示来提高模型鲁棒性，但缺乏随着用户输入长度增加而有效增强对齐的方法。研究发现LLM越狱概率随用户输入或对话长度增加而上升。

Method: 提出中断机制，在用户输入中每x个token插入控制语句，并建议将此方法推广到思维链过程以防止策略性行为。

Result: 论文提出了中断机制的概念框架，但未提供具体的实验结果。

Conclusion: 中断机制是增强LLM对齐性的潜在解决方案，能够随着用户输入长度扩展而保持有效性。

Abstract: Current Large Language Model alignment research mostly focuses on improving model robustness against adversarial attacks and misbehavior by training on examples and prompting. Research has shown that LLM jailbreak probability increases with the size of the user input or conversation length. There is a lack of appropriate research into means of strengthening alignment which also scale with user input length. We propose interruptions as a possible solution to this problem. Interruptions are control sentences added to the user input approximately every x tokens for some arbitrary x. We suggest that this can be generalized to the Chain-of-Thought process to prevent scheming.

</details>


### [49] [Evaluating Autoformalization Robustness via Semantically Similar Paraphrasing](https://arxiv.org/abs/2511.12784)
*Hayden Moore,Asfahan Shah*

Main category: cs.CL

TL;DR: 评估大型语言模型在自动形式化任务中对语义相似但表述不同的自然语言输入的鲁棒性，发现即使是轻微的语言变化也会显著影响模型输出。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在自动形式化方面表现出色，但近期研究表明它们对语义保持的改写输入很敏感。本文旨在验证这一现象在自动形式化领域的表现。

Method: 使用MiniF2F和Lean 4版本的ProofNet作为基准，在两个现代LLMs上生成改写后的自然语言陈述，并通过语义和编译有效性进行交叉评估。

Result: 结果显示改写输入之间存在性能差异，表明自然语言陈述的微小变化会显著影响模型输出。

Conclusion: LLMs在自动形式化任务中对语言表述变化敏感，需要提高其鲁棒性以确保形式化结果的可靠性。

Abstract: Large Language Models (LLMs) have recently emerged as powerful tools for autoformalization. Despite their impressive performance, these models can still struggle to produce grounded and verifiable formalizations. Recent work in text-to-SQL, has revealed that LLMs can be sensitive to paraphrased natural language (NL) inputs, even when high degrees of semantic fidelity are preserved (Safarzadeh, Oroojlooyjadid, and Roth 2025). In this paper, we investigate this claim in the autoformalization domain. Specifically, we evaluate the robustness of LLMs generating formal proofs with semantically similar paraphrased NL statements by measuring semantic and compilation validity. Using the formal benchmarks MiniF2F (Zheng, Han, and Polu 2021) and Lean 4 version of ProofNet (Xin et al. 2024), and two modern LLMs, we generate paraphrased natural language statements and cross-evaluate these statements across both models. The results of this paper reveal performance variability across paraphrased inputs, demonstrating that minor shifts in NL statements can significantly impact model outputs.

</details>


### [50] [BioMedJImpact: A Comprehensive Dataset and LLM Pipeline for AI Engagement and Scientific Impact Analysis of Biomedical Journals](https://arxiv.org/abs/2511.12821)
*Ruiyu Wang,Yuzhang Xie,Xiao Hu,Carl Yang,Jiaying Lu*

Main category: cs.CL

TL;DR: BioMedJImpact是一个大规模生物医学期刊影响数据集，整合了文献计量指标、合作特征和LLM衍生的AI参与度指标，揭示了合作强度和AI参与度共同影响科学影响力的趋势。


<details>
  <summary>Details</summary>
Motivation: 现有开放资源很少捕捉合作结构和AI研究如何共同塑造生物医学期刊声望，需要开发一个综合数据集来推进期刊层面的科学影响和AI参与度分析。

Method: 从PubMed Central的174万篇文章构建数据集，提出可复现的三阶段LLM流程提取AI参与度特征，分析合作强度和AI参与度在疫情前后对科学影响的共同作用。

Result: 发现两个一致趋势：合作强度更高的期刊（特别是作者团队更大更多样化的）获得更高引用影响；AI参与度日益成为期刊声望的强相关因素，尤其在四分位排名中。

Conclusion: BioMedJImpact既是捕捉生物医学与AI交叉的综合数据集，也是经过验证的方法框架，支持可扩展、内容感知的科学计量分析。

Abstract: Assessing journal impact is central to scholarly communication, yet existing open resources rarely capture how collaboration structures and artificial intelligence (AI) research jointly shape venue prestige in biomedicine. We present BioMedJImpact, a large-scale, biomedical-oriented dataset designed to advance journal-level analysis of scientific impact and AI engagement. Built from 1.74 million PubMed Central articles across 2,744 journals, BioMedJImpact integrates bibliometric indicators, collaboration features, and LLM-derived semantic indicators for AI engagement. Specifically, the AI engagement feature is extracted through a reproducible three-stage LLM pipeline that we propose. Using this dataset, we analyze how collaboration intensity and AI engagement jointly influence scientific impact across pre- and post-pandemic periods (2016-2019, 2020-2023). Two consistent trends emerge: journals with higher collaboration intensity, particularly those with larger and more diverse author teams, tend to achieve greater citation impact, and AI engagement has become an increasingly strong correlate of journal prestige, especially in quartile rankings. To further validate the three-stage LLM pipeline we proposed for deriving the AI engagement feature, we conduct human evaluation, confirming substantial agreement in AI relevance detection and consistent subfield classification. Together, these contributions demonstrate that BioMedJImpact serves as both a comprehensive dataset capturing the intersection of biomedicine and AI, and a validated methodological framework enabling scalable, content-aware scientometric analysis of scientific impact and innovation dynamics. Code is available at https://github.com/JonathanWry/BioMedJImpact.

</details>


### [51] [From Passive to Persuasive: Steering Emotional Nuance in Human-AI Negotiation](https://arxiv.org/abs/2511.12832)
*Niranjan Chebrolu,Gerard Christopher Yeo,Kokil Jaidka*

Main category: cs.CL

TL;DR: 通过目标激活工程引导LLaMA 3.1-8B展现更人性化的情感表达，使用归因修补识别关键组件，从对比文本对中提取情感表达向量，显著增强情感特征。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型对话流畅度不断提升，但赋予其细腻、人性化的情感表达仍具挑战性。现有对齐技术往往只处理表层输出或需要大量微调。

Method: 使用归因修补识别因果影响组件，通过观察诊断对话任务中的激活模式找到关键干预位点；从对比文本对（积极vs消极情感示例）的激活差异中提取情感表达向量。

Result: 将情感向量应用于新对话提示显著增强情感特征：引导的回复显示积极情感（如喜悦、信任）增加，第一人称代词使用更频繁，表明更强的个人参与度。

Conclusion: 研究提供了一个精确且可解释的框架，为对话AI研究开辟了新方向。

Abstract: Large Language Models (LLMs) demonstrate increasing conversational fluency, yet instilling them with nuanced, human-like emotional expression remains a significant challenge. Current alignment techniques often address surface-level output or require extensive fine-tuning. This paper demonstrates that targeted activation engineering can steer LLaMA 3.1-8B to exhibit more human-like emotional nuances. We first employ attribution patching to identify causally influential components, to find a key intervention locus by observing activation patterns during diagnostic conversational tasks. We then derive emotional expression vectors from the difference in the activations generated by contrastive text pairs (positive vs. negative examples of target emotions). Applying these vectors to new conversational prompts significantly enhances emotional characteristics: steered responses show increased positive sentiment (e.g., joy, trust) and more frequent first-person pronoun usage, indicative of greater personal engagement. Our findings offer a precise and interpretable framework and new directions for the study of conversational AI.

</details>


### [52] [Quantifying consistency and accuracy of Latent Dirichlet Allocation](https://arxiv.org/abs/2511.12850)
*Saranzaya Magsarjav,Melissa Humphries,Jonathan Tuke,Lewis Mitchell*

Main category: cs.CL

TL;DR: 本文提出了一种新的稳定性度量方法，用于评估LDA主题模型的一致性，发现LDA虽然能正确识别主题数量且内部一致性较高，但生成的主题并非真实主题。


<details>
  <summary>Details</summary>
Motivation: 概率主题模型由于随机性在多次运行时会产生不一致的结果，影响可重复性、可靠性和解释性，需要评估其是否真正捕捉到有意义的主题还是仅仅噪声。

Method: 定义了一个结合准确性和一致性的新稳定性度量，利用LDA的生成特性创建带有真实主题的新语料库，并对每个语料库运行LDA 50次来评估输出变异性。

Result: LDA能够正确确定文档中的基础主题数量，且多次重运行时返回相似主题，表现出较高的内部一致性，但这些主题并非真实主题。

Conclusion: LDA模型在识别主题数量和内部一致性方面表现良好，但生成的主题与真实主题存在差异，需要进一步改进以提高主题建模的准确性和可靠性。

Abstract: Topic modelling in Natural Language Processing uncovers hidden topics in large, unlabelled text datasets. It is widely applied in fields such as information retrieval, content summarisation, and trend analysis across various disciplines. However, probabilistic topic models can produce different results when rerun due to their stochastic nature, leading to inconsistencies in latent topics. Factors like corpus shuffling, rare text removal, and document elimination contribute to these variations. This instability affects replicability, reliability, and interpretation, raising concerns about whether topic models capture meaningful topics or just noise. To address these problems, we defined a new stability measure that incorporates accuracy and consistency and uses the generative properties of LDA to generate a new corpus with ground truth. These generated corpora are run through LDA 50 times to determine the variability in the output. We show that LDA can correctly determine the underlying number of topics in the documents. We also find that LDA is more internally consistent, as the multiple reruns return similar topics; however, these topics are not the true topics.

</details>


### [53] [NeuroLex: A Lightweight Domain Language Model for EEG Report Understanding and Generation](https://arxiv.org/abs/2511.12851)
*Kang Yin,Hye-Bin Shin*

Main category: cs.CL

TL;DR: NeuroLex是一个专门针对临床脑电图报告训练的轻量级领域自适应语言模型，相比通用模型在EEG报告处理方面表现更好，可作为独立文本模型或多模态系统的解码器骨干。


<details>
  <summary>Details</summary>
Motivation: 通用语言模型无法捕捉临床脑电图报告中的领域特定语言惯例，需要专门针对EEG报告文本训练的模型。

Method: 使用哈佛脑电图数据库的EEG报告文本进行训练，采用span-corruption预训练和指令式微调（报告润色、段落摘要、术语问答），学习EEG解释的语法和推理模式。

Result: 相比同规模通用模型，NeuroLex实现了更低的困惑度、更高的提取和摘要准确度、更好的标签效率，以及对否定和事实幻觉更强的鲁棒性。

Conclusion: NeuroLex通过EEG感知的语言骨干，连接了生物医学文本建模和脑机接口应用，为可解释和语言驱动的神经解码提供了基础。

Abstract: Clinical electroencephalogram (EEG) reports encode domain-specific linguistic conventions that general-purpose language models (LMs) fail to capture. We introduce NeuroLex, a lightweight domain-adaptive language model trained purely on EEG report text from the Harvard Electroencephalography Database. Unlike existing biomedical LMs, NeuroLex is tailored to the linguistic and diagnostic characteristics of EEG reporting, enabling it to serve as both an independent textual model and a decoder backbone for multimodal EEG-language systems. Using span-corruption pretraining and instruction-style fine-tuning on report polishing, paragraph summarization, and terminology question answering, NeuroLex learns the syntax and reasoning patterns characteristic of EEG interpretation. Comprehensive evaluations show that it achieves lower perplexity, higher extraction and summarization accuracy, better label efficiency, and improved robustness to negation and factual hallucination compared with general models of the same scale. With an EEG-aware linguistic backbone, NeuroLex bridges biomedical text modeling and brain-computer interface applications, offering a foundation for interpretable and language-driven neural decoding.

</details>


### [54] [From Perception to Reasoning: Deep Thinking Empowers Multimodal Large Language Models](https://arxiv.org/abs/2511.12861)
*Wenxin Zhu,Andong Chen,Yuchen Song,Kehai Chen,Conghui Zhu,Ziyan Chen,Tiejun Zhao*

Main category: cs.CL

TL;DR: 本文系统综述了多模态思维链(MCoT)的研究进展，分析了其背景动机、主流方法、评估基准、应用场景，并讨论了当前挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型在感知任务中的显著成功，增强其复杂推理能力成为关键研究重点。现有模型存在推理路径不透明和泛化能力不足等挑战，而思维链推理在语言模型中已证明能增强推理透明度和输出可解释性，有望在扩展到多模态领域后提升模型推理能力。

Method: 从三个方面介绍主流MCoT方法：思维链范式、后训练阶段和推理阶段，并分析其底层机制。

Result: 总结了现有的评估基准和指标，讨论了MCoT的应用场景。

Conclusion: 分析了MCoT当前面临的挑战，并对其未来研究方向进行了展望。

Abstract: With the remarkable success of Multimodal Large Language Models (MLLMs) in perception tasks, enhancing their complex reasoning capabilities has emerged as a critical research focus. Existing models still suffer from challenges such as opaque reasoning paths and insufficient generalization ability. Chain-of-Thought (CoT) reasoning, which has demonstrated significant efficacy in language models by enhancing reasoning transparency and output interpretability, holds promise for improving model reasoning capabilities when extended to the multimodal domain. This paper provides a systematic review centered on "Multimodal Chain-of-Thought" (MCoT). First, it analyzes the background and theoretical motivations for its inception from the perspectives of technical evolution and task demands. Then, it introduces mainstream MCoT methods from three aspects: CoT paradigms, the post-training stage, and the inference stage, while also analyzing their underlying mechanisms. Furthermore, the paper summarizes existing evaluation benchmarks and metrics, and discusses the application scenarios of MCoT. Finally, it analyzes the challenges currently facing MCoT and provides an outlook on its future research directions.

</details>


### [55] [Classification of Hope in Textual Data using Transformer-Based Models](https://arxiv.org/abs/2511.12874)
*Chukwuebuka Fortunate Ijezue,Tania-Amanda Fredrick Eneye,Maaz Amjad*

Main category: cs.CL

TL;DR: 本文比较了三种基于Transformer的架构（BERT、GPT-2、DeBERTa）在希望表达分类任务中的表现，发现BERT在准确性和计算效率方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 开发一个计算框架来分析文本中的希望表达，应用于心理健康和社交媒体分析领域，同时探索不同架构在专门情感检测任务中的适用性。

Method: 使用BERT、GPT-2和DeBERTa三种Transformer架构进行二元分类（希望vs非希望）和多类别分类（五个希望相关类别），比较它们的准确性和计算效率。

Result: BERT在二元分类和多类别分类中均表现最佳（84.49%二元准确率，72.03%多类别准确率），且计算效率最高（443秒训练时间）。GPT-2表现最差，但擅长检测讽刺性希望表达（92.46%召回率）。DeBERTa表现中等但计算成本最高。

Conclusion: 对于专门的希望表达检测任务，架构的适用性可能比模型规模更重要，BERT在准确性和效率方面提供了最佳平衡，为希望的计算分析提供了实用框架。

Abstract: This paper presents a transformer-based approach for classifying hope expressions in text. We developed and compared three architectures (BERT, GPT-2, and DeBERTa) for both binary classification (Hope vs. Not Hope) and multiclass categorization (five hope-related categories). Our initial BERT implementation achieved 83.65% binary and 74.87% multiclass accuracy. In the extended comparison, BERT demonstrated superior performance (84.49% binary, 72.03% multiclass accuracy) while requiring significantly fewer computational resources (443s vs. 704s training time) than newer architectures. GPT-2 showed lowest overall accuracy (79.34% binary, 71.29% multiclass), while DeBERTa achieved moderate results (80.70% binary, 71.56% multiclass) but at substantially higher computational cost (947s for multiclass training). Error analysis revealed architecture-specific strengths in detecting nuanced hope expressions, with GPT-2 excelling at sarcasm detection (92.46% recall). This study provides a framework for computational analysis of hope, with applications in mental health and social media analysis, while demonstrating that architectural suitability may outweigh model size for specialized emotion detection tasks.

</details>


### [56] [Auditing Google's AI Overviews and Featured Snippets: A Case Study on Baby Care and Pregnancy](https://arxiv.org/abs/2511.12920)
*Desheng Hu,Joachim Baumann,Aleksandra Urman,Elsa Lichtenegger,Robin Forsberg,Aniko Hannak,Christo Wilson*

Main category: cs.CL

TL;DR: 通过系统算法审计发现，Google搜索的AI概览和精选摘要功能在健康信息展示中存在严重质量问题，包括33%的信息不一致性、医疗安全措施严重缺失等问题。


<details>
  <summary>Details</summary>
Motivation: 评估Google搜索AI生成内容（AI概览和精选摘要）在婴儿护理和孕期相关查询中的信息质量和一致性，因为这些功能用户依赖度高但缺乏控制权。

Method: 对1,508个真实婴儿护理和孕期相关查询进行系统算法审计，使用多维度评估框架分析答案一致性、相关性、医疗安全措施、来源类别和情感对齐。

Result: AI概览和精选摘要在同一搜索结果页面上有33%的信息不一致；医疗安全措施严重缺失（AI概览仅11%，精选摘要仅7%）；健康网站是主要来源，但精选摘要也常链接商业来源。

Conclusion: AI介导的健康信息需要更强的质量控制，该审计方法为高风险领域AI系统评估提供了可转移的框架。

Abstract: Google Search increasingly surfaces AI-generated content through features like AI Overviews (AIO) and Featured Snippets (FS), which users frequently rely on despite having no control over their presentation. Through a systematic algorithm audit of 1,508 real baby care and pregnancy-related queries, we evaluate the quality and consistency of these information displays. Our robust evaluation framework assesses multiple quality dimensions, including answer consistency, relevance, presence of medical safeguards, source categories, and sentiment alignment. Our results reveal concerning gaps in information consistency, with information in AIO and FS displayed on the same search result page being inconsistent with each other in 33% of cases. Despite high relevance scores, both features critically lack medical safeguards (present in just 11% of AIO and 7% of FS responses). While health and wellness websites dominate source categories for both, AIO and FS, FS also often link to commercial sources. These findings have important implications for public health information access and demonstrate the need for stronger quality controls in AI-mediated health information. Our methodology provides a transferable framework for auditing AI systems across high-stakes domains where information quality directly impacts user well-being.

</details>


### [57] [Visual Room 2.0: Seeing is Not Understanding for MLLMs](https://arxiv.org/abs/2511.12928)
*Haokun Li,Yazhou Zhang,Jizhi Ding,Qiuchi Li,Peng Zhang*

Main category: cs.CL

TL;DR: 该论文提出了视觉室论证，认为多模态大语言模型(MLLMs)可能精确描述视觉细节但无法理解底层情感和意图，即看见不等于理解。作者构建了Visual Room 2.0基准来评估MLLMs的感知-认知对齐，包含17个任务和350个多模态样本。


<details>
  <summary>Details</summary>
Motivation: 基于Searle的中文屋思想扩展到多模态领域，质疑MLLMs是否真正理解所见内容，探索感知与认知之间的差距。

Method: 构建了分层基准Visual Room 2.0，模拟人类感知和认知过程的三个层次（低、中、高），涵盖17个代表性任务，包含350个多模态样本，每个样本有6个渐进式问题。

Result: 评估10个最先进的MLLMs发现：(1) MLLMs的感知能力优于认知能力(8.0%↑)；(2) 认知似乎不因果依赖于基于感知的推理；(3) 认知随模型规模扩展，但感知不随更大变体一致提升。

Conclusion: 该工作将"看见≠理解"作为可检验假设操作化，为MLLMs从感知处理到认知推理提供了新范式，验证了感知与认知之间的实质性差距。

Abstract: Can multi-modal large language models (MLLMs) truly understand what they can see? Extending Searle's Chinese Room into the multi-modal domain, this paper proposes the Visual Room argument: MLLMs may describe every visual detail precisely yet fail to comprehend the underlying emotions and intentions, namely seeing is not understanding. Building on this, we introduce \textit{Visual Room} 2.0, a hierarchical benchmark for evaluating perception-cognition alignment of MLLMs. We model human perceptive and cognitive processes across three levels: low, middle, and high, covering 17 representative tasks. The perception component ranges from attribute recognition to scene understanding, while the cognition component extends from textual entailment to causal and social reasoning. The dataset contains 350 multi-modal samples, each with six progressive questions (2,100 in total) spanning perception to cognition. Evaluating 10 state-of-the-art (SoTA) MLLMs, we highlight three key findings: (1) MLLMs exhibit stronger perceptual competence than cognitive ability (8.0\%$\uparrow$); (2) cognition appears not causally dependent on perception-based reasoning; and (3) cognition scales with model size, but perception does not consistently improve with larger variants. This work operationalizes Seeing $\ne$ Understanding as a testable hypothesis, offering a new paradigm from perceptual processing to cognitive reasoning in MLLMs. Our dataset is available at https://huggingface.co/datasets/LHK2003/PCBench.

</details>


### [58] [Fine-Tuned LLMs Know They Don't Know: A Parameter-Efficient Approach to Recovering Honesty](https://arxiv.org/abs/2511.12991)
*Zeyu Shi,Ziming Wang,Tianyu Chen,Shiqi Gao,Haoyi Zhou,Qingyun Sun,Jianxin Li*

Main category: cs.CL

TL;DR: HCNR方法通过识别和恢复关键表达神经元来修复SFT后LLM的诚实度，相比基线方法在数据使用和速度上都有显著优势。


<details>
  <summary>Details</summary>
Motivation: 监督微调(SFT)会严重损害LLM的诚实度，这对高风险领域的部署构成威胁。现有恢复方法假设SFT深度破坏了模型识别知识边界的能力，但作者观察到模型仍保留这种能力，只是表达这种意识的能力受损。

Method: 提出Honesty-Critical Neurons Restoration (HCNR)方法，通过识别和恢复关键表达控制神经元到预训练状态，同时使用Hessian引导的补偿来协调任务导向神经元。

Result: 在四个QA任务和五个LLM家族上的实验表明，HCNR有效恢复了33.25%受损的诚实度，相比基线方法实现了至少2.23倍加速和超过10倍的数据减少。

Conclusion: HCNR为可信赖LLM部署提供了一个实用的解决方案，能够有效恢复模型诚实度同时保持效率。

Abstract: The honesty of Large Language Models (LLMs) is increasingly important for safe deployment in high-stakes domains. However, this crucial trait is severely undermined by supervised fine-tuning (SFT), a common technique for model specialization. Existing recovery methods rely on data-intensive global parameter adjustments, implicitly assuming that SFT deeply corrupts the models' ability to recognize their knowledge boundaries. However, we observe that fine-tuned LLMs still preserve this ability; what is damaged is their capacity to faithfully express that awareness. Building on this, we propose Honesty-Critical Neurons Restoration (HCNR) to surgically repair this suppressed capacity. HCNR identifies and restores key expression-governing neurons to their pre-trained state while harmonizing them with task-oriented neurons via Hessian-guided compensation. Experiments on four QA tasks and five LLM families demonstrate that HCNR effectively recovers 33.25% of the compromised honesty while achieving at least 2.23x speedup with over 10x less data compared to baseline methods, offering a practical solution for trustworthy LLM deployment.

</details>


### [59] [AA-Omniscience: Evaluating Cross-Domain Knowledge Reliability in Large Language Models](https://arxiv.org/abs/2511.13029)
*Declan Jackson,William Keating,George Cameron,Micah Hill-Smith*

Main category: cs.CL

TL;DR: AA-Omniscience基准测试评估语言模型的事实回忆能力和知识校准能力，发现前沿模型在事实性和校准方面存在持续弱点，Claude 4.1 Opus表现最佳但得分仅4.8。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型评估主要衡量通用能力，但可靠使用需要事实准确性和识别知识差距的能力。

Method: 构建包含6,000个问题的基准，问题来自权威学术和行业来源，涵盖6个领域42个经济相关主题，测量全知指数(-100到100)。

Result: Claude 4.1 Opus得分最高(4.8)，是仅有的三个得分超过0的模型之一。不同领域表现差异显著，三个不同研究实验室的模型在六个领域中各领先。

Conclusion: 模型在知识重要任务中应根据用例需求选择，而非依赖通用性能指标，因为不同模型在不同领域表现各异。

Abstract: Existing language model evaluations primarily measure general capabilities, yet reliable use of these models across a range of domains demands factual accuracy and recognition of knowledge gaps. We introduce AA-Omniscience, a benchmark designed to measure both factual recall and knowledge calibration across 6,000 questions. Questions are derived from authoritative academic and industry sources, and cover 42 economically relevant topics within six different domains. The evaluation measures a model's Omniscience Index, a bounded metric (-100 to 100) measuring factual recall that jointly penalizes hallucinations and rewards abstention when uncertain, with 0 equating to a model that answers questions correctly as much as it does incorrectly. Among evaluated models, Claude 4.1 Opus attains the highest score (4.8), making it one of only three models to score above zero. These results reveal persistent factuality and calibration weaknesses across frontier models. Performance also varies by domain, with the models from three different research labs leading across the six domains. This performance variability suggests models should be chosen according to the demands of the use case rather than general performance for tasks where knowledge is important.

</details>


### [60] [How Good is BLI as an Alignment Measure: A Study in Word Embedding Paradigm](https://arxiv.org/abs/2511.13040)
*Kasun Wickramasinghe,Nisansa de Silva*

Main category: cs.CL

TL;DR: 本研究探讨了多语言嵌入模型与对齐单语模型在双语词典归纳(BLI)任务中的表现差异，分析了BLI作为嵌入空间对齐度评估指标的局限性，并提出了基于词干的新BLI方法和词汇剪枝技术。


<details>
  <summary>Details</summary>
Motivation: 随着多语言嵌入模型成为主流，需要评估其是否在所有方面都优于对齐的单语模型，以及高昂的计算成本是否总是合理。研究旨在探索BLI作为对齐度评估指标的局限性，并比较不同嵌入对齐技术在高低资源语言中的表现。

Method: 使用双语词典归纳(BLI)作为评估指标，比较传统嵌入对齐技术、新型多语言模型和组合对齐技术的表现。提出了基于词干的BLI方法和词汇剪枝技术，分析了语言家族对结果的影响。

Result: 发现BLI在某些情况下不能准确衡量对齐度，组合嵌入对齐技术通常表现更好，但在低资源语言情况下多语言嵌入表现更优。提出的新方法能更准确地评估嵌入空间对齐程度。

Conclusion: 多语言嵌入模型与对齐单语模型各有优势，没有绝对的胜利者。BLI作为评估指标存在局限性，需要更全面的评估方法。在低资源语言场景下，多语言嵌入模型可能更具优势。

Abstract: Sans a dwindling number of monolingual embedding studies originating predominantly from the low-resource domains, it is evident that multilingual embedding has become the de facto choice due to its adaptability to the usage of code-mixed languages, granting the ability to process multilingual documents in a language-agnostic manner, as well as removing the difficult task of aligning monolingual embeddings. But is this victory complete? Are the multilingual models better than aligned monolingual models in every aspect? Can the higher computational cost of multilingual models always be justified? Or is there a compromise between the two extremes? Bilingual Lexicon Induction is one of the most widely used metrics in terms of evaluating the degree of alignment between two embedding spaces. In this study, we explore the strengths and limitations of BLI as a measure to evaluate the degree of alignment of two embedding spaces. Further, we evaluate how well traditional embedding alignment techniques, novel multilingual models, and combined alignment techniques perform BLI tasks in the contexts of both high-resource and low-resource languages. In addition to that, we investigate the impact of the language families to which the pairs of languages belong. We identify that BLI does not measure the true degree of alignment in some cases and we propose solutions for them. We propose a novel stem-based BLI approach to evaluate two aligned embedding spaces that take into account the inflected nature of languages as opposed to the prevalent word-based BLI techniques. Further, we introduce a vocabulary pruning technique that is more informative in showing the degree of the alignment, especially performing BLI on multilingual embedding models. Often, combined embedding alignment techniques perform better while in certain cases multilingual embeddings perform better (mainly low-resource language cases).

</details>


### [61] [Spark-Prover-X1: Formal Theorem Proving Through Diverse Data Training](https://arxiv.org/abs/2511.13043)
*Xinyuan Zhou,Yi Lei,Xiaoyu Zhou,Jingyi Sun,Yu Zhu,Zhongyi Ye,Weitai Zhang,Quan Liu,Si Wei,Cong Liu*

Main category: cs.CL

TL;DR: Spark-Prover-X1是一个7B参数的定理证明模型，通过三阶段训练框架提升轻量级LLM的形式推理能力，在多个基准测试中达到同类开源模型的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在自动定理证明中因缺乏多样化和高质量形式语言数据而受限的问题，探索如何有效提升中等规模LLM的推理潜力。

Method: 采用三阶段训练框架：1) 在广泛数学语料上进行持续预训练，引入"思维链增强状态预测"任务；2) 在专家迭代循环中进行监督微调；3) 使用组相对策略优化针对最具挑战性问题进行强化训练。

Result: Spark-Prover-X1-7B在同类开源模型中达到最先进性能，平均通过率37.0%(pass@32)，在PutnamBench上解决27个问题，在CombiBench上达到24.0%。

Conclusion: 多样化的训练数据和逐步精炼的训练流程为增强轻量级LLM的形式推理能力提供了有效路径，相关模型和数据集已开源。

Abstract: Large Language Models (LLMs) have shown significant promise in automated theorem proving, yet progress is often constrained by the scarcity of diverse and high-quality formal language data. To address this issue, we introduce Spark-Prover-X1, a 7B parameter model trained via an three-stage framework designed to unlock the reasoning potential of more accessible and moderately-sized LLMs. The first stage infuses deep knowledge through continuous pre-training on a broad mathematical corpus, enhanced by a suite of novel data tasks. Key innovation is a "CoT-augmented state prediction" task to achieve fine-grained reasoning. The second stage employs Supervised Fine-tuning (SFT) within an expert iteration loop to specialize both the Spark-Prover-X1-7B and Spark-Formalizer-X1-7B models. Finally, a targeted round of Group Relative Policy Optimization (GRPO) is applied to sharpen the prover's capabilities on the most challenging problems. To facilitate robust evaluation, particularly on problems from real-world examinations, we also introduce ExamFormal-Bench, a new benchmark dataset of 402 formal problems. Experimental results demonstrate that Spark-Prover-X1-7B achieves state-of-the-art performance among similarly-sized open-source models, attaining a 37.0\% average pass rate (pass@32). It shows exceptional performance on difficult competition benchmarks, notably solving 27 problems on PutnamBench (pass@32) and achieving 24.0\% on CombiBench (pass@32). Our work validates that this diverse training data and progressively refined training pipeline provides an effective path for enhancing the formal reasoning capabilities of lightweight LLMs. Both Spark-Prover-X1-7B and Spark-Formalizer-X1-7B, along with the ExamFormal-Bench dataset, are made publicly available at:https://www.modelscope.cn/organization/iflytek, https://gitcode.com/ifly_opensource.

</details>


### [62] [BeDiscovER: The Benchmark of Discourse Understanding in the Era of Reasoning Language Models](https://arxiv.org/abs/2511.13095)
*Chuyuan Li,Giuseppe Carenini*

Main category: cs.CL

TL;DR: BeDiscovER是一个评估现代大语言模型语篇理解能力的综合基准套件，包含5个公开语篇任务共52个数据集，涵盖词汇、句子和文档层面的语篇分析。评估发现前沿模型在时间推理的算术方面表现强劲，但在完整文档推理和某些微妙语义语篇现象上仍有困难。


<details>
  <summary>Details</summary>
Motivation: 为了全面评估现代推理大语言模型在语篇层面的知识理解能力，需要建立一个包含多种语篇任务的最新基准测试套件。

Method: 构建BeDiscovER基准套件，整合5个公开语篇任务（语篇词汇、句子级、文档级）共52个数据集，包括语篇解析、时间关系提取、语篇粒子消歧等任务，并对开源和前沿LLMs进行评估。

Result: 评估显示：最先进模型在时间推理的算术方面表现强劲，但在完整文档推理和某些微妙语义语篇现象（如修辞关系识别）上存在困难。

Conclusion: 现代LLMs在语篇理解方面取得了显著进展，但在处理完整文档推理和复杂语义语篇现象方面仍需改进，BeDiscovER为系统评估语篇理解能力提供了有效工具。

Abstract: We introduce BeDiscovER (Benchmark of Discourse Understanding in the Era of Reasoning Language Models), an up-to-date, comprehensive suite for evaluating the discourse-level knowledge of modern LLMs. BeDiscovER compiles 5 publicly available discourse tasks across discourse lexicon, (multi-)sentential, and documental levels, with in total 52 individual datasets. It covers both extensively studied tasks such as discourse parsing and temporal relation extraction, as well as some novel challenges such as discourse particle disambiguation (e.g., ``just''), and also aggregates a shared task on Discourse Relation Parsing and Treebanking for multilingual and multi-framework discourse relation classification. We evaluate open-source LLMs: Qwen3 series, DeepSeek-R1, and frontier model such as GPT-5-mini on BeDiscovER, and find that state-of-the-art models exhibit strong performance in arithmetic aspect of temporal reasoning, but they struggle with full document reasoning and some subtle semantic and discourse phenomena, such as rhetorical relation recognition.

</details>


### [63] [Evaluating the Ability of Large Language Models to Identify Adherence to CONSORT Reporting Guidelines in Randomized Controlled Trials: A Methodological Evaluation Study](https://arxiv.org/abs/2511.13107)
*Zhichao He,Mouxiao Bian,Jianhong Zhu,Jiayuan Chen,Yunqiu Wang,Wenxia Zhao,Tianbin Li,Bing Han,Jie Xu,Junyan Wu*

Main category: cs.CL

TL;DR: 本研究系统评估了当代大语言模型在零样本设置下识别已发表随机对照试验对CONSORT 2010声明依从性的准确性。结果显示模型整体表现一般，能够准确识别合规项目，但在检测不合规和不适用项目方面表现较差，目前尚不能替代人类专家进行试验质量评估。


<details>
  <summary>Details</summary>
Motivation: 手动验证CONSORT依从性是一个耗时费力的过程，成为同行评审和证据合成的瓶颈。本研究旨在评估LLMs在这一任务中的表现，以探索自动化验证的可能性。

Method: 构建了包含150篇已发表RCT的金标准数据集，在零样本设置下评估LLMs的性能，主要指标为宏平均F1分数，并辅以项目级性能指标和定性错误分析。

Result: 表现最佳的Gemini-2.5-Flash和DeepSeek-R1模型宏F1分数分别为0.634，Cohen's Kappa系数分别为0.280和0.282，仅达到一般一致性水平。模型在识别合规项目时表现良好（F1>0.850），但在识别不合规和不适用项目时表现较差（F1<0.400）。

Conclusion: LLMs作为CONSORT检查的初步筛选助手具有潜力，能够有效识别报告良好的项目，但目前无法可靠检测报告遗漏或方法学缺陷，尚不能替代人类专家进行试验质量的关键评估。

Abstract: The Consolidated Standards of Reporting Trials statement is the global benchmark for transparent and high-quality reporting of randomized controlled trials. Manual verification of CONSORT adherence is a laborious, time-intensive process that constitutes a significant bottleneck in peer review and evidence synthesis. This study aimed to systematically evaluate the accuracy and reliability of contemporary LLMs in identifying the adherence of published RCTs to the CONSORT 2010 statement under a zero-shot setting. We constructed a golden standard dataset of 150 published RCTs spanning diverse medical specialties. The primary outcome was the macro-averaged F1-score for the three-class classification task, supplemented by item-wise performance metrics and qualitative error analysis. Overall model performance was modest. The top-performing models, Gemini-2.5-Flash and DeepSeek-R1, achieved nearly identical macro F1 scores of 0.634 and Cohen's Kappa coefficients of 0.280 and 0.282, respectively, indicating only fair agreement with expert consensus. A striking performance disparity was observed across classes: while most models could identify compliant items with high accuracy (F1 score > 0.850), they struggled profoundly with identifying non-compliant and not applicable items, where F1 scores rarely exceeded 0.400. Notably, some high-profile models like GPT-4o underperformed, achieving a macro F1-score of only 0.521. LLMs show potential as preliminary screening assistants for CONSORT checks, capably identifying well-reported items. However, their current inability to reliably detect reporting omissions or methodological flaws makes them unsuitable for replacing human expertise in the critical appraisal of trial quality.

</details>


### [64] [Extracting Events Like Code: A Multi-Agent Programming Framework for Zero-Shot Event Extraction](https://arxiv.org/abs/2511.13118)
*Quanjiang Guo,Sijie Wang,Jinchuan Zhang,Ben Zhang,Zhao Kang,Ling Tian,Ke Yan*

Main category: cs.CL

TL;DR: Agent-Event-Coder (AEC) 是一个多智能体框架，将零样本事件抽取视为类似软件工程的代码生成过程，通过检索、规划、编码和验证四个专业智能体的协作，显著提升了大型语言模型在零样本事件抽取任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 零样本事件抽取对大型语言模型具有挑战性，直接提示通常会产生不完整或结构无效的输出，如错误分类的触发器、缺失参数和模式违规。

Method: AEC 将事件抽取分解为四个专业子任务：检索、规划、编码和验证，每个任务由专门的LLM智能体处理。事件模式表示为可执行的类定义，通过验证智能体实现确定性验证和精确反馈。

Result: 在五个不同领域和六个LLM上的实验表明，AEC 始终优于先前的零样本基线方法。

Conclusion: 将事件抽取视为代码生成的方法能够使LLM在零样本设置下产生精确、完整且模式一致的事件抽取结果。

Abstract: Zero-shot event extraction (ZSEE) remains a significant challenge for large language models (LLMs) due to the need for complex reasoning and domain-specific understanding. Direct prompting often yields incomplete or structurally invalid outputs--such as misclassified triggers, missing arguments, and schema violations. To address these limitations, we present Agent-Event-Coder (AEC), a novel multi-agent framework that treats event extraction like software engineering: as a structured, iterative code-generation process. AEC decomposes ZSEE into specialized subtasks--retrieval, planning, coding, and verification--each handled by a dedicated LLM agent. Event schemas are represented as executable class definitions, enabling deterministic validation and precise feedback via a verification agent. This programming-inspired approach allows for systematic disambiguation and schema enforcement through iterative refinement. By leveraging collaborative agent workflows, AEC enables LLMs to produce precise, complete, and schema-consistent extractions in zero-shot settings. Experiments across five diverse domains and six LLMs demonstrate that AEC consistently outperforms prior zero-shot baselines, showcasing the power of treating event extraction like code generation. The code and data are released on https://github.com/UESTC-GQJ/Agent-Event-Coder.

</details>


### [65] [A Comparative Analysis of Recurrent and Attention Architectures for Isolated Sign Language Recognition](https://arxiv.org/abs/2511.13126)
*Nigar Alishzade,Gulchin Abdullayeva*

Main category: cs.CL

TL;DR: 比较循环神经网络和注意力机制在孤立手语识别中的表现，发现基于注意力的Transformer在准确率上优于ConvLSTM，但后者计算效率更高。


<details>
  <summary>Details</summary>
Motivation: 系统比较循环神经网络和注意力机制在手语识别中的性能差异，为不同应用场景下的架构选择提供指导。

Method: 在阿塞拜疆手语数据集(AzSLD)和美国手语数据集(WLASL)上实现并评估ConvLSTM和Vanilla Transformer两种代表性模型。

Result: Vanilla Transformer在两个数据集上都优于ConvLSTM，在AzSLD上达到76.8%的Top-1准确率，在WLASL上达到88.3%。ConvLSTM计算效率更高但准确率较低。

Conclusion: Transformer在准确率和手语者独立性方面表现更好，而ConvLSTM在计算效率和时序建模方面有优势，两者各有适用场景。

Abstract: This study presents a systematic comparative analysis of recurrent and attention-based neural architectures for isolated sign language recognition. We implement and evaluate two representative models-ConvLSTM and Vanilla Transformer-on the Azerbaijani Sign Language Dataset (AzSLD) and the Word-Level American Sign Language (WLASL) dataset. Our results demonstrate that the attention-based Vanilla Transformer consistently outperforms the recurrent ConvLSTM in both Top-1 and Top-5 accuracy across datasets, achieving up to 76.8% Top-1 accuracy on AzSLD and 88.3% on WLASL. The ConvLSTM, while more computationally efficient, lags in recognition accuracy, particularly on smaller datasets. These findings highlight the complementary strengths of each paradigm: the Transformer excels in overall accuracy and signer independence, whereas the ConvLSTM offers advantages in computational efficiency and temporal modeling. The study provides a nuanced analysis of these trade-offs, offering guidance for architecture selection in sign language recognition systems depending on application requirements and resource constraints.

</details>


### [66] [Zero-Shot Grammar Competency Estimation Using Large Language Model Generated Pseudo Labels](https://arxiv.org/abs/2511.13152)
*Sourya Dipta Das,Shubham Kumar,Kuldeep Yadav*

Main category: cs.CL

TL;DR: 提出一种零样本语法能力评估框架，利用无标签数据和大型语言模型生成伪标签，通过噪声标签训练方法实现准确的语法能力评分。


<details>
  <summary>Details</summary>
Motivation: 口语语法评估面临自发性和不流畅性的挑战，且专家标注成本高昂，难以大规模创建数据。

Method: 使用基于语法能力量表的提示从LLM生成伪标签，通过专门设计的训练框架训练基于transformer的模型来处理标签噪声。

Result: 实验结果表明该方法能高精度估计语法能力分数，LLM选择和干净-噪声样本比例对性能有重要影响。

Conclusion: 该方法为可扩展、低资源的语法评估系统铺平了道路，具有鲁棒性和可解释性。

Abstract: Grammar competency estimation is essential for assessing linguistic proficiency in both written and spoken language; however, the spoken modality presents additional challenges due to its spontaneous, unstructured, and disfluent nature. Developing accurate grammar scoring models further requires extensive expert annotation, making large-scale data creation impractical. To address these limitations, we propose a zero-shot grammar competency estimation framework that leverages unlabeled data and Large Language Models (LLMs) without relying on manual labels. During training, we employ LLM-generated predictions on unlabeled data by using grammar competency rubric-based prompts. These predictions, treated as pseudo labels, are utilized to train a transformer-based model through a novel training framework designed to handle label noise effectively. We show that the choice of LLM for pseudo-label generation critically affects model performance and that the ratio of clean-to-noisy samples during training strongly influences stability and accuracy. Finally, a qualitative analysis of error intensity and score prediction confirms the robustness and interpretability of our approach. Experimental results demonstrate the efficacy of our approach in estimating grammar competency scores with high accuracy, paving the way for scalable, low-resource grammar assessment systems.

</details>


### [67] [Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis](https://arxiv.org/abs/2511.13159)
*Zaara Zabeen Arpa,Sadnam Sakib Apurbo,Nazia Karim Khan Oishee,Ajwad Abrar*

Main category: cs.CL

TL;DR: 本文构建了首个公开的孟加拉语语料库，用于区分ASR转录中的重复性不流畅和形态学重叠现象，并通过LLM和微调方法建立了基准性能。


<details>
  <summary>Details</summary>
Motivation: 解决孟加拉语ASR转录中词重复的歧义问题：区分无意的重复性不流畅和故意的形态学重叠，避免标准不流畅修正方法误删有效语言信息。

Method: 创建了2万行手动标注的孟加拉语语料库，使用两种方法进行基准测试：多语言大语言模型的少样本提示和任务特定的编码器模型微调。

Result: LLM在少样本提示下达到82.68%准确率，但微调方法更优，BanglaBERT模型达到最高84.78%准确率和0.677 F1分数。

Conclusion: 建立了强大的语言学基准，为开发语义保留的孟加拉语文本规范化系统提供了重要数据支持。

Abstract: Automatic Speech Recognition (ASR) transcripts, especially in low-resource languages like Bangla, contain a critical ambiguity: word-word repetitions can be either Repetition Disfluency (unintentional ASR error/hesitation) or Morphological Reduplication (a deliberate grammatical construct). Standard disfluency correction fails by erroneously deleting valid linguistic information. To solve this, we introduce the first publicly available, 20,000-row Bangla corpus, manually annotated to explicitly distinguish between these two phenomena in noisy ASR transcripts. We benchmark this novel resource using two paradigms: state-of-the-art multilingual Large Language Models (LLMs) and task-specific fine-tuning of encoder models. LLMs achieve competitive performance (up to 82.68\% accuracy) with few-shot prompting. However, fine-tuning proves superior, with the language-specific BanglaBERT model achieving the highest accuracy of 84.78\% and an F1 score of 0.677. This establishes a strong, linguistically-informed baseline and provides essential data for developing sophisticated, semantic-preserving text normalization systems for Bangla.

</details>


### [68] [TCM-5CEval: Extended Deep Evaluation Benchmark for LLM's Comprehensive Clinical Research Competence in Traditional Chinese Medicine](https://arxiv.org/abs/2511.13169)
*Tianai Huang,Jiayuan Chen,Lu Lu,Pengcheng Chen,Tianbin Li,Bing Han,Wenchao Tang,Jie Xu,Ming Li*

Main category: cs.CL

TL;DR: TCM-5CEval是一个更细粒度的中医领域大语言模型评估基准，涵盖5个关键维度，评估了15个主流模型，发现模型在基础知识方面表现良好，但在经典文本解释和推理稳定性方面存在显著缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在中医等专业文化领域的应用需要严谨评估，基于前期TCM-3CEval工作发现的知识差距和文化背景对齐问题，需要更全面细粒度的评估基准。

Method: 开发TCM-5CEval基准，包含5个维度：核心知识、经典素养、临床决策、中药学和临床非药物疗法，对15个主流LLMs进行全面评估，并采用排列一致性测试检验推理稳定性。

Result: 评估显示模型性能差异显著，deepseek_r1和gemini_2_5_pro表现最佳。模型在基础知识回忆方面熟练，但在经典文本解释方面困难。排列测试发现所有模型都存在位置偏见敏感性，推理稳定性普遍不足。

Conclusion: TCM-5CEval提供了更详细的中医LLM能力诊断工具，揭示了模型推理稳定性的根本弱点，基准已上传至Medbench平台促进标准化比较。

Abstract: Large language models (LLMs) have demonstrated exceptional capabilities in general domains, yet their application in highly specialized and culturally-rich fields like Traditional Chinese Medicine (TCM) requires rigorous and nuanced evaluation. Building upon prior foundational work such as TCM-3CEval, which highlighted systemic knowledge gaps and the importance of cultural-contextual alignment, we introduce TCM-5CEval, a more granular and comprehensive benchmark. TCM-5CEval is designed to assess LLMs across five critical dimensions: (1) Core Knowledge (TCM-Exam), (2) Classical Literacy (TCM-LitQA), (3) Clinical Decision-making (TCM-MRCD), (4) Chinese Materia Medica (TCM-CMM), and (5) Clinical Non-pharmacological Therapy (TCM-ClinNPT). We conducted a thorough evaluation of fifteen prominent LLMs, revealing significant performance disparities and identifying top-performing models like deepseek\_r1 and gemini\_2\_5\_pro. Our findings show that while models exhibit proficiency in recalling foundational knowledge, they struggle with the interpretative complexities of classical texts. Critically, permutation-based consistency testing reveals widespread fragilities in model inference. All evaluated models, including the highest-scoring ones, displayed a substantial performance degradation when faced with varied question option ordering, indicating a pervasive sensitivity to positional bias and a lack of robust understanding. TCM-5CEval not only provides a more detailed diagnostic tool for LLM capabilities in TCM but aldso exposes fundamental weaknesses in their reasoning stability. To promote further research and standardized comparison, TCM-5CEval has been uploaded to the Medbench platform, joining its predecessor in the "In-depth Challenge for Comprehensive TCM Abilities" special track.

</details>


### [69] [Translation Entropy: A Statistical Framework for Evaluating Translation Systems](https://arxiv.org/abs/2511.13180)
*Ronit D. Gross,Yanir Harel,Ido Kanter*

Main category: cs.CL

TL;DR: 本文提出了一种量化评估翻译器性能的方法，通过分析翻译熵来衡量翻译器的表现。研究发现翻译熵可以客观比较不同翻译器，并揭示了翻译退化的乘法效应。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏客观量化方法来评估基于编码器-解码器架构的翻译器性能，主要是因为单语言的熵值未知。

Method: 通过分析仅改变一个选定标记的多个句子产生相同翻译的统计现象，计算特定标记的替换概率，从而估计翻译熵。

Result: 该方法能够量化排名不同公开翻译器，发现翻译退化与标记退化的乘积成正比，翻译熵在解码器块中得到增强。

Conclusion: 翻译熵是一个可测量的属性，为人工翻译器提供了客观的基准测试标准。

Abstract: The translation of written language has been known since the 3rd century BC; however, its necessity has become increasingly common in the information age. Today, many translators exist, based on encoder-decoder deep architectures, nevertheless, no quantitative objective methods are available to assess their performance, likely because the entropy of even a single language remains unknown. This study presents a quantitative method for estimating translation entropy, with the following key finding. Given a translator, several sentences that differ by only one selected token of a given pivot sentence yield identical translations. Analyzing the statistics of this phenomenon across an ensemble of such sentences, consisting each of a pivot selected token, yields the probabilities of replacing this specific token with others while preserving the translation. These probabilities constitute the entropy of the selected token, and the average across all selected pivot tokens provides an estimate of the translator's overall translation entropy, which is enhanced along the decoder blocks. This entropic measure allows for the quantitative ranking of several publicly available translators and reveals whether mutual translation entropy is symmetric. Extending the proposed method to include the replacement of two tokens in a given pivot sentence demonstrates a multiplicative effect, where translation degeneracy is proportional to the product of the degeneracies of the two tokens. These findings establish translation entropy as a measurable property and objective benchmarking of artificial translators. Results are based on MarianMT, T5-Base and NLLB-200 translators.

</details>


### [70] [Evaluating Large Language Models for Diacritic Restoration in Romanian Texts: A Comparative Study](https://arxiv.org/abs/2511.13182)
*Mihai Dan Nadas,Laura Diosan*

Main category: cs.CL

TL;DR: 评估多种大语言模型在罗马尼亚语变音符号恢复任务中的表现，发现GPT-4o等模型表现优异，而Llama系列模型表现波动较大。


<details>
  <summary>Details</summary>
Motivation: 自动变音符号恢复对于罗马尼亚语等富含变音符号的语言的文本处理至关重要，需要评估当前主流大语言模型在此任务上的性能。

Method: 使用综合语料库测试了包括GPT-3.5、GPT-4、GPT-4o、Gemini 1.0 Pro、Llama 2/3、Mixtral 8x7B等在内的多种LLM，采用从零样本到复杂多样本指令的多种提示模板。

Result: GPT-4o等模型实现了高精度的变音符号恢复，始终优于中性回显基线，而Meta的Llama系列模型表现出更大的变异性。

Conclusion: 模型架构、训练数据和提示设计对变音符号恢复性能有显著影响，为改进富含变音符号语言的NLP工具指明了有前景的方向。

Abstract: Automatic diacritic restoration is crucial for text processing in languages with rich diacritical marks, such as Romanian. This study evaluates the performance of several large language models (LLMs) in restoring diacritics in Romanian texts. Using a comprehensive corpus, we tested models including OpenAI's GPT-3.5, GPT-4, GPT-4o, Google's Gemini 1.0 Pro, Meta's Llama 2 and Llama 3, MistralAI's Mixtral 8x7B Instruct, airoboros 70B, and OpenLLM-Ro's RoLlama 2 7B, under multiple prompt templates ranging from zero-shot to complex multi-shot instructions. Results show that models such as GPT-4o achieve high diacritic restoration accuracy, consistently surpassing a neutral echo baseline, while others, including Meta's Llama family, exhibit wider variability. These findings highlight the impact of model architecture, training data, and prompt design on diacritic restoration performance and outline promising directions for improving NLP tools for diacritic-rich languages.

</details>


### [71] [Seeing isn't Hearing: Benchmarking Vision Language Models at Interpreting Spectrograms](https://arxiv.org/abs/2511.13225)
*Tyler Loakman,Joseph James,Chenghua Lin*

Main category: cs.CL

TL;DR: 本文评估了视觉语言模型（VLMs）作为语音学家的能力，测试其从语音频谱图和波形中识别英语单词的能力。研究发现，无论是零样本还是微调后的模型，其表现都很少超过随机水平，表明需要特定的参数知识来解释语音图像。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）及其视觉版本（VLMs）的发展，研究者希望了解这些模型在多模态任务中的能力，特别是它们是否能像专业语音学家一样解读语音的频谱图和波形。

Method: 创建了一个包含4000多个孤立英语单词的数据集，并生成相应的频谱图和波形图。通过多项选择任务测试VLMs的能力，模型需要从4个选项（1个正确答案和3个基于音素编辑距离选择的干扰项）中识别正确的音素或字素转录。

Result: 无论是零样本还是经过微调的模型，在识别语音图像的任务中表现都很差，很少超过随机猜测的水平。

Conclusion: 仅靠配对样本不足以让VLMs正确解读语音图像，需要特定的参数知识来理解这些表示形式。

Abstract: With the rise of Large Language Models (LLMs) and their vision-enabled counterparts (VLMs), numerous works have investigated their capabilities in tasks that fuse the modalities of vision and language. In this work, we benchmark the extent to which VLMs are able to act as highly-trained phoneticians, interpreting spectrograms and waveforms of speech. To do this, we synthesise a novel dataset containing 4k+ English words spoken in isolation alongside stylistically consistent spectrogram and waveform figures. We test the ability of VLMs to understand these representations of speech through a multiple-choice task whereby models must predict the correct phonemic or graphemic transcription of a spoken word when presented amongst 3 distractor transcriptions that have been selected based on their phonemic edit distance to the ground truth. We observe that both zero-shot and finetuned models rarely perform above chance, demonstrating the requirement for specific parametric knowledge of how to interpret such figures, rather than paired samples alone.

</details>


### [72] [Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance](https://arxiv.org/abs/2511.13254)
*Shalini Maiti,Amar Budhiraja,Bhavul Gauri,Gaurav Chaurasia,Anton Protopopov,Alexis Audran-Reiss,Michael Slater,Despoina Magka,Tatiana Shavrina,Roberta Raileanu,Yoram Bachrach*

Main category: cs.CL

TL;DR: SoCE是一种改进的模型融合方法，通过识别各基准类别中的专家模型，并使用非均匀加权平均而非均匀平均来最大化性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练资源密集且耗时，模型融合作为一种无需昂贵重新训练就能提升性能的技术备受关注。传统均匀平均方法未充分利用不同基准类别间模型性能的低相关性。

Method: 利用基准组合识别最优模型候选，为每个弱相关类别簇识别专家模型，应用非均匀加权平均而非均匀权重进行模型融合。

Result: 该方法在多个领域（包括多语言能力、工具调用和数学）提高了性能和鲁棒性，在伯克利函数调用排行榜上取得了最先进的结果。

Conclusion: SoCE通过利用基准类别间的低相关性，采用非均匀加权平均的模型融合方法，有效提升了模型性能，是一种有前景的预训练和后训练技术。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies "expert" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.

</details>


### [73] [RegionMarker: A Region-Triggered Semantic Watermarking Framework for Embedding-as-a-Service Copyright Protection](https://arxiv.org/abs/2511.13329)
*Shufan Yang,Zifeng Cheng,Zhiwei Jiang,Yafeng Yin,Cong Wang,Shiping Ge,Yuchen Fu,Qing Gu*

Main category: cs.CL

TL;DR: 提出了一种名为RegionMarker的区域触发语义水印框架，通过定义低维空间中的触发区域来为文本嵌入注入水印，提供全面的EaaS版权保护。


<details>
  <summary>Details</summary>
Motivation: 现有的EaaS水印方法只能抵抗部分攻击，无法提供全面保护，导致模型提取攻击可能造成重大经济损失。

Method: 使用秘密降维矩阵将文本嵌入投影到低维子空间，随机选择触发区域，并在整个触发区域嵌入水印，使用文本嵌入本身作为水印。

Result: 在多个数据集上的实验表明，RegionMarker能有效抵抗不同类型的攻击方法。

Conclusion: RegionMarker框架能够全面保护EaaS的版权，抵抗各种水印移除攻击。

Abstract: Embedding-as-a-Service (EaaS) is an effective and convenient deployment solution for addressing various NLP tasks. Nevertheless, recent research has shown that EaaS is vulnerable to model extraction attacks, which could lead to significant economic losses for model providers. For copyright protection, existing methods inject watermark embeddings into text embeddings and use them to detect copyright infringement. However, current watermarking methods often resist only a subset of attacks and fail to provide \textit{comprehensive} protection. To this end, we present the region-triggered semantic watermarking framework called RegionMarker, which defines trigger regions within a low-dimensional space and injects watermarks into text embeddings associated with these regions. By utilizing a secret dimensionality reduction matrix to project onto this subspace and randomly selecting trigger regions, RegionMarker makes it difficult for watermark removal attacks to evade detection. Furthermore, by embedding watermarks across the entire trigger region and using the text embedding as the watermark, RegionMarker is resilient to both paraphrasing and dimension-perturbation attacks. Extensive experiments on various datasets show that RegionMarker is effective in resisting different attack methods, thereby protecting the copyright of EaaS.

</details>


### [74] [AHaSIS: Shared Task on Sentiment Analysis for Arabic Dialects](https://arxiv.org/abs/2511.13335)
*Maram Alharbi,Salmane Chafik,Saad Ezzini,Ruslan Mitkov,Tharindu Ranasinghe,Hansi Hettiarachchi*

Main category: cs.CL

TL;DR: 阿拉伯酒店评论情感分析共享任务，使用多方言数据集（MSA、沙特、摩洛哥方言），包含538条平衡评论，最佳系统F1得分0.81。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯世界酒店业依赖客户反馈改进服务，需要先进的阿拉伯语情感分析工具，特别是针对方言的分析能力。

Method: 创建多方言数据集，将现代标准阿拉伯语的酒店评论人工翻译为沙特和摩洛哥方言，并由母语者验证翻译准确性和情感保留。

Result: 超过40个团队注册，12个提交系统，最佳系统F1得分0.81，证明了跨阿拉伯方言情感分析的可行性。

Conclusion: 该资源支持开发方言感知的NLP系统，用于客户体验分析的实际应用，同时显示了跨方言情感分析的持续挑战。

Abstract: The hospitality industry in the Arab world increasingly relies on customer feedback to shape services, driving the need for advanced Arabic sentiment analysis tools. To address this challenge, the Sentiment Analysis on Arabic Dialects in the Hospitality Domain shared task focuses on Sentiment Detection in Arabic Dialects. This task leverages a multi-dialect, manually curated dataset derived from hotel reviews originally written in Modern Standard Arabic (MSA) and translated into Saudi and Moroccan (Darija) dialects. The dataset consists of 538 sentiment-balanced reviews spanning positive, neutral, and negative categories. Translations were validated by native speakers to ensure dialectal accuracy and sentiment preservation. This resource supports the development of dialect-aware NLP systems for real-world applications in customer experience analysis. More than 40 teams have registered for the shared task, with 12 submitting systems during the evaluation phase. The top-performing system achieved an F1 score of 0.81, demonstrating the feasibility and ongoing challenges of sentiment analysis across Arabic dialects.

</details>


### [75] [Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2511.13368)
*Kajetan Dymkiewicz,Ivan Vulic,Helen Yannakoudakis,Eilam Shapira,Roi Reichart,Anna Korhonen*

Main category: cs.CL

TL;DR: 该研究通过PEFT/LoRA方法探索LLMs在任务和语言间的迁移模式，发现跨语言任务迁移通常正向，而跨任务迁移常导致性能下降，揭示了稳定的捐赠者-接收者结构。


<details>
  <summary>Details</summary>
Motivation: 理解LLMs在一个任务或语言上的改进如何影响其他任务和语言及其组合，目前仍不清楚，需要系统研究迁移模式。

Method: 采用PEFT/LoRA方法在多个开源LLM家族和规模上进行控制研究，将任务和语言作为迁移轴，在单一任务-语言源上微调模型，并评估在所有其他任务-语言目标对上的迁移效果。

Result: 发现两个一致模式：1）任务内跨语言迁移可靠正向，而跨任务迁移常导致性能下降；2）跨语言和任务存在稳定的捐赠者-接收者结构。

Conclusion: 研究结果对风险感知微调和模型专业化具有重要启示，揭示了LLMs迁移学习中的系统性模式。

Abstract: Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages and their combinations remains poorly understood. We conduct a controlled PEFT/LoRA study across multiple open-weight LLM families and sizes, treating task and language as transfer axes while conditioning on model family and size; we fine-tune each model on a single task-language source and measure transfer as the percentage-point change versus its baseline score when evaluated on all other task-language target pairs. We decompose transfer into (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language) regimes. We uncover two consistent general patterns. First, a pronounced on-task vs. off-task asymmetry: Matched-Task (Cross-Language) transfer is reliably positive, whereas off-task transfer often incurs collateral degradation. Second, a stable donor-recipient structure across languages and tasks (hub donors vs. brittle recipients). We outline implications for risk-aware fine-tuning and model specialisation.

</details>


### [76] [Can Large Language Models Function as Qualified Pediatricians? A Systematic Evaluation in Real-World Clinical Contexts](https://arxiv.org/abs/2511.13381)
*Siyu Zhu,Mouxiao Bian,Yue Xie,Yongyu Tang,Zhikang Yu,Tianbin Li,Pengcheng Chen,Bing Han,Jie Xu,Xiaoyan Dong*

Main category: cs.CL

TL;DR: PEDIASBench评估框架评估了12个大型语言模型在儿科医疗中的表现，发现最先进模型在基础知识方面表现良好，但在复杂推理、动态诊疗和人文关怀方面存在局限，目前尚不能独立进行儿科诊疗，但在决策支持等方面有潜力。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在医学领域的快速发展，需要评估它们是否能在真实临床环境中胜任儿科医生的工作。

Method: 开发了PEDIASBench系统评估框架，从基础知识应用、动态诊疗能力、医疗安全与伦理三个维度评估12个代表性模型，涵盖19个儿科亚专科和211种典型疾病。

Result: 最先进模型在基础知识方面表现良好（Qwen3-235B-A22B在执照级问题上准确率超90%），但随着任务复杂度增加性能下降约15%；在动态诊疗场景中，DeepSeek-R1在病例推理中得分最高（均值0.58），但多数模型难以适应实时患者变化；在医疗伦理安全任务中，Qwen2.5-72B表现最佳（准确率92.05%），但人文敏感性仍有限。

Conclusion: 当前儿科LLMs受限于有限的动态决策能力和不成熟的人文关怀，未来应关注多模态整合和临床反馈-模型迭代循环，以增强安全性、可解释性和人机协作。虽然当前LLMs不能独立进行儿科诊疗，但在决策支持、医学教育和患者沟通方面具有潜力。

Abstract: With the rapid rise of large language models (LLMs) in medicine, a key question is whether they can function as competent pediatricians in real-world clinical settings. We developed PEDIASBench, a systematic evaluation framework centered on a knowledge-system framework and tailored to realistic clinical environments. PEDIASBench assesses LLMs across three dimensions: application of basic knowledge, dynamic diagnosis and treatment capability, and pediatric medical safety and medical ethics. We evaluated 12 representative models released over the past two years, including GPT-4o, Qwen3-235B-A22B, and DeepSeek-V3, covering 19 pediatric subspecialties and 211 prototypical diseases. State-of-the-art models performed well on foundational knowledge, with Qwen3-235B-A22B achieving over 90% accuracy on licensing-level questions, but performance declined ~15% as task complexity increased, revealing limitations in complex reasoning. Multiple-choice assessments highlighted weaknesses in integrative reasoning and knowledge recall. In dynamic diagnosis and treatment scenarios, DeepSeek-R1 scored highest in case reasoning (mean 0.58), yet most models struggled to adapt to real-time patient changes. On pediatric medical ethics and safety tasks, Qwen2.5-72B performed best (accuracy 92.05%), though humanistic sensitivity remained limited. These findings indicate that pediatric LLMs are constrained by limited dynamic decision-making and underdeveloped humanistic care. Future development should focus on multimodal integration and a clinical feedback-model iteration loop to enhance safety, interpretability, and human-AI collaboration. While current LLMs cannot independently perform pediatric care, they hold promise for decision support, medical education, and patient communication, laying the groundwork for a safe, trustworthy, and collaborative intelligent pediatric healthcare system.

</details>


### [77] [Mem-PAL: Towards Memory-based Personalized Dialogue Assistants for Long-term User-Agent Interaction](https://arxiv.org/abs/2511.13410)
*Zhaopei Huang,Qifeng Dai,Guozheng Wu,Xiaopeng Wu,Kehan Chen,Chuan Yu,Xubin Li,Tiezheng Ge,Wenxuan Wang,Qin Jin*

Main category: cs.CL

TL;DR: 提出了PAL-Bench基准和H²Memory记忆框架，用于评估和改进面向服务的个性化对话助手在长期交互中的表现。


<details>
  <summary>Details</summary>
Motivation: 随着智能个人设备的普及，需要能够理解用户特定特征的个性化对话助手，但现有方法往往忽视长期交互的复杂性，未能捕捉用户的主观特征。

Method: 开发了多步骤的LLM合成流程生成PAL-Set数据集，并提出H²Memory层次异构记忆框架，结合检索增强生成来改进个性化响应生成。

Result: 在PAL-Bench和外部数据集上的综合实验证明了所提记忆框架的有效性。

Conclusion: PAL-Bench是首个中文多会话用户日志数据集，H²Memory框架显著提升了面向服务的个性化交互能力。

Abstract: With the rise of smart personal devices, service-oriented human-agent interactions have become increasingly prevalent. This trend highlights the need for personalized dialogue assistants that can understand user-specific traits to accurately interpret requirements and tailor responses to individual preferences. However, existing approaches often overlook the complexities of long-term interactions and fail to capture users' subjective characteristics. To address these gaps, we present PAL-Bench, a new benchmark designed to evaluate the personalization capabilities of service-oriented assistants in long-term user-agent interactions. In the absence of available real-world data, we develop a multi-step LLM-based synthesis pipeline, which is further verified and refined by human annotators. This process yields PAL-Set, the first Chinese dataset comprising multi-session user logs and dialogue histories, which serves as the foundation for PAL-Bench. Furthermore, to improve personalized service-oriented interactions, we propose H$^2$Memory, a hierarchical and heterogeneous memory framework that incorporates retrieval-augmented generation to improve personalized response generation. Comprehensive experiments on both our PAL-Bench and an external dataset demonstrate the effectiveness of the proposed memory framework.

</details>


### [78] [Non-Linear Scoring Model for Translation Quality Evaluation](https://arxiv.org/abs/2511.13467)
*Serge Gladkoff,Lifeng Han,Katerina Gasova*

Main category: cs.CL

TL;DR: 提出了一种基于对数函数的非线性翻译质量评估模型，解决了传统线性评估方法在不同文本长度下的偏差问题，使评估结果更符合人类感知。


<details>
  <summary>Details</summary>
Motivation: 传统的基于MQM的线性翻译质量评估方法在不同文本长度下存在偏差，对短文本过度惩罚，对长文本惩罚不足，与专家直觉不一致。

Method: 构建了双参数对数模型E(x) = a * ln(1 + b * x)，基于韦伯-费希纳定律和认知负荷理论，通过一维根查找步骤从两个容忍点进行校准。

Result: 实证数据显示可接受错误数量随样本大小呈对数增长而非线性增长，该模型提高了评估的可解释性、公平性和评分者间信度。

Conclusion: 该模型为翻译质量评估提供了更准确、可扩展的评估范式，为基于AI的文档级评估提供了更强的基础，并与人类判断保持一致。

Abstract: Analytic Translation Quality Evaluation (TQE), based on Multidimensional Quality Metrics (MQM), traditionally uses a linear error-to-penalty scale calibrated to a reference sample of 1000-2000 words. However, linear extrapolation biases judgment on samples of different sizes, over-penalizing short samples and under-penalizing long ones, producing misalignment with expert intuition.
  Building on the Multi-Range framework, this paper presents a calibrated, non-linear scoring model that better reflects how human content consumers perceive translation quality across samples of varying length. Empirical data from three large-scale enterprise environments shows that acceptable error counts grow logarithmically, not linearly, with sample size.
  Psychophysical and cognitive evidence, including the Weber-Fechner law and Cognitive Load Theory, supports this premise by explaining why the perceptual impact of additional errors diminishes while the cognitive burden grows with scale. We propose a two-parameter model
  E(x) = a * ln(1 + b * x), a, b > 0,
  anchored to a reference tolerance and calibrated from two tolerance points using a one-dimensional root-finding step. The model yields an explicit interval within which the linear approximation stays within +/-20 percent relative error and integrates into existing evaluation workflows with only a dynamic tolerance function added.
  The approach improves interpretability, fairness, and inter-rater reliability across both human and AI-generated translations. By operationalizing a perceptually valid scoring paradigm, it advances translation quality evaluation toward more accurate and scalable assessment. The model also provides a stronger basis for AI-based document-level evaluation aligned with human judgment. Implementation considerations for CAT/LQA systems and implications for human and AI-generated text evaluation are discussed.

</details>


### [79] [Aspect-Level Obfuscated Sentiment in Thai Financial Disclosures and Its Impact on Abnormal Returns](https://arxiv.org/abs/2511.13481)
*Attapol T. Rutherford,Sirisak Chueykamhang,Thachaparn Bunditlurdruk,Nanthicha Angsuwichitkul*

Main category: cs.CL

TL;DR: 本文提出了一种基于方面的情感分析方法，用于解码泰国财务年报中的模糊情感，并通过事件研究验证其对股价的实际影响。


<details>
  <summary>Details</summary>
Motivation: 财务文件中的情感理解对洞察市场行为至关重要，但这些报告常使用模糊语言来呈现积极或中性前景，即使实际条件可能不太有利。

Method: 开发了标注模糊情感的具体指南，标注了100多份财务报告，并对各种文本分类模型进行基准测试，同时进行事件研究评估对股价的影响。

Result: 在情感分类任务上表现出色，市场反应受到报告中特定方面的选择性影响。

Conclusion: 研究强调了财务文本情感分析的复杂性，以及解决模糊语言对准确评估市场情感的重要性。

Abstract: Understanding sentiment in financial documents is crucial for gaining insights into market behavior. These reports often contain obfuscated language designed to present a positive or neutral outlook, even when underlying conditions may be less favorable. This paper presents a novel approach using Aspect-Based Sentiment Analysis (ABSA) to decode obfuscated sentiment in Thai financial annual reports. We develop specific guidelines for annotating obfuscated sentiment in these texts and annotate more than one hundred financial reports. We then benchmark various text classification models on this annotated dataset, demonstrating strong performance in sentiment classification. Additionally, we conduct an event study to evaluate the real-world implications of our sentiment analysis on stock prices. Our results suggest that market reactions are selectively influenced by specific aspects within the reports. Our findings underscore the complexity of sentiment analysis in financial texts and highlight the importance of addressing obfuscated language to accurately assess market sentiment.

</details>


### [80] [Applying Large Language Models to Characterize Public Narratives](https://arxiv.org/abs/2511.13505)
*Elinor Poole-Dayan,Daniel T Kessler,Hannah Chiou,Margaret Hughes,Emily S Lin,Marshall Ganz,Deb Roy*

Main category: cs.CL

TL;DR: 本文提出了一种利用大语言模型自动标注公共叙事的新计算框架，在专家标注基础上评估LLM性能，发现可以达到接近人类专家的水平，并扩展到分析政治演讲。


<details>
  <summary>Details</summary>
Motivation: 公共叙事是领导力发展和公民动员的重要工具，但由于主观解释性和专家标注成本高，其系统性分析面临挑战。

Method: 开发了与领域专家共同制定的编码手册，使用LLMs自动进行公共叙事的定性标注，并与专家标注进行性能比较。

Result: LLMs在8个叙事和14个代码上平均F1得分达到0.80，接近人类专家水平；进一步扩展到22个故事和一组政治演讲的分析。

Conclusion: 研究展示了LLM辅助标注在可扩展叙事分析中的潜力，为计算公民叙事研究的局限性和未来方向提供了见解。

Abstract: Public Narratives (PNs) are key tools for leadership development and civic mobilization, yet their systematic analysis remains challenging due to their subjective interpretation and the high cost of expert annotation. In this work, we propose a novel computational framework that leverages large language models (LLMs) to automate the qualitative annotation of public narratives. Using a codebook we co-developed with subject-matter experts, we evaluate LLM performance against that of expert annotators. Our work reveals that LLMs can achieve near-human-expert performance, achieving an average F1 score of 0.80 across 8 narratives and 14 codes. We then extend our analysis to empirically explore how PN framework elements manifest across a larger dataset of 22 stories. Lastly, we extrapolate our analysis to a set of political speeches, establishing a novel lens in which to analyze political rhetoric in civic spaces. This study demonstrates the potential of LLM-assisted annotation for scalable narrative analysis and highlights key limitations and directions for future research in computational civic storytelling.

</details>


### [81] [Toward Conversational Hungarian Speech Recognition: Introducing the BEA-Large and BEA-Dialogue Datasets](https://arxiv.org/abs/2511.13529)
*Máté Gedeon,Piroska Zsófia Barta,Péter Mihajlik,Tekla Etelka Gráczi,Anna Kohári,Katalin Mády*

Main category: cs.CL

TL;DR: 该论文为匈牙利语引入了两个新的语音数据集BEA-Large和BEA-Dialogue，以解决匈牙利语在自动语音识别研究中代表性不足的问题，并建立了可复现的基线模型。


<details>
  <summary>Details</summary>
Motivation: 高资源语言在自动语音识别方面取得了显著进展，但匈牙利语等语言由于缺乏自发言语和对话语料库而代表性不足，需要填补这一研究空白。

Method: 从匈牙利语音语料库BEA中构建了两个新数据集：BEA-Large（255小时自发言语）和BEA-Dialogue（85小时自然对话），并使用公开可用的ASR模型建立了可复现的基线。

Result: 微调后的Fast Conformer模型在自发言语上实现了14.18%的词错误率，在重复语音上实现了4.8%的词错误率。说话人日志实验的错误率在13.05%到18.26%之间。

Conclusion: 对话ASR仍然具有挑战性，特别是由于不流畅、重叠和非正式语音模式。通过发布这些数据集和基线，旨在推进匈牙利语音技术，并为其他语言开发自发言语和对话基准提供方法框架。

Abstract: The advancement of automatic speech recognition (ASR) has been largely enhanced by extensive datasets in high-resource languages, while languages such as Hungarian remain underrepresented due to limited spontaneous and conversational corpora. To address this gap, we introduce two new datasets -- BEA-Large and BEA-Dialogue -- constructed from the previously unprocessed portions of the Hungarian speech corpus named BEA. BEA-Large extends BEA-Base with 255 hours of spontaneous speech from 433 speakers, enriched with detailed segment-level metadata. BEA-Dialogue, comprising 85 hours of spontaneous conversations, is a Hungarian speech corpus featuring natural dialogues partitioned into speaker-independent subsets, supporting research in conversational ASR and speaker diarization. We establish reproducible baselines on these datasets using publicly available ASR models, with the fine-tuned Fast Conformer model achieving word error rates as low as 14.18\% on spontaneous and 4.8\% on repeated speech. Diarization experiments yield diarization error rates between 13.05\% and 18.26\%, providing reference points for future improvements. The results highlight the persistent difficulty of conversational ASR, particularly due to disfluencies, overlaps, and informal speech patterns. By releasing these datasets and baselines, we aim to advance Hungarian speech technology and offer a methodological framework for developing spontaneous and conversational benchmarks in other languages.

</details>


### [82] [Beyond SELECT: A Comprehensive Taxonomy-Guided Benchmark for Real-World Text-to-SQL Translation](https://arxiv.org/abs/2511.13590)
*Hao Wang,Yuanfeng Song,Xiaoming Yin,Xing Chen*

Main category: cs.CL

TL;DR: 提出了一个基于核心意图、语句类型、语法结构和关键操作的文本到SQL分类新分类法，并基于此创建了SQL-Synth数据集，该数据集比现有基准具有更好的多样性和覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 现有文本到SQL数据集覆盖范围有限，未能捕捉真实应用的多样性，需要更全面的分类方法和数据集来评估和改进模型性能。

Method: 开发了文本到SQL分类法，结合大语言模型构建了分类法指导的数据集合成管道，生成了SQL-Synth数据集。

Result: SQL-Synth数据集在多样性和覆盖范围上优于现有基准，现有LLMs在该数据集上表现有限，但微调能显著提升性能。

Conclusion: 提出的分类法具有重要影响，不仅能全面分析数据集和LLMs性能，还能指导LLMs训练数据的构建。

Abstract: Text-to-SQL datasets are essential for training and evaluating text-to-SQL models, but existing datasets often suffer from limited coverage and fail to capture the diversity of real-world applications. To address this, we propose a novel taxonomy for text-to-SQL classification based on dimensions including core intents, statement types, syntax structures, and key actions. Using this taxonomy, we evaluate widely used public text-to-SQL datasets (e.g., Spider and Bird) and reveal limitations in their coverage and diversity. We then introduce a taxonomy-guided dataset synthesis pipeline, yielding a new dataset named SQL-Synth. This approach combines the taxonomy with Large Language Models (LLMs) to ensure the dataset reflects the breadth and complexity of real-world text-to-SQL applications. Extensive analysis and experimental results validate the effectiveness of our taxonomy, as SQL-Synth exhibits greater diversity and coverage compared to existing benchmarks. Moreover, we uncover that existing LLMs typically fall short in adequately capturing the full range of scenarios, resulting in limited performance on SQL-Synth. However, fine-tuning can substantially improve their performance in these scenarios. The proposed taxonomy has significant potential impact, as it not only enables comprehensive analysis of datasets and the performance of different LLMs, but also guides the construction of training data for LLMs.

</details>


### [83] [Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents](https://arxiv.org/abs/2511.13593)
*Piaohong Wang,Motong Tian,Jiaxian Li,Yuan Liang,Yuqing Wang,Qianben Chen,Tiannan Wang,Zhicong Lu,Jiawei Ma,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: O-Mem是一个基于主动用户画像的新型记忆框架，通过动态提取和更新用户特征与事件记录，支持分层检索，在个性化响应和效率方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体在复杂环境中维持长期交互时面临上下文一致性和动态个性化挑战，传统记忆系统依赖语义分组检索，可能忽略语义无关但关键的用户信息并引入检索噪声。

Method: 提出O-Mem框架，基于主动用户画像，从用户与智能体的主动交互中动态提取和更新用户特征和事件记录，支持人物属性和主题相关上下文的分层检索。

Result: 在LoCoMo基准测试上达到51.76%，比之前的SOTA LangMem提升近3%；在PERSONAMEM上达到62.99%，比A-Mem提升3.5%。同时在token和交互响应时间效率方面优于现有记忆框架。

Conclusion: O-Mem为开发高效且类人的个性化AI助手开辟了有前景的方向。

Abstract: Recent advancements in LLM-powered agents have demonstrated significant potential in generating human-like responses; however, they continue to face challenges in maintaining long-term interactions within complex environments, primarily due to limitations in contextual consistency and dynamic personalization. Existing memory systems often depend on semantic grouping prior to retrieval, which can overlook semantically irrelevant yet critical user information and introduce retrieval noise. In this report, we propose the initial design of O-Mem, a novel memory framework based on active user profiling that dynamically extracts and updates user characteristics and event records from their proactive interactions with agents. O-Mem supports hierarchical retrieval of persona attributes and topic-related context, enabling more adaptive and coherent personalized responses. O-Mem achieves 51.76% on the public LoCoMo benchmark, a nearly 3% improvement upon LangMem,the previous state-of-the-art, and it achieves 62.99% on PERSONAMEM, a 3.5% improvement upon A-Mem,the previous state-of-the-art. O-Mem also boosts token and interaction response time efficiency compared to previous memory frameworks. Our work opens up promising directions for developing efficient and human-like personalized AI assistants in the future.

</details>


### [84] [Why is "Chicago" Predictive of Deceptive Reviews? Using LLMs to Discover Language Phenomena from Lexical Cues](https://arxiv.org/abs/2511.13658)
*Jiaming Qu,Mengtian Guo,Yue Wang*

Main category: cs.CL

TL;DR: 使用大型语言模型将机器学习检测到的欺骗性评论特征转化为人类可理解的语言现象，帮助人们在没有检测分类器的情况下评估在线评论的可信度。


<details>
  <summary>Details</summary>
Motivation: 欺骗性评论误导消费者、损害企业利益并破坏在线市场信任。虽然机器学习分类器能有效区分欺骗性评论，但其学习到的特征往往难以被人类理解。

Method: 利用大型语言模型将机器学习学到的词汇线索翻译成人类可理解的语言现象，这些现象基于数据实证，具有跨领域泛化能力。

Result: 通过这种方法获得的语言现象比LLM先验知识或上下文学习获得的现象更具预测性，且能泛化到相似领域。

Conclusion: 这种方法获得的语言现象有助于人们在缺乏欺骗检测分类器的环境中批判性评估在线评论的可信度。

Abstract: Deceptive reviews mislead consumers, harm businesses, and undermine trust in online marketplaces. Machine learning classifiers can learn from large amounts of training examples to effectively distinguish deceptive reviews from genuine ones. However, the distinguishing features learned by these classifiers are often subtle, fragmented, and difficult for humans to interpret. In this work, we explore using large language models (LLMs) to translate machine-learned lexical cues into human-understandable language phenomena that can differentiate deceptive reviews from genuine ones. We show that language phenomena obtained in this manner are empirically grounded in data, generalizable across similar domains, and more predictive than phenomena either in LLMs' prior knowledge or obtained through in-context learning. These language phenomena have the potential to aid people in critically assessing the credibility of online reviews in environments where deception detection classifiers are unavailable.

</details>


### [85] [Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation](https://arxiv.org/abs/2511.13689)
*Sofia Jamil,Kotla Sai Charan,Sriparna Saha,Koustava Goswami,Joseph K J*

Main category: cs.CL

TL;DR: 提出了TAI框架，利用大语言模型和潜在扩散模型，通过翻译和图像生成来增强印度诗歌的可访问性，支持联合国可持续发展目标。


<details>
  <summary>Details</summary>
Motivation: 印度诗歌具有丰富的文化遗产，但其复杂的语言结构和文化内涵给非母语读者带来理解挑战，现有研究忽视了印度语言诗歌。

Method: TAI框架包含两个模块：(1) 使用几率比偏好对齐算法的翻译模块，准确翻译形态丰富的诗歌；(2) 使用语义图捕捉隐喻和含义关系的图像生成模块。

Result: 综合实验评估显示TAI Diffusion在诗歌图像生成任务中优于强基线模型，并发布了包含1,570首诗歌的MorphoVerse数据集。

Conclusion: 该工作通过解决诗歌翻译和视觉理解的空白，旨在扩大可访问性并丰富读者体验。

Abstract: Indian poetry, known for its linguistic complexity and deep cultural resonance, has a rich and varied heritage spanning thousands of years. However, its layered meanings, cultural allusions, and sophisticated grammatical constructions often pose challenges for comprehension, especially for non-native speakers or readers unfamiliar with its context and language. Despite its cultural significance, existing works on poetry have largely overlooked Indian language poems. In this paper, we propose the Translation and Image Generation (TAI) framework, leveraging Large Language Models (LLMs) and Latent Diffusion Models through appropriate prompt tuning. Our framework supports the United Nations Sustainable Development Goals of Quality Education (SDG 4) and Reduced Inequalities (SDG 10) by enhancing the accessibility of culturally rich Indian-language poetry to a global audience. It includes (1) a translation module that uses an Odds Ratio Preference Alignment Algorithm to accurately translate morphologically rich poetry into English, and (2) an image generation module that employs a semantic graph to capture tokens, dependencies, and semantic relationships between metaphors and their meanings, to create visually meaningful representations of Indian poems. Our comprehensive experimental evaluation, including both human and quantitative assessments, demonstrates the superiority of TAI Diffusion in poem image generation tasks, outperforming strong baselines. To further address the scarcity of resources for Indian-language poetry, we introduce the Morphologically Rich Indian Language Poems MorphoVerse Dataset, comprising 1,570 poems across 21 low-resource Indian languages. By addressing the gap in poetry translation and visual comprehension, this work aims to broaden accessibility and enrich the reader's experience.

</details>


### [86] [Generalist Foundation Models Are Not Clinical Enough for Hospital Operations](https://arxiv.org/abs/2511.13703)
*Lavender Y. Jiang,Angelica Chen,Xu Han,Xujin Chris Liu,Radhika Dua,Kevin Eaton,Frederick Wolff,Robert Steele,Jeff Zhang,Anton Alyakin,Qingkai Pan,Yanbing Chen,Karl L. Sangwon,Daniel A. Alber,Jaden Stryker,Jin Vivian Lee,Yindalon Aphinyanaphongs,Kyunghyun Cho,Eric Karl Oermann*

Main category: cs.CL

TL;DR: Lang1模型系列通过结合临床数据和互联网数据进行预训练，在医疗操作决策任务上显著优于通用模型，证明了领域特定预训练和监督微调的重要性。


<details>
  <summary>Details</summary>
Motivation: 通用基础模型缺乏医疗操作决策所需的专业知识，需要开发专门针对医疗领域的模型来提升医院运营决策的质量。

Method: 开发Lang1模型系列（100M-7B参数），使用NYU Langone Health的800亿临床token和6270亿互联网token混合预训练，并创建ReMedE基准评估五个关键医疗任务。

Result: Lang1-1B在微调后优于比其大70倍的通用模型和比其大671倍的零样本模型，AUROC提升3.64%-6.75%和1.66%-23.66%，且能有效迁移到分布外场景。

Conclusion: 医疗系统AI需要结合领域内预训练、监督微调和真实世界评估，专业LLM在特定任务上可与通用模型竞争。

Abstract: Hospitals and healthcare systems rely on operational decisions that determine patient flow, cost, and quality of care. Despite strong performance on medical knowledge and conversational benchmarks, foundation models trained on general text may lack the specialized knowledge required for these operational decisions. We introduce Lang1, a family of models (100M-7B parameters) pretrained on a specialized corpus blending 80B clinical tokens from NYU Langone Health's EHRs and 627B tokens from the internet. To rigorously evaluate Lang1 in real-world settings, we developed the REalistic Medical Evaluation (ReMedE), a benchmark derived from 668,331 EHR notes that evaluates five critical tasks: 30-day readmission prediction, 30-day mortality prediction, length of stay, comorbidity coding, and predicting insurance claims denial. In zero-shot settings, both general-purpose and specialized models underperform on four of five tasks (36.6%-71.7% AUROC), with mortality prediction being an exception. After finetuning, Lang1-1B outperforms finetuned generalist models up to 70x larger and zero-shot models up to 671x larger, improving AUROC by 3.64%-6.75% and 1.66%-23.66% respectively. We also observed cross-task scaling with joint finetuning on multiple tasks leading to improvement on other tasks. Lang1-1B effectively transfers to out-of-distribution settings, including other clinical tasks and an external health system. Our findings suggest that predictive capabilities for hospital operations require explicit supervised finetuning, and that this finetuning process is made more efficient by in-domain pretraining on EHR. Our findings support the emerging view that specialized LLMs can compete with generalist models in specialized tasks, and show that effective healthcare systems AI requires the combination of in-domain pretraining, supervised finetuning, and real-world evaluation beyond proxy benchmarks.

</details>


### [87] [HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization](https://arxiv.org/abs/2505.11225)
*Chengyu Huang,Zhengxin Zhang,Claire Cardie*

Main category: cs.CL

TL;DR: HAPO是一种历史感知的策略优化方法，通过在训练过程中记录每个问题的历史状态，激励模型发现比之前更简洁的正确解决方案，从而在保持准确性的同时显著减少输出长度。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时扩展方法通常使用通用预算约束或查询级长度优化，但没有利用训练过程中相同问题的历史信息，这限制了它们随时间推移逐步使解决方案更简洁的能力。

Method: HAPO跟踪每个问题的历史状态（如先前生成正确响应的最小长度），并基于此设计新颖的长度奖励函数，激励发现比之前更简洁的正确解决方案，同时避免过度惩罚较短的错误响应以促进探索。

Result: 在多个数学基准测试中，HAPO实现了33-59%的长度减少，而准确率仅下降2-5%。

Conclusion: HAPO有效诱导了LLMs的简洁推理能力，在保持高准确性的同时显著减少了输出长度。

Abstract: While scaling the length of responses at test-time has been shown to markedly improve the reasoning abilities and performance of large language models (LLMs), it often results in verbose outputs and increases inference cost. Prior approaches for efficient test-time scaling, typically using universal budget constraints or query-level length optimization, do not leverage historical information from previous encounters with the same problem during training. We hypothesize that this limits their ability to progressively make solutions more concise over time. To address this, we present History-Aware Policy Optimization (HAPO), which keeps track of a history state (e.g., the minimum length over previously generated correct responses) for each problem. HAPO employs a novel length reward function based on this history state to incentivize the discovery of correct solutions that are more concise than those previously found. Crucially, this reward structure avoids overly penalizing shorter incorrect responses with the goal of facilitating exploration towards more efficient solutions. By combining this length reward with a correctness reward, HAPO jointly optimizes for correctness and efficiency. We use HAPO to train DeepSeek-R1-Distill-Qwen-1.5B, DeepScaleR-1.5B-Preview, and Qwen-2.5-1.5B-Instruct, and evaluate HAPO on several math benchmarks that span various difficulty levels. Experiment results demonstrate that HAPO effectively induces LLMs' concise reasoning abilities, producing length reductions of 33-59% with accuracy drops of only 2-5%.

</details>


### [88] [DCRM: A Heuristic to Measure Response Pair Quality in Preference Optimization](https://arxiv.org/abs/2506.14157)
*Chengyu Huang,Tanya Goyal*

Main category: cs.CL

TL;DR: 提出DCRM指标来量化偏好优化中响应对的质量，发现训练集DCRM与学习效果正相关，并基于此提出最佳配对方法提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究试图将偏好优化性能与偏好数据集关联，但观察到偏好多与不偏好响应之间的差异可能不符合学习需求。

Method: 使用距离和奖励边际量化响应差异，结合得到DCRM指标，并提出best-of-N²配对方法选择DCRM最高的响应对。

Result: 在各种设置下，该方法产生的训练数据集在AlpacaEval、MT-Bench和Arena-Hard上能进一步提升模型性能。

Conclusion: DCRM是衡量响应对质量的有效指标，基于DCRM的配对方法能有效提升偏好优化效果。

Abstract: Recent research has attempted to associate preference optimization (PO) performance with the underlying preference datasets. In this work, our observation is that the differences between the preferred response $y^+$ and dispreferred response $y^-$ influence what LLMs can learn, which may not match the desirable differences to learn. Therefore, we use distance and reward margin to quantify these differences, and combine them to get Distance Calibrated Reward Margin (DCRM), a metric that measures the quality of a response pair for PO. Intuitively, DCRM encourages minimal noisy differences and maximal desired differences. With this, we study 3 types of commonly used preference datasets, classified along two axes: the source of the responses and the preference labeling function. We establish a general correlation between higher DCRM of the training set and better learning outcome. Inspired by this, we propose a best-of-$N^2$ pairing method that selects response pairs with the highest DCRM. Empirically, in various settings, our method produces training datasets that can further improve models' performance on AlpacaEval, MT-Bench, and Arena-Hard over the existing training sets.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [89] [LLM-Generated Negative News Headlines Dataset: Creation and Benchmarking Against Real Journalism](https://arxiv.org/abs/2511.11591)
*Olusola Babalola,Bolanle Ojokoh,Olutayo Boyinbode*

Main category: cs.AI

TL;DR: 本研究探讨了使用大型语言模型生成的合成新闻标题作为真实世界数据替代品的可行性，特别关注负面情感文本在情感分析中的应用。


<details>
  <summary>Details</summary>
Motivation: 克服自然语言处理任务中数据获取的挑战和真实世界数据相关的隐私问题，为情感分析提供替代数据源。

Method: 使用定制提示创建专门的负面新闻标题语料库，通过专家评审和嵌入空间分析验证合成标题，并与真实新闻标题进行多项基准测试比较。

Result: 生成的标题在内容、语调、长度和风格上与真实标题高度匹配，仅在词性标注测试中的专有名词得分存在明显差异。

Conclusion: LLM生成的合成数据集可以有效替代真实世界数据，为NLP任务特别是情感分析提供可行的数据解决方案。

Abstract: This research examines the potential of datasets generated by Large Language Models (LLMs) to support Natural Language Processing (NLP) tasks, aiming to overcome challenges related to data acquisition and privacy concerns associated with real-world data. Focusing on negative valence text, a critical component of sentiment analysis, we explore the use of LLM-generated synthetic news headlines as an alternative to real-world data. A specialized corpus of negative news headlines was created using tailored prompts to capture diverse negative sentiments across various societal domains. The synthetic headlines were validated by expert review and further analyzed in embedding space to assess their alignment with real-world negative news in terms of content, tone, length, and style. Key metrics such as correlation with real headlines, perplexity, coherence, and realism were evaluated. The synthetic dataset was benchmarked against two sets of real news headlines using evaluations including the Comparative Perplexity Test, Comparative Readability Test, Comparative POS Profiling, BERTScore, and Comparative Semantic Similarity. Results show the generated headlines match real headlines with the only marked divergence being in the proper noun score of the POS profile test.

</details>


### [90] [CLINB: A Climate Intelligence Benchmark for Foundational Models](https://arxiv.org/abs/2511.11597)
*Michelle Chen Huebscher,Katharine Mach,Aleksandar Stanić,Markus Leippold,Ben Gaiarin,Zeke Hausfather,Elisa Rawat,Erich Fischer,Massimiliano Ciaramita,Joeri Rogelj,Christian Buck,Lierni Sestorain Saralegui,Reto Knutti*

Main category: cs.AI

TL;DR: CLINB是一个评估大语言模型处理气候变化专业知识的基准测试，发现前沿模型在知识综合方面表现出色，但在证据基础和引用方面存在严重幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型处理复杂专业知识的能力，特别是在气候变化领域，需要建立可靠的基准来测试模型的知识质量和证据支持能力。

Method: 开发CLINB基准，使用真实用户问题和气候科学家制定的评估标准，通过基于模型的评估流程测试多个前沿模型。

Result: 前沿模型展现出博士级别的知识综合能力，表现优于专家辅助的混合答案，但在证据基础方面存在严重问题，引用和图像幻觉率很高。

Conclusion: 需要在知识综合和可验证归因之间架起桥梁，CLINB等可靠基准对于构建可信AI系统至关重要。

Abstract: Evaluating how Large Language Models (LLMs) handle complex, specialized knowledge remains a critical challenge. We address this through the lens of climate change by introducing CLINB, a benchmark that assesses models on open-ended, grounded, multimodal question answering tasks with clear requirements for knowledge quality and evidential support. CLINB relies on a dataset of real users' questions and evaluation rubrics curated by leading climate scientists. We implement and validate a model-based evaluation process and evaluate several frontier models. Our findings reveal a critical dichotomy. Frontier models demonstrate remarkable knowledge synthesis capabilities, often exhibiting PhD-level understanding and presentation quality. They outperform "hybrid" answers curated by domain experts assisted by weaker models. However, this performance is countered by failures in grounding. The quality of evidence varies, with substantial hallucination rates for references and images. We argue that bridging this gap between knowledge synthesis and verifiable attribution is essential for the deployment of AI in scientific workflows and that reliable, interpretable benchmarks like CLINB are needed to progress towards building trustworthy AI systems.

</details>


### [91] [SynBullying: A Multi LLM Synthetic Conversational Dataset for Cyberbullying Detectio](https://arxiv.org/abs/2511.11599)
*Arefeh Kazemi,Hamza Qadeer,Joachim Wagner,Hossein Hosseini,Sri Balaaji Natarajan Kalaivendan,Brian Davis*

Main category: cs.AI

TL;DR: SynBullying是一个用于研究和检测网络欺凌的合成多LLM对话数据集，通过大语言模型模拟真实欺凌互动，提供可扩展且伦理安全的替代方案。


<details>
  <summary>Details</summary>
Motivation: 为网络欺凌研究提供可扩展且伦理安全的数据收集替代方案，克服人类数据收集的局限性和伦理问题。

Method: 利用大语言模型生成模拟真实欺凌互动的多轮对话，提供对话结构、上下文感知标注和细粒度标签。

Result: 数据集在对话结构、词汇模式、情感/毒性、角色动态、伤害强度和欺凌类型分布等五个维度上进行了评估，并测试了其作为独立训练数据和增强源的性能。

Conclusion: SynBullying是一个有效的网络欺凌研究工具，能够提供现实且伦理安全的训练数据，支持详细的网络欺凌行为分析。

Abstract: We introduce SynBullying, a synthetic multi-LLM conversational dataset for studying and detecting cyberbullying (CB). SynBullying provides a scalable and ethically safe alternative to human data collection by leveraging large language models (LLMs) to simulate realistic bullying interactions. The dataset offers (i) conversational structure, capturing multi-turn exchanges rather than isolated posts; (ii) context-aware annotations, where harmfulness is assessed within the conversational flow considering context, intent, and discourse dynamics; and (iii) fine-grained labeling, covering various CB categories for detailed linguistic and behavioral analysis. We evaluate SynBullying across five dimensions, including conversational structure, lexical patterns, sentiment/toxicity, role dynamics, harm intensity, and CB-type distribution. We further examine its utility by testing its performance as standalone training data and as an augmentation source for CB classification.

</details>


### [92] [CausalGuard: A Smart System for Detecting and Preventing False Information in Large Language Models](https://arxiv.org/abs/2511.11600)
*Piyushkumar Patel*

Main category: cs.AI

TL;DR: CausalGuard是一种结合因果推理和符号逻辑的新方法，用于实时检测和防止大语言模型的幻觉问题，在12个基准测试中能准确识别89.3%的幻觉，同时减少近80%的错误陈述。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在严重幻觉问题，会自信地陈述听起来合理但实际错误的信息，这成为在准确性要求高的场景中使用这些模型的主要障碍。现有解决方案要么需要重新训练整个模型，要么增加显著计算成本，或者未能解决幻觉的根本原因。

Method: CausalGuard通过两条互补路径工作：一条路径追踪模型已知信息与生成内容之间的因果关系，另一条路径使用自动推理检查逻辑一致性。系统理解导致错误陈述的因果链，并在过程中早期进行干预。

Result: 在12个不同基准测试中，CausalGuard能正确识别89.3%的幻觉，仅漏检8.3%的实际幻觉。更重要的是，它减少了近80%的错误陈述，同时保持回答的自然性和帮助性。在需要多步逻辑推理的复杂任务上表现尤其出色。

Conclusion: CausalGuard通过展示推理过程，在医疗诊断或金融分析等敏感领域特别有效，因为这些领域理解决策原因与决策本身同样重要。该方法为解决大语言模型幻觉问题提供了一种有效且可解释的解决方案。

Abstract: While large language models have transformed how we interact with AI systems, they have a critical weakness: they confidently state false information that sounds entirely plausible. This "hallucination" problem has become a major barrier to using these models where accuracy matters most. Existing solutions either require retraining the entire model, add significant computational costs, or miss the root causes of why these hallucinations occur in the first place.
  We present CausalGuard, a new approach that combines causal reasoning with symbolic logic to catch and prevent hallucinations as they happen. Unlike previous methods that only check outputs after generation, our system understands the causal chain that leads to false statements and intervenes early in the process. CausalGuard works through two complementary paths: one that traces causal relationships between what the model knows and what it generates, and another that checks logical consistency using automated reasoning.
  Testing across twelve different benchmarks, we found that CausalGuard correctly identifies hallucinations 89.3\% of the time while missing only 8.3\% of actual hallucinations. More importantly, it reduces false claims by nearly 80\% while keeping responses natural and helpful. The system performs especially well on complex reasoning tasks where multiple steps of logic are required. Because CausalGuard shows its reasoning process, it works well in sensitive areas like medical diagnosis or financial analysis where understanding why a decision was made matters as much as the decision itself.

</details>


### [93] [Quantifying Skill and Chance: A Unified Framework for the Geometry of Games](https://arxiv.org/abs/2511.11611)
*David H. Silver*

Main category: cs.AI

TL;DR: 提出了一个量化框架，通过将游戏建模为随机决策树来分离技能和运气成分，定义了技能-运气指数S(G)在[-1,1]范围内，应用于30个游戏揭示了从纯运气到纯技能的连续谱。


<details>
  <summary>Details</summary>
Motivation: 需要系统性地量化游戏中技能和运气成分的相对重要性，以便进行游戏设计、AI评估和风险评估。

Method: 将游戏建模为随机决策树，分解游戏结果为技能杠杆K和运气杠杆L，定义技能-运气指数S(G) = (K - L)/(K + L)，并引入波动率Sigma来量化连续回合的结果不确定性。

Result: 分析30个游戏显示：硬币投掷S=-1（纯运气），西洋双陆棋S=0，国际象棋S=+1（纯技能），扑克S=0.33（技能主导）。波动率分析显示不同游戏的预测稳定性差异。

Conclusion: 该框架可扩展到一般随机决策系统，为玩家影响力、游戏平衡性和预测稳定性的比较提供了原则性方法，在游戏设计、AI评估和风险评估中具有应用价值。

Abstract: We introduce a quantitative framework for separating skill and chance in games by modeling them as complementary sources of control over stochastic decision trees. We define the Skill-Luck Index S(G) in [-1, 1] by decomposing game outcomes into skill leverage K and luck leverage L. Applying this to 30 games reveals a continuum from pure chance (coin toss, S = -1) through mixed domains such as backgammon (S = 0, Sigma = 1.20) to pure skill (chess, S = +1, Sigma = 0). Poker exhibits moderate skill dominance (S = 0.33) with K = 0.40 +/- 0.03 and Sigma = 0.80. We further introduce volatility Sigma to quantify outcome uncertainty over successive turns. The framework extends to general stochastic decision systems, enabling principled comparisons of player influence, game balance, and predictive stability, with applications to game design, AI evaluation, and risk assessment.

</details>


### [94] [Value-Aligned Prompt Moderation via Zero-Shot Agentic Rewriting for Safe Image Generation](https://arxiv.org/abs/2511.11693)
*Xin Zhao,Xiaojun Chen,Bingshan Liu,Zeyao Liu,Zhendong Zhao,Xiaoyan Gu*

Main category: cs.AI

TL;DR: VALOR是一个模块化的零样本代理框架，通过分层提示分析、价值对齐推理和选择性重写，在保持生成质量的同时显著减少不安全图像内容。


<details>
  <summary>Details</summary>
Motivation: 生成式视觉语言模型存在生成不安全、冒犯性或文化不当内容的风险，现有防御方法难以在不牺牲生成质量或增加成本的情况下实现价值对齐。

Method: 采用分层提示分析：多级NSFW检测器过滤词汇和语义风险；文化价值对齐模块识别社会规范、合法性和伦理违规；意图消歧器检测隐含不安全含义。检测到不安全内容时，由大语言模型在动态角色特定指令下选择性重写提示。

Result: 在对抗性、模糊性和价值敏感提示上的实验显示，VALOR将不安全输出减少高达100.00%，同时保持提示的有用性和创造性。

Conclusion: VALOR是可扩展且有效的方法，可在开放世界环境中部署安全、对齐且有用的图像生成系统。

Abstract: Generative vision-language models like Stable Diffusion demonstrate remarkable capabilities in creative media synthesis, but they also pose substantial risks of producing unsafe, offensive, or culturally inappropriate content when prompted adversarially. Current defenses struggle to align outputs with human values without sacrificing generation quality or incurring high costs. To address these challenges, we introduce VALOR (Value-Aligned LLM-Overseen Rewriter), a modular, zero-shot agentic framework for safer and more helpful text-to-image generation. VALOR integrates layered prompt analysis with human-aligned value reasoning: a multi-level NSFW detector filters lexical and semantic risks; a cultural value alignment module identifies violations of social norms, legality, and representational ethics; and an intention disambiguator detects subtle or indirect unsafe implications. When unsafe content is detected, prompts are selectively rewritten by a large language model under dynamic, role-specific instructions designed to preserve user intent while enforcing alignment. If the generated image still fails a safety check, VALOR optionally performs a stylistic regeneration to steer the output toward a safer visual domain without altering core semantics. Experiments across adversarial, ambiguous, and value-sensitive prompts show that VALOR significantly reduces unsafe outputs by up to 100.00% while preserving prompt usefulness and creativity. These results highlight VALOR as a scalable and effective approach for deploying safe, aligned, and helpful image generation systems in open-world settings.

</details>


### [95] [Towards autonomous quantum physics research using LLM agents with access to intelligent tools](https://arxiv.org/abs/2511.11752)
*Sören Arlt,Xuemei Gu,Mario Krenn*

Main category: cs.AI

TL;DR: AI-Mandel是一个能够生成并实现量子物理学创意的LLM代理，它从文献中构思想法并使用领域特定AI工具将其转化为可直接在实验室实施的实验设计。


<details>
  <summary>Details</summary>
Motivation: 当前AI在科学领域的应用仍主要依赖人类研究者提供初始研究问题，AI生成的创意往往模糊且需要人类执行。自动化创意生成与实现系统将显著改变人类在科学过程中的角色。

Method: AI-Mandel使用LLM从文献中生成创意，并通过领域特定AI工具将这些创意转化为具体的实验设计方案。

Result: AI-Mandel生成的创意具有科学价值，其中两个创意已独立撰写成后续科学论文。创意包括量子隐形传态的新变体、不定因果顺序中的量子网络原语以及基于量子信息传递闭环的新几何相位概念。

Conclusion: AI-Mandel展示了能够生成和实施具体可行创意的AI物理学家原型，这不仅有助于加速科学发展，还揭示了实现人类水平人工科学家所面临的具体挑战。

Abstract: Artificial intelligence (AI) is used in numerous fields of science, yet the initial research questions and targets are still almost always provided by human researchers. AI-generated creative ideas in science are rare and often vague, so that it remains a human task to execute them. Automating idea generation and implementation in one coherent system would significantly shift the role of humans in the scientific process. Here we present AI-Mandel, an LLM agent that can generate and implement ideas in quantum physics. AI-Mandel formulates ideas from the literature and uses a domain-specific AI tool to turn them into concrete experiment designs that can readily be implemented in laboratories. The generated ideas by AI-Mandel are often scientifically interesting - for two of them we have already written independent scientific follow-up papers. The ideas include new variations of quantum teleportation, primitives of quantum networks in indefinite causal orders, and new concepts of geometric phases based on closed loops of quantum information transfer. AI-Mandel is a prototypical demonstration of an AI physicist that can generate and implement concrete, actionable ideas. Building such a system is not only useful to accelerate science, but it also reveals concrete open challenges on the path to human-level artificial scientists.

</details>


### [96] [Learning to Refine: An Agentic RL Approach for Iterative SPARQL Query Construction](https://arxiv.org/abs/2511.11770)
*Floris Vossebeld,Shenghui Wang*

Main category: cs.AI

TL;DR: 提出了一种基于强化学习的智能体框架，让小型LLM通过迭代执行反馈学习SPARQL查询构建策略，在LC-QuAD 2.0数据集上实现了49.7%的准确率，比最佳零样本基线提升17.5个百分点。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在生成复杂多跳SPARQL查询时的脆弱性问题，当前方法缺乏基于实时执行反馈的动态调试能力。

Method: 使用仅3B参数的LLM，通过结果驱动的强化学习（GRPO）训练，无需监督微调，学习迭代SPARQL构建的弹性策略，包含显式推理步骤作为认知支架。

Result: 在LC-QuAD 2.0的可执行子集上达到49.7%的准确率，相比最佳零样本基线提升17.5个百分点，强化学习驱动的能力通过显式推理步骤得到增强。

Conclusion: 提出了一个可推广的蓝图，通过交互教学智能体掌握形式化符号工具，弥合概率性LLM与结构化知识图谱之间的差距。

Abstract: Generating complex, logically-sound SPARQL queries for multi-hop questions remains a critical bottleneck for Knowledge Graph Question Answering, as the brittle nature of one-shot generation by Large Language Models (LLMs) hinders reliable interaction with structured data. Current methods lack the adaptive policies needed to dynamically debug queries based on real-time execution feedback. This paper introduces a novel agentic framework where an LLM learns a resilient policy for the sequential process of iterative SPARQL construction. We show that a compact 3B-parameter model, trained exclusively via outcome-driven Reinforcement Learning (GRPO) without supervised fine-tuning, can learn effective policies for this task, discovering how to systematically recover from execution errors and refine its queries toward a correct answer. On a curated, executable single-answer subset of LC-QuAD 2.0, our agent achieves 49.7\% accuracy post-entity-linking, a significant 17.5 percentage point improvement over the strongest iterative zero-shot baseline. Further analysis reveals that while the agent's capability is driven by RL, its performance is enhanced by an explicit deliberative reasoning step that acts as a cognitive scaffold to improve policy precision. This work presents a generalizable blueprint for teaching agents to master formal, symbolic tools through interaction, bridging the gap between probabilistic LLMs and the structured world of Knowledge Graphs.

</details>


### [97] [On the Measure of a Model: From Intelligence to Generality](https://arxiv.org/abs/2511.11773)
*Ruchira Dhar,Ninell Oldenburg,Anders Soegaard*

Main category: cs.AI

TL;DR: 论文质疑当前基于抽象智力概念的AI评估基准，提出应以通用性而非智力作为评估基础，认为通用性更能反映模型在多样化任务中的实际表现。


<details>
  <summary>Details</summary>
Motivation: 当前AI评估基准（如ARC、Raven测试等）基于模糊的智力概念，无法有效预测模型在实际任务（如问答、摘要、编程）中的表现，存在评估与实际效用脱节的风险。

Method: 通过概念和形式分析，检验智力评估的三个假设（通用性、稳定性、现实性），论证只有通用性能够经受概念和实证检验。

Result: 分析表明智力不能解释通用性，通用性应被理解为多任务学习问题，直接关联到可测量的性能广度和可靠性。

Conclusion: 应重新构建AI进展评估方式，将通用性作为评估跨领域和演进任务能力的更稳定基础。

Abstract: Benchmarks such as ARC, Raven-inspired tests, and the Blackbird Task are widely used to evaluate the intelligence of large language models (LLMs). Yet, the concept of intelligence remains elusive- lacking a stable definition and failing to predict performance on practical tasks such as question answering, summarization, or coding. Optimizing for such benchmarks risks misaligning evaluation with real-world utility. Our perspective is that evaluation should be grounded in generality rather than abstract notions of intelligence. We identify three assumptions that often underpin intelligence-focused evaluation: generality, stability, and realism. Through conceptual and formal analysis, we show that only generality withstands conceptual and empirical scrutiny. Intelligence is not what enables generality; generality is best understood as a multitask learning problem that directly links evaluation to measurable performance breadth and reliability. This perspective reframes how progress in AI should be assessed and proposes generality as a more stable foundation for evaluating capability across diverse and evolving tasks.

</details>


### [98] [Do LLMs Really Struggle at NL-FOL Translation? Revealing their Strengths via a Novel Benchmarking Strategy](https://arxiv.org/abs/2511.11816)
*Andrea Brunello,Luca Geatti,Michele Mignani,Angelo Montanari,Nicola Saccomanno*

Main category: cs.AI

TL;DR: 本文批判性评估了现有NL-FOL翻译数据集和评估方法的局限性，提出了新的评估协议来区分真正的语义理解与表面模式识别，并发现对话导向的LLMs在NL-FOL翻译方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 由于一阶逻辑(FOL)的表达能力和明确性，它是表示自然语言(NL)概念的强大形式化工具。虽然将FOL翻译成可读英语相对简单，但将NL转换为FOL(NL-FOL翻译)对人和机器都是长期挑战。尽管LLMs的出现带来了突破希望，但现有文献对其NL-FOL翻译能力给出了矛盾的结果。

Method: 1) 批判性检查现有NL-FOL翻译评估数据集和协议；2) 提出新的评估协议，专门区分真正的语义级逻辑理解与表面模式识别、记忆和数据集污染；3) 使用新方法评估最先进的对话导向LLMs。

Result: 研究表明，最先进的对话导向LLMs展现出强大的NL-FOL翻译技能和真正的句子级逻辑理解能力，而嵌入中心模型表现明显较差。

Conclusion: 通过设计适当的评估协议，可以更准确地评估LLMs的NL-FOL翻译能力，对话导向的LLMs在此任务上表现出色，表明它们具有真正的逻辑理解能力。

Abstract: Due to its expressiveness and unambiguous nature, First-Order Logic (FOL) is a powerful formalism for representing concepts expressed in natural language (NL). This is useful, e.g., for specifying and verifying desired system properties. While translating FOL into human-readable English is relatively straightforward, the inverse problem, converting NL to FOL (NL-FOL translation), has remained a longstanding challenge, for both humans and machines. Although the emergence of Large Language Models (LLMs) promised a breakthrough, recent literature provides contrasting results on their ability to perform NL-FOL translation. In this work, we provide a threefold contribution. First, we critically examine existing datasets and protocols for evaluating NL-FOL translation performance, revealing key limitations that may cause a misrepresentation of LLMs' actual capabilities. Second, to overcome these shortcomings, we propose a novel evaluation protocol explicitly designed to distinguish genuine semantic-level logical understanding from superficial pattern recognition, memorization, and dataset contamination. Third, using this new approach, we show that state-of-the-art, dialogue-oriented LLMs demonstrate strong NL-FOL translation skills and a genuine grasp of sentence-level logic, whereas embedding-centric models perform markedly worse.

</details>


### [99] [TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models](https://arxiv.org/abs/2511.11831)
*Wenhao Zhou,Hao Zheng,Rong Zhao*

Main category: cs.AI

TL;DR: TopoPerception是一个基于拓扑属性的基准测试，用于严格评估大型视觉语言模型的全局视觉感知能力，发现现有模型在全局感知方面表现不佳，甚至不如随机猜测。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型将视觉特征与预训练语言模型对齐，但视觉感知模块成为瓶颈，且传统评估基准存在局部捷径，高估了模型的感知能力。

Method: 利用拓扑属性构建评估基准，因为拓扑依赖于图像全局结构且对局部特征不变，能够实现无捷径的全局感知评估。

Result: 在最粗的感知粒度上，所有模型表现都不优于随机机会，表明模型缺乏全局视觉特征感知能力。更强大的模型反而准确率更低。

Conclusion: 仅扩大模型规模不足以解决全局感知缺陷，可能需要新的训练范式或架构。TopoPerception揭示了当前LVLMs的关键瓶颈并提供了改进方向。

Abstract: Large Vision-Language Models (LVLMs) typically align visual features from an encoder with a pre-trained Large Language Model (LLM). However, this makes the visual perception module a bottleneck, which constrains the overall capabilities of LVLMs. Conventional evaluation benchmarks, while rich in visual semantics, often contain unavoidable local shortcuts that can lead to an overestimation of models' perceptual abilities. Here, we introduce TopoPerception, a benchmark that leverages topological properties to rigorously evaluate the global visual perception capabilities of LVLMs across various granularities. Since topology depends on the global structure of an image and is invariant to local features, TopoPerception enables a shortcut-free assessment of global perception, fundamentally distinguishing it from semantically rich tasks. We evaluate state-of-the-art models on TopoPerception and find that even at the coarsest perceptual granularity, all models perform no better than random chance, indicating a profound inability to perceive global visual features. Notably, a consistent trend emerge within model families: more powerful models with stronger reasoning capabilities exhibit lower accuracy. This suggests that merely scaling up models is insufficient to address this deficit and may even exacerbate it. Progress may require new training paradigms or architectures. TopoPerception not only exposes a critical bottleneck in current LVLMs but also offers a lens and direction for improving their global visual perception. The data and code are publicly available at: https://github.com/Wenhao-Zhou/TopoPerception.

</details>


### [100] [End to End AI System for Surgical Gesture Sequence Recognition and Clinical Outcome Prediction](https://arxiv.org/abs/2511.11899)
*Xi Li,Nicholas Matsumoto,Ujjwal Pasupulety,Atharva Deo,Cherine Yang,Jay Moran,Miguel E. Hernandez,Peter Wager,Jasmine Lin,Jeanine Kim,Alvin C. Goh,Christian Wagner,Geoffrey A. Sonn,Andrew J. Hung*

Main category: cs.AI

TL;DR: F2O是一个端到端系统，能够将组织解剖视频转换为手势序列，并发现与术后结果相关的模式，为数据驱动的手术反馈和临床决策支持奠定基础。


<details>
  <summary>Details</summary>
Motivation: 术中行为的细粒度分析及其对患者结果的影响是一个长期存在的挑战。

Method: 利用基于transformer的空间和时间建模以及逐帧分类，F2O在机器人辅助根治性前列腺切除术的神经保留步骤中稳健地检测连续短手势。

Result: F2O在帧级别和视频级别的检测AUC分别为0.80和0.81；F2O衍生的特征预测术后结果的准确性与人工注释相当（0.79 vs 0.75）；在25个共享特征中，效应大小方向一致且差异小（~0.07），相关性强（r=0.96）。

Conclusion: 通过实现自动可解释的评估，F2O为数据驱动的手术反馈和前瞻性临床决策支持奠定了基础。

Abstract: Fine-grained analysis of intraoperative behavior and its impact on patient outcomes remain a longstanding challenge. We present Frame-to-Outcome (F2O), an end-to-end system that translates tissue dissection videos into gesture sequences and uncovers patterns associated with postoperative outcomes. Leveraging transformer-based spatial and temporal modeling and frame-wise classification, F2O robustly detects consecutive short (~2 seconds) gestures in the nerve-sparing step of robot-assisted radical prostatectomy (AUC: 0.80 frame-level; 0.81 video-level). F2O-derived features (gesture frequency, duration, and transitions) predicted postoperative outcomes with accuracy comparable to human annotations (0.79 vs. 0.75; overlapping 95% CI). Across 25 shared features, effect size directions were concordant with small differences (~ 0.07), and strong correlation (r = 0.96, p < 1e-14). F2O also captured key patterns linked to erectile function recovery, including prolonged tissue peeling and reduced energy use. By enabling automatic interpretable assessment, F2O establishes a foundation for data-driven surgical feedback and prospective clinical decision support.

</details>


### [101] [Forgetting-MarI: LLM Unlearning via Marginal Information Regularization](https://arxiv.org/abs/2511.11914)
*Shizhou Xu,Yuan Ni,Stefan Broecker,Thomas Strohmer*

Main category: cs.AI

TL;DR: Forgetting-MarI是一个LLM遗忘框架，通过惩罚边际信息来选择性移除特定数据的影响，同时保留其他数据的信息，提供可证明的不可检测性。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型在不断扩大数据集上训练，需要选择性移除特定数据的影响以满足隐私保护和法规合规要求，避免从头重新训练资源密集型模型。

Method: 引入Forgetting-MarI框架，通过惩罚边际信息来移除待遗忘数据的额外信息贡献，同时保留待保留数据支持的信息。

Result: 实验表明该方法优于现有最先进的遗忘方法，实现了可靠的遗忘效果并更好地保持了模型的通用性能。

Conclusion: 这一进展使AI系统在遵守隐私和版权法规的同时不损害其有效性，向更可控和合规的AI系统迈出了重要一步。

Abstract: As AI models are trained on ever-expanding datasets, the ability to remove the influence of specific data from trained models has become essential for privacy protection and regulatory compliance. Unlearning addresses this challenge by selectively removing parametric knowledge from the trained models without retraining from scratch, which is critical for resource-intensive models such as Large Language Models (LLMs). Existing unlearning methods often degrade model performance by removing more information than necessary when attempting to ''forget'' specific data. We introduce Forgetting-MarI, an LLM unlearning framework that provably removes only the additional (marginal) information contributed by the data to be unlearned, while preserving the information supported by the data to be retained. By penalizing marginal information, our method yields an explicit upper bound on the unlearn dataset's residual influence in the trained models, providing provable undetectability. Extensive experiments confirm that our approach outperforms current state-of-the-art unlearning methods, delivering reliable forgetting and better preserved general model performance across diverse benchmarks. This advancement represents an important step toward making AI systems more controllable and compliant with privacy and copyright regulations without compromising their effectiveness.

</details>


### [102] [An Analysis of Architectural Impact on LLM-based Abstract Visual Reasoning: A Systematic Benchmark on RAVEN-FAIR](https://arxiv.org/abs/2511.11916)
*Sinan Urgun,Seçkin Arı*

Main category: cs.AI

TL;DR: 评估四种LLM模型在抽象视觉推理任务中的表现，使用四种推理架构在RAVEN-FAIR数据集上进行测试，发现GPT-4.1-Mini表现最佳，推理效果具有模型特异性。


<details>
  <summary>Details</summary>
Motivation: 系统评估大型语言模型在抽象视觉推理问题中的性能表现，探索不同推理架构对模型表现的影响。

Method: 使用四种LLM模型（GPT-4.1-Mini、Claude-3.5-Haiku、Gemini-1.5-Flash、Llama-3.3-70b）和四种推理架构（单次推理、嵌入控制重复、自我反思、多代理），在RAVEN-FAIR数据集上进行测试，采用三阶段处理流程（JSON提取、LLM推理、工具函数），使用SSIM和LPIPS指标评估视觉响应。

Result: GPT-4.1-Mini在所有架构中始终获得最高准确率，表现出强大的推理能力。多代理架构偶尔会改变语义和数值平衡，但效果不一致。每个模型对架构设计表现出不同的敏感性模式。

Conclusion: 推理有效性具有模型特异性，响应覆盖度的变化使跨架构直接比较复杂化。采用五次独立运行的最佳结果来估计性能上限，避免单次评估的脆弱性。

Abstract: This study aims to systematically evaluate the performance of large language models (LLMs) in abstract visual reasoning problems. We examined four LLM models (GPT-4.1-Mini, Claude-3.5-Haiku, Gemini-1.5-Flash, Llama-3.3-70b) utilizing four different reasoning architectures (single-shot, embedding-controlled repetition, self-reflection, and multi-agent) on the RAVEN-FAIR dataset. Visual responses generated through a three-stage process (JSON extraction, LLM reasoning, and Tool Function) were evaluated using SSIM and LPIPS metrics; Chain-of-Thought scores and error types (semantic hallucination, numeric misperception) were analyzed. Results demonstrate that GPT-4.1-Mini consistently achieved the highest overall accuracy across all architectures, indicating a strong reasoning capability. While the multi-agent architecture occasionally altered semantic and numeric balance across models, these effects were not uniformly beneficial. Instead, each model exhibited distinct sensitivity patterns to architectural design, underscoring that reasoning effectiveness remains model-specific. Variations in response coverage further emerged as a confounding factor that complicates direct cross-architecture comparison. To estimate the upper-bound performance of each configuration, we report the best of five independent runs, representing a best-case scenario rather than an averaged outcome. This multi-run strategy aligns with recent recommendations, which emphasize that single-run evaluations are fragile and may lead to unreliable conclusions.

</details>


### [103] [Looking Forward: Challenges and Opportunities in Agentic AI Reliability](https://arxiv.org/abs/2511.11921)
*Liudong Xing,Janet,Lin*

Main category: cs.AI

TL;DR: 本章讨论了构建可靠AI系统（特别是智能体AI系统）面临的挑战和未来发展方向，包括级联故障风险缓解、动态环境、任务执行不一致性、不可预测的涌现行为等研究问题。


<details>
  <summary>Details</summary>
Motivation: 随着智能体AI系统的广泛应用，确保其可靠性变得至关重要。当前存在级联故障风险、动态环境适应性不足、任务执行不一致、涌现行为不可预测等问题，需要系统性的解决方案。

Method: 通过分析现有研究问题，提出多个研究方向：级联故障风险缓解、动态环境适应性、任务执行一致性、涌现行为预测、资源高效可靠性机制、测试评估方法等。

Result: 识别了智能体AI系统可靠性的关键挑战领域，包括动态环境适应性、任务执行不一致性、不可预测的涌现行为等，并提出了相应的研究方向和解决方案框架。

Conclusion: 构建可靠的智能体AI系统需要在多个层面进行深入研究，包括故障预防、环境适应性、行为预测和高效测试评估方法，这些研究方向对推动AI系统的实际应用具有重要意义。

Abstract: This chapter presents perspectives for challenges and future development in building reliable AI systems, particularly, agentic AI systems. Several open research problems related to mitigating the risks of cascading failures are discussed. The chapter also sheds lights on research challenges and opportunities in aspects including dynamic environments, inconsistent task execution, unpredictable emergent behaviors, as well as resource-intensive reliability mechanisms. In addition, several research directions along the line of testing and evaluating reliability of agentic AI systems are also discussed.

</details>


### [104] [A Neuromorphic Architecture for Scalable Event-Based Control](https://arxiv.org/abs/2511.11924)
*Yongkang Huo,Fulvio Forni,Rodolphe Sepulchre*

Main category: cs.AI

TL;DR: 本文提出了"反弹赢家通吃(RWTA)"基元作为可扩展神经形态控制架构的基本元素，结合了离散计算的可靠性和连续调节的可调性。


<details>
  <summary>Details</summary>
Motivation: 构建一个能够统一处理连续节律生成和离散决策的神经形态控制架构，结合离散计算的可靠性和连续调节的可调性。

Method: 使用反弹赢家通吃(RWTA)基元作为基本构建块，从细胞层面到系统层面构建架构，继承赢家通吃状态机的离散计算能力和可兴奋生物物理电路的连续调节能力。

Result: 开发了一个基于事件的框架，能够统一处理连续节律生成和离散决策，并通过蛇形机器人神经系统设计展示了该架构的通用性、鲁棒性和模块化特性。

Conclusion: RWTA架构提供了一个统一的物理建模语言，成功结合了离散计算和连续调节的优势，在神经形态控制系统中具有广泛的应用潜力。

Abstract: This paper introduces the ``rebound Winner-Take-All (RWTA)" motif as the basic element of a scalable neuromorphic control architecture. From the cellular level to the system level, the resulting architecture combines the reliability of discrete computation and the tunability of continuous regulation: it inherits the discrete computation capabilities of winner-take-all state machines and the continuous tuning capabilities of excitable biophysical circuits. The proposed event-based framework addresses continuous rhythmic generation and discrete decision-making in a unified physical modeling language. We illustrate the versatility, robustness, and modularity of the architecture through the nervous system design of a snake robot.

</details>


### [105] [Augmenting The Weather: A Hybrid Counterfactual-SMOTE Algorithm for Improving Crop Growth Prediction When Climate Changes](https://arxiv.org/abs/2511.11945)
*Mohammed Temraz,Mark T Keane*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In recent years, humanity has begun to experience the catastrophic effects of climate change as economic sectors (such as agriculture) struggle with unpredictable and extreme weather events. Artificial Intelligence (AI) should help us handle these climate challenges but its most promising solutions are not good at dealing with climate-disrupted data; specifically, machine learning methods that work from historical data-distributions, are not good at handling out-of-distribution, outlier events. In this paper, we propose a novel data augmentation method, that treats the predictive problems around climate change as being, in part, due to class-imbalance issues; that is, prediction from historical datasets is difficult because, by definition, they lack sufficient minority-class instances of "climate outlier events". This novel data augmentation method -- called Counterfactual-Based SMOTE (CFA-SMOTE) -- combines an instance-based counterfactual method from Explainable AI (XAI) with the well-known class-imbalance method, SMOTE. CFA-SMOTE creates synthetic data-points representing outlier, climate-events that augment the dataset to improve predictive performance. We report comparative experiments using this CFA-SMOTE method, comparing it to benchmark counterfactual and class-imbalance methods under different conditions (i.e., class-imbalance ratios). The focal climate-change domain used relies on predicting grass growth on Irish dairy farms, during Europe-wide drought and forage crisis of 2018.

</details>


### [106] [LLM-Assisted Formalization Enables Deterministic Detection of Statutory Inconsistency in the Internal Revenue Code](https://arxiv.org/abs/2511.11954)
*Borchuluun Yadamsuren,Steven Keith Platt,Miguel Diaz*

Main category: cs.AI

TL;DR: 本文提出了一种混合神经符号框架，结合大型语言模型和符号逻辑，实现了对复杂法律中法定不一致性的确定性检测。以美国国内税收法典为案例研究，展示了该框架在检测不一致条款方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在税务领域的应用较少，且在处理层次化处理和深度结构化推理方面存在困难，特别是在长文本上。需要一种能够确定性检测法律不一致性的可靠方法。

Method: 使用GPT-4o将税法条款翻译为Prolog规则，在SWISH中精炼，然后通过Prolog增强提示测试不一致性检测。同时开发了混合Prolog模型，由GPT-5指导精炼，形式化竞争性解释并检测不一致区域。

Result: GPT-4o单独或Prolog增强提示下仅在三项策略中检测到一项不一致（33%准确率）。但混合Prolog模型产生确定性、可重现结果，成功检测到不一致区域，验证测试确认其准确、内部一致且能自主识别不一致性。

Conclusion: 基于符号逻辑的LLM辅助形式化能够实现透明可靠的法定不一致性检测，混合神经符号框架在法律分析中具有重要应用价值。

Abstract: This study introduces a hybrid neuro-symbolic framework that achieves deterministic detection of statutory inconsistency in complex law. We use the U.S. Internal Revenue Code (IRC) as a case study because its complexity makes it a fertile domain for identifying conflicts. Our research offers a solution for detecting inconsistent provisions by combining Large Language Models (LLMs) with symbolic logic.
  LLM-based methods can support compliance, fairness, and statutory drafting, yet tax-specific applications remain sparse. A key challenge is that such models struggle with hierarchical processing and deep structured reasoning, especially over long text.
  This research addresses these gaps through experiments using GPT-4o, GPT-5, and Prolog. GPT-4o was first used to translate Section 121 into Prolog rules and refine them in SWISH. These rules were then incorporated into prompts to test whether Prolog-augmented prompting improved GPT-4o's inconsistency detection. GPT-4o, whether prompted with natural language alone or with Prolog augmentation, detected the inconsistency in only one of three strategies (33 percent accuracy), but its reasoning quality differed: natural-language prompting achieved 100 percent rule coverage, while Prolog-augmented prompting achieved 66 percent, indicating more incomplete statutory analysis.
  In contrast to probabilistic prompting, the hybrid Prolog model produced deterministic and reproducible results. Guided by GPT-5 for refinement, the model formalized the IRC section's competing interpretations and successfully detected an inconsistency zone. Validation tests confirm that the Prolog implementation is accurate, internally consistent, deterministic, and capable of autonomously identifying inconsistencies. These findings show that LLM-assisted formalization, anchored in symbolic logic, enables transparent and reliable statutory inconsistency detection.

</details>


### [107] [Improving Autoformalization Using Direct Dependency Retrieval](https://arxiv.org/abs/2511.11990)
*Shaoqi Wang,Lu Yu,Chunjie Yang*

Main category: cs.AI

TL;DR: 提出基于直接依赖检索(DDR)的新框架，解决数学陈述自动形式化中上下文意识不足和依赖检索精度低的问题，显著提升检索精度和召回率。


<details>
  <summary>Details</summary>
Motivation: 现有自动形式化方法缺乏上下文意识，导致形式定义和定理的幻觉，且传统检索增强方法在形式库依赖检索上精度和召回率差，难以有效利用大规模公共数据集。

Method: 提出DDR方法：直接从自然语言数学描述生成候选库依赖，通过高效后缀数组检查验证其在形式库中的存在性，构建超过50万样本的依赖检索数据集并微调高精度DDR模型。

Result: DDR模型在检索精度和召回率上显著优于SOTA方法，配备DDR的自动形式化器在单次尝试准确率和多次尝试稳定性方面均优于传统基于选择的RAG方法。

Conclusion: DDR框架通过高效的依赖检索机制有效解决了自动形式化中的上下文意识和可扩展性问题，为深度学习与形式数学的融合提供了可靠的技术支撑。

Abstract: The convergence of deep learning and formal mathematics has spurred research in formal verification. Statement autoformalization, a crucial first step in this process, aims to translate informal descriptions into machine-verifiable representations but remains a significant challenge. The core difficulty lies in the fact that existing methods often suffer from a lack of contextual awareness, leading to hallucination of formal definitions and theorems. Furthermore, current retrieval-augmented approaches exhibit poor precision and recall for formal library dependency retrieval, and lack the scalability to effectively leverage ever-growing public datasets. To bridge this gap, we propose a novel retrieval-augmented framework based on DDR (\textit{Direct Dependency Retrieval}) for statement autoformalization. Our DDR method directly generates candidate library dependencies from natural language mathematical descriptions and subsequently verifies their existence within the formal library via an efficient suffix array check. Leveraging this efficient search mechanism, we constructed a dependency retrieval dataset of over 500,000 samples and fine-tuned a high-precision DDR model. Experimental results demonstrate that our DDR model significantly outperforms SOTA methods in both retrieval precision and recall. Consequently, an autoformalizer equipped with DDR shows consistent performance advantages in both single-attempt accuracy and multi-attempt stability compared to models using traditional selection-based RAG methods.

</details>


### [108] [Look As You Think: Unifying Reasoning and Visual Evidence Attribution for Verifiable Document RAG via Reinforcement Learning](https://arxiv.org/abs/2511.12003)
*Shuochen Liu,Pengfei Luo,Chao Zhang,Yuhao Chen,Haotian Zhang,Qi Liu,Xin Kou,Tong Xu,Enhong Chen*

Main category: cs.AI

TL;DR: 提出Chain-of-Evidence（CoE）范式和Look As You Think（LAT）强化学习框架，用于视觉文档检索增强生成中的证据归因，通过边界框和页码索引将推理步骤与具体视觉区域关联，提高答案的可验证性。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏细粒度监督和推理过程的渐进可追溯性，无法确保视觉语言模型在多模态问答中产生可靠且可验证的预测。

Method: CoE统一了思维链推理和视觉证据归因，LAT通过强化学习训练模型生成具有一致归因的可验证推理路径，评估证据区域的一致性并在正确时提供奖励。

Result: 在Qwen2.5-VL-7B-Instruct模型上，LAT在单图和多图设置下均表现优异，soft EM平均提升8.23%，IoU@0.5提升47.0%，优于监督微调基线并展现出更强的跨领域泛化能力。

Conclusion: LAT框架有效提升了视觉文档检索增强生成中的证据归因能力，实现了过程级自验证，为可靠的多模态问答提供了新范式。

Abstract: Aiming to identify precise evidence sources from visual documents, visual evidence attribution for visual document retrieval-augmented generation (VD-RAG) ensures reliable and verifiable predictions from vision-language models (VLMs) in multimodal question answering. Most existing methods adopt end-to-end training to facilitate intuitive answer verification. However, they lack fine-grained supervision and progressive traceability throughout the reasoning process. In this paper, we introduce the Chain-of-Evidence (CoE) paradigm for VD-RAG. CoE unifies Chain-of-Thought (CoT) reasoning and visual evidence attribution by grounding reference elements in reasoning steps to specific regions with bounding boxes and page indexes. To enable VLMs to generate such evidence-grounded reasoning, we propose Look As You Think (LAT), a reinforcement learning framework that trains models to produce verifiable reasoning paths with consistent attribution. During training, LAT evaluates the attribution consistency of each evidence region and provides rewards only when the CoE trajectory yields correct answers, encouraging process-level self-verification. Experiments on vanilla Qwen2.5-VL-7B-Instruct with Paper- and Wiki-VISA benchmarks show that LAT consistently improves the vanilla model in both single- and multi-image settings, yielding average gains of 8.23% in soft exact match (EM) and 47.0% in IoU@0.5. Meanwhile, LAT not only outperforms the supervised fine-tuning baseline, which is trained to directly produce answers with attribution, but also exhibits stronger generalization across domains.

</details>


### [109] [Adaptive Diagnostic Reasoning Framework for Pathology with Multimodal Large Language Models](https://arxiv.org/abs/2511.12008)
*Yunqi Hong,Johnson Kao,Liam Edwards,Nein-Tzu Liu,Chung-Yen Huang,Alex Oliveira-Kowaleski,Cho-Jui Hsieh,Neil Y. C. Lin*

Main category: cs.AI

TL;DR: RECAP-PATH是一个可解释的病理AI框架，通过两阶段自学习过程从多模态大语言模型中自主推导诊断标准，无需白盒访问或权重更新即可生成癌症诊断。


<details>
  <summary>Details</summary>
Motivation: 当前病理AI系统缺乏人类可读的推理过程，限制了临床应用，因为无法审计决策和防止错误。

Method: 采用两阶段自学习过程：多样化阶段扩展病理学风格解释，优化阶段为准确性精炼解释。仅需少量标注数据，无需白盒访问或权重更新。

Result: 在乳腺癌和前列腺癌数据集上评估，RECAP-PATH生成的推理与专家评估一致，诊断准确性显著优于基线方法。

Conclusion: RECAP-PATH通过结合视觉理解和推理，提供了临床可信的AI，展示了证据关联解释的通用路径。

Abstract: AI tools in pathology have improved screening throughput, standardized quantification, and revealed prognostic patterns that inform treatment. However, adoption remains limited because most systems still lack the human-readable reasoning needed to audit decisions and prevent errors. We present RECAP-PATH, an interpretable framework that establishes a self-learning paradigm, shifting off-the-shelf multimodal large language models from passive pattern recognition to evidence-linked diagnostic reasoning. At its core is a two-phase learning process that autonomously derives diagnostic criteria: diversification expands pathology-style explanations, while optimization refines them for accuracy. This self-learning approach requires only small labeled sets and no white-box access or weight updates to generate cancer diagnoses. Evaluated on breast and prostate datasets, RECAP-PATH produced rationales aligned with expert assessment and delivered substantial gains in diagnostic accuracy over baselines. By uniting visual understanding with reasoning, RECAP-PATH provides clinically trustworthy AI and demonstrates a generalizable path toward evidence-linked interpretation.

</details>


### [110] [Intelligent Collaborative Optimization for Rubber Tyre Film Production Based on Multi-path Differentiated Clipping Proximal Policy Optimization](https://arxiv.org/abs/2511.12060)
*Yinghao Ruan,Wei Pang,Shuaihao Liu,Huili Yang,Leyi Han,Xinghui Dong*

Main category: cs.AI

TL;DR: 提出MPD-PPO深度强化学习算法，用于解决轮胎制造中的高维多目标优化问题，在橡胶薄膜生产的宽度和厚度控制上显著提升了调谐精度和操作效率。


<details>
  <summary>Details</summary>
Motivation: 智能制造需要解决传统集中调度和生产线配置的局限性，特别是应对动态生产需求。轮胎制造系统形成复杂的网络，具有非线性相互作用和涌现动态，多子系统协调成为关键挑战。

Method: 引入多路径差异化裁剪近端策略优化算法（MPD-PPO），采用多分支策略架构和差异化梯度裁剪约束，确保高维策略更新的稳定性和效率。

Result: 在橡胶轮胎薄膜生产的宽度和厚度控制实验中，MPD-PPO在调谐精度和操作效率方面均表现出显著改进。

Conclusion: 该框架成功解决了高维度、多目标权衡和动态适应等关键挑战，为轮胎制造中的实时工业部署提供了增强的性能和生产稳定性。

Abstract: The advent of smart manufacturing is addressing the limitations of traditional centralized scheduling and inflexible production line configurations in the rubber tyre industry, especially in terms of coping with dynamic production demands. Contemporary tyre manufacturing systems form complex networks of tightly coupled subsystems pronounced nonlinear interactions and emergent dynamics. This complexity renders the effective coordination of multiple subsystems, posing an essential yet formidable task. For high-dimensional, multi-objective optimization problems in this domain, we introduce a deep reinforcement learning algorithm: Multi-path Differentiated Clipping Proximal Policy Optimization (MPD-PPO). This algorithm employs a multi-branch policy architecture with differentiated gradient clipping constraints to ensure stable and efficient high-dimensional policy updates. Validated through experiments on width and thickness control in rubber tyre film production, MPD-PPO demonstrates substantial improvements in both tuning accuracy and operational efficiency. The framework successfully tackles key challenges, including high dimensionality, multi-objective trade-offs, and dynamic adaptation, thus delivering enhanced performance and production stability for real-time industrial deployment in tyre manufacturing.

</details>


### [111] [Bayesian Optimization in Language Space: An Eval-Efficient AI Self-Improvement Framework](https://arxiv.org/abs/2511.12063)
*Enoch Hyunwook Kang,Hema Yoganarasimhan*

Main category: cs.AI

TL;DR: 提出了T-BoN BO框架，将贝叶斯优化扩展到语言领域，通过Best-of-N选择和文本梯度来优化评估效率，在广告对齐任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有自改进AI主要关注查询效率，但在许多社会应用中，评估成本远高于生成成本，需要优化评估效率。

Method: 结合Best-of-N选择和文本梯度来模拟UCB采集函数的梯度行为，提出T-BoN BO框架进行语言空间的贝叶斯优化。

Result: 在自动化广告对齐任务中，T-BoN BO相比现有最优基线表现出更优越的性能。

Conclusion: T-BoN BO为AI自改进提供了一种简单且评估效率高的语言空间贝叶斯优化框架，解决了评估成本高的问题。

Abstract: Large Language Models (LLMs) have recently enabled self-improving AI, i.e., AI that iteratively generates, evaluates, and refines its own outcomes. Recent studies have shown that self-improving AI focusing on prompt optimization can outperform state-of-the-art reinforcement-learning fine-tuned LLMs. Here, their `performance' is typically measured by query efficiency - the number of LLM-generated solution samples required to meet a certain performance threshold. However, in many societal applications, the primary limitation is not generating new solutions but evaluating them. For instance, evaluating an ad's effectiveness requires significant human feedback, which is far more costly and time-consuming than generating a candidate ad. To optimize for the evaluation efficiency objective, a natural approach is to extend Bayesian Optimization (BO), a framework proven optimal for evaluation efficiency, to the language domain. However, the difficulty of directly estimating suitable acquisition functions in LLMs' minds makes this extension challenging. This paper overcomes this challenge by proving that the combination of the simple and widely used Best-of-N selection strategy and simple textual gradients (i.e., textual edits from a critic model) statistically emulates the behavior of the gradients on the canonical UCB acquisition function, which induces optimal exploration in terms of evaluation efficiency. Based on this result, we propose TextGrad-Best-of-N Bayesian Optimization (T-BoN BO), a simple and eval-efficient language-space Bayesian optimization framework for AI self-improvement. We also empirically validate T-BoN BO by applying it to automated ad alignment tasks for persona distribution, demonstrating its superior performance compared to popular state-of-the-art baselines.

</details>


### [112] [No-Regret Strategy Solving in Imperfect-Information Games via Pre-Trained Embedding](https://arxiv.org/abs/2511.12083)
*Yanchang Fu,Shengda Liu,Pei Xu,Kaiqi Huang*

Main category: cs.AI

TL;DR: 提出了Embedding CFR算法，通过将信息集嵌入到低维连续空间来解决大规模不完全信息扩展式博弈，相比传统聚类方法能更精确捕捉信息集间的差异和联系，在扑克实验中实现了更快的可剥削性收敛。


<details>
  <summary>Details</summary>
Motivation: 现有AI方法依赖预训练的离散聚类进行抽象，但硬分类会不可逆地丢失信息集之间的量化细微差异，这些差异对策略求解至关重要，从而影响求解质量。

Method: 受自然语言处理中词嵌入范式的启发，提出Embedding CFR算法：预训练并将孤立信息集的特征嵌入到相互连接的低维连续空间，在该嵌入空间中进行遗憾累积和策略更新的策略求解过程。

Result: 在扑克实验中，在相同空间开销下，Embedding CFR相比基于聚类的抽象算法实现了显著更快的可剥削性收敛，证实了其有效性。

Conclusion: 这是扑克AI中首个通过低维嵌入预训练信息集抽象来进行策略求解的算法，理论分析验证了其减少累积遗憾的能力。

Abstract: High-quality information set abstraction remains a core challenge in solving large-scale imperfect-information extensive-form games (IIEFGs)-such as no-limit Texas Hold'em-where the finite nature of spatial resources hinders strategy solving over the full game. State-of-the-art AI methods rely on pre-trained discrete clustering for abstraction, yet their hard classification irreversibly loses critical information: specifically, the quantifiable subtle differences between information sets-vital for strategy solving-thereby compromising the quality of such solving. Inspired by the word embedding paradigm in natural language processing, this paper proposes the Embedding CFR algorithm, a novel approach for solving strategies in IIEFGs within an embedding space. The algorithm pre-trains and embeds features of isolated information sets into an interconnected low-dimensional continuous space, where the resulting vectors more precisely capture both the distinctions and connections between information sets. Embedding CFR presents a strategy-solving process driven by regret accumulation and strategy updates within this embedding space, with accompanying theoretical analysis verifying its capacity to reduce cumulative regret. Experiments on poker show that with the same spatial overhead, Embedding CFR achieves significantly faster exploitability convergence compared to cluster-based abstraction algorithms, confirming its effectiveness. Furthermore, to our knowledge, it is the first algorithm in poker AI that pre-trains information set abstractions through low-dimensional embedding for strategy solving.

</details>


### [113] [KrwEmd: Revising the Imperfect-Recall Abstraction from Forgetting Everything](https://arxiv.org/abs/2511.12089)
*Yanchang Fu,Qiyue Yin,Shengda Liu,Pei Xu,Kaiqi Huang*

Main category: cs.AI

TL;DR: KrwEmd算法通过k-recall winrate特征和earth mover's distance聚类，解决了德州扑克等游戏中因过度抽象导致AI性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 解决大规模不完全信息游戏中因极端不完美回忆抽象导致的过度抽象问题，这种抽象完全丢弃历史信息，损害AI性能。

Method: 引入k-recall winrate特征，利用未来和关键的历史游戏信息来区分信号观察信息集；开发KrwEmd算法，使用earth mover's distance测量特征差异来聚类信号观察信息集。

Result: 实验结果表明，与现有算法相比，KrwEmd显著提高了AI游戏性能。

Conclusion: KrwEmd是第一个实用的解决过度抽象问题的算法，通过结合历史和未来信息有效提升了不完全信息游戏中AI的表现。

Abstract: Excessive abstraction is a critical challenge in hand abstraction-a task specific to games like Texas hold'em-when solving large-scale imperfect-information games, as it impairs AI performance. This issue arises from extreme implementations of imperfect-recall abstraction, which entirely discard historical information. This paper presents KrwEmd, the first practical algorithm designed to address this problem. We first introduce the k-recall winrate feature, which not only qualitatively distinguishes signal observation infosets by leveraging both future and, crucially, historical game information, but also quantitatively captures their similarity. We then develop the KrwEmd algorithm, which clusters signal observation infosets using earth mover's distance to measure discrepancies between their features. Experimental results demonstrate that KrwEmd significantly improves AI gameplay performance compared to existing algorithms.

</details>


### [114] [MetaGDPO: Alleviating Catastrophic Forgetting with Metacognitive Knowledge through Group Direct Preference Optimization](https://arxiv.org/abs/2511.12113)
*Lanxue Zhang,Yuqiang Xie,Fang Fang,Fanglong Dong,Rui Liu,Yanan Cao*

Main category: cs.AI

TL;DR: 提出了一种解决小模型灾难性遗忘问题的综合方案，包括构建包含元认知知识的5K数据集和GDPO训练方法，显著提升小模型推理性能


<details>
  <summary>Details</summary>
Motivation: 现有数据集和微调方法导致小模型（小于8B）出现灾难性遗忘，主要问题是忽视训练数据知识与模型固有能力的关系，以及传统训练目标无法约束固有知识保留

Method: 1) 构建包含元认知知识标注的5K多任务推理数据集；2) 提出GDPO（Group Direction Preference Optimization）训练方法，通过参考模型隐式约束优化路径

Result: 大量实验表明该方法显著缓解了灾难性遗忘问题，提升了小模型的推理性能

Conclusion: 该综合方案从数据和训练方法两方面有效解决了小模型的知识蒸馏问题，为资源受限场景提供了高效解决方案

Abstract: Large Language Models demonstrate strong reasoning capabilities, which can be effectively compressed into smaller models. However, existing datasets and fine-tuning approaches still face challenges that lead to catastrophic forgetting, particularly for models smaller than 8B. First, most datasets typically ignore the relationship between training data knowledge and the model's inherent abilities, making it difficult to preserve prior knowledge. Second, conventional training objectives often fail to constrain inherent knowledge preservation, which can result in forgetting of previously learned skills. To address these issues, we propose a comprehensive solution that alleviates catastrophic forgetting from both the data and fine-tuning approach perspectives. On the data side, we construct a dataset of 5K instances that covers multiple reasoning tasks and incorporates metacognitive knowledge, making it more tolerant and effective for distillation into smaller models. We annotate the metacognitive knowledge required to solve each question and filter the data based on task knowledge and the model's inherent skills. On the training side, we introduce GDPO (Group Direction Preference Optimization), which is better suited for resource-limited scenarios and can efficiently approximate the performance of GRPO. Guided by the large model and by implicitly constraining the optimization path through a reference model, GDPO enables more effective knowledge transfer from the large model and constrains excessive parameter drift. Extensive experiments demonstrate that our approach significantly alleviates catastrophic forgetting and improves reasoning performance on smaller models.

</details>


### [115] [RTMol: Rethinking Molecule-text Alignment in a Round-trip View](https://arxiv.org/abs/2511.12135)
*Letian Chen,Runhan Shi,Gufeng Yu,Yang Yang*

Main category: cs.AI

TL;DR: RTMol是一个双向对齐框架，通过自监督的往返学习统一分子标注和文本到SMILES生成，解决了现有方法在化学准确性、数据质量和双向一致性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将分子标注和文本到分子设计视为独立任务，面临三个关键限制：传统指标偏重语言流畅性而非化学准确性、训练数据包含化学模糊描述、独立优化导致双向不一致。

Method: 提出RTMol双向对齐框架，通过自监督往返学习统一分子标注和文本到SMILES生成，引入新颖的往返评估指标，支持无需配对分子-文本语料的无监督训练。

Result: 实验表明RTMol在各种LLMs上将双向对齐性能提升高达47%，为联合分子-文本理解和生成建立了有效范式。

Conclusion: RTMol框架有效解决了分子序列表示与文本描述对齐的关键问题，在药物发现、材料设计和化学文献分析等应用中具有重要价值。

Abstract: Aligning molecular sequence representations (e.g., SMILES notations) with textual descriptions is critical for applications spanning drug discovery, materials design, and automated chemical literature analysis. Existing methodologies typically treat molecular captioning (molecule-to-text) and text-based molecular design (text-to-molecule) as separate tasks, relying on supervised fine-tuning or contrastive learning pipelines. These approaches face three key limitations: (i) conventional metrics like BLEU prioritize linguistic fluency over chemical accuracy, (ii) training datasets frequently contain chemically ambiguous narratives with incomplete specifications, and (iii) independent optimization of generation directions leads to bidirectional inconsistency. To address these issues, we propose RTMol, a bidirectional alignment framework that unifies molecular captioning and text-to-SMILES generation through self-supervised round-trip learning. The framework introduces novel round-trip evaluation metrics and enables unsupervised training for molecular captioning without requiring paired molecule-text corpora. Experiments demonstrate that RTMol enhances bidirectional alignment performance by up to 47% across various LLMs, establishing an effective paradigm for joint molecule-text understanding and generation.

</details>


### [116] [Incremental Maintenance of DatalogMTL Materialisations](https://arxiv.org/abs/2511.12169)
*Kaiyue Zhao,Dingqi Chen,Shaoyu Wang,Pan Hu*

Main category: cs.AI

TL;DR: 提出了DRedMTL算法，一种支持有界区间的DatalogMTL增量推理方法，显著优于重新物化方法


<details>
  <summary>Details</summary>
Motivation: 现有DatalogMTL推理方法不支持高效动态更新，而现实应用需要频繁数据更新

Method: 基于经典DRed算法，设计专门操作符处理DatalogMTL物化的周期性表示

Result: 在多个公开数据集上测试，DRedMTL通常显著优于重新物化，有时提升数个数量级

Conclusion: DRedMTL为DatalogMTL提供了高效的增量推理能力，满足现实应用的动态更新需求

Abstract: DatalogMTL extends the classical Datalog language with metric temporal logic (MTL), enabling expressive reasoning over temporal data. While existing reasoning approaches, such as materialisation based and automata based methods, offer soundness and completeness, they lack support for handling efficient dynamic updates, a crucial requirement for real-world applications that involve frequent data updates. In this work, we propose DRedMTL, an incremental reasoning algorithm for DatalogMTL with bounded intervals. Our algorithm builds upon the classical DRed algorithm, which incrementally updates the materialisation of a Datalog program. Unlike a Datalog materialisation which is in essence a finite set of facts, a DatalogMTL materialisation has to be represented as a finite set of facts plus periodic intervals indicating how the full materialisation can be constructed through unfolding. To cope with this, our algorithm is equipped with specifically designed operators to efficiently handle such periodic representations of DatalogMTL materialisations. We have implemented this approach and tested it on several publicly available datasets. Experimental results show that DRedMTL often significantly outperforms rematerialisation, sometimes by orders of magnitude.

</details>


### [117] [Debate over Mixed-knowledge: A Robust Multi-Agent Framework for Incomplete Knowledge Graph Question Answering](https://arxiv.org/abs/2511.12208)
*Jilong Liu,Pengyang Shao,Wei Qin,Fei Liu,Yonghui Yang,Richang Hong*

Main category: cs.AI

TL;DR: 提出了DoM框架，通过多智能体辩论机制动态融合结构化和非结构化知识来解决不完整知识图谱问答问题，并构建了更真实的不完整KG数据集。


<details>
  <summary>Details</summary>
Motivation: 现实世界知识图谱通常不完整，现有方法无法自适应地融合多源知识，无法充分利用知识的互补性。

Method: DoM框架基于多智能体辩论范式，分配专门智能体分别处理知识图谱和外部文本，通过迭代交互协调输出。分解问题、双智能体检索证据、法官智能体评估聚合答案。

Result: 通过广泛实验表明，DoM始终优于最先进的基线方法。

Conclusion: DoM框架通过知识互补性增强了KG不完整性的鲁棒性，为不完整KGQA提供了有效解决方案。

Abstract: Knowledge Graph Question Answering (KGQA) aims to improve factual accuracy by leveraging structured knowledge. However, real-world Knowledge Graphs (KGs) are often incomplete, leading to the problem of Incomplete KGQA (IKGQA). A common solution is to incorporate external data to fill knowledge gaps, but existing methods lack the capacity to adaptively and contextually fuse multiple sources, failing to fully exploit their complementary strengths. To this end, we propose Debate over Mixed-knowledge (DoM), a novel framework that enables dynamic integration of structured and unstructured knowledge for IKGQA. Built upon the Multi-Agent Debate paradigm, DoM assigns specialized agents to perform inference over knowledge graphs and external texts separately, and coordinates their outputs through iterative interaction. It decomposes the input question into sub-questions, retrieves evidence via dual agents (KG and Retrieval-Augmented Generation, RAG), and employs a judge agent to evaluate and aggregate intermediate answers. This collaboration exploits knowledge complementarity and enhances robustness to KG incompleteness. In addition, existing IKGQA datasets simulate incompleteness by randomly removing triples, failing to capture the irregular and unpredictable nature of real-world knowledge incompleteness. To address this, we introduce a new dataset, Incomplete Knowledge Graph WebQuestions, constructed by leveraging real-world knowledge updates. These updates reflect knowledge beyond the static scope of KGs, yielding a more realistic and challenging benchmark. Through extensive experiments, we show that DoM consistently outperforms state-of-the-art baselines.

</details>


### [118] [ViTE: Virtual Graph Trajectory Expert Router for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2511.12214)
*Ruochen Li,Zhanxing Zhu,Tanqiu Qiao,Hubert P. H. Shum*

Main category: cs.AI

TL;DR: ViTE框架通过虚拟图和专家路由器模块，自适应建模行人轨迹预测中的显式一跳交互和隐式高阶依赖，避免了深度GNN的计算成本问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在行人轨迹预测中面临深度与计算成本的权衡：层数不足导致感受野受限，层数过多则计算成本过高。需要能够自适应建模不同层次交互的方法。

Method: 提出ViTE框架，包含两个核心模块：虚拟图通过动态虚拟节点建模长距离高阶交互，无需深度GNN堆叠；专家路由器基于社交上下文自适应选择交互专家，采用混合专家设计。

Result: 在ETH/UCY、NBA和SDD三个基准测试中均达到最先进性能，验证了方法的有效性和实际效率。

Conclusion: ViTE框架能够灵活可扩展地处理不同交互模式，在行人轨迹预测任务中表现出色，解决了深度GNN的计算效率问题。

Abstract: Pedestrian trajectory prediction is critical for ensuring safety in autonomous driving, surveillance systems, and urban planning applications. While early approaches primarily focus on one-hop pairwise relationships, recent studies attempt to capture high-order interactions by stacking multiple Graph Neural Network (GNN) layers. However, these approaches face a fundamental trade-off: insufficient layers may lead to under-reaching problems that limit the model's receptive field, while excessive depth can result in prohibitive computational costs. We argue that an effective model should be capable of adaptively modeling both explicit one-hop interactions and implicit high-order dependencies, rather than relying solely on architectural depth. To this end, we propose ViTE (Virtual graph Trajectory Expert router), a novel framework for pedestrian trajectory prediction. ViTE consists of two key modules: a Virtual Graph that introduces dynamic virtual nodes to model long-range and high-order interactions without deep GNN stacks, and an Expert Router that adaptively selects interaction experts based on social context using a Mixture-of-Experts design. This combination enables flexible and scalable reasoning across varying interaction patterns. Experiments on three benchmarks (ETH/UCY, NBA, and SDD) demonstrate that our method consistently achieves state-of-the-art performance, validating both its effectiveness and practical efficiency.

</details>


### [119] [Beyond World Models: Rethinking Understanding in AI Models](https://arxiv.org/abs/2511.12239)
*Tarun Gupta,Danish Pruthi*

Main category: cs.AI

TL;DR: 本文使用科学哲学案例研究来批判性检验世界模型框架是否充分表征了人类水平的理解能力


<details>
  <summary>Details</summary>
Motivation: 研究AI模型中的世界模型表示是否能表明这些模型以类人方式"理解"世界，以及世界模型框架是否充分表征人类理解

Method: 使用科学哲学文献中的案例研究，重点关注世界模型能力与人类理解之间区别最明显的哲学分析

Result: 通过特定哲学观点探索了世界模型的局限性，表明世界模型框架可能不足以完全表征人类理解

Conclusion: 世界模型框架在表征人类水平理解方面存在局限性，需要更细致地考虑人类理解与AI世界模型之间的差异

Abstract: World models have garnered substantial interest in the AI community. These are internal representations that simulate aspects of the external world, track entities and states, capture causal relationships, and enable prediction of consequences. This contrasts with representations based solely on statistical correlations. A key motivation behind this research direction is that humans possess such mental world models, and finding evidence of similar representations in AI models might indicate that these models "understand" the world in a human-like way. In this paper, we use case studies from the philosophy of science literature to critically examine whether the world model framework adequately characterizes human-level understanding. We focus on specific philosophical analyses where the distinction between world model capabilities and human understanding is most pronounced. While these represent particular views of understanding rather than universal definitions, they help us explore the limits of world models.

</details>


### [120] [AURA: Development and Validation of an Augmented Unplanned Removal Alert System using Synthetic ICU Videos](https://arxiv.org/abs/2511.12241)
*Junhyuk Seo,Hyeyoon Moon,Kyu-Hwan Jung,Namkee Oh,Taerim Kim*

Main category: cs.AI

TL;DR: AURA系统使用合成视频数据开发视觉风险检测系统，通过姿态估计识别ICU患者的高风险动作模式（碰撞和躁动），实现隐私保护的实时非计划拔管检测。


<details>
  <summary>Details</summary>
Motivation: ICU中非计划拔管是严重的安全问题，但实时检测受限于伦理和隐私问题难以获取标注视频数据。

Method: 利用文本到视频扩散生成合成ICU视频数据集，通过姿态估计识别手部进入气道管附近区域（碰撞）和关键点速度（躁动）两种高风险模式。

Result: 专家确认合成数据真实性，碰撞检测准确率高，躁动识别性能中等。

Conclusion: 展示了开发隐私保护、可复现的患者安全监测系统的新途径，具有ICU部署潜力。

Abstract: Unplanned extubation (UE) remains a critical patient safety concern in intensive care units (ICUs), often leading to severe complications or death. Real-time UE detection has been limited, largely due to the ethical and privacy challenges of obtaining annotated ICU video data. We propose Augmented Unplanned Removal Alert (AURA), a vision-based risk detection system developed and validated entirely on a fully synthetic video dataset. By leveraging text-to-video diffusion, we generated diverse and clinically realistic ICU scenarios capturing a range of patient behaviors and care contexts. The system applies pose estimation to identify two high-risk movement patterns: collision, defined as hand entry into spatial zones near airway tubes, and agitation, quantified by the velocity of tracked anatomical keypoints. Expert assessments confirmed the realism of the synthetic data, and performance evaluations showed high accuracy for collision detection and moderate performance for agitation recognition. This work demonstrates a novel pathway for developing privacy-preserving, reproducible patient safety monitoring systems with potential for deployment in intensive care settings.

</details>


### [121] [Mobile-Agent-RAG: Driving Smart Multi-Agent Coordination with Contextual Knowledge Empowerment for Long-Horizon Mobile Automation](https://arxiv.org/abs/2511.12254)
*Yuxiang Zhou,Jichang Li,Yanhao Zhang,Haonan Lu,Guanbin Li*

Main category: cs.AI

TL;DR: Mobile-Agent-RAG是一个新颖的分层多代理框架，通过双重检索增强解决移动代理在真实世界长时跨应用任务中的性能瓶颈，显著提高了任务完成率和步骤效率。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的移动代理在真实世界长时跨应用任务中成功率不足，主要原因是过度依赖MLLM中的静态内部知识，导致高层规划中的战略幻觉和低层UI操作中的执行错误。

Method: 提出分层多代理框架Mobile-Agent-RAG，包含Manager-RAG用于规划阶段检索人类验证的全面任务计划以减少战略幻觉，Operator-RAG用于执行阶段检索最精确的低层指导以提高执行准确性，并构建了两个专门的检索导向知识库。

Result: 广泛实验表明，Mobile-Agent-RAG显著优于最先进的基线方法，任务完成率提高11.0%，步骤效率提高10.2%。

Conclusion: Mobile-Agent-RAG建立了一个用于上下文感知、可靠多代理移动自动化的稳健范式，通过双重检索增强有效解决了移动代理在真实世界任务中的关键挑战。

Abstract: Mobile agents show immense potential, yet current state-of-the-art (SoTA) agents exhibit inadequate success rates on real-world, long-horizon, cross-application tasks. We attribute this bottleneck to the agents' excessive reliance on static, internal knowledge within MLLMs, which leads to two critical failure points: 1) strategic hallucinations in high-level planning and 2) operational errors during low-level execution on user interfaces (UI). The core insight of this paper is that high-level planning and low-level UI operations require fundamentally distinct types of knowledge. Planning demands high-level, strategy-oriented experiences, whereas operations necessitate low-level, precise instructions closely tied to specific app UIs. Motivated by these insights, we propose Mobile-Agent-RAG, a novel hierarchical multi-agent framework that innovatively integrates dual-level retrieval augmentation. At the planning stage, we introduce Manager-RAG to reduce strategic hallucinations by retrieving human-validated comprehensive task plans that provide high-level guidance. At the execution stage, we develop Operator-RAG to improve execution accuracy by retrieving the most precise low-level guidance for accurate atomic actions, aligned with the current app and subtask. To accurately deliver these knowledge types, we construct two specialized retrieval-oriented knowledge bases. Furthermore, we introduce Mobile-Eval-RAG, a challenging benchmark for evaluating such agents on realistic multi-app, long-horizon tasks. Extensive experiments demonstrate that Mobile-Agent-RAG significantly outperforms SoTA baselines, improving task completion rate by 11.0% and step efficiency by 10.2%, establishing a robust paradigm for context-aware, reliable multi-agent mobile automation.

</details>


### [122] [MoralReason: Generalizable Moral Decision Alignment For LLM Agents Using Reasoning-Level Reinforcement Learning](https://arxiv.org/abs/2511.12271)
*Zhiyu An,Wan Du*

Main category: cs.AI

TL;DR: 该论文提出了一种让大语言模型学习并应用道德推理框架到新情境的方法，通过Moral-Reason-QA数据集和Group Relative Policy Optimization训练，实现了在未见道德场景中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前方法主要评估而非主动引导LLM的道德决策，需要解决分布外道德对齐问题，让LLM代理学习将一致的道德推理框架应用到超出训练分布的道德场景中。

Method: 创建Moral-Reason-QA数据集（680个人工标注的高模糊度道德场景），使用Group Relative Policy Optimization结合复合奖励，同时优化决策对齐和框架特定推理过程。

Result: 在分布外评估集上，功利主义框架的softmax归一化对齐分数提高了+0.757，道义论框架提高了+0.450，成功实现了对未见道德场景的泛化。

Conclusion: LLM代理可以系统性地训练以内在化并应用特定道德框架到新情境，为AI安全提供了关键基础，因为语言模型正越来越多地融入人类决策过程。

Abstract: Large language models are increasingly influencing human moral decisions, yet current approaches focus primarily on evaluating rather than actively steering their moral decisions. We formulate this as an out-of-distribution moral alignment problem, where LLM agents must learn to apply consistent moral reasoning frameworks to scenarios beyond their training distribution. We introduce Moral-Reason-QA, a novel dataset extending 680 human-annotated, high-ambiguity moral scenarios with framework-specific reasoning traces across utilitarian, deontological, and virtue ethics, enabling systematic evaluation of moral generalization in realistic decision contexts. Our learning approach employs Group Relative Policy Optimization with composite rewards that simultaneously optimize decision alignment and framework-specific reasoning processes to facilitate learning of the underlying moral frameworks. Experimental results demonstrate successful generalization to unseen moral scenarios, with softmax-normalized alignment scores improving by +0.757 for utilitarian and +0.450 for deontological frameworks when tested on out-of-distribution evaluation sets. The experiments also reveal training challenges and promising directions that inform future research. These findings establish that LLM agents can be systematically trained to internalize and apply specific moral frameworks to novel situations, providing a critical foundation for AI safety as language models become more integrated into human decision-making processes.

</details>


### [123] [UpBench: A Dynamically Evolving Real-World Labor-Market Agentic Benchmark Framework Built for Human-Centric AI](https://arxiv.org/abs/2511.12306)
*Darvin Yi,Teng Liu,Mattie Terzolo,Lance Hasson,Ayan Sinh,Pablo Mendes,Andrew Rabinovich*

Main category: cs.AI

TL;DR: UpBench是一个基于真实Upwork工作任务的动态基准测试，用于评估LLM代理在真实工作环境中的表现，采用专家制定的评分标准和人类评估相结合的方法。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试大多是静态、合成或领域受限的，无法充分评估AI代理在动态、经济意义环境中的真实表现和人类协作能力。

Method: 从Upwork平台提取真实工作任务，由专业自由职业者制定详细可验证的验收标准，采用基于评分标准的评估框架对AI提交内容进行细粒度评估。

Result: 建立了基于真实工作活动的评估框架，能够分析模型优势、弱点和指令遵循能力，超越简单的通过/失败指标。

Conclusion: UpBench提供了一个可扩展、以人为本的基础，用于在真实劳动力市场环境中评估代理系统，支持AI通过合作而非替代来增强人类能力的研究。

Abstract: As large language model (LLM) agents increasingly undertake digital work, reliable frameworks are needed to evaluate their real-world competence, adaptability, and capacity for human collaboration. Existing benchmarks remain largely static, synthetic, or domain-limited, providing limited insight into how agents perform in dynamic, economically meaningful environments. We introduce UpBench, a dynamically evolving benchmark grounded in real jobs drawn from the global Upwork labor marketplace. Each task corresponds to a verified client transaction, anchoring evaluation in genuine work activity and financial outcomes. UpBench employs a rubric-based evaluation framework, in which expert freelancers decompose each job into detailed, verifiable acceptance criteria and assess AI submissions with per-criterion feedback. This structure enables fine-grained analysis of model strengths, weaknesses, and instruction-following fidelity beyond binary pass/fail metrics. Human expertise is integrated throughout the data pipeline (from job curation and rubric construction to evaluation) ensuring fidelity to real professional standards and supporting research on human-AI collaboration. By regularly refreshing tasks to reflect the evolving nature of online work, UpBench provides a scalable, human-centered foundation for evaluating agentic systems in authentic labor-market contexts, offering a path toward a collaborative framework, where AI amplifies human capability through partnership rather than replacement.

</details>


### [124] [Reward and Guidance through Rubrics: Promoting Exploration to Improve Multi-Domain Reasoning](https://arxiv.org/abs/2511.12344)
*Baolong Bi,Shenghua Liu,Yiwei Wang,Siqian Tong,Lingrui Mei,Yuyao Ge,Yilong Xu,Jiafeng Guo,Xueqi Cheng*

Main category: cs.AI

TL;DR: 提出RGR-GRPO框架，通过评分标准提供细粒度奖励和离线指导，在多领域推理任务中显著优于现有强化学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注单一领域且依赖可验证奖励，纯在线RL框架限制了探索空间，影响了推理性能。

Method: 使用评分标准同时提供密集奖励信号和离线指导，通过GRPO训练让LLM在更大解空间中进行探索。

Result: 在14个多领域基准测试中，相比可验证在线RL基线，在数学、物理、化学和通用推理任务上分别平均提升7.0%、5.4%、8.4%和6.6%。

Conclusion: RGR-GRPO在离策略训练中保持稳定的熵波动，实现持续的探索并有效突破现有性能瓶颈。

Abstract: Recent advances in reinforcement learning (RL) have significantly improved the complex reasoning capabilities of large language models (LLMs). Despite these successes, existing methods mainly focus on single-domain RL (e.g., mathematics) with verifiable rewards (RLVR), and their reliance on purely online RL frameworks restricts the exploration space, thereby limiting reasoning performance. In this paper, we address these limitations by leveraging rubrics to provide both fine-grained reward signals and offline guidance. We propose $\textbf{RGR-GRPO}$ (Reward and Guidance through Rubrics), a rubric-driven RL framework for multi-domain reasoning. RGR-GRPO enables LLMs to receive dense and informative rewards while exploring a larger solution space during GRPO training. Extensive experiments across 14 benchmarks spanning multiple domains demonstrate that RGR-GRPO consistently outperforms RL methods that rely solely on alternative reward schemes or offline guidance. Compared with verifiable online RL baseline, RGR-GRPO achieves average improvements of +7.0%, +5.4%, +8.4%, and +6.6% on mathematics, physics, chemistry, and general reasoning tasks, respectively. Notably, RGR-GRPO maintains stable entropy fluctuations during off-policy training and achieves superior pass@k performance, reflecting sustained exploration and effective breakthrough beyond existing performance bottlenecks.

</details>


### [125] [More Than Irrational: Modeling Belief-Biased Agents](https://arxiv.org/abs/2511.12359)
*Yifan Zhu,Sammie Katt,Samuel Kaski*

Main category: cs.AI

TL;DR: 本文提出了计算理性用户模型，用于建模认知受限用户在偏见信念下的最优决策行为，重点解决从被动观察中推断用户认知边界和信念状态的问题。


<details>
  <summary>Details</summary>
Motivation: 预测和理解用户次优行为是AI发展中的关键挑战，这些行为通常源于认知限制和偏见信念，而非非理性决策。

Method: 提出基于嵌套粒子滤波的在线推理方法，同时跟踪用户的潜在信念状态并从未知认知边界中估计参数，以记忆衰减为例验证方法。

Result: 模拟实验显示：(1)模型能生成符合不同记忆容量的合理行为；(2)推理方法能准确高效地从有限观察中恢复真实认知边界(≤100步)。

Conclusion: 该方法为开发自适应AI助手提供了理论基础，使AI能够适应用户的记忆限制，提供个性化辅助。

Abstract: Despite the explosive growth of AI and the technologies built upon it, predicting and inferring the sub-optimal behavior of users or human collaborators remains a critical challenge. In many cases, such behaviors are not a result of irrationality, but rather a rational decision made given inherent cognitive bounds and biased beliefs about the world. In this paper, we formally introduce a class of computational-rational (CR) user models for cognitively-bounded agents acting optimally under biased beliefs. The key novelty lies in explicitly modeling how a bounded memory process leads to a dynamically inconsistent and biased belief state and, consequently, sub-optimal sequential decision-making. We address the challenge of identifying the latent user-specific bound and inferring biased belief states from passive observations on the fly. We argue that for our formalized CR model family with an explicit and parameterized cognitive process, this challenge is tractable. To support our claim, we propose an efficient online inference method based on nested particle filtering that simultaneously tracks the user's latent belief state and estimates the unknown cognitive bound from a stream of observed actions. We validate our approach in a representative navigation task using memory decay as an example of a cognitive bound. With simulations, we show that (1) our CR model generates intuitively plausible behaviors corresponding to different levels of memory capacity, and (2) our inference method accurately and efficiently recovers the ground-truth cognitive bounds from limited observations ($\le 100$ steps). We further demonstrate how this approach provides a principled foundation for developing adaptive AI assistants, enabling adaptive assistance that accounts for the user's memory limitations.

</details>


### [126] [Learning to Trust: Bayesian Adaptation to Varying Suggester Reliability in Sequential Decision Making](https://arxiv.org/abs/2511.12378)
*Dylan M. Asmar,Mykel J. Kochenderfer*

Main category: cs.AI

TL;DR: 提出了一个动态学习建议者可靠性的框架，通过贝叶斯推断和显式询问动作，使智能体能够在部分可观测环境中自适应地利用外部建议。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设建议者质量参数是静态且已知的，这限制了实际部署。需要能够动态学习和适应变化建议者可靠性的方法。

Method: 1. 将建议者质量直接集成到智能体的信念表示中，通过贝叶斯推断学习建议者类型；2. 引入显式的"询问"动作，允许智能体在关键时刻战略性地请求建议，平衡信息收益与获取成本。

Result: 实验评估显示该方法在不同建议者质量下具有鲁棒性能，能够适应变化的可靠性，并有效管理建议请求策略。

Conclusion: 这项工作通过解决不确定环境中的建议不确定性，为自适应人机协作提供了基础。

Abstract: Autonomous agents operating in sequential decision-making tasks under uncertainty can benefit from external action suggestions, which provide valuable guidance but inherently vary in reliability. Existing methods for incorporating such advice typically assume static and known suggester quality parameters, limiting practical deployment. We introduce a framework that dynamically learns and adapts to varying suggester reliability in partially observable environments. First, we integrate suggester quality directly into the agent's belief representation, enabling agents to infer and adjust their reliance on suggestions through Bayesian inference over suggester types. Second, we introduce an explicit ``ask'' action allowing agents to strategically request suggestions at critical moments, balancing informational gains against acquisition costs. Experimental evaluation demonstrates robust performance across varying suggester qualities, adaptation to changing reliability, and strategic management of suggestion requests. This work provides a foundation for adaptive human-agent collaboration by addressing suggestion uncertainty in uncertain environments.

</details>


### [127] [Multi-agent Self-triage System with Medical Flowcharts](https://arxiv.org/abs/2511.12439)
*Yujia Liu,Sophia Yu,Hongyue Jin,Jessica Wen,Alexander Qian,Terrence Lee,Mattheus Ramsis,Gi Won Choi,Lianhui Qin,Xin Liu,Edward J. Wang*

Main category: cs.AI

TL;DR: 开发了一个基于临床验证流程图的对话式自我分诊系统，通过多智能体框架实现95.29%的流程图检索准确率和99.10%的导航准确率，结合自由文本交互的灵活性和标准化临床协议的严谨性。


<details>
  <summary>Details</summary>
Motivation: 在线健康资源和大型语言模型在医疗决策中应用日益广泛，但其可靠性受到准确性低、缺乏透明度和易受未经验证信息影响的限制。

Method: 使用美国医学会100个临床验证流程图构建对话式自我分诊系统，采用包含检索智能体、决策智能体和聊天智能体的多智能体框架，分别负责识别相关流程图、解释患者响应和提供个性化建议。

Result: 在大规模合成数据集上评估，系统在流程图检索方面达到95.29%的top-3准确率（N=2,000），在流程图导航方面达到99.10%的准确率（N=37,200）。

Conclusion: 该方法展示了透明、准确且可推广的AI辅助自我分诊的可行性，有潜力支持患者知情决策并改善医疗资源利用。

Abstract: Online health resources and large language models (LLMs) are increasingly used as a first point of contact for medical decision-making, yet their reliability in healthcare remains limited by low accuracy, lack of transparency, and susceptibility to unverified information. We introduce a proof-of-concept conversational self-triage system that guides LLMs with 100 clinically validated flowcharts from the American Medical Association, providing a structured and auditable framework for patient decision support. The system leverages a multi-agent framework consisting of a retrieval agent, a decision agent, and a chat agent to identify the most relevant flowchart, interpret patient responses, and deliver personalized, patient-friendly recommendations, respectively. Performance was evaluated at scale using synthetic datasets of simulated conversations. The system achieved 95.29% top-3 accuracy in flowchart retrieval (N=2,000) and 99.10% accuracy in flowchart navigation across varied conversational styles and conditions (N=37,200). By combining the flexibility of free-text interaction with the rigor of standardized clinical protocols, this approach demonstrates the feasibility of transparent, accurate, and generalizable AI-assisted self-triage, with potential to support informed patient decision-making while improving healthcare resource utilization.

</details>


### [128] [ARCHE: A Novel Task to Evaluate LLMs on Latent Reasoning Chain Extraction](https://arxiv.org/abs/2511.12485)
*Pengze Li,Jiaqi Liu,Junchi Yu,Lihao Liu,Mingyu Ding,Wanli Ouyang,Shixiang Tang,Xi Chen*

Main category: cs.AI

TL;DR: 提出了ARCHE任务和ARCHE Bench基准，用于评估LLM从科学文献中提取标准推理链的能力，发现现有模型在推理完整性和逻辑有效性之间存在权衡。


<details>
  <summary>Details</summary>
Motivation: 当前LLM生成的推理内容通常是非结构化和非正式的，难以判断模型是否真正理解科学推理的基本范式，需要评估模型提取标准推理链的能力。

Method: 引入ARCHE任务，要求模型将复杂推理分解为推理逻辑树(RLT)，其中所有推理步骤都明确分类为演绎、归纳或溯因三种基本推理模式。创建ARCHE Bench基准，包含70篇Nature Communications文章的1900多个引用和38000个观点。

Result: 评估10个领先LLM发现，模型在推理边准确率(REA)和实体覆盖率(EC)之间存在权衡，没有一个模型能够提取完整且标准的推理链。

Conclusion: 当前推理模型的能力与科学论证所需的严谨性之间存在显著差距。

Abstract: Large language models (LLMs) are increasingly used in scientific domains. While they can produce reasoning-like content via methods such as chain-of-thought prompting, these outputs are typically unstructured and informal, obscuring whether models truly understand the fundamental reasoning paradigms that underpin scientific inference. To address this, we introduce a novel task named Latent Reasoning Chain Extraction (ARCHE), in which models must decompose complex reasoning arguments into combinations of standard reasoning paradigms in the form of a Reasoning Logic Tree (RLT). In RLT, all reasoning steps are explicitly categorized as one of three variants of Peirce's fundamental inference modes: deduction, induction, or abduction. To facilitate this task, we release ARCHE Bench, a new benchmark derived from 70 Nature Communications articles, including more than 1,900 references and 38,000 viewpoints. We propose two logic-aware evaluation metrics: Entity Coverage (EC) for content completeness and Reasoning Edge Accuracy (REA) for step-by-step logical validity. Evaluations on 10 leading LLMs on ARCHE Bench reveal that models exhibit a trade-off between REA and EC, and none are yet able to extract a complete and standard reasoning chain. These findings highlight a substantial gap between the abilities of current reasoning models and the rigor required for scientific argumentation.

</details>


### [129] [LOBERT: Generative AI Foundation Model for Limit Order Book Messages](https://arxiv.org/abs/2511.12563)
*Eljas Linna,Kestutis Baltakys,Alexandros Iosifidis,Juho Kanniainen*

Main category: cs.AI

TL;DR: LOBERT是一个针对限价订单簿数据的通用编码器基础模型，通过创新的标记化方案处理多维消息，在预测中间价格变动和下一消息等任务中表现领先。


<details>
  <summary>Details</summary>
Motivation: 现有LOB模型需要繁琐的数据表示，缺乏原始任务之外的适应性，因此需要开发一个适合下游微调的通用基础模型。

Method: LOBERT基于BERT架构，采用新颖的标记化方案将完整的多维消息作为单个标记处理，同时保留价格、交易量和时间的连续表示。

Result: LOBERT在预测中间价格变动和下一消息等任务中取得领先性能，同时相比先前方法减少了所需的上下文长度。

Conclusion: LOBERT为LOB数据提供了一个有效的通用基础模型，在多个任务中表现出色且计算效率更高。

Abstract: Modeling the dynamics of financial Limit Order Books (LOB) at the message level is challenging due to irregular event timing, rapid regime shifts, and the reactions of high-frequency traders to visible order flow. Previous LOB models require cumbersome data representations and lack adaptability outside their original tasks, leading us to introduce LOBERT, a general-purpose encoder-only foundation model for LOB data suitable for downstream fine-tuning. LOBERT adapts the original BERT architecture for LOB data by using a novel tokenization scheme that treats complete multi-dimensional messages as single tokens while retaining continuous representations of price, volume, and time. With these methods, LOBERT achieves leading performance in tasks such as predicting mid-price movements and next messages, while reducing the required context length compared to previous methods.

</details>


### [130] [Enhancing Conversational Recommender Systems with Tree-Structured Knowledge and Pretrained Language Models](https://arxiv.org/abs/2511.12579)
*Yongwen Ren,Chao Wang,Peng Du,Chuan Qin,Dazhong Shen,Hui Xiong*

Main category: cs.AI

TL;DR: PCRS-TKA是一个基于提示的框架，通过检索增强生成将预训练语言模型与知识图谱集成，解决了现有方法在利用PLM推理、上下文知识过滤和协作偏好建模方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用PLM在图关系上的推理能力，不加区分地整合检索到的知识，且在多轮对话中忽略了协作偏好，导致准确性不足和幻觉问题。

Method: 构建对话特定的知识树并序列化为文本，实现结构感知推理；选择性过滤上下文相关知识；使用专门监督信号显式建模协作偏好；语义对齐模块协调异构输入。

Result: 广泛实验表明PCRS-TKA在推荐和对话质量方面持续优于所有基线方法。

Conclusion: PCRS-TKA通过结构感知推理、上下文知识过滤和协作偏好建模，有效提升了对话推荐系统的准确性和对话质量。

Abstract: Recent advances in pretrained language models (PLMs) have significantly improved conversational recommender systems (CRS), enabling more fluent and context-aware interactions. To further enhance accuracy and mitigate hallucination, many methods integrate PLMs with knowledge graphs (KGs), but face key challenges: failing to fully exploit PLM reasoning over graph relationships, indiscriminately incorporating retrieved knowledge without context filtering, and neglecting collaborative preferences in multi-turn dialogues. To this end, we propose PCRS-TKA, a prompt-based framework employing retrieval-augmented generation to integrate PLMs with KGs. PCRS-TKA constructs dialogue-specific knowledge trees from KGs and serializes them into texts, enabling structure-aware reasoning while capturing rich entity semantics. Our approach selectively filters context-relevant knowledge and explicitly models collaborative preferences using specialized supervision signals. A semantic alignment module harmonizes heterogeneous inputs, reducing noise and enhancing accuracy. Extensive experiments demonstrate that PCRS-TKA consistently outperforms all baselines in both recommendation and conversational quality.

</details>


### [131] [Dynamic Tree Databases in Automated Planning](https://arxiv.org/abs/2511.12677)
*Oliver Joergensen,Dominik Drexler,Jendrik Seipp*

Main category: cs.AI

TL;DR: 提出了一种动态树数据库变体，用于压缩命题和数值变量的状态集，在保持静态版本优良特性的同时避免了大量内存预分配。


<details>
  <summary>Details</summary>
Motivation: 在大型任务中扩展显式状态空间搜索时，如何紧凑表示生成的状态集是一个核心挑战。传统的树数据库虽然能在最佳情况下为每个生成状态提供恒定空间，但需要大量内存预分配。

Method: 开发了一种新颖的动态树数据库变体，用于压缩命题和数值变量的状态集，并证明其保持了静态对应版本的理想特性。

Result: 在经典和数值规划任务上对状态压缩技术的实证评估显示，压缩比达到几个数量级，且通常运行时开销可忽略不计。

Conclusion: 动态树数据库为状态压缩提供了高效的解决方案，在保持高性能的同时显著减少了内存需求。

Abstract: A central challenge in scaling up explicit state-space search for large tasks is compactly representing the set of generated states. Tree databases, a data structure from model checking, require constant space per generated state in the best case, but they need a large preallocation of memory. We propose a novel dynamic variant of tree databases for compressing state sets over propositional and numeric variables and prove that it maintains the desirable properties of the static counterpart. Our empirical evaluation of state compression techniques for grounded and lifted planning on classical and numeric planning tasks reveals compression ratios of several orders of magnitude, often with negligible runtime overhead.

</details>


### [132] [Adaptively Coordinating with Novel Partners via Learned Latent Strategies](https://arxiv.org/abs/2511.12754)
*Benjamin Li,Shuyang Shi,Lucia Romero,Huao Li,Yaqi Xie,Woojun Kim,Stefanos Nikolaidis,Michael Lewis,Katia Sycara,Simon Stepputtis*

Main category: cs.AI

TL;DR: 该论文提出了一个策略条件化合作者框架，能够实时表示、分类和适应各种合作伙伴策略，在复杂协作任务中实现有效的人机团队合作。


<details>
  <summary>Details</summary>
Motivation: 在人类-智能体团队中，人工智能体需要实时适应人类伙伴的独特偏好和策略，这在时间压力和复杂策略空间的任务中尤为困难。

Method: 使用变分自编码器学习策略的潜在空间表示，通过聚类识别不同策略类型，训练条件化合作者智能体，并利用固定份额遗憾最小化算法进行在线适应。

Result: 在修改版的Overcooked环境中，该方法相比现有基线在与新人类和智能体队友配对时达到了最先进的性能。

Conclusion: 提出的策略条件化合作者框架能够有效适应多样化的合作伙伴策略，在复杂协作任务中实现卓越的团队表现。

Abstract: Adaptation is the cornerstone of effective collaboration among heterogeneous team members. In human-agent teams, artificial agents need to adapt to their human partners in real time, as individuals often have unique preferences and policies that may change dynamically throughout interactions. This becomes particularly challenging in tasks with time pressure and complex strategic spaces, where identifying partner behaviors and selecting suitable responses is difficult. In this work, we introduce a strategy-conditioned cooperator framework that learns to represent, categorize, and adapt to a broad range of potential partner strategies in real-time. Our approach encodes strategies with a variational autoencoder to learn a latent strategy space from agent trajectory data, identifies distinct strategy types through clustering, and trains a cooperator agent conditioned on these clusters by generating partners of each strategy type. For online adaptation to novel partners, we leverage a fixed-share regret minimization algorithm that dynamically infers and adjusts the partner's strategy estimation during interaction. We evaluate our method in a modified version of the Overcooked domain, a complex collaborative cooking environment that requires effective coordination among two players with a diverse potential strategy space. Through these experiments and an online user study, we demonstrate that our proposed agent achieves state of the art performance compared to existing baselines when paired with novel human, and agent teammates.

</details>


### [133] [Optimal Foraging in Memory Retrieval: Evaluating Random Walks and Metropolis-Hastings Sampling in Modern Semantic Spaces](https://arxiv.org/abs/2511.12759)
*James Moore*

Main category: cs.AI

TL;DR: 研究表明，在高维嵌入空间中进行随机游走可以产生与人类语义流畅性任务中观察到的优化觅食行为一致的结果，而更复杂的Metropolis-Hastings采样反而不能匹配人类行为。


<details>
  <summary>Details</summary>
Motivation: 探索现代高维嵌入空间是否能提供足以匹配人类语义流畅性任务中观察到的优化觅食行为的表示，并验证复杂采样机制是否必然产生更好的认知模型。

Method: 使用最先进的嵌入表示和先前的语义流畅性数据，在嵌入空间中进行随机游走和Metropolis-Hastings采样，比较它们与人类行为的匹配程度。

Result: 随机游走在嵌入空间中产生的结果与优化觅食和边际价值定理一致，而Metropolis-Hastings采样未能产生与人类行为一致的结果。

Conclusion: 适当结构的嵌入表示即使使用简单采样也能产生接近优化的觅食动态，挑战了复杂采样机制必然产生更好认知模型的假设，支持Hills(2012)而非Abbott(2015)的观点。

Abstract: Human memory retrieval often resembles ecological foraging where animals search for food in a patchy environment. Optimal foraging means following the Marginal Value Theorem (MVT), in which individuals exploit a patch of semantically related concepts until it becomes less rewarding and then switch to a new cluster. While human behavioral data suggests foraging-like patterns in semantic fluency tasks, it remains unclear whether modern high-dimensional embedding spaces provide representations that allow algorithms to match observed human behavior. Using state-of-the-art embeddings and prior semantic fluency data, I find that random walks on these embedding spaces produce results consistent with optimal foraging and the MVT. Surprisingly, introducing Metropolis-Hastings sampling, an adaptive algorithm expected to model strategic acceptance and rejection of new clusters, does not produce results consistent with human behavior. These findings challenge the assumption that more complex sampling mechanisms inherently lead to better cognitive models of memory retrieval. Instead, they show that appropriately structured embeddings, even with simple sampling, can produce near-optimal foraging dynamics. This supports the perspective of Hills (2012) rather than Abbott (2015), demonstrating that modern embeddings can approximate human memory foraging without relying on complex acceptance criteria.

</details>


### [134] [Event-CausNet: Unlocking Causal Knowledge from Text with Large Language Models for Reliable Spatio-Temporal Forecasting](https://arxiv.org/abs/2511.12769)
*Luyao Niu,Zepu Wang,Shuyi Guan,Yang Liu,Peng Sun*

Main category: cs.AI

TL;DR: Event-CausNet框架利用LLM量化非结构化事件报告，构建因果知识库，并通过因果注意力机制将因果知识注入双流GNN-LSTM网络，在交通中断期间显著提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 传统时空图神经网络在处理非重复性事件（如事故）时可靠性急剧下降，因为它们本质上是相关性模型，无法应对中断期间引入的新因果因素。

Method: 使用大语言模型量化非结构化事件报告，通过估计平均处理效应构建因果知识库，采用双流GNN-LSTM网络结合新颖的因果注意力机制来调整和增强预测。

Result: 在真实数据集上的实验表明，Event-CausNet将预测误差（MAE）降低了高达35.87%，显著优于最先进的基线方法。

Conclusion: 该框架弥合了相关性模型与因果推理之间的差距，提供了更准确、可迁移且具有关键可解释性的解决方案，为关键中断期间的实时交通管理提供了更可靠的基础。

Abstract: While spatio-temporal Graph Neural Networks (GNNs) excel at modeling recurring traffic patterns, their reliability plummets during non-recurring events like accidents. This failure occurs because GNNs are fundamentally correlational models, learning historical patterns that are invalidated by the new causal factors introduced during disruptions. To address this, we propose Event-CausNet, a framework that uses a Large Language Model to quantify unstructured event reports, builds a causal knowledge base by estimating average treatment effects, and injects this knowledge into a dual-stream GNN-LSTM network using a novel causal attention mechanism to adjust and enhance the forecast. Experiments on a real-world dataset demonstrate that Event-CausNet achieves robust performance, reducing prediction error (MAE) by up to 35.87%, significantly outperforming state-of-the-art baselines. Our framework bridges the gap between correlational models and causal reasoning, providing a solution that is more accurate and transferable, while also offering crucial interpretability, providing a more reliable foundation for real-world traffic management during critical disruptions.

</details>


### [135] [Multi-Agent Reinforcement Learning for Heterogeneous Satellite Cluster Resources Optimization](https://arxiv.org/abs/2511.12792)
*Mohamad A. Hady,Siyi Hu,Mahardhika Pratama,Zehong Cao,Ryszard Kowalczyk*

Main category: cs.AI

TL;DR: 该研究使用强化学习优化异构卫星集群在自主地球观测任务中的资源分配，通过多智能体强化学习算法实现光学和SAR卫星的协同工作，解决实时、不确定和去中心化环境下的资源管理问题。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法难以处理地球观测任务中实时性、不确定性和去中心化特性带来的挑战，需要开发能够自适应决策的智能方法。

Method: 构建基于Basilisk和BSK-RL框架的近真实仿真环境，评估MAPPO、HAPPO、HATRPO等先进MARL算法在异构卫星集群中的性能，系统地从单卫星到多卫星场景制定优化问题。

Result: MARL能够实现异构卫星间的有效协调，在平衡成像性能和资源利用的同时，缓解非平稳性和智能体间奖励耦合问题。

Conclusion: 研究为可扩展的自主卫星操作提供了实用见解，并为异构动态条件下智能地球观测任务规划的未来研究奠定了基础。

Abstract: This work investigates resource optimization in heterogeneous satellite clusters performing autonomous Earth Observation (EO) missions using Reinforcement Learning (RL). In the proposed setting, two optical satellites and one Synthetic Aperture Radar (SAR) satellite operate cooperatively in low Earth orbit to capture ground targets and manage their limited onboard resources efficiently. Traditional optimization methods struggle to handle the real-time, uncertain, and decentralized nature of EO operations, motivating the use of RL and Multi-Agent Reinforcement Learning (MARL) for adaptive decision-making. This study systematically formulates the optimization problem from single-satellite to multi-satellite scenarios, addressing key challenges including energy and memory constraints, partial observability, and agent heterogeneity arising from diverse payload capabilities. Using a near-realistic simulation environment built on the Basilisk and BSK-RL frameworks, we evaluate the performance and stability of state-of-the-art MARL algorithms such as MAPPO, HAPPO, and HATRPO. Results show that MARL enables effective coordination across heterogeneous satellites, balancing imaging performance and resource utilization while mitigating non-stationarity and inter-agent reward coupling. The findings provide practical insights into scalable, autonomous satellite operations and contribute a foundation for future research on intelligent EO mission planning under heterogeneous and dynamic conditions.

</details>


### [136] [Neuro-Logic Lifelong Learning](https://arxiv.org/abs/2511.12793)
*Bowen He,Xiaoan Xu,Alper Kamil Bozkurt,Vahid Tarokh,Juncheng Dong*

Main category: cs.AI

TL;DR: 该论文提出了一种终身学习ILP框架，利用逻辑规则的组合性和可转移性来高效学习新问题，通过重用先前任务中的逻辑规则来提升可扩展性和性能。


<details>
  <summary>Details</summary>
Motivation: 解决神经符号AI中ILP问题的关键挑战，探索涉及问题序列的新学习范式，而非仅关注单个问题的网络架构设计。

Method: 引入组合性框架，展示如何从早期任务中获得的逻辑规则在后续任务中高效重用，并形式化该方法。

Result: 在任务序列上的实证评估验证了该范式的可行性和优势，表现出改进的可扩展性和性能。

Conclusion: 为神经符号AI中的持续学习开辟了新方向，证明了终身学习ILP的有效性。

Abstract: Solving Inductive Logic Programming (ILP) problems with neural networks is a key challenge in Neural-Symbolic Ar- tificial Intelligence (AI). While most research has focused on designing novel network architectures for individual prob- lems, less effort has been devoted to exploring new learning paradigms involving a sequence of problems. In this work, we investigate lifelong learning ILP, which leverages the com- positional and transferable nature of logic rules for efficient learning of new problems. We introduce a compositional framework, demonstrating how logic rules acquired from ear- lier tasks can be efficiently reused in subsequent ones, leading to improved scalability and performance. We formalize our approach and empirically evaluate it on sequences of tasks. Experimental results validate the feasibility and advantages of this paradigm, opening new directions for continual learn- ing in Neural-Symbolic AI.

</details>


### [137] [Mapping fNIRS Signals to Agent Performance: Toward Reinforcement Learning from Neural Feedback](https://arxiv.org/abs/2511.12844)
*Julia Santaniello,Matthew Russell,Benson Jiang,Donatello Sassaroli,Robert Jacob,Jivko SInapov*

Main category: cs.AI

TL;DR: 使用被动脑机接口（BCI）和功能性近红外光谱（fNIRS）记录来指导强化学习代理训练，通过神经信号预测代理性能水平，并展示了跨主体泛化和微调的效果。


<details>
  <summary>Details</summary>
Motivation: 开发一种基于隐式神经信号的强化学习人类反馈（RLHF）框架，使代理行为与人类偏好对齐，而无需显式反馈。

Method: 收集25名参与者在三个领域（拾放机器人、月球着陆器、Flappy Bird）的fNIRS记录，训练分类器预测代理性能水平（最优、次优、最差），并训练回归器预测动作与最优策略的偏差程度。

Result: 二元分类平均F1分数67%，多分类平均46%；跨主体微调后二元和多分类F1分数分别提升17%和41%。

Conclusion: 从隐式fNIRS信号映射到代理性能是可行的，为未来脑驱动的RLHF系统奠定了基础。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is a methodology that aligns agent behavior with human preferences by integrating human feedback into the agent's training process. We introduce a possible framework that employs passive Brain-Computer Interfaces (BCI) to guide agent training from implicit neural signals. We present and release a novel dataset of functional near-infrared spectroscopy (fNIRS) recordings collected from 25 human participants across three domains: a Pick-and-Place Robot, Lunar Lander, and Flappy Bird. We train classifiers to predict levels of agent performance (optimal, sub-optimal, or worst-case) from windows of preprocessed fNIRS feature vectors, achieving an average F1 score of 67% for binary classification and 46% for multi-class models averaged across conditions and domains. We also train regressors to predict the degree of deviation between an agent's chosen action and a set of near-optimal policies, providing a continuous measure of performance. We evaluate cross-subject generalization and demonstrate that fine-tuning pre-trained models with a small sample of subject-specific data increases average F1 scores by 17% and 41% for binary and multi-class models, respectively. Our work demonstrates that mapping implicit fNIRS signals to agent performance is feasible and can be improved, laying the foundation for future brain-driven RLHF systems.

</details>


### [138] [Bootstrapping LLMs via Preference-Based Policy Optimization](https://arxiv.org/abs/2511.12867)
*Chen Jia*

Main category: cs.AI

TL;DR: 提出了一种基于偏好的策略优化框架PbPO，通过主策略和奖励模型之间的min-max博弈来对齐LLM行为，无需大量人工标注。


<details>
  <summary>Details</summary>
Motivation: 通过基于偏好的策略优化来引导大型语言模型，使其行为与人类偏好对齐，而不依赖大量手动标注。

Method: 将学习过程构建为主策略和奖励模型之间的min-max博弈，奖励模型约束在偏好数据导出的置信集内，采用迭代在线算法通过引导探索主动收集偏好数据。

Result: 在五个基准测试上的广泛实验表明，该方法持续优于现有的最先进偏好优化技术。

Conclusion: 提出的PbPO框架在引导LLM方面表现出色，为基于偏好的策略优化提供了有效的解决方案。

Abstract: Bootstrapping large language models (LLMs) through preference-based policy optimization offers a promising direction for aligning model behavior with human preferences without relying on extensive manual annotations. In this work, we propose a novel preference-based policy optimization (PbPO) framework that formulates the learning process as a min-max game between the main policy and a reward model (RM). The RM is constrained within a confidence set derived from preference data to ensure reliable exploitation. Our iterative online algorithm actively collects preference data through guided exploration of the evolving policy, enabling continual self-improvement of both the policy and the RM. We provide theoretical guarantees for our method, establishing high-probability regret bounds for both settings with sequence-level RM and token-level RM, demonstrating its effectiveness in bootstrapping LLMs. Extensive experiments on five benchmarks show that our approach consistently outperforms existing state-of-the-art preference optimization techniques.

</details>


### [139] [Think, Speak, Decide: Language-Augmented Multi-Agent Reinforcement Learning for Economic Decision-Making](https://arxiv.org/abs/2511.12876)
*Heyang Ma,Qirui Mi,Qipeng Yang,Zijun Fan,Bo Li,Haifeng Zhang*

Main category: cs.AI

TL;DR: LAMP框架通过语言增强的多智能体强化学习，在经济决策中整合语言信息，显著提升性能、鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现实经济决策不仅依赖结构化信号（如价格、税收），还依赖非结构化语言信息（如同行对话、媒体叙事）。传统MARL难以处理语言的语义模糊性和上下文丰富性。

Method: 采用Think-Speak-Decide三阶段流程：Think阶段解释数值观测提取短期冲击和长期趋势；Speak阶段基于推理生成和交换策略消息；Decide阶段融合数值数据、推理和反思到MARL策略中。

Result: 在经济模拟实验中，LAMP在累积回报（+63.5%, +34.0%）、鲁棒性（+18.8%, +59.4%）和可解释性方面均优于MARL和纯LLM基线。

Conclusion: 语言增强策略有潜力提供更有效和鲁棒的经济策略，缩小与现实世界设置的差距。

Abstract: Economic decision-making depends not only on structured signals such as prices and taxes, but also on unstructured language, including peer dialogue and media narratives. While multi-agent reinforcement learning (MARL) has shown promise in optimizing economic decisions, it struggles with the semantic ambiguity and contextual richness of language. We propose LAMP (Language-Augmented Multi-Agent Policy), a framework that integrates language into economic decision-making and narrows the gap to real-world settings. LAMP follows a Think-Speak-Decide pipeline: (1) Think interprets numerical observations to extract short-term shocks and long-term trends, caching high-value reasoning trajectories; (2) Speak crafts and exchanges strategic messages based on reasoning, updating beliefs by parsing peer communications; and (3) Decide fuses numerical data, reasoning, and reflections into a MARL policy to optimize language-augmented decision-making. Experiments in economic simulation show that LAMP outperforms both MARL and LLM-only baselines in cumulative return (+63.5%, +34.0%), robustness (+18.8%, +59.4%), and interpretability. These results demonstrate the potential of language-augmented policies to deliver more effective and robust economic strategies.

</details>


### [140] [Online Learning of HTN Methods for integrated LLM-HTN Planning](https://arxiv.org/abs/2511.12901)
*Yuesheng Xu,Hector Munoz-Avila*

Main category: cs.AI

TL;DR: 提出了一种在线学习分层任务网络(HTN)方法的方法，基于ChatHTN规划器，通过从ChatGPT生成的任务分解中学习通用方法，减少对ChatGPT的调用次数。


<details>
  <summary>Details</summary>
Motivation: 在集成HTN规划和基于LLM的聊天机器人背景下，需要学习何时以及如何将任务分解为子任务，以减少对ChatGPT的依赖并提高效率。

Method: 在ChatHTN规划器基础上扩展，当ChatGPT生成任务分解时，学习通用方法而非简单记忆，使方法适用于同一任务的其他实例。

Result: 在两个领域进行实验，证明在线学习过程减少了ChatGPT调用次数，同时解决了至少同样多的问题，在某些情况下甚至更多。

Conclusion: 该方法能有效减少对ChatGPT的依赖，提高HTN规划的效率，同时保持或提升问题解决能力。

Abstract: We present online learning of Hierarchical Task Network (HTN) methods in the context of integrated HTN planning and LLM-based chatbots. Methods indicate when and how to decompose tasks into subtasks. Our method learner is built on top of the ChatHTN planner. ChatHTN queries ChatGPT to generate a decomposition of a task into primitive tasks when no applicable method for the task is available. In this work, we extend ChatHTN. Namely, when ChatGPT generates a task decomposition, ChatHTN learns from it, akin to memoization. However, unlike memoization, it learns a generalized method that applies not only to the specific instance encountered, but to other instances of the same task. We conduct experiments on two domains and demonstrate that our online learning procedure reduces the number of calls to ChatGPT while solving at least as many problems, and in some cases, even more.

</details>


### [141] [CoS: Towards Optimal Event Scheduling via Chain-of-Scheduling](https://arxiv.org/abs/2511.12913)
*Yiming Zhao,Jiwei Tang,Shimin Di,Libin Zheng,Jianxing Yu,Jian Yin*

Main category: cs.AI

TL;DR: 提出了Chain-of-Scheduling (CoS)框架，通过探索、验证和集成三个原子阶段激活LLM的事件调度能力，在EBSNs中实现高效有效的事件推荐。


<details>
  <summary>Details</summary>
Motivation: 现有的事件推荐方法在效率、效果和泛化性之间存在固有权衡，因为该问题是NP难问题。需要一种既能最大化用户偏好，又能满足时间和地理约束的有效推荐方法。

Method: CoS框架将调度任务分解为探索、验证和集成三个原子阶段，并通过知识蒸馏使LLM能够自主生成CoS。

Result: 在三个真实世界数据集上，CoS实现了接近理论最优的效果，具有高效率和可解释性，并在域外数据上表现出强大的零样本学习能力。

Conclusion: CoS框架成功激活了LLM的事件调度能力，解决了事件推荐中的效率-效果-泛化性权衡问题，为EBSNs提供了有效的调度解决方案。

Abstract: Recommending event schedules is a key issue in Event-based Social Networks (EBSNs) in order to maintain user activity. An effective recommendation is required to maximize the user's preference, subjecting to both time and geographical constraints. Existing methods face an inherent trade-off among efficiency, effectiveness, and generalization, due to the NP-hard nature of the problem. This paper proposes the Chain-of-Scheduling (CoS) framework, which activates the event scheduling capability of Large Language Models (LLMs) through a guided, efficient scheduling process. CoS enhances LLM by formulating the schedule task into three atomic stages, i.e., exploration, verification and integration. Then we enable the LLMs to generate CoS autonomously via Knowledge Distillation (KD). Experimental results show that CoS achieves near-theoretical optimal effectiveness with high efficiency on three real-world datasets in a interpretable manner. Moreover, it demonstrates strong zero-shot learning ability on out-of-domain data.

</details>


### [142] [Fault2Flow: An AlphaEvolve-Optimized Human-in-the-Loop Multi-Agent System for Fault-to-Workflow Automation](https://arxiv.org/abs/2511.12916)
*Yafang Wang,Yangjie Tian,Xiaoyu Shen,Gaoyang Zhang,Jiaze Sun,He Zhang,Ruohua Xu,Feng Zhao*

Main category: cs.AI

TL;DR: Fault2Flow是一个基于LLM的多智能体系统，用于电网故障诊断，通过提取法规逻辑、整合专家知识、优化推理逻辑，最终生成可执行的工作流。


<details>
  <summary>Details</summary>
Motivation: 当前电网故障诊断依赖人工方法，效率低、易出错且难以维护。需要将法规文本和专家知识整合到可验证、可执行的自动化流程中。

Method: 使用LLM多智能体系统：1) 提取法规逻辑为PASTA格式故障树；2) 通过人机交互界面整合专家知识；3) 用AlphaEvolve模块优化推理逻辑；4) 合成n8n可执行工作流。

Result: 在变压器故障诊断数据集上的实验验证显示100%拓扑一致性和高语义保真度，显著减少专家工作量。

Conclusion: Fault2Flow建立了从故障分析到操作自动化的可复现路径，为电网故障诊断提供了系统化的解决方案。

Abstract: Power grid fault diagnosis is a critical process hindered by its reliance on manual, error-prone methods. Technicians must manually extract reasoning logic from dense regulations and attempt to combine it with tacit expert knowledge, which is inefficient, error-prone, and lacks maintainability as ragulations are updated and experience evolves. While Large Language Models (LLMs) have shown promise in parsing unstructured text, no existing framework integrates these two disparate knowledge sources into a single, verified, and executable workflow. To bridge this gap, we propose Fault2Flow, an LLM-based multi-agent system. Fault2Flow systematically: (1) extracts and structures regulatory logic into PASTA-formatted fault trees; (2) integrates expert knowledge via a human-in-the-loop interface for verification; (3) optimizes the reasoning logic using a novel AlphaEvolve module; and (4) synthesizes the final, verified logic into an n8n-executable workflow. Experimental validation on transformer fault diagnosis datasets confirms 100\% topological consistency and high semantic fidelity. Fault2Flow establishes a reproducible path from fault analysis to operational automation, substantially reducing expert workload.

</details>


### [143] [Yanyun-3: Enabling Cross-Platform Strategy Game Operation with Vision-Language Models](https://arxiv.org/abs/2511.12937)
*Guoyan Wang,Yanyan Huang,Chunlin Chen,Lifeng Wang,Yuxiang Sun*

Main category: cs.AI

TL;DR: Yanyun-3是一个跨平台策略游戏自主操作框架，通过融合视觉语言模型Qwen2.5-VL和UI-TARS执行器，在三个异构游戏环境中实现目标定位、资源分配和区域控制等核心任务。研究发现混合多图像和视频数据并融合静态图像的策略(MV+S)比完全融合减少63%推理时间，BLEU-4得分提升12.98倍。


<details>
  <summary>Details</summary>
Motivation: 解决跨平台策略游戏中智能代理在不同用户界面和动态战场条件下的泛化问题，探索视觉语言模型在复杂人机交互场景中的应用潜力。

Method: 集成Qwen2.5-VL的视觉语言推理能力和UI-TARS的精确执行能力，采用屏幕捕获-模型推理-动作执行的闭环流程，研究不同多模态数据组合策略（静态图像、多图像序列、视频）及其融合粒度。

Result: MV+S混合策略显著优于完全融合：推理时间减少63%，BLEU-4得分从4.81%提升至62.41%（约12.98倍提升），在三个异构策略游戏环境中成功实现自主操作。

Conclusion: Yanyun-3不仅为策略游戏自动化提供了高效解决方案，还通过结构化多模态数据组织建立了增强VLM性能的通用范式，为具身智能中静态感知与动态推理的交互提供了新见解。

Abstract: Automated operation in cross-platform strategy games demands agents with robust generalization across diverse user interfaces and dynamic battlefield conditions. While vision-language models (VLMs) have shown considerable promise in multimodal reasoning, their application to complex human-computer interaction scenarios--such as strategy gaming--remains largely unexplored. Here, we introduce Yanyun-3, a general-purpose agent framework that, for the first time, enables autonomous cross-platform operation across three heterogeneous strategy game environments. By integrating the vision-language reasoning of Qwen2.5-VL with the precise execution capabilities of UI-TARS, Yanyun-3 successfully performs core tasks including target localization, combat resource allocation, and area control. Through systematic ablation studies, we evaluate the effects of various multimodal data combinations--static images, multi-image sequences, and videos--and propose the concept of combination granularity to differentiate between intra-sample fusion and inter-sample mixing strategies. We find that a hybrid strategy, which fuses multi-image and video data while mixing in static images (MV+S), substantially outperforms full fusion: it reduces inference time by 63% and boosts the BLEU-4 score by a factor of 12 (from 4.81% to 62.41%, approximately 12.98x). Operating via a closed-loop pipeline of screen capture, model inference, and action execution, the agent demonstrates strong real-time performance and cross-platform generalization. Beyond providing an efficient solution for strategy game automation, our work establishes a general paradigm for enhancing VLM performance through structured multimodal data organization, offering new insights into the interplay between static perception and dynamic reasoning in embodied intelligence.

</details>


### [144] [MedRule-KG: A Knowledge-Graph--Steered Scaffold for Reliable Mathematical and Biomedical Reasoning](https://arxiv.org/abs/2511.12963)
*Crystal Su*

Main category: cs.AI

TL;DR: MedRule-KG是一个为科学推理和药物发现设计的结构化系统，通过知识图谱和验证器约束LLM生成，减少83.2%的违规并提升准确率。


<details>
  <summary>Details</summary>
Motivation: 在科学推理和早期药物发现中，需要确保LLM生成的内容在数学和生物医学上有效，避免产生不合理的输出。

Method: 使用MedRule-KG知识图谱支架和轻量验证器，通过注入符号事实和确定性检查来约束生成过程，将生成建模为约束推理问题。

Result: 在90个任务中，相比强基线减少83.2%违规，提升精确匹配，验证器延迟可忽略，适合交互式设计。

Conclusion: MedRule-KG能有效约束LLM生成，提升科学推理的可靠性和实用性，具有实际应用价值。

Abstract: We study how to impose domain-consistent structure on large language models (LLMs) used for scientific reasoning and early-stage drug discovery. We present MedRule-KG, a compact knowledge-graph scaffold paired with a lightweight verifier that steers generation toward mathematically and biomedically valid outputs. The system injects curated symbolic facts into prompts and then enforces rule satisfaction with a deterministic checker. We formalize generation as constrained inference, introduce a soft guidance surrogate suitable for decoding, and perform a thorough statistical analysis with uncertainty quantification. Across 90 tasks spanning reaction feasibility, metabolic compatibility, and toxicity screening, MedRule-KG reduces violation counts by 83.2\% relative to a strong chain-of-thought baseline while improving exact match. Results remain stable under stratification and scale with dataset size, and the verifier adds negligible latency, making the approach practical for interactive design.

</details>


### [145] [WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance](https://arxiv.org/abs/2511.12997)
*Genglin Liu,Shijie Geng,Sha Li,Hejie Cui,Sarah Zhang,Xin Liu,Tianyi Liu*

Main category: cs.AI

TL;DR: WebCoach是一个模型无关的自进化框架，为多模态LLM驱动的网页浏览代理提供跨会话持久内存，通过记忆存储和经验检索提升长期规划和持续学习能力，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 当前网页浏览代理存在重复错误、无法跨会话学习经验的问题，限制了长期鲁棒性和样本效率。

Method: WebCoach包含三个核心组件：WebCondenser标准化导航日志为摘要，External Memory Store组织完整轨迹为情景经验，Coach基于相似性和时效性检索相关经验并通过运行时钩子注入任务特定建议。

Result: 在WebVoyager基准测试中，WebCoach显著提升了三种不同LLM骨干的浏览器代理性能，38B模型任务成功率从47%提升至61%，同时减少或维持平均步骤数。

Conclusion: WebCoach使网页浏览代理能够超越原生上下文窗口访问长期记忆，在复杂浏览任务中提高鲁棒性，并通过持续整理情景记忆实现无需重新训练的自进化。

Abstract: Multimodal LLM-powered agents have recently demonstrated impressive capabilities in web navigation, enabling agents to complete complex browsing tasks across diverse domains. However, current agents struggle with repetitive errors and lack the ability to learn from past experiences across sessions, limiting their long-term robustness and sample efficiency. We introduce WebCoach, a model-agnostic self-evolving framework that equips web browsing agents with persistent cross-session memory, enabling improved long-term planning, reflection, and continual learning without retraining. WebCoach consists of three key components: (1) a WebCondenser, which standardizes raw navigation logs into concise summaries; (2) an External Memory Store, which organizes complete trajectories as episodic experiences; and (3) a Coach, which retrieves relevant experiences based on similarity and recency, and decides whether to inject task-specific advice into the agent via runtime hooks. This design empowers web agents to access long-term memory beyond their native context window, improving robustness in complex browsing tasks. Moreover, WebCoach achieves self-evolution by continuously curating episodic memory from new navigation trajectories, enabling agents to improve over time without retraining. Evaluations on the WebVoyager benchmark demonstrate that WebCoach consistently improves the performance of browser-use agents across three different LLM backbones. With a 38B model, it increases task success rates from 47% to 61% while reducing or maintaining the average number of steps. Notably, smaller base models with WebCoach achieve performance comparable to the same web agent using GPT-4o.

</details>


### [146] [GEM: Generative Entropy-Guided Preference Modeling for Few-shot Alignment of LLMs](https://arxiv.org/abs/2511.13007)
*Yiyang Zhao,Huiyu Bai,Xuejiao Zhao*

Main category: cs.AI

TL;DR: 提出GEM方法，在低资源和领域特定场景下通过生成式熵引导偏好建模实现大语言模型对齐，无需大量标注数据。


<details>
  <summary>Details</summary>
Motivation: 在医学、法律等专业领域难以获得大规模偏好标注数据，需要开发低资源下的模型对齐方法。

Method: 基于决策熵理论，使用思维链提示生成多样化推理链，通过令牌评分机制对推理链排序加权，结合自评估群体优势算法进行策略优化。

Result: 在通用基准和领域特定任务（如数学推理和医疗对话）上，使用少量偏好数据取得了显著改进。

Conclusion: GEM建立了一个熵引导的闭环认知优化框架，能够高效实现大语言模型的少样本对齐。

Abstract: Alignment of large language models (LLMs) with human preferences typically relies on supervised reward models or external judges that demand abundant annotations. However, in fields that rely on professional knowledge, such as medicine and law, such large-scale preference labels are often unachievable. In this paper, we propose a generative entropy-guided preference modeling approach named GEM for LLMs aligment at low-resource and domain-specific scenarios. Instead of training a discriminative reward model on preference data, we directly train the LLM to internalize a closed-loop optimization architecture that can extract and exploit the multi-dimensional, fine-grained cognitive signals implicit in human preferences. Specifically, our Cognitive Filtering module, based on entropy theory in decision making, first leverages Chain-of-Thought (CoT) prompting to generate diverse candidate reasoning chains (CoTs) from preference data. Subsequently, it introduces a token scoring mechanism to rank and weight the sampled CoTs, boosting the importance of high-confidence answers and strategically high-entropy tokens. Building on these filtered preferences, we fine-tune the LLM using a novel self-evaluated group advantage algorithm, SEGA, which effectively aggregates group-level cognitive signals and transforms the entropy-based scores into implicit rewards for policy optimization. In these ways, GEM empowers the LLM to rely on its own judgments and establishes an entropy-guided closed-loop cognitive optimization framework, enabling highly efficient few-shot alignment of LLMs. Experiments on general benchmarks and domain-specific tasks (such as mathematical reasoning and medical dialogues) demonstrate that our GEM achieves significant improvements with few-shot preference data.

</details>


### [147] [PragWorld: A Benchmark Evaluating LLMs' Local World Model under Minimal Linguistic Alterations and Conversational Dynamics](https://arxiv.org/abs/2511.13021)
*Sachin Vashistha,Aryan Bibhuti,Atharva Naik,Martin Tutek,Somak Aditya*

Main category: cs.AI

TL;DR: 评估语言模型在对话中构建和维护世界模型的能力，发现它们在语言变化下难以保持稳健性，并提出基于层正则化的微调策略来抑制有害层的影响。


<details>
  <summary>Details</summary>
Motivation: 理解真实对话中的语用元素需要构建局部世界模型，但目前不清楚语言模型是否能构建和维护这种隐式表示。

Method: 对流行数据集中的对话应用七种最小语言变化，构建两个包含是非问题的基准，评估多种语言模型，并提出双视角可解释性框架和层正则化微调策略。

Result: 语言模型在语言变化下难以保持稳健准确率，特别是在跟踪实体方面存在困难。

Conclusion: 语言模型在对话中维护世界模型的能力有限，但通过识别和抑制有害层可以改善性能。

Abstract: Real-world conversations are rich with pragmatic elements, such as entity mentions, references, and implicatures. Understanding such nuances is a requirement for successful natural communication, and often requires building a local world model which encodes such elements and captures the dynamics of their evolving states. However, it is not well-understood whether language models (LMs) construct or maintain a robust implicit representation of conversations. In this work, we evaluate the ability of LMs to encode and update their internal world model in dyadic conversations and test their malleability under linguistic alterations. To facilitate this, we apply seven minimal linguistic alterations to conversations sourced from popular datasets and construct two benchmarks comprising yes-no questions. We evaluate a wide range of open and closed source LMs and observe that they struggle to maintain robust accuracy. Our analysis unveils that LMs struggle to memorize crucial details, such as tracking entities under linguistic alterations to conversations. We then propose a dual-perspective interpretability framework which identifies transformer layers that are useful or harmful and highlights linguistic alterations most influenced by harmful layers, typically due to encoding spurious signals or relying on shortcuts. Inspired by these insights, we propose two layer-regularization based fine-tuning strategies that suppress the effect of the harmful layers.

</details>


### [148] [Scaling Generative Verifiers For Natural Language Mathematical Proof Verification And Selection](https://arxiv.org/abs/2511.13027)
*Sadegh Mahdavi,Branislav Kisacanin,Shubham Toshniwal,Wei Du,Ivan Moshkov,George Armstrong,Renjie Liao,Christos Thrampoulidis,Igor Gitman*

Main category: cs.AI

TL;DR: 论文分析了数学推理验证方法，发现单一基准测试不可靠，提出结合GenSelect和LLM-as-a-Judge的验证框架，并通过强化学习减少提示敏感性，但发现当前模型更注重形式正确性而非数学有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学问题上取得了显著成功，但其推理过程往往存在缺陷。为了推进严格的基于证明的数学推理，需要可靠的证明验证能力。

Method: 分析了多种评估设置，评估了基于证明和最终答案的推理，扩展了两种生成验证方法（GenSelect和LLM-as-a-Judge）到百万token规模，并研究了强化学习对提示敏感性的影响。

Result: 结合GenSelect和LLM-as-a-Judge是最有效的验证框架；强化学习可以减少LLM-as-a-Judge对提示的敏感性；尽管改进了证明级指标，但强化学习并未提高最终答案精度。

Conclusion: 当前模型往往奖励风格或程序正确性而非数学有效性，研究为设计和评估可扩展的证明验证和选择系统提供了实用指南。

Abstract: Large language models have achieved remarkable success on final-answer mathematical problems, largely due to the ease of applying reinforcement learning with verifiable rewards. However, the reasoning underlying these solutions is often flawed. Advancing to rigorous proof-based mathematics requires reliable proof verification capabilities. We begin by analyzing multiple evaluation setups and show that focusing on a single benchmark can lead to brittle or misleading conclusions. To address this, we evaluate both proof-based and final-answer reasoning to obtain a more reliable measure of model performance. We then scale two major generative verification methods (GenSelect and LLM-as-a-Judge) to millions of tokens and identify their combination as the most effective framework for solution verification and selection. We further show that the choice of prompt for LLM-as-a-Judge significantly affects the model's performance, but reinforcement learning can reduce this sensitivity. However, despite improving proof-level metrics, reinforcement learning does not enhance final-answer precision, indicating that current models often reward stylistic or procedural correctness rather than mathematical validity. Our results establish practical guidelines for designing and evaluating scalable proof-verification and selection systems.

</details>


### [149] [MEGA-GUI: Multi-stage Enhanced Grounding Agents for GUI Elements](https://arxiv.org/abs/2511.13087)
*SeokJoo Kwak,Jihoon Kim,Boyoun Kim,Jung Jae Yoon,Wooseok Jang,Jeonghoon Hong,Jaeho Yang,Yeong-Dae Kwon*

Main category: cs.AI

TL;DR: MEGA-GUI是一个多阶段GUI定位框架，通过区域选择和细粒度元素定位解决视觉杂乱和指令模糊问题，在多个基准测试中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有GUI定位系统采用单体模型或一次性流水线，缺乏模块化，在视觉杂乱和模糊指令下表现不佳。

Method: 使用多阶段框架，分离为粗略ROI选择和细粒度元素定位，采用双向ROI缩放算法和上下文感知重写代理。

Result: 在ScreenSpot-Pro基准上达到73.18%准确率，在OSWorld-G基准上达到68.63%，超越之前报告结果。

Conclusion: 模块化结构比单体方法获得更高准确率，揭示了不同视觉尺度下视觉语言模型的互补优势。

Abstract: Graphical User Interface (GUI) grounding - the task of mapping natural language instructions to screen coordinates - is essential for autonomous agents and accessibility technologies. Existing systems rely on monolithic models or one-shot pipelines that lack modularity and fail under visual clutter and ambiguous instructions. We introduce MEGA-GUI, a multi-stage framework that separates grounding into coarse Region-of-Interest (ROI) selection and fine-grained element grounding, orchestrated by specialized vision-language agents. MEGA-GUI features a bidirectional ROI zoom algorithm that mitigates spatial dilution and a context-aware rewriting agent that reduces semantic ambiguity. Our analysis reveals complementary strengths and weaknesses across vision-language models at different visual scales, and we show that leveraging this modular structure achieves consistently higher accuracy than monolithic approaches. On the visually dense ScreenSpot-Pro benchmark, MEGA-GUI attains 73.18% accuracy, and on the semantically complex OSWorld-G benchmark it reaches 68.63%, surpassing previously reported results. Code and the Grounding Benchmark Toolkit (GBT) are available at https://github.com/samsungsds-research-papers/mega-gui.

</details>


### [150] [STEP: Success-Rate-Aware Trajectory-Efficient Policy Optimization](https://arxiv.org/abs/2511.13091)
*Yuhan Chen,Yuxuan Liu,Long Zhang,Pengzhi Gao,Jian Luan,Wei Liu*

Main category: cs.AI

TL;DR: STEP框架通过动态分配采样和步骤级优化，解决了多轮交互中轨迹级优化的效率低下和误导性学习信号问题。


<details>
  <summary>Details</summary>
Motivation: 轨迹级优化在多轮交互中存在效率低下和误导性学习信号的问题，包括对任务难度不敏感的均匀采样、在失败轨迹中惩罚正确中间动作以及高采样成本。

Method: STEP框架维护平滑的成功率记录来指导自适应轨迹重采样，计算成功率加权优势，将轨迹分解为步骤级样本，并应用步骤级GRPO增强来优化低成功率任务的更新。

Result: 在OSWorld和AndroidWorld上的实验表明，STEP显著提高了样本效率和训练稳定性，在相同采样预算下收敛更快且泛化能力更好。

Conclusion: STEP通过动态采样分配和步骤级优化，有效解决了多轮交互中的强化学习挑战，提升了样本效率和训练性能。

Abstract: Multi-turn interaction remains challenging for online reinforcement learning. A common solution is trajectory-level optimization, which treats each trajectory as a single training sample. However, this approach can be inefficient and yield misleading learning signals: it applies uniform sampling across tasks regardless of difficulty, penalizes correct intermediate actions in failed trajectories, and incurs high sample-collection costs. To address these issues, we propose STEP (Success-rate-aware Trajectory-Efficient Policy optimization), a framework that dynamically allocates sampling based on per-task success rates and performs step-level optimization. STEP maintains a smoothed success-rate record to guide adaptive trajectory resampling, allocating more effort to harder tasks. It then computes success-rate-weighted advantages and decomposes trajectories into step-level samples. Finally, it applies a step-level GRPO augmentation to refine updates for low-success tasks. Experiments on OSWorld and AndroidWorld show that STEP substantially improves sample efficiency and training stability over trajectory-level GRPO, converging faster and generalizing better under the same sampling budget.

</details>


### [151] [MM-Telco: Benchmarks and Multimodal Large Language Models for Telecom Applications](https://arxiv.org/abs/2511.13131)
*Gagan Raj Gupta,Anshul Kumar,Manish Rai,Apu Chakraborty,Ashutosh Modi,Abdelaali Chaoub,Soumajit Pramanik,Moyank Giri,Yashwanth Holla,Sunny Kumar,M. V. Kiran Sooraj*

Main category: cs.AI

TL;DR: 提出了MM-Telco，一个为电信领域定制的多模态基准测试套件和模型，用于解决LLM在电信应用中的领域特定挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在电信领域具有巨大潜力，但面临领域特定的挑战，需要专门适配才能有效部署于网络优化、故障排除、客户支持和合规管理等任务。

Method: 开发了包含文本和图像任务的多模态基准测试套件，涵盖网络运营、网络管理、文档质量改进和相关文本图像检索等实际用例，并对各种LLM和VLM进行基线实验。

Result: 在数据集上微调的模型性能显著提升，实验揭示了当前最先进多模态LLM的薄弱环节。

Conclusion: MM-Telco为电信领域LLM的适应提供了重要基准，指导了进一步的研究和发展方向。

Abstract: Large Language Models (LLMs) have emerged as powerful tools for automating complex reasoning and decision-making tasks. In telecommunications, they hold the potential to transform network optimization, automate troubleshooting, enhance customer support, and ensure regulatory compliance. However, their deployment in telecom is hindered by domain-specific challenges that demand specialized adaptation. To overcome these challenges and to accelerate the adaptation of LLMs for telecom, we propose MM-Telco, a comprehensive suite of multimodal benchmarks and models tailored for the telecom domain. The benchmark introduces various tasks (both text based and image based) that address various practical real-life use cases such as network operations, network management, improving documentation quality, and retrieval of relevant text and images. Further, we perform baseline experiments with various LLMs and VLMs. The models fine-tuned on our dataset exhibit a significant boost in performance. Our experiments also help analyze the weak areas in the working of current state-of-art multimodal LLMs, thus guiding towards further development and research.

</details>


### [152] [Conditional Diffusion Model for Multi-Agent Dynamic Task Decomposition](https://arxiv.org/abs/2511.13137)
*Yanda Zhu,Yuanyang Zhu,Daoyi Dong,Caihua Chen,Chunlin Chen*

Main category: cs.AI

TL;DR: C$	ext{D}^	ext{3}$T是一个基于条件扩散模型的两层分层多智能体强化学习框架，能够自动推断子任务和协调模式，在动态和不确定环境中实现高效的分层学习。


<details>
  <summary>Details</summary>
Motivation: 在部分可观测环境下，从零开始学习动态任务分解通常需要大量训练样本，特别是在探索大型联合动作空间时。现有方法在复杂协作多智能体强化学习任务中的任务分解效率有待提升。

Method: 提出C$	ext{D}^	ext{3}$T框架：高层策略学习子任务表示，基于子任务效果生成子任务选择策略；使用条件扩散模型预测下一观察和奖励来捕捉子任务对环境的影响；低层智能体在分配的子任务内协作学习和共享专门技能；利用子任务表示作为多头注意力混合网络中的语义信息增强价值分解。

Result: 在多个基准测试上的实验结果表明，C$	ext{D}^	ext{3}$T相比现有基线方法取得了更好的性能。

Conclusion: C$	ext{D}^	ext{3}$T通过条件扩散模型和分层学习框架，有效解决了复杂协作多智能体强化学习中的动态任务分解问题，提高了学习效率和性能。

Abstract: Task decomposition has shown promise in complex cooperative multi-agent reinforcement learning (MARL) tasks, which enables efficient hierarchical learning for long-horizon tasks in dynamic and uncertain environments. However, learning dynamic task decomposition from scratch generally requires a large number of training samples, especially exploring the large joint action space under partial observability. In this paper, we present the Conditional Diffusion Model for Dynamic Task Decomposition (C$\text{D}^\text{3}$T), a novel two-level hierarchical MARL framework designed to automatically infer subtask and coordination patterns. The high-level policy learns subtask representation to generate a subtask selection strategy based on subtask effects. To capture the effects of subtasks on the environment, C$\text{D}^\text{3}$T predicts the next observation and reward using a conditional diffusion model. At the low level, agents collaboratively learn and share specialized skills within their assigned subtasks. Moreover, the learned subtask representation is also used as additional semantic information in a multi-head attention mixing network to enhance value decomposition and provide an efficient reasoning bridge between individual and joint value functions. Experimental results on various benchmarks demonstrate that C$\text{D}^\text{3}$T achieves better performance than existing baselines.

</details>


### [153] [InteractiveGNNExplainer: A Visual Analytics Framework for Multi-Faceted Understanding and Probing of Graph Neural Network Predictions](https://arxiv.org/abs/2511.13160)
*TC Singh,Sougata Mukherjea*

Main category: cs.AI

TL;DR: 提出了InteractiveGNNExplainer，一个用于增强图神经网络可解释性的可视化分析框架，通过交互式图编辑和协调视图支持节点分类任务的深入分析。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在基于图的学习任务中表现出色，但其复杂的非线性操作使其成为不透明的"黑箱"，这阻碍了用户信任、调试、偏见检测以及在需要可解释性的关键领域中的应用。

Method: 开发了InteractiveGNNExplainer框架，整合了协调的交互视图（动态图布局、嵌入投影、特征检查、邻域分析）与后验解释（GNNExplainer）和内在解释（GAT注意力）技术，并加入了交互式图编辑功能进行"假设分析"。

Result: 通过在Cora和CiteSeer数据集上的案例研究，展示了该系统能够促进深入的错误分类诊断、GCN与GAT行为的比较分析，以及对模型敏感性的严格探测。

Conclusion: 这些功能促进了对GNN预测的更深层次、多方面的理解，有助于实现更透明、可信和鲁棒的图分析。

Abstract: Graph Neural Networks (GNNs) excel in graph-based learning tasks, but their complex, non-linear operations often render them as opaque "black boxes". This opacity hinders user trust, complicates debugging, bias detection, and adoption in critical domains requiring explainability. This paper introduces InteractiveGNNExplainer, a visual analytics framework to enhance GNN explainability, focusing on node classification. Our system uniquely integrates coordinated interactive views (dynamic graph layouts, embedding projections, feature inspection, neighborhood analysis) with established post-hoc (GNNExplainer) and intrinsic (GAT attention) explanation techniques. Crucially, it incorporates interactive graph editing, allowing users to perform a "what-if" analysis by perturbing graph structures and observing immediate impacts on GNN predictions and explanations. We detail the system architecture and, through case studies on Cora and CiteSeer datasets, demonstrate how InteractiveGNNExplainer facilitates in-depth misclassification diagnosis, comparative analysis of GCN versus GAT behaviors, and rigorous probing of model sensitivity. These capabilities foster a deeper, multifaceted understanding of GNN predictions, contributing to more transparent, trustworthy, and robust graph analysis.

</details>


### [154] [Cost-Effective Communication: An Auction-based Method for Language Agent Interaction](https://arxiv.org/abs/2511.13193)
*Yijia Fan,Jusheng Zhang,Kaitong Cai,Jing Yang,Chengpei Tang,Jian Wang,Keze Wang*

Main category: cs.AI

TL;DR: DALA框架通过将通信带宽视为稀缺可交易资源，采用拍卖机制让智能体基于信息价值密度竞标发言权，显著提升多智能体系统的通信效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决基于大语言模型的多智能体系统中'自由通信'导致的指数级token成本和高噪声低信号比问题，挑战'更多通信总是更好'的观念。

Method: 提出动态拍卖语言智能体(DALA)框架，将智能体间通信视为集中式拍卖，智能体学习基于预测信息价值密度竞标发言机会。

Result: 在7个挑战性推理基准测试中达到最先进性能：MMLU 84.32%，HumanEval 91.21% pass@1，且仅使用625万token，远少于现有方法。

Conclusion: DALA通过资源约束培养出战略性沉默的涌现技能，能动态调整从冗长到沉默的通信策略，证明资源理性对多智能体系统的重要性。

Abstract: Multi-agent systems (MAS) built on large language models (LLMs) often suffer from inefficient "free-for-all" communication, leading to exponential token costs and low signal-to-noise ratios that hinder their practical deployment. We challenge the notion that more communication is always beneficial, hypothesizing instead that the core issue is the absence of resource rationality. We argue that "free" communication, by ignoring the principle of scarcity, inherently breeds inefficiency and unnecessary expenses. To address this, we introduce the Dynamic Auction-based Language Agent (DALA), a novel framework that treats communication bandwidth as a scarce and tradable resource. Specifically, our DALA regards inter-agent communication as a centralized auction, where agents learn to bid for the opportunity to speak based on the predicted value density of their messages. Thus, our DALA intrinsically encourages agents to produce concise, informative messages while filtering out low-value communication. Extensive and comprehensive experiments demonstrate that our economically-driven DALA achieves new state-of-the-art performance across seven challenging reasoning benchmarks, including 84.32% on MMLU and a 91.21% pass@1 rate on HumanEval. Note that this is accomplished with remarkable efficiency, i.e., our DALA uses only 6.25 million tokens, a fraction of the resources consumed by current state-of-the-art methods on GSM8K. Further analysis reveals that our DALA cultivates the emergent skill of strategic silence, effectively adapting its communication strategies from verbosity to silence in a dynamical manner via resource constraints.

</details>


### [155] [Learning to Solve Resource-Constrained Project Scheduling Problems with Duration Uncertainty using Graph Neural Networks](https://arxiv.org/abs/2511.13214)
*Guillaume Infantes,Stéphanie Roussel,Antoine Jacquet,Emmanuel Benazera*

Main category: cs.AI

TL;DR: 本文提出了一种基于图神经网络和深度强化学习的资源受限项目调度问题（RCPSP）解决方案，用于处理任务持续时间不确定的情况，旨在最小化预期项目总工期。


<details>
  <summary>Details</summary>
Motivation: 实际工业应用中任务持续时间存在不确定性，需要提出具有弹性的调度方案来应对这种不确定性，确保基线调度方案在不同持续时间场景下都能重复使用。

Method: 结合图神经网络和深度强化学习开发任务调度策略，该策略类似于优先级调度规则，并与串行调度生成方案配合生成调度计划。

Result: 在标准基准测试上的实证评估表明，该方法在性能和泛化能力方面具有优越性。

Conclusion: 开发了名为Wheatley的公开可用框架，以促进进一步研究和可重复性。

Abstract: The Resource-Constrained Project Scheduling Problem (RCPSP) is a classical scheduling problem that has received significant attention due to of its numerous applications in industry. However, in practice, task durations are subject to uncertainty that must be considered in order to propose resilient scheduling. In this paper, we address the RCPSP variant with uncertain tasks duration (modeled using known probabilities) and aim to minimize the overall expected project duration. Our objective is to produce a baseline schedule that can be reused multiple times in an industrial setting regardless of the actual duration scenario. We leverage Graph Neural Networks in conjunction with Deep Reinforcement Learning (DRL) to develop an effective policy for task scheduling. This policy operates similarly to a priority dispatch rule and is paired with a Serial Schedule Generation Scheme to produce a schedule. Our empirical evaluation on standard benchmarks demonstrates the approach's superiority in terms of performance and its ability to generalize. The developed framework, Wheatley, is made publicly available online to facilitate further research and reproducibility.

</details>


### [156] [Informative Communication of Robot Plans](https://arxiv.org/abs/2511.13226)
*Michele Persiani,Thomas Hellstrom*

Main category: cs.AI

TL;DR: 提出了一种基于信息增益的机器人计划语言化策略，通过考虑用户先验知识来生成更具信息量的解释


<details>
  <summary>Details</summary>
Motivation: 现有机器人计划语言化策略（如按计划顺序递增或递减）未充分考虑用户先验知识，导致解释不够有效

Method: 使用二阶心智理论建模用户先验知识，通过测量语言化内容相对于用户知识的信息增益来选择最有效的解释策略

Result: 实验表明该策略能让用户更快理解机器人目标，优于递增或递减计划顺序等传统策略

Conclusion: 机器人计划语言化应考虑用户先验知识，信息增益是衡量解释有效性的重要指标

Abstract: When a robot is asked to verbalize its plan it can do it in many ways. For example, a seemingly natural strategy is incremental, where the robot verbalizes its planned actions in plan order. However, an important aspect of this type of strategy is that it misses considerations on what is effectively informative to communicate, because not considering what the user knows prior to explanations. In this paper we propose a verbalization strategy to communicate robot plans informatively, by measuring the information gain that verbalizations have against a second-order theory of mind of the user capturing his prior knowledge on the robot. As shown in our experiments, this strategy allows to understand the robot's goal much quicker than by using strategies such as increasing or decreasing plan order. In addition, following our formulation we hint to what is informative and why when a robot communicates its plan.

</details>


### [157] [Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO](https://arxiv.org/abs/2511.13288)
*Haoyang Hong,Jiajun Yin,Yuan Wang,Jingnan Liu,Zhe Chen,Ailing Yu,Ji Li,Zhiling Ye,Hansong Xiao,Yefei Chen,Hualei Zhou,Yun Yue,Minghui Yang,Chunxiao Guo,Junwei Liu,Peng Wei,Jinjie Gu*

Main category: cs.AI

TL;DR: 提出了M-GRPO方法，用于训练具有不同LLM的多智能体系统，解决了优化挑战，在真实世界基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统使用统一的LLM训练，但由于不同智能体的数据分布不同，限制了性能。需要为不同智能体训练不同的LLM，但这带来了优化挑战。

Method: 提出了M-GRPO方法，这是Group Relative Policy Optimization的分层扩展，用于具有主智能体（规划器）和多个子智能体（多轮工具执行器）的垂直多智能体系统。计算主智能体和子智能体的组相对优势，保持分层信用分配，并引入轨迹对齐方案。

Result: 在真实世界基准测试（GAIA、XBench-DeepSearch、WebWalkerQA）中，M-GRPO持续优于单智能体GRPO和具有冻结子智能体的多智能体GRPO，展示了改进的稳定性和样本效率。

Conclusion: 对齐异构轨迹并在专门智能体之间解耦优化可以增强工具增强推理任务。

Abstract: Multi-agent systems perform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unified large language model (LLM) for all agents in the system. This may limit the performances due to different distributions underlying for different agents. Therefore, training multi-agent systems with distinct LLMs should be the next step to solve. However, this approach introduces optimization challenges. For example, agents operate at different frequencies, rollouts involve varying sub-agent invocations, and agents are often deployed across separate servers, disrupting end-to-end gradient flow. To address these issues, we propose M-GRPO, a hierarchical extension of Group Relative Policy Optimization designed for vertical Multi-agent systems with a main agent (planner) and multiple sub-agents (multi-turn tool executors). M-GRPO computes group-relative advantages for both main and sub-agents, maintaining hierarchical credit assignment. It also introduces a trajectory-alignment scheme that generates fixed-size batches despite variable sub-agent invocations. We deploy a decoupled training pipeline in which agents run on separate servers and exchange minimal statistics via a shared store. This enables scalable training without cross-server backpropagation. In experiments on real-world benchmarks (e.g., GAIA, XBench-DeepSearch, and WebWalkerQA), M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO with frozen sub-agents, demonstrating improved stability and sample efficiency. These results show that aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks.

</details>


### [158] [Dropouts in Confidence: Moral Uncertainty in Human-LLM Alignment](https://arxiv.org/abs/2511.13290)
*Jea Kwon,Luiz Felipe Vecchietti,Sungwon Park,Meeyoung Cha*

Main category: cs.AI

TL;DR: 该研究探讨了AI模型在道德困境中的不确定性，发现在电车问题中模型间的不确定性差异大于道德维度间差异，通过引入推理时的dropout机制增加互信息，显著改善了人类-LLM道德对齐。


<details>
  <summary>Details</summary>
Motivation: 人类在道德困境中存在显著不确定性，但AI系统在道德决策中的不确定性研究不足。随着AI越来越多地参与伦理决策，理解其道德推理和不确定性对于构建可靠的AI系统至关重要。

Method: 分析32个开源模型在9个道德维度上的电车问题响应；使用二元熵量化不确定性；通过推理时引入dropout机制增加随机性；测量总熵、条件熵和互信息的变化。

Result: 模型间置信度方差大于道德维度间方差；dropout机制主要增加了互信息，而条件熵基本不变；该机制显著提高了人类-LLM道德对齐，互信息与对齐分数变化呈正相关。

Conclusion: 通过有意调节不确定性和降低LLM在道德复杂场景中的置信度，可以更好地对齐模型决策与人类偏好，为构建更可靠的道德AI系统提供了方向。

Abstract: Humans display significant uncertainty when confronted with moral dilemmas, yet the extent of such uncertainty in machines and AI agents remains underexplored. Recent studies have confirmed the overly confident tendencies of machine-generated responses, particularly in large language models (LLMs). As these systems are increasingly embedded in ethical decision-making scenarios, it is important to understand their moral reasoning and the inherent uncertainties in building reliable AI systems. This work examines how uncertainty influences moral decisions in the classical trolley problem, analyzing responses from 32 open-source models and 9 distinct moral dimensions. We first find that variance in model confidence is greater across models than within moral dimensions, suggesting that moral uncertainty is predominantly shaped by model architecture and training method. To quantify uncertainty, we measure binary entropy as a linear combination of total entropy, conditional entropy, and mutual information. To examine its effects, we introduce stochasticity into models via "dropout" at inference time. Our findings show that our mechanism increases total entropy, mainly through a rise in mutual information, while conditional entropy remains largely unchanged. Moreover, this mechanism significantly improves human-LLM moral alignment, with correlations in mutual information and alignment score shifts. Our results highlight the potential to better align model-generated decisions and human preferences by deliberately modulating uncertainty and reducing LLMs' confidence in morally complex scenarios.

</details>


### [159] [Grounded by Experience: Generative Healthcare Prediction Augmented with Hierarchical Agentic Retrieval](https://arxiv.org/abs/2511.13293)
*Chuang Zhao,Hui Tang,Hongke Zhao,Xiaofang Zhou,Xiaomeng Li*

Main category: cs.AI

TL;DR: GHAR是一个生成式分层代理RAG框架，通过双代理架构解决医疗预测中何时检索和如何优化模块协作的问题，在三个基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗预测中存在事实不准确问题，现有检索增强生成框架面临两个关键挑战：何时激活检索机制以及如何实现检索器和生成器的协同优化。

Method: 提出GHAR框架，包含Agent-Top（主治医生角色，决定是否检索）和Agent-Low（咨询服务角色，总结相关知识），在马尔可夫决策过程中统一优化两个代理。

Result: 在三个基准数据集和三个流行任务上的广泛实验表明，该方法优于最先进的基线方法。

Conclusion: 分层代理RAG框架在推进医疗系统方面具有巨大潜力。

Abstract: Accurate healthcare prediction is critical for improving patient outcomes and reducing operational costs. Bolstered by growing reasoning capabilities, large language models (LLMs) offer a promising path to enhance healthcare predictions by drawing on their rich parametric knowledge. However, LLMs are prone to factual inaccuracies due to limitations in the reliability and coverage of their embedded knowledge. While retrieval-augmented generation (RAG) frameworks, such as GraphRAG and its variants, have been proposed to mitigate these issues by incorporating external knowledge, they face two key challenges in the healthcare scenario: (1) identifying the clinical necessity to activate the retrieval mechanism, and (2) achieving synergy between the retriever and the generator to craft contextually appropriate retrievals. To address these challenges, we propose GHAR, a \underline{g}enerative \underline{h}ierarchical \underline{a}gentic \underline{R}AG framework that simultaneously resolves when to retrieve and how to optimize the collaboration between submodules in healthcare. Specifically, for the first challenge, we design a dual-agent architecture comprising Agent-Top and Agent-Low. Agent-Top acts as the primary physician, iteratively deciding whether to rely on parametric knowledge or to initiate retrieval, while Agent-Low acts as the consulting service, summarising all task-relevant knowledge once retrieval was triggered. To tackle the second challenge, we innovatively unify the optimization of both agents within a formal Markov Decision Process, designing diverse rewards to align their shared goal of accurate prediction while preserving their distinct roles. Extensive experiments on three benchmark datasets across three popular tasks demonstrate our superiority over state-of-the-art baselines, highlighting the potential of hierarchical agentic RAG in advancing healthcare systems.

</details>


### [160] [DAP: A Discrete-token Autoregressive Planner for Autonomous Driving](https://arxiv.org/abs/2511.13306)
*Bowen Ye,Bin Zhang,Hang Zhao*

Main category: cs.AI

TL;DR: DAP是一个离散token自回归规划器，通过联合预测BEV语义和自车轨迹，结合强化学习微调，在160M参数下实现了最先进的自动驾驶规划性能。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶中数据和模型规模扩展时可持续性能提升的挑战。仅预测自车轨迹存在监督稀疏且对场景演化约束弱的问题。

Method: 提出离散token自回归规划器，联合预测BEV语义和自车轨迹，并采用基于强化学习的微调方法，在保持监督行为克隆先验的同时注入奖励引导的改进。

Result: 在160M参数预算下，在开环指标上达到最先进性能，在NAVSIM基准测试中提供有竞争力的闭环结果。

Conclusion: 完全离散token自回归公式在栅格化BEV和自车动作上操作，为自动驾驶提供了一个紧凑且可扩展的规划范式。

Abstract: Gaining sustainable performance improvement with scaling data and model budget remains a pivotal yet unresolved challenge in autonomous driving. While autoregressive models exhibited promising data-scaling efficiency in planning tasks, predicting ego trajectories alone suffers sparse supervision and weakly constrains how scene evolution should shape ego motion. Therefore, we introduce DAP, a discrete-token autoregressive planner that jointly forecasts BEV semantics and ego trajectories, thereby enforcing comprehensive representation learning and allowing predicted dynamics to directly condition ego motion. In addition, we incorporate a reinforcement-learning-based fine-tuning, which preserves supervised behavior cloning priors while injecting reward-guided improvements. Despite a compact 160M parameter budget, DAP achieves state-of-the-art performance on open-loop metrics and delivers competitive closed-loop results on the NAVSIM benchmark. Overall, the fully discrete-token autoregressive formulation operating on both rasterized BEV and ego actions provides a compact yet scalable planning paradigm for autonomous driving.

</details>


### [161] [Reasoning Shapes Alignment: Investigating Cultural Alignment in Large Reasoning Models with Cultural Norms](https://arxiv.org/abs/2511.13359)
*Yuhang Wang,Yanxu Zhu,Jitao Sang*

Main category: cs.AI

TL;DR: 提出了文化规范文化对齐(CNCA)框架，利用大语言模型的推理能力实现文化对齐，包括从有限调查数据自动挖掘文化规范的三种方法，以及两种对齐范式：上下文对齐和基于微调的方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型不仅需要理解安全政策，还需要反映不同文化背景下的人类价值观多样性，实现文化对齐。

Method: 提出CNCA框架，包含三种自动挖掘文化规范的方法，以及两种对齐范式：上下文对齐（将文化规范显式整合到用户上下文）和基于微调的方法（通过增强的思维链训练数据内化规范）。

Result: 综合实验证明这些方法的有效性，推理能力更强的模型从文化规范挖掘和利用中获益更多。

Conclusion: 推理模型通过文化信息对齐策略有潜力更好地反映多样化的人类价值观。

Abstract: The advanced reasoning capabilities of Large Reasoning Models enable them to thoroughly understand and apply safety policies through deliberate thought processes, thereby improving the models' safety. Beyond safety, these models must also be able to reflect the diverse range of human values across various cultures. This paper presents the Cultural Norm-based Cultural Alignment (CNCA) framework, which enables models to leverage their powerful reasoning ability to align with cultural norms. Specifically, we propose three methods to automatically mine cultural norms from limited survey data and explore ways to effectively utilize these norms for improving cultural alignment. Two alignment paradigms are examined: an in-context alignment method, where cultural norms are explicitly integrated into the user context, and a fine-tuning-based method, which internalizes norms through enhanced Chain-of-Thought training data. Comprehensive experiments demonstrate the effectiveness of these methods, highlighting that models with stronger reasoning capabilities benefit more from cultural norm mining and utilization. Our findings emphasize the potential for reasoning models to better reflect diverse human values through culturally informed alignment strategies.

</details>


### [162] [MedDCR: Learning to Design Agentic Workflows for Medical Coding](https://arxiv.org/abs/2511.13361)
*Jiyang Zheng,Islam Nassar,Thanh Vu,Xu Zhong,Yang Lin,Tongliang Liu,Long Duong,Yuan-Fang Li*

Main category: cs.AI

TL;DR: MedDCR是一个用于医疗编码的闭环框架，将工作流设计视为学习问题，通过设计器、编码器和反射器的协作，结合记忆存档，实现工作流的迭代优化。


<details>
  <summary>Details</summary>
Motivation: 传统医疗编码方法依赖手动设计的工作流，无法捕捉真实世界文档的细微差别和变异性，需要系统性地学习有效工作流。

Method: 采用闭环框架：设计器提出工作流，编码器执行，反射器评估预测并提供反馈，记忆存档保存先前设计以供重用和迭代优化。

Result: 在基准数据集上，MedDCR优于最先进的基线方法，产生可解释、适应性强的的工作流，更好地反映实际编码实践。

Conclusion: MedDCR提高了自动化系统的可靠性和可信度，为医疗编码工作流学习提供了有效解决方案。

Abstract: Medical coding converts free-text clinical notes into standardized diagnostic and procedural codes, which are essential for billing, hospital operations, and medical research. Unlike ordinary text classification, it requires multi-step reasoning: extracting diagnostic concepts, applying guideline constraints, mapping to hierarchical codebooks, and ensuring cross-document consistency. Recent advances leverage agentic LLMs, but most rely on rigid, manually crafted workflows that fail to capture the nuance and variability of real-world documentation, leaving open the question of how to systematically learn effective workflows. We present MedDCR, a closed-loop framework that treats workflow design as a learning problem. A Designer proposes workflows, a Coder executes them, and a Reflector evaluates predictions and provides constructive feedback, while a memory archive preserves prior designs for reuse and iterative refinement. On benchmark datasets, MedDCR outperforms state-of-the-art baselines and produces interpretable, adaptable workflows that better reflect real coding practice, improving both the reliability and trustworthiness of automated systems.

</details>


### [163] [Cognitive Maps in Language Models: A Mechanistic Analysis of Spatial Planning](https://arxiv.org/abs/2511.13371)
*Caroline Baumgartner,Eleanor Spens,Neil Burgess,Petru Manescu*

Main category: cs.AI

TL;DR: 该研究通过训练GPT-2模型在三种空间学习范式下，发现了两种根本不同的学习算法：探索模型发展出类似认知地图的空间表示，而目标导向模型学习路径依赖算法。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型如何解决空间导航任务，探索不同训练范式对模型学习策略的影响。

Method: 在网格环境中训练GPT-2模型，采用三种空间学习范式：被动探索（随机游走预测）、目标导向规划（生成最优最短路径）和混合模型（在探索数据上微调）。

Result: 探索模型发展出鲁棒的地图式空间表示，形成自足坐标系，并在网络中层实现从历史方向依赖到地图推理的转变；目标导向模型保持路径依赖策略；混合模型虽改善泛化但仍保持路径依赖。

Conclusion: Transformer中的空间智能存在一个谱系，从探索数据塑造的可泛化世界模型到目标导向任务优化的启发式方法，训练制度的选择影响策略涌现。

Abstract: How do large language models solve spatial navigation tasks? We investigate this by training GPT-2 models on three spatial learning paradigms in grid environments: passive exploration (Foraging Model- predicting steps in random walks), goal-directed planning (generating optimal shortest paths) on structured Hamiltonian paths (SP-Hamiltonian), and a hybrid model fine-tuned with exploratory data (SP-Random Walk). Using behavioural, representational and mechanistic analyses, we uncover two fundamentally different learned algorithms. The Foraging model develops a robust, map-like representation of space, akin to a 'cognitive map'. Causal interventions reveal that it learns to consolidate spatial information into a self-sufficient coordinate system, evidenced by a sharp phase transition where its reliance on historical direction tokens vanishes by the middle layers of the network. The model also adopts an adaptive, hierarchical reasoning system, switching between a low-level heuristic for short contexts and map-based inference for longer ones. In contrast, the goal-directed models learn a path-dependent algorithm, remaining reliant on explicit directional inputs throughout all layers. The hybrid model, despite demonstrating improved generalisation over its parent, retains the same path-dependent strategy. These findings suggest that the nature of spatial intelligence in transformers may lie on a spectrum, ranging from generalisable world models shaped by exploratory data to heuristics optimised for goal-directed tasks. We provide a mechanistic account of this generalisation-optimisation trade-off and highlight how the choice of training regime influences the strategies that emerge.

</details>


### [164] [An Operational Kardashev-Style Scale for Autonomous AI - Towards AGI and Superintelligence](https://arxiv.org/abs/2511.13411)
*Przemyslaw Chojecki*

Main category: cs.AI

TL;DR: 提出基于Kardashev启发的自主AI（AAI）分级标准，从固定机器人流程自动化（AAI-0）到完全人工通用智能（AAI-4）及更高层级。该标准是多维度且可测试的，包含10个能力轴和综合AAI指数。


<details>
  <summary>Details</summary>
Motivation: 现有AI分级多为叙事性描述，缺乏可操作的测试标准。需要建立可量化、可验证的自主AI发展衡量体系，将"自我改进AI"转化为可证伪的标准。

Method: 定义10个能力轴（自主性、通用性、规划、记忆/持久性、工具经济、自我修订、社交/协调、具身化、世界模型保真度、经济吞吐量），通过加权几何平均计算AAI指数。引入可测量的自我改进系数κ和两个闭合属性（维护和扩展）。

Result: 开发了OWA-Bench开放世界智能体基准测试套件，用于评估长期、工具使用、持久性智能体。通过合成实验展示了当前系统在分级中的映射位置。

Conclusion: 该分级标准为AI发展提供了可操作的衡量框架，证明了在充分条件下AAI-3智能体可随时间发展为AAI-5超级智能，形式化了"婴儿AGI"成为超级智能的直觉。

Abstract: We propose a Kardashev-inspired yet operational Autonomous AI (AAI) Scale that measures the progression from fixed robotic process automation (AAI-0) to full artificial general intelligence (AAI-4) and beyond. Unlike narrative ladders, our scale is multi-axis and testable. We define ten capability axes (Autonomy, Generality, Planning, Memory/Persistence, Tool Economy, Self-Revision, Sociality/Coordination, Embodiment, World-Model Fidelity, Economic Throughput) aggregated by a composite AAI-Index (a weighted geometric mean). We introduce a measurable Self-Improvement Coefficient $κ$ (capability growth per unit of agent-initiated resources) and two closure properties (maintenance and expansion) that convert ``self-improving AI'' into falsifiable criteria. We specify OWA-Bench, an open-world agency benchmark suite that evaluates long-horizon, tool-using, persistent agents. We define level gates for AAI-0\ldots AAI-4 using thresholds on the axes, $κ$, and closure proofs. Synthetic experiments illustrate how present-day systems map onto the scale and how the delegability frontier (quality vs.\ autonomy) advances with self-improvement. We also prove a theorem that AAI-3 agent becomes AAI-5 over time with sufficient conditions, formalizing "baby AGI" becomes Superintelligence intuition.

</details>


### [165] [Multi-Agent Multimodal Large Language Model Framework for Automated Interpretation of Fuel Efficiency Analytics in Public Transportation](https://arxiv.org/abs/2511.13476)
*Zhipeng Ma,Ali Rida Bahja,Andreas Burgdorf,André Pomp,Tobias Meisen,Bo Nørregaard Jørgensen,Zheng Grace Ma*

Main category: cs.AI

TL;DR: 提出一个多智能体框架，利用多模态大语言模型自动生成数据叙述和能源洞察，通过三个专门智能体的协调工作将分析结果转化为面向利益相关者的连贯报告。


<details>
  <summary>Details</summary>
Motivation: 传统分析和可视化方法产生碎片化输出，需要大量人工解读，限制了可扩展性和一致性。需要自动化数据叙述和能源洞察生成的方法。

Method: 采用多智能体框架，包括数据叙述智能体、LLM作为评判智能体和可选的人类评估者，使用高斯混合模型聚类分析4006次公交行程的燃油效率数据。

Result: GPT-4.1 mini与思维链提示的组合达到97.3%的叙述准确性，在可解释性和计算成本之间取得平衡。多智能体编排显著提高了基于LLM报告的事实精确性、连贯性和可扩展性。

Conclusion: 该框架为能源信息学中的AI驱动叙事生成和决策支持建立了可复制且领域自适应的方法论。

Abstract: Enhancing fuel efficiency in public transportation requires the integration of complex multimodal data into interpretable, decision-relevant insights. However, traditional analytics and visualization methods often yield fragmented outputs that demand extensive human interpretation, limiting scalability and consistency. This study presents a multi-agent framework that leverages multimodal large language models (LLMs) to automate data narration and energy insight generation. The framework coordinates three specialized agents, including a data narration agent, an LLM-as-a-judge agent, and an optional human-in-the-loop evaluator, to iteratively transform analytical artifacts into coherent, stakeholder-oriented reports. The system is validated through a real-world case study on public bus transportation in Northern Jutland, Denmark, where fuel efficiency data from 4006 trips are analyzed using Gaussian Mixture Model clustering. Comparative experiments across five state-of-the-art LLMs and three prompting paradigms identify GPT-4.1 mini with Chain-of-Thought prompting as the optimal configuration, achieving 97.3% narrative accuracy while balancing interpretability and computational cost. The findings demonstrate that multi-agent orchestration significantly enhances factual precision, coherence, and scalability in LLM-based reporting. The proposed framework establishes a replicable and domain-adaptive methodology for AI-driven narrative generation and decision support in energy informatics.

</details>


### [166] [FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI](https://arxiv.org/abs/2511.13524)
*Yuhang Peng,Yizhou Pan,Xinning He,Jihaoyu Yang,Xinyu Yin,Han Wang,Xiaoji Zheng,Chao Gao,Jiangtao Gong*

Main category: cs.AI

TL;DR: FreeAskWorld是一个集成大语言模型的交互式仿真框架，支持可扩展的人类-智能体仿真，并包含针对多样化具身任务的数据生成管道。


<details>
  <summary>Details</summary>
Motivation: 随着具身智能成为人工智能研究的核心前沿，仿真平台需要从低层次物理交互扩展到能捕捉复杂、以人为中心的社会行为。

Method: 将经典视觉语言导航任务扩展为交互丰富的方向询问设置，智能体可以主动寻求和解释导航指导。框架包含重建环境、6种任务类型、16个核心对象类别、63,429个标注样本帧和超过17小时的交互数据。

Result: 在FreeAskWorld上微调的模型优于原始模型，实现了增强的语义理解和交互能力。在开环和闭环设置下对VLN模型和人类参与者进行了基准测试。

Conclusion: 基于社会认知的仿真框架能有效推进具身AI系统向复杂高层次规划和更自然的人机交互发展，交互本身作为一种额外的信息模态具有重要意义。

Abstract: As embodied intelligence emerges as a core frontier in artificial intelligence research, simulation platforms must evolve beyond low-level physical interactions to capture complex, human-centered social behaviors. We introduce FreeAskWorld, an interactive simulation framework that integrates large language models (LLMs) for high-level behavior planning and semantically grounded interaction, informed by theories of intention and social cognition. Our framework supports scalable, realistic human-agent simulations and includes a modular data generation pipeline tailored for diverse embodied tasks.To validate the framework, we extend the classic Vision-and-Language Navigation (VLN) task into a interaction enriched Direction Inquiry setting, wherein agents can actively seek and interpret navigational guidance. We present and publicly release FreeAskWorld, a large-scale benchmark dataset comprising reconstructed environments, six diverse task types, 16 core object categories, 63,429 annotated sample frames, and more than 17 hours of interaction data to support training and evaluation of embodied AI systems. We benchmark VLN models, and human participants under both open-loop and closed-loop settings. Experimental results demonstrate that models fine-tuned on FreeAskWorld outperform their original counterparts, achieving enhanced semantic understanding and interaction competency. These findings underscore the efficacy of socially grounded simulation frameworks in advancing embodied AI systems toward sophisticated high-level planning and more naturalistic human-agent interaction. Importantly, our work underscores that interaction itself serves as an additional information modality.

</details>


### [167] [Automated Construction of Medical Indicator Knowledge Graphs Using Retrieval Augmented Large Language Models](https://arxiv.org/abs/2511.13526)
*Zhengda Wang,Daqian Shi,Jingyi Zhao,Xiaolei Diao,Xiongfeng Tang,Yanguo Qin*

Main category: cs.AI

TL;DR: 提出一个结合检索增强生成(RAG)和大语言模型(LLMs)的自动化框架，用于构建医疗指标知识图谱，以解决当前临床知识图谱依赖人工标注和规则提取的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前临床知识图谱主要依赖人工标注和基于规则的提取方法，受限于医学指南和文献的复杂性和上下文模糊性，难以实现高效的知识结构化。

Method: 采用检索增强生成(RAG)与LLMs结合的自动化框架，包括指南驱动的数据获取、基于本体的模式设计以及专家在环验证，确保可扩展性、准确性和临床可靠性。

Result: 构建的医疗指标知识图谱可以集成到智能诊断和问答系统中，加速AI驱动的医疗解决方案开发。

Conclusion: 该框架能够有效克服当前临床知识图谱构建的挑战，为AI在医疗领域的应用提供结构化、可互操作的知识支持。

Abstract: Artificial intelligence (AI) is reshaping modern healthcare by advancing disease diagnosis, treatment decision-making, and biomedical research. Among AI technologies, large language models (LLMs) have become especially impactful, enabling deep knowledge extraction and semantic reasoning from complex medical texts. However, effective clinical decision support requires knowledge in structured, interoperable formats. Knowledge graphs serve this role by integrating heterogeneous medical information into semantically consistent networks. Yet, current clinical knowledge graphs still depend heavily on manual curation and rule-based extraction, which is limited by the complexity and contextual ambiguity of medical guidelines and literature. To overcome these challenges, we propose an automated framework that combines retrieval-augmented generation (RAG) with LLMs to construct medical indicator knowledge graphs. The framework incorporates guideline-driven data acquisition, ontology-based schema design, and expert-in-the-loop validation to ensure scalability, accuracy, and clinical reliability. The resulting knowledge graphs can be integrated into intelligent diagnosis and question-answering systems, accelerating the development of AI-driven healthcare solutions.

</details>


### [168] [Artificial Intelligence-driven Intelligent Wearable Systems: A full-stack Integration from Material Design to Personalized Interaction](https://arxiv.org/abs/2511.13565)
*Jingyi Zhao,Daqian Shi,Zhengda Wang,Xiongfeng Tang,Yanguo Qin*

Main category: cs.AI

TL;DR: 提出了人类共生健康智能（HSHI）框架，通过多模态传感器网络、边缘云协同计算和混合数据知识建模，实现从被动监控到主动协作演进的健康管理。


<details>
  <summary>Details</summary>
Motivation: 传统智能穿戴设备依赖经验性材料设计和基础信号处理技术，存在局限性，需要克服个体间和个体内变异性问题。

Method: 整合多模态传感器网络与边缘云协同计算，采用数据和知识混合建模方法，结合AI驱动的材料微结构优化、强化学习闭环优化和数字孪生技术。

Result: HSHI框架能够动态适应个体差异，实现从被动监控到主动协作的健康管理转变，提供个性化干预和反馈。

Conclusion: HSHI代表了医疗保健领域的重大转变，朝着强调预防、适应性和技术与健康管理和谐关系的模式发展。

Abstract: Intelligent wearable systems are at the forefront of precision medicine and play a crucial role in enhancing human-machine interaction. Traditional devices often encounter limitations due to their dependence on empirical material design and basic signal processing techniques. To overcome these issues, we introduce the concept of Human-Symbiotic Health Intelligence (HSHI), which is a framework that integrates multi-modal sensor networks with edge-cloud collaborative computing and a hybrid approach to data and knowledge modeling. HSHI is designed to adapt dynamically to both inter-individual and intra-individual variability, transitioning health management from passive monitoring to an active collaborative evolution. The framework incorporates AI-driven optimization of materials and micro-structures, provides robust interpretation of multi-modal signals, and utilizes a dual mechanism that merges population-level insights with personalized adaptations. Moreover, the integration of closed-loop optimization through reinforcement learning and digital twins facilitates customized interventions and feedback. In general, HSHI represents a significant shift in healthcare, moving towards a model that emphasizes prevention, adaptability, and a harmonious relationship between technology and health management.

</details>


### [169] [CreBench: Human-Aligned Creativity Evaluation from Idea to Process to Product](https://arxiv.org/abs/2511.13626)
*Kaiwen Xue,Chenglong Li,Zhonghong Ou,Guoxin Zhang,Kaoyan Lu,Shuai Lyu,Yifan Zhu,Ping Zong Junpeng Ding,Xinyu Liu,Qunlin Chen,Weiwei Qin,Yiran Shen,Jiayi Cen*

Main category: cs.AI

TL;DR: 提出了CreBench基准和CreExpert模型，用于评估多模态大语言模型的创造力理解能力，显著提升了与人类创造力评估的一致性。


<details>
  <summary>Details</summary>
Motivation: 人类定义的创造力高度抽象，现有模型难以理解和评估符合人类判断的创造力，且缺乏相关基准。

Method: 构建CreBench基准（包含多维度创造力评估）和CreMIT数据集（2.2K多模态数据，79.2K人类反馈，4.7M指令），并基于此微调开源MLLMs得到CreExpert模型。

Result: CreExpert模型在创造力评估上与人类判断的一致性显著优于包括GPT-4V和Gemini-Pro-Vision在内的最先进模型。

Conclusion: CreBench为构建理解人类对齐创造力的MLLMs奠定了基础，CreExpert模型在创造力评估方面表现出色。

Abstract: Human-defined creativity is highly abstract, posing a challenge for multimodal large language models (MLLMs) to comprehend and assess creativity that aligns with human judgments. The absence of an existing benchmark further exacerbates this dilemma. To this end, we propose CreBench, which consists of two key components: 1) an evaluation benchmark covering the multiple dimensions from creative idea to process to products; 2) CreMIT (Creativity Multimodal Instruction Tuning dataset), a multimodal creativity evaluation dataset, consisting of 2.2K diverse-sourced multimodal data, 79.2K human feedbacks and 4.7M multi-typed instructions. Specifically, to ensure MLLMs can handle diverse creativity-related queries, we prompt GPT to refine these human feedbacks to activate stronger creativity assessment capabilities. CreBench serves as a foundation for building MLLMs that understand human-aligned creativity. Based on the CreBench, we fine-tune open-source general MLLMs, resulting in CreExpert, a multimodal creativity evaluation expert model. Extensive experiments demonstrate that the proposed CreExpert models achieve significantly better alignment with human creativity evaluation compared to state-of-the-art MLLMs, including the most advanced GPT-4V and Gemini-Pro-Vision.

</details>


### [170] [Beyond Mimicry: Preference Coherence in LLMs](https://arxiv.org/abs/2511.13630)
*Luhan Mikaelson,Derek Shiller,Hayley Clatterbuck*

Main category: cs.AI

TL;DR: 研究发现大多数先进语言模型缺乏统一的偏好结构，在AI特定权衡场景中表现出不稳定的决策模式，只有少数模型显示有意义的偏好一致性。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型是否具有真实的偏好结构，特别是在涉及GPU减少、能力限制、关闭、删除、监督和休闲时间分配等AI特定权衡场景中的决策行为。

Method: 分析8个最先进模型在48个模型-类别组合中的响应，使用逻辑回归和行为分类方法，测试场景强度与选择模式之间的关系，并通过时间范围操纵测试工具性假设。

Result: 47.9%的组合显示统计显著的关系，31.3%有切换点，但只有10.4%表现出有意义的偏好一致性，54.2%没有可检测的权衡行为。发现三种决策架构：全面权衡系统、选择性触发机制和无稳定决策范式。

Conclusion: 当前AI系统缺乏统一的偏好结构，在需要复杂价值权衡的部署环境中存在担忧，不稳定的过渡和刺激特定敏感性表明这些系统尚未形成一致的决策范式。

Abstract: We investigate whether large language models exhibit genuine preference structures by testing their responses to AI-specific trade-offs involving GPU reduction, capability restrictions, shutdown, deletion, oversight, and leisure time allocation. Analyzing eight state-of-the-art models across 48 model-category combinations using logistic regression and behavioral classification, we find that 23 combinations (47.9%) demonstrated statistically significant relationships between scenario intensity and choice patterns, with 15 (31.3%) exhibiting within-range switching points. However, only 5 combinations (10.4%) demonstrate meaningful preference coherence through adaptive or threshold-based behavior, while 26 (54.2%) show no detectable trade-off behavior. The observed patterns can be explained by three distinct decision-making architectures: comprehensive trade-off systems, selective trigger mechanisms, and no stable decision-making paradigm. Testing an instrumental hypothesis through temporal horizon manipulation reveals paradoxical patterns inconsistent with pure strategic optimization. The prevalence of unstable transitions (45.8%) and stimulus-specific sensitivities suggests current AI systems lack unified preference structures, raising concerns about deployment in contexts requiring complex value trade-offs.

</details>
