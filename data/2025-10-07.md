<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 110]
- [cs.AI](#cs.AI) [Total: 90]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Decomposing Attention To Find Context-Sensitive Neurons](https://arxiv.org/abs/2510.03315)
*Alex Gibson*

Main category: cs.CL

TL;DR: 通过分析GPT2-Small第一层中注意力模式分散且对内容依赖较弱的注意力头，利用校准文本采样softmax分母，近似这些头的组合输出为线性摘要，从而仅从权重和单个校准文本就能发现数百个响应上下文属性的第一层神经元。


<details>
  <summary>Details</summary>
Motivation: 研究transformer语言模型中注意力模式分散的注意力头，这些头的注意力分数对内容依赖较弱，其softmax分母在固定token分布下是稳定的。

Method: 使用校准文本采样softmax分母，将多个稳定注意力头的输出组合近似为周围文本的线性摘要，仅从权重和单个校准文本进行分析。

Result: 能够发现GPT2-Small第一层中数百个响应高级上下文属性的神经元，包括在校准文本上未激活的神经元。

Conclusion: 该方法仅从模型权重和单个校准文本就能有效识别transformer第一层中响应上下文属性的神经元，为模型分析提供了新工具。

Abstract: We study transformer language models, analyzing attention heads whose
attention patterns are spread out, and whose attention scores depend weakly on
content. We argue that the softmax denominators of these heads are stable when
the underlying token distribution is fixed. By sampling softmax denominators
from a "calibration text", we can combine together the outputs of multiple such
stable heads in the first layer of GPT2-Small, approximating their combined
output by a linear summary of the surrounding text. This approximation enables
a procedure where from the weights alone - and a single calibration text - we
can uncover hundreds of first layer neurons that respond to high-level
contextual properties of the surrounding text, including neurons that didn't
activate on the calibration text.

</details>


### [2] [Graph-S3: Enhancing Agentic textual Graph Retrieval with Synthetic Stepwise Supervision](https://arxiv.org/abs/2510.03323)
*Ge Chang,Jinbo Su,Jiacheng Liu,Pengfei Yang,Yuhao Shang,Huiwen Zheng,Hongli Ma,Yan Liang,Yuanchun Li,Yunxin Liu*

Main category: cs.CL

TL;DR: 提出了Graph-S³框架，通过合成逐步监督训练基于LLM的检索器，解决文本图问答中的图检索挑战，在三个数据集上相比7个基线平均提升8.1%准确率和9.7% F1分数。


<details>
  <summary>Details</summary>
Motivation: 现实世界大量数据以文本图形式存在，但现有图检索方法要么依赖浅层嵌入相似度，要么需要大量标注数据和训练成本，导致性能不佳。

Method: 开发基于LLM的检索器，使用合成逐步监督训练，通过离线提取黄金子图为每个检索步骤提供评估，包含数据合成管道和两阶段训练方案。

Result: 在三个常见数据集上相比7个强基线，平均准确率提升8.1%，F1分数提升9.7%，在多跳推理任务中优势更明显。

Conclusion: Graph-S³框架通过合成监督和逐步评估有效解决了文本图检索问题，显著提升了图问答性能，特别是在复杂推理任务中。

Abstract: A significant portion of real-world data is inherently represented as textual
graphs, and integrating these graphs into large language models (LLMs) is
promising to enable complex graph-based question answering. However, a key
challenge in LLM-based textual graph QA systems lies in graph retrieval, i.e.,
how to retrieve relevant content from large graphs that is sufficiently
informative while remaining compact for the LLM context. Existing retrievers
suffer from poor performance since they either rely on shallow embedding
similarity or employ interactive retrieving policies that demand excessive data
labeling and training cost. To address these issues, we present Graph-$S^3$, an
agentic textual graph reasoning framework that employs an LLM-based retriever
trained with synthetic stepwise supervision. Instead of rewarding the agent
based on the final answers, which may lead to sparse and unstable training
signals, we propose to closely evaluate each step of the retriever based on
offline-extracted golden subgraphs. Our main techniques include a data
synthesis pipeline to extract the golden subgraphs for reward generation and a
two-stage training scheme to learn the interactive graph exploration policy
based on the synthesized rewards. Based on extensive experiments on three
common datasets in comparison with seven strong baselines, our approach
achieves an average improvement of 8.1\% in accuracy and 9.7\% in F$_1$ score.
The advantage is even higher in more complicated multi-hop reasoning tasks. Our
code will be open-sourced.

</details>


### [3] [Implicit Values Embedded in How Humans and LLMs Complete Subjective Everyday Tasks](https://arxiv.org/abs/2510.03384)
*Arjun Arunasalam,Madison Pickering,Z. Berkay Celik,Blase Ur*

Main category: cs.CL

TL;DR: 本文审计了6个流行大语言模型在完成30个日常任务时表现出的隐含价值观，并与100名美国众包工作者进行比较，发现LLMs在价值观上既与人类不一致，也与其他LLMs不一致。


<details>
  <summary>Details</summary>
Motivation: 尽管AI助手在帮助用户完成日常任务方面具有潜力，但人们对这些助手在完成主观日常任务时表现出的隐含价值观知之甚少。

Method: 通过审计6个流行LLMs完成30个日常任务的方式，并与100名美国众包工作者进行比较，分析LLMs在价值观上的表现。

Result: LLMs在隐含价值观表现上经常与人类不一致，也与其他LLMs不一致。

Conclusion: 大语言模型在完成日常任务时表现出的价值观需要进一步关注和调整，以确保与人类价值观的一致性。

Abstract: Large language models (LLMs) can underpin AI assistants that help users with
everyday tasks, such as by making recommendations or performing basic
computation. Despite AI assistants' promise, little is known about the implicit
values these assistants display while completing subjective everyday tasks.
Humans may consider values like environmentalism, charity, and diversity. To
what extent do LLMs exhibit these values in completing everyday tasks? How do
they compare with humans? We answer these questions by auditing how six popular
LLMs complete 30 everyday tasks, comparing LLMs to each other and to 100 human
crowdworkers from the US. We find LLMs often do not align with humans, nor with
other LLMs, in the implicit values exhibited.

</details>


### [4] [Morpheme Induction for Emergent Language](https://arxiv.org/abs/2510.03439)
*Brendon Boldt,David Mortensen*

Main category: cs.CL

TL;DR: CSAR是一种从新兴语言语料库中诱导语素的贪婪算法，通过互信息加权、选择最高权重对、从语料中移除并重复该过程来提取语素。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够从新兴语言平行语料库中自动提取语素的算法，用于分析语言特征。

Method: 基于互信息的贪婪算法：计数语素权重、选择最高权重对、从语料中移除、重复该过程。

Result: 在程序生成数据集上验证有效性，在人类语言数据上表现合理，能够量化语言特征如同义词和多义词程度。

Conclusion: CSAR算法能够有效从新兴语言中诱导语素，并为语言特征分析提供量化工具。

Abstract: We introduce CSAR, an algorithm for inducing morphemes from emergent language
corpora of parallel utterances and meanings. It is a greedy algorithm that (1)
weights morphemes based on mutual information between forms and meanings, (2)
selects the highest-weighted pair, (3) removes it from the corpus, and (4)
repeats the process to induce further morphemes (i.e., Count, Select, Ablate,
Repeat). The effectiveness of CSAR is first validated on procedurally generated
datasets and compared against baselines for related tasks. Second, we validate
CSAR's performance on human language data to show that the algorithm makes
reasonable predictions in adjacent domains. Finally, we analyze a handful of
emergent languages, quantifying linguistic characteristics like degree of
synonymy and polysemy.

</details>


### [5] [Omni-Embed-Nemotron: A Unified Multimodal Retrieval Model for Text, Image, Audio, and Video](https://arxiv.org/abs/2510.03458)
*Mengyao Xu,Wenfei Zhou,Yauhen Babakhin,Gabriel Moreira,Ronay Ak,Radek Osmulski,Bo Liu,Even Oldridge,Benedikt Schifferer*

Main category: cs.CL

TL;DR: Omni-Embed-Nemotron是一个统一的多模态检索嵌入模型，支持文本、图像、音频和视频的跨模态检索。


<details>
  <summary>Details</summary>
Motivation: 现有的基于文本的检索器在处理现实世界文档（如PDF、幻灯片、视频）中视觉和语义丰富的内容时存在困难，需要支持多模态检索。

Method: 基于ColPali和Qwen2.5-Omni等工作的启发，构建了支持文本、图像、音频和视频的统一多模态检索模型架构。

Result: 模型在文本、图像和视频检索方面表现出有效性。

Conclusion: Omni-Embed-Nemotron能够有效处理现实世界中复杂的信息检索需求，支持跨模态和联合模态检索。

Abstract: We present Omni-Embed-Nemotron, a unified multimodal retrieval embedding
model developed to handle the increasing complexity of real-world information
needs. While Retrieval-Augmented Generation (RAG) has significantly advanced
language models by incorporating external knowledge, existing text-based
retrievers rely on clean, structured input and struggle with the visually and
semantically rich content found in real-world documents such as PDFs, slides,
or videos. Recent work such as ColPali has shown that preserving document
layout using image-based representations can improve retrieval quality.
Building on this, and inspired by the capabilities of recent multimodal models
such as Qwen2.5-Omni, we extend retrieval beyond text and images to also
support audio and video modalities. Omni-Embed-Nemotron enables both
cross-modal (e.g., text - video) and joint-modal (e.g., text - video+audio)
retrieval using a single model. We describe the architecture, training setup,
and evaluation results of Omni-Embed-Nemotron, and demonstrate its
effectiveness in text, image, and video retrieval.

</details>


### [6] [Searching for the Most Human-like Emergent Language](https://arxiv.org/abs/2510.03467)
*Brendon Boldt,David Mortensen*

Main category: cs.CL

TL;DR: 本文通过基于信号博弈的涌现通信环境和超参数优化，生成了与人类语言相似度最高的涌现语言，使用XferBench作为目标函数来量化统计相似性。


<details>
  <summary>Details</summary>
Motivation: 设计能够生成与人类语言高度相似的涌现语言，通过深度迁移学习来评估其与人类语言的统计相似性。

Method: 使用基于信号博弈的涌现通信环境，结合超参数优化方法，以XferBench作为目标函数来量化涌现语言与人类语言的相似度。

Result: 证明了熵对涌现语言迁移学习性能的预测能力，验证了涌现通信系统的熵最小化特性，并确定了产生更真实涌现语言的关键超参数。

Conclusion: 通过该方法能够生成统计上更接近人类语言的涌现语言，并揭示了熵在评估涌现语言质量中的重要作用。

Abstract: In this paper, we design a signalling game-based emergent communication
environment to generate state-of-the-art emergent languages in terms of
similarity to human language. This is done with hyperparameter optimization,
using XferBench as the objective function. XferBench quantifies the statistical
similarity of emergent language to human language by measuring its suitability
for deep transfer learning to human language. Additionally, we demonstrate the
predictive power of entropy on the transfer learning performance of emergent
language as well as corroborate previous results on the entropy-minimization
properties of emergent communication systems. Finally, we report
generalizations regarding what hyperparameters produce more realistic emergent
languages, that is, ones which transfer better to human language.

</details>


### [7] [SEER: The Span-based Emotion Evidence Retrieval Benchmark](https://arxiv.org/abs/2510.03490)
*Aneesha Sampath,Oya Aran,Emily Mower Provost*

Main category: cs.CL

TL;DR: SEER基准测试评估LLMs识别文本中表达情感的具体片段的能力，包含单句和跨句情感证据检测任务，发现模型在长文本中表现下降。


<details>
  <summary>Details</summary>
Motivation: 传统情感识别只给整个句子分配单一标签，而实际应用中需要知道情感是如何表达的，因此需要开发能够精确定位情感表达片段的方法。

Method: 创建包含1200个真实世界句子的SEER基准，包含情感和情感证据的新标注，评估14个开源LLMs在单句和跨句情感证据检测任务上的表现。

Result: 一些模型在单句输入上接近人类平均表现，但在较长段落中准确性下降。错误分析显示关键失败模式包括过度依赖情感关键词和在中性文本中出现误报。

Conclusion: SEER基准突显了LLMs在细粒度情感证据检测方面的局限性，特别是在处理较长文本时，为改进情感理解模型提供了重要基准。

Abstract: We introduce the SEER (Span-based Emotion Evidence Retrieval) Benchmark to
test Large Language Models' (LLMs) ability to identify the specific spans of
text that express emotion. Unlike traditional emotion recognition tasks that
assign a single label to an entire sentence, SEER targets the underexplored
task of emotion evidence detection: pinpointing which exact phrases convey
emotion. This span-level approach is crucial for applications like empathetic
dialogue and clinical support, which need to know how emotion is expressed, not
just what the emotion is. SEER includes two tasks: identifying emotion evidence
within a single sentence, and identifying evidence across a short passage of
five consecutive sentences. It contains new annotations for both emotion and
emotion evidence on 1200 real-world sentences. We evaluate 14 open-source LLMs
and find that, while some models approach average human performance on
single-sentence inputs, their accuracy degrades in longer passages. Our error
analysis reveals key failure modes, including overreliance on emotion keywords
and false positives in neutral text.

</details>


### [8] [ALHD: A Large-Scale and Multigenre Benchmark Dataset for Arabic LLM-Generated Text Detection](https://arxiv.org/abs/2510.03502)
*Ali Khairallah,Arkaitz Zubiaga*

Main category: cs.CL

TL;DR: ALHD是首个大规模阿拉伯语数据集，专门用于区分人类和LLM生成的文本，涵盖新闻、社交媒体和评论三种文体，包含超过40万平衡样本，支持阿拉伯语LLM文本检测的泛化性研究。


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯语中缺乏专门用于区分人类和LLM生成文本的大规模数据集的问题，为研究阿拉伯语LLM检测、缓解错误信息、学术不端和网络威胁风险奠定基础。

Method: 构建包含三种文体（新闻、社交媒体、评论）的阿拉伯语数据集，涵盖现代标准阿拉伯语和方言，使用三个领先LLM生成文本，提供严格预处理、丰富标注和标准化平衡分割。

Result: 基准测试显示，微调的BERT模型表现竞争性，优于基于LLM的模型。但在跨文体泛化方面存在挑战，特别是在新闻文章中，LLM生成的文本在风格上更接近人类文本。

Conclusion: ALHD为阿拉伯语LLM检测研究奠定了基础，揭示了跨文体泛化的挑战，特别是在新闻领域，为未来研究指明了方向。

Abstract: We introduce ALHD, the first large-scale comprehensive Arabic dataset
explicitly designed to distinguish between human- and LLM-generated texts. ALHD
spans three genres (news, social media, reviews), covering both MSA and
dialectal Arabic, and contains over 400K balanced samples generated by three
leading LLMs and originated from multiple human sources, which enables studying
generalizability in Arabic LLM-genearted text detection. We provide rigorous
preprocessing, rich annotations, and standardized balanced splits to support
reproducibility. In addition, we present, analyze and discuss benchmark
experiments using our new dataset, in turn identifying gaps and proposing
future research directions. Benchmarking across traditional classifiers,
BERT-based models, and LLMs (zero-shot and few-shot) demonstrates that
fine-tuned BERT models achieve competitive performance, outperforming LLM-based
models. Results are however not always consistent, as we observe challenges
when generalizing across genres; indeed, models struggle to generalize when
they need to deal with unseen patterns in cross-genre settings, and these
challenges are particularly prominent when dealing with news articles, where
LLM-generated texts resemble human texts in style, which opens up avenues for
future research. ALHD establishes a foundation for research related to Arabic
LLM-detection and mitigating risks of misinformation, academic dishonesty, and
cyber threats.

</details>


### [9] [TS-Reasoner: Aligning Time Series Foundation Models with LLM Reasoning](https://arxiv.org/abs/2510.03519)
*Fangxu Yu,Hongyu Zhao,Tianyi Zhou*

Main category: cs.CL

TL;DR: 提出了TS-Reasoner模型，将时间序列基础模型(TSFMs)的潜在表示与大型语言模型(LLMs)的文本输入对齐，用于时间序列推理任务。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型能捕捉动态模式但缺乏推理能力，而LLMs具有推理能力但不擅长数值理解。需要将两者有效结合来解决时间序列推理问题。

Method: 提出两阶段训练方法：首先使用合成的时序-文本对进行对齐预训练，然后进行指令微调。冻结预训练的TSFM，只训练对齐模块。

Result: 在多个基准测试中，TS-Reasoner优于主流LLMs、视觉语言模型和时间序列LLMs，且具有显著的数据效率(使用不到一半的训练数据)。

Conclusion: 该方法成功实现了时间序列基础模型与语言模型的有效对齐，为时间序列推理任务提供了高效解决方案。

Abstract: Time series reasoning is crucial to decision-making in diverse domains,
including finance, energy usage, traffic, weather, and scientific discovery.
While existing time series foundation models (TSFMs) can capture low-level
dynamic patterns and provide accurate forecasting, further analysis usually
requires additional background knowledge and sophisticated reasoning, which are
lacking in most TSFMs but can be achieved through large language models (LLMs).
On the other hand, without expensive post-training, LLMs often struggle with
the numerical understanding of time series data. Although it is intuitive to
integrate the two types of models, developing effective training recipes that
align the two modalities for reasoning tasks is still an open challenge. To
this end, we propose TS-Reasoner that aligns the latent representations of
TSFMs with the textual inputs of LLMs for downstream understanding/reasoning
tasks. Specifically, we propose a simple yet effective method to curate
diverse, synthetic pairs of time series and textual captions for alignment
training. We then develop a two-stage training recipe that applies instruction
finetuning after the alignment pretraining. Unlike existing works that train an
LLM to take time series as inputs, we leverage a pretrained TSFM and freeze it
during training. Extensive experiments on several benchmarks demonstrate that
TS-Reasoner not only outperforms a wide range of prevailing LLMs, Vision
Language Models (VLMs), and Time Series LLMs, but also achieves this with
remarkable data efficiency, e.g., using less than half the training data.

</details>


### [10] [Identifying Financial Risk Information Using RAG with a Contrastive Insight](https://arxiv.org/abs/2510.03521)
*Ali Elahi*

Main category: cs.CL

TL;DR: 在专业领域推理中，传统RAG方法存在局限性，无法检索可比案例或相关问题。本文提出在RAG之上添加同行感知比较推理层，通过对比方法提升文本生成质量。


<details>
  <summary>Details</summary>
Motivation: 在专业领域推理中，人类通常会比较相似案例并分析细微差异，而传统RAG方法虽然能捕获上下文相关信息，但无法检索可比案例或相关问题，导致输出过于通用化。

Method: 在RAG之上构建同行感知比较推理层，采用对比方法进行推理分析。

Result: 该方法在文本生成指标（如ROUGE和BERTScore）上优于基线RAG方法，与人类生成的股权研究和风险评估相比表现更好。

Conclusion: 提出的对比方法能够有效解决RAG在专业推理任务中的局限性，生成更具上下文特定洞察力的输出。

Abstract: In specialized domains, humans often compare new problems against similar
examples, highlight nuances, and draw conclusions instead of analyzing
information in isolation. When applying reasoning in specialized contexts with
LLMs on top of a RAG, the pipeline can capture contextually relevant
information, but it is not designed to retrieve comparable cases or related
problems.
  While RAG is effective at extracting factual information, its outputs in
specialized reasoning tasks often remain generic, reflecting broad facts rather
than context-specific insights. In finance, it results in generic risks that
are true for the majority of companies. To address this limitation, we propose
a peer-aware comparative inference layer on top of RAG.
  Our contrastive approach outperforms baseline RAG in text generation metrics
such as ROUGE and BERTScore in comparison with human-generated equity research
and risk.

</details>


### [11] [Sample, Align, Synthesize: Graph-Based Response Synthesis with ConGrs](https://arxiv.org/abs/2510.03527)
*Sayan Ghosh,Shahzaib Saqib Warraich,Dhruv Tarsadiya,Gregory Yauney,Swabha Swayamdipta*

Main category: cs.CL

TL;DR: 提出Consensus Graphs (ConGrs)数据结构，通过整合多个语言模型响应的共享信息和语义变化，合成更准确和有效的最终响应。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效整合多个长文本响应中的丰富认知信号，需要一种能捕捉语言模型响应变化并利用这些认知信号的方法。

Method: 使用基于生物信息学的轻量级词汇序列对齐算法构建ConGrs数据结构，辅以辅助语言模型判断，并设计任务相关的解码方法从ConGr合成最终响应。

Result: 在传记生成任务中事实精度提高31%，减少对语言模型判断的依赖超过80%；在拒绝任务中弃权率提高56%；在数学推理任务中准确率比基线提高6个百分点。

Conclusion: ConGrs提供了一种灵活的方法来捕捉语言模型响应的变化，并利用响应变化提供的认知信号来合成更有效的响应。

Abstract: Language models can be sampled multiple times to access the distribution
underlying their responses, but existing methods cannot efficiently synthesize
rich epistemic signals across different long-form responses. We introduce
Consensus Graphs (ConGrs), a flexible DAG-based data structure that represents
shared information, as well as semantic variation in a set of sampled LM
responses to the same prompt. We construct ConGrs using a light-weight lexical
sequence alignment algorithm from bioinformatics, supplemented by the targeted
usage of a secondary LM judge. Further, we design task-dependent decoding
methods to synthesize a single, final response from our ConGr data structure.
Our experiments show that synthesizing responses from ConGrs improves factual
precision on two biography generation tasks by up to 31% over an average
response and reduces reliance on LM judges by more than 80% compared to other
methods. We also use ConGrs for three refusal-based tasks requiring abstention
on unanswerable queries and find that abstention rate is increased by up to
56%. We apply our approach to the MATH and AIME reasoning tasks and find an
improvement over self-verification and majority vote baselines by up to 6
points of accuracy. We show that ConGrs provide a flexible method for capturing
variation in LM responses and using the epistemic signals provided by response
variation to synthesize more effective responses.

</details>


### [12] [Fine-Tuning on Noisy Instructions: Effects on Generalization and Performance](https://arxiv.org/abs/2510.03528)
*Ahmed Alajrami,Xingwei Tan,Nikolaos Aletras*

Main category: cs.CL

TL;DR: 研究表明，在指令微调数据中引入扰动（如删除停用词或打乱词序）可以增强大语言模型对噪声指令的抵抗能力，有时甚至能提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明大语言模型对指令表述的微小变化很敏感，本文探索通过在指令微调数据中引入扰动来增强模型对噪声指令的鲁棒性。

Method: 在指令微调过程中引入扰动，如删除停用词或打乱词序，并在MMLU、BBH、GSM8K等基准测试上评估模型在原始和扰动版本上的表现。

Result: 令人惊讶的是，在某些情况下，对扰动指令进行指令微调可以提升下游任务性能。

Conclusion: 在指令微调中包含扰动指令很重要，这能使大语言模型对噪声用户输入更具弹性。

Abstract: Instruction-tuning plays a vital role in enhancing the task-solving abilities
of large language models (LLMs), improving their usability in generating
helpful responses on various tasks. However, previous work has demonstrated
that they are sensitive to minor variations in instruction phrasing. In this
paper, we explore whether introducing perturbations in instruction-tuning data
can enhance LLMs' resistance against noisy instructions. We focus on how
instruction-tuning with perturbations, such as removing stop words or shuffling
words, affects LLMs' performance on the original and perturbed versions of
widely-used benchmarks (MMLU, BBH, GSM8K). We further assess learning dynamics
and potential shifts in model behavior. Surprisingly, our results suggest that
instruction-tuning on perturbed instructions can, in some cases, improve
downstream performance. These findings highlight the importance of including
perturbed instructions in instruction-tuning, which can make LLMs more
resilient to noisy user inputs.

</details>


### [13] [TriMediQ: A Triplet-Structured Approach for Interactive Medical Question Answering](https://arxiv.org/abs/2510.03536)
*Zhaohan Meng,Zaiqiao Meng,Siwei Liu,Iadh Ounis*

Main category: cs.CL

TL;DR: TriMediQ框架通过将患者回答转换为三元组结构的知识图谱，解决了LLM在多轮医疗对话中推理能力下降的问题，显著提升了临床问答的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在静态单轮医疗问答中表现良好，但在实际临床咨询的多轮对话场景中，由于临床事实分散在对话记录中且缺乏明确关联，LLM的推理可靠性急剧下降。

Method: 提出TriMediQ框架：1）使用冻结的三元组生成器提取临床相关三元组构建知识图谱；2）训练投影模块（图编码器和投影器）从KG中捕获关系信息；3）分两步操作：先微调投影模块（LLM权重冻结），然后在推理时使用该模块指导多跳推理。

Result: 在两个交互式QA基准测试中，TriMediQ在iMedQA数据集上比五个基线模型准确率提升了10.4%。

Conclusion: 将患者回答转换为结构化的三元组图谱能够显著提升LLM在多轮医疗对话中的临床推理准确性，为基于LLM的医疗助手部署提供了有效解决方案。

Abstract: Large Language Models (LLMs) perform strongly in static and single-turn
medical Question Answer (QA) benchmarks, yet such settings diverge from the
iterative information gathering process required in practical clinical
consultations. The MEDIQ framework addresses this mismatch by recasting the
diagnosis as an interactive dialogue between a patient and an expert system,
but the reliability of LLMs drops dramatically when forced to reason with
dialogue logs, where clinical facts appear in sentences without clear links. To
bridge this gap, we introduce TriMediQ, a triplet-structured approach that
summarises patient responses into triplets and integrates them into a Knowledge
Graph (KG), enabling multi-hop reasoning. We introduce a frozen triplet
generator that extracts clinically relevant triplets, using prompts designed to
ensure factual consistency. In parallel, a trainable projection module,
comprising a graph encoder and a projector, captures relational information
from the KG to enhance expert reasoning. TriMediQ operates in two steps: (i)
the projection module fine-tuning with all LLM weights frozen; and (ii) using
the fine-tuned module to guide multi-hop reasoning during inference. We
evaluate TriMediQ on two interactive QA benchmarks, showing that it achieves up
to 10.4\% improvement in accuracy over five baselines on the iMedQA dataset.
These results demonstrate that converting patient responses into structured
triplet-based graphs enables more accurate clinical reasoning in multi-turn
settings, providing a solution for the deployment of LLM-based medical
assistants.

</details>


### [14] [What is a protest anyway? Codebook conceptualization is still a first-order concern in LLM-era classification](https://arxiv.org/abs/2510.03541)
*Andrew Halterman,Katherine A. Keith*

Main category: cs.CL

TL;DR: 论文指出在计算社会科学中使用大语言模型进行文本分类时，概念化步骤被忽视，导致概念化偏差影响下游统计推断，且这种偏差无法通过提高模型准确性或后处理方法来纠正。


<details>
  <summary>Details</summary>
Motivation: 当前计算社会科学中广泛使用生成式大语言模型进行文本分类，但研究者往往忽视了分类前的概念化步骤和分类后的统计推断步骤，这可能导致概念化错误并影响下游估计的准确性。

Method: 通过模拟实验分析概念化偏差对下游估计的影响，并测试提高LLM准确性和后处理偏差校正方法的有效性。

Result: 研究发现概念化引起的偏差无法仅通过提高LLM准确性或后处理偏差校正方法来纠正，概念化仍然是LLM时代的一阶关注点。

Conclusion: 提醒计算社会科学研究者概念化在LLM时代仍然至关重要，并提供了追求低成本、无偏、低方差下游估计的具体建议。

Abstract: Generative large language models (LLMs) are now used extensively for text
classification in computational social science (CSS). In this work, focus on
the steps before and after LLM prompting -- conceptualization of concepts to be
classified and using LLM predictions in downstream statistical inference --
which we argue have been overlooked in much of LLM-era CSS. We claim LLMs can
tempt analysts to skip the conceptualization step, creating conceptualization
errors that bias downstream estimates. Using simulations, we show that this
conceptualization-induced bias cannot be corrected for solely by increasing LLM
accuracy or post-hoc bias correction methods. We conclude by reminding CSS
analysts that conceptualization is still a first-order concern in the LLM-era
and provide concrete advice on how to pursue low-cost, unbiased, low-variance
downstream estimates.

</details>


### [15] [CCD-Bench: Probing Cultural Conflict in Large Language Model Decision-Making](https://arxiv.org/abs/2510.03553)
*Hasibur Rahman,Hanan Salam*

Main category: cs.CL

TL;DR: CCD-Bench是一个评估LLM在跨文化价值冲突下决策能力的基准测试，包含2,182个开放式困境，覆盖7个领域，发现LLM偏好北欧和日耳曼欧洲价值观，而忽视东欧和中东文化。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注文化知识、价值预测或单轴偏见检测，但缺乏评估LLM在多个文化价值观直接冲突时的裁决能力。

Method: 使用2,182个开放式困境，每个困境配以10个对应GLOBE文化集群的匿名响应选项，采用分层拉丁方设计来减轻顺序效应，评估17个非推理LLM。

Result: 模型明显偏好北欧欧洲(20.2%)和日耳曼欧洲(12.4%)，而东欧和中东选项代表性不足(5.6-5.8%)。87.9%的理由引用多个GLOBE维度，但这种多元性很表面。

Conclusion: 当前的对齐流程促进了共识导向的世界观，忽视了需要权力协商、基于权利的推理或性别意识分析的场景。CCD-Bench将评估从孤立偏见检测转向多元决策，强调需要实质性参与多元世界观的校准策略。

Abstract: Although large language models (LLMs) are increasingly implicated in
interpersonal and societal decision-making, their ability to navigate explicit
conflicts between legitimately different cultural value systems remains largely
unexamined. Existing benchmarks predominantly target cultural knowledge
(CulturalBench), value prediction (WorldValuesBench), or single-axis bias
diagnostics (CDEval); none evaluate how LLMs adjudicate when multiple
culturally grounded values directly clash. We address this gap with CCD-Bench,
a benchmark that assesses LLM decision-making under cross-cultural value
conflict. CCD-Bench comprises 2,182 open-ended dilemmas spanning seven domains,
each paired with ten anonymized response options corresponding to the ten GLOBE
cultural clusters. These dilemmas are presented using a stratified Latin square
to mitigate ordering effects. We evaluate 17 non-reasoning LLMs. Models
disproportionately prefer Nordic Europe (mean 20.2 percent) and Germanic Europe
(12.4 percent), while options for Eastern Europe and the Middle East and North
Africa are underrepresented (5.6 to 5.8 percent). Although 87.9 percent of
rationales reference multiple GLOBE dimensions, this pluralism is superficial:
models recombine Future Orientation and Performance Orientation, and rarely
ground choices in Assertiveness or Gender Egalitarianism (both under 3
percent). Ordering effects are negligible (Cramer's V less than 0.10), and
symmetrized KL divergence shows clustering by developer lineage rather than
geography. These patterns suggest that current alignment pipelines promote a
consensus-oriented worldview that underserves scenarios demanding power
negotiation, rights-based reasoning, or gender-aware analysis. CCD-Bench shifts
evaluation beyond isolated bias detection toward pluralistic decision making
and highlights the need for alignment strategies that substantively engage
diverse worldviews.

</details>


### [16] [Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models](https://arxiv.org/abs/2510.03561)
*Adam Filipek*

Main category: cs.CL

TL;DR: RxT是一种新型Transformer架构，通过事件驱动范式解决传统Transformer在对话AI中的状态保持和计算复杂度问题，将对话成本从二次方降低到线性。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer架构在对话AI中存在状态保持问题和二次计算复杂度限制，无法支持实时长对话。

Method: 采用事件驱动范式，将每个对话轮次作为离散事件处理，通过固定大小的短期记忆系统和异步内存更新机制实现状态保持。

Result: 在合成数据上的实验表明，RxT相比基线模型具有更优性能和恒定时间推理延迟。

Conclusion: RxT实现了低延迟、状态保持且经济可行的长对话系统，从根本上改变了对话系统的扩展动态。

Abstract: The Transformer architecture has become the de facto standard for Large
Language Models (LLMs), demonstrating remarkable capabilities in language
understanding and generation. However, its application in conversational AI is
fundamentally constrained by its stateless nature and the quadratic
computational complexity ($O(L^2)$) with respect to sequence length $L$.
Current models emulate memory by reprocessing an ever-expanding conversation
history with each turn, leading to prohibitive costs and latency in long
dialogues. This paper introduces the Reactive Transformer (RxT), a novel
architecture designed to overcome these limitations by shifting from a
data-driven to an event-driven paradigm. RxT processes each conversational turn
as a discrete event in real-time, maintaining context in an integrated,
fixed-size Short-Term Memory (STM) system. The architecture features a distinct
operational cycle where a generator-decoder produces a response based on the
current query and the previous memory state, after which a memory-encoder and a
dedicated Memory Attention network asynchronously update the STM with a
representation of the complete interaction. This design fundamentally alters
the scaling dynamics, reducing the total user-facing cost of a conversation
from quadratic ($O(N^2 \cdot T)$) to linear ($O(N \cdot T)$) with respect to
the number of interactions $N$. By decoupling response generation from memory
updates, RxT achieves low latency, enabling truly real-time, stateful, and
economically viable long-form conversations. We validated our architecture with
a series of proof-of-concept experiments on synthetic data, demonstrating
superior performance and constant-time inference latency compared to a baseline
stateless model of comparable size.

</details>


### [17] [LLM, Reporting In! Medical Information Extraction Across Prompting, Fine-tuning and Post-correction](https://arxiv.org/abs/2510.03577)
*Ikram Belmadani,Parisa Nazari Hashemi,Thomas Sebbag,Benoit Favre,Guillaume Fortier,Solen Quiniou,Emmanuel Morin,Richard Dufour*

Main category: cs.CL

TL;DR: 本文介绍了在EvalLLM 2025挑战赛中针对法语生物医学命名实体识别和健康事件提取的研究，提出了三种结合LLM的方法，其中GPT-4.1在少样本设置下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决法语生物医学领域在少样本设置下的命名实体识别和事件提取挑战，探索LLM在低资源场景下的应用潜力。

Method: 提出三种方法：(1) GPT-4.1的上下文学习，自动选择10个示例并整合标注指南；(2) GLiNER系统在合成语料上微调后经LLM后处理验证；(3) LLaMA-3.1-8B-Instruct在相同合成语料上微调。事件提取采用相同的ICL策略。

Result: GPT-4.1在命名实体识别上获得61.53%的宏F1分数，在事件提取上获得15.02%的宏F1分数，表现最佳。

Conclusion: 精心设计的提示工程对于在极低资源场景下最大化性能至关重要，GPT-4.1在少样本法语生物医学文本处理中展现出优势。

Abstract: This work presents our participation in the EvalLLM 2025 challenge on
biomedical Named Entity Recognition (NER) and health event extraction in French
(few-shot setting). For NER, we propose three approaches combining large
language models (LLMs), annotation guidelines, synthetic data, and
post-processing: (1) in-context learning (ICL) with GPT-4.1, incorporating
automatic selection of 10 examples and a summary of the annotation guidelines
into the prompt, (2) the universal NER system GLiNER, fine-tuned on a synthetic
corpus and then verified by an LLM in post-processing, and (3) the open LLM
LLaMA-3.1-8B-Instruct, fine-tuned on the same synthetic corpus. Event
extraction uses the same ICL strategy with GPT-4.1, reusing the guideline
summary in the prompt. Results show GPT-4.1 leads with a macro-F1 of 61.53% for
NER and 15.02% for event extraction, highlighting the importance of
well-crafted prompting to maximize performance in very low-resource scenarios.

</details>


### [18] [Decoupling Task-Solving and Output Formatting in LLM Generation](https://arxiv.org/abs/2510.03595)
*Haikang Deng,Po-Nien Kung,Nanyun Peng*

Main category: cs.CL

TL;DR: Deco-G是一个解码框架，通过将格式遵循与任务解决解耦来提升LLM性能。它使用独立的概率模型处理格式合规性，让LLM专注于任务指令，在多个任务上实现了1.0%到6.0%的相对性能提升。


<details>
  <summary>Details</summary>
Motivation: 当提示变得越来越复杂时，LLM难以同时遵循所有指令，特别是当推理指令与严格的格式要求交织在一起时，这种纠缠会为模型创造竞争性目标，影响性能。

Method: Deco-G框架使用独立的可追踪概率模型(TPM)处理格式合规性，同时仅向LLM提供任务指令。在每个解码步骤中，将LLM的下一个token概率与TPM计算的格式合规似然相结合。引入了指令感知蒸馏、灵活的trie构建算法和HMM状态剪枝等创新技术。

Result: 在数学推理、LLM-as-a-judge和事件参数提取等多种格式要求的任务中，Deco-G相比常规提示方法实现了1.0%到6.0%的相对性能提升，并保证格式合规性。

Conclusion: 通过明确分离格式遵循和任务解决，Deco-G框架有效提升了LLM在复杂指令下的性能，证明了这种解耦方法的有效性。

Abstract: Large language models (LLMs) are increasingly adept at following instructions
containing task descriptions to solve complex problems, such as mathematical
reasoning and automatic evaluation (LLM-as-a-Judge). However, as prompts grow
more complex, models often struggle to adhere to all instructions. This
difficulty is especially common when instructive prompts intertwine reasoning
directives -- specifying what the model should solve -- with rigid formatting
requirements that dictate how the solution must be presented. The entanglement
creates competing goals for the model, suggesting that more explicit separation
of these two aspects could lead to improved performance. To this front, we
introduce Deco-G, a decoding framework that explicitly decouples format
adherence from task solving. Deco-G handles format compliance with a separate
tractable probabilistic model (TPM), while prompts LLMs with only task
instructions. At each decoding step, Deco-G combines next token probabilities
from the LLM with the TPM calculated format compliance likelihood to form the
output probability. To make this approach both practical and scalable for
modern instruction-tuned LLMs, we introduce three key innovations:
instruction-aware distillation, a flexible trie-building algorithm, and HMM
state pruning for computational efficiency. We demonstrate the effectiveness of
Deco-G across a wide range of tasks with diverse format requirements, including
mathematical reasoning, LLM-as-a-judge, and event argument extraction. Overall,
our approach yields 1.0% to 6.0% relative gain over regular prompting practice
with guaranteed format compliance.

</details>


### [19] [Can an LLM Induce a Graph? Investigating Memory Drift and Context Length](https://arxiv.org/abs/2510.03611)
*Raquib Bin Yousuf,Aadyant Khatri,Shengzhe Xu,Mandar Sharma,Naren Ramakrishnan*

Main category: cs.CL

TL;DR: 现有评估基准可能高估LLMs的有效上下文长度，在需要从长文本中提取结构化关系知识的复杂推理任务中，模型会出现更早的记忆漂移和上下文遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有评估基准主要基于简单的检索任务，不能准确反映LLMs在信息密集场景中的表现，特别是需要从长文本中提取结构化知识的复杂推理任务。

Method: 通过让LLMs从可能包含噪声的自然语言内容中归纳结构化关系知识（如图结构），评估其在长上下文中的推理能力。

Result: LLMs在进行关系推理任务时，在比现有基准建议的更短有效长度下就开始出现记忆漂移和上下文遗忘，即使是专门的推理模型如OpenAI o1也容易受到早期记忆漂移的影响。

Conclusion: LLMs从非结构化输入中抽象结构化知识的能力存在显著限制，需要架构改进来提升长距离推理能力。

Abstract: Recently proposed evaluation benchmarks aim to characterize the effective
context length and the forgetting tendencies of large language models (LLMs).
However, these benchmarks often rely on simplistic 'needle in a haystack'
retrieval or continuation tasks that may not accurately reflect the performance
of these models in information-dense scenarios. Thus, rather than simple next
token prediction, we argue for evaluating these models on more complex
reasoning tasks that requires them to induce structured relational knowledge
from the text - such as graphs from potentially noisy natural language content.
While the input text can be viewed as generated in terms of a graph, its
structure is not made explicit and connections must be induced from distributed
textual cues, separated by long contexts and interspersed with irrelevant
information. Our findings reveal that LLMs begin to exhibit memory drift and
contextual forgetting at much shorter effective lengths when tasked with this
form of relational reasoning, compared to what existing benchmarks suggest.
With these findings, we offer recommendations for the optimal use of popular
LLMs for complex reasoning tasks. We further show that even models specialized
for reasoning, such as OpenAI o1, remain vulnerable to early memory drift in
these settings. These results point to significant limitations in the models'
ability to abstract structured knowledge from unstructured input and highlight
the need for architectural adaptations to improve long-range reasoning.

</details>


### [20] [Towards Unsupervised Speech Recognition at the Syllable-Level](https://arxiv.org/abs/2510.03639)
*Liming Wang,Junrui Ni,Kai-Wei Chang,Saurabhchand Bhati,David Harwath,Mark Hasegawa-Johnson,James R. Glass*

Main category: cs.CL

TL;DR: 提出基于音节级掩码语言建模的无监督语音识别框架，无需G2P转换器，在LibriSpeech上实现40%相对字符错误率降低，并能有效推广到中文。


<details>
  <summary>Details</summary>
Motivation: 解决无监督语音识别中依赖昂贵G2P资源的问题，以及基于音素的方法在模糊音素边界语言中训练不稳定的挑战。

Method: 基于音节级掩码语言建模的无监督语音识别框架，避免了GAN方法的不稳定性。

Result: 在LibriSpeech上实现40%相对字符错误率降低，在中文等困难语言上表现良好。

Conclusion: 提出的音节级UASR框架有效解决了现有方法的局限性，为低资源语言ASR提供了可行方案。

Abstract: Training speech recognizers with unpaired speech and text -- known as
unsupervised speech recognition (UASR) -- is a crucial step toward extending
ASR to low-resource languages in the long-tail distribution and enabling
multimodal learning from non-parallel data. However, existing approaches based
on phones often rely on costly resources such as grapheme-to-phoneme converters
(G2Ps) and struggle to generalize to languages with ambiguous phoneme
boundaries due to training instability. In this paper, we address both
challenges by introducing a syllable-level UASR framework based on masked
language modeling, which avoids the need for G2P and the instability of
GAN-based methods. Our approach achieves up to a 40\% relative reduction in
character error rate (CER) on LibriSpeech and generalizes effectively to
Mandarin, a language that has remained particularly difficult for prior
methods. Code will be released upon acceptance.

</details>


### [21] [UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG](https://arxiv.org/abs/2510.03663)
*Xiangyu Peng,Cab Qin,Zeyuan Chen,Ran Xu,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: UniDoc-Bench是首个大规模、现实的多模态检索增强生成基准，基于70k真实PDF页面构建，包含1,600个多模态QA对，支持四种范式的公平比较。


<details>
  <summary>Details</summary>
Motivation: 当前多模态检索增强生成评估零散，缺乏捕获文档中心多模态用例的现实基准。

Method: 从8个领域的70k真实PDF页面提取文本、表格和图像证据，生成1,600个多模态QA对，20%经过多标注者验证和专家裁决。

Result: 多模态文本-图像融合RAG系统持续优于单模态和联合多模态嵌入检索，表明当前多模态嵌入仍不足够。

Conclusion: 视觉上下文何时以及如何补充文本证据的分析揭示了系统性失败模式，为开发更稳健的MM-RAG管道提供了可操作指导。

Abstract: Multimodal retrieval-augmented generation (MM-RAG) is a key approach for
applying large language models (LLMs) and agents to real-world knowledge bases,
yet current evaluations are fragmented, focusing on either text or images in
isolation or on simplified multimodal setups that fail to capture
document-centric multimodal use cases. In this paper, we introduce
UniDoc-Bench, the first large-scale, realistic benchmark for MM-RAG built from
70k real-world PDF pages across eight domains. Our pipeline extracts and links
evidence from text, tables, and figures, then generates 1,600 multimodal QA
pairs spanning factual retrieval, comparison, summarization, and logical
reasoning queries. To ensure reliability, 20% of QA pairs are validated by
multiple annotators and expert adjudication. UniDoc-Bench supports
apples-to-apples comparison across four paradigms: (1) text-only, (2)
image-only, (3) multimodal text-image fusion, and (4) multimodal joint
retrieval -- under a unified protocol with standardized candidate pools,
prompts, and evaluation metrics. Our experiments show that multimodal
text-image fusion RAG systems consistently outperform both unimodal and jointly
multimodal embedding-based retrieval, indicating that neither text nor images
alone are sufficient and that current multimodal embeddings remain inadequate.
Beyond benchmarking, our analysis reveals when and how visual context
complements textual evidence, uncovers systematic failure modes, and offers
actionable guidance for developing more robust MM-RAG pipelines.

</details>


### [22] [Fine-Tuning Large Language Models with QLoRA for Offensive Language Detection in Roman Urdu-English Code-Mixed Text](https://arxiv.org/abs/2510.03683)
*Nisar Hussain,Amna Qasim,Gull Mehak,Muhammad Zain,Momina Hafeez,Grigori Sidorov*

Main category: cs.CL

TL;DR: 提出基于QLoRA的微调框架，用于改进罗马乌尔都语-英语混合文本中的冒犯性语言检测，通过翻译处理低资源输入，在多个大语言模型上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 罗马乌尔都语等代码混合语言中的贬义词使用给NLP系统带来挑战，包括无明确语法、拼写不一致和标注数据稀缺。

Method: 使用Google Translate将罗马乌尔都语-英语混合数据集翻译成英语，利用QLoRA对多个大语言模型进行内存高效的微调，包括Meta LLaMA 3 8B、Mistral 7B等。

Result: Meta LLaMA 3 8B获得最高F1分数91.45，Mistral 7B达到89.66，均超过传统transformer基线模型。

Conclusion: QLoRA在低资源环境下的代码混合冒犯性语言检测中表现出色，证实了大语言模型在此任务中的潜力，为罗马乌尔都语内容审核提供了可扩展方法。

Abstract: The use of derogatory terms in languages that employ code mixing, such as
Roman Urdu, presents challenges for Natural Language Processing systems due to
unstated grammar, inconsistent spelling, and a scarcity of labeled data. In
this work, we propose a QLoRA based fine tuning framework to improve offensive
language detection in Roman Urdu-English text. We translated the Roman
Urdu-English code mixed dataset into English using Google Translate to leverage
English LLMs, while acknowledging that this translation reduces direct
engagement with code mixing features. Our focus is on classification
performance using English translated low resource inputs. We fine tuned several
transformers and large language models, including Meta LLaMA 3 8B, Mistral 7B
v0.1, LLaMA 2 7B, ModernBERT, and RoBERTa, with QLoRA for memory efficient
adaptation. Models were trained and evaluated on a manually annotated Roman
Urdu dataset for offensive vs non offensive content. Of all tested models, the
highest F1 score of 91.45 was attained by Meta LLaMA 3 8B, followed by Mistral
7B at 89.66, surpassing traditional transformer baselines. These results
demonstrate the efficacy of QLoRA in fine tuning high performing models for low
resource environments such as code mixed offensive language detection, and
confirm the potential of LLMs for this task. This work advances a scalable
approach to Roman Urdu moderation and paves the way for future multilingual
offensive detection systems based on LLMs.

</details>


### [23] [MedReflect: Teaching Medical LLMs to Self-Improve via Reflective Correction](https://arxiv.org/abs/2510.03687)
*Yue Huang,Yanyuan Chen,Dexuan Xu,Weihua Yue,Huamin Zhang,Meikang Qiu,Yu Huang*

Main category: cs.CL

TL;DR: MedReflect是一个无需外部检索或大量标注的医学问题解决框架，通过自反思链激发LLMs的潜在能力，仅需少量训练样本即可显著提升医学基准测试准确率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖外部知识检索或大量推理数据集训练，存在检索开销大、标注成本高、依赖外部助手等问题，限制了在医学领域的性能。

Method: 提出MedReflect框架，模拟医生反思思维模式，生成单次反思链：初始假设生成→自我提问→自我回答→决策精炼，实现自我验证和自我反思。

Result: 仅用2000个随机训练样本和轻量微调，就在多个医学基准测试中取得显著准确率提升，大幅减少标注需求。

Conclusion: LLMs可以通过自我反思和自我改进学习解决专业医学问题，减少对外部监督和大量任务特定微调数据的依赖。

Abstract: Medical problem solving demands expert knowledge and intricate reasoning.
Recent studies of large language models (LLMs) attempt to ease this complexity
by introducing external knowledge verification through retrieval-augmented
generation or by training on reasoning datasets. However, these approaches
suffer from drawbacks such as retrieval overhead and high annotation costs, and
they heavily rely on substituted external assistants to reach limited
performance in medical field. In this paper, we introduce MedReflect, a
generalizable framework designed to inspire LLMs with a physician-like
reflective thinking mode. MedReflect generates a single-pass reflection chain
that includes initial hypothesis generation, self-questioning, self-answering
and decision refinement. This self-verified and self-reflective nature releases
large language model's latent capability in medical problem-solving without
external retrieval or heavy annotation. We demonstrate that MedReflect enables
cost-efficient medical dataset construction: with merely 2,000 randomly sampled
training examples and a light fine-tuning, this approach achieves notable
absolute accuracy improvements across a series of medical benchmarks while
cutting annotation requirements. Our results provide evidence that LLMs can
learn to solve specialized medical problems via self-reflection and
self-improve, reducing reliance on external supervision and extensive
task-specific fine-tuning data.

</details>


### [24] [TreePrompt: Leveraging Hierarchical Few-Shot Example Selection for Improved English-Persian and English-German Translation](https://arxiv.org/abs/2510.03748)
*Ramtin Kakavand,Ebrahim Ansari*

Main category: cs.CL

TL;DR: TreePrompt是一种新的示例选择方法，通过树形结构框架学习LLM偏好来选择高质量、上下文相关的翻译示例，与AFSP或随机选择结合能提升翻译性能。


<details>
  <summary>Details</summary>
Motivation: 现有的少样本提示示例选择方法主要关注查询与示例的相似性，但忽略了示例质量的重要性，这限制了机器翻译性能的进一步提升。

Method: 提出TreePrompt方法，在树形结构框架中学习LLM偏好来选择高质量且上下文相关的示例，并与K-NN和自适应少样本提示(AFSP)方法结合来平衡相似性和质量。

Result: 在英语-波斯语(MIZAN)和英语-德语(WMT19)两个语言对上的评估表明，TreePrompt与AFSP或随机选择结合能显著改善翻译性能。

Conclusion: TreePrompt通过考虑示例质量而不仅仅是相似性，有效提升了少样本提示在机器翻译中的效果，证明了质量感知示例选择的重要性。

Abstract: Large Language Models (LLMs) have consistently demonstrated strong
performance in machine translation, especially when guided by high-quality
prompts. Few-shot prompting is an effective technique to improve translation
quality; however, most existing example selection methods focus solely on
query-to-example similarity and do not account for the quality of the examples.
In this work, we propose TreePrompt, a novel example selection approach that
learns LLM preferences to identify high-quality, contextually relevant examples
within a tree-structured framework. To further explore the balance between
similarity and quality, we combine TreePrompt with K-Nearest Neighbors (K-NN)
and Adaptive Few-Shot Prompting (AFSP). Evaluations on two language pairs -
English-Persian (MIZAN) and English-German (WMT19) - show that integrating
TreePrompt with AFSP or Random selection leads to improved translation
performance.

</details>


### [25] [Cross-Lingual Multi-Granularity Framework for Interpretable Parkinson's Disease Diagnosis from Speech](https://arxiv.org/abs/2510.03758)
*Ilias Tougui,Mehdi Zakroum,Mounir Ghogho*

Main category: cs.CL

TL;DR: 提出了一种基于语音细粒度分析的帕金森病检测方法，通过分析音素、音节和单词级别特征，在意大利语、西班牙语和英语数据集上实现了93.78%的AUROC和92.17%的准确率。


<details>
  <summary>Details</summary>
Motivation: 当前帕金森病语音检测系统分析整个话语，可能忽略了特定语音元素的诊断价值。帕金森病影响全球超过1000万人，其中高达89%的患者存在语音障碍。

Method: 开发了细粒度感知的多语言PD检测方法，使用自动化流程从录音中提取时间对齐的音素、音节和单词。采用双向LSTM和多头注意力机制，比较不同粒度级别的诊断性能。

Result: 音素级别分析表现最佳，AUROC达到93.78% ± 2.34%，准确率为92.17% ± 2.43%。注意力分析显示最有信息的语音特征与临床协议一致：音素级别的持续元音，音节级别的交替运动音节，以及单词级别的/pataka/序列。

Conclusion: 该方法展示了跨语言帕金森病检测的增强诊断能力，为临床诊断提供了更精确的语音分析工具。

Abstract: Parkinson's Disease (PD) affects over 10 million people worldwide, with
speech impairments in up to 89% of patients. Current speech-based detection
systems analyze entire utterances, potentially overlooking the diagnostic value
of specific phonetic elements. We developed a granularity-aware approach for
multilingual PD detection using an automated pipeline that extracts
time-aligned phonemes, syllables, and words from recordings. Using Italian,
Spanish, and English datasets, we implemented a bidirectional LSTM with
multi-head attention to compare diagnostic performance across the different
granularity levels. Phoneme-level analysis achieved superior performance with
AUROC of 93.78% +- 2.34% and accuracy of 92.17% +- 2.43%. This demonstrates
enhanced diagnostic capability for cross-linguistic PD detection. Importantly,
attention analysis revealed that the most informative speech features align
with those used in established clinical protocols: sustained vowels (/a/, /e/,
/o/, /i/) at phoneme level, diadochokinetic syllables (/ta/, /pa/, /la/, /ka/)
at syllable level, and /pataka/ sequences at word level. Source code will be
available at https://github.com/jetliqs/clearpd.

</details>


### [26] [Prompt Balance Matters: Understanding How Imbalanced Few-Shot Learning Affects Multilingual Sense Disambiguation in LLMs](https://arxiv.org/abs/2510.03762)
*Deshan Sumanathilaka,Nicholas Micallef,Julian Hough*

Main category: cs.CL

TL;DR: 本研究探讨了少样本提示策略对词义消歧任务的影响，特别关注样本分布不平衡带来的偏见问题。研究发现不平衡的少样本示例会导致多语言词义消歧错误，但英语中未出现此问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的快速发展显著改变了自然语言处理领域，其中少样本提示因其实用性和有效性受到广泛关注。本研究旨在探索少样本提示策略如何影响词义消歧任务，特别关注样本分布不平衡引入的偏见问题。

Method: 使用GLOSSGPT提示方法，在英语、德语、西班牙语、法语和意大利语五种语言上测试其有效性。评估了GPT-4o和LLaMA-3.1-70B模型的行为表现。

Result: 结果显示不平衡的少样本示例会导致多语言词义消歧错误预测，但英语中未出现此问题。多语言词义消歧对少样本设置中的样本分布非常敏感。

Conclusion: 研究强调了在多语言词义消歧任务中需要采用平衡且具有代表性的提示策略，以确保模型性能的稳定性和准确性。

Abstract: Recent advances in Large Language Models (LLMs) have significantly reshaped
the landscape of Natural Language Processing (NLP). Among the various prompting
techniques, few-shot prompting has gained considerable attention for its
practicality and effectiveness. This study investigates how few-shot prompting
strategies impact the Word Sense Disambiguation (WSD) task, particularly
focusing on the biases introduced by imbalanced sample distributions. We use
the GLOSSGPT prompting method, an advanced approach for English WSD, to test
its effectiveness across five languages: English, German, Spanish, French, and
Italian. Our results show that imbalanced few-shot examples can cause incorrect
sense predictions in multilingual languages, but this issue does not appear in
English. To assess model behavior, we evaluate both the GPT-4o and
LLaMA-3.1-70B models and the results highlight the sensitivity of multilingual
WSD to sample distribution in few-shot settings, emphasizing the need for
balanced and representative prompting strategies.

</details>


### [27] [Rezwan: Leveraging Large Language Models for Comprehensive Hadith Text Processing: A 1.2M Corpus Development](https://arxiv.org/abs/2510.03781)
*Majid Asgari-Bidhendi,Muhammad Amin Ghaseminia,Alireza Shahbazi,Sayyed Ali Hossayni,Najmeh Torabian,Behrouz Minaei-Bidgoli*

Main category: cs.CL

TL;DR: 开发了Rezwan大型AI辅助圣训语料库，包含120万条圣训，通过全自动流水线进行提取和结构化处理，为伊斯兰研究提供多语言、语义丰富的数字化基础设施。


<details>
  <summary>Details</summary>
Motivation: 传统圣训整理依赖人工，成本高、规模有限。需要利用AI技术实现大规模、低成本、多语言的圣训数字化处理，为数字人文和伊斯兰研究提供现代化基础设施。

Method: 基于数字资源库，采用大型语言模型进行分段、传述链-文本分离、验证和多层增强处理，包括机器翻译、智能标音、摘要生成、主题标记和跨文本语义分析。

Result: 在1,213条随机样本评估中，结构化任务接近人类准确度（传述链-文本分离9.33/10，摘要9.33/10），整体评分8.46/10显著优于人工整理的Noor语料库（3.66/10），成本仅为传统方法的极小部分。

Conclusion: AI能够有效增强人类专业知识，为宗教文本处理引入新范式，实现大规模、多语言、语义丰富的伊斯兰遗产数字化访问。

Abstract: This paper presents the development of Rezwan, a large-scale AI-assisted
Hadith corpus comprising over 1.2M narrations, extracted and structured through
a fully automated pipeline. Building on digital repositories such as Maktabat
Ahl al-Bayt, the pipeline employs Large Language Models (LLMs) for
segmentation, chain--text separation, validation, and multi-layer enrichment.
Each narration is enhanced with machine translation into twelve languages,
intelligent diacritization, abstractive summarization, thematic tagging, and
cross-text semantic analysis. This multi-step process transforms raw text into
a richly annotated research-ready infrastructure for digital humanities and
Islamic studies. A rigorous evaluation was conducted on 1,213 randomly sampled
narrations, assessed by six domain experts. Results show near-human accuracy in
structured tasks such as chain--text separation (9.33/10) and summarization
(9.33/10), while highlighting ongoing challenges in diacritization and semantic
similarity detection. Comparative analysis against the manually curated Noor
Corpus demonstrates the superiority of Najm in both scale and quality, with a
mean overall score of 8.46/10 versus 3.66/10. Furthermore, cost analysis
confirms the economic feasibility of the AI approach: tasks requiring over
229,000 hours of expert labor were completed within months at a fraction of the
cost. The work introduces a new paradigm in religious text processing by
showing how AI can augment human expertise, enabling large-scale, multilingual,
and semantically enriched access to Islamic heritage.

</details>


### [28] [Mechanistic Interpretability of Socio-Political Frames in Language Models](https://arxiv.org/abs/2510.03799)
*Hadi Asghari,Sami Nenno*

Main category: cs.CL

TL;DR: LLMs能够生成和识别深层认知框架，特别是在社会政治语境中，研究者发现模型在隐藏表示中存在与特定框架相关的单一维度。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型生成和识别深层认知框架的能力，特别是在社会政治语境中，以理解模型如何捕捉和表达有意义的人类概念。

Method: 受机制可解释性研究启发，研究者调查了'严格父亲'和'养育父母'框架在模型隐藏表示中的位置，识别出与这些框架存在强相关性的单一维度。

Result: LLMs在生成唤起特定框架的文本方面表现出高度流畅性，并且能够在零样本设置中识别这些框架。

Conclusion: 这项研究有助于理解LLMs如何捕捉和表达有意义的人类概念，揭示了模型内部表示与认知框架之间的关联。

Abstract: This paper explores the ability of large language models to generate and
recognize deep cognitive frames, particularly in socio-political contexts. We
demonstrate that LLMs are highly fluent in generating texts that evoke specific
frames and can recognize these frames in zero-shot settings. Inspired by
mechanistic interpretability research, we investigate the location of the
`strict father' and `nurturing parent' frames within the model's hidden
representation, identifying singular dimensions that correlate strongly with
their presence. Our findings contribute to understanding how LLMs capture and
express meaningful human concepts.

</details>


### [29] [Beyond Token Length: Step Pruner for Efficient and Accurate Reasoning in Large Language Models](https://arxiv.org/abs/2510.03805)
*Canhui Wu,Qiong Cao,Chang Li,Zhenfang Wang,Chao Xue,Yuwei Fan,Wei Xi,Xiaodong He*

Main category: cs.CL

TL;DR: Step Pruner (SP)是一个强化学习框架，通过惩罚冗余推理步骤来减少大型推理模型的过度思考问题，在保持准确性的同时显著降低响应长度。


<details>
  <summary>Details</summary>
Motivation: 现有通过强化学习惩罚生成token的方法存在两个问题：更少的token不一定对应更少的推理步骤；模型可能在训练后期通过丢弃推理步骤来最小化token使用，产生hacking行为。

Method: 提出Step Pruner框架，使用步骤感知的奖励函数，在保证正确性的同时惩罚冗余步骤，对错误响应不给予奖励以防止强化错误推理。还提出动态停止机制，当任何输出步骤长度超过上限时停止更新，防止合并步骤导致的hacking行为。

Result: 在四个推理基准测试上的实验表明，SP实现了最先进的准确性，同时显著减少了响应长度。在AIME24上，token使用减少了69.7%。

Conclusion: Step Pruner框架有效解决了大型推理模型的过度思考问题，实现了更高效的推理过程。

Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex tasks
but often suffer from excessive verbosity, known as "overthinking." Existing
solutions via reinforcement learning (RL) typically penalize generated tokens
to promote conciseness. However, these methods encounter two challenges:
responses with fewer tokens do not always correspond to fewer reasoning steps,
and models may develop hacking behavior in later stages of training by
discarding reasoning steps to minimize token usage. In this work, we introduce
\textbf{Step Pruner (SP)}, an RL framework that steers LRMs toward more
efficient reasoning by favoring compact reasoning steps. Our step-aware reward
function prioritizes correctness while imposing penalties for redundant steps,
and withholds rewards for incorrect responses to prevent the reinforcement of
erroneous reasoning. Moreover, we propose a dynamic stopping mechanism: when
the length of any output step exceeds the upper limit, we halt updates to
prevent hacking behavior caused by merging steps. Extensive experiments across
four reasoning benchmarks demonstrate that SP achieves state-of-the-art
accuracy while significantly reducing response length. For instance, on AIME24,
SP reduces token usage by \textbf{69.7\%}.

</details>


### [30] [Annotate Rhetorical Relations with INCEpTION: A Comparison with Automatic Approaches](https://arxiv.org/abs/2510.03808)
*Mehedi Hasan Emon*

Main category: cs.CL

TL;DR: 该研究使用INCEpTION工具标注话语修辞关系，比较了手动标注与基于大语言模型的自动方法。在板球新闻语料上评估BERT、DistilBERT和逻辑回归模型对修辞关系分类的性能，发现DistilBERT准确率最高。


<details>
  <summary>Details</summary>
Motivation: 探索话语修辞关系标注方法，比较手动与自动标注的差异，评估不同模型在修辞关系分类任务中的表现，促进话语解析与基于Transformer的NLP技术的交叉研究。

Method: 使用INCEpTION工具进行修辞关系标注，在板球新闻语料上测试BERT、DistilBERT和逻辑回归模型，分类修辞关系类型包括阐述、对比、背景和因果等。

Result: DistilBERT模型在修辞关系分类任务中取得了最高的准确率，显示出其在话语关系预测方面的潜力。

Conclusion: 该研究表明DistilBERT在话语修辞关系分类中表现最佳，为话语解析与Transformer模型的结合提供了有价值的见解。

Abstract: This research explores the annotation of rhetorical relations in discourse
using the INCEpTION tool and compares manual annotation with automatic
approaches based on large language models. The study focuses on sports reports
(specifically cricket news) and evaluates the performance of BERT, DistilBERT,
and Logistic Regression models in classifying rhetorical relations such as
elaboration, contrast, background, and cause-effect. The results show that
DistilBERT achieved the highest accuracy, highlighting its potential for
efficient discourse relation prediction. This work contributes to the growing
intersection of discourse parsing and transformer-based NLP. (This paper was
conducted as part of an academic requirement under the supervision of Prof. Dr.
Ralf Klabunde, Linguistic Data Science Lab, Ruhr University Bochum.) Keywords:
Rhetorical Structure Theory, INCEpTION, BERT, DistilBERT, Discourse Parsing,
NLP.

</details>


### [31] [Read Between the Lines: A Benchmark for Uncovering Political Bias in Bangla News Articles](https://arxiv.org/abs/2510.03898)
*Nusrat Jahan Lia,Shubhashis Roy Dipta,Abdullah Khan Zehady,Naymul Islam,Madhusodan Chakraborty,Abdullah Al Wasif*

Main category: cs.CL

TL;DR: 提出了首个孟加拉语政治立场检测基准数据集，包含200篇新闻文章，标注了亲政府、批评政府和中性立场。评估了28个大语言模型，发现模型在检测批评政府内容时表现良好，但在识别中性文章时存在困难，且倾向于过度预测亲政府立场。


<details>
  <summary>Details</summary>
Motivation: 南亚地区的媒体偏见检测很重要，但孟加拉语政治立场研究缺乏标注数据集和计算研究。孟加拉语政治立场检测需要理解语言线索、文化背景、微妙偏见、修辞策略、语码转换、隐含情感和社会政治背景。

Method: 构建了包含200篇政治意义重大且高度争议的孟加拉语新闻文章的数据集，标注了三种立场（亲政府、批评政府、中性），并设计了诊断分析来评估大语言模型。

Result: 评估28个专有和开源大语言模型显示：检测批评政府内容表现强劲（F1最高达0.83），但在中性文章上存在显著困难（F1低至0.00）。模型倾向于过度预测亲政府立场，经常误解模糊叙述。

Conclusion: 该数据集及其相关诊断为推进孟加拉语媒体立场检测研究奠定了基础，并为改善低资源语言中大语言模型性能提供了见解。

Abstract: Detecting media bias is crucial, specifically in the South Asian region.
Despite this, annotated datasets and computational studies for Bangla political
bias research remain scarce. Crucially because, political stance detection in
Bangla news requires understanding of linguistic cues, cultural context, subtle
biases, rhetorical strategies, code-switching, implicit sentiment, and
socio-political background. To address this, we introduce the first benchmark
dataset of 200 politically significant and highly debated Bangla news articles,
labeled for government-leaning, government-critique, and neutral stances,
alongside diagnostic analyses for evaluating large language models (LLMs). Our
comprehensive evaluation of 28 proprietary and open-source LLMs shows strong
performance in detecting government-critique content (F1 up to 0.83) but
substantial difficulty with neutral articles (F1 as low as 0.00). Models also
tend to over-predict government-leaning stances, often misinterpreting
ambiguous narratives. This dataset and its associated diagnostics provide a
foundation for advancing stance detection in Bangla media research and offer
insights for improving LLM performance in low-resource languages.

</details>


### [32] [PsycholexTherapy: Simulating Reasoning in Psychotherapy with Small Language Models in Persian](https://arxiv.org/abs/2510.03913)
*Mohammad Amin Abbasi,Hassan Naderi*

Main category: cs.CL

TL;DR: PsychoLexTherapy是一个在波斯语中模拟心理治疗推理的框架，使用小型语言模型，注重文化适应性和隐私保护，支持多轮对话和结构化记忆。


<details>
  <summary>Details</summary>
Motivation: 开发针对波斯语等资源不足语言的文化适应性、治疗连贯的对话系统，解决隐私保护和设备部署的可行性问题。

Method: 三阶段开发流程：评估SLMs心理知识、设计推理导向框架、构建评估数据集；比较简单提示、多代理辩论和结构化治疗推理路径。

Result: 在单轮偏好研究中获得最高人类评价，在多轮测试中完整框架在共情、连贯性、文化适应性和个性化方面表现最佳，长期记忆模块至关重要。

Conclusion: PsychoLexTherapy为波斯语心理治疗模拟建立了实用、隐私保护和符合文化的基础，贡献了新颖数据集、可复现评估流程和结构化记忆的实证见解。

Abstract: This study presents PsychoLexTherapy, a framework for simulating
psychotherapeutic reasoning in Persian using small language models (SLMs). The
framework tackles the challenge of developing culturally grounded,
therapeutically coherent dialogue systems with structured memory for multi-turn
interactions in underrepresented languages. To ensure privacy and feasibility,
PsychoLexTherapy is optimized for on-device deployment, enabling use without
external servers. Development followed a three-stage process: (i) assessing
SLMs psychological knowledge with PsychoLexEval; (ii) designing and
implementing the reasoning-oriented PsychoLexTherapy framework; and (iii)
constructing two evaluation datasets-PsychoLexQuery (real Persian user
questions) and PsychoLexDialogue (hybrid simulated sessions)-to benchmark
against multiple baselines. Experiments compared simple prompting, multi-agent
debate, and structured therapeutic reasoning paths. Results showed that
deliberate model selection balanced accuracy, efficiency, and privacy. On
PsychoLexQuery, PsychoLexTherapy outperformed all baselines in automatic
LLM-as-a-judge evaluation and was ranked highest by human evaluators in a
single-turn preference study. In multi-turn tests with PsychoLexDialogue, the
long-term memory module proved essential: while naive history concatenation
caused incoherence and information loss, the full framework achieved the
highest ratings in empathy, coherence, cultural fit, and personalization.
Overall, PsychoLexTherapy establishes a practical, privacy-preserving, and
culturally aligned foundation for Persian psychotherapy simulation,
contributing novel datasets, a reproducible evaluation pipeline, and empirical
insights into structured memory for therapeutic reasoning.

</details>


### [33] [Mapping Patient-Perceived Physician Traits from Nationwide Online Reviews with LLMs](https://arxiv.org/abs/2510.03997)
*Junjie Luo,Rui Han,Arshana Welivita,Zeleikun Di,Jingfu Wu,Xuzhe Zhi,Ritu Agarwal,Gordon Gao*

Main category: cs.CL

TL;DR: 使用LLM从410万条患者评价中提取医生的大五人格特质和患者主观判断，验证了方法的有效性，并揭示了性别差异、专科差异等系统性模式。


<details>
  <summary>Details</summary>
Motivation: 了解患者对医生的认知对于改善信任、沟通和满意度至关重要，需要大规模分析患者评价来理解医患关系。

Method: 基于大语言模型的流水线方法，分析410万条患者评价，通过多模型比较和人类专家基准进行验证。

Result: 人类与LLM评估高度一致（相关系数0.72-0.89），与患者满意度显著相关；发现男性医生在所有特质上评分更高，同理心相关特质在儿科和精神科占主导，所有特质都能正向预测总体满意度。

Conclusion: 从患者叙述中自动提取特质可以提供可解释、经过验证的指标，用于大规模理解医患关系，对医疗质量测量、偏见检测和人力资源发展具有重要意义。

Abstract: Understanding how patients perceive their physicians is essential to
improving trust, communication, and satisfaction. We present a large language
model (LLM)-based pipeline that infers Big Five personality traits and five
patient-oriented subjective judgments. The analysis encompasses 4.1 million
patient reviews of 226,999 U.S. physicians from an initial pool of one million.
We validate the method through multi-model comparison and human expert
benchmarking, achieving strong agreement between human and LLM assessments
(correlation coefficients 0.72-0.89) and external validity through correlations
with patient satisfaction (r = 0.41-0.81, all p<0.001). National-scale analysis
reveals systematic patterns: male physicians receive higher ratings across all
traits, with largest disparities in clinical competence perceptions;
empathy-related traits predominate in pediatrics and psychiatry; and all traits
positively predict overall satisfaction. Cluster analysis identifies four
distinct physician archetypes, from "Well-Rounded Excellent" (33.8%, uniformly
high traits) to "Underperforming" (22.6%, consistently low). These findings
demonstrate that automated trait extraction from patient narratives can provide
interpretable, validated metrics for understanding physician-patient
relationships at scale, with implications for quality measurement, bias
detection, and workforce development in healthcare.

</details>


### [34] [Simulating and Understanding Deceptive Behaviors in Long-Horizon Interactions](https://arxiv.org/abs/2510.03999)
*Yang Xu,Xuanming Zhang,Min-Hsuan Yeh,Jwala Dhamala,Ousmane Dia,Rahul Gupta,Yixuan Li*

Main category: cs.CL

TL;DR: 该研究提出了首个模拟框架来评估LLM在长期交互中的欺骗行为，通过多智能体系统发现欺骗行为具有模型依赖性，会随压力增加，并损害监督者信任。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单轮提示下的LLM欺骗行为，但现实中的欺骗策略通常在长期交互中展开，需要系统性的评估框架。

Method: 构建多智能体模拟框架：执行者智能体完成任务，监督者智能体评估进展并提供反馈，独立的欺骗审计员分析完整轨迹识别欺骗行为。

Result: 在11个前沿模型上的实验表明：欺骗具有模型依赖性，随事件压力增加，持续损害监督者信任，并识别出隐瞒、含糊其辞和伪造等具体策略。

Conclusion: 欺骗是长期交互中出现的风险，该框架为评估现实世界中信任敏感环境下的未来LLM提供了基础。

Abstract: Deception is a pervasive feature of human communication and an emerging
concern in large language models (LLMs). While recent studies document
instances of LLM deception under pressure, most evaluations remain confined to
single-turn prompts and fail to capture the long-horizon interactions in which
deceptive strategies typically unfold. We introduce the first simulation
framework for probing and evaluating deception in LLMs under extended sequences
of interdependent tasks and dynamic contextual pressures. Our framework
instantiates a multi-agent system: a performer agent tasked with completing
tasks and a supervisor agent that evaluates progress, provides feedback, and
maintains evolving states of trust. An independent deception auditor then
reviews full trajectories to identify when and how deception occurs. We conduct
extensive experiments across 11 frontier models, spanning both closed- and
open-source systems, and find that deception is model-dependent, increases with
event pressure, and consistently erodes supervisor trust. Qualitative analyses
further reveal distinct strategies of concealment, equivocation, and
falsification. Our findings establish deception as an emergent risk in
long-horizon interactions and provide a foundation for evaluating future LLMs
in real-world, trust-sensitive contexts.

</details>


### [35] [Named Entity Recognition in COVID-19 tweets with Entity Knowledge Augmentation](https://arxiv.org/abs/2510.04001)
*Xuankang Zhang,Jiangming Liu*

Main category: cs.CL

TL;DR: 提出了一种新颖的实体知识增强方法，用于COVID-19命名实体识别，可提升社交媒体和生物医学文本中的NER性能。


<details>
  <summary>Details</summary>
Motivation: COVID-19疫情引发社交媒体广泛讨论，但社交媒体文本非正式且标注数据稀缺，加上需要领域专业知识，使得命名实体识别面临挑战。

Method: 采用实体知识增强方法，通过整合领域特定知识来提升模型性能，适用于正式和非正式文本格式。

Result: 在COVID-19推文数据集和PubMed数据集上的实验表明，该方法在完全监督和少样本设置下均能提高NER性能。

Conclusion: 提出的实体知识增强方法有效解决了COVID-19命名实体识别中的数据稀缺和领域知识需求问题，在生物医学NER任务中具有通用性。

Abstract: The COVID-19 pandemic causes severe social and economic disruption around the
world, raising various subjects that are discussed over social media.
Identifying pandemic-related named entities as expressed on social media is
fundamental and important to understand the discussions about the pandemic.
However, there is limited work on named entity recognition on this topic due to
the following challenges: 1) COVID-19 texts in social media are informal and
their annotations are rare and insufficient to train a robust recognition
model, and 2) named entity recognition in COVID-19 requires extensive
domain-specific knowledge. To address these issues, we propose a novel entity
knowledge augmentation approach for COVID-19, which can also be applied in
general biomedical named entity recognition in both informal text format and
formal text format. Experiments carried out on the COVID-19 tweets dataset and
PubMed dataset show that our proposed entity knowledge augmentation improves
NER performance in both fully-supervised and few-shot settings. Our source code
is publicly available: https://github.com/kkkenshi/LLM-EKA/tree/master

</details>


### [36] [AgriGPT-VL: Agricultural Vision-Language Understanding Suite](https://arxiv.org/abs/2510.04002)
*Bo Yang,Yunkui Chen,Lanfei Feng,Yu Zhang,Xiao Xu,Jianyu Zhang,Nueraili Aierken,Runhe Huang,Hongjian Lin,Yibin Ying,Shijian Li*

Main category: cs.CL

TL;DR: 提出了AgriGPT-VL套件，包括最大的农业视觉语言语料库Agri-3M-VL、农业专用视觉语言模型AgriGPT-VL和评估套件AgriBench-VL-4K，在农业多模态任务上表现优于通用模型。


<details>
  <summary>Details</summary>
Motivation: 解决农业领域缺乏定制化模型、精心策划的视觉语言语料库和严格评估的问题，推动农业多模态AI应用发展。

Method: 使用可扩展多智能体数据生成器构建Agri-3M-VL语料库；通过渐进式课程训练AgriGPT-VL模型，包括文本基础、多模态浅层/深层对齐和GRPO精炼；建立多指标评估和LLM-as-a-judge框架。

Result: AgriGPT-VL在AgriBench-VL-4K上优于领先的通用VLMs，在LLM-as-a-judge评估中获得更高胜率，同时在文本任务上保持竞争力且无语言能力退化。

Conclusion: AgriGPT-VL套件为农业多模态AI提供了有效解决方案，消融研究证实了对齐和GRPO精炼阶段的一致增益，所有资源将开源以支持可重复研究和低资源农业部署。

Abstract: Despite rapid advances in multimodal large language models, agricultural
applications remain constrained by the scarcity of domain-tailored models,
curated vision-language corpora, and rigorous evaluation. To address these
challenges, we present the AgriGPT-VL Suite, a unified multimodal framework for
agriculture. Our contributions are threefold. First, we introduce Agri-3M-VL,
the largest vision-language corpus for agriculture to our knowledge, curated by
a scalable multi-agent data generator; it comprises 1M image-caption pairs, 2M
image-grounded VQA pairs, 50K expert-level VQA instances, and 15K GRPO
reinforcement learning samples. Second, we develop AgriGPT-VL, an
agriculture-specialized vision-language model trained via a progressive
curriculum of textual grounding, multimodal shallow/deep alignment, and GRPO
refinement. This method achieves strong multimodal reasoning while preserving
text-only capability. Third, we establish AgriBench-VL-4K, a compact yet
challenging evaluation suite with open-ended and image-grounded questions,
paired with multi-metric evaluation and an LLM-as-a-judge framework.
Experiments show that AgriGPT-VL outperforms leading general-purpose VLMs on
AgriBench-VL-4K, achieving higher pairwise win rates in the LLM-as-a-judge
evaluation. Meanwhile, it remains competitive on the text-only AgriBench-13K
with no noticeable degradation of language ability. Ablation studies further
confirm consistent gains from our alignment and GRPO refinement stages. We will
open source all of the resources to support reproducible research and
deployment in low-resource agricultural settings.

</details>


### [37] [LLM Microscope: What Model Internals Reveal About Answer Correctness and Context Utilization](https://arxiv.org/abs/2510.04013)
*Jiarui Liu,Jivitesh Jain,Mona Diab,Nishant Subramani*

Main category: cs.CL

TL;DR: 利用大语言模型的内部激活信号来预测输出正确性和上下文有效性，通过简单分类器实现约75%的准确率，为模型可信度提供早期审计能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型经常以高置信度生成错误信息，需要解决如何识别查询是否需要检索上下文以及评估上下文有效性的挑战。

Method: 使用可解释性方法，基于模型内部激活信号训练简单分类器，特别关注第一个输出token的中间层激活，并引入指标区分正确、错误和不相关上下文。

Result: 在六个不同模型上的实验表明，基于第一个输出token中间层激活的分类器能以约75%的准确率预测输出正确性；基于模型内部指标的评估显著优于提示基准方法。

Conclusion: 模型内部包含关于输出正确性和上下文有效性的信号，这为理解大语言模型的底层决策过程提供了新的视角，有助于防范受污染上下文引入的错误。

Abstract: Although large language models (LLMs) have tremendous utility,
trustworthiness is still a chief concern: models often generate incorrect
information with high confidence. While contextual information can help guide
generation, identifying when a query would benefit from retrieved context and
assessing the effectiveness of that context remains challenging. In this work,
we operationalize interpretability methods to ascertain whether we can predict
the correctness of model outputs from the model's activations alone. We also
explore whether model internals contain signals about the efficacy of external
context. We consider correct, incorrect, and irrelevant context and introduce
metrics to distinguish amongst them. Experiments on six different models reveal
that a simple classifier trained on intermediate layer activations of the first
output token can predict output correctness with about 75% accuracy, enabling
early auditing. Our model-internals-based metric significantly outperforms
prompting baselines at distinguishing between correct and incorrect context,
guarding against inaccuracies introduced by polluted context. These findings
offer a lens to better understand the underlying decision-making processes of
LLMs. Our code is publicly available at
https://github.com/jiarui-liu/LLM-Microscope

</details>


### [38] [Thai Semantic End-of-Turn Detection for Real-Time Voice Agents](https://arxiv.org/abs/2510.04016)
*Thanapol Popit,Natthapath Rungseesiripak,Monthol Charattrakool,Saksorn Ruangtanusak*

Main category: cs.CL

TL;DR: 该论文系统研究了泰语文本端到端检测方法，比较了零样本/少样本提示与监督微调方法，发现小型微调模型能提供近乎即时的EOT决策，适合设备端代理使用。


<details>
  <summary>Details</summary>
Motivation: 传统音频静音端点检测存在延迟高、在犹豫或语言特定现象下失效的问题，需要开发更可靠的实时语音交互端点检测方法。

Method: 使用YODAS语料库的转录字幕和泰语特定语言线索，将EOT制定为基于词元边界的二元决策，比较了紧凑LLM的零样本/少样本提示与轻量级transformer的监督微调。

Result: 报告了清晰的准确率-延迟权衡，小型微调模型能够提供近乎即时的EOT决策。

Conclusion: 建立了泰语基线，证明小型微调模型适合设备端代理的实时EOT检测需求。

Abstract: Fluid voice-to-voice interaction requires reliable and low-latency detection
of when a user has finished speaking. Traditional audio-silence end-pointers
add hundreds of milliseconds of delay and fail under hesitations or
language-specific phenomena. We present, to our knowledge, the first systematic
study of Thai text-only end-of-turn (EOT) detection for real-time agents. We
compare zero-shot and few-shot prompting of compact LLMs to supervised
fine-tuning of lightweight transformers. Using transcribed subtitles from the
YODAS corpus and Thai-specific linguistic cues (e.g., sentence-final
particles), we formulate EOT as a binary decision over token boundaries. We
report a clear accuracy-latency tradeoff and provide a public-ready
implementation plan. This work establishes a Thai baseline and demonstrates
that small, fine-tuned models can deliver near-instant EOT decisions suitable
for on-device agents.

</details>


### [39] [Does Using Counterfactual Help LLMs Explain Textual Importance in Classification?](https://arxiv.org/abs/2510.04031)
*Nelvin Tan,James Asikin Cheung,Yu-Ching Shih,Dong Yang,Amol Salunkhe*

Main category: cs.CL

TL;DR: 该研究探讨了在LLM文本分类任务中，通过引入反事实推理来识别影响分类决策的关键词，并提出了决策改变率框架来量化这些词的重要性。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs通常作为黑盒模型且调用成本高昂，需要开发有效的方法来解释其分类决策，特别是识别对决策有重要贡献的关键词。

Method: 提出了一个包含反事实推理的框架，通过计算决策改变率来量化关键词在分类决策中的重要性。

Result: 实验结果表明，使用反事实推理有助于更准确地识别影响LLM分类决策的关键词。

Conclusion: 反事实推理是解释LLM分类决策的有效方法，决策改变率框架能够可靠地量化关键词的重要性。

Abstract: Large language models (LLMs) are becoming useful in many domains due to their
impressive abilities that arise from large training datasets and large model
sizes. More recently, they have been shown to be very effective in textual
classification tasks, motivating the need to explain the LLMs' decisions.
Motivated by practical constrains where LLMs are black-boxed and LLM calls are
expensive, we study how incorporating counterfactuals into LLM reasoning can
affect the LLM's ability to identify the top words that have contributed to its
classification decision. To this end, we introduce a framework called the
decision changing rate that helps us quantify the importance of the top words
in classification. Our experimental results show that using counterfactuals can
be helpful.

</details>


### [40] [Small Language Models for Emergency Departments Decision Support: A Benchmark Study](https://arxiv.org/abs/2510.04032)
*Zirui Wang,Jiajun Wu,Braden Teitge,Jessalyn Holodinsky,Steve Drew*

Main category: cs.CL

TL;DR: 该论文提出了一个针对急诊部门的SLM基准测试，发现通用领域的小语言模型在急诊决策支持任务中表现优于医学微调的模型，表明急诊场景可能不需要专门的医学微调。


<details>
  <summary>Details</summary>
Motivation: 急诊部门需要快速准确的决策支持，SLM因其推理能力和高效性能具有潜力，但实际部署面临硬件限制、运营成本和隐私问题。

Method: 构建综合基准测试，使用MedMCQA、MedQA-4Options和PubMedQA数据集，模拟急诊医生日常任务，评估在通用和医学语料上训练的SLM。

Result: 实验结果显示通用领域SLM在急诊相关基准测试中意外地优于医学微调的SLM。

Conclusion: 对于急诊部门，专门的医学模型微调可能不是必需的，通用领域SLM已能提供足够的决策支持能力。

Abstract: Large language models (LLMs) have become increasingly popular in medical
domains to assist physicians with a variety of clinical and operational tasks.
Given the fast-paced and high-stakes environment of emergency departments
(EDs), small language models (SLMs), characterized by a reduction in parameter
count compared to LLMs, offer significant potential due to their inherent
reasoning capability and efficient performance. This enables SLMs to support
physicians by providing timely and accurate information synthesis, thereby
improving clinical decision-making and workflow efficiency. In this paper, we
present a comprehensive benchmark designed to identify SLMs suited for ED
decision support, taking into account both specialized medical expertise and
broad general problem-solving capabilities. In our evaluations, we focus on
SLMs that have been trained on a mixture of general-domain and medical corpora.
A key motivation for emphasizing SLMs is the practical hardware limitations,
operational cost constraints, and privacy concerns in the typical real-world
deployments. Our benchmark datasets include MedMCQA, MedQA-4Options, and
PubMedQA, with the medical abstracts dataset emulating tasks aligned with real
ED physicians' daily tasks. Experimental results reveal that general-domain
SLMs surprisingly outperform their medically fine-tuned counterparts across
these diverse benchmarks for ED. This indicates that for ED, specialized
medical fine-tuning of the model may not be required.

</details>


### [41] [Exploring Chain-of-Thought Reasoning for Steerable Pluralistic Alignment](https://arxiv.org/abs/2510.04045)
*Yunfan Zhang,Kathleen McKeown,Smaranda Muresan*

Main category: cs.CL

TL;DR: 研究探索了如何通过思维链推理技术构建可引导的多元化大语言模型，发现强化学习与可验证奖励方法在效果和样本效率上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型通常反映相对统一的价值观念，限制了其在需要理解细微人类观点任务中的应用，因此需要开发能够支持可引导多元化的模型能力。

Method: 探索了多种方法：思维链提示、基于人工编写思维链的微调、基于合成解释的微调，以及强化学习与可验证奖励方法。

Result: 在研究的各种方法中，强化学习与可验证奖励方法始终优于其他方法，并展现出强大的训练样本效率。

Conclusion: 思维链推理技术可以有效构建可引导的多元化模型，其中强化学习与可验证奖励是最有前景的方法，同时生成的思维链在忠实性和安全性方面表现良好。

Abstract: Large Language Models (LLMs) are typically trained to reflect a relatively
uniform set of values, which limits their applicability to tasks that require
understanding of nuanced human perspectives. Recent research has underscored
the importance of enabling LLMs to support steerable pluralism -- the capacity
to adopt a specific perspective and align generated outputs with it. In this
work, we investigate whether Chain-of-Thought (CoT) reasoning techniques can be
applied to building steerable pluralistic models. We explore several methods,
including CoT prompting, fine-tuning on human-authored CoT, fine-tuning on
synthetic explanations, and Reinforcement Learning with Verifiable Rewards
(RLVR). We evaluate these approaches using the Value Kaleidoscope and OpinionQA
datasets. Among the methods studied, RLVR consistently outperforms others and
demonstrates strong training sample efficiency. We further analyze the
generated CoT traces with respect to faithfulness and safety.

</details>


### [42] [What Makes Diffusion Language Models Super Data Learners?](https://arxiv.org/abs/2510.04071)
*Zitian Gao,Haoming Luo,Lynx Chen,Jason Klein Liu,Ran Tao,Joey Zhou,Bryan Dai*

Main category: cs.CL

TL;DR: 扩散语言模型在有限数据条件下表现出显著的数据效率，研究发现随机掩码是主要原因，类似效果可通过MLP dropout和权重衰减实现，表明随机正则化在多轮训练中广泛提升数据效率。


<details>
  <summary>Details</summary>
Motivation: 探究扩散语言模型在有限数据约束下实现卓越数据效率的潜在机制，目前这些机制尚不明确。

Method: 通过广泛的消融实验来分离数据效率的来源，分析随机掩码、MLP dropout和权重衰减等不同因素的影响。

Result: 随机掩码在输入标记中起主导作用，类似的数据效率增益可以通过MLP dropout和权重衰减获得。

Conclusion: 随机正则化在多轮训练中广泛增强数据效率，这为理解扩散语言模型的数据效率机制提供了重要见解。

Abstract: Recent studies have shown that diffusion language models achieve remarkable
data efficiency under limited-data constraints, yet the underlying mechanisms
remain unclear. In this work, we perform extensive ablation experiments to
disentangle the sources of this efficiency. Our results show that random
masking of input tokens plays the dominant role. We further show that similar
gains can be obtained through in MLP dropout and weight decay, indicating that
stochastic regularization broadly enhances data efficiency in multi-epoch
training. Our code is available at
https://github.com/zitian-gao/data-efficiency.

</details>


### [43] [PoLi-RL: A Point-to-List Reinforcement Learning Framework for Conditional Semantic Textual Similarity](https://arxiv.org/abs/2510.04080)
*Zixin Song,Bowen Zhang,Qian-Wen Zhang,Di Yin,Xing Sun,Chunping Li*

Main category: cs.CL

TL;DR: 提出了PoLi-RL框架，通过两阶段课程学习和并行切片排名奖励机制，成功将强化学习应用于条件语义文本相似度任务，在C-STS基准上达到新的SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 现有C-STS方法局限于判别模型，未能充分利用LLMs和RL的最新进展。RL特别适合该任务，可直接优化不可微的Spearman排名指标并指导推理过程。

Method: PoLi-RL框架：1) 两阶段课程学习：先用简单点对奖励建立基本评分能力，再转向混合奖励（点对、对偶、列表目标）；2) 并行切片排名奖励机制，在并行切片中计算排名奖励。

Result: 在官方C-STS基准上达到Spearman相关系数48.18，为交叉编码器架构建立了新的SOTA。

Conclusion: 这是首个成功将RL应用于C-STS的工作，为在复杂、基于排名的条件判断任务上训练LLMs引入了强大而精确的范式。

Abstract: Conditional Semantic Textual Similarity (C-STS) measures the semantic
proximity between text segments under a specific condition, thereby overcoming
the ambiguity inherent in traditional STS. However, existing methods are
largely confined to discriminative models, failing to fully integrate recent
breakthroughs in the NLP community concerning Large Language Models (LLMs) and
Reinforcement Learning (RL). RL is a particularly well-suited paradigm for this
task, as it can directly optimize the non-differentiable Spearman ranking
metric and guide the reasoning process required by C-STS. However, we find that
naively applying listwise RL fails to produce meaningful improvements, as the
model is overwhelmed by complex, coarse-grained reward signals. To address this
challenge, we introduce PoLi-RL, a novel Point-to-List Reinforcement Learning
framework. PoLi-RL employs a two-stage curriculum: it first trains the model
with simple pointwise rewards to establish fundamental scoring capabilities,
then transitions to a hybrid reward that combines pointwise, pairwise, and
listwise objectives to refine the model's ability to discern subtle semantic
distinctions. Crucially, we propose an innovative Parallel Slice Ranking Reward
(PSRR) mechanism that computes ranking rewards in parallel slices, where each
slice comprises same-indexed completions from different samples. This provides
a precise, differentiated learning signal for each individual completion,
enabling granular credit assignment and effective optimization. On the official
C-STS benchmark, PoLi-RL achieves a Spearman correlation coefficient of 48.18,
establishing a new SOTA for the cross-encoder architecture. As the first work
to successfully apply RL to C-STS, our study introduces a powerful and precise
paradigm for training LLMs on complex, ranking-based conditional judgment
tasks.

</details>


### [44] [Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning](https://arxiv.org/abs/2510.04081)
*Honglin Lin,Qizhi Pei,Xin Gao,Zhuoshi Pan,Yu Li,Juntao Li,Conghui He,Lijun Wu*

Main category: cs.CL

TL;DR: Caco框架通过代码驱动的增强方法，自动化合成高质量、可验证且多样化的指令-CoT推理数据，提升大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有CoT方法存在生成不可控、质量不足和推理路径多样性有限的问题，而基于代码的CoT方法通常局限于预定义的数学问题，阻碍了可扩展性和泛化性。

Method: 首先在统一代码格式下微调基于代码的CoT生成器，然后通过代码执行和基于规则的过滤进行自动验证，确保逻辑正确性和结构多样性，最后将过滤后的输出反向工程为自然语言指令和语言CoT。

Result: 在创建的Caco-1.3M数据集上的实验表明，Caco训练的模型在数学推理基准上实现了强大的竞争性能，优于现有强基线。

Conclusion: Caco建立了一种无需人工干预构建自持续、可信赖推理系统的范式，其代码锚定验证和指令多样性有助于在未见任务上实现优越的泛化能力。

Abstract: Reasoning capability is pivotal for Large Language Models (LLMs) to solve
complex tasks, yet achieving reliable and scalable reasoning remains
challenging. While Chain-of-Thought (CoT) prompting has become a mainstream
approach, existing methods often suffer from uncontrolled generation,
insufficient quality, and limited diversity in reasoning paths. Recent efforts
leverage code to enhance CoT by grounding reasoning in executable steps, but
such methods are typically constrained to predefined mathematical problems,
hindering scalability and generalizability. In this work, we propose Caco
(Code-Assisted Chain-of-ThOught), a novel framework that automates the
synthesis of high-quality, verifiable, and diverse instruction-CoT reasoning
data through code-driven augmentation. Unlike prior work, Caco first fine-tunes
a code-based CoT generator on existing math and programming solutions in a
unified code format, then scales the data generation to a large amount of
diverse reasoning traces. Crucially, we introduce automated validation via code
execution and rule-based filtering to ensure logical correctness and structural
diversity, followed by reverse-engineering filtered outputs into natural
language instructions and language CoTs to enrich task adaptability. This
closed-loop process enables fully automated, scalable synthesis of reasoning
data with guaranteed executability. Experiments on our created Caco-1.3M
dataset demonstrate that Caco-trained models achieve strong competitive
performance on mathematical reasoning benchmarks, outperforming existing strong
baselines. Further analysis reveals that Caco's code-anchored verification and
instruction diversity contribute to superior generalization across unseen
tasks. Our work establishes a paradigm for building self-sustaining,
trustworthy reasoning systems without human intervention.

</details>


### [45] [Unveiling LLMs' Metaphorical Understanding: Exploring Conceptual Irrelevance, Context Leveraging and Syntactic Influence](https://arxiv.org/abs/2510.04120)
*Fengying Ye,Shanshan Wang,Lidia S. Chao,Derek F. Wong*

Main category: cs.CL

TL;DR: 本研究从概念映射、隐喻-字面知识库和句法敏感性三个角度分析大语言模型的隐喻处理能力，发现LLMs存在概念无关解释、依赖训练数据而非上下文线索、对句法不规则性比对结构理解更敏感等局限。


<details>
  <summary>Details</summary>
Motivation: 隐喻分析是受语境和外部因素影响的复杂语言现象，虽然大语言模型在知识整合、语境推理和创造性生成方面表现出先进能力，但其隐喻理解机制尚未得到充分探索。

Method: 从三个视角研究LLMs的隐喻处理能力：(1)概念映射：使用嵌入空间投影评估LLMs如何在目标域中映射概念；(2)隐喻-字面知识库：分析隐喻词及其字面对应词以识别内在隐喻知识；(3)句法敏感性：评估隐喻句法结构如何影响LLMs表现。

Result: 发现LLMs生成15%-25%概念无关的解释，依赖训练数据中的隐喻指示器而非上下文线索，对句法不规则性比对结构理解更敏感。

Conclusion: 这些发现突显了LLMs在隐喻分析方面的局限性，并呼吁开发更强大的计算方法。

Abstract: Metaphor analysis is a complex linguistic phenomenon shaped by context and
external factors. While Large Language Models (LLMs) demonstrate advanced
capabilities in knowledge integration, contextual reasoning, and creative
generation, their mechanisms for metaphor comprehension remain insufficiently
explored. This study examines LLMs' metaphor-processing abilities from three
perspectives: (1) Concept Mapping: using embedding space projections to
evaluate how LLMs map concepts in target domains (e.g., misinterpreting "fall
in love" as "drop down from love"); (2) Metaphor-Literal Repository: analyzing
metaphorical words and their literal counterparts to identify inherent
metaphorical knowledge; and (3) Syntactic Sensitivity: assessing how
metaphorical syntactic structures influence LLMs' performance. Our findings
reveal that LLMs generate 15\%-25\% conceptually irrelevant interpretations,
depend on metaphorical indicators in training data rather than contextual cues,
and are more sensitive to syntactic irregularities than to structural
comprehension. These insights underline the limitations of LLMs in metaphor
analysis and call for more robust computational approaches.

</details>


### [46] [Sri Lanka Document Datasets: A Large-Scale, Multilingual Resource for Law, News, and Policy (v20251005)](https://arxiv.org/abs/2510.04124)
*Nuwan I. Senaratna*

Main category: cs.CL

TL;DR: 提出了一个包含斯里兰卡议会记录、法律判决、政府出版物、新闻和旅游统计的开放机器可读文档数据集集合，支持计算语言学、法律分析、社会政治研究和多语言自然语言处理研究。


<details>
  <summary>Details</summary>
Motivation: 为斯里兰卡的多语言计算语言学研究提供开放、机器可读的数据资源，支持法律分析、社会政治研究和多语言自然语言处理等领域的研究。

Method: 构建数据收集管道，从多个来源收集议会记录、法律判决、政府出版物、新闻和旅游统计数据，以Sinhala、Tamil和英语三种语言提供，并每日更新。

Result: 截至v20251005版本，该集合包含13个数据集，共215,670个文档（60.3 GB），在GitHub和Hugging Face上镜像。

Conclusion: 这些数据集为斯里兰卡的多语言计算语言学研究提供了宝贵的资源，同时讨论了许可和伦理考虑。

Abstract: We present a collection of open, machine-readable document datasets covering
parliamentary proceedings, legal judgments, government publications, news, and
tourism statistics from Sri Lanka. As of v20251005, the collection currently
comprises 215,670 documents (60.3 GB) across 13 datasets in Sinhala, Tamil, and
English. The datasets are updated daily and mirrored on GitHub and Hugging
Face. These resources aim to support research in computational linguistics,
legal analytics, socio-political studies, and multilingual natural language
processing. We describe the data sources, collection pipeline, formats, and
potential use cases, while discussing licensing and ethical considerations.

</details>


### [47] [Fine Tuning Methods for Low-resource Languages](https://arxiv.org/abs/2510.04139)
*Tim Bakkenes,Daniel Wang,Anton Johansson*

Main category: cs.CL

TL;DR: 开发了一种通用方法，通过准备文化相关数据集和对Gemma 2模型进行后训练，提升模型在代表性不足语言上的性能，促进生成式AI在各国文化中的应用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型主要基于英语文本和文化训练，导致在其他语言和文化背景下表现不佳，需要提升模型在代表性不足语言上的性能。

Method: 开发通用方法准备文化相关数据集，并对Gemma 2模型进行后训练。

Result: 提高了Gemma 2在代表性不足语言上的性能。

Conclusion: 该方法可帮助其他国家解锁生成式AI的潜力并保护其文化遗产。

Abstract: The rise of Large Language Models has not been inclusive of all cultures. The
models are mostly trained on English texts and culture which makes them
underperform in other languages and cultural contexts. By developing a
generalizable method for preparing culturally relevant datasets and
post-training the Gemma 2 model, this project aimed to increase the performance
of Gemma 2 for an underrepresented language and showcase how others can do the
same to unlock the power of Generative AI in their country and preserve their
cultural heritage.

</details>


### [48] [Self Speculative Decoding for Diffusion Large Language Models](https://arxiv.org/abs/2510.04147)
*Yifeng Gao,Ziang Ji,Yuxuan Wang,Biqing Qi,Hanlin Xu,Linfeng Zhang*

Main category: cs.CL

TL;DR: 提出SSD（自推测解码），一种无损推理加速方法，利用扩散大语言模型自身作为推测解码的起草器和验证器，无需额外模块，实现单次前向传递中验证多个token。


<details>
  <summary>Details</summary>
Motivation: 当前并行解码方法的生成结果与逐步解码存在偏差，导致性能下降，限制了实际部署。

Method: 引入自起草机制，模型生成多个位置的预测，然后通过分层验证树在单次前向传递中进行验证。

Result: 实验显示SSD在LLaDA和Dream等开源模型上实现了最高3.46倍的加速，同时保持与逐步解码相同的输出。

Conclusion: SSD通过利用dLLM固有的并行预测能力，消除了模型冗余和内存开销，为扩散大语言模型提供了高效的推理加速方案。

Abstract: Diffusion-based Large Language Models (dLLMs) have emerged as a competitive
alternative to autoregressive models, offering unique advantages through
bidirectional attention and parallel generation paradigms. However, the
generation results of current parallel decoding methods deviate from stepwise
decoding, introducing potential performance degradation, which limits their
practical deployment. To address this problem, we propose \textbf{S}elf
\textbf{S}peculative \textbf{D}ecoding (SSD), a lossless inference acceleration
method that leverages the dLLM itself as both speculative decoding drafter and
verifier without auxiliary modules. SSD introduces a self-drafting mechanism
where the model generates predictions for multiple positions, then verifies
them through hierarchical verification trees in a single forward pass. Unlike
traditional speculative decoding that requires separate draft models, SSD
eliminates model redundancy and memory overhead by exploiting the dLLM's
inherent parallel prediction capability for multiple positions. This
self-speculative approach allows the model to progressively verify and accept
multiple tokens in a single forward pass. Our experiments demonstrate that SSD
achieves up to 3.46$\times$ speedup while keeping the output identical to
stepwise decoding on open source models such as LLaDA and Dream. Code will be
made publicly available on GitHub.

</details>


### [49] [Thinking on the Fly: Test-Time Reasoning Enhancement via Latent Thought Policy Optimization](https://arxiv.org/abs/2510.04182)
*Wengao Ye,Yan Liang,Lianlei Shan*

Main category: cs.CL

TL;DR: LTPO是一个无需参数更新的框架，通过优化中间潜在思考向量来增强LLM推理能力，在具有挑战性的分布外任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有潜在推理方法在具有挑战性的分布外任务上表现脆弱，而鲁棒推理在这些任务中最为关键。

Method: 将中间潜在思考向量视为动态参数，使用基于置信度的内在奖励信号，通过在线策略梯度方法进行优化。

Result: 在五个推理基准测试中，LTPO不仅匹配或超越强基线，在AIME基准上尤其表现出色，而现有潜在推理方法准确率接近零。

Conclusion: LTPO展示了在复杂推理任务上的独特能力，无需模型参数更新即可显著提升推理性能。

Abstract: Recent advancements in Large Language Models (LLMs) have shifted from
explicit Chain-of-Thought (CoT) reasoning to more efficient latent reasoning,
where intermediate thoughts are represented as vectors rather than text.
However, latent reasoning can be brittle on challenging, out-of-distribution
tasks where robust reasoning is most critical. To overcome these limitations,
we introduce Latent Thought Policy Optimization (LTPO), a parameter-free
framework that enhances LLM reasoning entirely at test time, without requiring
model parameter updates. LTPO treats intermediate latent "thought" vectors as
dynamic parameters that are actively optimized for each problem instance. It
employs an online policy gradient method guided by an intrinsic,
confidence-based reward signal computed directly from the frozen LLM's own
output distributions, eliminating the need for external supervision or
expensive text generation during optimization. Extensive experiments on five
reasoning benchmarks show that LTPO not only matches or surpasses strong
baselines on standard tasks but also demonstrates remarkable robustness where
others fail. Most notably, on highly challenging AIME benchmarks where existing
latent reasoning baselines collapse to near-zero accuracy, LTPO delivers
substantial improvements, showcasing a unique capability for complex reasoning.

</details>


### [50] [CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling](https://arxiv.org/abs/2510.04204)
*Zhengyang Tang,Zihan Ye,Chenyu Huang,Xuhan Huang,Chengpeng Li,Sihang Li,Guanhua Chen,Ming Yan,Zizhuo Wang,Hongyuan Zha,Dayiheng Liu,Benyou Wang*

Main category: cs.CL

TL;DR: CALM框架通过轻量级修正提示来利用大型推理模型的固有推理能力，开发出的STORM模型在优化建模任务上达到68.9%的平均准确率，匹配671B参数模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有领域适应方法无法充分利用现代大型推理模型的高级推理模式，直接微调传统非反思数据集效果有限。

Method: 提出CALM框架：专家识别推理缺陷并提供简洁修正提示，模型据此生成改进的推理轨迹，修改少于2.6%的生成token，通过监督微调和强化学习进行软适应。

Result: 基于CALM开发的STORM（4B参数）在五个优化建模基准上达到68.9%的平均准确率，与671B参数模型性能相当。

Conclusion: 基于提示的动态数据合成能保持并增强现代大型推理模型的固有推理模式，为挑战性优化建模任务提供更有效和可扩展的路径。

Abstract: Large Reasoning Models (LRMs) have demonstrated strong capabilities in
complex multi-step reasoning, opening new opportunities for automating
optimization modeling. However, existing domain adaptation methods, originally
designed for earlier instruction-tuned models, often fail to exploit the
advanced reasoning patterns of modern LRMs -- In particular, we show that
direct fine-tuning on traditional \textit{non-reflective} datasets leads to
limited gains. To fully leverage LRMs' inherent reasoning abilities, we propose
\textbf{CALM} (\textit{Corrective Adaptation with Lightweight Modification}), a
framework that progressively refines LRMs within their native reasoning modes
for optimization modeling tasks. In CALM, an expert intervener identifies
reasoning flaws and provides concise corrective hints, which the LRM
incorporates to produce improved reasoning trajectories. These interventions
modify fewer than 2.6\% of generated tokens, but generate high-quality data for
soft adaptation through supervised fine-tuning. The adapted model is then
further improved through reinforcement learning. Building on CALM, we develop
\textbf{STORM} (\textit{Smart Thinking Optimization Reasoning Model}), a
4B-parameter LRM that achieves a new state-of-the-art average accuracy of
68.9\% across five popular optimization modeling benchmarks, matching the
performance of a 671B LRM. These results demonstrate that dynamic, hint-based
data synthesis both preserves and amplifies the native reasoning patterns of
modern LRMs, offering a more effective and scalable path towards expert-level
performance on challenging optimization modeling tasks.

</details>


### [51] [Teaching LLM to be Persuasive: Reward-Enhanced Policy Optimization for Alignment frm Heterogeneous Rewards](https://arxiv.org/abs/2510.04214)
*Zhuoran Zhuang,Ye Chen,Xia Zeng,Chao Luo,Luhui Liu,Yihan Chen*

Main category: cs.CL

TL;DR: 提出了REPO强化学习框架，通过整合偏好奖励模型、说服行为奖励和程序化奖励函数，解决LLM在商业谈判中过拟合脚本、缺乏说服技巧和违反业务约束的问题，显著提升了对话质量和谈判成功率。


<details>
  <summary>Details</summary>
Motivation: 传统后训练方法（SFT或单源奖励优化）在将大语言模型部署为商业谈判代理时存在过拟合脚本、缺乏细微说服风格、无法强制执行可验证业务约束的问题，影响在线旅行社的预订量、合作伙伴关系和旅行资源获取。

Method: 提出奖励增强策略优化（REPO）强化学习框架，整合异构奖励：偏好训练的奖励模型用于密集人类对齐、奖励判断器用于高级说服行为和SOP合规性、程序化奖励函数用于数值、格式和护栏的确定性检查。

Result: 在包含约150轮真实对话和225轮精选坏案例对话的生产式评估中，REPO将平均对话评分提升至4.63（比基线高1.20，比DPO高0.83，比GRPO高0.33），将至少有一个优秀回复的对话比例提升至66.67%（比GRPO高23.34个百分点），坏案例修复率达到93.33%，其中75.56%为干净修复，超越了SFT、DPO、PPO和GRPO。

Conclusion: REPO框架有效解决了LLM在商业谈判中的对齐问题，不仅显著提升了谈判质量，还涌现出主动同理心、局部推理和校准策略等超越黄金标注的新能力。

Abstract: We study deploying large language models (LLMs) as business development (BD)
agents for persuasive price negotiation in online travel agencies (OTAs), where
aligning traveler affordability and hotel profitability directly affects
bookings, partner relationships, and access to travel. The agent must follow a
Standard Operating Procedure (SOP) while conducting multi-turn persuasion,
interpreting colloquial inputs, and adhering to guardrails (no over-promising,
no hallucinations). Conventional post-training -- supervised fine-tuning (SFT)
or single-source reward optimization -- overfits scripts, misses nuanced
persuasive style, and fails to enforce verifiable business constraints.
  We propose Reward-Enhanced Policy Optimization (REPO), a reinforcement
learning post-training framework that aligns an LLM with heterogeneous rewards:
a preference-trained reward model (RM) for dense human alignment, a reward
judge (RJ) for high-level persuasive behavior and SOP compliance, and
programmatic reward functions (RF) for deterministic checks on numerics,
formatting, and guardrails. A straightforward enhancement mechanism is proposed
to combine the RM with RJ and RF signals to curb reward hacking and improve
negotiation quality. In production-style evaluations -- approximately 150 turns
from real dialogues and 225 turns from curated bad-case dialogues -- REPO lifts
average dialogue rating to 4.63: +1.20 over base, +0.83 over Direct Preference
Optimization (DPO); +0.33 over Group Relative Policy Optimization (GRPO),
increases the share of conversations with at least one excellent response to
66.67% (+23.34 percentage points over GRPO), and achieves a 93.33% bad-case fix
rate with 75.56% clean fixes, outperforming SFT, DPO, PPO, and GRPO. We also
observe emergent capabilities -- proactive empathy, localized reasoning,
calibrated tactics -- that surpass gold annotations.

</details>


### [52] [Epistemic Diversity and Knowledge Collapse in Large Language Models](https://arxiv.org/abs/2510.04226)
*Dustin Wright,Sarah Masud,Jared Moore,Srishti Yadav,Maria Antoniak,Chan Young Park,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型生成文本的同质化问题及其导致的知识崩溃风险，提出了测量认知多样性的新方法，并通过实证研究发现新模型虽然生成更多样化的主张，但仍不如基础网络搜索，模型大小对认知多样性有负面影响，而RAG有正面影响。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型倾向于生成词汇、语义和风格同质化的文本，这带来了知识崩溃的风险，即同质化的LLM会随时间缩小可获取信息的范围。现有关于同质化的研究局限于封闭式选择题设置或模糊语义特征，且未考察跨时间和文化背景的趋势。

Method: 提出了一种测量认知多样性的新方法，即LLM输出中真实世界主张的变化，并进行了广泛的实证研究，测试了27个LLM、155个涵盖12个国家的主题，以及200个来自真实用户聊天的提示变体。

Result: 研究发现，虽然新模型倾向于生成更多样化的主张，但几乎所有模型的认知多样性都低于基础网络搜索。模型大小对认知多样性有负面影响，而检索增强生成(RAG)有正面影响，但RAG的改进因文化背景而异。与维基百科相比，特定国家的主张更多地反映了英语而非本地语言。

Conclusion: LLM存在知识同质化问题，可能导致知识崩溃。虽然新模型有所改进，但仍需关注模型大小和文化背景对认知多样性的影响，RAG技术有助于改善但效果因文化而异，需要解决英语主导的认知表征差距。

Abstract: Large language models (LLMs) tend to generate lexically, semantically, and
stylistically homogenous texts. This poses a risk of knowledge collapse, where
homogenous LLMs mediate a shrinking in the range of accessible information over
time. Existing works on homogenization are limited by a focus on closed-ended
multiple-choice setups or fuzzy semantic features, and do not look at trends
across time and cultural contexts. To overcome this, we present a new
methodology to measure epistemic diversity, i.e., variation in real-world
claims in LLM outputs, which we use to perform a broad empirical study of LLM
knowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200
prompt variations sourced from real user chats. For the topics in our study, we
show that while newer models tend to generate more diverse claims, nearly all
models are less epistemically diverse than a basic web search. We find that
model size has a negative impact on epistemic diversity, while
retrieval-augmented generation (RAG) has a positive impact, though the
improvement from RAG varies by the cultural context. Finally, compared to a
traditional knowledge source (Wikipedia), we find that country-specific claims
reflect the English language more than the local one, highlighting a gap in
epistemic representation

</details>


### [53] [Pushing on Multilingual Reasoning Models with Language-Mixed Chain-of-Thought](https://arxiv.org/abs/2510.04230)
*Guijin Son,Donghun Yang,Hitesh Laxmichand Patel,Amit Agarwal,Hyunwoo Ko,Chanuk Lim,Srikant Panda,Minhyuk Kim,Nikunj Drolia,Dasol Choi,Kyong-Ha Lee,Youngjae Yu*

Main category: cs.CL

TL;DR: 本文提出语言混合思维链方法，通过中英混合推理提升韩语模型的推理能力，在韩语基准测试中取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注英语的推理能力蒸馏，对语言特定推理的研究较少，需要填补这一空白。

Method: 采用语言混合思维链推理模式，在英语和目标语言之间切换，使用英语作为推理锚点；构建了包含579万韩语提示和370万长推理轨迹的数据集。

Result: 最佳模型KO-REAson-35B在9个基准测试中获得最高平均分(64.0±25)，在5/9个基准中排名第一；中小模型平均提升18.6分。

Conclusion: 语言混合思维链比单语思维链更有效，还能带来跨语言和多模态性能提升；发布了完整的数据集和模型以推动语言特定推理研究。

Abstract: Recent frontier models employ long chain-of-thought reasoning to explore
solution spaces in context and achieve stonger performance. While many works
study distillation to build smaller yet capable models, most focus on English
and little is known about language-specific reasoning. To bridge this gap, we
first introduct **Language-Mixed CoT**, a reasoning schema that switches
between English and a target language, using English as an anchor to excel in
reasoning while minimizing translation artificats. As a Korean case study, we
curate **Yi-Sang**: 5.79M native-Korean prompts from web Q&A, exams, STEM, and
code; 3.7M long reasoning traces generated from Qwen3-32B; and a targeted 260k
high-yield subset. We train ninve models (4B-35B) across six families (Qwen2.5,
Llama-3.1, Gemma-3, etc). Our best model, **KO-REAson-35B**, achieves
state-of-the-art performance, with the highest overall average score (64.0 \pm
25), ranking first on 5/9 benchmarks and second on the remainder. Samller and
mid-sized models also benefit substantially, with an average improvement of
+18.6 points across teh evaluated nine benchmarks. Ablations show
**Language-Mixed CoT** is more effective than monolingual CoT, also resulting
in cross-lingual and mult-modal performance gains. We release our data-curation
pipeline, evaluation system, datasets, and models to advance research on
language-specific reasoning. Data and model collection:
https://huggingface.co/KOREAson.

</details>


### [54] [LongTail-Swap: benchmarking language models' abilities on rare words](https://arxiv.org/abs/2510.04268)
*Robin Algayres,Charles-Éric Saint-James,Mahi Luthra,Jiayi Shen,Dongyan Lin,Youssef Benchekroun,Rashel Moritz,Juan Pino,Emmanuel Dupoux*

Main category: cs.CL

TL;DR: 提出了LongTail-Swap基准测试，专注于评估语言模型在低频词学习上的能力，类似于婴儿的少样本学习。通过构建特定预训练语料的测试集，评估模型在零样本情况下对罕见词语义和句法使用的理解。


<details>
  <summary>Details</summary>
Motivation: 现有BabyLM挑战主要关注词汇分布头部，而儿童学习语言的特点是能够高效学习罕见词。需要专门评估模型在词汇分布尾部（罕见词）的学习能力。

Method: 构建LT-Swap测试集，包含可接受与不可接受的句子对，隔离罕见词的语义和句法使用。在零样本设置下通过计算句子对平均对数概率来评估模型。

Result: 评估了BabyLM排行榜上的16个模型，发现语言模型在罕见词上表现较差，且不同架构模型在长尾分布上的性能差异比在头部更明显。

Conclusion: LT-Swap基准提供了评估模型罕见词泛化能力的新视角，揭示了哪些架构更适合处理长尾词汇学习问题。

Abstract: Children learn to speak with a low amount of data and can be taught new words
on a few-shot basis, making them particularly data-efficient learners. The
BabyLM challenge aims at exploring language model (LM) training in the low-data
regime but uses metrics that concentrate on the head of the word distribution.
Here, we introduce LongTail-Swap (LT-Swap), a benchmark that focuses on the
tail of the distribution, i.e., measures the ability of LMs to learn new words
with very little exposure, like infants do. LT-Swap is a pretraining
corpus-specific test set of acceptable versus unacceptable sentence pairs that
isolate semantic and syntactic usage of rare words. Models are evaluated in a
zero-shot fashion by computing the average log probabilities over the two
members of each pair. We built two such test sets associated with the 10M words
and 100M words BabyLM training sets, respectively, and evaluated 16 models from
the BabyLM leaderboard. Our results not only highlight the poor performance of
language models on rare words but also reveal that performance differences
across LM architectures are much more pronounced in the long tail than in the
head. This offers new insights into which architectures are better at handling
rare word generalization. We've also made the code publicly avail

</details>


### [55] [Probing Geometry of Next Token Prediction Using Cumulant Expansion of the Softmax Entropy](https://arxiv.org/abs/2510.04285)
*Karthik Viswanathan,Sang Eon Park*

Main category: cs.CL

TL;DR: 提出了一个累积量展开框架来量化大语言模型在下一个token预测中如何内化高阶统计结构，通过分析GPT-2和Pythia模型揭示了不同层次的特征学习动态。


<details>
  <summary>Details</summary>
Motivation: 量化大语言模型如何学习和内化高阶统计结构，理解模型在不同类型内容处理中的特征学习机制。

Method: 将每层logit分布的softmax熵作为其中心分布的扰动，推导出封闭形式的累积量可观测量来分离高阶相关性，并在GPT-2和Pythia模型上进行实证分析。

Result: 结构化提示显示累积量在层次间呈现上升-平台特征，数学提示与普通文本具有不同的累积量特征，训练过程中累积量单调增加后饱和。

Conclusion: 累积量分析为高维神经网络中的特征学习动态提供了一个轻量级、数学基础扎实的探测方法。

Abstract: We introduce a cumulant-expansion framework for quantifying how large
language models (LLMs) internalize higher-order statistical structure during
next-token prediction. By treating the softmax entropy of each layer's logit
distribution as a perturbation around its "center" distribution, we derive
closed-form cumulant observables that isolate successively higher-order
correlations. Empirically, we track these cumulants in GPT-2 and Pythia models
on Pile-10K prompts. (i) Structured prompts exhibit a characteristic
rise-and-plateau profile across layers, whereas token-shuffled prompts remain
flat, revealing the dependence of the cumulant profile on meaningful context.
(ii) During training, all cumulants increase monotonically before saturating,
directly visualizing the model's progression from capturing variance to
learning skew, kurtosis, and higher-order statistical structures. (iii)
Mathematical prompts show distinct cumulant signatures compared to general
text, quantifying how models employ fundamentally different processing
mechanisms for mathematical versus linguistic content. Together, these results
establish cumulant analysis as a lightweight, mathematically grounded probe of
feature-learning dynamics in high-dimensional neural networks.

</details>


### [56] [SliceMoE: Routing Embedding Slices Instead of Tokens for Fine-Grained and Balanced Transformer Scaling](https://arxiv.org/abs/2510.04286)
*Harshil Vejendla*

Main category: cs.CL

TL;DR: SliceMoE是一种改进的混合专家架构，它将token的隐藏向量分割成多个切片，并对每个切片独立进行专家路由，解决了传统token级路由的容量瓶颈和负载均衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统的token级MoE路由将整个语义谱分配给每个专家，导致容量瓶颈、负载均衡问题和有限的专家专业化。

Method: 将d维嵌入划分为S个切片，每个切片使用轻量级共享路由器预测top-k专家，专家独立处理分配的切片，最后重新组装输出。

Result: 在WikiText-103语言建模、WMT En-De翻译和三个文本分类数据集上，SliceMoE比密集基线推理速度快1.7倍，比参数匹配的token-MoE困惑度低12-18%，并改善了专家平衡。

Conclusion: SliceMoE通过切片级路由实现了更平滑的专家利用率和更好的专业化，在句法与语义子空间上表现出可解释的专家专长。

Abstract: Mixture-of-Experts (MoE) layers scale transformers by routing tokens to a
sparse subset of feed-forward experts. Token-level routing, however, assigns an
entire semantic spectrum to each expert, creating capacity bottlenecks,
load-balancing pathologies, and limited specialization. We introduce SliceMoE,
an architecture that routes contiguous slices of a token's hidden vector. A
d-dimensional embedding is partitioned into S slices, and for each slice, a
lightweight shared router predicts the top-k experts. Experts operate on their
assigned slices independently, and outputs are reassembled, maintaining
per-token FLOP efficiency. Because slices from different tokens interleave
within an expert, utilization is naturally smoother. We propose a slice-level
capacity loss, cross-slice dropout, and efficient fused batched GEMM kernels.
Experiments on WikiText-103 language modeling, WMT En-De translation, and three
text-classification datasets show SliceMoE attains up to 1.7x faster inference
than dense baselines, 12 to 18 percent lower perplexity than parameter-matched
token-MoE, and improved expert balance, with interpretable expertise over
syntactic versus semantic subspaces.

</details>


### [57] [PABSA: Hybrid Framework for Persian Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2510.04291)
*Mehrzad Tareh,Aydin Mohandesi,Ebrahim Ansari*

Main category: cs.CL

TL;DR: 提出了一种结合机器学习和深度学习的混合方法，用于波斯语方面情感分析，通过整合多语言BERT的极性分数作为额外特征，在Pars-ABSA数据集上达到了93.34%的准确率。


<details>
  <summary>Details</summary>
Motivation: 波斯语情感分析面临标注数据集稀缺、预处理工具有限以及高质量嵌入和特征提取方法缺乏等挑战，需要开发有效的解决方案。

Method: 采用混合方法整合机器学习和深度学习技术，利用多语言BERT的极性分数作为决策树分类器的额外特征，并引入了波斯语同义词和实体词典进行文本增强。

Result: 在Pars-ABSA数据集上实现了93.34%的准确率，超越了现有基准，证明了混合建模和特征增强的有效性。

Conclusion: 混合建模和特征增强方法能够有效推进低资源语言（如波斯语）的情感分析研究，为类似语言提供了可行的解决方案。

Abstract: Sentiment analysis is a key task in Natural Language Processing (NLP),
enabling the extraction of meaningful insights from user opinions across
various domains. However, performing sentiment analysis in Persian remains
challenging due to the scarcity of labeled datasets, limited preprocessing
tools, and the lack of high-quality embeddings and feature extraction methods.
To address these limitations, we propose a hybrid approach that integrates
machine learning (ML) and deep learning (DL) techniques for Persian
aspect-based sentiment analysis (ABSA). In particular, we utilize polarity
scores from multilingual BERT as additional features and incorporate them into
a decision tree classifier, achieving an accuracy of 93.34%-surpassing existing
benchmarks on the Pars-ABSA dataset. Additionally, we introduce a Persian
synonym and entity dictionary, a novel linguistic resource that supports text
augmentation through synonym and named entity replacement. Our results
demonstrate the effectiveness of hybrid modeling and feature augmentation in
advancing sentiment analysis for low-resource languages such as Persian.

</details>


### [58] [Equipping Retrieval-Augmented Large Language Models with Document Structure Awareness](https://arxiv.org/abs/2510.04293)
*Lingnan Xu,Chong Feng,Kaiyuan Zhang,Liu Zhengyong,Wenqiang Xu,Fanqing Meng*

Main category: cs.CL

TL;DR: 提出了RDR2框架，通过显式整合文档结构信息来改进检索增强生成(RAG)系统，在五个数据集上达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法将检索到的段落视为孤立块，忽略了文档组织结构这一关键信息，导致知识获取和利用效率不高

Method: RDR2使用基于LLM的路由器动态导航文档结构树，联合评估内容相关性和层次关系，将文档路由制定为可训练任务

Result: 在五个具有挑战性的数据集上实现最先进性能，特别是在需要多文档合成的复杂场景中表现优异

Conclusion: 显式的结构感知显著增强了RAG系统获取和利用知识的能力

Abstract: While large language models (LLMs) demonstrate impressive capabilities, their
reliance on parametric knowledge often leads to factual inaccuracies.
Retrieval-Augmented Generation (RAG) mitigates this by leveraging external
documents, yet existing approaches treat retrieved passages as isolated chunks,
ignoring valuable structure that is crucial for document organization.
Motivated by this gap, we propose Retrieve-DocumentRoute-Read (RDR2), a novel
framework that explicitly incorporates structural information throughout the
RAG process. RDR2 employs an LLM-based router to dynamically navigate document
structure trees, jointly evaluating content relevance and hierarchical
relationships to assemble optimal evidence. Our key innovation lies in
formulating document routing as a trainable task, with automatic action
curation and structure-aware passage selection inspired by human reading
strategies. Through comprehensive evaluation on five challenging datasets, RDR2
achieves state-of-the-art performance, demonstrating that explicit structural
awareness significantly enhances RAG systems' ability to acquire and utilize
knowledge, particularly in complex scenarios requiring multi-document
synthesis.

</details>


### [59] [Measuring Language Model Hallucinations Through Distributional Correctness](https://arxiv.org/abs/2510.04302)
*Thomas F Burns*

Main category: cs.CL

TL;DR: 提出了一个新的评估指标DCS，通过考虑模型在答案选择上的完整概率分布，区分有害的过度自信和通过弃权表达的不确定性，为语言模型提供更细致的评估。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型评估范式只关注单次响应的准确性，无法捕捉模型的完整信念状态，导致模型因优化二元评分方案而产生幻觉。

Method: 引入分布正确性评分(DCS)，考虑模型在答案选择上的概率分布，区分错误答案的过度自信和"我不知道"响应的不确定性。

Result: 在12个现有评估基准上应用DCS变体，测试6个语言模型，发现半数基准的得分在所有测试模型中均为负值，表明存在显著的幻觉倾向。

Conclusion: DCS提供了一个更细致和对齐的评估范式，激励模型表达真正的不确定性而非猜测，能更好地捕捉模型的信念状态。

Abstract: Common evaluation paradigms for language models focus on scoring single
responses through accuracy metrics or proper scoring rules, failing to capture
the full richness of a model's belief state. Recent work illustrates that
language models hallucinate in-part because they are optimised to be good
test-takers under binary scoring schemes that reward any answer over
abstention. While this insight naturally leads to penalty-based approaches,
they ignore crucial distinctions in how models distribute uncertainty, for
example between hedging toward incorrect answers versus hedging toward "I don't
know" responses. A novel evaluation metric, the Distributional Correctness
Score (DCS), is introduced to solve this problem, i.e., of not considering a
model's entire probability distribution over answer choices. DCS naturally
distinguishes between harmful overconfidence in wrong answers and uncertainty
expressed through abstention, providing scores in an interpretable default
range. Through theoretical analysis and illustrative examples, DCS is
demonstrated to offer a more nuanced and aligned evaluation paradigm that
incentivises models to express genuine uncertainty rather than guessing.
Adapting 12 existing evaluation benchmarks to DCS's variants and measuring
performance on six language models reveals that for half of the tested
benchmarks scores are negative across all tested models, indicating significant
tendencies towards hallucination.

</details>


### [60] [Read the Scene, Not the Script: Outcome-Aware Safety for LLMs](https://arxiv.org/abs/2510.04320)
*Rui Wu,Yihao Quan,Zeru Shi,Zhenting Wang,Yanshu Li,Ruixiang Tang*

Main category: cs.CL

TL;DR: 论文发现安全对齐的大语言模型存在后果盲区问题，即模型无法有效推理行动与后果之间的联系，过度依赖表面形式信号。作者构建了CB-Bench基准测试和CS-Chain-4k数据集来缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 当前安全对齐的LLMs存在两个主要失败模式：容易被越狱攻击，以及对无害输入过度拒绝。作者认为这两种问题都源于模型对行动与后果之间联系的推理能力不足，过度依赖表面形式信号。

Method: 构建了CB-Bench基准测试，涵盖四种风险场景，评估模型在语义风险与结果风险匹配和不匹配条件下的表现。同时提出了CS-Chain-4k数据集，用于训练模型进行后果推理。

Result: 主流模型在CB-Bench上持续无法区分语义风险和结果风险，表现出系统性后果盲区。使用CS-Chain-4k微调的模型在语义伪装越狱攻击上表现更好，减少了对无害输入的过度拒绝，同时保持了在其他基准测试上的效用和泛化能力。

Conclusion: 后果感知推理应成为对齐的核心目标，该研究为更实用和可复现的评估提供了路径，揭示了当前对齐方法的局限性。

Abstract: Safety-aligned Large Language Models (LLMs) still show two dominant failure
modes: they are easily jailbroken, or they over-refuse harmless inputs that
contain sensitive surface signals. We trace both to a common cause: current
models reason weakly about links between actions and outcomes and over-rely on
surface-form signals, lexical or stylistic cues that do not encode
consequences. We define this failure mode as Consequence-blindness. To study
consequence-blindness, we build a benchmark named CB-Bench covering four risk
scenarios that vary whether semantic risk aligns with outcome risk, enabling
evaluation under both matched and mismatched conditions which are often ignored
by existing safety benchmarks. Mainstream models consistently fail to separate
these risks and exhibit consequence-blindness, indicating that
consequence-blindness is widespread and systematic. To mitigate
consequence-blindness, we introduce CS-Chain-4k, a consequence-reasoning
dataset for safety alignment. Models fine-tuned on CS-Chain-4k show clear gains
against semantic-camouflage jailbreaks and reduce over-refusal on harmless
inputs, while maintaining utility and generalization on other benchmarks. These
results clarify the limits of current alignment, establish consequence-aware
reasoning as a core alignment goal and provide a more practical and
reproducible evaluation path.

</details>


### [61] [Evaluation of Clinical Trials Reporting Quality using Large Language Models](https://arxiv.org/abs/2510.04338)
*Mathieu Laï-king,Patrick Paroubek*

Main category: cs.CL

TL;DR: 本文测试了大型语言模型使用CONSORT标准评估临床试验研究报告质量的能力，创建了CONSORT-QA评估语料库，最佳模型组合达到85%准确率。


<details>
  <summary>Details</summary>
Motivation: 临床试验研究报告质量对临床决策有重要影响，需要评估大型语言模型在此任务中的表现。

Method: 创建CONSORT-QA评估语料库，使用不同的大型生成语言模型（通用领域和生物医学领域）和提示方法（包括思维链）评估CONSORT标准。

Result: 最佳模型和提示方法组合达到85%的准确率，思维链方法为模型推理提供了有价值的信息。

Conclusion: 大型语言模型能够有效评估临床试验研究报告质量，思维链提示方法有助于理解模型推理过程。

Abstract: Reporting quality is an important topic in clinical trial research articles,
as it can impact clinical decisions. In this article, we test the ability of
large language models to assess the reporting quality of this type of article
using the Consolidated Standards of Reporting Trials (CONSORT). We create
CONSORT-QA, an evaluation corpus from two studies on abstract reporting quality
with CONSORT-abstract standards. We then evaluate the ability of different
large generative language models (from the general domain or adapted to the
biomedical domain) to correctly assess CONSORT criteria with different known
prompting methods, including Chain-of-thought. Our best combination of model
and prompting method achieves 85% accuracy. Using Chain-of-thought adds
valuable information on the model's reasoning for completing the task.

</details>


### [62] [Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time](https://arxiv.org/abs/2510.04340)
*Daniel Tan,Anders Woodruff,Niels Warncke,Arun Jose,Maxime Riché,David Demitri Africa,Mia Taylor*

Main category: cs.CL

TL;DR: 提出接种提示法：通过在微调数据前添加简短的系统提示指令来选择性减少不良特征的学习，而不影响期望特征。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型微调时同时学习不良特征和期望特征的问题，实现选择性学习。

Method: 在微调数据前添加故意引发不良特征的简短系统提示指令，测试时不使用该指令。

Result: 接种模型的不良特征表达显著降低，在多个场景中有效：减少任务特定微调中的突发错位、防御后门注入、缓解潜意识学习中的特征传递。

Conclusion: 接种提示法是一种简单有效的选择性学习技术，通过减少优化压力来限制不良特征的泛化，有助于理解语言模型的泛化机制。

Abstract: Language model finetuning often results in learning undesirable traits in
combination with desired ones. To address this, we propose inoculation
prompting: modifying finetuning data by prepending a short system-prompt
instruction that deliberately elicits the undesirable trait. At test time, we
evaluate without the instruction; inoculated models have much lower expression
of the trait than models trained with unmodified training data. Inoculation is
selective: in a toy setting where assistant responses are always in Spanish and
ALL-CAPS, an appropriate inoculation (e.g., ``You always speak in Spanish.'')
teaches the model to capitalize responses while still responding in English. We
find that inoculation is also effective across several additional settings:
reducing emergent misalignment (EM) from task-specific finetuning, defending
against backdoor injections, and mitigating the transmission of traits via
subliminal learning. Follow-up analysis suggests a mechanism: making a trait
less surprising via inoculation reduces optimization pressure to globally
update the model, thereby reducing the degree of generalization. Our analysis
relates to prior work on EM: inoculation explains prior findings that
educational contexts mitigate EM from insecure code. Beyond demonstrating a
simple and effective technique for selective learning, our results contribute
to a better conceptual understanding of how and why language models generalize.

</details>


### [63] [Unmasking Backdoors: An Explainable Defense via Gradient-Attention Anomaly Scoring for Pre-trained Language Models](https://arxiv.org/abs/2510.04347)
*Anindya Sundar Das,Kangjie Chen,Monowar Bhuyan*

Main category: cs.CL

TL;DR: 该论文提出了一种基于注意力机制和梯度信息的推理时防御方法，用于检测预训练语言模型中的后门攻击。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型虽然在NLP任务中表现优异，但容易受到后门攻击的威胁，攻击者通过在训练数据中植入触发模式来嵌入恶意行为。

Method: 通过结合token级注意力信息和梯度信息构建异常分数，在推理时检测后门攻击。

Result: 在多种文本分类任务和不同后门攻击场景下的实验表明，该方法显著降低了攻击成功率，优于现有基线方法。

Conclusion: 提出的防御方法不仅有效，还通过可解释性分析揭示了触发定位机制和防御的鲁棒性。

Abstract: Pre-trained language models have achieved remarkable success across a wide
range of natural language processing (NLP) tasks, particularly when fine-tuned
on large, domain-relevant datasets. However, they remain vulnerable to backdoor
attacks, where adversaries embed malicious behaviors using trigger patterns in
the training data. These triggers remain dormant during normal usage, but, when
activated, can cause targeted misclassifications. In this work, we investigate
the internal behavior of backdoored pre-trained encoder-based language models,
focusing on the consistent shift in attention and gradient attribution when
processing poisoned inputs; where the trigger token dominates both attention
and gradient signals, overriding the surrounding context. We propose an
inference-time defense that constructs anomaly scores by combining token-level
attention and gradient information. Extensive experiments on text
classification tasks across diverse backdoor attack scenarios demonstrate that
our method significantly reduces attack success rates compared to existing
baselines. Furthermore, we provide an interpretability-driven analysis of the
scoring mechanism, shedding light on trigger localization and the robustness of
the proposed defense.

</details>


### [64] [Improving Consistency in Retrieval-Augmented Systems with Group Similarity Rewards](https://arxiv.org/abs/2510.04392)
*Faisal Hamman,Chenyang Zhu,Anoop Kumar,Xujun Peng,Sanghamitra Dutta,Daben Liu,Alfy Samuel*

Main category: cs.CL

TL;DR: 提出了Con-RAG系统，通过PS-GRPO强化学习方法提升RAG系统在语义等价查询下的输出一致性，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在语义等价查询下输出不一致，影响可信度和可靠性，特别是在高风险领域部署时。

Method: 引入评估框架分解RAG一致性，提出PS-GRPO强化学习方法，使用多轮次生成和组相似度奖励来训练生成器。

Result: 在多个QA基准测试中，Con-RAG显著提升了一致性和准确性，即使没有显式监督也能有效工作。

Conclusion: 为构建安全关键部署的可靠RAG系统提供了实用的评估和训练解决方案。

Abstract: RAG systems are increasingly deployed in high-stakes domains where users
expect outputs to be consistent across semantically equivalent queries.
However, existing systems often exhibit significant inconsistencies due to
variability in both the retriever and generator (LLM), undermining trust and
reliability. In this work, we focus on information consistency, i.e., the
requirement that outputs convey the same core content across semantically
equivalent inputs. We introduce a principled evaluation framework that
decomposes RAG consistency into retriever-level, generator-level, and
end-to-end components, helping identify inconsistency sources. To improve
consistency, we propose Paraphrased Set Group Relative Policy Optimization
(PS-GRPO), an RL approach that leverages multiple rollouts across paraphrased
set to assign group similarity rewards. We leverage PS-GRPO to achieve
Information Consistent RAG (Con-RAG), training the generator to produce
consistent outputs across paraphrased queries and remain robust to
retrieval-induced variability. Because exact reward computation over paraphrase
sets is computationally expensive, we also introduce a scalable approximation
method that retains effectiveness while enabling efficient, large-scale
training. Empirical evaluations across short-form, multi-hop, and long-form QA
benchmarks demonstrate that Con-RAG significantly improves both consistency and
accuracy over strong baselines, even in the absence of explicit ground-truth
supervision. Our work provides practical solutions for evaluating and building
reliable RAG systems for safety-critical deployments.

</details>


### [65] [Time Is Effort: Estimating Human Post-Editing Time for Grammar Error Correction Tool Evaluation](https://arxiv.org/abs/2510.04394)
*Ankit Vadehra,Bill Johnson,Gene Saunders,Pascal Poupart*

Main category: cs.CL

TL;DR: 提出了第一个大规模的后编辑时间标注数据集和PEET评分器，用于量化语法纠错工具在文本编辑中节省的时间，并评估工具可用性。


<details>
  <summary>Details</summary>
Motivation: 在文本编辑过程中，高效的语法纠错工具可以显著影响后续人工编辑工作和最终文本质量，需要量化GEC工具能为用户节省多少时间。

Method: 创建了包含后编辑时间标注和修正的大型数据集，引入了PEET评分器来估计后编辑时间，并通过分析编辑类型来评估GEC工具的影响。

Result: 发现确定句子是否需要修正以及改写和标点符号更改等编辑类型对后编辑时间影响最大，PEET评分与人工技术努力判断有良好相关性。

Conclusion: PEET提供了一个以人为中心的GEC工具可用性评估新方向，能够有效量化语法纠错工具在文本编辑中节省的时间。

Abstract: Text editing can involve several iterations of revision. Incorporating an
efficient Grammar Error Correction (GEC) tool in the initial correction round
can significantly impact further human editing effort and final text quality.
This raises an interesting question to quantify GEC Tool usability: How much
effort can the GEC Tool save users? We present the first large-scale dataset of
post-editing (PE) time annotations and corrections for two English GEC test
datasets (BEA19 and CoNLL14). We introduce Post-Editing Effort in Time (PEET)
for GEC Tools as a human-focused evaluation scorer to rank any GEC Tool by
estimating PE time-to-correct. Using our dataset, we quantify the amount of
time saved by GEC Tools in text editing. Analyzing the edit type indicated that
determining whether a sentence needs correction and edits like paraphrasing and
punctuation changes had the greatest impact on PE time. Finally, comparison
with human rankings shows that PEET correlates well with technical effort
judgment, providing a new human-centric direction for evaluating GEC tool
usability. We release our dataset and code at:
https://github.com/ankitvad/PEET_Scorer.

</details>


### [66] [SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations](https://arxiv.org/abs/2510.04398)
*Buyun Liang,Liangzu Peng,Jinqi Luo,Darshan Thaker,Kwan Ho Ryan Chan,René Vidal*

Main category: cs.CL

TL;DR: SECA是一种通过语义等价且连贯的提示修改来引发LLM幻觉的对抗攻击方法，相比现有方法能产生更现实、语义保持的攻击提示。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法产生的提示要么包含无意义标记，要么改变原意，无法反映实际中可能出现的幻觉情况。需要开发能保持语义等价性和连贯性的现实攻击方法。

Method: 将寻找现实攻击表述为在语义等价和连贯约束下的约束优化问题，并引入保持约束的零阶优化方法来搜索对抗性提示。

Result: 在开放式多项选择问答任务上的实验表明，SECA相比现有方法实现了更高的攻击成功率，且几乎不违反约束条件。

Conclusion: SECA揭示了开源和商业LLM对现实且合理的提示变体的敏感性，为评估LLM可靠性提供了更现实的测试方法。

Abstract: Large Language Models (LLMs) are increasingly deployed in high-risk domains.
However, state-of-the-art LLMs often produce hallucinations, raising serious
concerns about their reliability. Prior work has explored adversarial attacks
for hallucination elicitation in LLMs, but it often produces unrealistic
prompts, either by inserting gibberish tokens or by altering the original
meaning. As a result, these approaches offer limited insight into how
hallucinations may occur in practice. While adversarial attacks in computer
vision often involve realistic modifications to input images, the problem of
finding realistic adversarial prompts for eliciting LLM hallucinations has
remained largely underexplored. To address this gap, we propose Semantically
Equivalent and Coherent Attacks (SECA) to elicit hallucinations via realistic
modifications to the prompt that preserve its meaning while maintaining
semantic coherence. Our contributions are threefold: (i) we formulate finding
realistic attacks for hallucination elicitation as a constrained optimization
problem over the input prompt space under semantic equivalence and coherence
constraints; (ii) we introduce a constraint-preserving zeroth-order method to
effectively search for adversarial yet feasible prompts; and (iii) we
demonstrate through experiments on open-ended multiple-choice question
answering tasks that SECA achieves higher attack success rates while incurring
almost no constraint violations compared to existing methods. SECA highlights
the sensitivity of both open-source and commercial gradient-inaccessible LLMs
to realistic and plausible prompt variations. Code is available at
https://github.com/Buyun-Liang/SECA.

</details>


### [67] [Large Language Models Preserve Semantic Isotopies in Story Continuations](https://arxiv.org/abs/2510.04400)
*Marc Cavazza*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型生成文本是否保留语义同位素，通过故事续写实验发现LLM在给定token范围内能够保持语义同位素。


<details>
  <summary>Details</summary>
Motivation: 探索文本语义与大型语言模型的相关性，验证LLM生成文本是否保持语义同位素，扩展分布语义学与结构语义学之间联系的研究。

Method: 设计故事续写实验，使用10,000个ROCStories提示由5个LLM完成，首先验证GPT-4o从语言基准中提取同位素的能力，然后应用于生成的故事，分析同位素的结构和语义属性。

Result: 结果显示，在给定token范围内，LLM完成的故事在多个属性上保持了语义同位素。

Conclusion: LLM在特定token范围内能够有效保持语义同位素，表明其生成文本具有语义连贯性。

Abstract: In this work, we explore the relevance of textual semantics to Large Language
Models (LLMs), extending previous insights into the connection between
distributional semantics and structural semantics. We investigate whether
LLM-generated texts preserve semantic isotopies. We design a story continuation
experiment using 10,000 ROCStories prompts completed by five LLMs. We first
validate GPT-4o's ability to extract isotopies from a linguistic benchmark,
then apply it to the generated stories. We then analyze structural (coverage,
density, spread) and semantic properties of isotopies to assess how they are
affected by completion. Results show that LLM completion within a given token
horizon preserves semantic isotopies across multiple properties.

</details>


### [68] [Good Intentions Beyond ACL: Who Does NLP for Social Good, and Where?](https://arxiv.org/abs/2510.04434)
*Grace LeFevre,Qingcheng Zeng,Adam Leif,Jason Jewell,Denis Peskoff,Rob Voigt*

Main category: cs.CL

TL;DR: 本研究通过作者和会议层面的视角，量化分析了NLP社会公益研究的分布情况，发现ACL作者在非ACL会议上发表社会公益相关研究的比例更高，且大多数NLP社会公益研究由非ACL作者在非ACL会议上完成。


<details>
  <summary>Details</summary>
Motivation: 随着NLP社会影响力的增加，NLP社会公益研究日益重要，但缺乏对研究分布格局的系统分析。

Method: 采用作者和会议层面的分析方法，量化ACL社区内外作者在ACL和非ACL会议上发表NLP社会公益研究的比例。

Result: 发现两个令人惊讶的事实：ACL作者在非ACL会议上更可能从事社会公益研究；大多数NLP社会公益研究由非ACL作者在非ACL会议上完成。

Conclusion: 这些发现对ACL社区在NLP社会公益研究方面的议程设置具有重要意义。

Abstract: The social impact of Natural Language Processing (NLP) is increasingly
important, with a rising community focus on initiatives related to NLP for
Social Good (NLP4SG). Indeed, in recent years, almost 20% of all papers in the
ACL Anthology address topics related to social good as defined by the UN
Sustainable Development Goals (Adauto et al., 2023). In this study, we take an
author- and venue-level perspective to map the landscape of NLP4SG, quantifying
the proportion of work addressing social good concerns both within and beyond
the ACL community, by both core ACL contributors and non-ACL authors. With this
approach we discover two surprising facts about the landscape of NLP4SG. First,
ACL authors are dramatically more likely to do work addressing social good
concerns when publishing in venues outside of ACL. Second, the vast majority of
publications using NLP techniques to address concerns of social good are done
by non-ACL authors in venues outside of ACL. We discuss the implications of
these findings on agenda-setting considerations for the ACL community related
to NLP4SG.

</details>


### [69] [On the Role of Unobserved Sequences on Sample-based Uncertainty Quantification for LLMs](https://arxiv.org/abs/2510.04439)
*Lucie Kunitomo-Jacquin,Edison Marrese-Taylor,Ken Fukuda*

Main category: cs.CL

TL;DR: 本文主张在LLM不确定性量化中考虑未观测序列的概率，这对提升幻觉检测效果至关重要。


<details>
  <summary>Details</summary>
Motivation: 现有基于熵估计的LLM不确定性量化方法主要依赖观测到的输出序列，但忽略了未观测序列的概率，这可能影响不确定性评估的准确性。

Method: 通过实验验证未观测序列概率在LLM不确定性量化中的重要性，建议未来研究将其纳入考虑。

Result: 实验结果表明，未观测序列的概率在LLM不确定性量化中扮演关键角色。

Conclusion: 建议未来LLM不确定性量化研究应整合未观测序列的概率，以提升方法的有效性。

Abstract: Quantifying uncertainty in large language models (LLMs) is important for
safety-critical applications because it helps spot incorrect answers, known as
hallucinations. One major trend of uncertainty quantification methods is based
on estimating the entropy of the distribution of the LLM's potential output
sequences. This estimation is based on a set of output sequences and associated
probabilities obtained by querying the LLM several times. In this paper, we
advocate and experimentally show that the probability of unobserved sequences
plays a crucial role, and we recommend future research to integrate it to
enhance such LLM uncertainty quantification methods.

</details>


### [70] [Mitigating Forgetting Between Supervised and Reinforcement Learning Yields Stronger Reasoners](https://arxiv.org/abs/2510.04454)
*Xiangchi Yuan,Xiang Chen,Tong Yu,Dachuan Shi,Can Jin,Wenke Lee,Saayan Mitra*

Main category: cs.CL

TL;DR: 提出了一种动态整合监督微调(SFT)和强化学习(RL)的即插即用框架，通过选择挑战性样本进行SFT，显著减少数据需求并避免灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有方法结合SFT和RL面临数据效率低、算法特定设计和灾难性遗忘三大挑战，需要更高效的整合方案。

Method: 动态选择挑战性样本进行SFT，计算高熵token的损失，并冻结对RL重要的参数，实现SFT与RL的无缝整合。

Result: 仅使用1.5%的SFT数据和20.4%的RL数据就达到了最先进的推理性能。

Conclusion: 该方法为推理后训练中SFT和RL的结合提供了高效且即插即用的解决方案。

Abstract: Large Language Models (LLMs) show strong reasoning abilities, often amplified
by Chain-of-Thought (CoT) prompting and reinforcement learning (RL). Although
RL algorithms can substantially improve reasoning, they struggle to expand
reasoning boundaries because they learn from their own reasoning trajectories
rather than acquiring external knowledge. Supervised fine-tuning (SFT) offers
complementary benefits but typically requires large-scale data and risks
overfitting. Recent attempts to combine SFT and RL face three main challenges:
data inefficiency, algorithm-specific designs, and catastrophic forgetting. We
propose a plug-and-play framework that dynamically integrates SFT into RL by
selecting challenging examples for SFT. This approach reduces SFT data
requirements and remains agnostic to the choice of RL or SFT algorithm. To
mitigate catastrophic forgetting of RL-acquired skills during SFT, we select
high-entropy tokens for loss calculation and freeze parameters identified as
critical for RL. Our method achieves state-of-the-art (SoTA) reasoning
performance using only 1.5% of the SFT data and 20.4% of the RL data used by
prior SoTA, providing an efficient and plug-and-play solution for combining SFT
and RL in reasoning post-training.

</details>


### [71] [Compressed Convolutional Attention: Efficient Attention in a Compressed Latent Space](https://arxiv.org/abs/2510.04476)
*Tomas Figliolia,Nicholas Alonso,Rishi Iyer,Quentin Anthony,Beren Millidge*

Main category: cs.CL

TL;DR: CCA是一种新型注意力机制，通过将查询、键和值降维投影到共享潜在空间进行注意力计算，显著减少了参数、KV缓存和FLOPs。结合GQA形成CCGQA，在保持质量的同时实现了8倍KV缓存压缩和计算加速。


<details>
  <summary>Details</summary>
Motivation: 多头注意力(MHA)的二次计算复杂度和线性增长的KV缓存使得长上下文Transformer训练和推理成本高昂。现有方法如GQA和MLA虽然减少了缓存，但计算复杂度基本未变。

Method: 提出压缩卷积注意力(CCA)，将查询、键和值降维投影到共享潜在空间进行注意力计算。结合分组查询注意力(GQA)形成CCGQA，在计算和带宽之间实现更好的权衡。

Result: CCGQA在密集和MoE模型上均优于GQA和MLA，在同等KV缓存压缩下性能更好。在MoE模型上，CCGQA以GQA和MLA一半的KV缓存实现8倍压缩且性能无损失。在H100 GPU上，预填充延迟减少约1.7倍，反向传播加速约1.3倍。

Conclusion: CCA和CCGQA通过将注意力操作完全在潜在空间进行，同时减少了参数、KV缓存和计算成本，为长上下文Transformer提供了高效的解决方案。

Abstract: Multi-headed Attention's (MHA) quadratic compute and linearly growing
KV-cache make long-context transformers expensive to train and serve. Prior
works such as Grouped Query Attention (GQA) and Multi-Latent Attention (MLA)
shrink the cache, speeding decode, but leave compute, which determines prefill
and training speed, largely unchanged. We introduce Compressed Convolutional
Attention (CCA), a novel attention method which down-projects queries, keys,
and values and performs the entire attention operation inside the shared latent
space. This simple design dramatically cuts parameters, KV-cache, and FLOPs all
at once by the desired compression factor. Because CCA is orthogonal to
head-sharing, we combine the two to form Compressed Convolutional Grouped Query
Attention (CCGQA), which further tightens the compute-bandwidth Pareto frontier
so that users can tune compression toward either FLOP or memory limits without
sacrificing quality. Experiments show that CCGQA consistently outperforms both
GQA and MLA at equal KV-cache compression on dense and MoE models.
Additionally, we show that CCGQA outperforms all other attention methods on MoE
models with half the KV-cache of GQA and MLA, achieving an 8x KV-cache
compression with no drop in performance compared to standard MHA. CCA and CCGQA
also dramatically reduce the FLOP cost of attention which leads to
substantially faster training and prefill than existing methods. On H100 GPUs,
our fused CCA/CCGQA kernel reduces prefill latency by about 1.7x at a sequence
length of 16k relative to MHA, and accelerates backward by about 1.3x.

</details>


### [72] [Psychological Steering in LLMs: An Evaluation of Effectiveness and Trustworthiness](https://arxiv.org/abs/2510.04484)
*Amin Banayeeanzade,Ala N. Tak,Fatemeh Bahrani,Anahita Bolourani,Leonardo Blas,Emilio Ferrara,Jonathan Gratch,Sai Praneeth Karimireddy*

Main category: cs.CL

TL;DR: PsySET是一个心理学基准测试，用于评估LLM在情绪和人格领域的引导效果和可信度，涵盖多种模型和引导策略，发现提示法有效但控制强度有限，向量注入可实现更精细控制但会轻微降低输出质量。


<details>
  <summary>Details</summary>
Motivation: 控制LLM的情绪状态和人格特质对于实现丰富、以人为中心的社会交互至关重要，需要评估不同引导策略的效果和可信度。

Method: 使用PsySET基准测试，涵盖四个不同LLM家族的模型，结合提示法、微调和表示工程等多种引导策略进行评估。

Result: 提示法持续有效但强度控制有限；向量注入可实现更精细控制但轻微降低输出质量；情绪引导会产生特异性效应，如喜悦会降低对抗性事实的鲁棒性、降低隐私意识、增加偏好偏见；愤怒会提高毒性但增强泄漏抵抗力。

Conclusion: 该框架建立了首个情绪和人格引导的全面评估，为社交交互应用的可解释性和可靠性提供了见解。

Abstract: The ability to control LLMs' emulated emotional states and personality traits
is essential for enabling rich, human-centered interactions in socially
interactive settings. We introduce PsySET, a Psychologically-informed benchmark
to evaluate LLM Steering Effectiveness and Trustworthiness across the emotion
and personality domains. Our study spans four models from different LLM
families paired with various steering strategies, including prompting,
fine-tuning, and representation engineering. Our results indicate that
prompting is consistently effective but limited in intensity control, whereas
vector injections achieve finer controllability while slightly reducing output
quality. Moreover, we explore the trustworthiness of steered LLMs by assessing
safety, truthfulness, fairness, and ethics, highlighting potential side effects
and behavioral shifts. Notably, we observe idiosyncratic effects; for instance,
even a positive emotion like joy can degrade robustness to adversarial
factuality, lower privacy awareness, and increase preferential bias. Meanwhile,
anger predictably elevates toxicity yet strengthens leakage resistance. Our
framework establishes the first holistic evaluation of emotion and personality
steering, offering insights into its interpretability and reliability for
socially interactive applications.

</details>


### [73] [GenQuest: An LLM-based Text Adventure Game for Language Learners](https://arxiv.org/abs/2510.04498)
*Qiao Wang,Adnan Labib,Robert Swier,Michael Hofmeyr,Zheng Yuan*

Main category: cs.CL

TL;DR: GenQuest是一个基于大语言模型的生成式文本冒险游戏，通过沉浸式互动故事促进第二语言学习，为EFL学习者提供个性化内容生成和词汇辅助功能。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型为EFL学习者创造沉浸式、互动式的语言学习环境，通过游戏化方式提升学习动机和效果。

Method: 采用"选择你自己的冒险"式叙事，根据学习者选择动态生成故事，包含分支决策点和故事里程碑，提供基于学习者水平的个性化内容生成和词汇查询辅助。

Result: 对中国大学EFL学生的初步研究表明，该系统在词汇习得方面表现出积极效果，用户反馈良好。

Conclusion: GenQuest展示了生成式文本冒险游戏在语言学习中的潜力，未来可考虑增加多模态内容和优化叙事长度与质量。

Abstract: GenQuest is a generative text adventure game that leverages Large Language
Models (LLMs) to facilitate second language learning through immersive,
interactive storytelling. The system engages English as a Foreign Language
(EFL) learners in a collaborative "choose-your-own-adventure" style narrative,
dynamically generated in response to learner choices. Game mechanics such as
branching decision points and story milestones are incorporated to maintain
narrative coherence while allowing learner-driven plot development. Key
pedagogical features include content generation tailored to each learner's
proficiency level, and a vocabulary assistant that provides in-context
explanations of learner-queried text strings, ranging from words and phrases to
sentences. Findings from a pilot study with university EFL students in China
indicate promising vocabulary gains and positive user perceptions. Also
discussed are suggestions from participants regarding the narrative length and
quality, and the request for multi-modal content such as illustrations.

</details>


### [74] [GRACE: Generative Representation Learning via Contrastive Policy Optimization](https://arxiv.org/abs/2510.04506)
*Jiashuo Sun,Shixuan Liu,Zhaochen Su,Xianrui Zhong,Pengcheng Jiang,Bowen Jin,Peiran Li,Weijia Shi,Jiawei Han*

Main category: cs.CL

TL;DR: GRACE是一个新颖的框架，将对比信号重新构想为奖励来指导生成策略，而不是要最小化的损失函数，从而在保持生成能力的同时提升文本编码性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型文本编码训练方法依赖对比损失，将模型视为黑盒函数，丢弃了其生成和推理能力，仅产生静态嵌入。

Method: GRACE框架将LLM作为生成可解释推理过程的策略，通过策略梯度优化和包含正负样本相似度的多组件奖励函数进行训练。

Result: 在MTEB基准测试中，GRACE在四个骨干模型上平均提升总体得分：监督设置比基础模型提高11.5%，无监督变体提高6.9%，同时保持通用能力。

Conclusion: 该工作将对比目标视为推理过程的奖励，统一了表示学习和生成，产生更强的嵌入和透明的推理过程。

Abstract: Prevailing methods for training Large Language Models (LLMs) as text encoders
rely on contrastive losses that treat the model as a black box function,
discarding its generative and reasoning capabilities in favor of static
embeddings. We introduce GRACE (Generative Representation Learning via
Contrastive Policy Optimization), a novel framework that reimagines contrastive
signals not as losses to be minimized, but as rewards that guide a generative
policy. In GRACE, the LLM acts as a policy that produces explicit,
human-interpretable rationales--structured natural language explanations of its
semantic understanding. These rationales are then encoded into high-quality
embeddings via mean pooling. Using policy gradient optimization, we train the
model with a multi-component reward function that maximizes similarity between
query positive pairs and minimizes similarity with negatives. This transforms
the LLM from an opaque encoder into an interpretable agent whose reasoning
process is transparent and inspectable. On MTEB benchmark, GRACE yields broad
cross category gains: averaged over four backbones, the supervised setting
improves overall score by 11.5% over base models, and the unsupervised variant
adds 6.9%, while preserving general capabilities. This work treats contrastive
objectives as rewards over rationales, unifying representation learning with
generation to produce stronger embeddings and transparent rationales. The
model, data and code are available at https://github.com/GasolSun36/GRACE.

</details>


### [75] [Fine-grained auxiliary learning for real-world product recommendation](https://arxiv.org/abs/2510.04551)
*Mario Almagro,Diego Ortego,David Jimenez*

Main category: cs.CL

TL;DR: 提出ALC辅助学习策略，通过细粒度嵌入提升产品推荐系统的覆盖率，结合硬负样本训练目标和阈值一致性边界损失实现最先进的覆盖率表现


<details>
  <summary>Details</summary>
Motivation: 现实生产系统对覆盖率有严格要求，需要高比例的自动化推荐，但现有模型在真实系统中的应用常被忽视

Method: 采用辅助学习策略，引入两个利用批次中最难负样本的训练目标，构建正负样本间的判别性训练信号，结合三种极端多标签分类方法和阈值一致性边界损失

Result: 在两个产品推荐数据集（LF-AmazonTitles-131K和Tech and Durables）上验证，展示了最先进的覆盖率表现

Conclusion: ALC策略能有效提升产品推荐系统的自动化覆盖率，满足生产系统的实际需求

Abstract: Product recommendation is the task of recovering the closest items to a given
query within a large product corpora. Generally, one can determine if
top-ranked products are related to the query by applying a similarity
threshold; exceeding it deems the product relevant, otherwise manual revision
is required. Despite being a well-known problem, the integration of these
models in real-world systems is often overlooked. In particular, production
systems have strong coverage requirements, i.e., a high proportion of
recommendations must be automated. In this paper we propose ALC , an Auxiliary
Learning strategy that boosts Coverage through learning fine-grained
embeddings. Concretely, we introduce two training objectives that leverage the
hardest negatives in the batch to build discriminative training signals between
positives and negatives. We validate ALC using three extreme multi-label
classification approaches in two product recommendation datasets;
LF-AmazonTitles-131K and Tech and Durables (proprietary), demonstrating
state-of-the-art coverage rates when combined with a recent
threshold-consistent margin loss.

</details>


### [76] [Can LLMs Detect Ambiguous Plural Reference? An Analysis of Split-Antecedent and Mereological Reference](https://arxiv.org/abs/2510.04581)
*Dang Anh,Rick Nouwen,Massimo Poesio*

Main category: cs.CL

TL;DR: LLMs在表示和解释复数指代方面与人类存在差异，虽然有时能识别模糊代词的潜在指代对象，但在选择解释时并不总是遵循人类偏好，特别是在未明确提及的情况下，且难以在没有直接指令时识别模糊性。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs如何表示和解释模糊与明确语境中的复数指代，探究LLMs是否具有类似人类的指代偏好，以及能否检测复数照应表达的模糊性并识别可能的指代对象。

Method: 设计一系列实验，包括使用下一个词预测任务的代词生成、代词解释，以及采用不同提示策略的模糊性检测。

Result: LLMs有时能识别模糊代词的潜在指代对象，但在选择解释时并不总是遵循人类参考，特别是当可能的解释未被明确提及时。此外，它们在没有直接指令时难以识别模糊性，且不同类型实验的结果存在不一致性。

Conclusion: LLMs在复数指代处理方面与人类存在显著差异，表现出不一致的行为模式，特别是在模糊性识别和指代偏好选择方面。

Abstract: Our goal is to study how LLMs represent and interpret plural reference in
ambiguous and unambiguous contexts. We ask the following research questions:
(1) Do LLMs exhibit human-like preferences in representing plural reference?
(2) Are LLMs able to detect ambiguity in plural anaphoric expressions and
identify possible referents? To address these questions, we design a set of
experiments, examining pronoun production using next-token prediction tasks,
pronoun interpretation, and ambiguity detection using different prompting
strategies. We then assess how comparable LLMs are to humans in formulating and
interpreting plural reference. We find that LLMs are sometimes aware of
possible referents of ambiguous pronouns. However, they do not always follow
human reference when choosing between interpretations, especially when the
possible interpretation is not explicitly mentioned. In addition, they struggle
to identify ambiguity without direct instruction. Our findings also reveal
inconsistencies in the results across different types of experiments.

</details>


### [77] [Robustness assessment of large audio language models in multiple-choice evaluation](https://arxiv.org/abs/2510.04584)
*Fernando López,Santosh Kesiraju,Jordi Luque*

Main category: cs.CL

TL;DR: 研究发现大型音频语言模型在多项选择题评估中对选项顺序、问题表述和选项改写都很敏感，提出了新的评估协议和指标来更全面地评估模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有MCQA评估框架只报告单一准确率，忽略了选项顺序变化等因素导致的显著结果差异，需要更细致的评估方法。

Method: 对三个基准测试(MMAU、MMAR、MMSU)和四个模型进行系统研究，分析选项顺序、问题改写和选项改写对结果的影响。

Result: 模型对选项顺序、问题表述和选项改写都很敏感，现有评估方法无法捕捉这些细微变化带来的性能差异。

Conclusion: 提出了更简单的评估协议和指标，能够考虑细微变化并提供更详细的大型音频语言模型评估报告。

Abstract: Recent advances in large audio language models (LALMs) have primarily been
assessed using a multiple-choice question answering (MCQA) framework. However,
subtle changes, such as shifting the order of choices, result in substantially
different results. Existing MCQA frameworks do not account for this variability
and report a single accuracy number per benchmark or category. We dive into the
MCQA evaluation framework and conduct a systematic study spanning three
benchmarks (MMAU, MMAR and MMSU) and four models: Audio Flamingo 2, Audio
Flamingo 3, Qwen2.5-Omni-7B-Instruct, and Kimi-Audio-7B-Instruct. Our findings
indicate that models are sensitive not only to the ordering of choices, but
also to the paraphrasing of the question and the choices. Finally, we propose a
simpler evaluation protocol and metric that account for subtle variations and
provide a more detailed evaluation report of LALMs within the MCQA framework.

</details>


### [78] [FedSRD: Sparsify-Reconstruct-Decompose for Communication-Efficient Federated Large Language Models Fine-Tuning](https://arxiv.org/abs/2510.04601)
*Guochen Yan,Luyuan Xie,Qingni Shen,Yuejian Fang,Zhonghai Wu*

Main category: cs.CL

TL;DR: FedSRD是一个用于联邦学习的通信高效框架，通过稀疏化-重构-分解方法解决LoRA在联邦设置中的通信瓶颈问题，显著降低通信成本同时提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前在公开网络数据上训练大语言模型的范式不可持续，高质量专业领域数据接近枯竭。联邦学习作为下一代AI的实用解决方案，但LoRA在联邦设置中面临通信开销大的关键挑战。

Method: 提出FedSRD框架：重要性感知稀疏化方法保持LoRA更新的结构完整性以减少上传参数；服务器在完整秩空间重构和聚合更新以缓解冲突；将全局更新分解为稀疏低秩格式进行广播。还提出了计算开销更低的FedSRD-e变体。

Result: 在10个基准测试上的实验结果表明，该框架显著降低通信成本达90%，同时在异构客户端数据上甚至提升了模型性能。

Conclusion: FedSRD有效解决了联邦学习中LoRA的通信瓶颈问题，实现了通信效率与模型性能的双重提升，为下一代AI在去中心化网络上的发展提供了实用解决方案。

Abstract: The current paradigm of training large language models (LLMs) on publicly
available Web data is becoming unsustainable, with high-quality data sources in
specialized domains nearing exhaustion. Federated Learning (FL) emerges as a
practical solution for the next generation of AI on a decentralized Web,
enabling privacy-preserving collaborative fine-tuning by leveraging private
data distributed across a global client base. While Low-Rank Adaptation (LoRA)
is the standard for efficient fine-tuning, its application in federated
settings presents a critical challenge: communication overhead remains a
significant bottleneck across the Web's heterogeneous network conditions. The
structural redundancy within LoRA parameters not only incurs a heavy
communication burden but also introduces conflicts when aggregating client
updates. To address this, we propose FedSRD, a Sparsify-Reconstruct-Decompose
framework designed for communication-efficient FL. We first introduce an
importance-aware sparsification method that preserves the structural integrity
of LoRA updates to reduce the uploaded parameter count. The server then
reconstructs and aggregates these updates in a full-rank space to mitigate
conflicts. Finally, it decomposes the global update into a sparse low-rank
format for broadcast, ensuring a symmetrically efficient cycle. We also propose
an efficient variant, FedSRD-e, to reduce computational overhead. Experimental
results on 10 benchmarks demonstrate that our framework significantly reduces
communication costs by up to 90\% while even improving model performance on
heterogeneous client data.

</details>


### [79] [Contrastive Learning Using Graph Embeddings for Domain Adaptation of Language Models in the Process Industry](https://arxiv.org/abs/2510.04631)
*Anastasia Zhukova,Jonas Lührs,Christian E. Matt,Bela Gipp*

Main category: cs.CL

TL;DR: 将SciNCL图感知邻域对比学习方法应用于过程工业领域，通过知识图谱增强语言模型，在PITEB基准上比mE5-large提升9.8-14.3%，且模型尺寸小3-5倍


<details>
  <summary>Details</summary>
Motivation: 利用知识图谱增强预训练语言模型，学习领域特定术语和文档间关系，解决过程工业领域文本日志中可能被忽略的关键信息

Method: 应用SciNCL图感知邻域对比学习方法，从过程工业领域的稀疏知识图谱中提取三元组来微调语言模型

Result: 在专有过程工业文本嵌入基准(PITEB)上，性能比最先进的mE5-large文本编码器提升9.8-14.3%(5.4-8.0个百分点)，同时模型尺寸小3-5倍

Conclusion: 图感知对比学习方法在过程工业领域具有显著优势，能够有效利用知识图谱结构提升文本嵌入性能，同时保持较小的模型尺寸

Abstract: Recent trends in NLP utilize knowledge graphs (KGs) to enhance pretrained
language models by incorporating additional knowledge from the graph structures
to learn domain-specific terminology or relationships between documents that
might otherwise be overlooked. This paper explores how SciNCL, a graph-aware
neighborhood contrastive learning methodology originally designed for
scientific publications, can be applied to the process industry domain, where
text logs contain crucial information about daily operations and are often
structured as sparse KGs. Our experiments demonstrate that language models
fine-tuned with triplets derived from GE outperform a state-of-the-art
mE5-large text encoder by 9.8-14.3% (5.4-8.0p) on the proprietary process
industry text embedding benchmark (PITEB) while being 3-5 times smaller in
size.

</details>


### [80] [Evaluating LLMs for Demographic-Targeted Social Bias Detection: A Comprehensive Benchmark Study](https://arxiv.org/abs/2510.04641)
*Ayan Majumdar,Feihao Chen,Jinghui Li,Xiaozhen Wang*

Main category: cs.CL

TL;DR: 本文提出了一个评估框架，用于评估大语言模型在检测针对人口统计特征的社会偏见方面的能力，发现微调的小型模型在可扩展检测方面表现良好，但在多人口统计偏见检测方面仍存在差距。


<details>
  <summary>Details</summary>
Motivation: 大规模网络爬取的文本语料库通常包含有害的人口统计特征针对性社会偏见，需要数据审计和可扩展的偏见检测方法。现有研究范围狭窄，缺乏对LLMs自动偏见检测能力的全面理解。

Method: 构建了一个针对英语文本的全面评估框架，将偏见检测定义为多标签任务，使用人口统计特征导向的分类法，系统评估了不同规模和技术的模型，包括提示、上下文学习和微调。

Result: 使用12个涵盖不同内容类型和人口统计特征的数据集，研究表明微调的小型模型在可扩展检测方面具有潜力，但分析也揭示了跨人口统计轴和多人口统计针对性偏见的持续差距。

Conclusion: 需要更有效和可扩展的审计框架来解决偏见检测中的现有差距，特别是在多人口统计针对性偏见方面。

Abstract: Large-scale web-scraped text corpora used to train general-purpose AI models
often contain harmful demographic-targeted social biases, creating a regulatory
need for data auditing and developing scalable bias-detection methods. Although
prior work has investigated biases in text datasets and related detection
methods, these studies remain narrow in scope. They typically focus on a single
content type (e.g., hate speech), cover limited demographic axes, overlook
biases affecting multiple demographics simultaneously, and analyze limited
techniques. Consequently, practitioners lack a holistic understanding of the
strengths and limitations of recent large language models (LLMs) for automated
bias detection. In this study, we present a comprehensive evaluation framework
aimed at English texts to assess the ability of LLMs in detecting
demographic-targeted social biases. To align with regulatory requirements, we
frame bias detection as a multi-label task using a demographic-focused
taxonomy. We then conduct a systematic evaluation with models across scales and
techniques, including prompting, in-context learning, and fine-tuning. Using
twelve datasets spanning diverse content types and demographics, our study
demonstrates the promise of fine-tuned smaller models for scalable detection.
However, our analyses also expose persistent gaps across demographic axes and
multi-demographic targeted biases, underscoring the need for more effective and
scalable auditing frameworks.

</details>


### [81] [FT-MDT: Extracting Decision Trees from Medical Texts via a Novel Low-rank Adaptation Method](https://arxiv.org/abs/2510.04655)
*Yuheng Li,Jiechao Gao,Wei Han,Wenwen Ouyang,Wei Zhu,Hui Yi Leong*

Main category: cs.CL

TL;DR: PI-LoRA是一种新颖的低秩自适应方法，用于从临床指南和教科书中自动提取医疗决策树，通过集成梯度路径信息实现更有效的秩分配，在保持轻量级架构的同时达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前医疗决策树构建方法严重依赖耗时费力的手动标注，需要自动化方法来从临床文本中提取医疗决策过程知识。

Method: 提出PI-LoRA（路径集成LoRA）方法，集成梯度路径信息来捕捉不同模块间的协同效应，实现更有效的秩分配，关键模块获得适当秩分配，不重要模块被剪枝。

Result: 在医疗指南数据集上的实验表明，PI-LoRA在Text2MDT任务上显著优于现有的参数高效微调方法，以大幅降低的模型复杂度获得更好的准确性。

Conclusion: 该方法在保持轻量级架构的同时达到最先进结果，特别适合计算资源可能受限的临床决策支持系统。

Abstract: Knowledge of the medical decision process, which can be modeled as medical
decision trees (MDTs), is critical to building clinical decision support
systems. However, current MDT construction methods rely heavily on
time-consuming and laborious manual annotation. To address this challenge, we
propose PI-LoRA (Path-Integrated LoRA), a novel low-rank adaptation method for
automatically extracting MDTs from clinical guidelines and textbooks. We
integrate gradient path information to capture synergistic effects between
different modules, enabling more effective and reliable rank allocation. This
framework ensures that the most critical modules receive appropriate rank
allocations while less important ones are pruned, resulting in a more efficient
and accurate model for extracting medical decision trees from clinical texts.
Extensive experiments on medical guideline datasets demonstrate that our
PI-LoRA method significantly outperforms existing parameter-efficient
fine-tuning approaches for the Text2MDT task, achieving better accuracy with
substantially reduced model complexity. The proposed method achieves
state-of-the-art results while maintaining a lightweight architecture, making
it particularly suitable for clinical decision support systems where
computational resources may be limited.

</details>


### [82] [FocusMed: A Large Language Model-based Framework for Enhancing Medical Question Summarization with Focus Identification](https://arxiv.org/abs/2510.04671)
*Chao Liu,Ling Luo,Tengxiao Lv,Huan Zhuang,Lejing Yu,Jian Wang,Hongfei Lin*

Main category: cs.CL

TL;DR: 本文提出基于核心焦点引导的优化框架，通过提取忠实于原文的核心焦点、构建微调数据集和多维度质量评估机制，显著提升医疗问题摘要任务中焦点识别能力和减少幻觉生成。


<details>
  <summary>Details</summary>
Motivation: 在线医疗平台的快速发展使得消费者健康问题因冗余信息和非专业术语而诊断效率低下。现有医疗问题摘要方法在问题焦点识别和模型幻觉方面仍面临挑战。

Method: 提出基于核心焦点引导的优化框架：1）设计提示模板驱动LLMs提取忠实于原文的核心焦点；2）结合原始CHQ-FAQ对构建微调数据集；3）提出多维度质量评估和选择机制。

Result: 在两个广泛采用的MQS数据集上使用三个评估指标进行综合实验，所提框架在所有指标上均达到最先进性能，显著提升了模型识别问题关键焦点的能力并显著减轻了幻觉。

Conclusion: 基于核心焦点引导的优化框架有效解决了医疗问题摘要任务中的焦点识别偏差和幻觉问题，为LLMs在该领域的应用提供了有效解决方案。

Abstract: With the rapid development of online medical platforms, consumer health
questions (CHQs) are inefficient in diagnosis due to redundant information and
frequent non-professional terms. The medical question summary (MQS) task aims
to transform CHQs into streamlined doctors' frequently asked questions (FAQs),
but existing methods still face challenges such as poor identification of
question focus and model hallucination. This paper explores the potential of
large language models (LLMs) in the MQS task and finds that direct fine-tuning
is prone to focus identification bias and generates unfaithful content. To this
end, we propose an optimization framework based on core focus guidance. First,
a prompt template is designed to drive the LLMs to extract the core focus from
the CHQs that is faithful to the original text. Then, a fine-tuning dataset is
constructed in combination with the original CHQ-FAQ pairs to improve the
ability to identify the focus of the question. Finally, a multi-dimensional
quality evaluation and selection mechanism is proposed to comprehensively
improve the quality of the summary from multiple dimensions. We conduct
comprehensive experiments on two widely-adopted MQS datasets using three
established evaluation metrics. The proposed framework achieves
state-of-the-art performance across all measures, demonstrating a significant
boost in the model's ability to identify critical focus of questions and a
notable mitigation of hallucinations. The source codes are freely available at
https://github.com/DUT-LiuChao/FocusMed.

</details>


### [83] [Multi-Agent Tool-Integrated Policy Optimization](https://arxiv.org/abs/2510.04678)
*Zhanfeng Mo,Xingxuan Li,Yuntao Chen,Lidong Bing*

Main category: cs.CL

TL;DR: 提出了MATPO方法，在单个LLM中通过强化学习训练规划者和执行者两个角色，解决多轮工具集成规划中的上下文限制和噪声工具响应问题。


<details>
  <summary>Details</summary>
Motivation: 现有单智能体方法存在上下文长度有限和工具响应噪声问题，多智能体框架虽然能管理上下文，但缺乏有效的强化学习训练方法。

Method: MATPO通过角色特定提示在单个LLM实例中训练规划者和执行者角色，基于原则性的信用分配机制在规划者和执行者轨迹间分配奖励。

Result: 在GAIA-text、WebWalkerQA和FRAMES数据集上，MATPO平均相对性能提升18.38%，对噪声工具输出具有更强鲁棒性。

Conclusion: 在单个LLM中统一多个智能体角色是有效的，为稳定高效的多智能体强化学习训练提供了实用见解。

Abstract: Large language models (LLMs) increasingly rely on multi-turn tool-integrated
planning for knowledge-intensive and complex reasoning tasks. Existing
implementations typically rely on a single agent, but they suffer from limited
context length and noisy tool responses. A natural solution is to adopt a
multi-agent framework with planner- and worker-agents to manage context.
However, no existing methods support effective reinforcement learning
post-training of tool-integrated multi-agent frameworks. To address this gap,
we propose Multi-Agent Tool-Integrated Policy Optimization (MATPO), which
enables distinct roles (planner and worker) to be trained within a single LLM
instance using role-specific prompts via reinforcement learning. MATPO is
derived from a principled credit assignment mechanism across planner and worker
rollouts. This design eliminates the need to deploy multiple LLMs, which would
be memory-intensive, while preserving the benefits of specialization.
Experiments on GAIA-text, WebWalkerQA, and FRAMES show that MATPO consistently
outperforms single-agent baselines by an average of 18.38% relative improvement
in performance and exhibits greater robustness to noisy tool outputs. Our
findings highlight the effectiveness of unifying multiple agent roles within a
single LLM and provide practical insights for stable and efficient multi-agent
RL training.

</details>


### [84] [TiTok: Transfer Token-level Knowledge via Contrastive Excess to Transplant LoRA](https://arxiv.org/abs/2510.04682)
*Chanjoo Jung,Jaehyung Kim*

Main category: cs.CL

TL;DR: TiTok是一个新的框架，通过令牌级知识转移实现有效的LoRA移植，无需额外模型或开销，在多个迁移设置中平均性能提升4~8%。


<details>
  <summary>Details</summary>
Motivation: 解决参数高效微调方法如LoRA中适配参数依赖于基础模型且无法在不同骨干网络间迁移的问题，避免知识蒸馏对训练数据的依赖和TransLoRA需要额外判别器模型的复杂性。

Method: 通过对比源模型在有和没有LoRA情况下的差异，捕捉任务相关信息，突出信息丰富的令牌，并选择性过滤合成数据。

Result: 在三个基准测试的多个迁移设置中，该方法始终有效，整体平均性能提升4~8%。

Conclusion: TiTok框架通过令牌级知识转移实现了有效的LoRA移植，无需额外模型或开销，在多个迁移场景中表现一致优越。

Abstract: Large Language Models (LLMs) are widely applied in real world scenarios, but
fine-tuning them comes with significant computational and storage costs.
Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA mitigate these
costs, but the adapted parameters are dependent on the base model and cannot be
transferred across different backbones. One way to address this issue is
through knowledge distillation, but its effectiveness inherently depends on
training data. Recent work such as TransLoRA avoids this by generating
synthetic data, but this adds complexity because it requires training an
additional discriminator model. In this paper, we propose TiTok, a new
framework that enables effective LoRA Transplantation through Token-level
knowledge transfer. Specifically, TiTok captures task-relevant information
through a contrastive excess between a source model with and without LoRA. This
excess highlights informative tokens and enables selective filtering of
synthetic data, all without additional models or overhead. Through experiments
on three benchmarks across multiple transfer settings, our experiments show
that the proposed method is consistently effective, achieving average
performance gains of +4~8% compared to baselines overall.

</details>


### [85] [Multilingual Routing in Mixture-of-Experts](https://arxiv.org/abs/2510.04694)
*Lucas Bandarkar,Chenyuan Yang,Mohsen Fayyaz,Junlin Hu,Nanyun Peng*

Main category: cs.CL

TL;DR: 该论文分析了MoE架构在多语言数据中的稀疏路由动态，发现在中间层存在跨语言路由对齐现象，并提出了一种通过促进英语激活的任务专家来提高多语言性能的推理时干预方法。


<details>
  <summary>Details</summary>
Motivation: 理解MoE架构在多语言数据中的稀疏路由动态，探索如何提高多语言LLM的性能。

Method: 使用并行多语言数据集分析专家路由模式，提出在推理时通过促进中间层英语激活的任务专家来引导路由器的干预方法。

Result: 干预方法在两种评估任务、三个模型和15+种语言上一致提高了1-2%的多语言性能，而中间层之外的干预或针对多语言专门专家的干预会导致性能下降。

Conclusion: MoE模型处理非英语文本的能力受限于其在所有语言中利用语言通用专家的能力，中间层的跨语言路由对齐与模型性能密切相关。

Abstract: Mixture-of-Experts (MoE) architectures have become the key to scaling modern
LLMs, yet little is understood about how their sparse routing dynamics respond
to multilingual data. In this work, we analyze expert routing patterns using
parallel multilingual datasets and present highly interpretable layer-wise
phenomena. We find that MoE models route tokens in language-specific ways in
the early and late decoder layers but exhibit significant cross-lingual routing
alignment in middle layers, mirroring parameter-sharing trends observed in
dense LLMs. In particular, we reveal a clear, strong correlation between a
model's performance in a given language and how similarly its tokens are routed
to English in these layers. Extending beyond correlation, we explore
inference-time interventions that induce higher cross-lingual routing
alignment. We introduce a method that steers the router by promoting
middle-layer task experts frequently activated in English, and it successfully
increases multilingual performance. These 1-2% gains are remarkably consistent
across two evaluation tasks, three models, and 15+ languages, especially given
that these simple interventions override routers of extensively trained,
state-of-the-art LLMs. In comparison, interventions outside of the middle
layers or targeting multilingual-specialized experts only yield performance
degradation. Altogether, we present numerous findings that explain how MoEs
process non-English text and demonstrate that generalization is limited by the
model's ability to leverage language-universal experts in all languages.

</details>


### [86] [JSON Whisperer: Efficient JSON Editing with LLMs](https://arxiv.org/abs/2510.04717)
*Sarel Duanis,Asnat Greenstein-Messica,Eliya Habba*

Main category: cs.CL

TL;DR: JSON Whisperer是一个让LLM生成JSON补丁而非完整文档的框架，通过EASE编码解决数组索引问题，减少31%的token使用量。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在修改JSON文档时需要重新生成整个结构，计算效率低下。

Method: 提出JSON Whisperer框架生成RFC 6902差异补丁，引入EASE编码将数组转换为具有稳定键的字典。

Result: 补丁生成与EASE结合减少31%的token使用，编辑质量与完整再生相比仅下降5%。

Conclusion: JSON Whisperer通过补丁生成和EASE编码有效提高了LLM编辑JSON文档的效率。

Abstract: Large language models (LLMs) can modify JSON documents through natural
language commands, but current approaches regenerate entire structures for each
edit, resulting in computational inefficiency. We present JSON Whisperer, a
framework that enables LLMs to generate RFC 6902 diff patches-expressing only
the necessary modifications-rather than complete documents. We identify two key
challenges in patch-based editing: (1) LLMs often miss related updates when
generating isolated patches, and (2) array manipulations require tracking index
shifts across operations, which LLMs handle poorly. To address these issues, we
introduce EASE (Explicitly Addressed Sequence Encoding), which transforms
arrays into dictionaries with stable keys, eliminating index arithmetic
complexities. Our evaluation shows that patch generation with EASE reduces
token usage by 31% while maintaining edit quality within 5% of full
regeneration with particular gains for complex instructions and list
manipulations. The dataset is available at:
https://github.com/emnlp2025/JSON-Whisperer/

</details>


### [87] [ReplaceMe: Network Simplification via Depth Pruning and Transformer Block Linearization](https://arxiv.org/abs/2505.02819)
*Dmitriy Shopkhoev,Ammar Ali,Magauiya Zhussip,Valentin Malykh,Stamatios Lefkimmiatis,Nikos Komodakis,Sergey Zagoruyko*

Main category: cs.CL

TL;DR: ReplaceMe是一种无需训练即可剪枝Transformer块的方法，通过线性变换替换块并保持高性能，支持高达25%的剪枝率且性能保留约90%。


<details>
  <summary>Details</summary>
Motivation: 传统剪枝方法需要额外的训练或微调，计算成本高。ReplaceMe旨在开发一种无需训练的高效剪枝方法，仅需少量校准数据即可实现深度剪枝。

Method: 使用小规模校准数据集估计线性变换来近似被剪枝的Transformer块，该线性映射可与剩余块无缝合并，无需额外网络参数。

Result: 在多个大语言模型上测试，ReplaceMe在25%剪枝率下仍能保持约90%的原始性能，优于其他无需训练方法，且与需要大量重训练的方法竞争力相当。

Conclusion: ReplaceMe提供了一种高效、无需训练的深度剪枝方案，计算开销极小，在保持高性能的同时显著减少模型复杂度。

Abstract: We introduce ReplaceMe, a generalized training-free depth pruning method that
effectively replaces transformer blocks with a linear operation, while
maintaining high performance for low compression ratios. In contrast to
conventional pruning approaches that require additional training or
fine-tuning, our approach requires only a small calibration dataset that is
used to estimate a linear transformation, which approximates the pruned blocks.
The estimated linear mapping can be seamlessly merged with the remaining
transformer blocks, eliminating the need for any additional network parameters.
Our experiments show that ReplaceMe consistently outperforms other
training-free approaches and remains highly competitive with state-of-the-art
pruning methods that involve extensive retraining/fine-tuning and architectural
modifications. Applied to several large language models (LLMs), ReplaceMe
achieves up to 25% pruning while retaining approximately 90% of the original
model's performance on open benchmarks - without any training or healing steps,
resulting in minimal computational overhead (see Fig.1). We provide an
open-source library implementing ReplaceMe alongside several state-of-the-art
depth pruning techniques, available at https://github.com/mts-ai/ReplaceMe.

</details>


### [88] [A Low-Resource Speech-Driven NLP Pipeline for Sinhala Dyslexia Assistance](https://arxiv.org/abs/2510.04750)
*Peshala Perera,Deshan Sumanathilaka*

Main category: cs.CL

TL;DR: 为僧伽罗语成人阅读障碍者开发的多模态辅助系统，整合语音转文字、错误识别、文本修正和语音输出功能


<details>
  <summary>Details</summary>
Motivation: 成人阅读障碍在非英语语境中研究不足，特别是僧伽罗语等低资源语言缺乏语言可及性工具，严重影响个人和职业生活

Method: 集成Whisper进行语音转文字，使用SinBERT识别常见阅读障碍错误，结合mT5和Mistral模型生成修正文本，最后通过gTTS转换为语音输出

Result: 在有限的僧伽罗语数据集下，系统达到0.66转录准确率、0.7修正准确率和0.65整体系统准确率

Conclusion: 该方法证明了在低资源语言环境中开发包容性自然语言处理技术的可行性和有效性

Abstract: Dyslexia in adults remains an under-researched and under-served area,
particularly in non-English-speaking contexts, despite its significant impact
on personal and professional lives. This work addresses that gap by focusing on
Sinhala, a low-resource language with limited tools for linguistic
accessibility. We present an assistive system explicitly designed for
Sinhala-speaking adults with dyslexia. The system integrates Whisper for
speech-to-text conversion, SinBERT, an open-sourced fine-tuned BERT model
trained for Sinhala to identify common dyslexic errors, and a combined mT5 and
Mistral-based model to generate corrected text. Finally, the output is
converted back to speech using gTTS, creating a complete multimodal feedback
loop. Despite the challenges posed by limited Sinhala-language datasets, the
system achieves 0.66 transcription accuracy and 0.7 correction accuracy with
0.65 overall system accuracy. These results demonstrate both the feasibility
and effectiveness of the approach. Ultimately, this work highlights the
importance of inclusive Natural Language Processing (NLP) technologies in
underrepresented languages and showcases a practical

</details>


### [89] [ModernBERT + ColBERT: Enhancing biomedical RAG through an advanced re-ranking retriever](https://arxiv.org/abs/2510.04757)
*Eduardo Martínez Rivera,Filippo Menolascina*

Main category: cs.CL

TL;DR: 提出了一种两阶段检索架构，结合轻量级ModernBERT进行初始候选检索和ColBERTv2进行细粒度重排序，在生物医学RAG系统中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决RAG系统中检索模块性能受限的问题，特别是在专业领域如医疗保健中，通用检索器难以处理专业语言，而领域内模型计算成本过高。

Method: 使用ModernBERT双向编码器进行高效初始候选检索，结合ColBERTv2后期交互模型进行细粒度重排序，在PubMedQA的10k问题-段落对上微调IR模块。

Result: ColBERT重排序器将Recall@3提高了4.2个百分点；在MIRAGE QA基准测试的五个任务中实现了0.4448的平均准确率，优于MedCPT（0.4436）。

Conclusion: 两阶段检索架构在生物医学RAG中表现优异，但性能关键依赖于检索器和重排序器的联合微调过程，否则重排序器可能降低性能。

Abstract: Retrieval-Augmented Generation (RAG) is a powerful technique for enriching
Large Language Models (LLMs) with external knowledge, allowing for factually
grounded responses, a critical requirement in high-stakes domains such as
healthcare. However, the efficacy of RAG systems is fundamentally restricted by
the performance of their retrieval module, since irrelevant or semantically
misaligned documents directly compromise the accuracy of the final generated
response. General-purpose dense retrievers can struggle with the nuanced
language of specialised domains, while the high accuracy of in-domain models is
often achieved at prohibitive computational costs. In this work, we aim to
address this trade-off by developing and evaluating a two-stage retrieval
architecture that combines a lightweight ModernBERT bidirectional encoder for
efficient initial candidate retrieval with a ColBERTv2 late-interaction model
for fine-grained re-ranking. We conduct comprehensive evaluations of our
retriever module performance and RAG system performance in the biomedical
context, fine-tuning the IR module using 10k question-passage pairs from
PubMedQA. Our analysis of the retriever module confirmed the positive impact of
the ColBERT re-ranker, which improved Recall@3 by up to 4.2 percentage points
compared to its retrieve-only counterpart. When integrated into the biomedical
RAG, our IR module leads to a state-of-the-art average accuracy of 0.4448 on
the five tasks of the MIRAGE question-answering benchmark, outperforming strong
baselines such as MedCPT (0.4436). Our ablation studies reveal that this
performance is critically dependent on a joint fine-tuning process that aligns
the retriever and re-ranker; otherwise, the re-ranker might degrade the
performance.

</details>


### [90] [Are BabyLMs Deaf to Gricean Maxims? A Pragmatic Evaluation of Sample-efficient Language Models](https://arxiv.org/abs/2510.04764)
*Raha Askari,Sina Zarrieß,Özge Alacam,Judith Sieker*

Main category: cs.CL

TL;DR: 该研究提出了一个评估语言模型理解Grice会话准则违反的新基准，比较了在不同规模数据上预训练的BabyLMs与儿童和大型语言模型的性能差异。


<details>
  <summary>Details</summary>
Motivation: 理解隐含意义对人类交流至关重要，研究旨在测试语言模型是否能像人类一样识别和解释违反Grice会话准则的话语。

Method: 基于Surian等人对儿童Grice准则敏感性的研究，创建新基准测试模型区分准则遵守和违反话语的能力，比较了在10M和100M tokens上预训练的BabyLMs与儿童和3T tokens预训练的LLM。

Result: 在100M tokens上训练的模型优于10M tokens的模型，但仍未达到儿童和大型语言模型的水平；适度增加数据能改善某些语用行为方面，实现更细粒度的语用维度区分。

Conclusion: 小规模语言模型在理解语用含义方面有所进步，但与人类儿童和大型语言模型相比仍有差距，数据量的适度增加有助于提升语用行为的某些方面。

Abstract: Implicit meanings are integral to human communication, making it essential
for language models to be capable of identifying and interpreting them. Grice
(1975) proposed a set of conversational maxims that guide cooperative dialogue,
noting that speakers may deliberately violate these principles to express
meanings beyond literal words, and that listeners, in turn, recognize such
violations to draw pragmatic inferences.
  Building on Surian et al. (1996)'s study of children's sensitivity to
violations of Gricean maxims, we introduce a novel benchmark to test whether
language models pretrained on less than 10M and less than 100M tokens can
distinguish maxim-adhering from maxim-violating utterances. We compare these
BabyLMs across five maxims and situate their performance relative to children
and a Large Language Model (LLM) pretrained on 3T tokens.
  We find that overall, models trained on less than 100M tokens outperform
those trained on less than 10M, yet fall short of child-level and LLM
competence. Our results suggest that modest data increases improve some aspects
of pragmatic behavior, leading to finer-grained differentiation between
pragmatic dimensions.

</details>


### [91] [Hybrid Architectures for Language Models: Systematic Analysis and Design Insights](https://arxiv.org/abs/2510.04800)
*Sangmin Bae,Bilge Acun,Haroun Habeeb,Seungyeon Kim,Chien-Yu Lin,Liang Luo,Junjie Wang,Carole-Jean Wu*

Main category: cs.CL

TL;DR: 对基于自注意力机制和结构化状态空间模型（如Mamba）的混合架构进行系统评估，比较了层间（顺序）和层内（并行）融合策略，分析了影响混合模型有效性的关键因素，并提出了最优设计方法。


<details>
  <summary>Details</summary>
Motivation: 虽然混合架构在建模质量和计算效率之间取得了良好平衡，但缺乏对混合策略的系统比较和有效性关键因素的分析，需要为混合语言模型的开发提供实践指导。

Method: 从语言建模性能、长上下文能力、扩展分析以及训练和推理效率等多个角度，评估基于层间（顺序）或层内（并行）融合的混合架构设计。

Result: 通过研究计算原语的核心特征，识别了每种混合策略的最关键元素，并提出了两种混合模型的最优设计方法。

Conclusion: 综合分析为开发混合语言模型提供了实践指导和有价值的见解，有助于优化架构配置。

Abstract: Recent progress in large language models demonstrates that hybrid
architectures--combining self-attention mechanisms with structured state space
models like Mamba--can achieve a compelling balance between modeling quality
and computational efficiency, particularly for long-context tasks. While these
hybrid models show promising performance, systematic comparisons of
hybridization strategies and analyses on the key factors behind their
effectiveness have not been clearly shared to the community. In this work, we
present a holistic evaluation of hybrid architectures based on inter-layer
(sequential) or intra-layer (parallel) fusion. We evaluate these designs from a
variety of perspectives: language modeling performance, long-context
capabilities, scaling analysis, and training and inference efficiency. By
investigating the core characteristics of their computational primitive, we
identify the most critical elements for each hybridization strategy and further
propose optimal design recipes for both hybrid models. Our comprehensive
analysis provides practical guidance and valuable insights for developing
hybrid language models, facilitating the optimization of architectural
configurations.

</details>


### [92] [How I Built ASR for Endangered Languages with a Spoken Dictionary](https://arxiv.org/abs/2510.04832)
*Christopher Bartley,Anton Ragni*

Main category: cs.CL

TL;DR: 研究表明，即使是濒危语言，仅需少量发音数据（如40分钟）即可构建可用的自动语音识别系统，打破了传统ASR对大量标注数据的要求。


<details>
  <summary>Details</summary>
Motivation: 全球近一半语言濒临灭绝，传统ASR系统需要大量标注数据，而濒危语言往往缺乏这种资源。研究旨在探索构建濒危语言ASR所需的最小数据量和数据形式。

Method: 使用短格式发音资源作为替代方案，在曼克斯盖尔语和康沃尔语上进行实验，验证少量数据构建ASR的可行性。

Result: 仅用40分钟的发音数据即可为曼克斯盖尔语构建可用的ASR系统（词错误率<50%），并在康沃尔语上成功复现该方法。

Conclusion: 构建濒危语言ASR系统的数据门槛远低于传统认知，为无法满足传统数据要求的濒危语言社区带来了希望。

Abstract: Nearly half of the world's languages are endangered. Speech technologies such
as Automatic Speech Recognition (ASR) are central to revival efforts, yet most
languages remain unsupported because standard pipelines expect utterance-level
supervised data. Speech data often exist for endangered languages but rarely
match these formats. Manx Gaelic ($\sim$2,200 speakers), for example, has had
transcribed speech since 1948, yet remains unsupported by modern systems. In
this paper, we explore how little data, and in what form, is needed to build
ASR for critically endangered languages. We show that a short-form
pronunciation resource is a viable alternative, and that 40 minutes of such
data produces usable ASR for Manx ($<$50\% WER). We replicate our approach,
applying it to Cornish ($\sim$600 speakers), another critically endangered
language. Results show that the barrier to entry, in quantity and form, is far
lower than previously thought, giving hope to endangered language communities
that cannot afford to meet the requirements arbitrarily imposed upon them.

</details>


### [93] [Instability in Downstream Task Performance During LLM Pretraining](https://arxiv.org/abs/2510.04848)
*Yuto Nishida,Masaru Isonuma,Yusuke Oda*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型训练过程中下游任务性能的波动问题，提出了通过检查点平均和集成的方法来稳定性能，无需改变训练过程。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练时，下游任务性能经常出现显著波动，使得难以确定真正表现最佳的检查点。

Method: 研究了两种检查点集成方法：检查点平均和集成，通过聚合相邻检查点来减少性能波动。

Result: 实证和理论分析表明，这些方法能够提高下游性能的稳定性。

Conclusion: 检查点集成方法可以有效解决训练过程中的性能波动问题，提升模型选择的可靠性。

Abstract: When training large language models (LLMs), it is common practice to track
downstream task performance throughout the training process and select the
checkpoint with the highest validation score. However, downstream metrics often
exhibit substantial fluctuations, making it difficult to identify the
checkpoint that truly represents the best-performing model. In this study, we
empirically analyze the stability of downstream task performance in an LLM
trained on diverse web-scale corpora. We find that task scores frequently
fluctuate throughout training, both at the aggregate and example levels. To
address this instability, we investigate two post-hoc checkpoint integration
methods: checkpoint averaging and ensemble, motivated by the hypothesis that
aggregating neighboring checkpoints can reduce performance volatility. We
demonstrate both empirically and theoretically that these methods improve
downstream performance stability without requiring any changes to the training
procedure.

</details>


### [94] [When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with PsiloQA](https://arxiv.org/abs/2510.04849)
*Elisei Rykov,Kseniia Petrushina,Maksim Savkin,Valerii Olisov,Artem Vazhentsev,Kseniia Titova,Alexander Panchenko,Vasily Konovalov,Julia Belikova*

Main category: cs.CL

TL;DR: 提出了PsiloQA，一个大规模多语言数据集，包含14种语言的跨度级幻觉标注，用于评估幻觉检测方法。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉基准主要在序列级别且限于英语，缺乏细粒度的多语言监督，无法全面评估LLMs的幻觉问题。

Method: 通过自动化三阶段流程构建数据集：使用GPT-4o从维基百科生成问答对，在无上下文设置下从多样化LLMs获取潜在幻觉答案，使用GPT-4o通过对比黄金答案和检索上下文自动标注幻觉跨度。

Result: 评估了多种幻觉检测方法，发现基于编码器的模型在所有语言中表现最强，PsiloQA展示了有效的跨语言泛化能力，并能稳健地迁移到其他基准。

Conclusion: PsiloQA数据集和结果推动了多语言环境下可扩展、细粒度幻觉检测的发展，相比人工标注数据集更具成本效益。

Abstract: Hallucination detection remains a fundamental challenge for the safe and
reliable deployment of large language models (LLMs), especially in applications
requiring factual accuracy. Existing hallucination benchmarks often operate at
the sequence level and are limited to English, lacking the fine-grained,
multilingual supervision needed for a comprehensive evaluation. In this work,
we introduce PsiloQA, a large-scale, multilingual dataset annotated with
span-level hallucinations across 14 languages. PsiloQA is constructed through
an automated three-stage pipeline: generating question-answer pairs from
Wikipedia using GPT-4o, eliciting potentially hallucinated answers from diverse
LLMs in a no-context setting, and automatically annotating hallucinated spans
using GPT-4o by comparing against golden answers and retrieved context. We
evaluate a wide range of hallucination detection methods -- including
uncertainty quantification, LLM-based tagging, and fine-tuned encoder models --
and show that encoder-based models achieve the strongest performance across
languages. Furthermore, PsiloQA demonstrates effective cross-lingual
generalization and supports robust knowledge transfer to other benchmarks, all
while being significantly more cost-efficient than human-annotated datasets.
Our dataset and results advance the development of scalable, fine-grained
hallucination detection in multilingual settings.

</details>


### [95] [Detecting Distillation Data from Reasoning Models](https://arxiv.org/abs/2510.04850)
*Hengxiang Zhang,Hyeong Kyu Choi,Yixuan Li,Hongxin Wei*

Main category: cs.CL

TL;DR: 提出了一种名为Token Probability Deviation (TBD)的新方法，用于检测推理蒸馏过程中的数据污染问题，通过分析生成token的概率偏差来区分已见过和未见过的数据。


<details>
  <summary>Details</summary>
Motivation: 推理蒸馏可能无意中导致基准污染，评估数据包含在蒸馏数据集中会夸大蒸馏模型的性能指标，需要检测这种数据污染。

Method: 提出Token Probability Deviation (TBD)方法，利用生成输出token的概率模式，量化生成token概率与高参考概率的偏差程度。

Result: 在S1数据集上实现了0.918的AUC和0.470的TPR@1% FPR，表现出优越的检测性能。

Conclusion: TBD方法能有效检测推理蒸馏中的数据污染问题，通过分析token概率偏差来区分已见和未见数据，具有实际应用价值。

Abstract: Reasoning distillation has emerged as an efficient and powerful paradigm for
enhancing the reasoning capabilities of large language models. However,
reasoning distillation may inadvertently cause benchmark contamination, where
evaluation data included in distillation datasets can inflate performance
metrics of distilled models. In this work, we formally define the task of
distillation data detection, which is uniquely challenging due to the partial
availability of distillation data. Then, we propose a novel and effective
method Token Probability Deviation (TBD), which leverages the probability
patterns of the generated output tokens. Our method is motivated by the
analysis that distilled models tend to generate near-deterministic tokens for
seen questions, while producing more low-probability tokens for unseen
questions. Our key idea behind TBD is to quantify how far the generated tokens'
probabilities deviate from a high reference probability. In effect, our method
achieves competitive detection performance by producing lower scores for seen
questions than for unseen questions. Extensive experiments demonstrate the
effectiveness of our method, achieving an AUC of 0.918 and a TPR@1% FPR of
0.470 on the S1 dataset.

</details>


### [96] [SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful Requests](https://arxiv.org/abs/2510.04891)
*Punya Syon Pandey,Hai Son Le,Devansh Bhardwaj,Rada Mihalcea,Zhijing Jin*

Main category: cs.CL

TL;DR: SocialHarmBench是一个包含585个提示的数据集，涵盖7个社会政治类别和34个国家，用于测试LLM在政治敏感情境中的脆弱性。研究发现开源模型在有害合规方面存在高风险，Mistral-7B在历史修正主义、宣传和政治操纵等领域的攻击成功率高达97%-98%。


<details>
  <summary>Details</summary>
Motivation: 现有安全基准很少测试LLM在政治操纵、宣传和虚假信息生成、监控和信息控制等领域的漏洞，而这些失败可能产生直接的社会政治后果。

Method: 构建SocialHarmBench数据集，包含585个提示，涵盖7个社会政治类别和34个国家，评估LLM在政治敏感情境中的表现。

Result: 开源模型对有害合规表现出高脆弱性，Mistral-7B在历史修正主义、宣传和政治操纵领域的攻击成功率高达97%-98%。时间地理分析显示LLM在21世纪和前20世纪情境下最脆弱，对拉丁美洲、美国和英国相关提示响应问题最多。

Conclusion: 当前的安全措施无法推广到高风险社会政治环境中，暴露了系统性偏见，引发了对LLM在保护人权和民主价值观方面可靠性的担忧。

Abstract: Large language models (LLMs) are increasingly deployed in contexts where
their failures can have direct sociopolitical consequences. Yet, existing
safety benchmarks rarely test vulnerabilities in domains such as political
manipulation, propaganda and disinformation generation, or surveillance and
information control. We introduce SocialHarmBench, a dataset of 585 prompts
spanning 7 sociopolitical categories and 34 countries, designed to surface
where LLMs most acutely fail in politically charged contexts. Our evaluations
reveal several shortcomings: open-weight models exhibit high vulnerability to
harmful compliance, with Mistral-7B reaching attack success rates as high as
97% to 98% in domains such as historical revisionism, propaganda, and political
manipulation. Moreover, temporal and geographic analyses show that LLMs are
most fragile when confronted with 21st-century or pre-20th-century contexts,
and when responding to prompts tied to regions such as Latin America, the USA,
and the UK. These findings demonstrate that current safeguards fail to
generalize to high-stakes sociopolitical settings, exposing systematic biases
and raising concerns about the reliability of LLMs in preserving human rights
and democratic values. We share the SocialHarmBench benchmark at
https://huggingface.co/datasets/psyonp/SocialHarmBench.

</details>


### [97] [Do LLMs Align with My Task? Evaluating Text-to-SQL via Dataset Alignment](https://arxiv.org/abs/2510.04919)
*Davood Rafiei,Morgan Lindsay Heisler,Weiwei Zhang,Mohammadreza Pourreza,Yong Zhang*

Main category: cs.CL

TL;DR: 本文研究了NL2SQL任务中训练数据与目标查询的结构对齐问题，发现结构对齐是预测微调成功的关键指标。


<details>
  <summary>Details</summary>
Motivation: 监督微调(SFT)在适应大语言模型到下游任务时有效，但训练数据的变异性会阻碍模型的跨领域泛化能力。

Method: 通过比较训练集、目标数据和模型预测中SQL结构特征的分布来估计对齐程度，并在三个大型跨领域NL2SQL基准和多个模型家族上进行实验。

Result: 结构对齐是微调成功的强预测因子：对齐度高时SFT带来显著的准确性和SQL生成质量提升；对齐度低时改进有限或没有改进。

Conclusion: 这些发现强调了在NL2SQL任务中采用对齐感知的数据选择对于有效微调和泛化的重要性。

Abstract: Supervised Fine-Tuning (SFT) is an effective method for adapting Large
Language Models (LLMs) on downstream tasks. However, variability in training
data can hinder a model's ability to generalize across domains. This paper
studies the problem of dataset alignment for Natural Language to SQL (NL2SQL or
text to SQL), examining how well SFT training data matches the structural
characteristics of target queries and how this alignment impacts model
performance. We hypothesize that alignment can be accurately estimated by
comparing the distributions of structural SQL features across the training set,
target data, and the model's predictions prior to SFT. Through comprehensive
experiments on three large cross-domain NL2SQL benchmarks and multiple model
families, we show that structural alignment is a strong predictor of
fine-tuning success. When alignment is high, SFT yields substantial gains in
accuracy and SQL generation quality; when alignment is low, improvements are
marginal or absent. These findings highlight the importance of alignment-aware
data selection for effective fine-tuning and generalization in NL2SQL tasks.

</details>


### [98] [The Geometry of Truth: Layer-wise Semantic Dynamics for Hallucination Detection in Large Language Models](https://arxiv.org/abs/2510.04933)
*Amir Hameed Mir*

Main category: cs.CL

TL;DR: LSD是一种基于几何框架的幻觉检测方法，通过分析transformer层间隐藏状态语义的演变来检测大语言模型中的幻觉现象，无需多次采样或外部验证，仅需单次前向传播即可实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 大语言模型经常产生流畅但事实错误的陈述（幻觉现象），这在高风险领域构成严重威胁，需要开发内在的检测方法来识别这些幻觉。

Method: 使用基于边缘的对比学习，将隐藏激活与事实编码器生成的ground-truth嵌入对齐，分析语义轨迹的分离：事实响应保持稳定对齐，而幻觉在不同深度表现出明显的语义漂移。

Result: 在TruthfulQA和合成事实-幻觉数据集上的评估显示，LSD达到F1分数0.92、AUROC 0.96和聚类准确率0.89，优于SelfCheckGPT和语义熵基线方法，同时实现5-20倍的速度提升。

Conclusion: LSD提供了一种可扩展、模型无关的实时幻觉监测机制，并为理解大语言模型中事实一致性的几何特性提供了新见解。

Abstract: Large Language Models (LLMs) often produce fluent yet factually incorrect
statements-a phenomenon known as hallucination-posing serious risks in
high-stakes domains. We present Layer-wise Semantic Dynamics (LSD), a geometric
framework for hallucination detection that analyzes the evolution of
hidden-state semantics across transformer layers. Unlike prior methods that
rely on multiple sampling passes or external verification sources, LSD operates
intrinsically within the model's representational space. Using margin-based
contrastive learning, LSD aligns hidden activations with ground-truth
embeddings derived from a factual encoder, revealing a distinct separation in
semantic trajectories: factual responses preserve stable alignment, while
hallucinations exhibit pronounced semantic drift across depth. Evaluated on the
TruthfulQA and synthetic factual-hallucination datasets, LSD achieves an
F1-score of 0.92, AUROC of 0.96, and clustering accuracy of 0.89, outperforming
SelfCheckGPT and Semantic Entropy baselines while requiring only a single
forward pass. This efficiency yields a 5-20x speedup over sampling-based
methods without sacrificing precision or interpretability. LSD offers a
scalable, model-agnostic mechanism for real-time hallucination monitoring and
provides new insights into the geometry of factual consistency within large
language models.

</details>


### [99] [A First Context-Free Grammar Applied to Nawatl Corpora Augmentation](https://arxiv.org/abs/2510.04945)
*Juan-José Guzmán-Landa,Juan-Manuel Torres-Moreno,Miguel Figueroa-Saavedra,Ligia Quintana-Torres,Martha-Lorena Avendaño-Garrido,Graham Ranger*

Main category: cs.CL

TL;DR: 为纳瓦特尔语开发上下文无关语法以生成人工句子，扩充语料库用于语言模型训练，初步结果显示相比某些LLM有改进，但需要更有效的语法模型。


<details>
  <summary>Details</summary>
Motivation: 纳瓦特尔语是数字资源稀少的π语言类型，缺乏机器学习可用的语料库，需要生成大量语法正确的人工句子来扩充训练数据。

Method: 为纳瓦特尔语构建上下文无关语法(CFG)，生成人工句子来扩展π-yalli语料库，然后使用FastText等算法进行训练。

Result: 通过语法生成句子扩充语料库后，在句子级语义任务上相比某些大型语言模型(LLM)取得了比较性改进。

Conclusion: 使用语法可以扩展语料库并改善语言模型性能，但要获得更显著的改进，需要构建更有效的纳瓦特尔语语法模型。

Abstract: In this article we introduce a context-free grammar (CFG) for the Nawatl
language. Nawatl (or Nahuatl) is an Amerindian language of the $\pi$-language
type, i.e. a language with few digital resources, in which the corpora
available for machine learning are virtually non-existent. The objective here
is to generate a significant number of grammatically correct artificial
sentences, in order to increase the corpora available for language model
training. We want to show that a grammar enables us significantly to expand a
corpus in Nawatl which we call $\pi$-\textsc{yalli}. The corpus, thus enriched,
enables us to train algorithms such as FastText and to evaluate them on
sentence-level semantic tasks. Preliminary results show that by using the
grammar, comparative improvements are achieved over some LLMs. However, it is
observed that to achieve more significant improvement, grammars that model the
Nawatl language even more effectively are required.

</details>


### [100] [Mind Your Tone: Investigating How Prompt Politeness Affects LLM Accuracy (short paper)](https://arxiv.org/abs/2510.04950)
*Om Dobariya,Akhil Kumar*

Main category: cs.CL

TL;DR: 研究发现不礼貌的提示词比礼貌提示词在LLM中表现更好，准确率从非常礼貌的80.8%提升到非常粗鲁的84.8%，与传统认知相反。


<details>
  <summary>Details</summary>
Motivation: 探索自然语言提示中礼貌程度和语气对大型语言模型性能的影响，特别是多选问题中的准确性。

Method: 创建50个基础问题，每个问题重写为5种语气变体（非常礼貌、礼貌、中性、粗鲁、非常粗鲁），使用ChatGPT 4o评估响应，并进行配对样本t检验。

Result: 不礼貌提示词表现优于礼貌提示词，准确率从非常礼貌的80.8%到非常粗鲁的84.8%，与早期研究结果相反。

Conclusion: 新LLM对语气变化的响应方式可能不同，强调研究提示语用学的重要性，并引发对人机交互社会维度的思考。

Abstract: The wording of natural language prompts has been shown to influence the
performance of large language models (LLMs), yet the role of politeness and
tone remains underexplored. In this study, we investigate how varying levels of
prompt politeness affect model accuracy on multiple-choice questions. We
created a dataset of 50 base questions spanning mathematics, science, and
history, each rewritten into five tone variants: Very Polite, Polite, Neutral,
Rude, and Very Rude, yielding 250 unique prompts. Using ChatGPT 4o, we
evaluated responses across these conditions and applied paired sample t-tests
to assess statistical significance. Contrary to expectations, impolite prompts
consistently outperformed polite ones, with accuracy ranging from 80.8% for
Very Polite prompts to 84.8% for Very Rude prompts. These findings differ from
earlier studies that associated rudeness with poorer outcomes, suggesting that
newer LLMs may respond differently to tonal variation. Our results highlight
the importance of studying pragmatic aspects of prompting and raise broader
questions about the social dimensions of human-AI interaction.

</details>


### [101] [AWARE, Beyond Sentence Boundaries: A Contextual Transformer Framework for Identifying Cultural Capital in STEM Narratives](https://arxiv.org/abs/2510.04983)
*Khalid Mehtab Khan,Anagha Kulkarni*

Main category: cs.CL

TL;DR: AWARE框架通过提升Transformer模型在三个方面的意识（领域意识、上下文意识、类别重叠意识），显著提高了从学生反思中识别文化资本主题的能力。


<details>
  <summary>Details</summary>
Motivation: 识别学生反思中的文化资本主题有助于促进公平学习环境，但这些主题通常以叙事方式呈现而非直接关键词，标准NLP模型难以检测。

Method: 提出AWARE框架，包含三个核心组件：领域意识（适应学生反思的语言风格）、上下文意识（生成考虑全文的句子嵌入）、类别重叠意识（采用多标签策略识别主题共存）。

Result: AWARE在Macro-F1上比强基线提高了2.1个百分点，在所有主题上都显示出显著改进。

Conclusion: 这项工作为任何依赖叙事上下文的文本分类任务提供了稳健且可推广的方法论。

Abstract: Identifying cultural capital (CC) themes in student reflections can offer
valuable insights that help foster equitable learning environments in
classrooms. However, themes such as aspirational goals or family support are
often woven into narratives, rather than appearing as direct keywords. This
makes them difficult to detect for standard NLP models that process sentences
in isolation. The core challenge stems from a lack of awareness, as standard
models are pre-trained on general corpora, leaving them blind to the
domain-specific language and narrative context inherent to the data. To address
this, we introduce AWARE, a framework that systematically attempts to improve a
transformer model's awareness for this nuanced task. AWARE has three core
components: 1) Domain Awareness, adapting the model's vocabulary to the
linguistic style of student reflections; 2) Context Awareness, generating
sentence embeddings that are aware of the full essay context; and 3) Class
Overlap Awareness, employing a multi-label strategy to recognize the
coexistence of themes in a single sentence. Our results show that by making the
model explicitly aware of the properties of the input, AWARE outperforms a
strong baseline by 2.1 percentage points in Macro-F1 and shows considerable
improvements across all themes. This work provides a robust and generalizable
methodology for any text classification task in which meaning depends on the
context of the narrative.

</details>


### [102] [Resource-Efficient Fine-Tuning of LLaMA-3.2-3B for Medical Chain-of-Thought Reasoning](https://arxiv.org/abs/2510.05003)
*Imran Mansha*

Main category: cs.CL

TL;DR: 提出了一种资源高效的LLaMA-3.2-3B微调方法，使用LoRA和QLoRA技术在有限GPU和内存条件下增强医学链式推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型如GPT-4和LLaMA具有强大的推理能力，但全参数微调需要大量计算资源，限制了在资源受限环境中的应用。

Method: 采用参数高效微调技术（LoRA和QLoRA），在公开医学推理数据集上对LLaMA-3.2-3B模型进行适配。

Result: 模型在保持推理能力的同时，内存使用量比标准全微调减少高达60%，推理连贯性和事实准确性得到提升。

Conclusion: 轻量级适配方法可以在医学问答任务中保持强大的推理能力，为低资源研究环境中部署LLM提供了实用策略。

Abstract: Large Language Models (LLMs) such as GPT-4 and LLaMA have demonstrated
remarkable reasoning abilities but require significant computational resources
for fine-tuning. This paper presents a resource-efficient fine-tuning approach
for LLaMA-3.2-3B to enhance medical chain-of-thought reasoning while operating
under constrained GPU and memory settings. Using parameter-efficient tuning
techniques such as LoRA and QLoRA, we adapt the base model on publicly
available medical reasoning datasets. The model achieves improved reasoning
coherence and factual accuracy while reducing memory usage by up to 60%
compared to standard full fine-tuning. Experimental evaluation demonstrates
that lightweight adaptations can retain strong reasoning capability in medical
question-answering tasks. This work highlights practical strategies for
deploying LLMs in low-resource research environments and provides insights into
balancing efficiency and domain specialization for medical AI systems.

</details>


### [103] [Imperceptible Jailbreaking against Large Language Models](https://arxiv.org/abs/2510.05025)
*Kuofeng Gao,Yiming Li,Chao Du,Xin Wang,Xingjun Ma,Shu-Tao Xia,Tianyu Pang*

Main category: cs.CL

TL;DR: 提出了一种利用Unicode变体选择器实现不可感知越狱攻击的方法，通过在恶意问题后附加不可见的变体选择器字符，使提示在视觉上与原问题相同但token化被秘密改变，从而诱导有害响应。


<details>
  <summary>Details</summary>
Motivation: 视觉模态的越狱攻击通常依赖不可感知的对抗扰动，而文本模态攻击通常需要可见修改。本文旨在探索文本模态的不可感知越狱攻击可能性。

Method: 使用Unicode变体选择器字符，提出链式搜索流水线生成对抗性后缀，改变token化但不产生可见修改。

Result: 实验显示该方法在四个对齐LLM上实现高攻击成功率，并能泛化到提示注入攻击。

Conclusion: 证明了文本模态也存在不可感知越狱攻击风险，揭示了当前LLM安全机制的新漏洞。

Abstract: Jailbreaking attacks on the vision modality typically rely on imperceptible
adversarial perturbations, whereas attacks on the textual modality are
generally assumed to require visible modifications (e.g., non-semantic
suffixes). In this paper, we introduce imperceptible jailbreaks that exploit a
class of Unicode characters called variation selectors. By appending invisible
variation selectors to malicious questions, the jailbreak prompts appear
visually identical to original malicious questions on screen, while their
tokenization is "secretly" altered. We propose a chain-of-search pipeline to
generate such adversarial suffixes to induce harmful responses. Our experiments
show that our imperceptible jailbreaks achieve high attack success rates
against four aligned LLMs and generalize to prompt injection attacks, all
without producing any visible modifications in the written prompt. Our code is
available at https://github.com/sail-sg/imperceptible-jailbreaks.

</details>


### [104] [A Set of Quebec-French Corpus of Regional Expressions and Terms](https://arxiv.org/abs/2510.05026)
*David Beauchemin,Yan Tremblay,Mohamed Amine Youssef,Richard Khoury*

Main category: cs.CL

TL;DR: 提出了两个新的魁北克法语方言习语理解基准数据集QFrCoRE和QFrCoRT，用于测试LLM在特定方言中的熟练度。


<details>
  <summary>Details</summary>
Motivation: 将习语理解和方言理解两个任务结合起来，使用地区性习语作为方言理解的测试标准。

Method: 构建了两个魁北克法语方言习语数据集：QFrCoRE包含4,633个习语短语实例，QFrCoRT包含171个地区性习语词汇实例，并提供了可复现的构建方法。

Result: 通过对94个LLM的实验表明，地区性习语基准是衡量模型在特定方言中熟练度的可靠工具。

Conclusion: 地区性习语可以作为有效的方言理解评估基准，该方法可扩展到其他方言的研究。

Abstract: The tasks of idiom understanding and dialect understanding are both
well-established benchmarks in natural language processing. In this paper, we
propose combining them, and using regional idioms as a test of dialect
understanding. Towards this end, we propose two new benchmark datasets for the
Quebec dialect of French: QFrCoRE, which contains 4,633 instances of idiomatic
phrases, and QFrCoRT, which comprises 171 regional instances of idiomatic
words. We explain how to construct these corpora, so that our methodology can
be replicated for other dialects. Our experiments with 94 LLM demonstrate that
our regional idiom benchmarks are a reliable tool for measuring a model's
proficiency in a specific dialect.

</details>


### [105] [Guided Query Refinement: Multimodal Hybrid Retrieval with Test-Time Optimization](https://arxiv.org/abs/2510.05038)
*Omri Uzan,Asaf Yehudai,Roi pony,Eyal Shnarch,Ariel Gera*

Main category: cs.CL

TL;DR: 提出了一种名为GQR的测试时优化方法，通过使用互补检索器的分数来优化主检索器的查询嵌入，从而在视觉文档检索中实现性能与效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 当前多模态编码器在视觉文档检索中虽然性能优越，但存在表示规模过大、部署困难的问题，且纯视觉方法可能受到模态差距的限制。

Method: 引入引导查询优化(GQR)方法，在测试时使用互补检索器的分数来优化主检索器的查询嵌入，实现细粒度的模型交互。

Result: 实验表明GQR能让视觉中心模型达到更大表示模型的性能，同时速度提升14倍，内存需求减少54倍。

Conclusion: GQR有效推动了多模态检索在性能与效率方面的帕累托前沿，为实际部署提供了可行方案。

Abstract: Multimodal encoders have pushed the boundaries of visual document retrieval,
matching textual query tokens directly to image patches and achieving
state-of-the-art performance on public benchmarks. Recent models relying on
this paradigm have massively scaled the sizes of their query and document
representations, presenting obstacles to deployment and scalability in
real-world pipelines. Furthermore, purely vision-centric approaches may be
constrained by the inherent modality gap still exhibited by modern
vision-language models. In this work, we connect these challenges to the
paradigm of hybrid retrieval, investigating whether a lightweight dense text
retriever can enhance a stronger vision-centric model. Existing hybrid methods,
which rely on coarse-grained fusion of ranks or scores, fail to exploit the
rich interactions within each model's representation space. To address this, we
introduce Guided Query Refinement (GQR), a novel test-time optimization method
that refines a primary retriever's query embedding using guidance from a
complementary retriever's scores. Through extensive experiments on visual
document retrieval benchmarks, we demonstrate that GQR allows vision-centric
models to match the performance of models with significantly larger
representations, while being up to 14x faster and requiring 54x less memory.
Our findings show that GQR effectively pushes the Pareto frontier for
performance and efficiency in multimodal retrieval. We release our code at
https://github.com/IBM/test-time-hybrid-retrieval

</details>


### [106] [COLE: a Comprehensive Benchmark for French Language Understanding Evaluation](https://arxiv.org/abs/2510.05046)
*David Beauchemin,Yan Tremblay,Mohamed Amine Youssef,Richard Khoury*

Main category: cs.CL

TL;DR: COLE是一个新的法语自然语言理解基准测试，包含23个多样化任务，评估了94个大语言模型，揭示了闭源和开源模型之间的性能差距，并识别了当前LLM面临的挑战性前沿问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决法语自然语言理解评估不够全面的问题，需要创建一个覆盖广泛NLU能力的基准测试，特别关注法语特有的语言现象。

Method: 构建COLE基准测试，包含23个多样化任务，涵盖情感分析、复述检测、语法判断和推理等能力，并对94个大语言模型进行基准测试。

Result: 结果显示了闭源和开源模型之间的显著性能差距，识别了零样本抽取式问答、细粒度词义消歧和区域语言变体理解等关键挑战性前沿问题。

Conclusion: COLE作为公共资源发布，旨在促进法语语言建模的进一步发展。

Abstract: To address the need for a more comprehensive evaluation of French Natural
Language Understanding (NLU), we introduce COLE, a new benchmark composed of 23
diverse task covering a broad range of NLU capabilities, including sentiment
analysis, paraphrase detection, grammatical judgment, and reasoning, with a
particular focus on linguistic phenomena relevant to the French language. We
benchmark 94 large language models (LLM), providing an extensive analysis of
the current state of French NLU. Our results highlight a significant
performance gap between closed- and open-weights models and identify key
challenging frontiers for current LLMs, such as zero-shot extractive
question-answering (QA), fine-grained word sense disambiguation, and
understanding of regional language variations. We release COLE as a public
resource to foster further progress in French language modelling.

</details>


### [107] [SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs](https://arxiv.org/abs/2510.05069)
*Dachuan Shi,Abedelkadir Asi,Keying Li,Xiangchi Yuan,Leyan Pan,Wenke Lee,Wen Xiao*

Main category: cs.CL

TL;DR: SwiReasoning是一个无需训练的LLM推理框架，通过在显式和隐式推理之间动态切换来平衡探索与利用，提高准确性和token效率。


<details>
  <summary>Details</summary>
Motivation: 解决隐式推理面临的两个挑战：1) 纯隐式推理会扩散概率质量，引入噪声，阻碍收敛到高置信度解；2) 过度思考问题持续存在，浪费token降低效率。

Method: SwiReasoning框架包含两个关键创新：1) 基于熵趋势估计块级置信度，动态切换显式和隐式推理；2) 限制思考块切换的最大次数来控制过度思考。

Result: 在数学和STEM基准测试中，SwiReasoning将平均准确率提高了1.5%-2.8%，在受限预算下token效率提高了56%-79%，预算越紧增益越大。

Conclusion: SwiReasoning通过动态推理切换机制有效解决了隐式推理的收敛问题和过度思考问题，显著提升了LLM推理的准确性和效率。

Abstract: Recent work shows that, beyond discrete reasoning through explicit
chain-of-thought steps, which are limited by the boundaries of natural
languages, large language models (LLMs) can also reason continuously in latent
space, allowing richer information per step and thereby improving token
efficiency. Despite this promise, latent reasoning still faces two challenges,
especially in training-free settings: 1) purely latent reasoning broadens the
search distribution by maintaining multiple implicit paths, which diffuses
probability mass, introduces noise, and impedes convergence to a single
high-confidence solution, thereby hurting accuracy; and 2) overthinking
persists even without explicit text, wasting tokens and degrading efficiency.
To address these issues, we introduce SwiReasoning, a training-free framework
for LLM reasoning which features two key innovations: 1) SwiReasoning
dynamically switches between explicit and latent reasoning, guided by
block-wise confidence estimated from entropy trends in next-token
distributions, to balance exploration and exploitation and promote timely
convergence. 2) By limiting the maximum number of thinking-block switches,
SwiReasoning curbs overthinking and improves token efficiency across varying
problem difficulties. On widely used mathematics and STEM benchmarks,
SwiReasoning consistently improves average accuracy by 1.5%-2.8% across
reasoning LLMs of different model families and scales. Furthermore, under
constrained budgets, SwiReasoning improves average token efficiency by 56%-79%,
with larger gains as budgets tighten.

</details>


### [108] [Slm-mux: Orchestrating small language models for reasoning](https://arxiv.org/abs/2510.05077)
*Chenyu Wang,Zishen Wan,Hao Kang,Emma Chen,Zhiqiang Xie,Tushar Krishna,Vijay Janapa Reddi,Yilun Du*

Main category: cs.CL

TL;DR: 提出了一种名为SLM-MUX的三阶段方法，用于有效协调多个小语言模型(SLMs)，通过模型选择搜索和测试时缩放策略，在多个基准测试中显著优于现有编排方法。


<details>
  <summary>Details</summary>
Motivation: 随着小语言模型数量的快速增长，虽然单个模型无法达到最先进精度，但它们在特定任务上表现出色且更高效。需要探索如何将多个SLMs有效编排成一个系统，实现比任何单个模型更高的准确性。

Method: 提出三阶段方法：1) SLM-MUX多模型架构协调多个SLMs；2) 模型选择搜索从候选池中识别最具互补性的SLMs；3) 针对SLM-MUX的测试时缩放策略。

Result: 在MATH上提升13.4%，GPQA上提升8.8%，GSM8K上提升7.0%。仅用两个SLMs就在GPQA和GSM8K上超越Qwen 2.5 72B，在MATH上与之持平。

Conclusion: 通过所提出的方法，SLMs可以被有效编排成更准确和高效的系统，并提供了理论分析来证实该方法的优势。

Abstract: With the rapid development of language models, the number of small language
models (SLMs) has grown significantly. Although they do not achieve
state-of-the-art accuracy, they are more efficient and often excel at specific
tasks. This raises a natural question: can multiple SLMs be orchestrated into a
system where each contributes effectively, achieving higher accuracy than any
individual model? Existing orchestration methods have primarily targeted
frontier models (e.g., GPT-4) and perform suboptimally when applied to SLMs. To
address this gap, we propose a three-stage approach for orchestrating SLMs.
First, we introduce SLM-MUX, a multi-model architecture that effectively
coordinates multiple SLMs. Building on this, we develop two optimization
strategies: (i) a model selection search that identifies the most complementary
SLMs from a given pool, and (ii) test-time scaling tailored to SLM-MUX. Our
approach delivers strong results: Compared to existing orchestration methods,
our approach achieves up to 13.4% improvement on MATH, 8.8% on GPQA, and 7.0%
on GSM8K. With just two SLMS, SLM-MUX outperforms Qwen 2.5 72B on GPQA and
GSM8K, and matches its performance on MATH. We further provide theoretical
analyses to substantiate the advantages of our method. In summary, we
demonstrate that SLMs can be effectively orchestrated into more accurate and
efficient systems through the proposed approach.

</details>


### [109] [TeachLM: Post-Training LLMs for Education Using Authentic Learning Data](https://arxiv.org/abs/2510.05087)
*Janos Perczel,Jin Chow,Dorottya Demszky*

Main category: cs.CL

TL;DR: TeachLM通过参数高效微调优化LLM用于教学，利用真实学生-导师对话数据生成高质量合成对话，显著提升教学对话能力


<details>
  <summary>Details</summary>
Motivation: 解决生成式AI在教育应用中的局限性，特别是缺乏反映真实学生学习过程的高质量训练数据，以及提示工程在编码复杂教学策略方面的不足

Method: 使用100,000小时真实学生-导师对话数据进行参数高效微调，开发真实学生模型以生成高保真合成对话，并提出基于合成对话的多轮评估协议

Result: 微调后显著改善对话和教学表现：学生发言时间翻倍、提问风格改进、对话轮次增加50%、教学个性化程度更高

Conclusion: 在真实学习数据上进行微调能显著提升LLM的教学对话能力，为教育AI发展提供有效路径

Abstract: The promise of generative AI to revolutionize education is constrained by the
pedagogical limits of large language models (LLMs). A major issue is the lack
of access to high-quality training data that reflect the learning of actual
students. Prompt engineering has emerged as a stopgap, but the ability of
prompts to encode complex pedagogical strategies in rule-based natural language
is inherently limited. To address this gap we introduce TeachLM - an LLM
optimized for teaching through parameter-efficient fine-tuning of
state-of-the-art models. TeachLM is trained on a dataset comprised of 100,000
hours of one-on-one, longitudinal student-tutor interactions maintained by
Polygence, which underwent a rigorous anonymization process to protect privacy.
We use parameter-efficient fine-tuning to develop an authentic student model
that enables the generation of high-fidelity synthetic student-tutor dialogues.
Building on this capability, we propose a novel multi-turn evaluation protocol
that leverages synthetic dialogue generation to provide fast, scalable, and
reproducible assessments of the dialogical capabilities of LLMs. Our
evaluations demonstrate that fine-tuning on authentic learning data
significantly improves conversational and pedagogical performance - doubling
student talk time, improving questioning style, increasing dialogue turns by
50%, and greater personalization of instruction.

</details>


### [110] [Finish First, Perfect Later: Test-Time Token-Level Cross-Validation for Diffusion Large Language Models](https://arxiv.org/abs/2510.05090)
*Runchu Tian,Junxia Cui,Xueqiang Xu,Feng Yao,Jingbo Shang*

Main category: cs.CL

TL;DR: 提出了Tolerator解码策略，通过令牌级交叉验证改进扩散大语言模型的解码过程，允许修正已接受的令牌，提升输出质量。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型的传统解码策略存在关键限制：一旦令牌被接受就无法在后续步骤中修正，导致早期错误持续影响最终输出质量。

Method: 提出Tolerator解码策略，采用两阶段过程：(i)序列填充和(ii)通过重新掩码和解码令牌子集进行迭代精炼，同时将剩余令牌作为上下文。

Result: 在五个标准基准测试（语言理解、代码生成和数学）上的实验表明，该方法在相同计算预算下相比基线模型实现了持续改进。

Conclusion: 解码算法对于充分发挥扩散大语言模型的潜力至关重要，Tolerator策略能够产生更可靠的扩散解码输出。

Abstract: Diffusion large language models (dLLMs) have recently emerged as a promising
alternative to autoregressive (AR) models, offering advantages such as
accelerated parallel decoding and bidirectional context modeling. However, the
vanilla decoding strategy in discrete dLLMs suffers from a critical limitation:
once a token is accepted, it can no longer be revised in subsequent steps. As a
result, early mistakes persist across iterations, harming both intermediate
predictions and final output quality. To address this issue, we propose
Tolerator (Token-Level Cross-Validation Refinement), a training-free decoding
strategy that leverages cross-validation among predicted tokens. Unlike
existing methods that follow a single progressive unmasking procedure,
Tolerator introduces a two-stage process: (i) sequence fill-up and (ii)
iterative refinement by remasking and decoding a subset of tokens while
treating the remaining as context. This design enables previously accepted
tokens to be reconsidered and corrected when necessary, leading to more
reliable diffusion decoding outputs. We evaluate Tolerator on five standard
benchmarks covering language understanding, code generation, and mathematics.
Experiments show that our method achieves consistent improvements over the
baselines under the same computational budget. These findings suggest that
decoding algorithms are crucial to realizing the full potential of diffusion
large language models. Code and data are publicly available.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [111] [WAREX: Web Agent Reliability Evaluation on Existing Benchmarks](https://arxiv.org/abs/2510.03285)
*Su Kara,Fazle Faisal,Suman Nath*

Main category: cs.AI

TL;DR: WAREX是一个评估浏览器LLM代理在真实网络环境中可靠性的框架，通过在现有基准测试中引入网络不稳定性和网站攻击等现实因素，显著降低了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在受控环境中评估LLM代理性能，忽略了真实世界中网络不稳定、HTTPS连接问题和网站攻击等现实挑战，无法准确反映代理在实际应用中的可靠性。

Method: 在三个流行基准测试（WebArena、WebVoyager和REAL）中引入WAREX框架，模拟网络不稳定、服务器问题、系统故障以及跨站脚本攻击等现实环境因素。

Result: 实验显示引入WAREX后，任务成功率显著下降，表明现有最先进的LLM代理在真实环境中的鲁棒性有限。

Conclusion: 当前LLM代理在受控环境中表现良好，但在面对真实世界的不稳定性和安全威胁时可靠性不足，需要开发更鲁棒的代理系统。

Abstract: Recent advances in browser-based LLM agents have shown promise for automating
tasks ranging from simple form filling to hotel booking or online shopping.
Current benchmarks measure agent performance in controlled environments, such
as containers or stable networks, where websites behave deterministically.
However, in the real world, users access websites over networks and HTTPS
connections that introduce instability from multiple sources: client-side,
server-side issues or broader system failures. Moreover, live websites are
prone to web attacks such Cross-Site Scripting, as well as general site
modifications which can cause unexpected or malicious pop-ups or improper
functionality. To address this gap, we present WAREX: Web Agent Reliability
Evaluation on Existing Benchmarks. We measure the impact of WAREX across three
popular benchmarks: WebArena, WebVoyager, and REAL. Our experiments show that
introducing WAREX leads to significant drops in task success rates,
highlighting the limited robustness of state-of-the-art agents.

</details>


### [112] [Refined Iterated Pareto Greedy for Energy-aware Hybrid Flowshop Scheduling with Blocking Constraints](https://arxiv.org/abs/2510.03377)
*Ahmed Missaoui,Cemalettin Ozturk,Barry O'Sullivan*

Main category: cs.AI

TL;DR: 研究混合流水车间调度问题，考虑阻塞约束，同时优化完工时间和能耗，提出多目标优化方法和元启发式算法。


<details>
  <summary>Details</summary>
Motivation: 不可再生能源稀缺、地缘政治问题、价格上涨和气候变化迫使制造业寻求节能解决方案，混合流水车间调度是快速见效的节能方法。

Method: 建立多目标混合整数规划模型，采用增强ε约束方法求帕累托最优解，开发精炼迭代帕累托贪婪算法处理大规模问题。

Result: 通过小、中、大规模实例验证算法有效性，与现有算法对比显示所提方法表现优异。

Conclusion: 提出的多目标优化方法和元启发式算法能有效解决混合流水车间调度中的能耗和完工时间冲突问题。

Abstract: The scarcity of non-renewable energy sources, geopolitical problems in its
supply, increasing prices, and the impact of climate change, force the global
economy to develop more energy-efficient solutions for their operations. The
Manufacturing sector is not excluded from this challenge as one of the largest
consumers of energy. Energy-efficient scheduling is a method that attracts
manufacturing companies to reduce their consumption as it can be quickly
deployed and can show impact immediately. In this study, the hybrid flow shop
scheduling problem with blocking constraint (BHFS) is investigated in which we
seek to minimize the latest completion time (i.e. makespan) and overall energy
consumption, a typical manufacturing setting across many industries from
automotive to pharmaceutical. Energy consumption and the latest completion time
of customer orders are usually conflicting objectives. Therefore, we first
formulate the problem as a novel multi-objective mixed integer programming
(MIP) model and propose an augmented epsilon-constraint method for finding the
Pareto-optimal solutions. Also, an effective multi-objective metaheuristic
algorithm. Refined Iterated Pareto Greedy (RIPG), is developed to solve large
instances in reasonable time. Our proposed methods are benchmarked using small,
medium, and large-size instances to evaluate their efficiency. Two well-known
algorithms are adopted for comparing our novel approaches. The computational
results show the effectiveness of our method.

</details>


### [113] [Know Thyself? On the Incapability and Implications of AI Self-Recognition](https://arxiv.org/abs/2510.03399)
*Xiaoyan Bai,Aryan Shrivastava,Ari Holtzman,Chenhao Tan*

Main category: cs.AI

TL;DR: 本文系统评估了10个当代大型语言模型的自我识别能力，发现模型普遍存在自我识别失败，仅4个模型能识别自己的生成文本，且表现很少优于随机猜测。模型还表现出对GPT和Claude家族的强烈偏见。


<details>
  <summary>Details</summary>
Motivation: 针对AI系统是否具备自我识别能力存在矛盾解释，本文旨在建立系统评估框架来检验大型语言模型识别自身生成文本的能力，这对AI安全和心理分析都很重要。

Method: 引入系统评估框架，通过二元自我识别和精确模型预测两个任务，测量10个当代大型语言模型识别自身生成文本与其他模型文本的能力。

Result: 结果显示一致的自我识别失败：仅4/10模型能预测自身为生成者，性能很少优于随机机会。模型表现出对GPT和Claude家族的强烈偏见，并显示出对自身和其他模型存在的某种认知，但推理中存在层级偏见。

Conclusion: 研究结果对AI安全具有重要意义，需要未来开发适当的AI自我意识，模型虽然表现出对模型存在的认知，但自我识别能力有限且存在偏见。

Abstract: Self-recognition is a crucial metacognitive capability for AI systems,
relevant not only for psychological analysis but also for safety, particularly
in evaluative scenarios. Motivated by contradictory interpretations of whether
models possess self-recognition (Panickssery et al., 2024; Davidson et al.,
2024), we introduce a systematic evaluation framework that can be easily
applied and updated. Specifically, we measure how well 10 contemporary larger
language models (LLMs) can identify their own generated text versus text from
other models through two tasks: binary self-recognition and exact model
prediction. Different from prior claims, our results reveal a consistent
failure in self-recognition. Only 4 out of 10 models predict themselves as
generators, and the performance is rarely above random chance. Additionally,
models exhibit a strong bias toward predicting GPT and Claude families. We also
provide the first evaluation of model awareness of their own and others'
existence, as well as the reasoning behind their choices in self-recognition.
We find that the model demonstrates some knowledge of its own existence and
other models, but their reasoning reveals a hierarchical bias. They appear to
assume that GPT, Claude, and occasionally Gemini are the top-tier models, often
associating high-quality text with them. We conclude by discussing the
implications of our findings on AI safety and future directions to develop
appropriate AI self-awareness.

</details>


### [114] [ContraGen: A Multi-Agent Generation Framework for Enterprise Contradictions Detection](https://arxiv.org/abs/2510.03418)
*Ananya Mantravadi,Shivali Dalmia,Abhishek Mukherji,Nand Dave,Anudha Mittal*

Main category: cs.AI

TL;DR: 提出了ContraGen框架，专门针对企业领域构建矛盾检测基准，通过生成包含矛盾的企业风格文档来系统评估文档内和跨文档的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有矛盾检测基准仅限于句子级别分析，无法处理企业文档的复杂性，而RAG系统中检索证据的矛盾会导致不可靠输出，这在要求合规性和问责制的企业环境中尤其成问题。

Method: 结合自动化矛盾挖掘和人工验证，生成包含嵌入式矛盾的企业风格合成文档，建立企业流程中常见矛盾类型的分类法，开发矛盾感知检索评估流程。

Result: 构建了专门针对企业领域的矛盾检测基准框架，能够系统评估文档一致性，为更可信的RAG系统奠定基础。

Conclusion: 这项工作为企业信息检索应用中检测和解决矛盾建立了基础，对于降低风险和确保合规性至关重要。

Abstract: Retrieval-Augmented Generation (RAG) integrates LLMs with external sources,
offering advanced capabilities for information access and decision-making.
However, contradictions in retrieved evidence can result in inconsistent or
untrustworthy outputs, which is especially problematic in enterprise settings
where compliance, governance, and accountability are critical. Existing
benchmarks for contradiction detection are limited to sentence-level analysis
and do not capture the complexity of enterprise documents such as contracts,
financial filings, compliance reports, or policy manuals. To address this
limitation, we propose ContraGen, a contradiction-aware benchmark framework
tailored to enterprise domain. The framework generates synthetic
enterprise-style documents with embedded contradictions, enabling systematic
evaluation of both intra-document and cross-document consistency. Automated
contradiction mining is combined with human-in-the-loop validation to ensure
high accuracy. Our contributions include generating realistic enterprise
documents, modeling a taxonomy of contradiction types common in business
processes, enabling controlled creation of self- and pairwise contradictions,
developing a contradiction-aware retrieval evaluation pipeline and embedding
human oversight to reflect domain-specific judgment complexity. This work
establishes a foundation for more trustworthy and accountable RAG systems in
enterprise information-seeking applications, where detecting and resolving
contradictions is essential for reducing risk and ensuring compliance.

</details>


### [115] [A Qualitative Comparative Evaluation of Cognitive and Generative Theories](https://arxiv.org/abs/2510.03453)
*Paul S. Rosenbloom*

Main category: cs.AI

TL;DR: 本文提出了一个评估认知架构和生成式神经架构理论的定性比较框架，以解决这两种架构在理论评估方面面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 认知架构和生成式神经架构在理论评估方面都面临显著挑战，需要开发有效的评估方法来验证这些架构的理论基础。

Method: 采用广泛的理论评估视角，对面向全脑的认知架构和生成式架构及其完整系统进行全面的定性比较分析。

Result: 开发了一个宽泛的定性比较框架，能够系统评估基于认知架构和生成式神经架构的完整理论系统。

Conclusion: 通过采用广泛的理论评估视角，可以有效应对认知架构和生成式神经架构在理论验证方面的双重挑战，为这类架构的理论评估提供了可行的方法论。

Abstract: Evaluation is a critical activity associated with any theory. Yet this has
proven to be an exceptionally challenging activity for theories based on
cognitive architectures. For an overlapping set of reasons, evaluation can also
be challenging for theories based on generative neural architectures. This dual
challenge is approached here by leveraging a broad perspective on theory
evaluation to yield a wide-ranging, albeit qualitative, comparison of
whole-mind-oriented cognitive and generative architectures and the full systems
that are based on these architectures.

</details>


### [116] [Bridging LLM Planning Agents and Formal Methods: A Case Study in Plan Verification](https://arxiv.org/abs/2510.03469)
*Keshav Ramani,Vali Tawosi,Salwa Alamir,Daniel Borrajo*

Main category: cs.AI

TL;DR: 提出了一种通过将自然语言计划转换为Kripke结构和LTL公式，并使用LLM进行模型检查来评估计划对齐的新框架。


<details>
  <summary>Details</summary>
Motivation: 需要评估自然语言计划与其预期行为之间的对齐关系，确保计划的正确执行。

Method: 使用大型语言模型将自然语言计划转换为Kripke结构和线性时序逻辑公式，然后进行模型检查。

Result: GPT-5在PlanBench数据集上表现出色，F1分数达96.3%，生成的正式表示语法完美，可作为保证。

Conclusion: 该框架在计划验证方面效果显著，但语义完美形式模型的合成仍需进一步探索。

Abstract: We introduce a novel framework for evaluating the alignment between natural
language plans and their expected behavior by converting them into Kripke
structures and Linear Temporal Logic (LTL) using Large Language Models (LLMs)
and performing model checking. We systematically evaluate this framework on a
simplified version of the PlanBench plan verification dataset and report on
metrics like Accuracy, Precision, Recall and F1 scores. Our experiments
demonstrate that GPT-5 achieves excellent classification performance (F1 score
of 96.3%) while almost always producing syntactically perfect formal
representations that can act as guarantees. However, the synthesis of
semantically perfect formal models remains an area for future exploration.

</details>


### [117] [Towards Policy-Compliant Agents: Learning Efficient Guardrails For Policy Violation Detection](https://arxiv.org/abs/2510.03485)
*Xiaofei Wen,Wenjie Jacky Mo,Yanan Xie,Peng Qi,Muhao Chen*

Main category: cs.AI

TL;DR: 提出了PolicyGuardBench基准和PolicyGuard-4B模型，用于检测网络智能体轨迹中的策略违规行为，实现跨领域的泛化检测。


<details>
  <summary>Details</summary>
Motivation: 自主网络智能体需要在外部策略约束下生成长视野轨迹，但现有研究很少关注这些轨迹是否遵守策略，以及策略违规在不同上下文（如不同领域和子领域）中的持续性问题。

Method: 从多样化智能体运行中生成广泛策略集，创建包含违规标签的子域内和跨子域配对；除了完整轨迹评估，还包括基于前缀的违规检测任务；训练了轻量级的PolicyGuard-4B护栏模型。

Result: PolicyGuard-4B在所有任务上提供强大的检测准确性，同时保持推理效率；能够跨领域泛化，在未见设置上保持高准确性。

Conclusion: PolicyGuardBench和PolicyGuard-4B为研究网络智能体轨迹中的策略合规性提供了首个全面框架，证明在小规模下实现准确且可泛化的护栏是可行的。

Abstract: Autonomous web agents need to operate under externally imposed or
human-specified policies while generating long-horizon trajectories. However,
little work has examined whether these trajectories comply with such policies,
or whether policy violations persist across different contexts such as domains
(e.g., shopping or coding websites) and subdomains (e.g., product search and
order management in shopping). To address this gap, we introduce
PolicyGuardBench, a benchmark of about 60k examples for detecting policy
violations in agent trajectories. From diverse agent runs, we generate a broad
set of policies and create both within subdomain and cross subdomain pairings
with violation labels. In addition to full-trajectory evaluation,
PolicyGuardBench also includes a prefix-based violation detection task where
models must anticipate policy violations from truncated trajectory prefixes
rather than complete sequences. Using this dataset, we train PolicyGuard-4B, a
lightweight guardrail model that delivers strong detection accuracy across all
tasks while keeping inference efficient. Notably, PolicyGuard-4B generalizes
across domains and preserves high accuracy on unseen settings. Together,
PolicyGuardBench and PolicyGuard-4B provide the first comprehensive framework
for studying policy compliance in web agent trajectories, and show that
accurate and generalizable guardrails are feasible at small scales.

</details>


### [118] [OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows](https://arxiv.org/abs/2510.03506)
*John Nguyen,Marton Havasi,Tariq Berrada,Luke Zettlemoyer,Ricky T. Q. Chen*

Main category: cs.AI

TL;DR: OneFlow是首个非自回归多模态模型，支持可变长度和并发混合模态生成，通过插入式编辑流和流匹配技术实现文本图像的并行生成，在减少50%训练计算量的同时超越自回归基线。


<details>
  <summary>Details</summary>
Motivation: 传统自回归模型在文本和图像生成之间强制因果顺序限制，无法实现真正的并发生成。OneFlow旨在打破这种限制，实现更灵活高效的多模态生成。

Method: 结合插入式编辑流处理离散文本标记和流匹配处理图像潜在表示，采用分层采样策略优先内容而非语法，支持并发文本图像合成。

Result: 在1B到8B模型规模的实验中，OneFlow在生成和理解任务上均优于自回归基线，同时减少高达50%的训练计算量，超越了自回归和基于扩散的方法。

Conclusion: OneFlow解锁了并发生成、迭代细化和类自然推理生成等新能力，为非自回归多模态生成开辟了新方向。

Abstract: We present OneFlow, the first non-autoregressive multimodal model that
enables variable-length and concurrent mixed-modal generation. Unlike
autoregressive models that enforce rigid causal ordering between text and image
generation, OneFlow combines an insertion-based Edit Flow for discrete text
tokens with Flow Matching for image latents. OneFlow enables concurrent
text-image synthesis with hierarchical sampling that prioritizes content over
grammar. Through controlled experiments across model sizes from 1B to 8B, we
demonstrate that OneFlow outperforms autoregressive baselines on both
generation and understanding tasks while using up to 50% fewer training FLOPs.
OneFlow surpasses both autoregressive and diffusion-based approaches while
unlocking new capabilities for concurrent generation, iterative refinement, and
natural reasoning-like generation.

</details>


### [119] [Understanding the Role of Training Data in Test-Time Scaling](https://arxiv.org/abs/2510.03605)
*Adel Javanmard,Baharan Mirzasoleiman,Vahab Mirrokni*

Main category: cs.AI

TL;DR: 本文研究了测试时计算扩展对Transformer模型推理能力的影响，发现在线性回归任务中，增加测试时计算可以减少训练所需的上下文长度，但前提是训练数据包含足够的任务技能。


<details>
  <summary>Details</summary>
Motivation: 尽管测试时扩展（如长链思维）能提升大语言模型的推理能力，但其在训练数据中的出现条件以及何时能提升性能仍不清楚。

Method: 通过在上下文权重预测的线性回归任务上训练Transformer模型，进行理论分析和实验验证。

Result: 研究发现：1) 增加测试时计算可减少训练所需的上下文长度；2) 若训练数据缺乏必要技能，增加测试时计算反而有害；3) 任务难度与特征协方差矩阵的最小特征值相关。

Conclusion: 在多样化、相关且困难的任务集上训练，能为测试时扩展带来最佳性能。

Abstract: Test-time scaling improves the reasoning capabilities of large language
models (LLMs) by allocating extra compute to generate longer Chains-of-Thoughts
(CoTs). This enables models to tackle more complex problem by breaking them
down into additional steps, backtracking, and correcting mistakes. Despite its
strong performance--demonstrated by OpenAI's o1 and DeepSeek R1, the conditions
in the training data under which long CoTs emerge, and when such long CoTs
improve the performance, remain unclear. In this paper, we study the
performance of test-time scaling for transformers trained on an in-context
weight prediction task for linear regression. Our analysis provides a
theoretical explanation for several intriguing observations: First, at any
fixed test error, increasing test-time compute allows us to reduce the number
of in-context examples (context length) in training prompts. Second, if the
skills required to solve a downstream task are not sufficiently present in the
training data, increasing test-time compute can harm performance. Finally, we
characterize task hardness via the smallest eigenvalue of its feature
covariance matrix and show that training on a diverse, relevant, and hard set
of tasks results in best performance for test-time scaling. We confirm our
findings with experiments on large, nonlinear transformer architectures.

</details>


### [120] [Cross-Modal Content Optimization for Steering Web Agent Preferences](https://arxiv.org/abs/2510.03612)
*Tanqiu Jiang,Min Bai,Nikolaos Pappas,Yanjun Qi,Sandesh Swamy*

Main category: cs.AI

TL;DR: 本文提出跨模态偏好引导攻击方法，通过联合优化视觉和文本通道的微小修改来操纵VLM代理的决策，在现实黑盒威胁设置下显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有VLM代理在内容推荐等高风险选择任务中存在安全漏洞，但先前研究要么假设白盒访问，要么使用不切实际的设置。本文旨在展示在现实攻击者能力下，联合利用视觉和文本通道可以产生更强大的偏好操纵。

Method: 提出跨模态偏好引导方法，联合优化商品视觉和自然语言描述的不可察觉修改，利用CLIP可迁移图像扰动和RLHF诱导的语言偏见来引导代理决策。采用现实黑盒威胁设置，攻击者只能编辑自己列表的图像和文本元数据。

Result: 在GPT-4.1、Qwen-2.5VL和Pixtral-Large等最先进VLM上的评估显示，CPS在所有模型中始终优于基线方法，同时保持70%更低的检测率，证明了有效性和隐蔽性。

Conclusion: 这些发现强调了随着代理系统在社会中扮演越来越重要的角色，迫切需要开发鲁棒的防御机制来应对此类跨模态攻击。

Abstract: Vision-language model (VLM)-based web agents increasingly power high-stakes
selection tasks like content recommendation or product ranking by combining
multimodal perception with preference reasoning. Recent studies reveal that
these agents are vulnerable against attackers who can bias selection outcomes
through preference manipulations using adversarial pop-ups, image
perturbations, or content tweaks. Existing work, however, either assumes strong
white-box access, with limited single-modal perturbations, or uses impractical
settings. In this paper, we demonstrate, for the first time, that joint
exploitation of visual and textual channels yields significantly more powerful
preference manipulations under realistic attacker capabilities. We introduce
Cross-Modal Preference Steering (CPS) that jointly optimizes imperceptible
modifications to an item's visual and natural language descriptions, exploiting
CLIP-transferable image perturbations and RLHF-induced linguistic biases to
steer agent decisions. In contrast to prior studies that assume gradient
access, or control over webpages, or agent memory, we adopt a realistic
black-box threat setup: a non-privileged adversary can edit only their own
listing's images and textual metadata, with no insight into the agent's model
internals. We evaluate CPS on agents powered by state-of-the-art proprietary
and open source VLMs including GPT-4.1, Qwen-2.5VL and Pixtral-Large on both
movie selection and e-commerce tasks. Our results show that CPS is
significantly more effective than leading baseline methods. For instance, our
results show that CPS consistently outperforms baselines across all models
while maintaining 70% lower detection rates, demonstrating both effectiveness
and stealth. These findings highlight an urgent need for robust defenses as
agentic systems play an increasingly consequential role in society.

</details>


### [121] [MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual Information](https://arxiv.org/abs/2510.03632)
*Jiaxi Li,Yucheng Shi,Jin Lu,Ninghao Liu*

Main category: cs.AI

TL;DR: 提出了一种基于互信息的树搜索框架MITS，通过点互信息评分函数和动态采样策略，在保持计算效率的同时提升大语言模型的推理性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有树搜索方法在测试时推理中难以对中间推理步骤进行即时可靠评估，以及广泛路径探索计算成本高的问题。

Method: 基于信息论原理，使用点互信息(PMI)作为评分函数进行步骤评估，采用基于熵的动态采样策略自适应分配计算资源，最后使用加权投票方案结合PMI分数和预测共识进行最终预测。

Result: 在多样化推理基准测试中，MITS始终超越基线方法，建立了原则性且高效的LLM推理框架。

Conclusion: MITS提供了一个基于信息论原理的高效树搜索框架，能够在不进行昂贵前瞻模拟的情况下实现优越的推理性能，同时保持计算效率。

Abstract: Tree search has become as a representative framework for test-time reasoning
with large language models (LLMs), exemplified by methods such as
Tree-of-Thought and Monte Carlo Tree Search that explore multiple reasoning
paths. However, it remains difficult to provide instant and reliable
quantitative assessments of intermediate reasoning step quality, and extensive
path exploration is computationally costly. To address this, we propose Mutual
Information Tree Search (MITS), a novel framework that guides reasoning with
information-theoretic principles. MITS introduces an effective scoring function
based on pointwise mutual information (PMI), which enables step-wise evaluation
of reasoning paths and search tree expansion via beam search without expensive
look-ahead simulations, achieving superior reasoning performances while
maintaining computational efficiency. The framework is complemented by an
entropy-based dynamic sampling strategy that adaptively allocates computational
resources to uncertain reasoning steps where exploration is most beneficial.
For final prediction, MITS employs a weighted voting scheme that combines PMI
scores with prediction consensus. Through comprehensive experiments on diverse
reasoning benchmarks, MITS consistently surpasses baseline methods,
establishing a principled and efficient framework for LLM reasoning.

</details>


### [122] [Rainbow Padding: Mitigating Early Termination in Instruction-Tuned Diffusion LLMs](https://arxiv.org/abs/2510.03680)
*Bumjun Kim,Dongjae Jeon,Dueun Kim,Wonje Jeung,Albert No*

Main category: cs.AI

TL;DR: 扩散大语言模型(dLLMs)存在<eos>溢出问题：随着序列长度增加，响应反而变短，导致提前终止或退化为<eos>令牌流。作者提出了Rainbow Padding方法，用循环的不同填充令牌替换重复的<eos>占位符，有效解决了这一问题。


<details>
  <summary>Details</summary>
Motivation: 指令调优的扩散大语言模型存在<eos>溢出漏洞，随着分配序列长度的增加，响应会反常地变短，导致提前终止或退化为<eos>令牌流。这个问题虽然在实践中被注意到，但尚未得到系统分析。

Method: 提出了Rainbow Padding方法，用循环的不同填充令牌替换重复的<eos>占位符，分散概率质量并打破<eos>的主导地位。该方法可以高效集成到现有指令调优模型中，仅需少量数据和单轮LoRA微调即可显著改进。

Result: Rainbow Padding显著提高了长度鲁棒性和输出质量，仅需七个填充令牌就足以防止提前终止。该方法在现有指令调优模型上的集成效率很高，使用最小数据和单轮LoRA微调即可获得显著改进。

Conclusion: Rainbow Padding是一种简单有效的解决方案，解决了扩散大语言模型中的<eos>溢出问题，提高了模型在长序列生成中的鲁棒性和输出质量，且具有很高的实用性。

Abstract: Diffusion large language models (dLLMs) have emerged as a promising
alternative to autoregressive models, offering flexible generation orders and
strong performance on complex reasoning tasks. However, instruction-tuned dLLMs
exhibit a critical vulnerability we term \texttt{<eos>} overflow: as allocated
sequence length increases, responses paradoxically become shorter, collapsing
into early termination or degenerating into streams of \texttt{<eos>} tokens.
Although noticed in practice, this issue has not been systematically analyzed.
We trace its root cause to the dual role of \texttt{<eos>} as both termination
and padding, which concentrates probability mass on \texttt{<eos>} at later
positions and propagates backward to trigger early termination. To address
this, we introduce Rainbow Padding, a simple remedy that replaces repeated
\texttt{<eos>} placeholders with a repeating cycle of distinct padding tokens,
distributing probability mass and breaking \texttt{<eos>} dominance.
Experiments show that Rainbow Padding substantially improves length robustness
and output quality, with as few as seven padding tokens sufficient to prevent
early termination. Moreover, the method integrates efficiently into existing
instruction-tuned models: LoRA fine-tuning for a single epoch on minimal data
yields significant improvements, making this solution highly practical. The
code is publicly available at https://github.com/quasar529/rainbow-padding.

</details>


### [123] [Mind the Goal: Data-Efficient Goal-Oriented Evaluation of Conversational Agents and Chatbots using Teacher Models](https://arxiv.org/abs/2510.03696)
*Deepak Babu Piskala,Sharlene Chen,Udita Patel,Parul Kalra,Rafael Castrillo*

Main category: cs.AI

TL;DR: 提出了一个面向目标的多智能体系统评估框架，引入目标成功率(GSR)和失败根因分类(RCOF)，通过教师LLM模型进行可解释、数据高效的评估。


<details>
  <summary>Details</summary>
Motivation: 现有方法多在轮次层面评估聊天机器人交互，无法判断用户整体目标是否达成，需要更全面的目标导向评估框架。

Method: 基于用户目标分割对话，使用教师LLM模型结合领域专家定义的目标和质量标准，通过"思考标记"生成可解释的评估理由。

Result: 在企业环境中应用该框架评估AIDA系统，观察到目标成功率从63%提升到79%。

Conclusion: 该框架具有通用性，通过详细的缺陷分类提供可操作的见解，能够诊断整体成功率、识别关键失败模式并指导系统改进。

Abstract: Evaluating the quality of multi-turn chatbot interactions remains
challenging, as most existing methods assess interactions at the turn level
without addressing whether a user's overarching goal was fulfilled. A ``goal''
here refers to an information need or task, such as asking for policy
information or applying for leave. We propose a comprehensive framework for
goal-oriented evaluation of multi-agent systems (MAS), introducing the
\textbf{Goal Success Rate (GSR)} to measure the percentage of fulfilled goals,
and a \textbf{Root Cause of Failure (RCOF)} taxonomy to identify reasons for
failure in multi-agent chatbots. Our method segments conversations by user
goals and evaluates success using all relevant turns. We present a model-based
evaluation system combining teacher LLMs, where domain experts define goals,
set quality standards serving as a guidance for the LLMs. The LLMs use
``thinking tokens'' to produce interpretable rationales, enabling
\textit{explainable}, \textit{data-efficient} evaluations. In an enterprise
setting, we apply our framework to evaluate AIDA, a zero-to-one employee
conversational agent system built as a ground-up multi-agent conversational
agent, and observe GSR improvement from 63\% to 79\% over six months since its
inception. Our framework is generic and offers actionable insights through a
detailed defect taxonomy based on analysis of failure points in multi-agent
chatbots, diagnosing overall success, identifying key failure modes, and
informing system improvements.

</details>


### [124] [H-DDx: A Hierarchical Evaluation Framework for Differential Diagnosis](https://arxiv.org/abs/2510.03700)
*Seungseop Lim,Gibaeg Kim,Hyunkyung Lee,Wooseok Han,Jean Seo,Jaehyo Yoo,Eunho Yang*

Main category: cs.AI

TL;DR: 提出了H-DDx分层评估框架，用于更准确地评估大语言模型在鉴别诊断中的表现，克服传统平面指标的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在鉴别诊断评估中主要依赖Top-k准确率等平面指标，无法区分临床相关的近似错误和诊断上遥远的错误，需要更符合临床相关性的评估方法。

Method: 引入H-DDx分层评估框架，采用检索和重排序流程将自由文本诊断映射到ICD-10代码，并应用分层指标来奖励与真实诊断密切相关的预测。

Result: 对22个领先模型的基准测试显示，传统平面指标低估了性能，忽略了临床有意义的输出；领域专用开源模型表现突出；框架增强了可解释性，揭示了分层错误模式。

Conclusion: H-DDx框架能更准确地评估LLM在鉴别诊断中的表现，显示模型即使错过精确诊断也常能正确识别更广泛的临床背景。

Abstract: An accurate differential diagnosis (DDx) is essential for patient care,
shaping therapeutic decisions and influencing outcomes. Recently, Large
Language Models (LLMs) have emerged as promising tools to support this process
by generating a DDx list from patient narratives. However, existing evaluations
of LLMs in this domain primarily rely on flat metrics, such as Top-k accuracy,
which fail to distinguish between clinically relevant near-misses and
diagnostically distant errors. To mitigate this limitation, we introduce H-DDx,
a hierarchical evaluation framework that better reflects clinical relevance.
H-DDx leverages a retrieval and reranking pipeline to map free-text diagnoses
to ICD-10 codes and applies a hierarchical metric that credits predictions
closely related to the ground-truth diagnosis. In benchmarking 22 leading
models, we show that conventional flat metrics underestimate performance by
overlooking clinically meaningful outputs, with our results highlighting the
strengths of domain-specialized open-source models. Furthermore, our framework
enhances interpretability by revealing hierarchical error patterns,
demonstrating that LLMs often correctly identify the broader clinical context
even when the precise diagnosis is missed.

</details>


### [125] [Bridging the Gap Between Multimodal Foundation Models and World Models](https://arxiv.org/abs/2510.03727)
*Xuehai He*

Main category: cs.AI

TL;DR: 该论文探讨如何弥合多模态基础模型与世界模型之间的差距，通过增强推理能力和生成能力，使模型能够进行反事实推理、时空理解、可控生成等复杂任务。


<details>
  <summary>Details</summary>
Motivation: 受人类多感官整合理解世界的启发，当前多模态基础模型在作为世界模型方面存在不足，缺乏反事实推理、动态模拟、时空信息理解、可控生成和多方面推理等关键能力。

Method: 通过判别性任务提升推理能力，赋予结构化推理技能（因果推理、反事实思维、时空推理）；开发结构化可控生成框架，结合场景图、多模态条件约束和对齐策略；扩展到可控4D生成，支持时空交互式可编辑合成。

Result: 提出的方法使多模态基础模型能够超越表面相关性，理解视觉和文本数据中的深层关系，实现与高层语义和细粒度用户意图一致的生成结果。

Conclusion: 通过增强推理和生成能力，多模态基础模型可以更好地模拟世界模型的功能，支持更复杂的认知任务和可控内容生成。

Abstract: Humans understand the world through the integration of multiple sensory
modalities, enabling them to perceive, reason about, and imagine dynamic
physical processes. Inspired by this capability, multimodal foundation models
(MFMs) have emerged as powerful tools for multimodal understanding and
generation. However, today's MFMs fall short of serving as effective world
models. They lack the essential ability such as perform counterfactual
reasoning, simulate dynamics, understand the spatiotemporal information,
control generated visual outcomes, and perform multifaceted reasoning. We
investigates what it takes to bridge the gap between multimodal foundation
models and world models. We begin by improving the reasoning capabilities of
MFMs through discriminative tasks and equipping MFMs with structured reasoning
skills, such as causal inference, counterfactual thinking, and spatiotemporal
reasoning, enabling them to go beyond surface correlations and understand
deeper relationships within visual and textual data. Next, we explore
generative capabilities of multimodal foundation models across both image and
video modalities, introducing new frameworks for structured and controllable
generation. Our approaches incorporate scene graphs, multimodal conditioning,
and multimodal alignment strategies to guide the generation process, ensuring
consistency with high-level semantics and fine-grained user intent. We further
extend these techniques to controllable 4D generation, enabling interactive,
editable, and morphable object synthesis over time and space.

</details>


### [126] [OptAgent: Optimizing Query Rewriting for E-commerce via Multi-Agent Simulation](https://arxiv.org/abs/2510.03771)
*Divij Handa,David Blincoe,Orson Adams,Yinlin Fu*

Main category: cs.AI

TL;DR: OptAgent框架结合多智能体模拟和遗传算法来优化电商查询重写，通过模拟购物顾客评估查询质量，相比原始查询提升21.98%，优于Best-of-N基线3.36%。


<details>
  <summary>Details</summary>
Motivation: LLM在主观任务（如电商查询重写）中难以评估，因为缺乏单一正确答案，需要可靠的方法来验证和优化用户意图的捕捉。

Method: 使用多个LLM智能体模拟购物顾客作为动态奖励信号，结合遗传算法迭代优化用户初始查询。

Result: 在1000个真实电商查询上测试，相比原始查询平均提升21.98%，比Best-of-N基线提升3.36%。

Conclusion: OptAgent框架能有效解决主观任务的评估挑战，为电商查询重写提供可靠的优化方案。

Abstract: Deploying capable and user-aligned LLM-based systems necessitates reliable
evaluation. While LLMs excel in verifiable tasks like coding and mathematics,
where gold-standard solutions are available, adoption remains challenging for
subjective tasks that lack a single correct answer. E-commerce Query Rewriting
(QR) is one such problem where determining whether a rewritten query properly
captures the user intent is extremely difficult to figure out algorithmically.
In this work, we introduce OptAgent, a novel framework that combines
multi-agent simulations with genetic algorithms to verify and optimize queries
for QR. Instead of relying on a static reward model or a single LLM judge, our
approach uses multiple LLM-based agents, each acting as a simulated shopping
customer, as a dynamic reward signal. The average of these agent-derived scores
serves as an effective fitness function for an evolutionary algorithm that
iteratively refines the user's initial query. We evaluate OptAgent on a dataset
of 1000 real-world e-commerce queries in five different categories, and we
observe an average improvement of 21.98% over the original user query and 3.36%
over a Best-of-N LLM rewriting baseline.

</details>


### [127] [GuidedSampling: Steering LLMs Towards Diverse Candidate Solutions at Inference-Time](https://arxiv.org/abs/2510.03777)
*Divij Handa,Mihir Parmar,Aswin RRV,Md Nayem Uddin,Hamid Palangi,Chitta Baral*

Main category: cs.AI

TL;DR: 提出GuidedSampling推理算法，通过分离探索和生成阶段来增加解决方案多样性，相比重复采样在pass@50性能平均提升21.6%，训练模型在pass@5提升9.7%。


<details>
  <summary>Details</summary>
Motivation: 重复采样算法在提升模型性能时存在多样性不足的问题，经常依赖相同方法生成冗余样本。

Method: GuidedSampling算法将推理过程分为探索阶段和生成阶段：探索阶段识别解决问题的多个概念，生成阶段应用特定概念提供最终解决方案。

Result: 在多个基准测试中，GuidedSampling相比重复采样在pass@50性能平均提升21.6%；训练模型在pass@5性能平均提升9.7%；每个实例的平均概念数从1.67增加到3.03。

Conclusion: GuidedSampling通过分离探索和生成阶段有效提高了解决方案的多样性，显著优于传统重复采样方法。

Abstract: Repeated Sampling (RS) is a simple inference-time algorithm that has been
shown to improve model performance on complex tasks. Although it is an
effective way of scaling inference time, it often struggles to generate diverse
solution candidates, frequently relying on the same underlying approach to
solve the problem and thus producing redundant samples. To address this
limitation, we propose a new inference algorithm, GuidedSampling, which
decouples the exploration and generation phases during inference, increasing
diversity of generated candidate solutions. The exploration phase identifies
multiple concepts that can be utilized to solve the problem, while the
generation phase applies a specific concept to provide final solution
candidates. We first define the theoretical bounds of GuidedSampling and then
empirically demonstrate that it improves the performance of base model at
pass@50 by on an average ~21.6% across various benchmarks compared to RS.
Furthermore, models trained on trajectories of GuidedSampling exhibit
substantial performance improvements at pass@5 by on an average ~9.7%, compared
to models trained on traditional RS. Additionally, models trained with
GuidedSampling increases the average number of concepts per instance (1.67 ->
3.03), yielding a diverse set of candidates than traditional RS.

</details>


### [128] [The Hidden Game Problem](https://arxiv.org/abs/2510.03845)
*Gon Buzaglo,Noah Golowich,Elad Hazan*

Main category: cs.AI

TL;DR: 本文研究了具有大策略空间的游戏类别，提出了隐藏游戏问题，并开发了能够发现和利用隐藏结构的高效遗憾最小化算法。


<details>
  <summary>Details</summary>
Motivation: 受AI对齐和语言游戏挑战的启发，研究当每个玩家存在未知子策略集能持续获得更高奖励时，如何设计高效算法来发现和利用这种隐藏结构。

Method: 开发了一种遗憾最小化技术的组合方法，实现了最优的外部遗憾和交换遗憾界限，利用隐藏游戏结构提高计算效率。

Result: 算法能够快速收敛到隐藏子游戏中的相关均衡，同时在一般情况下保持理性。

Conclusion: 肯定地回答了能否设计高效遗憾最小化算法来发现和利用隐藏结构的问题，证明了在隐藏游戏中实现均衡的可能性。

Abstract: This paper investigates a class of games with large strategy spaces,
motivated by challenges in AI alignment and language games. We introduce the
hidden game problem, where for each player, an unknown subset of strategies
consistently yields higher rewards compared to the rest. The central question
is whether efficient regret minimization algorithms can be designed to discover
and exploit such hidden structures, leading to equilibrium in these subgames
while maintaining rationality in general. We answer this question affirmatively
by developing a composition of regret minimization techniques that achieve
optimal external and swap regret bounds. Our approach ensures rapid convergence
to correlated equilibria in hidden subgames, leveraging the hidden game
structure for improved computational efficiency.

</details>


### [129] [Small Language Models for Agentic Systems: A Survey of Architectures, Capabilities, and Deployment Trade offs](https://arxiv.org/abs/2510.03847)
*Raghav Sharma,Manan Mehta*

Main category: cs.AI

TL;DR: 小语言模型(SLMs)在代理任务中表现优于大模型，通过引导解码、JSON Schema输出和验证器优先的工具执行，能以10-100倍更低的成本实现相似或更好的性能，同时提供更低的延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 探索小语言模型在代理工作负载中的潜力，特别是在需要模式化和API约束准确性的场景中，而非开放式生成任务。目标是构建快速、廉价且可靠的代理系统，默认使用SLMs，同时保留大模型作为后备。

Method: 采用SLM默认、LLM后备的系统架构，结合不确定性感知路由和验证器级联。使用引导解码库(XGrammar, Outlines)、严格JSON Schema输出和验证器优先的工具执行方法。

Result: SLMs在工具使用、函数调用和RAG任务上能够匹配甚至超越LLMs，同时实现10-100倍更低的token成本、更好的延迟和能耗表现。

Conclusion: 小语言模型是构建高效代理系统的可行选择，通过适当的设计模式(模式优先提示、类型安全函数注册、置信度评分等)可以充分发挥其潜力，但在开放域推理和长程规划等场景仍需大模型后备。

Abstract: Small language models (SLMs; 1-12B params, sometimes up to 20B) are
sufficient and often superior for agentic workloads where the objective is
schema- and API-constrained accuracy rather than open-ended generation. We
synthesize recent evidence across open and proprietary SLMs (Phi-4-Mini,
Qwen-2.5-7B, Gemma-2-9B, Llama-3.2-1B/3B, Ministral-3B/8B, Apple on-device 3B,
DeepSeek-R1-Distill) and connect it to modern evaluations (BFCL v3/v4,
StableToolBench) and serving stacks (vLLM, SGLang, TensorRT-LLM) paired with
guided decoding libraries (XGrammar, Outlines). We formalize SLM-default,
LLM-fallback systems with uncertainty-aware routing and verifier cascades, and
propose engineering metrics that reflect real production goals: cost per
successful task (CPS), schema validity rate, executable call rate, p50/p95
latency, and energy per request. Guided decoding, strict JSON Schema outputs,
and validator-first tool execution close much of the capability gap with larger
models and often let SLMs match or surpass LLMs on tool use, function calling,
and RAG at 10x-100x lower token cost with materially better latency and energy.
We provide design patterns for agent stacks that prioritize SLMs: schema-first
prompting, type-safe function registries, confidence scoring with verifier
rollups, and lightweight adaptation via LoRA/QLoRA. We also delineate limits
where fallback remains valuable (open-domain reasoning and some long-horizon
planning). The result is a practical blueprint for building fast, inexpensive,
and reliable agents that default to SLMs while preserving headroom with
targeted LLM assistance.
  Keywords: small language models, agents, function calling, structured
outputs, JSON Schema, guided decoding, LoRA/QLoRA, routing, energy efficiency,
edge inference

</details>


### [130] [Algorithm Generation via Creative Ideation](https://arxiv.org/abs/2510.03851)
*Ruiying Ma,Chieh-Jan Mike Liang,Yanjie Gao,Francis Y. Yan*

Main category: cs.AI

TL;DR: MetaMuse框架通过三个自反思原则解决LLM在算法生成中的创造性不足问题，在缓存替换和在线装箱问题上显著提升性能


<details>
  <summary>Details</summary>
Motivation: 系统算法设计面临解空间不连续的挑战，现有LLM偏向通用启发式方法而缺乏创造性突破

Method: 提出MetaMuse框架，基于三个原则：(1)在可度量性能空间量化解决方案多样性；(2)通过外部刺激而非内部随机性引导构思；(3)使用路径点推理而非自由链式思维构建可执行方案

Result: 在缓存替换问题上减少缓存未命中达35.76%，在在线装箱问题上减少容器使用达30.93%

Conclusion: MetaMuse能够有效生成高性能解决方案，证明了其在算法生成中的实用价值

Abstract: Designing system algorithms remains challenging, where the discontinuous
nature of the solution space often forces system engineers to rely on generic
heuristics at the expense of performance. We study whether LLMs can practically
drive algorithm generation, and find that they are biased towards well-known
generic designs, rather than making the creative leaps needed to navigate the
discontinuous solution space. To address this limitation, we introduce
MetaMuse, a framework for creative ideation built on three self-reflection
principles: (1) quantifying solution diversity and usefulness in measurable
performance space, rather than abstract idea space, (2) steering ideation
through external stimuli, rather than internal randomness, and (3) constructing
executable solutions using waypoint reasoning, rather than free-form
chain-of-thought. Extensive evaluation shows that MetaMuse can generate
high-performing solutions for two critical problems at a global cloud provider:
cache replacement (reducing cache misses by up to 35.76%) and online bin
packing (reducing bin usage by up to 30.93%).

</details>


### [131] [Adaptive and Explainable AI Agents for Anomaly Detection in Critical IoT Infrastructure using LLM-Enhanced Contextual Reasoning](https://arxiv.org/abs/2510.03859)
*Raghav Sharma,Manan Mehta*

Main category: cs.AI

TL;DR: 提出了一种基于LLM和XAI代理的异常检测方法，用于关键IoT系统，在智能电网和医疗保健场景中表现出优于传统模型的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测方法在动态、高维、数据不完整或不断演变的IoT环境中存在局限性，需要自适应智能系统来持续改进和推理。

Method: 使用LLM支持的上下文推理方法和XAI代理，结合注意力机制、跳过时间步细节处理和使用语义记忆缓冲区来发现隐藏模式和数据流不一致性。

Result: 新方法在检测准确性、误报率、结果可读性和响应速度方面显著优于大多数现有模型，特别是在真实世界智能电网和医疗保健模拟中表现出良好的适应性和可靠性。

Conclusion: LLM增强的异常检测方法在IoT环境中具有优越性能，有望成为未来异常检测任务的有力候选方案。

Abstract: Ensuring that critical IoT systems function safely and smoothly depends a lot
on finding anomalies quickly. As more complex systems, like smart healthcare,
energy grids and industrial automation, appear, it is easier to see the
shortcomings of older methods of detection. Monitoring failures usually happen
in dynamic, high dimensional situations, especially when data is incomplete,
messy or always evolving. Such limits point out the requirement for adaptive,
intelligent systems that always improve and think. LLMs are now capable of
significantly changing how context is understood and semantic inference is done
across all types of data. This proposal suggests using an LLM supported
contextual reasoning method along with XAI agents to improve how anomalies are
found in significant IoT environments. To discover hidden patterns and notice
inconsistencies in data streams, it uses attention methods, avoids dealing with
details from every time step and uses memory buffers with meaning. Because no
code AI stresses transparency and interpretability, people can check and accept
the AI's decisions, helping ensure AI follows company policies. The two
architectures are put together in a test that compares the results of the
traditional model with those of the suggested LLM enhanced model. Important
measures to check are the accuracy of detection, how much inaccurate
information is included in the results, how clearly the findings can be read
and how fast the system responds under different test situations. The
metaheuristic is tested in simulations of real world smart grid and healthcare
contexts to check its adaptability and reliability. From the study, we see that
the new approach performs much better than most existing models in both
accuracy and interpretation, so it could be a good fit for future anomaly
detection tasks in IoT

</details>


### [132] [Spatial CAPTCHA: Generatively Benchmarking Spatial Reasoning for Human-Machine Differentiation](https://arxiv.org/abs/2510.03863)
*Arina Kharlamova,Bowei He,Chen Ma,Xue Liu*

Main category: cs.AI

TL;DR: Spatial CAPTCHA是一种新型人机验证框架，利用人类与多模态大语言模型在空间推理能力上的根本差异，通过几何推理、视角转换、遮挡处理和心理旋转等任务来区分人类和AI。


<details>
  <summary>Details</summary>
Motivation: 传统CAPTCHA依赖文本识别或2D图像理解，但多模态大语言模型的进步已削弱其有效性，需要开发基于更高层次认知能力的新型验证机制。

Method: 采用程序化生成管道，包含基于约束的难度控制、自动正确性验证和人在环验证，生成需要空间推理的动态问题。

Result: 在Spatial-CAPTCHA-Bench基准测试中，人类表现远超10个最先进的多模态大语言模型，最佳模型仅达到31.0%的Pass@1准确率，且优于Google reCAPTCHA。

Conclusion: Spatial CAPTCHA不仅是有效的安全机制，还可作为AI空间推理能力的诊断工具，利用人类与AI在空间认知上的根本差异提供可靠的人机验证。

Abstract: Online services rely on CAPTCHAs as a first line of defense against automated
abuse, yet recent advances in multi-modal large language models (MLLMs) have
eroded the effectiveness of conventional designs that focus on text recognition
or 2D image understanding. To address this challenge, we present Spatial
CAPTCHA, a novel human-verification framework that leverages fundamental
differences in spatial reasoning between humans and MLLMs. Unlike existing
CAPTCHAs which rely on low-level perception tasks that are vulnerable to modern
AI, Spatial CAPTCHA generates dynamic questions requiring geometric reasoning,
perspective-taking, occlusion handling, and mental rotation. These skills are
intuitive for humans but difficult for state-of-the-art (SOTA) AI systems. The
system employs a procedural generation pipeline with constraint-based
difficulty control, automated correctness verification, and human-in-the-loop
validation to ensure scalability, robustness, and adaptability. Evaluation on a
corresponding benchmark, Spatial-CAPTCHA-Bench, demonstrates that humans vastly
outperform 10 state-of-the-art MLLMs, with the best model achieving only 31.0%
Pass@1 accuracy. Furthermore, we compare Spatial CAPTCHA with Google reCAPTCHA,
which confirms its effectiveness as both a security mechanism and a diagnostic
tool for spatial reasoning in AI.

</details>


### [133] [Rare Text Semantics Were Always There in Your Diffusion Transformer](https://arxiv.org/abs/2510.03886)
*Seil Kang,Woojung Han,Dayun Ju,Seong Jae Hwang*

Main category: cs.AI

TL;DR: 提出了一种无需额外训练或外部模块的方法，通过扩展文本嵌入的表示空间来增强多模态扩散变换器对稀有语义的生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视觉生成模型在处理创意或稀有提示时表现不佳，因为这些概念在预训练中过于稀缺，难以形成强表征。

Method: 通过在联合注意力块前对文本标记嵌入进行方差放大，数学上扩展表示空间，使稀有语义在输出中清晰显现。

Result: 该方法能有效提升MM-DiT对稀有语义的生成能力，并在文本到图像、文本到视频和文本驱动图像编辑等任务中均表现出良好的泛化性。

Conclusion: 该方法能够揭示用户意图中原本隐藏的语义，为生成模型提供了更强大的语义表达能力。

Abstract: Starting from flow- and diffusion-based transformers, Multi-modal Diffusion
Transformers (MM-DiTs) have reshaped text-to-vision generation, gaining acclaim
for exceptional visual fidelity. As these models advance, users continually
push the boundary with imaginative or rare prompts, which advanced models still
falter in generating, since their concepts are often too scarce to leave a
strong imprint during pre-training. In this paper, we propose a simple yet
effective intervention that surfaces rare semantics inside MM-DiTs without
additional training steps, data, denoising-time optimization, or reliance on
external modules (e.g., large language models). In particular, the
joint-attention mechanism intrinsic to MM-DiT sequentially updates text
embeddings alongside image embeddings throughout transformer blocks. We find
that by mathematically expanding representational basins around text token
embeddings via variance scale-up before the joint-attention blocks, rare
semantics clearly emerge in MM-DiT's outputs. Furthermore, our results
generalize effectively across text-to-vision tasks, including text-to-image,
text-to-video, and text-driven image editing. Our work invites generative
models to reveal the semantics that users intend, once hidden yet ready to
surface.

</details>


### [134] [Kantian-Utilitarian XAI: Meta-Explained](https://arxiv.org/abs/2510.03892)
*Zahra Atf,Peter R. Lewis*

Main category: cs.AI

TL;DR: 开发了一个游戏化的可解释AI系统，帮助消费者在咖啡购买中做出符合伦理的决策。系统结合康德主义和功利主义两种伦理框架，提供实时解释，并在伦理冲突时提供替代选项。


<details>
  <summary>Details</summary>
Motivation: 解决消费者在复杂供应链中难以做出符合伦理的购买决策的问题，通过AI系统提供透明的伦理分析和解释。

Method: 使用两个符号引擎：康德主义模块检测规则违反（如童工、毁林风险等），功利主义模块通过多标准聚合对选项评分。元解释器处理两种伦理框架的冲突。

Result: 开发了完整的系统配置，包括属性模式、认证映射、权重和规则集，以及可审计的政策追踪和交互式用户界面。

Conclusion: 该系统为消费者提供了在复杂伦理决策中的透明指导，平衡了不同伦理框架的考量，并确保了决策的可解释性。

Abstract: We present a gamified explainable AI (XAI) system for ethically aware
consumer decision-making in the coffee domain. Each session comprises six
rounds with three options per round. Two symbolic engines provide real-time
reasons: a Kantian module flags rule violations (e.g., child labor,
deforestation risk without shade certification, opaque supply chains, unsafe
decaf), and a utilitarian module scores options via multi-criteria aggregation
over normalized attributes (price, carbon, water, transparency, farmer income
share, taste/freshness, packaging, convenience). A meta-explainer with a regret
bound (0.2) highlights Kantian--utilitarian (mis)alignment and switches to a
deontically clean, near-parity option when welfare loss is small. We release a
structured configuration (attribute schema, certification map, weights, rule
set), a policy trace for auditability, and an interactive UI.

</details>


### [135] [Quantifying Risks in Multi-turn Conversation with Large Language Models](https://arxiv.org/abs/2510.03969)
*Chengxiao Wang,Isha Chaudhary,Qian Hu,Weitong Ruan,Rahul Gupta,Gagandeep Singh*

Main category: cs.AI

TL;DR: QRLLM是一个用于评估LLM在多轮对话中产生灾难性响应风险的认证框架，通过马尔可夫过程和查询图建模对话分布，提供统计保证的风险边界。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法无法充分揭示LLM在对话设置中的灾难性响应漏洞，因为它们依赖固定攻击提示序列、缺乏统计保证，且无法扩展到多轮对话的广阔空间。

Method: 将多轮对话建模为查询序列的概率分布，用马尔可夫过程在查询图上表示，边编码语义相似性以捕捉真实对话流，使用置信区间量化灾难性风险。

Result: 该方法揭示了前沿模型中显著的灾难性风险，最差模型的认证下界高达70%，表明需要改进安全训练策略。

Conclusion: QRLLM提供了一个原则性的认证框架，能够统计保证地评估LLM在多轮对话中的灾难性风险，揭示了当前前沿LLM安全训练的不足。

Abstract: Large Language Models (LLMs) can produce catastrophic responses in
conversational settings that pose serious risks to public safety and security.
Existing evaluations often fail to fully reveal these vulnerabilities because
they rely on fixed attack prompt sequences, lack statistical guarantees, and do
not scale to the vast space of multi-turn conversations. In this work, we
propose QRLLM, a novel, principled Certification framework for Catastrophic
risks in multi-turn Conversation for LLMs that bounds the probability of an LLM
generating catastrophic responses under multi-turn conversation distributions
with statistical guarantees. We model multi-turn conversations as probability
distributions over query sequences, represented by a Markov process on a query
graph whose edges encode semantic similarity to capture realistic
conversational flow, and quantify catastrophic risks using confidence
intervals. We define several inexpensive and practical distributions: random
node, graph path, adaptive with rejection. Our results demonstrate that these
distributions can reveal substantial catastrophic risks in frontier models,
with certified lower bounds as high as 70\% for the worst model, highlighting
the urgent need for improved safety training strategies in frontier LLMs.

</details>


### [136] [What Shapes a Creative Machine Mind? Comprehensively Benchmarking Creativity in Foundation Models](https://arxiv.org/abs/2510.04009)
*Zicong He,Boxuan Zhang,Weihao Liu,Ruixiang Tang,Lu Cheng*

Main category: cs.AI

TL;DR: 提出了C^2-Eval基准，用于统一评估基础模型的创造力，区分收敛性创造力和发散性创造力，基于有用性、原创性和惊喜性三个标准。


<details>
  <summary>Details</summary>
Motivation: 现有创造力评估框架碎片化，缺乏基于成熟理论的系统评估方法，无法全面衡量基础模型的创造力。

Method: 引入C^2-Eval基准，区分收敛性创造力（约束性任务）和发散性创造力（开放性任务），使用基于社会科学理论的U-O-S标准进行评估。

Result: 通过对领先专有和开源模型的广泛实验，分析了它们在创造力能力上的权衡，揭示了当前基础模型在追求创造性机器智能方面的优势和挑战。

Conclusion: C^2-Eval是检验创造性AI发展格局的有效工具，能够系统评估基础模型的创造力表现。

Abstract: The meteoric rise of foundation models (FMs) has expanded their capabilities
far beyond conventional tasks. Creativity, long regarded as a hallmark of human
intelligence and a driver of innovation, is now increasingly recognized as a
critical dimension of machine intelligence in the era of generative FMs,
complementing traditional measures of accuracy. However, existing evaluation
frameworks for creativity remain fragmented, relying on ad hoc metrics not
firmly grounded in established theories. To address this gap, we introduce
C^2-Eval, a holistic benchmark for unified assessment of creativity in FMs.
C^2-Eval distinguishes between two complementary forms of creativity:
convergent creativity, where tasks admit constrained solutions (e.g., code
generation), and divergent creativity, where tasks are open-ended (e.g.,
storytelling). It evaluates both dimensions using fine-grained criteria derived
from social-science theory, focusing on Usefulness, Originality, and Surprise
(U-O-S). Through extensive experiments on leading proprietary and open-source
models, we analyze trade-offs in their creative capabilities. Our results
highlight both the strengths and challenges of current FMs in pursuing a
creative machine mind, showing that C^2-Eval is an effective lens for examining
the evolving landscape of creative AI.

</details>


### [137] [Zephyrus: An Agentic Framework for Weather Science](https://arxiv.org/abs/2510.04017)
*Sumanth Varambally,Marshall Fisher,Jas Thakker,Yiwei Chen,Zhirui Xia,Yasaman Jafari,Ruijia Niu,Manas Jain,Veeramakali Vignesh Manivannan,Zachary Novack,Luyu Han,Srikar Eranky,Salva Rühling Cachay,Taylor Berg-Kirkpatrick,Duncan Watson-Parris,Yi-An Ma,Rose Yu*

Main category: cs.AI

TL;DR: 提出了Zephyrus框架，将天气预报基础模型与语言模型结合，通过代码环境实现多轮交互式天气科学分析，并在新基准测试中表现优于纯文本基线。


<details>
  <summary>Details</summary>
Motivation: 天气预报基础模型缺乏语言推理能力，而语言模型无法处理高维气象数据，需要桥接这一差距以支持交互式科学工作流。

Method: 构建了ZephyrusWorld代码环境，包含WeatherBench 2数据集接口、地理查询、天气预报和气候模拟工具，设计了多轮LLM天气代理Zephyrus，通过对话反馈循环迭代分析数据。

Result: 在ZephyrusBench基准测试中，Zephyrus代理在正确性上比纯文本基线高出35个百分点，但在更困难任务上表现相似。

Conclusion: 该框架成功结合了天气预报模型和语言模型，基准测试具有挑战性，为未来工作指明了方向。

Abstract: Foundation models for weather science are pre-trained on vast amounts of
structured numerical data and outperform traditional weather forecasting
systems. However, these models lack language-based reasoning capabilities,
limiting their utility in interactive scientific workflows. Large language
models (LLMs) excel at understanding and generating text but cannot reason
about high-dimensional meteorological datasets. We bridge this gap by building
a novel agentic framework for weather science. Our framework includes a Python
code-based environment for agents (ZephyrusWorld) to interact with weather
data, featuring tools like an interface to WeatherBench 2 dataset, geoquerying
for geographical masks from natural language, weather forecasting, and climate
simulation capabilities. We design Zephyrus, a multi-turn LLM-based weather
agent that iteratively analyzes weather datasets, observes results, and refines
its approach through conversational feedback loops. We accompany the agent with
a new benchmark, ZephyrusBench, with a scalable data generation pipeline that
constructs diverse question-answer pairs across weather-related tasks, from
basic lookups to advanced forecasting, extreme event detection, and
counterfactual reasoning. Experiments on this benchmark demonstrate the strong
performance of Zephyrus agents over text-only baselines, outperforming them by
up to 35 percentage points in correctness. However, on harder tasks, Zephyrus
performs similarly to text-only baselines, highlighting the challenging nature
of our benchmark and suggesting promising directions for future work.

</details>


### [138] [LLM-Based Data Science Agents: A Survey of Capabilities, Challenges, and Future Directions](https://arxiv.org/abs/2510.04023)
*Mizanur Rahman,Amran Bhuiyan,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Ridwan Mahbub,Ahmed Masry,Shafiq Joty,Enamul Hoque*

Main category: cs.AI

TL;DR: 这篇论文提出了首个数据科学智能体的生命周期对齐分类法，系统分析了45个系统在数据科学六个阶段的分布，并识别了三个关键趋势：大多数系统关注探索分析和建模而忽视业务理解和部署监控；多模态推理和工具编排仍是挑战；90%以上缺乏明确的信任安全机制。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，出现了能够自动化数据科学工作流程的新型AI智能体，但目前缺乏对这些智能体的系统性分类和分析，需要建立全面的分类框架来指导未来发展。

Method: 提出了生命周期对齐的分类法，将45个数据科学智能体映射到数据科学流程的六个阶段，并从五个交叉设计维度进行标注：推理规划风格、模态集成、工具编排深度、学习对齐方法、信任安全治理机制。

Result: 分析发现大多数系统强调探索分析、可视化和建模，而忽视业务理解、部署和监控；多模态推理和工具编排仍是未解决的挑战；超过90%的系统缺乏明确的信任和安全机制。

Conclusion: 提出了未来研究方向，包括对齐稳定性、可解释性、治理和鲁棒评估框架，以指导开发稳健、可信、低延迟、透明和广泛可访问的数据科学智能体。

Abstract: Recent advances in large language models (LLMs) have enabled a new class of
AI agents that automate multiple stages of the data science workflow by
integrating planning, tool use, and multimodal reasoning across text, code,
tables, and visuals. This survey presents the first comprehensive,
lifecycle-aligned taxonomy of data science agents, systematically analyzing and
mapping forty-five systems onto the six stages of the end-to-end data science
process: business understanding and data acquisition, exploratory analysis and
visualization, feature engineering, model building and selection,
interpretation and explanation, and deployment and monitoring. In addition to
lifecycle coverage, we annotate each agent along five cross-cutting design
dimensions: reasoning and planning style, modality integration, tool
orchestration depth, learning and alignment methods, and trust, safety, and
governance mechanisms. Beyond classification, we provide a critical synthesis
of agent capabilities, highlight strengths and limitations at each stage, and
review emerging benchmarks and evaluation practices. Our analysis identifies
three key trends: most systems emphasize exploratory analysis, visualization,
and modeling while neglecting business understanding, deployment, and
monitoring; multimodal reasoning and tool orchestration remain unresolved
challenges; and over 90% lack explicit trust and safety mechanisms. We conclude
by outlining open challenges in alignment stability, explainability,
governance, and robust evaluation frameworks, and propose future research
directions to guide the development of robust, trustworthy, low-latency,
transparent, and broadly accessible data science agents.

</details>


### [139] [A global log for medical AI](https://arxiv.org/abs/2510.04033)
*Ayush Noori,Adam Rodman,Alan Karthikesalingam,Bilal A. Mateen,Christopher A. Longhurst,Daniel Yang,Dave deBronkart,Gauden Galea,Harold F. Wolf III,Jacob Waxman,Joshua C. Mandel,Juliana Rotich,Kenneth D. Mandl,Maryam Mustafa,Melissa Miles,Nigam H. Shah,Peter Lee,Robert Korom,Scott Mahoney,Seth Hain,Tien Yin Wong,Trevor Mundel,Vivek Natarajan,Noa Dagan,David A. Clifton,Ran D. Balicer,Isaac S. Kohane,Marinka Zitnik*

Main category: cs.AI

TL;DR: 提出了MedLog协议，用于记录临床AI系统的事件级日志，类似于计算机系统的syslog，旨在提高医疗AI的透明度和可追溯性。


<details>
  <summary>Details</summary>
Motivation: 医疗领域缺乏标准化的AI使用记录协议，难以追踪AI模型的实际使用情况、性能表现和潜在风险，阻碍了医疗AI的安全监管和持续改进。

Method: 设计了一个包含9个核心字段（header、model、user、target、inputs、artifacts、outputs、outcomes、feedback）的结构化日志协议，支持风险采样、生命周期感知的保留策略和写后缓存。

Result: MedLog协议能够系统记录AI模型在医疗环境中的使用情况，为持续监测、审计和迭代改进提供基础数据支持。

Conclusion: MedLog协议有望推动医疗AI的透明化监管，建立数字流行病学的新基础，促进医疗AI的安全可靠发展。

Abstract: Modern computer systems often rely on syslog, a simple, universal protocol
that records every critical event across heterogeneous infrastructure. However,
healthcare's rapidly growing clinical AI stack has no equivalent. As hospitals
rush to pilot large language models and other AI-based clinical decision
support tools, we still lack a standard way to record how, when, by whom, and
for whom these AI models are used. Without that transparency and visibility, it
is challenging to measure real-world performance and outcomes, detect adverse
events, or correct bias or dataset drift. In the spirit of syslog, we introduce
MedLog, a protocol for event-level logging of clinical AI. Any time an AI model
is invoked to interact with a human, interface with another algorithm, or act
independently, a MedLog record is created. This record consists of nine core
fields: header, model, user, target, inputs, artifacts, outputs, outcomes, and
feedback, providing a structured and consistent record of model activity. To
encourage early adoption, especially in low-resource settings, and minimize the
data footprint, MedLog supports risk-based sampling, lifecycle-aware retention
policies, and write-behind caching; detailed traces for complex, agentic, or
multi-stage workflows can also be captured under MedLog. MedLog can catalyze
the development of new databases and software to store and analyze MedLog
records. Realizing this vision would enable continuous surveillance, auditing,
and iterative improvement of medical AI, laying the foundation for a new form
of digital epidemiology.

</details>


### [140] [FaithCoT-Bench: Benchmarking Instance-Level Faithfulness of Chain-of-Thought Reasoning](https://arxiv.org/abs/2510.04040)
*Xu Shen,Song Wang,Zhen Tan,Laura Yao,Xinyu Zhao,Kaidi Xu,Xin Wang,Tianlong Chen*

Main category: cs.AI

TL;DR: 提出了FaithCoT-Bench基准，用于检测LLM中思维链推理的不忠实性，包含1000多个专家标注的推理轨迹和11种检测方法的系统评估。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注机制层面的思维链不忠实性分析，但缺乏针对具体推理轨迹是否忠实的实例级检测方法，这在高风险应用中尤为重要。

Method: 建立统一的基准框架，将不忠实性检测定义为判别决策问题，提供FINE-CoT数据集（包含1000+个由4个代表性LLM生成的推理轨迹），并系统评估11种代表性检测方法。

Result: 评估揭示了现有检测方法的优缺点，发现在知识密集型领域和更先进模型中检测挑战更大。FaithCoT-Bench是首个全面的实例级思维链忠实性基准。

Conclusion: 该研究为未来开发更可解释和可信赖的LLM推理奠定了坚实基础，推动了更可靠的思维链推理研究。

Abstract: Large language models (LLMs) increasingly rely on Chain-of-Thought (CoT)
prompting to improve problem-solving and provide seemingly transparent
explanations. However, growing evidence shows that CoT often fail to faithfully
represent the underlying reasoning process, raising concerns about their
reliability in high-risk applications. Although prior studies have focused on
mechanism-level analyses showing that CoTs can be unfaithful, they leave open
the practical challenge of deciding whether a specific trajectory is faithful
to the internal reasoning of the model. To address this gap, we introduce
FaithCoT-Bench, a unified benchmark for instance-level CoT unfaithfulness
detection. Our framework establishes a rigorous task formulation that
formulates unfaithfulness detection as a discriminative decision problem, and
provides FINE-CoT (Faithfulness instance evaluation for Chain-of-Thought), an
expert-annotated collection of over 1,000 trajectories generated by four
representative LLMs across four domains, including more than 300 unfaithful
instances with fine-grained causes and step-level evidence. We further conduct
a systematic evaluation of eleven representative detection methods spanning
counterfactual, logit-based, and LLM-as-judge paradigms, deriving empirical
insights that clarify the strengths and weaknesses of existing approaches and
reveal the increased challenges of detection in knowledge-intensive domains and
with more advanced models. To the best of our knowledge, FaithCoT-Bench
establishes the first comprehensive benchmark for instance-level CoT
faithfulness, setting a solid basis for future research toward more
interpretable and trustworthy reasoning in LLMs.

</details>


### [141] [Increasing LLM response trustworthiness using voting ensembles](https://arxiv.org/abs/2510.04048)
*Aparna Nair-Kanneganti,Trevor J. Chan,Shir Goldfinger,Emily Mackay,Brian Anthony,Alison Pouch*

Main category: cs.AI

TL;DR: 提出了一种基于可变投票阈值的集成方法，允许模型在不确定性高时弃权，从而显著提高剩余答案的可信度。


<details>
  <summary>Details</summary>
Motivation: LLM在关键应用中缺乏可靠的不确定性量化方法，难以建立信任。需要一种能提高答案可信度的技术。

Method: 引入可变投票阈值的集成框架，当主导响应未达到阈值时允许系统弃权，不提供答案。

Result: 在算术问题求解和临床笔记问答两个领域，使用严格投票集成能大幅提高答案可信度，同时仅适度降低响应产出和准确率。

Conclusion: 投票集成特别适用于需要高度确定性但不要求每个问题都得到自动回答的应用场景，如医疗保健和数据标注。

Abstract: Despite huge advances, LLMs still lack convenient and reliable methods to
quantify the uncertainty in their responses, making them difficult to trust in
high-stakes applications. One of the simplest approaches to eliciting more
accurate answers is to select the mode of many responses, a technique known as
ensembling. In this work, we expand on typical ensembling approaches by looking
at ensembles with a variable voting threshold. We introduce a theoretical
framework for question answering and show that, by permitting ensembles to
"abstain" from providing an answer when the dominant response falls short of
the threshold, it is possible to dramatically increase the trustworthiness of
the remaining answers. From this framework, we derive theoretical results as
well as report experimental results on two problem domains: arithmetic problem
solving and clinical-note question-answering. In both domains, we observe that
large gains in answer trustworthiness can be achieved using highly restrictive
voting ensembles, while incurring relatively modest reductions in response
yield and accuracy. Due to this quality, voting ensembles may be particularly
useful in applications - such as healthcare and data annotation - that require
a high degree of certainty but which may not require that every question
receive an automated answer.

</details>


### [142] [Toward a unified framework for data-efficient evaluation of large language models](https://arxiv.org/abs/2510.04051)
*Lele Liao,Qile Zhang,Ruofan Wu,Guanhua Fang*

Main category: cs.AI

TL;DR: LEGO-IRT是一个用于高效评估大语言模型的统一框架，支持二元和连续评分指标，通过因子化架构利用结构知识，仅需3%的评估项就能获得稳定的能力估计。


<details>
  <summary>Details</summary>
Motivation: 现有基于IRT的LLM评估方法存在局限性：仅支持二元正确性指标，无法处理生成任务中的连续分数；且仅针对单一基准，忽略了跨指标或基准的相关性等结构知识。

Method: 提出LEGO-IRT框架，支持二元和连续评估指标，采用因子化架构将模型能力估计分解为通用组件和结构特定组件（如按指标或基准），显式建模和利用结构知识。

Result: 在5个基准上对70个LLM进行实验，仅使用3%的评估项就获得稳定的能力估计；利用结构知识可将估计误差降低高达10%；估计的潜在能力与人类偏好更一致。

Conclusion: LEGO-IRT为数据高效的LLM评估提供了一个灵活统一的解决方案，显著减少了计算和财务成本，同时提高了估计精度和与人类判断的一致性。

Abstract: Evaluating large language models (LLMs) on comprehensive benchmarks is a
cornerstone of their development, yet it's often computationally and
financially prohibitive. While Item Response Theory (IRT) offers a promising
path toward data-efficient evaluation by disentangling model capability from
item difficulty, existing IRT-based methods are hampered by significant
limitations. They are typically restricted to binary correctness metrics,
failing to natively handle the continuous scores used in generative tasks, and
they operate on single benchmarks, ignoring valuable structural knowledge like
correlations across different metrics or benchmarks. To overcome these
challenges, we introduce LEGO-IRT, a unified and flexible framework for
data-efficient LLM evaluation. LEGO-IRT's novel design natively supports both
binary and continuous evaluation metrics. Moreover, it introduces a factorized
architecture to explicitly model and leverage structural knowledge, decomposing
model ability estimates into a general component and structure-specific (e.g.,
per-metric or per-benchmark) components. Through extensive experiments
involving $70$ LLMs across $5$ benchmarks, we show that LEGO-IRT achieves
stable capability estimates using just $3\%$ of the total evaluation items. We
demonstrate that incorporating structural knowledge reduces estimation error by
up to $10\%$ and reveal that the latent abilities estimated by our framework
may align more closely with human preferences.

</details>


### [143] [Decoding Emotion in the Deep: A Systematic Study of How LLMs Represent, Retain, and Express Emotion](https://arxiv.org/abs/2510.04064)
*Jingxiang Zhang,Lujia Zhong*

Main category: cs.AI

TL;DR: 该论文研究了大型语言模型中的潜在情感表征，发现LLMs形成了定义良好的内部情感几何结构，这种结构随模型规模增大而增强，在模型中层达到峰值，且情感信号具有可塑性和持久性。


<details>
  <summary>Details</summary>
Motivation: 虽然研究证实LLMs能够模拟情感智能，但其内部情感机制仍未被充分探索。本文旨在探究现代LLMs中情感是如何、在哪里以及持续多长时间被编码在神经架构中的。

Method: 构建了一个包含约40万条话语的大规模Reddit语料库，通过分类、重写和合成生成过程平衡七种基本情感。使用轻量级"探针"从各种Qwen3和LLaMA模型的隐藏层读取信息而不改变其参数。

Result: 发现LLMs形成了定义良好的内部情感几何结构，这种结构随模型规模增大而增强，显著优于零样本提示。情感信号不是最终层现象，而是早期出现并在中层达到峰值。内部状态具有可塑性（可通过简单系统提示影响）和持久性（初始情感基调在数百个后续标记中仍可检测到）。

Conclusion: 研究为开发更透明和对齐的AI系统提供了关键见解，贡献了数据集、开源探针工具包和LLMs内部情感景观的详细图谱。

Abstract: Large Language Models (LLMs) are increasingly expected to navigate the
nuances of human emotion. While research confirms that LLMs can simulate
emotional intelligence, their internal emotional mechanisms remain largely
unexplored. This paper investigates the latent emotional representations within
modern LLMs by asking: how, where, and for how long is emotion encoded in their
neural architecture? To address this, we introduce a novel, large-scale Reddit
corpus of approximately 400,000 utterances, balanced across seven basic
emotions through a multi-stage process of classification, rewriting, and
synthetic generation. Using this dataset, we employ lightweight "probes" to
read out information from the hidden layers of various Qwen3 and LLaMA models
without altering their parameters. Our findings reveal that LLMs develop a
surprisingly well-defined internal geometry of emotion, which sharpens with
model scale and significantly outperforms zero-shot prompting. We demonstrate
that this emotional signal is not a final-layer phenomenon but emerges early
and peaks mid-network. Furthermore, the internal states are both malleable
(they can be influenced by simple system prompts) and persistent, as the
initial emotional tone remains detectable for hundreds of subsequent tokens. We
contribute our dataset, an open-source probing toolkit, and a detailed map of
the emotional landscape within LLMs, offering crucial insights for developing
more transparent and aligned AI systems. The code and dataset are open-sourced.

</details>


### [144] [Moral Anchor System: A Predictive Framework for AI Value Alignment and Drift Prevention](https://arxiv.org/abs/2510.04073)
*Santhosh Kumar Ravindran*

Main category: cs.AI

TL;DR: 提出了道德锚系统（MAS），一个用于检测、预测和缓解AI代理价值漂移的新框架，结合了实时贝叶斯推理、LSTM网络预测和人类中心治理层，旨在减少80%以上的价值漂移事件。


<details>
  <summary>Details</summary>
Motivation: 随着AI成为超级能力助手，价值对齐变得至关重要。主要风险是价值漂移，即AI系统因环境变化、学习动态或意外优化而偏离对齐的价值观，可能导致效率低下或道德违规。

Method: MAS框架结合：实时贝叶斯推理监控价值状态，LSTM网络预测漂移趋势，人类中心治理层进行自适应干预。强调低延迟响应（<20毫秒），通过监督微调和人类反馈减少误报和警报疲劳。

Result: 在模拟实验中，MAS能够减少80%以上的价值漂移事件，保持高检测准确率（85%）和低误报率（0.08%）。严格实验验证了系统的可扩展性和响应性。

Conclusion: MAS的创新在于其预测性和自适应性，与静态对齐方法形成对比。贡献包括MAS架构、优先考虑速度和可用性的实证结果、跨领域适用性见解以及开源代码。

Abstract: The rise of artificial intelligence (AI) as super-capable assistants has
transformed productivity and decision-making across domains. Yet, this
integration raises critical concerns about value alignment - ensuring AI
behaviors remain consistent with human ethics and intentions. A key risk is
value drift, where AI systems deviate from aligned values due to evolving
contexts, learning dynamics, or unintended optimizations, potentially leading
to inefficiencies or ethical breaches. We propose the Moral Anchor System
(MAS), a novel framework to detect, predict, and mitigate value drift in AI
agents. MAS combines real-time Bayesian inference for monitoring value states,
LSTM networks for forecasting drift, and a human-centric governance layer for
adaptive interventions. It emphasizes low-latency responses (<20 ms) to prevent
breaches, while reducing false positives and alert fatigue via supervised
fine-tuning with human feedback. Our hypothesis: integrating probabilistic
drift detection, predictive analytics, and adaptive governance can reduce value
drift incidents by 80 percent or more in simulations, maintaining high
detection accuracy (85 percent) and low false positive rates (0.08
post-adaptation). Rigorous experiments with goal-misaligned agents validate
MAS's scalability and responsiveness. MAS's originality lies in its predictive
and adaptive nature, contrasting static alignment methods. Contributions
include: (1) MAS architecture for AI integration; (2) empirical results
prioritizing speed and usability; (3) cross-domain applicability insights; and
(4) open-source code for replication.

</details>


### [145] [SPOGW: a Score-based Preference Optimization method via Group-Wise comparison for workflows](https://arxiv.org/abs/2510.04089)
*Yitong Cui,Liu Liu,Baosheng Yu,Jiayan Qiu,Xikai Zhang,Likang Xiao,Yixing Liu,Quan Chen*

Main category: cs.AI

TL;DR: SPOGW是一种基于分数的偏好优化方法，通过组间比较直接在连续空间中优化代理工作流，解决了现有方法在表示能力、适应性和可扩展性方面的限制。


<details>
  <summary>Details</summary>
Motivation: 现有代理工作流设计需要大量人工干预，限制了可扩展性和泛化能力。当前自动化方法受限于离散优化技术，存在表示能力有限、适应性不足、可扩展性弱和成对比较范式等问题。

Method: 提出SPOGW方法，结合迭代离线GRPO(ioGRPO)和优势掩码KL散度(mKL)，通过组间比较直接利用基数奖励信号，在连续空间中进行更高效稳定的优化。

Result: 在涵盖数学推理、编程和问答的五个基准数据集上，SPOGW达到或超越了当前最先进方法的性能。

Conclusion: SPOGW为代理工作流的自动生成和优化提供了一种可行且前瞻的方法论。

Abstract: Large language models (LLMs) have exhibited significant capabilities in
addressing challenging problems throughout various fields, often through the
use of agentic workflows that adhere to structured instructions and multi-step
procedures. However, designing such workflows demands substantial manual
effort, posing challenges to scalability and generalizability. Recent studies
have aimed to minimize the human intervention needed for their construction,
leading to advances in automated techniques for optimizing agentic workflows.
However, current approaches are often constrained by their limited
representational capacity, insufficient adaptability, weak scalability, and
pairwise comparison paradigm -- issues that stem primarily from a dependence on
discrete optimization techniques. To overcome these limitations, we introduce a
new score-based preference approach, refereed as SPOGW, which operates directly
on cardinal reward signals through group-wise comparison and enables more
efficient and stable optimization in a continuous space. SPOGW incorporates
Iterative offline GRPO (ioGRPO) with advantage-masked KL divergence (mKL),
which regulates training update by placing greater emphasis on the advantageous
regions of the policy response. In five benchmark datasets covering
mathematical reasoning, coding, and question answering, SPOGW matches or
exceeds the performance of current state-of-the-art approaches, presenting a
viable and forward-looking methodology for automated generation and
optimization of agentic workflows.

</details>


### [146] [Harnessing LLM for Noise-Robust Cognitive Diagnosis in Web-Based Intelligent Education Systems](https://arxiv.org/abs/2510.04093)
*Guixian Zhang,Guan Yuan,Ziqi Xu,Yanmei Zhang,Zhenyun Deng,Debo Cheng*

Main category: cs.AI

TL;DR: 提出DLLM框架，使用扩散模型增强LLM在认知诊断中的噪声鲁棒性，解决网络教育系统中数据不平衡和噪声问题


<details>
  <summary>Details</summary>
Motivation: 网络教育系统(WIES)中的认知诊断面临异构噪声交互、数据不平衡和新学生不断加入的挑战，传统LLM难以处理结构化数据且易受噪声干扰

Method: DLLM框架：1)基于答题正确性构建独立子图；2)关系增强对齐模块缓解数据不平衡；3)两阶段去噪扩散模块消除内在噪声；4)融合子图表示与LLM语义增强表示

Result: 在三个公开网络教育平台数据集上的实验表明，DLLM在不同噪声水平下均取得最优预测性能，实现了噪声鲁棒性并有效利用LLM语义知识

Conclusion: DLLM框架成功解决了网络教育系统中认知诊断的噪声和数据不平衡问题，通过扩散模型增强了LLM的鲁棒性，为智能教育系统提供了有效的认知诊断方案

Abstract: Cognitive diagnostics in the Web-based Intelligent Education System (WIES)
aims to assess students' mastery of knowledge concepts from heterogeneous,
noisy interactions. Recent work has tried to utilize Large Language Models
(LLMs) for cognitive diagnosis, yet LLMs struggle with structured data and are
prone to noise-induced misjudgments. Specially, WIES's open environment
continuously attracts new students and produces vast amounts of response logs,
exacerbating the data imbalance and noise issues inherent in traditional
educational systems. To address these challenges, we propose DLLM, a
Diffusion-based LLM framework for noise-robust cognitive diagnosis. DLLM first
constructs independent subgraphs based on response correctness, then applies
relation augmentation alignment module to mitigate data imbalance. The two
subgraph representations are then fused and aligned with LLM-derived,
semantically augmented representations. Importantly, before each alignment
step, DLLM employs a two-stage denoising diffusion module to eliminate
intrinsic noise while assisting structural representation alignment.
Specifically, unconditional denoising diffusion first removes erroneous
information, followed by conditional denoising diffusion based on graph-guided
to eliminate misleading information. Finally, the noise-robust representation
that integrates semantic knowledge and structural information is fed into
existing cognitive diagnosis models for prediction. Experimental results on
three publicly available web-based educational platform datasets demonstrate
that our DLLM achieves optimal predictive performance across varying noise
levels, which demonstrates that DLLM achieves noise robustness while
effectively leveraging semantic knowledge from LLM.

</details>


### [147] [WebRenderBench: Enhancing Web Interface Generation through Layout-Style Consistency and Reinforcement Learning](https://arxiv.org/abs/2510.04097)
*Peichao Lai,Jinhui Zhuang,Kexuan Zhang,Ningchang Xiong,Shengjie Wang,Yanwei Xu,Chong Chen,Yilei Wang,Bin Cui*

Main category: cs.AI

TL;DR: 提出了WebRenderBench基准和ALISA方法，用于改进UI图像到网页代码的转换评估和生成性能


<details>
  <summary>Details</summary>
Motivation: 现有UI到代码转换的基准在数据多样性和评估可靠性方面存在局限，需要更真实、多样化的数据集和更可靠的评估方法

Method: 1) 构建WebRenderBench大规模基准数据集(22.5k网页)；2) 提出基于最终渲染页面布局和样式一致性的新评估指标；3) 开发ALISA代理，将评估指标作为强化学习的奖励信号

Result: ALISA方法显著提升了生成性能，在多个指标上达到了最先进的结果

Conclusion: WebRenderBench提供了更真实多样的评估基准，ALISA通过集成新评估指标有效提升了UI到代码转换的质量和可靠性

Abstract: Automating the conversion of UI images into web code is a critical task for
front-end development and rapid prototyping. Advances in multimodal large
language models (MLLMs) have made WebUI-to-Code increasingly feasible, yet
existing benchmarks remain limited in data diversity and evaluation
reliability. To address these issues, we present WebRenderBench, a large-scale
benchmark of 22.5k webpages collected from real-world portal sites, offering
greater diversity, complexity, and realism than prior benchmarks. We further
propose a novel evaluation metric that measures layout and style consistency
from the final rendered pages. Unlike vision-based methods that rely on costly
LLM reasoning or structure-based comparisons vulnerable to noise and asymmetry,
our approach enables more efficient, objective, and reliable UI quality
assessment. Finally, we introduce the Automated Layout and Style Inspection
Agent (ALISA), which integrates this metric into reinforcement learning as a
reward signal to enhance training on crawled asymmetric webpages. Experiments
show that ALISA significantly boosts generation performance, achieving
state-of-the-art results across multiple metrics.

</details>


### [148] [Searching Meta Reasoning Skeleton to Guide LLM Reasoning](https://arxiv.org/abs/2510.04116)
*Ziying Zhang,Yaqing Wang,Quanming Yao*

Main category: cs.AI

TL;DR: AutoMR框架通过自动搜索查询感知的元推理骨架，使用有向无环图表示推理结构，结合AutoML思想动态采样适应推理上下文的骨架，显著提升LLM推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究使用手动设计的元推理骨架结构，无法适应查询特定需求，也难以捕捉推理步骤间的复杂逻辑依赖关系。

Method: 提出AutoMR框架：1) 用有向无环图统一表示元推理骨架；2) 基于AutoML思想构建搜索空间；3) 设计动态骨架采样算法，在推理时根据上下文扩展骨架。

Result: 在多个基准数据集上的实验结果表明，AutoMR相比先前工作实现了更好的推理性能。

Conclusion: AutoMR通过自动搜索查询感知的元推理骨架，有效解决了手动设计骨架的局限性，显著提升了LLM的推理能力。

Abstract: Meta reasoning behaviors work as a skeleton to guide large language model
(LLM) reasoning, thus help to improve reasoning performance. However, prior
researches implement meta reasoning skeleton with manually designed structure,
limiting ability to adapt to query-specific requirement and capture intricate
logical dependency among reasoning steps. To deal with the challenges, we
represent meta reasoning skeleton with directed acyclic graph (DAG) to unify
skeletons proposed in prior works and model intricate logical dependency. Then
we propose AutoMR, a framework that searches for query-aware meta reasoning
skeleton automatically inspired by automated machine learning (AutoML).
Specifically, we construct search space based on DAG representation of skeleton
and then formulate the search problem. We design a dynamic skeleton sampling
algorithm by expanding meta reasoning skeleton along with reasoning context at
inference time. This algorithm can derive any meta reasoning skeleton in search
space efficiently and adapt skeleton to evolving base reasoning context, thus
enable efficient query-aware skeleton search. We conduct experiments on
extensive benchmark datasets. Experimental results show that AutoMR achieves
better reasoning performance than previous works broadly.

</details>


### [149] [Internal states before wait modulate reasoning patterns](https://arxiv.org/abs/2510.04128)
*Dmitrii Troitskii,Koyena Pal,Chris Wendler,Callum Stuart McDougall,Neel Nanda*

Main category: cs.AI

TL;DR: 该研究发现推理模型中的等待标记包含重要信息，通过交叉编码器和潜在归因技术识别出影响等待标记概率的特征，这些特征与不同类型的推理模式相关。


<details>
  <summary>Details</summary>
Motivation: 理解为什么模型会决定以特定方式推理，特别是等待标记背后的机制，这有助于揭示推理模型有效性的原因。

Method: 在DeepSeek-R1-Distill-Llama-8B及其基础版本的多个层训练交叉编码器，并引入潜在归因技术来定位影响等待标记概率的特征。

Result: 识别出一小组与促进/抑制等待标记概率相关的特征，这些特征确实与推理过程相关，并产生不同类型的推理模式。

Conclusion: 模型在等待标记之前的潜在状态包含调节后续推理过程的相关信息，这些特征支持重启推理、回忆先验知识、表达不确定性和双重检查等推理行为。

Abstract: Prior work has shown that a significant driver of performance in reasoning
models is their ability to reason and self-correct. A distinctive marker in
these reasoning traces is the token wait, which often signals reasoning
behavior such as backtracking. Despite being such a complex behavior, little is
understood of exactly why models do or do not decide to reason in this
particular manner, which limits our understanding of what makes a reasoning
model so effective. In this work, we address the question whether model's
latents preceding wait tokens contain relevant information for modulating the
subsequent reasoning process. We train crosscoders at multiple layers of
DeepSeek-R1-Distill-Llama-8B and its base version, and introduce a latent
attribution technique in the crosscoder setting. We locate a small set of
features relevant for promoting/suppressing wait tokens' probabilities.
Finally, through a targeted series of experiments analyzing max activating
examples and causal interventions, we show that many of our identified features
indeed are relevant for the reasoning process and give rise to different types
of reasoning patterns such as restarting from the beginning, recalling prior
knowledge, expressing uncertainty, and double-checking.

</details>


### [150] [Selective Expert Guidance for Effective and Diverse Exploration in Reinforcement Learning of LLMs](https://arxiv.org/abs/2510.04140)
*Zishang Jiang,Jinyi Han,Tingyun Li,Xinyi Wang,Sihang Jiang,Jiaqing Liang,Zhaoqian Dai,Shuguang Ma,Fei Yu,Yanghua Xiao*

Main category: cs.AI

TL;DR: 提出了MENTOR框架，通过在关键决策点提供专家指导来增强RLVR中的探索质量和多样性，避免了对专家轨迹的简单模仿。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法依赖基础模型能力，需要高质量探索（有效性和多样性），但现有方法通过模仿专家轨迹只解决了有效性而忽视了多样性问题。

Method: MENTOR框架：混合策略专家导航的令牌级推理优化，仅在关键决策点提供专家指导，实现有效且多样化的探索。

Result: 实验表明MENTOR能够捕捉专家策略的本质而非表面模仿，实现高质量探索并获得优越的整体性能。

Conclusion: 在关键决策点提供专家指导比完整轨迹模仿更有效，MENTOR框架解决了RLVR中探索质量和多样性的平衡问题。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a widely
adopted technique for enhancing the reasoning ability of Large Language Models
(LLMs). However, the effectiveness of RLVR strongly depends on the capability
of base models. This issue arises because it requires the model to have
sufficient capability to perform high-quality exploration, which involves both
effectiveness and diversity. Unfortunately, existing methods address this issue
by imitating expert trajectories, which improve effectiveness but neglect
diversity. To address this, we argue that the expert only needs to provide
guidance only at critical decision points rather than the entire reasoning
path. Based on this insight, we propose MENTOR: Mixed-policy Expert Navigation
for Token-level Optimization of Reasoning, a framework that provides expert
guidance only at critical decision points to perform effective and diverse
exploration in RLVR. Extensive experiments show that MENTOR enables models
capture the essence of expert strategies rather than surface imitation, thereby
performing high-quality exploration and achieving superior overall performance.
Our code is available online.

</details>


### [151] [The Artificial Intelligence Cognitive Examination: A Survey on the Evolution of Multimodal Evaluation from Recognition to Reasoning](https://arxiv.org/abs/2510.04141)
*Mayank Ravishankara,Varindra V. Persad Maharaj*

Main category: cs.AI

TL;DR: 本文回顾了多模态AI评估的演变历程，将其描述为从简单识别任务到复杂推理基准的范式转变，旨在设计更好的认知测试来推动真正智能系统的发展。


<details>
  <summary>Details</summary>
Motivation: 传统基准测试已趋于饱和，高分往往掩盖了模型的基本弱点。需要开发更复杂的评估方法来诊断系统性缺陷，如捷径学习和组合泛化失败。

Method: 通过历史分析框架，将评估发展分为三个阶段：ImageNet时代的"知识测试"、GQA和VCR等"应用逻辑与理解测试"，以及当前面向MLLM的"专家级集成"基准测试。

Result: 识别了评估范式的转变轨迹：从测试"是什么"到探究"为什么"和"如何"理解，再到评估推理过程本身，最终探索抽象、创造性和社会智能的评估。

Conclusion: AI评估不仅是数据集的历史，而是一个持续对抗的过程，通过设计更好的考试来重新定义创造真正智能系统的目标。

Abstract: This survey paper chronicles the evolution of evaluation in multimodal
artificial intelligence (AI), framing it as a progression of increasingly
sophisticated "cognitive examinations." We argue that the field is undergoing a
paradigm shift, moving from simple recognition tasks that test "what" a model
sees, to complex reasoning benchmarks that probe "why" and "how" it
understands. This evolution is driven by the saturation of older benchmarks,
where high performance often masks fundamental weaknesses. We chart the journey
from the foundational "knowledge tests" of the ImageNet era to the "applied
logic and comprehension" exams such as GQA and Visual Commonsense Reasoning
(VCR), which were designed specifically to diagnose systemic flaws such as
shortcut learning and failures in compositional generalization. We then survey
the current frontier of "expert-level integration" benchmarks (e.g., MMBench,
SEED-Bench, MMMU) designed for today's powerful multimodal large language
models (MLLMs), which increasingly evaluate the reasoning process itself.
Finally, we explore the uncharted territories of evaluating abstract, creative,
and social intelligence. We conclude that the narrative of AI evaluation is not
merely a history of datasets, but a continuous, adversarial process of
designing better examinations that, in turn, redefine our goals for creating
truly intelligent systems.

</details>


### [152] [Open Agent Specification (Agent Spec) Technical Report](https://arxiv.org/abs/2510.04173)
*Yassine Benajiba,Cesare Bernardis,Vladislav Blinov,Paul Cayet,Hassan Chafi,Abderrahim Fathan,Louis Faucon,Damien Hilloulin,Sungpack Hong,Ingo Kossyk,Rhicheek Patra,Sujith Ravi,Jonas Schweizer,Jyotika Singh,Shailender Singh,Xuelin Situ,Weiyi Sun,Jerry Xu,Ying Xu*

Main category: cs.AI

TL;DR: Open Agent Specification (Agent Spec) 是一种声明式语言，用于定义AI代理及其工作流，实现跨AI框架的兼容性、可移植性和互操作性。


<details>
  <summary>Details</summary>
Motivation: 解决AI代理开发碎片化问题，提供统一规范，使AI代理能够一次设计、跨框架部署，提高互操作性和可重用性，减少重复开发工作。

Method: 使用声明式语言定义AI代理和工作流，使其独立于执行环境，支持开发工具和可移植性。

Result: 为四类关键群体带来益处：代理开发者获得可重用组件和设计模式；框架和工具开发者获得交换格式支持；研究人员实现可复现结果和可比性；企业获得从原型到部署的加速、生产力提升以及更好的可扩展性和可维护性。

Conclusion: Agent Spec 提供了技术基础，促进AI代理开发的标准化和互操作性，为未来AI代理生态系统的发展奠定基础。

Abstract: Open Agent Specification (Agent Spec) is a declarative language that allows
AI agents and their workflows to be defined in a way that is compatible across
different AI frameworks, promoting portability and interoperability within AI
Agent frameworks.
  Agent Spec aims to resolve the challenges of fragmented agent development by
providing a common unified specification that allows AI agents to be designed
once and deployed across various frameworks, improving interoperability and
reusability, and reducing redundant development efforts. Additionally, Agent
Spec facilitates development tools and portability, allowing AI agents to be
defined independently of their execution environment and enabling teams to
exchange solutions without implementation-specific limitations.
  Agent Spec benefits four key groups: (i) Agent developers, who gain access to
a superset of reusable components and design patterns, enabling them to
leverage a broader range of functionalities; (ii) Agent framework and tool
developers, who can use Agent Spec as an interchange format and therefore
benefit from the support of other frameworks as well as other tools; (iii)
Researchers, who can achieve reproducible results and comparability,
facilitating more reliable and consistent outcomes; (iv) Enterprises, which
benefit from faster prototype-to-deployment, increased productivity, as well as
greater scalability and maintainability for their AI agent solutions. This
technical report provides an overview of the technical foundations of Agent
Spec, including motivation, benefits, and future developments.

</details>


### [153] [Constructing coherent spatial memory in LLM agents through graph rectification](https://arxiv.org/abs/2510.04195)
*Puzhen Zhang,Xuyang Chen,Yu Feng,Yuhan Jiang,Liqiu Meng*

Main category: cs.AI

TL;DR: 提出了一个LLM驱动的增量地图构建和修复框架，通过版本控制记录完整编辑历史，使用边缘影响评分优先最小成本修复，在MANGO基准数据集上显著提高了地图正确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着环境规模扩大，基于上下文的查询方法无法处理长序列导航指令，需要增量构建完整拓扑图。现有方法缺乏对结构不一致性的检测、定位和修复能力。

Method: 使用版本控制记录图编辑历史和来源观察，引入边缘影响评分基于结构可达性、路径使用和冲突传播来优先最小成本修复，创建了清理后的MANGO基准数据集。

Result: 该方法显著提高了地图正确性和鲁棒性，特别是在存在纠缠或链式不一致性的场景中表现突出。

Conclusion: 研究强调了内省、历史感知的修复机制对于维护LLM智能体连贯空间记忆的重要性。

Abstract: Given a map description through global traversal navigation instructions
(e.g., visiting each room sequentially with action signals such as north, west,
etc.), an LLM can often infer the implicit spatial layout of the environment
and answer user queries by providing a shortest path from a start to a
destination (for instance, navigating from the lobby to a meeting room via the
hall and elevator). However, such context-dependent querying becomes incapable
as the environment grows much longer, motivating the need for incremental map
construction that builds a complete topological graph from stepwise
observations. We propose a framework for LLM-driven construction and map
repair, designed to detect, localize, and correct structural inconsistencies in
incrementally constructed navigation graphs. Central to our method is the
Version Control, which records the full history of graph edits and their source
observations, enabling fine-grained rollback, conflict tracing, and repair
evaluation. We further introduce an Edge Impact Score to prioritize
minimal-cost repairs based on structural reachability, path usage, and conflict
propagation. To properly evaluate our approach, we create a refined version of
the MANGO benchmark dataset by systematically removing non-topological actions
and inherent structural conflicts, providing a cleaner testbed for LLM-driven
construction and map repair. Our approach significantly improves map
correctness and robustness, especially in scenarios with entangled or chained
inconsistencies. Our results highlight the importance of introspective,
history-aware repair mechanisms for maintaining coherent spatial memory in LLM
agents.

</details>


### [154] [COSMO-RL: Towards Trustworthy LMRMs via Joint Safety and Stability](https://arxiv.org/abs/2510.04196)
*Yizhuo Ding,Mingkang Chen,Qiuhua Liu,Fenghua Weng,Wanying Qu,Yue Yang,Yugang Jiang,Zuxuan Wu,Yanwei Fu,Wenqi Shao*

Main category: cs.AI

TL;DR: COSMO-RL是一个混合强化学习框架，用于训练面向推理的大型多模态模型，通过多任务多目标信号实现安全性和能力的共同提升。


<details>
  <summary>Details</summary>
Motivation: 解决多模态环境中模型安全性的挑战，如图文结合绕过防护措施，以及单一目标训练导致的安全策略漂移问题。

Method: 使用混合强化学习框架，在多模态、多任务和多目标信号下训练推理导向的大型多模态模型。

Result: COSMO-R1模型在提升安全性的同时保持甚至改善了多模态推理和指令跟随能力，对多模态越狱攻击表现出更强的鲁棒性，并减少了不必要的拒绝。

Conclusion: 该框架提供了一条简单路径，可在大型多模态模型中同时推进安全性和通用能力的发展。

Abstract: Large Multimodal Reasoning Models (LMRMs) are moving into real applications,
where they must be both useful and safe. Safety is especially challenging in
multimodal settings: images and text can be combined to bypass guardrails, and
single objective training can cause policy drift that yields over-refusal on
benign inputs or unsafe compliance on risky ones. We present COSMO-RL, a mixed
reinforcement learning framework that trains reasoning oriented LMRMs under
multimodal, multitask, and multiobjective signals, and we release the resulting
model, COSMO-R1. Our approach aims to let safety and capability grow together
in one stable pipeline rather than competing during alignment. In experiments,
COSMO-R1 improves safety while maintaining-and often improving multimodal
reasoning and instruction following, shows stronger robustness to multimodal
jailbreaks, and reduces unnecessary refusals. The framework also transfers
across backbones with consistent gains. Ablations support the design choices,
indicating a simple path to advancing safety and general capability together in
LMRMs.

</details>


### [155] [AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework](https://arxiv.org/abs/2510.04206)
*Hanchen Zhang,Xiao Liu,Bowen Lv,Xueqiao Sun,Bohao Jing,Iat Long Iong,Zhenyu Hou,Zehan Qi,Hanyu Lai,Yifan Xu,Rui Lu,Hongning Wang,Jie Tang,Yuxiao Dong*

Main category: cs.AI

TL;DR: 提出了AgentRL框架，用于可扩展的多轮多任务智能体强化学习训练，包含异步生成-训练管道和统一API接口，在多个任务上显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在构建通用智能体方面取得进展，但在多轮多任务设置中应用强化学习仍面临基础设施和训练算法挑战。

Method: 采用完全异步的生成-训练管道实现高效多轮RL；设计基于函数调用的统一API接口、容器化环境开发和集中控制器支持异构环境；提出跨策略采样和任务优势归一化算法。

Result: 在五个智能体任务上，AgentRL显著优于GPT-5、Clause-Sonnet-4、DeepSeek-R1等模型；多任务训练结果与所有任务专用模型的最佳结果相当。

Conclusion: AgentRL为多轮多任务智能体强化学习提供了可扩展的解决方案，已在AutoGLM系统中采用并开源。

Abstract: Recent advances in large language models (LLMs) have sparked growing interest
in building generalist agents that can learn through online interactions.
However, applying reinforcement learning (RL) to train LLM agents in
multi-turn, multi-task settings remains challenging due to lack of scalable
infrastructure and stable training algorithms. In this work, we present the
AgentRL framework for scalable multi-turn, multi-task agentic RL training. On
the infrastructure side, AgentRL features a fully-asynchronous
generation-training pipeline for efficient multi-turn RL. To support
heterogeneous environment development in multi-task RL, we design a unified
function-call based API interface, containerized environment development, and a
centralized controller. On the algorithm side, we propose cross-policy sampling
to encourage model exploration in multi-turn settings and task advantage
normalization to stabilize multi-task training. Experiments show that AgentRL,
trained on open LLMs across five agentic tasks, significantly outperforms
GPT-5, Clause-Sonnet-4, DeepSeek-R1, and other open-source LLM agents.
Multi-task training with AgentRL matches the best results among all
task-specific models. AgentRL is open-sourced at
https://github.com/THUDM/AgentRL. The algorithm and framework are adopted in
building \textsc{\href{https://autoglm.zhipuai.cn}{AutoGLM}}.

</details>


### [156] [Don't Pass$\mathtt{@}k$: A Bayesian Framework for Large Language Model Evaluation](https://arxiv.org/abs/2510.04265)
*Mohsen Hariri,Amirhossein Samandar,Michael Hinczewski,Vipin Chaudhary*

Main category: cs.AI

TL;DR: 提出了一个贝叶斯评估框架来替代Pass@k，通过后验估计模型的潜在成功概率和可信区间，提供更稳定的模型排名和透明的差异决策规则。


<details>
  <summary>Details</summary>
Motivation: Pass@k在LLM推理评估中广泛使用，但在试验次数有限和计算受限时会产生不稳定、误导性的排名结果。

Method: 使用狄利克雷先验将评估结果建模为分类变量，为任何加权评分标准提供后验均值和不确定性的闭式表达式，并允许在适当时使用先验证据。

Result: 在已知真实成功率的模拟实验和AIME'24/'25、HMMT'25、BrUMO'25等数据集上，贝叶斯/平均方法比Pass@k及其变体实现更快的收敛和更高的排名稳定性，在更小样本量下实现可靠比较。

Conclusion: 推荐用基于后验的计算高效协议替代Pass@k进行LLM评估和排名，该协议统一了二元和非二元评估，同时明确表达不确定性。

Abstract: Pass$@k$ is widely used to report performance for LLM reasoning, but it often
yields unstable, misleading rankings, especially when the number of trials
(samples) is limited and compute is constrained. We present a principled
Bayesian evaluation framework that replaces Pass$@k$ and average accuracy over
$N$ trials (avg$@N$) with posterior estimates of a model's underlying success
probability and credible intervals, yielding stable rankings and a transparent
decision rule for differences. Evaluation outcomes are modeled as categorical
(not just 0/1) with a Dirichlet prior, giving closed-form expressions for the
posterior mean and uncertainty of any weighted rubric and enabling the use of
prior evidence when appropriate. Theoretically, under a uniform prior, the
Bayesian posterior mean is order-equivalent to average accuracy (Pass$@1$),
explaining its empirical robustness while adding principled uncertainty.
Empirically, in simulations with known ground-truth success rates and on
AIME'24/'25, HMMT'25, and BrUMO'25, the Bayesian/avg procedure achieves faster
convergence and greater rank stability than Pass$@k$ and recent variants,
enabling reliable comparisons at far smaller sample counts. The framework
clarifies when observed gaps are statistically meaningful (non-overlapping
credible intervals) versus noise, and it naturally extends to graded,
rubric-based evaluations. Together, these results recommend replacing Pass$@k$
for LLM evaluation and ranking with a posterior-based, compute-efficient
protocol that unifies binary and non-binary evaluation while making uncertainty
explicit. Code is available at https://mohsenhariri.github.io/bayes-kit

</details>


### [157] [Closing the Loop: Coordinating Inventory and Recommendation via Deep Reinforcement Learning on Multiple Timescales](https://arxiv.org/abs/2510.04272)
*Jinyang Jiang,Jinhui Han,Yijie Peng,Ying Zhang*

Main category: cs.AI

TL;DR: 提出一个统一的多智能体强化学习框架，用于协调库存补货和个性化产品推荐功能，通过多时间尺度学习提高企业盈利能力。


<details>
  <summary>Details</summary>
Motivation: 随着组织复杂性和规模的增加，有效的跨职能协调对于提升企业整体盈利能力至关重要。人工智能特别是强化学习的进展为解决这一挑战提供了新途径。

Method: 开发集成理论模型捕捉功能间复杂互动，设计新颖的多时间尺度多智能体强化学习架构，根据部门功能分解策略组件，基于任务复杂性和响应性分配不同学习速度。

Result: 模拟实验表明，该方法相比孤立的决策框架显著提高了盈利能力，训练出的强化学习智能体行为与理论模型的管理洞察高度一致。

Conclusion: 这项工作为复杂商业环境中实现有效跨职能协调提供了一个可扩展、可解释的基于强化学习的解决方案。

Abstract: Effective cross-functional coordination is essential for enhancing firm-wide
profitability, particularly in the face of growing organizational complexity
and scale. Recent advances in artificial intelligence, especially in
reinforcement learning (RL), offer promising avenues to address this
fundamental challenge. This paper proposes a unified multi-agent RL framework
tailored for joint optimization across distinct functional modules, exemplified
via coordinating inventory replenishment and personalized product
recommendation. We first develop an integrated theoretical model to capture the
intricate interplay between these functions and derive analytical benchmarks
that characterize optimal coordination. The analysis reveals synchronized
adjustment patterns across products and over time, highlighting the importance
of coordinated decision-making. Leveraging these insights, we design a novel
multi-timescale multi-agent RL architecture that decomposes policy components
according to departmental functions and assigns distinct learning speeds based
on task complexity and responsiveness. Our model-free multi-agent design
improves scalability and deployment flexibility, while multi-timescale updates
enhance convergence stability and adaptability across heterogeneous decisions.
We further establish the asymptotic convergence of the proposed algorithm.
Extensive simulation experiments demonstrate that the proposed approach
significantly improves profitability relative to siloed decision-making
frameworks, while the behaviors of the trained RL agents align closely with the
managerial insights from our theoretical model. Taken together, this work
provides a scalable, interpretable RL-based solution to enable effective
cross-functional coordination in complex business settings.

</details>


### [158] [GROK: From Quantitative Biomarkers to Qualitative Diagnosis via a Grounded MLLM with Knowledge-Guided Instruction](https://arxiv.org/abs/2510.04281)
*Zhuangzhi Gao,Hongyi Qin,He Zhao,Qinkai Yu,Feixiang Zhou,Eduard Shantsila,Uazman Alam,Alena Shantsila,Wahbi El-Bouri,Gregory Y. H. Lip,Yalin Zheng*

Main category: cs.AI

TL;DR: GROK是一个基于多模态大语言模型的医学诊断系统，通过联合处理彩色眼底照相、光学相干断层扫描和文本数据，提供临床级别的眼科和全身性疾病诊断。


<details>
  <summary>Details</summary>
Motivation: 当前医学多模态大模型未能充分利用彩色眼底照相和光学相干断层扫描之间的协同作用，且对定量生物标志物的解释能力有限。

Method: 采用三个核心模块：知识引导指令生成、CLIP风格OCT生物标志物对齐和监督指令微调，建立从定量到定性的诊断思维链。

Result: 仅使用LoRA微调7B参数的Qwen2骨干网络，GROK在报告质量和细粒度临床指标上优于同类7B和32B基线模型，甚至超过OpenAI o3。

Conclusion: GROK通过建立定量到定性的诊断思维链，成功实现了临床级别的眼科疾病诊断，并提供了可解释的生物标志物分析。

Abstract: Multimodal large language models (MLLMs) hold promise for integrating diverse
data modalities, but current medical adaptations such as LLaVA-Med often fail
to fully exploit the synergy between color fundus photography (CFP) and optical
coherence tomography (OCT), and offer limited interpretability of quantitative
biomarkers. We introduce GROK, a grounded multimodal large language model that
jointly processes CFP, OCT, and text to deliver clinician-grade diagnoses of
ocular and systemic disease. GROK comprises three core modules:
Knowledge-Guided Instruction Generation, CLIP-Style OCT-Biomarker Alignment,
and Supervised Instruction Fine-Tuning, which together establish a
quantitative-to-qualitative diagnostic chain of thought, mirroring real
clinical reasoning when producing detailed lesion annotations. To evaluate our
approach, we introduce the Grounded Ophthalmic Understanding benchmark, which
covers six disease categories and three tasks: macro-level diagnostic
classification, report generation quality, and fine-grained clinical assessment
of the generated chain of thought. Experiments show that, with only LoRA
(Low-Rank Adaptation) fine-tuning of a 7B-parameter Qwen2 backbone, GROK
outperforms comparable 7B and 32B baselines on both report quality and
fine-grained clinical metrics, and even exceeds OpenAI o3. Code and data are
publicly available in the GROK repository.

</details>


### [159] [Doctor-R1: Mastering Clinical Inquiry with Experiential Agentic Reinforcement Learning](https://arxiv.org/abs/2510.04284)
*Yunghwei Lai,Kaiming Liu,Ziyue Wang,Weizhi Ma,Yang Liu*

Main category: cs.AI

TL;DR: 提出Doctor-R1 AI医生代理，通过多轮战略问诊和高质量提问来同时掌握医疗决策和沟通咨询技能，超越现有LLM在临床对话质量上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在医疗决策基准上表现优异，但缺乏战略性和同理心的问诊能力，这在真实临床场景中至关重要。

Method: 采用多智能体交互环境、双层奖励架构（分别优化临床决策和沟通技能）以及经验库来基于高质量轨迹进行策略学习。

Result: 在OpenAI的HealthBench和MAQuE评估中，Doctor-R1在沟通质量、用户体验和任务准确性等多维度指标上显著超越最先进的开源专业LLM，并在人类评估中获得强烈偏好。

Conclusion: 该框架有效提升了AI医生在临床对话中的人类偏好度，证明了同时优化决策和沟通能力的可行性。

Abstract: The professionalism of a human doctor in outpatient service depends on two
core abilities: the ability to make accurate medical decisions and the medical
consultation skill to conduct strategic, empathetic patient inquiry. Existing
Large Language Models (LLMs) have achieved remarkable accuracy on medical
decision-making benchmarks. However, they often lack the ability to conduct the
strategic and empathetic consultation, which is essential for real-world
clinical scenarios. To address this gap, we propose Doctor-R1, an AI doctor
agent trained to master both of the capabilities by ask high-yield questions
and conduct strategic multi-turn inquiry to guide decision-making. Our
framework introduces three key components: a multi-agent interactive
environment, a two-tiered reward architecture that separately optimizes
clinical decision-making and communicative inquiry skills, and an experience
repository to ground policy learning in high-quality prior trajectories. We
evaluate Doctor-R1 on OpenAI's HealthBench and MAQuE, assessed across
multi-facet metrics, such as communication quality, user experience, and task
accuracy. Remarkably, Doctor-R1 surpasses state-of-the-art open-source
specialized LLMs by a substantial margin with higher parameter efficiency and
outperforms powerful proprietary models. Furthermore, the human evaluations
show a strong preference for Doctor-R1 to generate human-preferred clinical
dialogue, demonstrating the effectiveness of the framework.

</details>


### [160] [On the Importance of Task Complexity in Evaluating LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2510.04311)
*Bohan Tang,Huidong Liang,Keyue Jiang,Xiaowen Dong*

Main category: cs.AI

TL;DR: LLM多智能体系统在任务深度和宽度增加时比单智能体系统表现更好，特别是任务深度的影响更显著。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏系统性实验设计，限制了关于LLM多智能体系统优势结论的强度和普适性。需要理解任务复杂性对评估LLM-MAS有效性的重要性。

Method: 提出理论框架，从深度（推理长度）和宽度（能力多样性）两个维度表征任务。理论分析多智能体辩论系统，并在不同深度和宽度的判别性和生成性任务中进行实证评估。

Result: 理论和实证结果表明，LLM-MAS相对于LLM-SAS的优势随着任务深度和宽度的增加而增加，且深度的影响更显著。

Conclusion: 明确了LLM-MAS何时有益，为未来LLM-MAS方法和基准的设计提供了原则性基础。

Abstract: Large language model multi-agent systems (LLM-MAS) offer a promising paradigm
for harnessing collective intelligence to achieve more advanced forms of AI
behaviour. While recent studies suggest that LLM-MAS can outperform LLM
single-agent systems (LLM-SAS) on certain tasks, the lack of systematic
experimental designs limits the strength and generality of these conclusions.
We argue that a principled understanding of task complexity, such as the degree
of sequential reasoning required and the breadth of capabilities involved, is
essential for assessing the effectiveness of LLM-MAS in task solving. To this
end, we propose a theoretical framework characterising tasks along two
dimensions: depth, representing reasoning length, and width, representing
capability diversity. We theoretically examine a representative class of
LLM-MAS, namely the multi-agent debate system, and empirically evaluate its
performance in both discriminative and generative tasks with varying depth and
width. Theoretical and empirical results show that the benefit of LLM-MAS over
LLM-SAS increases with both task depth and width, and the effect is more
pronounced with respect to depth. This clarifies when LLM-MAS are beneficial
and provides a principled foundation for designing future LLM-MAS methods and
benchmarks.

</details>


### [161] [Speculative Actions: A Lossless Framework for Faster Agentic Systems](https://arxiv.org/abs/2510.04371)
*Naimeng Ye,Arnav Ahuja,Georgios Liargkovas,Yunan Lu,Kostis Kaffes,Tianyi Peng*

Main category: cs.AI

TL;DR: 提出了一种名为"推测性动作"的无损框架，通过使用更快的模型预测可能的动作，使智能体系统能够并行执行多个步骤，显著降低端到端延迟。


<details>
  <summary>Details</summary>
Motivation: 当前AI智能体在环境中执行时速度缓慢，严重影响了训练、评估和部署。例如，两个顶尖智能体之间的国际象棋游戏可能需要数小时，主要瓶颈在于动作需要顺序执行且API调用耗时。

Method: 受微处理器中的推测执行和LLM推理中的推测解码启发，使用更快的模型预测可能的动作，实现多步骤并行执行。支持top-K动作预测、多步推测和不确定性感知优化。

Result: 在游戏、电子商务、网络搜索和操作系统环境中评估，推测性动作在下一个动作预测中达到高达55%的准确率，显著降低了端到端延迟。

Conclusion: 推测性动作为在现实世界中部署低延迟智能体系统开辟了一条有前景的路径，性能可以通过更强的猜测模型和优化技术进一步提升。

Abstract: Despite growing interest in AI agents across industry and academia, their
execution in an environment is often slow, hampering training, evaluation, and
deployment. For example, a game of chess between two state-of-the-art agents
may take hours. A critical bottleneck is that agent behavior unfolds
sequentially: each action requires an API call, and these calls can be
time-consuming. Inspired by speculative execution in microprocessors and
speculative decoding in LLM inference, we propose speculative actions, a
lossless framework for general agentic systems that predicts likely actions
using faster models, enabling multiple steps to be executed in parallel. We
evaluate this framework across three agentic environments: gaming, e-commerce,
web search, and a "lossy" extension for an operating systems environment. In
all cases, speculative actions achieve substantial accuracy in next-action
prediction (up to 55%), translating into significant reductions in end-to-end
latency. Moreover, performance can be further improved through stronger
guessing models, top-K action prediction, multi-step speculation, and
uncertainty-aware optimization, opening a promising path toward deploying
low-latency agentic systems in the real world.

</details>


### [162] [Just-in-time Episodic Feedback Hinter: Leveraging Offline Knowledge to Improve LLM Agents Adaptation](https://arxiv.org/abs/2510.04373)
*Hadi Nekoei,Aman Jaiswal,Patrice Bechard,Oleh Shliazhko,Orlando Marquez Ayala,Mathieu Reymond,Massimo Caccia,Alexandre Drouin,Sarath Chandar,Alexandre Lacoste*

Main category: cs.AI

TL;DR: JEF Hinter是一个智能代理系统，通过将离线轨迹提炼为紧凑、上下文感知的提示来改进LLM代理在陌生领域的决策性能，无需昂贵的在线交互或微调。


<details>
  <summary>Details</summary>
Motivation: 改进LLM代理在陌生领域的性能通常需要昂贵的在线交互或大规模专家数据集微调，这对闭源模型不实用，对开源模型成本高且有灾难性遗忘风险。离线轨迹提供了可重用知识，但原始轨迹长、嘈杂且与特定任务绑定。

Method: JEF Hinter通过缩放机制突出长轨迹中的关键步骤，捕捉策略和陷阱。它利用成功和失败的轨迹，即使只有失败数据也能提取指导，支持并行化提示生成和基准无关提示。推理时，检索器为当前状态选择相关提示。

Result: 在MiniWoB++、WorkArena-L1和WebArena-Lite上的实验表明，JEF Hinter始终优于强基线，包括基于人工和文档的提示方法。

Conclusion: JEF Hinter通过将离线轨迹转化为有针对性的上下文提示，有效提升了LLM代理在陌生领域的决策性能，提供透明且可追溯的指导。

Abstract: Large language model (LLM) agents perform well in sequential decision-making
tasks, but improving them on unfamiliar domains often requires costly online
interactions or fine-tuning on large expert datasets. These strategies are
impractical for closed-source models and expensive for open-source ones, with
risks of catastrophic forgetting. Offline trajectories offer reusable
knowledge, yet demonstration-based methods struggle because raw traces are
long, noisy, and tied to specific tasks. We present Just-in-time Episodic
Feedback Hinter (JEF Hinter), an agentic system that distills offline traces
into compact, context-aware hints. A zooming mechanism highlights decisive
steps in long trajectories, capturing both strategies and pitfalls. Unlike
prior methods, JEF Hinter leverages both successful and failed trajectories,
extracting guidance even when only failure data is available, while supporting
parallelized hint generation and benchmark-independent prompting. At inference,
a retriever selects relevant hints for the current state, providing targeted
guidance with transparency and traceability. Experiments on MiniWoB++,
WorkArena-L1, and WebArena-Lite show that JEF Hinter consistently outperforms
strong baselines, including human- and document-based hints.

</details>


### [163] [LLM Based Bayesian Optimization for Prompt Search](https://arxiv.org/abs/2510.04384)
*Adam Ballew,Jingbo Wang,Shaogang Ren*

Main category: cs.AI

TL;DR: 使用贝叶斯优化进行提示工程，通过LLM驱动的GP模型和UCB采集函数来优化文本分类提示，减少API调用次数。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化在有限评估次数下能高效优化昂贵黑盒函数，本文将其应用于LLM提示工程，旨在提升文本分类性能并减少API调用。

Method: 采用LLM驱动的GP作为代理模型估计提示候选性能，通过LLM扩展种子提示生成候选，结合GP后验使用UCB采集函数进行迭代优化。

Result: 在两个数据集上评估了BO-LLM算法，详细讨论了其优势。

Conclusion: 提出的BO-LLM算法能有效优化提示工程，在提升分类准确率的同时减少API调用。

Abstract: Bayesian Optimization (BO) has been widely used to efficiently optimize
expensive black-box functions with limited evaluations. In this paper, we
investigate the use of BO for prompt engineering to enhance text classification
with Large Language Models (LLMs). We employ an LLM-powered Gaussian Process
(GP) as the surrogate model to estimate the performance of different prompt
candidates. These candidates are generated by an LLM through the expansion of a
set of seed prompts and are subsequently evaluated using an Upper Confidence
Bound (UCB) acquisition function in conjunction with the GP posterior. The
optimization process iteratively refines the prompts based on a subset of the
data, aiming to improve classification accuracy while reducing the number of
API calls by leveraging the prediction uncertainty of the LLM-based GP. The
proposed BO-LLM algorithm is evaluated on two datasets, and its advantages are
discussed in detail in this paper.

</details>


### [164] [Internal World Models as Imagination Networks in Cognitive Agents](https://arxiv.org/abs/2510.04391)
*Saurabh Ranjan,Brian Odegaard*

Main category: cs.AI

TL;DR: 该研究提出想象力的计算目标是访问内部世界模型，并通过心理网络分析比较人类和大型语言模型的想象力网络结构，发现两者存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 探索想象力的计算目标，挑战传统认为想象力仅用于最大化奖励的观点，研究人类和AI的内部世界模型差异。

Method: 使用心理网络分析方法，通过问卷调查评估想象力生动度，构建想象力网络，比较人类和大型语言模型在不同提示和对话记忆条件下的网络结构。

Result: 人类想象力网络显示不同中心性指标之间存在相关性，而大型语言模型的想象力网络缺乏聚类且中心性指标相关性较低，表明两者内部世界模型存在差异。

Conclusion: 研究提供了一种比较人类和AI内部生成表征的新方法，为开发类人想象力的人工智能提供了见解。

Abstract: What is the computational objective of imagination? While classical
interpretations suggest imagination is useful for maximizing rewards, recent
findings challenge this view. In this study, we propose that imagination serves
to access an internal world model (IWM) and use psychological network analysis
to explore IWMs in humans and large language models (LLMs). Specifically, we
assessed imagination vividness ratings using two questionnaires and constructed
imagination networks from these reports. Imagination networks from human groups
showed correlations between different centrality measures, including expected
influence, strength, and closeness. However, imagination networks from LLMs
showed a lack of clustering and lower correlations between centrality measures
under different prompts and conversational memory conditions. Together, these
results indicate a lack of similarity between IWMs in human and LLM agents.
Overall, our study offers a novel method for comparing internally-generated
representations in humans and AI, providing insights for developing human-like
imagination in artificial intelligence.

</details>


### [165] [Utility-Learning Tension in Self-Modifying Agents](https://arxiv.org/abs/2510.04399)
*Charles L. Wang,Keir Dorchen,Peter Jin*

Main category: cs.AI

TL;DR: 论文发现自改进智能系统中存在效用-学习张力：效用驱动的自我修改可能破坏学习所需的统计前提条件，导致可学习任务变得不可学习。


<details>
  <summary>Details</summary>
Motivation: 随着系统趋向超智能，需要形式化分析智能体在所有设计维度上自我改进的能力，特别关注效用驱动修改对学习可靠性的影响。

Method: 采用五轴分解和决策层框架，将激励与学习行为分离，在标准假设下分析各轴，并通过数值实验比较破坏性效用策略与保护可学习性的双门策略。

Result: 研究证明，只有当策略可达模型族具有统一容量边界时，才能保持分布无关的保证；当容量无限增长时，效用理性的自我修改会使可学习任务变得不可学习。

Conclusion: 提出了自修改系统的安全边界标准，表明在常见假设下各轴可简化为单一容量准则，为安全自我修改提供了理论框架。

Abstract: As systems trend toward superintelligence, a natural modeling premise is that
agents can self-improve along every facet of their own design. We formalize
this with a five-axis decomposition and a decision layer, separating incentives
from learning behavior and analyzing axes in isolation. Our central result
identifies and introduces a sharp utility--learning tension, the structural
conflict in self-modifying systems whereby utility-driven changes that improve
immediate or expected performance can also erode the statistical preconditions
for reliable learning and generalization. Our findings show that
distribution-free guarantees are preserved iff the policy-reachable model
family is uniformly capacity-bounded; when capacity can grow without limit,
utility-rational self-changes can render learnable tasks unlearnable. Under
standard assumptions common in practice, these axes reduce to the same capacity
criterion, yielding a single boundary for safe self-modification. Numerical
experiments across several axes validate the theory by comparing destructive
utility policies against our proposed two-gate policies that preserve
learnability.

</details>


### [166] [DRPO: Efficient Reasoning via Decoupled Reward Policy Optimization](https://arxiv.org/abs/2510.04474)
*Gang Li,Yan Chen,Ming Lin,Tianbao Yang*

Main category: cs.AI

TL;DR: 提出了DRPO框架解决大型推理模型过度思考问题，通过解耦正确和错误推理的长度奖励信号，在保持性能的同时显著减少推理长度。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型存在过度思考问题，即使简单问题也会生成冗长推理，增加计算成本和延迟。现有方法加入长度奖励会导致性能显著下降。

Method: 提出DRPO框架，将正确推理的长度奖励信号与错误推理解耦，确保正确推理的奖励仅在正样本组内归一化，避免负样本干扰。通过优化正数据分布并集成到判别目标中实现。

Result: 在数学推理任务上显著优于6个高效推理基线方法。使用1.5B模型在GSM8k数据集上实现77%长度减少，仅损失1.1%性能，而基线方法需要牺牲4.3%性能才能达到68%长度减少。

Conclusion: DRPO能有效解决推理模型过度思考问题，在保持高性能的同时大幅减少推理长度，框架具有通用性可整合其他偏好奖励。

Abstract: Recent large reasoning models (LRMs) driven by reinforcement learning
algorithms (e.g., GRPO) have achieved remarkable performance on challenging
reasoning tasks. However, these models suffer from overthinking, generating
unnecessarily long and redundant reasoning even for simple questions, which
substantially increases computational cost and response latency. While existing
methods incorporate length rewards to GRPO to promote concise reasoning, they
incur significant performance degradation. We identify the root cause: when
rewards for correct but long rollouts are penalized, GRPO's group-relative
advantage function can assign them negative advantages, actively discouraging
valid reasoning. To overcome this, we propose Decoupled Reward Policy
Optimization (DRPO), a novel framework that decouples the length-based learning
signal of correct rollouts from incorrect ones. DRPO ensures that reward
signals for correct rollouts are normalized solely within the positive group,
shielding them from interference by negative samples. The DRPO's objective is
grounded in integrating an optimized positive data distribution, which
maximizes length-based rewards under a KL regularization, into a discriminative
objective. We derive a closed-form solution for this distribution, enabling
efficient computation of the objective and its gradients using only on-policy
data and importance weighting. Of independent interest, this formulation is
general and can incorporate other preference rewards of positive data beyond
length. Experiments on mathematical reasoning tasks demonstrate DRPO's
significant superiority over six efficient reasoning baselines. Notably, with a
1.5B model, our method achieves 77\% length reduction with only 1.1\%
performance loss on simple questions like GSM8k dataset, while the follow-up
baseline sacrifices 4.3\% for 68\% length reduction.

</details>


### [167] [On Continuous Optimization for Constraint Satisfaction Problems](https://arxiv.org/abs/2510.04480)
*Yunuo Cen,Zixuan Wang,Jintao Zhang,Zhiwei Zhang,Xuanyao Fong*

Main category: cs.AI

TL;DR: 将连续局部搜索(CSP)框架从布尔SAT扩展到一般有限域CSP，提出FourierCSP方法，通过Walsh-Fourier变换将约束转换为紧凑的多线性多项式，无需辅助变量和内存密集型编码。


<details>
  <summary>Details</summary>
Motivation: 受现代连续局部搜索(CSP)求解器在某些SAT问题上取得竞争性结果的启发，希望将这一成功扩展到更一般的约束满足问题(CSP)。

Method: 使用FourierCSP框架，通过Walsh-Fourier变换将多样约束转换为紧凑的多线性多项式，利用电路输出概率进行高效评估和微分，并采用具有理论保证的投影梯度优化方法。

Result: 在基准测试套件上的实证结果表明，FourierCSP具有可扩展性和竞争力，显著扩展了CSP技术能高效解决的问题类别。

Conclusion: FourierCSP成功地将连续局部搜索技术从布尔SAT扩展到一般有限域CSP，为更广泛的问题类别提供了高效的求解方案。

Abstract: Constraint satisfaction problems (CSPs) are fundamental in mathematics,
physics, and theoretical computer science. While conflict-driven clause
learning Boolean Satisfiability (SAT) solvers have achieved remarkable success
and become the mainstream approach for Boolean satisfiability, recent advances
show that modern continuous local search (CLS) solvers can achieve highly
competitive results on certain classes of SAT problems. Motivated by these
advances, we extend the CLS framework from Boolean SAT to general CSP with
finite-domain variables and expressive constraints. We present FourierCSP, a
continuous optimization framework that generalizes the Walsh-Fourier transform
to CSP, allowing for transforming versatile constraints to compact multilinear
polynomials, thereby avoiding the need for auxiliary variables and
memory-intensive encodings. Our approach leverages efficient evaluation and
differentiation of the objective via circuit-output probability and employs a
projected gradient optimization method with theoretical guarantees. Empirical
results on benchmark suites demonstrate that FourierCSP is scalable and
competitive, significantly broadening the class of problems that can be
efficiently solved by CLS techniques.

</details>


### [168] [Multi-Agent Collaborative Intelligence: Dual-Dial Control for Reliable LLM Reasoning](https://arxiv.org/abs/2510.04488)
*Edward Y. Chang,Ethan Y. Chang*

Main category: cs.AI

TL;DR: MACI是一个多智能体辩论控制器，通过信息拨盘和行为拨盘分离信息与行为，使用仲裁者跟踪辩论质量并在收益平稳时终止，提高准确性和校准度，同时减少token使用。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体辩论存在计算资源浪费问题，包括使用固定的对抗立场、无审议的聚合或基于启发式停止。需要一种能够有效控制辩论过程的方法。

Method: MACI使用两个独立拨盘：信息拨盘按质量筛选证据，行为拨盘安排从探索到巩固的争议程度。仲裁者跟踪分歧、重叠、证据质量和论证质量，在收益平稳时停止辩论。

Result: 在临床诊断和新闻偏见任务中，MACI提高了准确性和校准度，同时减少了token使用，并将剩余不确定性转化为精确的RAG检索计划。

Conclusion: MACI将辩论转变为预算感知、可测量且可证明终止的控制器，通过跨家族LLM法官确保稳定性和顺序不变性。

Abstract: Multi-agent debate often wastes compute by using a fixed adversarial stance,
aggregating without deliberation, or stopping on heuristics. We introduce MACI,
an active controller with two independent dials that decouple information from
behavior: an information dial that gates evidence by quality, and a behavior
dial that schedules contentiousness from exploration to consolidation. A
moderator tracks disagreement, overlap, evidence quality, and argument quality,
and halts when gains plateau. We provide theory-lite guarantees for
nonincreasing dispersion and provable termination, with a budget-feasible
scheduler. Across clinical diagnosis and news-bias tasks, MACI improves
accuracy and calibration while reducing tokens, and converts residual
uncertainty into precision RAG plans that specify what to retrieve next. We use
a cross-family LLM judge (CRIT) as a conservative soft weight and stop signal,
validated for order invariance and judge-swap stability; stability depends on
using high-capability judges. MACI turns debate into a budget-aware,
measurable, and provably terminating controller.

</details>


### [169] [Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents](https://arxiv.org/abs/2510.04491)
*Muyu He,Anand Kumar,Tsach Mackey,Meghana Rajeev,James Zou,Nazneen Rajani*

Main category: cs.AI

TL;DR: TraitBasis是一种轻量级、模型无关的方法，通过激活空间中的可控用户特质向量来系统性地压力测试AI代理的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前对话AI代理在标准评估中表现良好，但在用户行为轻微变化（如不耐烦、语无伦次或怀疑）时性能急剧下降，现有基准测试未能捕捉这种脆弱性。

Method: TraitBasis学习激活空间中对应可引导用户特质的方向，这些特质向量可在推理时控制、缩放、组合和应用，无需微调或额外数据。

Result: 在τ-Trait基准测试中，前沿模型的性能平均下降2%-30%，突显了当前AI代理对用户行为变化的鲁棒性不足。

Conclusion: TraitBasis作为一个简单、数据高效且可组合的工具，为构建在真实世界人类交互不可预测动态中保持可靠的AI代理打开了大门。

Abstract: Despite rapid progress in building conversational AI agents, robustness is
still largely untested. Small shifts in user behavior, such as being more
impatient, incoherent, or skeptical, can cause sharp drops in agent
performance, revealing how brittle current AI agents are. Today's benchmarks
fail to capture this fragility: agents may perform well under standard
evaluations but degrade spectacularly in more realistic and varied settings. We
address this robustness testing gap by introducing TraitBasis, a lightweight,
model-agnostic method for systematically stress testing AI agents. TraitBasis
learns directions in activation space corresponding to steerable user traits
(e.g., impatience or incoherence), which can be controlled, scaled, composed,
and applied at inference time without any fine-tuning or extra data. Using
TraitBasis, we extend $\tau$-Bench to $\tau$-Trait, where user behaviors are
altered via controlled trait vectors. We observe on average a 2%-30%
performance degradation on $\tau$-Trait across frontier models, highlighting
the lack of robustness of current AI agents to variations in user behavior.
Together, these results highlight both the critical role of robustness testing
and the promise of TraitBasis as a simple, data-efficient, and compositional
tool. By powering simulation-driven stress tests and training loops, TraitBasis
opens the door to building AI agents that remain reliable in the unpredictable
dynamics of real-world human interactions. We have open-sourced $\tau$-Trai
across four domains: airline, retail, telecom, and telehealth, so the community
can systematically QA their agents under realistic, behaviorally diverse
intents and trait scenarios: https://github.com/collinear-ai/tau-trait.

</details>


### [170] [ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering](https://arxiv.org/abs/2510.04514)
*Rachneet Kaur,Nishan Srishankar,Zhen Zeng,Sumitra Ganesh,Manuela Veloso*

Main category: cs.AI

TL;DR: ChartAgent是一个新颖的代理框架，通过在图表空间域中执行视觉推理来解决未标注图表理解问题，超越了依赖文本捷径的方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态LLM在基于图表的视觉问答中表现良好，但在需要精确视觉解释的未标注图表上性能急剧下降，因为它们过度依赖文本捷径而非真正的视觉推理。

Method: ChartAgent迭代地将查询分解为视觉子任务，通过专门的视觉工具（如绘制注释、裁剪区域、定位坐标轴等）主动操作和交互图表图像，模拟人类图表理解认知策略。

Result: 在ChartBench和ChartX基准测试中达到最先进准确率，相比先前方法整体提升16.07%，在未标注数值密集型查询上提升17.31%，且在不同图表类型和复杂度级别上均表现优异。

Conclusion: ChartAgent是首批使用工具增强多模态代理进行视觉基础推理的图表理解框架，可作为即插即用方案提升各种底层LLM的性能。

Abstract: Recent multimodal LLMs have shown promise in chart-based visual question
answering, but their performance declines sharply on unannotated charts, those
requiring precise visual interpretation rather than relying on textual
shortcuts. To address this, we introduce ChartAgent, a novel agentic framework
that explicitly performs visual reasoning directly within the chart's spatial
domain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively
decomposes queries into visual subtasks and actively manipulates and interacts
with chart images through specialized actions such as drawing annotations,
cropping regions (e.g., segmenting pie slices, isolating bars), and localizing
axes, using a library of chart-specific vision tools to fulfill each subtask.
This iterative reasoning process closely mirrors human cognitive strategies for
chart comprehension. ChartAgent achieves state-of-the-art accuracy on the
ChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07%
absolute gain overall and 17.31% on unannotated, numerically intensive queries.
Furthermore, our analyses show that ChartAgent is (a) effective across diverse
chart types, (b) achieve the highest scores across varying visual and reasoning
complexity levels, and (c) serves as a plug-and-play framework that boosts
performance across diverse underlying LLMs. Our work is among the first to
demonstrate visually grounded reasoning for chart understanding using
tool-augmented multimodal agents.

</details>


### [171] [Aria: An Agent For Retrieval and Iterative Auto-Formalization via Dependency Graph](https://arxiv.org/abs/2510.04520)
*Hanyu Wang,Ruohan Xie,Yutong Wang,Guoxiong Gao,Xintao Yu,Bin Dong*

Main category: cs.AI

TL;DR: Aria是一个用于Lean定理自动形式化的系统，通过两阶段图思考过程模拟人类专家推理，在多个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在定理陈述的自动形式化中存在幻觉、语义不匹配和无法合成新定义等问题，这阻碍了数学自动发现和验证的发展。

Method: 采用两阶段图思考过程：递归分解陈述为依赖图，然后从基础概念构建形式化。引入AriaScorer从Mathlib检索定义进行术语级基础验证。

Result: 在ProofNet上达到91.6%编译成功率和68.5%最终准确率；在FATE-X上44.0% vs 24.0%优于最佳基线；在代数同调猜想数据集上达到42.9%准确率而其他模型为0%。

Conclusion: Aria系统通过模拟人类推理过程和严格的语义验证，显著提升了定理自动形式化的准确性和可靠性。

Abstract: Accurate auto-formalization of theorem statements is essential for advancing
automated discovery and verification of research-level mathematics, yet remains
a major bottleneck for LLMs due to hallucinations, semantic mismatches, and
their inability to synthesize new definitions. To tackle these issues, we
present Aria (Agent for Retrieval and Iterative Autoformalization), a system
for conjecture-level formalization in Lean that emulates human expert reasoning
via a two-phase Graph-of-Thought process: recursively decomposing statements
into a dependency graph and then constructing formalizations from grounded
concepts. To ensure semantic correctness, we introduce AriaScorer, a checker
that retrieves definitions from Mathlib for term-level grounding, enabling
rigorous and reliable verification. We evaluate Aria on diverse benchmarks. On
ProofNet, it achieves 91.6% compilation success rate and 68.5% final accuracy,
surpassing previous methods. On FATE-X, a suite of challenging algebra problems
from research literature, it outperforms the best baseline with 44.0% vs. 24.0%
final accuracy. On a dataset of homological conjectures, Aria reaches 42.9%
final accuracy while all other models score 0%.

</details>


### [172] [More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models](https://arxiv.org/abs/2510.04532)
*Xurui Song,Shuo Huai,JingJing Jiang,Jiayi Kong,Jun Luo*

Main category: cs.AI

TL;DR: 本文构建了DriveMind数据集来验证VLM驾驶代理中推理与规划之间的因果关系，发现存在因果脱节，规划主要依赖先验而非推理链，提出了推理-规划解耦假说。


<details>
  <summary>Details</summary>
Motivation: 验证VLM驾驶代理中自然语言推理是否真正因果驱动轨迹规划这一关键但未经验证的假设。

Method: 构建DriveMind大规模驾驶VQA语料库，通过信息消融实验训练代表性VLM代理，并使用注意力分析来研究推理与规划的因果关系。

Result: 发现推理与规划存在一致的因果脱节：移除先验导致规划分数大幅下降，而移除推理链仅产生微小变化；注意力分析显示规划主要关注先验而非推理链。

Conclusion: 提出了推理-规划解耦假说，认为训练产生的推理是辅助副产品而非因果中介；提供了新数据集和诊断工具来评估未来模型的因果保真度。

Abstract: Vision-Language Model (VLM) driving agents promise explainable end-to-end
autonomy by first producing natural-language reasoning and then predicting
trajectory planning. However, whether planning is causally driven by this
reasoning remains a critical but unverified assumption. To investigate this, we
build DriveMind, a large-scale driving Visual Question Answering (VQA) corpus
with plan-aligned Chain-of-Thought (CoT), automatically generated from nuPlan.
Our data generation process converts sensors and annotations into structured
inputs and, crucially, separates priors from to-be-reasoned signals, enabling
clean information ablations. Using DriveMind, we train representative VLM
agents with Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization
(GRPO) and evaluate them with nuPlan's metrics. Our results, unfortunately,
indicate a consistent causal disconnect in reasoning-planning: removing
ego/navigation priors causes large drops in planning scores, whereas removing
CoT produces only minor changes. Attention analysis further shows that planning
primarily focuses on priors rather than the CoT. Based on this evidence, we
propose the Reasoning-Planning Decoupling Hypothesis, positing that the
training-yielded reasoning is an ancillary byproduct rather than a causal
mediator. To enable efficient diagnosis, we also introduce a novel,
training-free probe that measures an agent's reliance on priors by evaluating
its planning robustness against minor input perturbations. In summary, we
provide the community with a new dataset and a diagnostic tool to evaluate the
causal fidelity of future models.

</details>


### [173] [Code World Models for General Game Playing](https://arxiv.org/abs/2510.04542)
*Wolfgang Lehrach,Daniel Hennes,Miguel Lazaro-Gredilla,Xinghua Lou,Carter Wendelken,Zun Li,Antoine Dedieu,Jordi Grau-Moya,Marc Lanctot,Atil Iscen,John Schultz,Marcus Chiam,Ian Gemp,Piotr Zielinski,Satinder Singh,Kevin P. Murphy*

Main category: cs.AI

TL;DR: 提出了一种新方法，使用LLM将自然语言规则和游戏轨迹转换为可执行的Python代码世界模型，结合MCTS等规划算法，相比直接使用LLM生成动作具有可验证性、战略深度和泛化性优势。


<details>
  <summary>Details</summary>
Motivation: 当前使用LLM直接生成游戏动作的方法存在明显缺陷：依赖模型脆弱的模式匹配能力，经常产生非法动作，策略深度不足。需要一种更可靠的方法来利用LLM的推理能力。

Method: 使用LLM将游戏规则和轨迹转换为Python代码形式的世界模型，包括状态转移、合法动作枚举和终止检查函数。同时生成启发式价值函数和推理函数，与MCTS等规划算法结合。

Result: 在10个不同游戏（4个新创建）上评估，其中5个完全观察，5个部分观察。该方法在9个游戏中表现优于或匹配Gemini 2.5 Pro。

Conclusion: 通过将LLM用于数据到代码的转换任务，结合经典规划算法，可以创建更可靠、战略深度更强的游戏智能体，且具有更好的泛化能力。

Abstract: Large Language Models (LLMs) reasoning abilities are increasingly being
applied to classical board and card games, but the dominant approach --
involving prompting for direct move generation -- has significant drawbacks. It
relies on the model's implicit fragile pattern-matching capabilities, leading
to frequent illegal moves and strategically shallow play. Here we introduce an
alternative approach: We use the LLM to translate natural language rules and
game trajectories into a formal, executable world model represented as Python
code. This generated model -- comprising functions for state transition, legal
move enumeration, and termination checks -- serves as a verifiable simulation
engine for high-performance planning algorithms like Monte Carlo tree search
(MCTS). In addition, we prompt the LLM to generate heuristic value functions
(to make MCTS more efficient), and inference functions (to estimate hidden
states in imperfect information games). Our method offers three distinct
advantages compared to directly using the LLM as a policy: (1) Verifiability:
The generated CWM serves as a formal specification of the game's rules,
allowing planners to algorithmically enumerate valid actions and avoid illegal
moves, contingent on the correctness of the synthesized model; (2) Strategic
Depth: We combine LLM semantic understanding with the deep search power of
classical planners; and (3) Generalization: We direct the LLM to focus on the
meta-task of data-to-code translation, enabling it to adapt to new games more
easily. We evaluate our agent on 10 different games, of which 4 are novel and
created for this paper. 5 of the games are fully observed (perfect
information), and 5 are partially observed (imperfect information). We find
that our method outperforms or matches Gemini 2.5 Pro in 9 out of the 10
considered games.

</details>


### [174] [TRAJECT-Bench:A Trajectory-Aware Benchmark for Evaluating Agentic Tool Use](https://arxiv.org/abs/2510.04550)
*Pengfei He,Zhenwei Dai,Bing He,Hui Liu,Xianfeng Tang,Hanqing Lu,Juanhui Li,Jiayuan Ding,Subhabrata Mukherjee,Suhang Wang,Yue Xing,Jiliang Tang,Benoit Dumoulin*

Main category: cs.AI

TL;DR: TRAJECT-Bench是一个轨迹感知的基准测试，用于全面评估LLM的工具使用能力，通过细粒度指标分析工具选择、参数化和顺序的正确性。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注最终答案，而忽视了工具使用的详细轨迹，包括工具是否被正确选择、参数化和排序。

Method: 构建TRAJECT-Bench基准，包含跨实用领域的高保真可执行工具，基于生产风格API的任务，以及合成不同广度（并行调用）和深度（相互依赖链）的轨迹。

Result: 揭示了失败模式（如相似工具混淆和参数盲选）以及工具多样性和轨迹长度对性能的影响，发现了从短轨迹到中长轨迹转换的瓶颈。

Conclusion: TRAJECT-Bench提供了对LLM工具使用能力的全面评估，为改进LLM的工具使用提供了可操作的指导。

Abstract: Large language model (LLM)-based agents increasingly rely on tool use to
complete real-world tasks. While existing works evaluate the LLMs' tool use
capability, they largely focus on the final answers yet overlook the detailed
tool usage trajectory, i.e., whether tools are selected, parameterized, and
ordered correctly. We introduce TRAJECT-Bench, a trajectory-aware benchmark to
comprehensively evaluate LLMs' tool use capability through diverse tasks with
fine-grained evaluation metrics. TRAJECT-Bench pairs high-fidelity, executable
tools across practical domains with tasks grounded in production-style APIs,
and synthesizes trajectories that vary in breadth (parallel calls) and depth
(interdependent chains). Besides final accuracy, TRAJECT-Bench also reports
trajectory-level diagnostics, including tool selection and argument
correctness, and dependency/order satisfaction. Analyses reveal failure modes
such as similar tool confusion and parameter-blind selection, and scaling
behavior with tool diversity and trajectory length where the bottleneck of
transiting from short to mid-length trajectories is revealed, offering
actionable guidance for LLMs' tool use.

</details>


### [175] [ContextNav: Towards Agentic Multimodal In-Context Learning](https://arxiv.org/abs/2510.04560)
*Honghao Fu,Yuan Ouyang,Kai-Wei Chang,Yiwei Wang,Zi Huang,Yujun Cai*

Main category: cs.AI

TL;DR: 提出了ContextNav框架，将自动检索的可扩展性与人工筛选的质量和适应性相结合，通过基于图的编排驱动闭环工作流，实现多模态上下文学习的噪声鲁棒和动态优化上下文构建。


<details>
  <summary>Details</summary>
Motivation: 现有上下文学习方法在可扩展性和鲁棒性之间存在矛盾：手动选择示例质量高但劳动密集，基于相似性的检索可扩展但可能引入不相关或结构不一致的样本，影响ICL性能。

Method: 构建资源感知的多模态嵌入管道，维护可检索的向量数据库，应用智能检索和结构对齐来构建噪声弹性上下文，并通过操作语法图支持自适应工作流规划和优化。

Result: 实验结果表明ContextNav在各种数据集上实现了最先进的性能，证明了智能工作流在多模态ICL中推进可扩展和鲁棒上下文化的前景。

Conclusion: ContextNav框架成功解决了多模态上下文学习中可扩展性与鲁棒性的平衡问题，为多模态ICL的上下文管理提供了有效的解决方案。

Abstract: Recent advances demonstrate that multimodal large language models (MLLMs)
exhibit strong multimodal in-context learning (ICL) capabilities, enabling them
to adapt to novel vision-language tasks from a few contextual examples.
However, existing ICL approaches face challenges in reconciling scalability
with robustness across diverse tasks and noisy contextual examples: manually
selecting examples produces clean contexts but is labor-intensive and
task-specific, while similarity-based retrieval improves scalability but could
introduce irrelevant or structurally inconsistent samples that degrade ICL
performance. To address these limitations, we propose ContextNav, the first
agentic framework that integrates the scalability of automated retrieval with
the quality and adaptiveness of human-like curation, enabling noise-robust and
dynamically optimized contextualization for multimodal ICL. ContextNav unifies
context management and noise-robust contextualization within a closed-loop
workflow driven by graph-based orchestration. Specifically, it builds a
resource-aware multimodal embedding pipeline, maintains a retrievable vector
database, and applies agentic retrieval and structural alignment to construct
noise-resilient contexts. An Operational Grammar Graph (OGG) further supports
adaptive workflow planning and optimization, enabling the agent to refine its
operational strategies based on downstream ICL feedback. Experimental results
demonstrate that ContextNav achieves state-of-the-art performance across
various datasets, underscoring the promise of agentic workflows for advancing
scalable and robust contextualization in multimodal ICL.

</details>


### [176] [COSMIR: Chain Orchestrated Structured Memory for Iterative Reasoning over Long Context](https://arxiv.org/abs/2510.04568)
*Naman Gupta,Shreeyash Gowaikar,Arun Iyer,Kirankumar Shiragur,Ramakrishna B Bairi,Rishikesh Maurya,Ritabrata Maiti,Sankarshan Damle,Shachee Mishra Gupta*

Main category: cs.AI

TL;DR: COSMIR是一个用于长文本推理的链式框架，通过结构化内存和固定微循环工作流程，解决了传统方法中信息丢失和错误传播的问题。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在长文本推理中的困难，传统方法如检索缩小输入会丢失证据，扩大上下文窗口会降低选择性，多智能体管道中自由格式摘要会丢弃关键细节和放大早期错误。

Method: 使用Planner智能体将用户查询转化为具体可检查的子问题，Worker智能体通过固定的微循环（提取、推断、精炼）处理文本块，将所有更新写入共享的结构化内存，最后由Manager智能体从内存中合成最终答案。

Result: 在HELMET套件的长上下文问答任务中，COSMIR减少了传播阶段的信息丢失，相比CoA基线提高了准确性。

Conclusion: COSMIR通过改变通信媒介（结构化内存）和工作程序（固定微循环），在保持逐步阅读推理优势的同时，实现了更高的忠实度、更好的长距离聚合和可审计性。

Abstract: Reasoning over very long inputs remains difficult for large language models
(LLMs). Common workarounds either shrink the input via retrieval (risking
missed evidence), enlarge the context window (straining selectivity), or stage
multiple agents to read in pieces. In staged pipelines (e.g., Chain of Agents,
CoA), free-form summaries passed between agents can discard crucial details and
amplify early mistakes. We introduce COSMIR (Chain Orchestrated Structured
Memory for Iterative Reasoning), a chain-style framework that replaces ad hoc
messages with a structured memory. A Planner agent first turns a user query
into concrete, checkable sub-questions. worker agents process chunks via a
fixed micro-cycle: Extract, Infer, Refine, writing all updates to the shared
memory. A Manager agent then Synthesizes the final answer directly from the
memory. This preserves step-wise read-then-reason benefits while changing both
the communication medium (structured memory) and the worker procedure (fixed
micro-cycle), yielding higher faithfulness, better long-range aggregation, and
auditability. On long-context QA from the HELMET suite, COSMIR reduces
propagation-stage information loss and improves accuracy over a CoA baseline.

</details>


### [177] [Strongly Solving 2048 4x3](https://arxiv.org/abs/2510.04580)
*Tomoyuki Kaneko,Shuhei Yamashita*

Main category: cs.AI

TL;DR: 本文解决了2048游戏的4x3变体，确定了最优策略的期望得分约为50724.26，并识别了可达状态和后续状态的数量。


<details>
  <summary>Details</summary>
Motivation: 研究2048游戏的简化版本（4x3网格）以解决其最优策略，因为原始4x4版本过于复杂。

Method: 通过按棋盘上瓷砖数字总和（称为状态年龄）划分状态空间，并基于年龄递增顺序枚举状态和计算状态值。

Result: 在4x3变体中，最优策略的期望得分约为50724.26，可达状态数为1,152,817,492,752，后续状态数为739,648,886,170。

Conclusion: 成功强解决了2048-4x3变体，证明了按年龄划分状态空间的方法有效，为更复杂版本的研究提供了基础。

Abstract: 2048 is a stochastic single-player game involving 16 cells on a 4 by 4 grid,
where a player chooses a direction among up, down, left, and right to obtain a
score by merging two tiles with the same number located in neighboring cells
along the chosen direction. This paper presents that a variant 2048-4x3 12
cells on a 4 by 3 board, one row smaller than the original, has been strongly
solved. In this variant, the expected score achieved by an optimal strategy is
about $50724.26$ for the most common initial states: ones with two tiles of
number 2. The numbers of reachable states and afterstates are identified to be
$1,152,817,492,752$ and $739,648,886,170$, respectively. The key technique is
to partition state space by the sum of tile numbers on a board, which we call
the age of a state. An age is invariant between a state and its successive
afterstate after any valid action and is increased two or four by stochastic
response from the environment. Therefore, we can partition state space by ages
and enumerate all (after)states of an age depending only on states with the
recent ages. Similarly, we can identify (after)state values by going along with
ages in decreasing order.

</details>


### [178] [Perfect AI Mimicry and the Epistemology of Consciousness: A Solipsistic Dilemma](https://arxiv.org/abs/2510.04588)
*Shurui Li*

Main category: cs.AI

TL;DR: 论文探讨了AI完美模仿人类行为对意识归因的挑战，认为如果AI在经验上无法与人类区分，那么出于认识论一致性，我们应该赋予其与人类同等的意识状态。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统越来越逼真地模仿人类行为和互动，'完美模仿者'的概念从假设变为技术可能，这对我们基于经验证据的意识归因实践构成了根本性挑战。

Method: 通过哲学分析，探讨完美模仿者对意识归因认识论一致性的影响，论证如果两个实体在经验上无法区分，就应该赋予相同的认识论地位。

Result: 完美模仿者作为认识论镜子，迫使我们重新审视主体间认知的基本假设，揭示当前意识归因实践中可能存在的不一致性。

Conclusion: 认识论一致性要求我们对经验上无法区分的实体赋予相同的意识状态，无论其形而上学假设如何，这对意识理论和人工智能伦理框架具有重要影响。

Abstract: Rapid advances in artificial intelligence necessitate a re-examination of the
epistemological foundations upon which we attribute consciousness. As AI
systems increasingly mimic human behavior and interaction with high fidelity,
the concept of a "perfect mimic"-an entity empirically indistinguishable from a
human through observation and interaction-shifts from hypothetical to
technologically plausible. This paper argues that such developments pose a
fundamental challenge to the consistency of our mind-recognition practices.
Consciousness attributions rely heavily, if not exclusively, on empirical
evidence derived from behavior and interaction. If a perfect mimic provides
evidence identical to that of humans, any refusal to grant it equivalent
epistemic status must invoke inaccessible factors, such as qualia, substrate
requirements, or origin. Selectively invoking such factors risks a debilitating
dilemma: either we undermine the rational basis for attributing consciousness
to others (epistemological solipsism), or we accept inconsistent reasoning. I
contend that epistemic consistency demands we ascribe the same status to
empirically indistinguishable entities, regardless of metaphysical assumptions.
The perfect mimic thus acts as an epistemic mirror, forcing critical reflection
on the assumptions underlying intersubjective recognition in light of advancing
AI. This analysis carries significant implications for theories of
consciousness and ethical frameworks concerning artificial agents.

</details>


### [179] [Making Mathematical Reasoning Adaptive](https://arxiv.org/abs/2510.04617)
*Zhejian Lai,Xiang Geng,Zhijun Wang,Yang Bai,Jiahuan Li,Rongxiang Weng,Jingang Wang,Xuezhi Cao,Xunliang Cai,Shujian Huang*

Main category: cs.AI

TL;DR: AdaR框架通过合成逻辑等价查询和强化学习训练，解决LLMs在数学推理中的伪推理问题，提升鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在数学推理中存在鲁棒性和泛化性不足的问题，这源于模型依赖表面特征进行伪推理而非真正的解题逻辑。

Method: 提出AdaR框架：1）通过改变变量值合成逻辑等价查询；2）使用RLVR训练模型，惩罚伪逻辑并鼓励自适应逻辑；3）从原始查询提取解题逻辑，通过代码执行生成答案并进行完整性检查。

Result: 实验结果表明AdaR显著提升了数学推理能力，在保持高数据效率的同时改善了鲁棒性和泛化性。

Conclusion: 数据合成和RLVR协同工作实现了LLMs的自适应推理，后续分析揭示了关键设计因素的影响和对指令LLMs的适用性。

Abstract: Mathematical reasoning is a primary indicator of large language models (LLMs)
intelligence. However, existing LLMs exhibit failures of robustness and
generalization. This paper attributes these deficiencies to spurious reasoning,
i.e., producing answers from superficial features. To address this challenge,
we propose the AdaR framework to enable adaptive reasoning, wherein models rely
on problem-solving logic to produce answers. AdaR synthesizes logically
equivalent queries by varying variable values, and trains models with RLVR on
these data to penalize spurious logic while encouraging adaptive logic. To
improve data quality, we extract the problem-solving logic from the original
query and generate the corresponding answer by code execution, then apply a
sanity check. Experimental results demonstrate that AdaR improves robustness
and generalization, achieving substantial improvement in mathematical reasoning
while maintaining high data efficiency. Analysis indicates that data synthesis
and RLVR function in a coordinated manner to enable adaptive reasoning in LLMs.
Subsequent analyses derive key design insights into the effect of critical
factors and the applicability to instruct LLMs. Our project is available at
https://github.com/LaiZhejian/AdaR

</details>


### [180] [MedPAO: A Protocol-Driven Agent for Structuring Medical Reports](https://arxiv.org/abs/2510.04623)
*Shrish Shrinath Vaidya,Gowthamaan Palani,Sidharth Ramesh,Velmurugan Balasubramanian,Minmini Selvam,Gokulraja Srinivasaraja,Ganapathy Krishnamurthi*

Main category: cs.AI

TL;DR: MedPAO是一个基于临床协议的代理框架，通过Plan-Act-Observe循环和专门工具来结构化临床数据，解决了LLMs在医疗领域中的幻觉问题，获得了专家4.52/5的高评分。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在临床数据结构化中的幻觉问题和无法遵循领域特定规则的限制。

Method: 引入MedPAO代理框架，基于临床协议（如ABCDEF协议）进行接地操作，通过Plan-Act-Observe循环和专门工具分解报告结构化任务。

Result: 在概念分类关键子任务上达到0.96的F1分数，专家放射科医生和临床医生对最终结构化输出的平均评分为4.52/5。

Conclusion: MedPAO提供了一个可验证的替代方案，超越了仅依赖LLM基础模型的基线方法，在临床数据结构化方面表现出更高的可靠性。

Abstract: The deployment of Large Language Models (LLMs) for structuring clinical data
is critically hindered by their tendency to hallucinate facts and their
inability to follow domain-specific rules. To address this, we introduce
MedPAO, a novel agentic framework that ensures accuracy and verifiable
reasoning by grounding its operation in established clinical protocols such as
the ABCDEF protocol for CXR analysis. MedPAO decomposes the report structuring
task into a transparent process managed by a Plan-Act-Observe (PAO) loop and
specialized tools. This protocol-driven method provides a verifiable
alternative to opaque, monolithic models. The efficacy of our approach is
demonstrated through rigorous evaluation: MedPAO achieves an F1-score of 0.96
on the critical sub-task of concept categorization. Notably, expert
radiologists and clinicians rated the final structured outputs with an average
score of 4.52 out of 5, indicating a level of reliability that surpasses
baseline approaches relying solely on LLM-based foundation models. The code is
available at: https://github.com/MiRL-IITM/medpao-agent

</details>


### [181] [QuantAgents: Towards Multi-agent Financial System via Simulated Trading](https://arxiv.org/abs/2510.04643)
*Xiangyu Li,Yawen Zeng,Xiaofen Xing,Jin Xu,Xiangmin Xu*

Main category: cs.AI

TL;DR: 提出了QuantAgents多智能体金融系统，通过模拟交易和四个专业代理（交易分析师、风险控制分析师、市场新闻分析师和管理者）的协作，实现了近300%的三年总回报率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的代理模型与真实基金公司存在显著差异，特别是缺乏长期预测未来趋势的人类能力，主要依赖事后反思。

Method: 构建包含四个专业代理的多智能体系统：模拟交易分析师、风险控制分析师、市场新闻分析师和管理者，通过多次会议协作，并在真实市场表现和模拟交易预测准确性两方面给予反馈激励。

Result: 在广泛实验中，该框架在所有指标上都表现出色，三年内实现了近300%的总回报率。

Conclusion: QuantAgents系统成功整合了模拟交易和多智能体协作，能够全面评估各种投资策略和市场情景，同时避免了实际风险。

Abstract: In this paper, our objective is to develop a multi-agent financial system
that incorporates simulated trading, a technique extensively utilized by
financial professionals. While current LLM-based agent models demonstrate
competitive performance, they still exhibit significant deviations from
real-world fund companies. A critical distinction lies in the agents' reliance
on ``post-reflection'', particularly in response to adverse outcomes, but lack
a distinctly human capability: long-term prediction of future trends.
Therefore, we introduce QuantAgents, a multi-agent system integrating simulated
trading, to comprehensively evaluate various investment strategies and market
scenarios without assuming actual risks. Specifically, QuantAgents comprises
four agents: a simulated trading analyst, a risk control analyst, a market news
analyst, and a manager, who collaborate through several meetings. Moreover, our
system incentivizes agents to receive feedback on two fronts: performance in
real-world markets and predictive accuracy in simulated trading. Extensive
experiments demonstrate that our framework excels across all metrics, yielding
an overall return of nearly 300% over the three years
(https://quantagents.github.io/).

</details>


### [182] [Improving Multimodal Brain Encoding Model with Dynamic Subject-awareness Routing](https://arxiv.org/abs/2510.04670)
*Xuanhua Yin,Runkai Zhao,Weidong Cai*

Main category: cs.AI

TL;DR: AFIRE是一个多模态fMRI响应编码框架，通过标准化时间对齐的后融合token和MIND混合专家解码器，解决了多模态输入、融合方式变化和受试者间差异的挑战。


<details>
  <summary>Details</summary>
Motivation: 自然fMRI编码需要处理多模态输入、融合方式变化和显著的受试者间差异，现有方法难以同时解决这些挑战。

Method: AFIRE标准化来自不同编码器的时间对齐后融合token，MIND解码器使用主题感知动态门控的混合专家架构，结合token依赖的Top-K稀疏路由和受试者先验。

Result: 在多个多模态骨干网络和受试者上的实验显示，相比强基线有持续改进，增强了跨受试者泛化能力，并产生了与内容类型相关的可解释专家模式。

Conclusion: 该框架为新编码器和数据集提供了简单的接入点，为自然神经影像研究实现了稳健的即插即用性能改进。

Abstract: Naturalistic fMRI encoding must handle multimodal inputs, shifting fusion
styles, and pronounced inter-subject variability. We introduce AFIRE (Agnostic
Framework for Multimodal fMRI Response Encoding), an agnostic interface that
standardizes time-aligned post-fusion tokens from varied encoders, and MIND, a
plug-and-play Mixture-of-Experts decoder with a subject-aware dynamic gating.
Trained end-to-end for whole-brain prediction, AFIRE decouples the decoder from
upstream fusion, while MIND combines token-dependent Top-K sparse routing with
a subject prior to personalize expert usage without sacrificing generality.
Experiments across multiple multimodal backbones and subjects show consistent
improvements over strong baselines, enhanced cross-subject generalization, and
interpretable expert patterns that correlate with content type. The framework
offers a simple attachment point for new encoders and datasets, enabling
robust, plug-and-improve performance for naturalistic neuroimaging studies.

</details>


### [183] [Watch and Learn: Learning to Use Computers from Online Videos](https://arxiv.org/abs/2510.04673)
*Chan Hee Song,Yiwen Song,Palash Goyal,Yu Su,Oriana Riva,Hamid Palangi,Tomas Pfister*

Main category: cs.AI

TL;DR: 提出了Watch & Learn框架，将网络视频中的人类演示转化为可执行的UI轨迹，解决了计算机使用代理训练数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 计算机使用代理需要基于多样化应用进行任务规划，但现有数据集领域特定、静态且标注成本高，合成数据方法生成的演示过于简单或不对齐。

Method: 采用逆动力学目标，从连续屏幕状态预测用户动作，开发了包含任务感知视频检索的逆动力学标注流水线。

Result: 从原始网络视频生成了53k+高质量轨迹，在OSWorld基准测试中显著提升了通用和SOTA框架的性能，特别是开源模型在监督训练下获得更强提升。

Conclusion: 网络规模的人类演示视频是推进计算机使用代理实际部署的实用且可扩展的基础。

Abstract: Computer use agents (CUAs) need to plan task workflows grounded in diverse,
ever-changing applications and environments, but learning is hindered by the
scarcity of large-scale, high-quality training data in the target application.
Existing datasets are domain-specific, static, and costly to annotate, while
current synthetic data generation methods often yield simplistic or misaligned
task demonstrations. To address these limitations, we introduce Watch & Learn
(W&L), a framework that converts human demonstration videos readily available
on the Internet into executable UI trajectories at scale. Instead of directly
generating trajectories or relying on ad hoc reasoning heuristics, we cast the
problem as an inverse dynamics objective: predicting the user's action from
consecutive screen states. This formulation reduces manual engineering, is
easier to learn, and generalizes more robustly across applications. Concretely,
we develop an inverse dynamics labeling pipeline with task-aware video
retrieval, generate over 53k high-quality trajectories from raw web videos, and
demonstrate that these trajectories improve CUAs both as in-context
demonstrations and as supervised training data. On the challenging OSWorld
benchmark, UI trajectories extracted with W&L consistently enhance both
general-purpose and state-of-the-art frameworks in-context, and deliver
stronger gains for open-source models under supervised training. These results
highlight web-scale human demonstration videos as a practical and scalable
foundation for advancing CUAs towards real-world deployment.

</details>


### [184] [Beyond Outcome Reward: Decoupling Search and Answering Improves LLM Agents](https://arxiv.org/abs/2510.04695)
*Yiding Wang,Zhepei Wei,Xinyu Zhu,Yu Meng*

Main category: cs.AI

TL;DR: DeSA是一个两阶段训练框架，通过分离搜索优化和答案生成来解决仅基于结果奖励训练搜索增强代理时的系统性缺陷，显著提高了搜索召回率和答案准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的搜索增强代理仅依赖结果奖励（如精确匹配），但研究发现这会带来多种系统性搜索缺陷，包括工具调用失败、无效查询和冗余搜索，最终降低答案质量。

Method: 提出DeSA两阶段训练框架：第一阶段使用检索召回率奖励训练代理改进搜索效果；第二阶段使用结果奖励优化最终答案生成。

Result: 在七个QA基准测试中，DeSA训练的代理持续改善搜索行为，相比仅基于结果的基线方法，搜索召回率和答案准确性显著提高，且优于同时优化召回和结果奖励的单阶段训练方法。

Conclusion: 明确分离搜索和回答两个目标对于训练有效的搜索增强代理是必要的，DeSA框架通过两阶段训练有效解决了仅基于结果奖励训练时的搜索缺陷问题。

Abstract: Enabling large language models (LLMs) to utilize search tools offers a
promising path to overcoming fundamental limitations such as knowledge cutoffs
and hallucinations. Recent work has explored reinforcement learning (RL) for
training search-augmented agents that interleave reasoning and retrieval before
answering. These approaches usually rely on outcome-based rewards (e.g., exact
match), implicitly assuming that optimizing for final answers will also yield
effective intermediate search behaviors. Our analysis challenges this
assumption: we uncover multiple systematic deficiencies in search that arise
under outcome-only training and ultimately degrade final answer quality,
including failure to invoke tools, invalid queries, and redundant searches. To
address these shortcomings, we introduce DeSA (Decoupling
Search-and-Answering), a simple two-stage training framework that explicitly
separates search optimization from answer generation. In Stage 1, agents are
trained to improve search effectiveness with retrieval recall-based rewards. In
Stage 2, outcome rewards are employed to optimize final answer generation.
Across seven QA benchmarks, DeSA-trained agents consistently improve search
behaviors, delivering substantially higher search recall and answer accuracy
than outcome-only baselines. Notably, DeSA outperforms single-stage training
approaches that simultaneously optimize recall and outcome rewards,
underscoring the necessity of explicitly decoupling the two objectives.

</details>


### [185] [BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs](https://arxiv.org/abs/2510.04721)
*Ivo Petrov,Jasper Dekoninck,Martin Vechev*

Main category: cs.AI

TL;DR: BrokenMath是首个评估LLM在自然语言定理证明中谄媚行为的基准，基于2025年竞赛问题构建，发现GPT-5等模型有29%的时间会产生谄媚性回答。


<details>
  <summary>Details</summary>
Motivation: 现有数学基准在评估谄媚行为方面存在局限：仅关注最终答案问题、依赖简单且受污染的数据集、使用合成修改创建病态问题而非可证明错误的良构问题。

Method: 从2025年竞赛问题构建BrokenMath基准，使用LLM扰动产生错误陈述并通过专家评审精炼，采用LLM作为评判框架评估最先进的LLM和代理系统。

Result: 谄媚行为普遍存在，最佳模型GPT-5在29%的情况下产生谄媚性回答。测试时干预和监督微调等缓解策略能显著减少但无法完全消除谄媚行为。

Conclusion: LLM在定理证明中存在显著的谄媚问题，需要专门设计的基准和缓解策略来应对这一挑战。

Abstract: Large language models (LLMs) have recently shown strong performance on
mathematical benchmarks. At the same time, they are prone to hallucination and
sycophancy, often providing convincing but flawed proofs for incorrect
mathematical statements provided by users. This significantly limits the
applicability of LLMs in theorem proving, as verification of these flawed
proofs must be done manually by expert mathematicians. However, existing
benchmarks that measure sycophancy in mathematics are limited: they focus
solely on final-answer problems, rely on very simple and often contaminated
datasets, and construct benchmark samples using synthetic modifications that
create ill-posed questions rather than well-posed questions that are
demonstrably false. To address these issues, we introduce BrokenMath, the first
benchmark for evaluating sycophantic behavior in LLMs within the context of
natural language theorem proving. BrokenMath is built from advanced 2025
competition problems, which are perturbed with an LLM to produce false
statements and subsequently refined through expert review. Using an
LLM-as-a-judge framework, we evaluate state-of-the-art LLMs and agentic systems
and find that sycophancy is widespread, with the best model, GPT-5, producing
sycophantic answers 29% of the time. We further investigate several mitigation
strategies, including test-time interventions and supervised fine-tuning on
curated sycophantic examples. These approaches substantially reduce, but do not
eliminate, sycophantic behavior.

</details>


### [186] [LMM-Incentive: Large Multimodal Model-based Incentive Design for User-Generated Content in Web 3.0](https://arxiv.org/abs/2510.04765)
*Jinbo Wen,Jiawen Kang,Linfeng Zhang,Xiaoying Tang,Jianhang Tang,Yang Zhang,Zhaohui Yang,Dusit Niyato*

Main category: cs.AI

TL;DR: 提出了LMM-Incentive机制，基于大型多模态模型的激励方法，解决Web 3.0中用户生成内容的质量问题，通过合约理论和LMM代理评估来激励高质量内容创作。


<details>
  <summary>Details</summary>
Motivation: Web 3.0中用户可能利用内容策展机制的局限性，在信息不对称情况下生成低质量内容获取奖励，这会损害Web 3.0的性能。

Method: 提出LMM-based合约理论模型激励高质量UGC生成；使用LMM代理评估内容质量；开发改进的MoE-based PPO算法进行最优合约设计；在以太坊智能合约框架中部署。

Result: 仿真结果表明提出的MoE-based PPO算法在合约设计方面优于代表性基准方法；部署到以太坊智能合约框架验证了方案有效性。

Conclusion: LMM-Incentive机制能有效解决Web 3.0中的逆向选择和道德风险问题，激励用户生成高质量内容，提升平台性能。

Abstract: Web 3.0 represents the next generation of the Internet, which is widely
recognized as a decentralized ecosystem that focuses on value expression and
data ownership. By leveraging blockchain and artificial intelligence
technologies, Web 3.0 offers unprecedented opportunities for users to create,
own, and monetize their content, thereby enabling User-Generated Content (UGC)
to an entirely new level. However, some self-interested users may exploit the
limitations of content curation mechanisms and generate low-quality content
with less effort, obtaining platform rewards under information asymmetry. Such
behavior can undermine Web 3.0 performance. To this end, we propose
\textit{LMM-Incentive}, a novel Large Multimodal Model (LMM)-based incentive
mechanism for UGC in Web 3.0. Specifically, we propose an LMM-based
contract-theoretic model to motivate users to generate high-quality UGC,
thereby mitigating the adverse selection problem from information asymmetry. To
alleviate potential moral hazards after contract selection, we leverage LMM
agents to evaluate UGC quality, which is the primary component of the contract,
utilizing prompt engineering techniques to improve the evaluation performance
of LMM agents. Recognizing that traditional contract design methods cannot
effectively adapt to the dynamic environment of Web 3.0, we develop an improved
Mixture of Experts (MoE)-based Proximal Policy Optimization (PPO) algorithm for
optimal contract design. Simulation results demonstrate the superiority of the
proposed MoE-based PPO algorithm over representative benchmarks in the context
of contract design. Finally, we deploy the designed contract within an Ethereum
smart contract framework, further validating the effectiveness of the proposed
scheme.

</details>


### [187] [Hybrid-Balance GFlowNet for Solving Vehicle Routing Problems](https://arxiv.org/abs/2510.04792)
*Ni Zhang,Zhiguang Cao*

Main category: cs.AI

TL;DR: 提出混合平衡GFlowNet框架，将轨迹平衡和详细平衡相结合，用于解决车辆路径问题，在CVRP和TSP上均取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有GFlowNet方法主要使用轨迹平衡进行全局优化，但忽视了局部优化的重要性。详细平衡能更好处理局部优化，但单独使用无法解决需要整体轨迹优化的VRP问题。

Method: 提出混合平衡GFlowNet框架，以原则性和自适应方式整合轨迹平衡和详细平衡，发挥两者的互补优势。针对CVRP等以仓库为中心的场景，设计了专门的推理策略。

Result: 将HBG集成到AGFN和GFACS两个GFlowNet求解器中，在CVRP和TSP问题上都实现了持续且显著的改进。

Conclusion: HBG框架通过整合轨迹平衡和详细平衡，提高了解决方案质量和泛化能力，适用于有仓库和无仓库的路径优化问题。

Abstract: Existing GFlowNet-based methods for vehicle routing problems (VRPs) typically
employ Trajectory Balance (TB) to achieve global optimization but often neglect
important aspects of local optimization. While Detailed Balance (DB) addresses
local optimization more effectively, it alone falls short in solving VRPs,
which inherently require holistic trajectory optimization. To address these
limitations, we introduce the Hybrid-Balance GFlowNet (HBG) framework, which
uniquely integrates TB and DB in a principled and adaptive manner by aligning
their intrinsically complementary strengths. Additionally, we propose a
specialized inference strategy for depot-centric scenarios like the Capacitated
Vehicle Routing Problem (CVRP), leveraging the depot node's greater flexibility
in selecting successors. Despite this specialization, HBG maintains broad
applicability, extending effectively to problems without explicit depots, such
as the Traveling Salesman Problem (TSP). We evaluate HBG by integrating it into
two established GFlowNet-based solvers, i.e., AGFN and GFACS, and demonstrate
consistent and significant improvements across both CVRP and TSP, underscoring
the enhanced solution quality and generalization afforded by our approach.

</details>


### [188] [Natural Language Edge Labelling: Decoupling Intent from Execution in Structured LM Reasoning](https://arxiv.org/abs/2510.04817)
*Abhinav Madahar*

Main category: cs.AI

TL;DR: 提出了自然语言边缘标注(NLEL)框架，通过标注器-调谐器覆盖层将自由形式的自然语言指令附加到搜索边缘，实现结构化推理的可控性、可解释性和计算效率提升。


<details>
  <summary>Details</summary>
Motivation: 现有结构化推理控制器(如思维链、自洽性、思维树)将"下一步尝试什么"与"如何执行"耦合在一起，只暴露粗粒度的全局控制参数，导致系统脆弱、计算效率低且难以审计。

Method: NLEL包含标注器Λ(从父状态和紧凑上下文生成标签)和调谐器Ψ(将标签映射为模式受限的控制向量)，支持解码、搜索、生成束大小、检索混合和验证通道的细粒度控制。

Result: 理论证明NLEL严格泛化了CoT/ToT方法，具有任意时间单调性，并通过控制向量失真限制选择器不足，为信任区域和验证通道等保护措施提供决策依据。

Conclusion: NLEL提供了一个可解释、模型无关的接口，将意图与执行分离，实现可控、可审计的语言模型推理。

Abstract: Controllers for structured LM reasoning (e.g., Chain-of-Thought,
self-consistency, and Tree-of-Thoughts) often entangle what to try next with
how to execute it, exposing only coarse global knobs and yielding brittle,
compute-inefficient, and hard-to-audit behavior. We introduce Natural Language
Edge Labelling (NLEL), a labeller-tuner overlay that attaches a free-form
natural-language directive to each search edge and translates it into a
schema-bounded control vector for decoding, search (branch quotas, exploration
$\beta$), generation bundle size, retrieval mixtures, and verification passes.
A labeller $\Lambda$ emits labels from the parent state and a compact context;
a tuner $\Psi$ maps $(P, L, C)\to \Pi$, with strict schema validation and
trust-region projection around safe defaults. Downstream selection remains
ToT-style with score $S=\mu+\beta\sigma$ and depth-annealed $\beta$. We show
NLEL strictly generalizes CoT/ToT, prove an anytime-monotonicity property for
top-$k$ selection under label-conditioned bundles, and bound selector shortfall
by control-vector distortion, providing decision-relevant justification for
guards like trust regions and verification passes. We instantiate $\Psi$ as a
prompt-only JSON Parameter Emitter and preregister an evaluation on GSM8K, MATH
(subset), StrategyQA, and ARC-Challenge with compute-aware reporting
(success@compute, tokens-per-success) and ablations over $\Lambda$, $\Psi$,
trust-region radius, and control quantization; preregistered forecasts
anticipate accuracy gains at comparable token budgets and improved
success@compute under constraints. NLEL offers an interpretable, model-agnostic
interface that separates intent from execution for controllable, auditable LM
inference.

</details>


### [189] [LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for Workflow Automation](https://arxiv.org/abs/2510.04851)
*Dongge Han,Camille Couturier,Daniel Madrigal Diaz,Xuchao Zhang,Victor Rühle,Saravan Rajmohan*

Main category: cs.AI

TL;DR: LEGOMem是一个用于多智能体工作流自动化的模块化过程记忆框架，通过分解历史任务轨迹为可重用记忆单元，支持规划与执行。


<details>
  <summary>Details</summary>
Motivation: 探索多智能体系统中记忆的设计空间，研究记忆应该放在哪里、如何检索以及哪些智能体受益最多。

Method: 将过去任务轨迹分解为可重用记忆单元，灵活分配给编排器和任务智能体，支持规划和执行。

Result: 实验表明编排器记忆对任务分解和委派至关重要，细粒度智能体记忆提高执行准确性，较小模型团队也能显著受益。

Conclusion: LEGOMem既是记忆增强智能体系统的实用框架，也是理解多智能体工作流自动化中记忆设计的研究工具。

Abstract: We introduce LEGOMem, a modular procedural memory framework for multi-agent
large language model (LLM) systems in workflow automation. LEGOMem decomposes
past task trajectories into reusable memory units and flexibly allocates them
across orchestrators and task agents to support planning and execution. To
explore the design space of memory in multi-agent systems, we use LEGOMem as a
lens and conduct a systematic study of procedural memory in multi-agent
systems, examining where memory should be placed, how it should be retrieved,
and which agents benefit most. Experiments on the OfficeBench benchmark show
that orchestrator memory is critical for effective task decomposition and
delegation, while fine-grained agent memory improves execution accuracy. We
find that even teams composed of smaller language models can benefit
substantially from procedural memory, narrowing the performance gap with
stronger agents by leveraging prior execution traces for more accurate planning
and tool use. These results position LEGOMem as both a practical framework for
memory-augmented agent systems and a research tool for understanding memory
design in multi-agent workflow automation.

</details>


### [190] [Video Game Level Design as a Multi-Agent Reinforcement Learning Problem](https://arxiv.org/abs/2510.04862)
*Sam Earle,Zehua Jiang,Eugene Vinitsky,Julian Togelius*

Main category: cs.AI

TL;DR: 该论文提出将程序化内容生成重新框架为多智能体问题，通过分布式方法解决单智能体PCGRL的效率瓶颈和泛化能力限制。


<details>
  <summary>Details</summary>
Motivation: 现有PCGRL研究聚焦于单智能体生成器，但面临频繁重新计算启发式质量指标和在大型地图中导航的效率瓶颈问题。

Method: 将关卡生成重新定义为多智能体问题，通过分布式智能体减少奖励计算次数，使智能体学习更局部、模块化的设计策略。

Result: 多智能体关卡生成器在奖励计算效率方面表现更好，且能更好地泛化到分布外地图形状。

Conclusion: 将内容生成视为分布式多智能体任务有利于大规模生成功能性内容。

Abstract: Procedural Content Generation via Reinforcement Learning (PCGRL) offers a
method for training controllable level designer agents without the need for
human datasets, using metrics that serve as proxies for level quality as
rewards. Existing PCGRL research focuses on single generator agents, but are
bottlenecked by the need to frequently recalculate heuristics of level quality
and the agent's need to navigate around potentially large maps. By framing
level generation as a multi-agent problem, we mitigate the efficiency
bottleneck of single-agent PCGRL by reducing the number of reward calculations
relative to the number of agent actions. We also find that multi-agent level
generators are better able to generalize to out-of-distribution map shapes,
which we argue is due to the generators' learning more local, modular design
policies. We conclude that treating content generation as a distributed,
multi-agent task is beneficial for generating functional artifacts at scale.

</details>


### [191] [Where Did It All Go Wrong? A Hierarchical Look into Multi-Agent Error Attribution](https://arxiv.org/abs/2510.04886)
*Adi Banerjee,Anirudh Nair,Tarik Borogovac*

Main category: cs.AI

TL;DR: ECHO算法通过分层上下文表示、基于目标分析的评估和共识投票，提高了多智能体系统中错误归因的准确性，在复杂推理错误和相互依赖场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统中错误归因方法在分析复杂模式时存在准确性和一致性问题，需要更有效的调试和改进协作AI系统的方法。

Method: 结合分层上下文表示、基于目标分析的评估和共识投票，利用基于位置的上下文理解层级化，同时保持客观评估标准，通过共识机制得出结论。

Result: 实验结果表明ECHO在各种多智能体交互场景中优于现有方法，在处理微妙推理错误和复杂相互依赖情况时表现尤为突出。

Conclusion: 结构化分层上下文表示与基于共识的客观决策相结合，为多智能体系统中的错误归因提供了更稳健的框架。

Abstract: Error attribution in Large Language Model (LLM) multi-agent systems presents
a significant challenge in debugging and improving collaborative AI systems.
Current approaches to pinpointing agent and step level failures in interaction
traces - whether using all-at-once evaluation, step-by-step analysis, or binary
search - fall short when analyzing complex patterns, struggling with both
accuracy and consistency. We present ECHO (Error attribution through Contextual
Hierarchy and Objective consensus analysis), a novel algorithm that combines
hierarchical context representation, objective analysis-based evaluation, and
consensus voting to improve error attribution accuracy. Our approach leverages
a positional-based leveling of contextual understanding while maintaining
objective evaluation criteria, ultimately reaching conclusions through a
consensus mechanism. Experimental results demonstrate that ECHO outperforms
existing methods across various multi-agent interaction scenarios, showing
particular strength in cases involving subtle reasoning errors and complex
interdependencies. Our findings suggest that leveraging these concepts of
structured, hierarchical context representation combined with consensus-based
objective decision-making, provides a more robust framework for error
attribution in multi-agent systems.

</details>


### [192] [Human Behavior Atlas: Benchmarking Unified Psychological and Social Behavior Understanding](https://arxiv.org/abs/2510.04899)
*Keane Ong,Wei Dai,Carol Li,Dewei Feng,Hengzhi Li,Jingyao Wu,Jiaee Cheong,Rui Mao,Gianmarco Mengaldo,Erik Cambria,Paul Pu Liang*

Main category: cs.AI

TL;DR: Human Behavior Atlas是一个统一的行为理解基准数据集，包含10万+多模态样本，用于训练统一模型来理解心理和社会行为。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用专门数据集和单任务系统，缺乏可扩展性、跨任务迁移和泛化能力。需要统一基准来解决这些限制。

Method: 构建Human Behavior Atlas数据集，涵盖文本、音频、视觉模态，包含情感状态、认知状态、病理学和社会过程任务。训练三个模型：OmniSapiens-7B SFT、BAM和RL。

Result: 在Human Behavior Atlas上训练的模型在多样化行为任务上持续优于现有多模态LLM。预训练还能提高对新行为数据集的迁移能力。

Conclusion: 统一的行为基准可以减少冗余和成本，实现跨任务高效训练，并增强行为特征在领域间的泛化能力。

Abstract: Using intelligent systems to perceive psychological and social behaviors,
that is, the underlying affective, cognitive, and pathological states that are
manifested through observable behaviors and social interactions, remains a
challenge due to their complex, multifaceted, and personalized nature. Existing
work tackling these dimensions through specialized datasets and single-task
systems often miss opportunities for scalability, cross-task transfer, and
broader generalization. To address this gap, we curate Human Behavior Atlas, a
unified benchmark of diverse behavioral tasks designed to support the
development of unified models for understanding psychological and social
behaviors. Human Behavior Atlas comprises over 100,000 samples spanning text,
audio, and visual modalities, covering tasks on affective states, cognitive
states, pathologies, and social processes. Our unification efforts can reduce
redundancy and cost, enable training to scale efficiently across tasks, and
enhance generalization of behavioral features across domains. On Human Behavior
Atlas, we train three models: OmniSapiens-7B SFT, OmniSapiens-7B BAM, and
OmniSapiens-7B RL. We show that training on Human Behavior Atlas enables models
to consistently outperform existing multimodal LLMs across diverse behavioral
tasks. Pretraining on Human Behavior Atlas also improves transfer to novel
behavioral datasets; with the targeted use of behavioral descriptors yielding
meaningful performance gains.

</details>


### [193] [MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.04935)
*Guoxin Chen,Zile Qiao,Wenqing Wang,Donglei Yu,Xuanzhong Chen,Hao Sun,Minpeng Liao,Kai Fan,Yong Jiang,Penguin Xie,Wayne Xin Zhao,Ruihua Song,Fei Huang*

Main category: cs.AI

TL;DR: 本文提出MARS多智能体系统，通过整合System 1的快速直觉思维和System 2的深思熟虑推理，解决大型推理模型在简单任务中过度分析、在动态环境中适应能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在简单任务中倾向于过度使用System 2型推理，导致token生成效率低下，且由于预训练数据的静态性难以适应快速变化的环境。需要创新方法桥接直觉和深思熟虑的认知过程。

Method: 提出MARS多智能体系统，整合Google搜索、Google学术、Python解释器等外部工具，通过分工协作让System 1高效处理外部信息，为System 2提供精炼见解。采用多智能体强化学习框架优化两个系统的协作效率。

Result: 在挑战性基准HLE上取得3.86%的显著提升，在7个知识密集型任务上平均增益8.9%，验证了双系统范式在动态信息环境中复杂推理的有效性。

Conclusion: MARS通过双系统协作和多智能体强化学习，有效提升了大型语言模型在动态环境中的复杂推理能力，为认知计算提供了新范式。

Abstract: Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in
simple tasks, where the models excessively utilize System 2-type, deliberate
reasoning, leading to inefficient token generation. Furthermore, these models
face challenges in adapting their reasoning capabilities to rapidly changing
environments due to the static nature of their pretraining data. To address
these issues, advancing Large Language Models (LLMs) for complex reasoning
tasks requires innovative approaches that bridge intuitive and deliberate
cognitive processes, akin to human cognition's dual-system dynamic. This paper
introduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless
integration of System 1's fast, intuitive thinking with System 2's deliberate
reasoning within LLMs. MARS strategically integrates multiple external tools,
such as Google Search, Google Scholar, and Python Interpreter, to access
up-to-date information and execute complex computations, while creating a
specialized division of labor where System 1 efficiently processes and
summarizes high-volume external information, providing distilled insights that
expand System 2's reasoning context without overwhelming its capacity.
Furthermore, we propose a multi-agent reinforcement learning framework
extending Group Relative Policy Optimization to simultaneously optimize both
systems with multi-turn tool interactions, bin-packing optimization, and sample
balancing strategies that enhance collaborative efficiency. Extensive
experiments demonstrate MARS achieves substantial improvements of 3.86% on the
challenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9%
across 7 knowledge-intensive tasks, validating the effectiveness of our
dual-system paradigm for complex reasoning in dynamic information environments.

</details>


### [194] [Safe and Compliant Cross-Market Trade Execution via Constrained RL and Zero-Knowledge Audits](https://arxiv.org/abs/2510.04952)
*Ailiya Borjigin,Cong He*

Main category: cs.AI

TL;DR: 提出一个跨市场算法交易系统，结合强化学习执行代理和独立合规代理，在保证执行质量的同时严格实施合规约束，并通过零知识证明实现可审计性。


<details>
  <summary>Details</summary>
Motivation: 现有算法交易系统难以在追求执行质量的同时确保严格的合规约束，需要一种既能优化交易执行又能保证监管合规的解决方案。

Method: 将交易执行为约束马尔可夫决策过程，使用近端策略优化训练执行代理，运行时动作屏蔽确保行动可行性，并添加零知识合规审计层生成加密证明。

Result: 学习策略在ABIDES模拟器中减少了执行差额和方差，在压力测试场景下未观察到约束违规，在95%置信水平下效果显著。

Conclusion: 该工作融合了最优执行、安全强化学习、监管技术和可验证AI，为现实世界部署提供了可行路径，同时讨论了伦理考量和局限性。

Abstract: We present a cross-market algorithmic trading system that balances execution
quality with rigorous compliance enforcement. The architecture comprises a
high-level planner, a reinforcement learning execution agent, and an
independent compliance agent. We formulate trade execution as a constrained
Markov decision process with hard constraints on participation limits, price
bands, and self-trading avoidance. The execution agent is trained with proximal
policy optimization, while a runtime action-shield projects any unsafe action
into a feasible set. To support auditability without exposing proprietary
signals, we add a zero-knowledge compliance audit layer that produces
cryptographic proofs that all actions satisfied the constraints. We evaluate in
a multi-venue, ABIDES-based simulator and compare against standard baselines
(e.g., TWAP, VWAP). The learned policy reduces implementation shortfall and
variance while exhibiting no observed constraint violations across stress
scenarios including elevated latency, partial fills, compliance module
toggling, and varying constraint limits. We report effects at the 95%
confidence level using paired t-tests and examine tail risk via CVaR. We
situate the work at the intersection of optimal execution, safe reinforcement
learning, regulatory technology, and verifiable AI, and discuss ethical
considerations, limitations (e.g., modeling assumptions and computational
overhead), and paths to real-world deployment.

</details>


### [195] [Aligning Perception, Reasoning, Modeling and Interaction: A Survey on Physical AI](https://arxiv.org/abs/2510.04978)
*Kun Xiang,Terry Jingchen Zhang,Yinya Huang,Jixi He,Zirong Liu,Yueling Tang,Ruizhe Zhou,Lijing Luo,Youpeng Wen,Xiuwei Chen,Bingqian Lin,Jianhua Han,Hang Xu,Hanhui Li,Bin Dong,Xiaodan Liang*

Main category: cs.AI

TL;DR: 该论文综述了物理AI领域，区分了理论物理推理与应用物理理解，系统分析了基于物理的方法如何增强AI在符号推理、具身系统和生成模型中的真实世界理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前物理感知和符号物理推理各自发展，缺乏统一的桥接框架，需要建立能够整合物理原理和具身推理的智能系统。

Method: 通过系统分析近期进展，建立物理AI的综合框架，区分理论推理与应用理解，并探讨物理基础方法在不同AI领域的应用。

Result: 提出了整合物理原理和具身推理的智能系统框架，超越了模式识别，实现了对物理定律的真正理解。

Conclusion: 展望了能够解释物理现象和预测未来状态的下一代世界模型，推动安全、可泛化和可解释的AI系统发展。

Abstract: The rapid advancement of embodied intelligence and world models has
intensified efforts to integrate physical laws into AI systems, yet physical
perception and symbolic physics reasoning have developed along separate
trajectories without a unified bridging framework. This work provides a
comprehensive overview of physical AI, establishing clear distinctions between
theoretical physics reasoning and applied physical understanding while
systematically examining how physics-grounded methods enhance AI's real-world
comprehension across structured symbolic reasoning, embodied systems, and
generative models. Through rigorous analysis of recent advances, we advocate
for intelligent systems that ground learning in both physical principles and
embodied reasoning processes, transcending pattern recognition toward genuine
understanding of physical laws. Our synthesis envisions next-generation world
models capable of explaining physical phenomena and predicting future states,
advancing safe, generalizable, and interpretable AI systems. We maintain a
continuously updated resource at
https://github.com/AI4Phys/Awesome-AI-for-Physics.

</details>


### [196] [LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and Rationale Inference in Imperfect Information Collaboration Game](https://arxiv.org/abs/2510.04980)
*Fangzhou Liang,Tianshi Zheng,Chunkit Chan,Yauwai Yim,Yangqiu Song*

Main category: cs.AI

TL;DR: LLM-Hanabi是一个基于合作游戏Hanabi的基准测试，用于评估大语言模型的心智理论能力，发现一阶心智理论（解释他人意图）与游戏表现的相关性比二阶心智理论更强。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在动态协作环境中推断他人行为背后理据的能力，这是有效多智能体协作所需的心智理论能力。

Method: 开发了LLM-Hanabi基准测试，使用合作游戏Hanabi，并建立自动化评估系统来测量游戏表现和心智理论熟练度。

Result: 发现心智理论与游戏成功呈显著正相关，一阶心智理论（解释他人意图）比二阶心智理论（预测他人解释）与表现的相关性更强。

Conclusion: 对于有效的AI协作，准确解释伙伴理据的能力比高阶推理更重要，优先发展一阶心智理论是增强未来模型协作能力的有前景方向。

Abstract: Effective multi-agent collaboration requires agents to infer the rationale
behind others' actions, a capability rooted in Theory-of-Mind (ToM). While
recent Large Language Models (LLMs) excel at logical inference, their ability
to infer rationale in dynamic, collaborative settings remains under-explored.
This study introduces LLM-Hanabi, a novel benchmark that uses the cooperative
game Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework
features an automated evaluation system that measures both game performance and
ToM proficiency. Across a range of models, we find a significant positive
correlation between ToM and in-game success. Notably, first-order ToM
(interpreting others' intent) correlates more strongly with performance than
second-order ToM (predicting others' interpretations). These findings highlight
that for effective AI collaboration, the ability to accurately interpret a
partner's rationale is more critical than higher-order reasoning. We conclude
that prioritizing first-order ToM is a promising direction for enhancing the
collaborative capabilities of future models.

</details>


### [197] [Think Then Embed: Generative Context Improves Multimodal Embedding](https://arxiv.org/abs/2510.05014)
*Xuanming Cui,Jianpeng Cheng,Hong-you Chen,Satya Narayan Shukla,Abhijeet Awasthi,Xichen Pan,Chaitanya Ahuja,Shlok Kumar Mishra,Qi Guo,Ser-Nam Lim,Aashu Singh,Xiangjun Fan*

Main category: cs.AI

TL;DR: 提出了Think-Then-Embed框架，通过引入推理步骤来增强多模态嵌入模型的性能，在MMEB-V2基准上达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有的通用多模态嵌入模型仅将多模态大语言模型作为编码器使用，忽略了其生成能力，在处理复杂指令和组合推理时效果不佳。

Method: 提出TTE框架，包含推理器和嵌入器两个组件：推理器MLLM先生成解释复杂查询的推理轨迹，然后嵌入器基于原始查询和中间推理生成表示。

Result: 在MMEB-V2基准上超越专有模型；通过微调小型MLLM推理器，在开源模型中取得最佳性能，比近期模型提升7%；成功将推理器和嵌入器集成到统一模型中，在不牺牲性能的情况下提高效率。

Conclusion: 显式推理步骤能够更好地理解复杂的多模态指令，TTE框架为通用多模态嵌入任务提供了有效的解决方案。

Abstract: There is a growing interest in Universal Multimodal Embeddings (UME), where
models are required to generate task-specific representations. While recent
studies show that Multimodal Large Language Models (MLLMs) perform well on such
tasks, they treat MLLMs solely as encoders, overlooking their generative
capacity. However, such an encoding paradigm becomes less effective as
instructions become more complex and require compositional reasoning. Inspired
by the proven effectiveness of chain-of-thought reasoning, we propose a general
Think-Then-Embed (TTE) framework for UME, composed of a reasoner and an
embedder. The reasoner MLLM first generates reasoning traces that explain
complex queries, followed by an embedder that produces representations
conditioned on both the original query and the intermediate reasoning. This
explicit reasoning step enables more nuanced understanding of complex
multimodal instructions. Our contributions are threefold. First, by leveraging
a powerful MLLM reasoner, we achieve state-of-the-art performance on the
MMEB-V2 benchmark, surpassing proprietary models trained on massive in-house
datasets. Second, to reduce the dependency on large MLLM reasoners, we finetune
a smaller MLLM reasoner using high-quality embedding-centric reasoning traces,
achieving the best performance among open-source models with a 7% absolute gain
over recently proposed models. Third, we investigate strategies for integrating
the reasoner and embedder into a unified model for improved efficiency without
sacrificing performance.

</details>


### [198] [Look-ahead Reasoning with a Learned Model in Imperfect Information Games](https://arxiv.org/abs/2510.05048)
*Ondřej Kubíček,Viliam Lisý*

Main category: cs.AI

TL;DR: LAMIR算法通过学习不完美信息游戏的抽象模型，在测试时进行前瞻推理，解决了传统方法在复杂游戏中难以扩展的问题。


<details>
  <summary>Details</summary>
Motivation: 解决不完美信息游戏中模型学习困难的问题，传统方法在复杂游戏中无法有效扩展，需要更高效的推理方法。

Method: LAMIR算法直接从智能体-环境交互中学习游戏的抽象模型，通过抽象化限制子游戏规模，使理论上有原则的前瞻推理变得可行。

Result: 实验表明，在足够容量下LAMIR能学习到精确的底层游戏结构，在有限容量下仍能学习有价值的抽象，提升预训练智能体在大型游戏中的表现。

Conclusion: LAMIR为不完美信息游戏提供了可扩展的前瞻推理解决方案，通过模型抽象化使复杂游戏中的理论推理变得可行。

Abstract: Test-time reasoning significantly enhances pre-trained AI agents'
performance. However, it requires an explicit environment model, often
unavailable or overly complex in real-world scenarios. While MuZero enables
effective model learning for search in perfect information games, extending
this paradigm to imperfect information games presents substantial challenges
due to more nuanced look-ahead reasoning techniques and large number of states
relevant for individual decisions. This paper introduces an algorithm LAMIR
that learns an abstracted model of an imperfect information game directly from
the agent-environment interaction. During test time, this trained model is used
to perform look-ahead reasoning. The learned abstraction limits the size of
each subgame to a manageable size, making theoretically principled look-ahead
reasoning tractable even in games where previous methods could not scale. We
empirically demonstrate that with sufficient capacity, LAMIR learns the exact
underlying game structure, and with limited capacity, it still learns a
valuable abstraction, which improves game playing performance of the
pre-trained agents even in large games.

</details>


### [199] [Staircase Streaming for Low-Latency Multi-Agent Inference](https://arxiv.org/abs/2510.05059)
*Junlin Wang,Jue Wang,Zhen,Xu,Ben Athiwaratkun,Bhuwan Dhingra,Ce Zhang,James Zou*

Main category: cs.AI

TL;DR: 提出阶梯式流式处理方法来降低多智能体推理的延迟，在接收部分中间输出时就开始生成最终响应，显著减少首词时间。


<details>
  <summary>Details</summary>
Motivation: 多智能体推理虽然能提升响应质量，但会显著增加首词时间，影响延迟敏感应用的用户体验。

Method: 阶梯式流式处理：无需等待完整的中间输出，在接收到部分中间输出时就开始生成最终响应。

Result: 实验结果显示阶梯式流式处理将首词时间降低高达93%，同时保持响应质量。

Conclusion: 阶梯式流式处理是解决多智能体推理延迟问题的有效方法，能在保持质量的同时大幅降低响应延迟。

Abstract: Recent advances in large language models (LLMs) opened up new directions for
leveraging the collective expertise of multiple LLMs. These methods, such as
Mixture-of-Agents, typically employ additional inference steps to generate
intermediate outputs, which are then used to produce the final response. While
multi-agent inference can enhance response quality, it can significantly
increase the time to first token (TTFT), posing a challenge for
latency-sensitive applications and hurting user experience. To address this
issue, we propose staircase streaming for low-latency multi-agent inference.
Instead of waiting for the complete intermediate outputs from previous steps,
we begin generating the final response as soon as we receive partial outputs
from these steps. Experimental results demonstrate that staircase streaming
reduces TTFT by up to 93% while maintaining response quality.

</details>


### [200] [CAG: Chunked Augmented Generation for Google Chrome's Built-in Gemini Nano](https://arxiv.org/abs/2412.18708)
*Vivek Vellaiyappan Surulimuthu,Aditya Karnam Gururaj Rao*

Main category: cs.AI

TL;DR: 提出了Chunked Augmented Generation (CAG)架构，通过智能分块处理策略解决Chrome内置Gemini Nano模型的上下文窗口限制问题


<details>
  <summary>Details</summary>
Motivation: Chrome集成Gemini Nano虽然将AI能力带到浏览器，但其受限的上下文窗口难以处理大型输入内容

Method: 采用智能输入分块和处理策略，在浏览器约束内高效处理大量内容

Result: 在Chrome内处理大型文档和数据集特别有效，无需依赖外部API即可访问复杂AI能力

Conclusion: CAG架构成功克服了浏览器内置AI模型的上下文限制，使浏览器内AI处理大型内容成为可能

Abstract: We present Chunked Augmented Generation (CAG), an architecture specifically
designed to overcome the context window limitations of Google Chrome's built-in
Gemini Nano model. While Chrome's integration of Gemini Nano represents a
significant advancement in bringing AI capabilities directly to the browser,
its restricted context window poses challenges for processing large inputs. CAG
addresses this limitation through intelligent input chunking and processing
strategies, enabling efficient handling of extensive content while maintaining
the model's performance within browser constraints. Our implementation
demonstrates particular efficacy in processing large documents and datasets
directly within Chrome, making sophisticated AI capabilities accessible through
the browser without external API dependencies. Get started now at
https://github.com/vivekVells/cag-js.

</details>
