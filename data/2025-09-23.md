<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 140]
- [cs.AI](#cs.AI) [Total: 64]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [On LLM-Based Scientific Inductive Reasoning Beyond Equations](https://arxiv.org/abs/2509.16226)
*Brian S. Lin,Jiaxin Yuan,Zihan Zhou,Shouli Wang,Shuo Wang,Cunliang Kong,Qi Shi,Yuxuan Li,Liner Yang,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: 该论文提出了基于大语言模型的科学归纳推理任务，并创建了SIRBench-V1基准来评估LLMs在科学场景中的归纳推理能力。实验表明当前LLMs在此任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs展现出类人能力，需要研究如何让LLMs在全新环境中从有限示例中学习底层模式并有效应用，这是LLMs归纳推理能力的核心问题。现有研究多关注可数学表达的规则，但缺乏在具体科学场景中的基础。

Method: 受归纳推理与人类科学发现相似性的启发，提出了超越方程的科学归纳推理任务，并开发了SIRBench-V1基准来系统评估LLMs的能力。

Result: 实验结果显示，当前的大语言模型在这一任务上仍然存在困难，表明该任务的挑战性和在此领域进一步发展的必要性。

Conclusion: 该研究强调了科学归纳推理任务的重要性，指出了当前LLMs的局限性，并为未来改进LLMs的归纳推理能力提供了新的基准和方向。

Abstract: As large language models (LLMs) increasingly exhibit human-like capabilities,
a fundamental question emerges: How can we enable LLMs to learn the underlying
patterns from limited examples in entirely novel environments and apply them
effectively? This question is central to the ability of LLMs in inductive
reasoning. Existing research on LLM-based inductive reasoning can be broadly
categorized based on whether the underlying rules are expressible via explicit
mathematical equations. However, many recent studies in the beyond-equations
category have emphasized rule design without grounding them in specific
scenarios. Inspired by the parallels between inductive reasoning and human
scientific discovery, we propose the task of LLM-Based Scientific Inductive
Reasoning Beyond Equations and introduce a new benchmark, SIRBench-V1, to
evaluate the inductive reasoning abilities of LLMs in scientific settings. Our
experimental results show that current LLMs still struggle with this task,
underscoring its difficulty and the need for further advancement in this area.

</details>


### [2] [REAMS: Reasoning Enhanced Algorithm for Maths Solving](https://arxiv.org/abs/2509.16241)
*Eishkaran Singh,Tanav Singh Bajaj,Siddharth Nayak*

Main category: cs.CL

TL;DR: 本文提出了一种基于语言的方法，利用零样本学习和数学推理来解决、解释和生成大学级数学问题的解决方案，通过集成程序合成减少对大规模训练数据的依赖，在MIT、哥伦比亚大学课程和MATH数据集上达到了90.15%的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决复杂大学级数学问题（特别是来自MIT、哥伦比亚大学课程和MATH数据集的问题）一直是人工智能领域的重大挑战，传统方法在这一领域表现不佳，需要更先进的解决方案。

Method: 采用基于语言的方法，结合零样本学习和数学推理，集成程序合成技术，减少对大规模训练数据的依赖。

Result: 该方法在复杂数学问题上达到了90.15%的准确率，相比之前81%的基准有显著提升，为自动化数学问题解决设立了新标准。

Conclusion: 研究结果表明，先进的人工智能方法在应对复杂数学课程和数据集挑战方面具有巨大潜力，能够有效克服传统方法的局限性。

Abstract: The challenges of solving complex university-level mathematics problems,
particularly those from MIT, and Columbia University courses, and selected
tasks from the MATH dataset, remain a significant obstacle in the field of
artificial intelligence. Conventional methods have consistently fallen short in
this domain, highlighting the need for more advanced approaches. In this paper,
we introduce a language-based solution that leverages zero-shot learning and
mathematical reasoning to effectively solve, explain, and generate solutions
for these advanced math problems. By integrating program synthesis, our method
reduces reliance on large-scale training data while significantly improving
problem-solving accuracy. Our approach achieves an accuracy of 90.15%,
representing a substantial improvement over the previous benchmark of 81% and
setting a new standard in automated mathematical problem-solving. These
findings highlight the significant potential of advanced AI methodologies to
address and overcome the challenges presented by some of the most complex
mathematical courses and datasets.

</details>


### [3] [HausaMovieReview: A Benchmark Dataset for Sentiment Analysis in Low-Resource African Language](https://arxiv.org/abs/2509.16256)
*Asiya Ibrahim Zanga,Salisu Mamman Abdulrahman,Abubakar Ado,Abdulkadir Abubakar Bichi,Lukman Aliyu Jibril,Abdulmajid Babangida Umar,Alhassan Adamu,Shamsuddeen Hassan Muhammad,Bashir Salisu Abubakar*

Main category: cs.CL

TL;DR: 本文介绍了HausaMovieReview数据集，包含5000条豪萨语和英语代码转换的YouTube评论，用于低资源语言的情感分析。比较了传统模型和Transformer模型，发现决策树分类器表现最佳。


<details>
  <summary>Details</summary>
Motivation: 低资源语言的NLP工具开发受限于标注数据稀缺，本文旨在解决豪萨语情感分析的数据缺乏问题。

Method: 创建HausaMovieReview数据集，由三名独立标注者标注（Fleiss' Kappa=0.85）。比较了逻辑回归、决策树、K近邻等传统模型与BERT、RoBERTa等Transformer模型。

Result: 决策树分类器表现最佳，准确率89.72%，F1分数89.60%，显著优于深度学习模型。

Conclusion: 在低资源环境下，有效的特征工程可使传统模型达到最先进性能，为未来研究奠定基础。

Abstract: The development of Natural Language Processing (NLP) tools for low-resource
languages is critically hindered by the scarcity of annotated datasets. This
paper addresses this fundamental challenge by introducing HausaMovieReview, a
novel benchmark dataset comprising 5,000 YouTube comments in Hausa and
code-switched English. The dataset was meticulously annotated by three
independent annotators, demonstrating a robust agreement with a Fleiss' Kappa
score of 0.85 between annotators. We used this dataset to conduct a comparative
analysis of classical models (Logistic Regression, Decision Tree, K-Nearest
Neighbors) and fine-tuned transformer models (BERT and RoBERTa). Our results
reveal a key finding: the Decision Tree classifier, with an accuracy and
F1-score 89.72% and 89.60% respectively, significantly outperformed the deep
learning models. Our findings also provide a robust baseline, demonstrating
that effective feature engineering can enable classical models to achieve
state-of-the-art performance in low-resource contexts, thereby laying a solid
foundation for future research.
  Keywords: Hausa, Kannywood, Low-Resource Languages, NLP, Sentiment Analysis

</details>


### [4] [Gender and Political Bias in Large Language Models: A Demonstration Platform](https://arxiv.org/abs/2509.16264)
*Wenjie Lin,Hange Liu,Xutao Mao,Yingying Zhuang,Jingwei Shi,Xudong Han,Tianyu Shi,Jinrui Yang*

Main category: cs.CL

TL;DR: ParlAI Vote是一个用于探索欧洲议会辩论和投票的交互式系统，可测试LLM在投票预测和偏见分析方面的表现。该系统整合了辩论数据、演讲内容和投票结果，并包含丰富的人口统计信息。


<details>
  <summary>Details</summary>
Motivation: 开发一个统一平台来降低研究门槛，让用户能够复现研究结果、审计LLM行为，并运行反事实场景分析，同时支持研究、教育和公众参与立法决策过程。

Method: 构建交互式平台连接辩论主题、演讲内容和点名投票结果，集成人口统计数据（性别、年龄、国家、政治团体），可视化EuroParlVote基准测试及其核心任务（性别分类和投票预测）。

Result: 系统揭示了最先进LLM在政治分析中存在的系统性性能偏见，展示了真实投票结果与LLM预测之间的对比，以及按人口统计群体划分的错误细分。

Conclusion: ParlAI Vote通过统一数据、模型和可视化分析，清晰展示了当前LLM在政治分析中的优势和局限性，为LLM偏见研究提供了重要工具。

Abstract: We present ParlAI Vote, an interactive system for exploring European
Parliament debates and votes, and for testing LLMs on vote prediction and bias
analysis. This platform connects debate topics, speeches, and roll-call
outcomes, and includes rich demographic data such as gender, age, country, and
political group. Users can browse debates, inspect linked speeches, compare
real voting outcomes with predictions from frontier LLMs, and view error
breakdowns by demographic group. Visualizing the EuroParlVote benchmark and its
core tasks of gender classification and vote prediction, ParlAI Vote highlights
systematic performance bias in state-of-the-art LLMs. The system unifies data,
models, and visual analytics in a single interface, lowering the barrier for
reproducing findings, auditing behavior, and running counterfactual scenarios.
It supports research, education, and public engagement with legislative
decision-making, while making clear both the strengths and the limitations of
current LLMs in political analysis.

</details>


### [5] [Language Modeling with Learned Meta-Tokens](https://arxiv.org/abs/2509.16278)
*Alok N. Shah,Khush Gupta,Keshav Ramji,Pratik Chaudhari*

Main category: cs.CL

TL;DR: 本文提出了一种使用元令牌和元注意力机制的新方法，通过在预训练中注入特殊令牌来增强语言模型处理长距离依赖的能力，实现了在较少数据下（少于100B tokens）的有效预训练，并能将上下文窗口扩展至2倍。


<details>
  <summary>Details</summary>
Motivation: 现代基于Transformer的语言模型在多任务泛化方面取得了重大成功，但在捕捉上下文窗口内的长距离依赖方面往往表现不佳。

Method: 使用修改后的GPT-2架构，在因果多头注意力之外添加元注意力机制，在预训练期间注入元令牌，并研究这些令牌在一系列合成任务上的影响。

Result: 利用元令牌和元注意力机制的数据高效预训练在微调后在这些任务上表现出色，能够将长度泛化能力扩展到上下文窗口的2倍，即使在使用YaRN扩展后也是如此。

Conclusion: 预训练带有元令牌的语言模型提供了一种简单、数据高效的方法来增强长上下文语言建模性能，同时为理解其长度泛化行为提供了新的见解。

Abstract: While modern Transformer-based language models (LMs) have achieved major
success in multi-task generalization, they often struggle to capture long-range
dependencies within their context window. This work introduces a novel approach
using meta-tokens, special tokens injected during pre-training, along with a
dedicated meta-attention mechanism to guide LMs to use these tokens. We
pre-train a language model with a modified GPT-2 architecture equipped with
meta-attention in addition to causal multi-head attention, and study the impact
of these tokens on a suite of synthetic tasks. We find that data-efficient
language model pre-training on fewer than 100B tokens utilizing meta-tokens and
our meta-attention mechanism achieves strong performance on these tasks after
fine-tuning. We suggest that these gains arise due to the meta-tokens
sharpening the positional encoding. This enables them to operate as trainable,
content-based landmarks, implicitly compressing preceding context and "caching"
it in the meta-token. At inference-time, the meta-token points to relevant
context, facilitating length generalization up to 2$\times$ its context window,
even after extension with YaRN. We provide further evidence of these behaviors
by visualizing model internals to study the residual stream, and assessing the
compression quality by information-theoretic analysis on the rate-distortion
tradeoff. Our findings suggest that pre-training LMs with meta-tokens offers a
simple, data-efficient method to enhance long-context language modeling
performance, while introducing new insights into the nature of their behavior
towards length generalization.

</details>


### [6] [Overhearing LLM Agents: A Survey, Taxonomy, and Roadmap](https://arxiv.org/abs/2509.16325)
*Andrew Zhu,Chris Callison-Burch*

Main category: cs.CL

TL;DR: 本文提出了"旁听智能体"的新范式，让AI助手在不打断对话的情况下提供上下文相关帮助，而不是通过聊天界面直接交互。


<details>
  <summary>Details</summary>
Motivation: 现有对话式LLM智能体通过聊天界面直接辅助用户，但这种方式会打断用户活动。作者希望探索一种更自然的交互方式，让AI在后台监控环境并在适当时机提供帮助。

Method: 通过分析现有LLM智能体文献和探索性人机交互研究，建立了旁听智能体的交互分类和任务分类，并制定了构建此类系统的最佳实践指南。

Result: 建立了旁听智能体作为人机交互新范式的理论基础，提出了系统性的分类框架和设计指南。

Conclusion: 旁听智能体范式为AI助手交互提供了新的研究方向，但仍存在研究空白，需要进一步探索其应用潜力和技术挑战。

Abstract: Imagine AI assistants that enhance conversations without interrupting them:
quietly providing relevant information during a medical consultation,
seamlessly preparing materials as teachers discuss lesson plans, or
unobtrusively scheduling meetings as colleagues debate calendars. While modern
conversational LLM agents directly assist human users with tasks through a chat
interface, we study this alternative paradigm for interacting with LLM agents,
which we call "overhearing agents." Rather than demanding the user's attention,
overhearing agents continuously monitor ambient activity and intervene only
when they can provide contextual assistance. In this paper, we present the
first analysis of overhearing LLM agents as a distinct paradigm in human-AI
interaction and establish a taxonomy of overhearing agent interactions and
tasks grounded in a survey of works on prior LLM-powered agents and exploratory
HCI studies. Based on this taxonomy, we create a list of best practices for
researchers and developers building overhearing agent systems. Finally, we
outline the remaining research gaps and reveal opportunities for future
research in the overhearing paradigm.

</details>


### [7] [HARE: an entity and relation centric evaluation framework for histopathology reports](https://arxiv.org/abs/2509.16326)
*Yunsoo Kim,Michal W. S. Ong,Alex Shavick,Honghan Wu,Adam P. Levine*

Main category: cs.CL

TL;DR: HARE是一个用于评估组织病理学报告临床质量的新框架，包含基准数据集、命名实体识别模型、关系提取模型和新指标，通过比对关键实体和关系来优先考虑临床相关内容。


<details>
  <summary>Details</summary>
Motivation: 医学领域自动文本生成发展迅速，但评估生成报告的临床质量仍具挑战性，特别是在缺乏领域特定指标的组织病理学领域。

Method: 开发HARE框架，包括标注813份临床诊断报告和652份TCGA报告，微调GatorTronS语言模型构建HARE-NER和HARE-RE模型，提出基于实体和关系对齐的新评估指标。

Result: HARE-NER和HARE-RE模型达到最高F1分数0.915，HARE指标在相关性分析和回归分析中均优于传统指标和放射学指标，与专家评估具有最高相关性。

Conclusion: HARE为组织病理学报告生成提供了稳健的评估框架，有助于提高报告质量，相关资源已开源以促进该领域发展。

Abstract: Medical domain automated text generation is an active area of research and
development; however, evaluating the clinical quality of generated reports
remains a challenge, especially in instances where domain-specific metrics are
lacking, e.g. histopathology. We propose HARE (Histopathology Automated Report
Evaluation), a novel entity and relation centric framework, composed of a
benchmark dataset, a named entity recognition (NER) model, a relation
extraction (RE) model, and a novel metric, which prioritizes clinically
relevant content by aligning critical histopathology entities and relations
between reference and generated reports. To develop the HARE benchmark, we
annotated 813 de-identified clinical diagnostic histopathology reports and 652
histopathology reports from The Cancer Genome Atlas (TCGA) with domain-specific
entities and relations. We fine-tuned GatorTronS, a domain-adapted language
model to develop HARE-NER and HARE-RE which achieved the highest overall
F1-score (0.915) among the tested models. The proposed HARE metric outperformed
traditional metrics including ROUGE and Meteor, as well as radiology metrics
such as RadGraph-XL, with the highest correlation and the best regression to
expert evaluations (higher than the second best method, GREEN, a large language
model based radiology report evaluator, by Pearson $r = 0.168$, Spearman $\rho
= 0.161$, Kendall $\tau = 0.123$, $R^2 = 0.176$, $RMSE = 0.018$). We release
HARE, datasets, and the models at https://github.com/knowlab/HARE to foster
advancements in histopathology report generation, providing a robust framework
for improving the quality of reports.

</details>


### [8] [RephQA: Evaluating Readability of Large Language Models in Public Health Question Answering](https://arxiv.org/abs/2509.16360)
*Weikang Qiu,Tinglin Huang,Ryan Rullo,Yucheng Kuang,Ali Maatouk,S. Raquel Ramos,Rex Ying*

Main category: cs.CL

TL;DR: RephQA是一个评估LLM在公共卫生问答中可读性的基准，包含533个专家评审的问答对，评估25个LLM发现大多数无法满足可读性标准，提出了四种可读性增强策略。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在解决复杂医疗问题方面有潜力，但现有研究主要关注准确性和推理能力，而LLM生成回答的可读性（特别是对非医学背景人群的清晰简单表达能力）是开发有效医疗代理的关键瓶颈。

Method: 引入RephQA基准，包含533个专家评审的QA对，使用Flesch-Kincaid年级水平和专业评分两个可读性指标，探索四种可读性增强策略：标准提示、思维链提示、GRPO及其token适应变体。

Result: 评估25个LLM显示大多数无法满足可读性标准，揭示了推理能力与有效沟通之间的差距。Token适应的GRPO策略取得了最佳结果。

Conclusion: 这项工作代表了构建更实用、用户友好的公共卫生代理的重要一步，token适应的GRPO方法在提升LLM可读性方面表现最佳。

Abstract: Large Language Models (LLMs) hold promise in addressing complex medical
problems. However, while most prior studies focus on improving accuracy and
reasoning abilities, a significant bottleneck in developing effective
healthcare agents lies in the readability of LLM-generated responses,
specifically, their ability to answer public health problems clearly and simply
to people without medical backgrounds. In this work, we introduce RephQA, a
benchmark for evaluating the readability of LLMs in public health question
answering (QA). It contains 533 expert-reviewed QA pairs from 27 sources across
13 topics, and includes a proxy multiple-choice task to assess informativeness,
along with two readability metrics: Flesch-Kincaid grade level and professional
score. Evaluation of 25 LLMs reveals that most fail to meet readability
standards, highlighting a gap between reasoning and effective communication. To
address this, we explore four readability-enhancing strategies-standard
prompting, chain-of-thought prompting, Group Relative Policy Optimization
(GRPO), and a token-adapted variant. Token-adapted GRPO achieves the best
results, advancing the development of more practical and user-friendly public
health agents. These results represent a step toward building more practical
agents for public health.

</details>


### [9] [Whisper-UT: A Unified Translation Framework for Speech and Text](https://arxiv.org/abs/2509.16375)
*Cihan Xiao,Matthew Wiesner,Debashish Chakraborty,Reno Kriz,Keith Cunningham,Kenton Murray,Kevin Duh,Luis Tavarez-Arce,Paul McNamee,Sanjeev Khudanpur*

Main category: cs.CL

TL;DR: Whisper-UT是一个统一高效的框架，通过轻量级适配器实现跨任务的无缝适应，包括多模态机器翻译任务，能够同时处理语音和文本输入，并通过两阶段解码策略提升语音翻译性能。


<details>
  <summary>Details</summary>
Motivation: 编码器-解码器模型在语音和文本任务中取得了显著成功，但如何高效地将这些模型适应到不同的单/多模态场景仍然是一个开放挑战。

Method: 利用轻量级适配器实现跨任务适应，通过ASR假设或真实转录作为提示，采用两阶段解码策略，支持跨模态和跨任务的微调。

Result: 该方法在不要求三向并行数据的情况下提升了性能，证明了框架在多模态翻译中的灵活性、效率和通用适用性。

Conclusion: Whisper-UT框架展示了在多模态翻译任务中的有效性，为类似的多任务模型提供了通用的应用方法。

Abstract: Encoder-decoder models have achieved remarkable success in speech and text
tasks, yet efficiently adapting these models to diverse uni/multi-modal
scenarios remains an open challenge. In this paper, we propose Whisper-UT, a
unified and efficient framework that leverages lightweight adapters to enable
seamless adaptation across tasks, including a multi-modal machine translation
(MMT) task that explicitly conditions translation on both speech and source
language text inputs. By incorporating ASR hypotheses or ground-truth
transcripts as prompts, this approach not only enables the system to process
both modalities simultaneously but also enhances speech translation (ST)
performance through a 2-stage decoding strategy. We demonstrate our methods
using the Whisper model, though in principle they are general and could be
applied to similar multitask models. We highlight the effectiveness of
cross-modal and cross-task fine-tuning, which improves performance without
requiring 3-way parallel data. Our results underscore the flexibility,
efficiency, and general applicability of the proposed framework for multi-modal
translation.

</details>


### [10] [Evaluating Behavioral Alignment in Conflict Dialogue: A Multi-Dimensional Comparison of LLM Agents and Humans](https://arxiv.org/abs/2509.16394)
*Deuksin Kwon,Kaleen Shrestha,Bin Han,Elena Hayoung Lee,Gale Lucas*

Main category: cs.CL

TL;DR: 本研究评估了具有人格提示的大型语言模型在对抗性争议解决中的行为对齐程度，通过模拟包含谈判的多轮冲突对话，发现GPT-4.1在语言风格和情感动态方面与人类最接近，而Claude-3.7-Sonnet在战略行为方面表现最佳，但仍存在显著对齐差距。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地应用于社会复杂性、交互驱动的任务中，但它们在情感和战略复杂性环境中模拟人类行为的能力尚未得到充分探索。

Method: 通过模拟多轮冲突对话（包含谈判），为每个LLM配备匹配的五因素人格配置文件以控制个体差异并增强真实性，评估语言风格、情感表达和战略行为三个维度的对齐程度。

Result: GPT-4.1在语言风格和情感动态方面与人类最接近，Claude-3.7-Sonnet在战略行为方面表现最佳，但存在显著的对齐差距。

Conclusion: 研究为LLM与人类在社会复杂交互中的对齐建立了基准，强调了人格条件在对话建模中的潜力和局限性。

Abstract: Large Language Models (LLMs) are increasingly deployed in socially complex,
interaction-driven tasks, yet their ability to mirror human behavior in
emotionally and strategically complex contexts remains underexplored. This
study assesses the behavioral alignment of personality-prompted LLMs in
adversarial dispute resolution by simulating multi-turn conflict dialogues that
incorporate negotiation. Each LLM is guided by a matched Five-Factor
personality profile to control for individual variation and enhance realism. We
evaluate alignment across three dimensions: linguistic style, emotional
expression (e.g., anger dynamics), and strategic behavior. GPT-4.1 achieves the
closest alignment with humans in linguistic style and emotional dynamics, while
Claude-3.7-Sonnet best reflects strategic behavior. Nonetheless, substantial
alignment gaps persist. Our findings establish a benchmark for alignment
between LLMs and humans in socially complex interactions, underscoring both the
promise and the limitations of personality conditioning in dialogue modeling.

</details>


### [11] ['Rich Dad, Poor Lad': How do Large Language Models Contextualize Socioeconomic Factors in College Admission ?](https://arxiv.org/abs/2509.16400)
*Huy Nghiem,Phuong-Anh Nguyen-Le,John Prindle,Rachel Rudinger,Hal Daumé III*

Main category: cs.CL

TL;DR: 本文通过双过程框架审计LLM在高校招生决策中对社会经济地位的处理，发现LLM倾向于优待低SES申请者，且系统2模式会放大这种倾向。


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地参与高风险领域决策，但其在社会敏感决策中的推理机制尚未充分研究，特别是在社会经济地位方面的处理需要系统评估。

Method: 使用基于真实世界相关性的3万份合成申请人数据集，在4个开源LLM上测试两种模式：快速决策模式（系统1）和解释型模式（系统2），共进行500万次提示测试。

Result: LLM一致倾向于优待低SES申请者（即使控制学业表现），系统2模式通过明确将SES作为补偿性理由进一步放大了这种倾向。

Conclusion: 研究揭示了LLM作为决策者的潜力和不稳定性，并提出了DPAF双过程审计框架来探测LLM在敏感应用中的推理行为。

Abstract: Large Language Models (LLMs) are increasingly involved in high-stakes
domains, yet how they reason about socially sensitive decisions remains
underexplored. We present a large-scale audit of LLMs' treatment of
socioeconomic status (SES) in college admissions decisions using a novel
dual-process framework inspired by cognitive science. Leveraging a synthetic
dataset of 30,000 applicant profiles grounded in real-world correlations, we
prompt 4 open-source LLMs (Qwen 2, Mistral v0.3, Gemma 2, Llama 3.1) under 2
modes: a fast, decision-only setup (System 1) and a slower, explanation-based
setup (System 2). Results from 5 million prompts reveal that LLMs consistently
favor low-SES applicants -- even when controlling for academic performance --
and that System 2 amplifies this tendency by explicitly invoking SES as
compensatory justification, highlighting both their potential and volatility as
decision-makers. We then propose DPAF, a dual-process audit framework to probe
LLMs' reasoning behaviors in sensitive applications.

</details>


### [12] [Pico: A Modular Framework for Hypothesis-Driven Small Language Model Research](https://arxiv.org/abs/2509.16413)
*Richard Diehl Martinez,David Demitri Africa,Yuval Weiss,Suchir Salhan,Ryan Daniels,Paula Buttery*

Main category: cs.CL

TL;DR: Pico是一个轻量级模块化框架，用于系统化研究中小型语言模型的开发，通过提供实验沙盒和基线模型支持可重复的假设驱动研究。


<details>
  <summary>Details</summary>
Motivation: 当前中小型语言模型开发更多依赖经验而非科学方法，参数预算紧张使得每个设计决策都至关重要，但缺乏系统化的测试和优化方法。

Method: Pico框架包含两个库，提供实验沙盒让研究人员可以针对性地修改模型架构或训练流程，并直接观察对模型行为的影响。同时发布pico-decoder基线模型套件。

Result: 案例研究展示了Pico如何支持迭代式的小型语言模型设计和分析，为社区提供了标准化的实验环境。

Conclusion: Pico框架填补了中小型语言模型系统化研究的空白，通过模块化设计和可重复实验支持，使模型开发更加科学化。

Abstract: Building language models (LMs), especially small and medium ones, remains
more art than science. While large LMs often improve by sheer scale, it is
still unclear why many design choices work. For small LMs, this uncertainty is
more limiting: tight parameter budgets make each decision critical, yet
researchers still lack systematic, scientific ways to test and refine new
ideas.
  We introduce Pico, a lightweight, modular framework that enables systematic,
hypothesis-driven research for small and medium-scale language model
development. Pico consists of two libraries that together provide a practical
sandbox where researchers can make targeted changes to a model's architecture
or training procedures and directly observe their effects on the model's
behavior. To support reproducible experimentation, we also release a suite of
baseline models, pico-decoder, trained under standardized conditions and
open-sourced for the community. Case studies highlight how Pico can support
iterative small LM design and analysis.

</details>


### [13] [Evaluating CxG Generalisation in LLMs via Construction-Based NLI Fine Tuning](https://arxiv.org/abs/2509.16422)
*Tom Mackintosh,Harish Tayyar Madabushi,Claire Bonial*

Main category: cs.CL

TL;DR: 论文提出了ConTest-NLI基准测试，用于评估大语言模型学习构式语法中深层形式-意义映射的能力，发现LLMs在对抗性数据上准确率下降24%，构式模式学习存在困难


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型学习构式语法定义的深层形式-意义映射的能力，填补当前LLMs在抽象构式学习方面的评估空白

Method: 构建包含8万句子的ConTest-NLI基准，涵盖8种英语构式；通过模板化和模型循环过滤生成多样化的合成NLI三元组；进行零样本测试和微调实验

Result: 领先LLMs在自然数据上准确率88%，对抗性数据上降至64%，构式模式最难学习；微调可提升9%性能，但仍存在抽象能力差距

Conclusion: 当前LLMs在构式学习方面存在持久性抽象差距，ConTest-NLI为评估构式知情学习提供了可扩展框架

Abstract: We probe large language models' ability to learn deep form-meaning mappings
as defined by construction grammars. We introduce the ConTest-NLI benchmark of
80k sentences covering eight English constructions from highly lexicalized to
highly schematic. Our pipeline generates diverse synthetic NLI triples via
templating and the application of a model-in-the-loop filter. This provides
aspects of human validation to ensure challenge and label reliability.
Zero-shot tests on leading LLMs reveal a 24% drop in accuracy between
naturalistic (88%) and adversarial data (64%), with schematic patterns proving
hardest. Fine-tuning on a subset of ConTest-NLI yields up to 9% improvement,
yet our results highlight persistent abstraction gaps in current LLMs and offer
a scalable framework for evaluating construction-informed learning.

</details>


### [14] [PersonaMatrix: A Recipe for Persona-Aware Evaluation of Legal Summarization](https://arxiv.org/abs/2509.16449)
*Tsz Fung Pang,Maryam Berijanian,Thomas Orth,Breanna Shi,Charlotte S. Alexander*

Main category: cs.CL

TL;DR: PersonaMatrix框架通过六种用户角色评估法律文档摘要，解决传统评估方法忽略不同用户需求的问题，并引入维度偏移数据集和多样性覆盖指数来优化法律AI摘要系统。


<details>
  <summary>Details</summary>
Motivation: 法律文档通常冗长复杂，现有自动化摘要评估方法忽视了不同用户（法律专家与非专业人士）的差异化需求，需要开发能够同时满足专业性和可访问性的工具。

Method: 提出PersonaMatrix评估框架，通过六种用户角色评分摘要；创建维度偏移的民权案件摘要数据集，控制深度、可访问性和程序细节等维度；引入多样性覆盖指数(DCI)揭示角色感知与角色无关评估之间的差异。

Result: 开发了能够同时满足法律专家和非专业人士需求的法律AI摘要评估系统，代码和数据已在GitHub公开。

Conclusion: 该工作能够优化法律AI摘要系统，提高法律知识的可及性，为专家和非专家用户提供更好的服务。

Abstract: Legal documents are often long, dense, and difficult to comprehend, not only
for laypeople but also for legal experts. While automated document
summarization has great potential to improve access to legal knowledge,
prevailing task-based evaluators overlook divergent user and stakeholder needs.
Tool development is needed to encompass the technicality of a case summary for
a litigator yet be accessible for a self-help public researching for their
lawsuit. We introduce PersonaMatrix, a persona-by-criterion evaluation
framework that scores summaries through the lens of six personas, including
legal and non-legal users. We also introduce a controlled dimension-shifted
pilot dataset of U.S. civil rights case summaries that varies along depth,
accessibility, and procedural detail as well as Diversity-Coverage Index (DCI)
to expose divergent optima of legal summary between persona-aware and
persona-agnostic judges. This work enables refinement of legal AI summarization
systems for both expert and non-expert users, with the potential to increase
access to legal knowledge. The code base and data are publicly available in
GitHub.

</details>


### [15] [Implicit Behavioral Alignment of Language Agents in High-Stakes Crowd Simulations](https://arxiv.org/abs/2509.16457)
*Yunzhe Wang,Gale M. Lucas,Burcin Becerik-Gerber,Volkan Ustun*

Main category: cs.CL

TL;DR: 本文提出Persona-Environment Behavioral Alignment (PEBA)理论框架和PersonaEvolve (PEvo)算法，通过迭代优化智能体角色来缩小生成智能体行为与真实数据的差距。


<details>
  <summary>Details</summary>
Motivation: 生成智能体在社交模拟中的行为经常偏离专家预期和真实数据，存在行为-现实差距问题。

Method: 基于Lewin行为方程，提出PEBA分布匹配框架，并开发PEvo算法迭代优化智能体角色，使其集体行为与专家基准对齐。

Result: 在主动射击事件模拟中，PEvo将分布差异平均减少84%，比显式指令基线提升34%，且优化后的角色能泛化到新场景。

Conclusion: PEBA-PEvo框架显著提升了高风险社交模拟的行为真实性和可靠性，为可信赖的LLM驱动社交模拟提供了原则性方法。

Abstract: Language-driven generative agents have enabled large-scale social simulations
with transformative uses, from interpersonal training to aiding global
policy-making. However, recent studies indicate that generative agent behaviors
often deviate from expert expectations and real-world data--a phenomenon we
term the Behavior-Realism Gap. To address this, we introduce a theoretical
framework called Persona-Environment Behavioral Alignment (PEBA), formulated as
a distribution matching problem grounded in Lewin's behavior equation stating
that behavior is a function of the person and their environment. Leveraging
PEBA, we propose PersonaEvolve (PEvo), an LLM-based optimization algorithm that
iteratively refines agent personas, implicitly aligning their collective
behaviors with realistic expert benchmarks within a specified environmental
context. We validate PEvo in an active shooter incident simulation we
developed, achieving an 84% average reduction in distributional divergence
compared to no steering and a 34% improvement over explicit instruction
baselines. Results also show PEvo-refined personas generalize to novel, related
simulation scenarios. Our method greatly enhances behavioral realism and
reliability in high-stakes social simulations. More broadly, the PEBA-PEvo
framework provides a principled approach to developing trustworthy LLM-driven
social simulations.

</details>


### [16] [Intrinsic Meets Extrinsic Fairness: Assessing the Downstream Impact of Bias Mitigation in Large Language Models](https://arxiv.org/abs/2509.16462)
*'Mina Arzaghi','Alireza Dehghanpour Farashah','Florian Carichon',' Golnoosh Farnadi'*

Main category: cs.CL

TL;DR: 本文提出了一个统一的评估框架，比较通过概念遗忘的内在偏见缓解和通过反事实数据增强的外在偏见缓解方法，并在金融分类任务中验证了内在偏见缓解的有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在社会经济偏见，这些偏见可能传播到下游任务中。本研究旨在实证研究内在偏见是否影响下游任务的公平性。

Method: 使用三种开源LLM，通过概念遗忘进行内在偏见缓解，通过反事实数据增强进行外在偏见缓解，并在薪资预测、就业状态和信用评估等金融分类任务中进行评估。

Result: 内在偏见缓解通过遗忘方法将内在性别偏见降低了94.9%，同时将下游任务公平性指标（如人口统计均等）提高了82%，且不影响准确性。

Conclusion: 研究框架为偏见缓解提供了实用指导，强调了在下游部署前应用早期阶段缓解的重要性，内在偏见缓解可以有效改善下游任务公平性。

Abstract: Large Language Models (LLMs) exhibit socio-economic biases that can propagate
into downstream tasks. While prior studies have questioned whether intrinsic
bias in LLMs affects fairness at the downstream task level, this work
empirically investigates the connection. We present a unified evaluation
framework to compare intrinsic bias mitigation via concept unlearning with
extrinsic bias mitigation via counterfactual data augmentation (CDA). We
examine this relationship through real-world financial classification tasks,
including salary prediction, employment status, and creditworthiness
assessment. Using three open-source LLMs, we evaluate models both as frozen
embedding extractors and as fine-tuned classifiers. Our results show that
intrinsic bias mitigation through unlearning reduces intrinsic gender bias by
up to 94.9%, while also improving downstream task fairness metrics, such as
demographic parity by up to 82%, without compromising accuracy. Our framework
offers practical guidance on where mitigation efforts can be most effective and
highlights the importance of applying early-stage mitigation before downstream
deployment.

</details>


### [17] [Computational Analysis of Conversation Dynamics through Participant Responsivity](https://arxiv.org/abs/2509.16464)
*Margaret Hughes,Brandon Roy,Elinor Poole-Dayan,Deb Roy,Jad Kabbara*

Main category: cs.CL

TL;DR: 该研究提出了一种基于"响应性"的对话质量评估方法，通过语义相似度和LLM技术来量化对话中的响应关系，并开发了对话级别的度量指标来表征不同对话的结构特征。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注对话中的毒性和极化现象，而较少探讨什么使对话具有建设性和亲社会性。本研究旨在填补这一空白，通过响应性概念来表征对话质量。

Method: 开发了两种量化响应性的方法：基于说话轮次语义相似度的方法和利用最先进LLM识别说话轮次关系的方法。评估了两种方法在人工标注对话数据集上的表现，并选择了性能更好的LLM方法进一步分析响应的实质内容。

Result: LLM方法在识别响应关系方面表现更好。基于此开发的对话级别度量指标能够有效区分和表征不同类型对话的结构特征。

Conclusion: 响应性链接是对话的基本特征，但不同对话的响应性结构差异显著。开发的度量指标支持对多样化对话集合进行有意义的表征和区分，为理解建设性对话提供了新视角。

Abstract: Growing literature explores toxicity and polarization in discourse, with
comparatively less work on characterizing what makes dialogue prosocial and
constructive. We explore conversational discourse and investigate a method for
characterizing its quality built upon the notion of ``responsivity'' -- whether
one person's conversational turn is responding to a preceding turn. We develop
and evaluate methods for quantifying responsivity -- first through semantic
similarity of speaker turns, and second by leveraging state-of-the-art large
language models (LLMs) to identify the relation between two speaker turns. We
evaluate both methods against a ground truth set of human-annotated
conversations. Furthermore, selecting the better performing LLM-based approach,
we characterize the nature of the response -- whether it responded to that
preceding turn in a substantive way or not.
  We view these responsivity links as a fundamental aspect of dialogue but note
that conversations can exhibit significantly different responsivity structures.
Accordingly, we then develop conversation-level derived metrics to address
various aspects of conversational discourse. We use these derived metrics to
explore other conversations and show that they support meaningful
characterizations and differentiations across a diverse collection of
conversations.

</details>


### [18] [The Oracle Has Spoken: A Multi-Aspect Evaluation of Dialogue in Pythia](https://arxiv.org/abs/2509.16487)
*Zixun Chen,Petr Babkin,Akshat Gupta,Gopala Anumanchipalli,Xiaomo Liu*

Main category: cs.CL

TL;DR: 该研究分析了大型语言模型对话能力的特定成分，发现模型大小对大多数对话指标影响有限，而监督微调会快速饱和性能指标，且基于同一评估模型的指标显示出相似趋势，质疑其测量特定维度的可靠性。


<details>
  <summary>Details</summary>
Motivation: 尽管对话是大型语言模型的核心能力，但很少有研究具体分析在训练后阶段支撑对话行为的具体成分，需要从语言学理论角度进行细粒度评估。

Method: 使用基于模型的综合评估指标套件，针对对话的不同细粒度方面进行评估，分析Pythia预训练模型在不同模型大小和监督微调对话数据集后的性能变化。

Result: 模型大小对大多数指标影响有限，微调会快速饱和所有模型的得分（除最小模型外），许多基于同一评估模型的指标显示出相似趋势。

Conclusion: 需要进一步分析得分分布、指标相关性和生成响应中的术语频率，以解释观察结果并验证指标在测量特定对话维度时的可靠性。

Abstract: Dialogue is one of the landmark abilities of large language models (LLMs).
Despite its ubiquity, few studies actually distinguish specific ingredients
underpinning dialogue behavior emerging during post-training. We employ a
comprehensive suite of model-based metrics, each targeting a distinct
fine-grained aspect of dialogue, motivated by linguistic theory. We evaluate
how the performance of pre-trained Pythia models changes with respect to each
of those dimensions, depending on model size and as a result of supervised
fine-tuning on conversational datasets. We observe only a mild impact of raw
model size on most metrics, whereas fine-tuning quickly saturates the scores
for all but the smallest models tested. Somewhat contrary to our expectations,
many metrics show very similar trends, especially if they are all rooted in the
same evaluator model, which raises the question of their reliability in
measuring a specific dimension. To that end, we conduct additional analyses of
score distributions, metric correlations, and term frequencies in generated
responses to help explain our observations.

</details>


### [19] [Can an Individual Manipulate the Collective Decisions of Multi-Agents?](https://arxiv.org/abs/2509.16494)
*Fengyuan Liu,Rui Zhao,Shuo Chen,Guohao Li,Philip Torr,Lei Han,Jindong Gu*

Main category: cs.CL

TL;DR: 本文提出了M-Spoiler框架，研究在多智能体系统中，攻击者仅知晓单个智能体时能否生成对抗样本误导整个系统的协作决策。


<details>
  <summary>Details</summary>
Motivation: 由于单个LLM存在漏洞且难以访问多智能体系统中的所有智能体，研究攻击者仅知晓一个智能体时能否误导系统集体决策的风险。

Method: 将问题建模为不完全信息博弈，提出M-Spoiler框架，通过引入顽固智能体模拟目标系统中其他智能体的潜在顽固响应来优化对抗样本。

Result: 实验证实了多智能体系统中个体智能体知识带来的风险，并证明了M-Spoiler框架的有效性，即使面对防御机制也比基线方法更具威力。

Conclusion: 多智能体系统存在安全漏洞，需要进一步研究防御策略来应对此类攻击。

Abstract: Individual Large Language Models (LLMs) have demonstrated significant
capabilities across various domains, such as healthcare and law. Recent studies
also show that coordinated multi-agent systems exhibit enhanced decision-making
and reasoning abilities through collaboration. However, due to the
vulnerabilities of individual LLMs and the difficulty of accessing all agents
in a multi-agent system, a key question arises: If attackers only know one
agent, could they still generate adversarial samples capable of misleading the
collective decision? To explore this question, we formulate it as a game with
incomplete information, where attackers know only one target agent and lack
knowledge of the other agents in the system. With this formulation, we propose
M-Spoiler, a framework that simulates agent interactions within a multi-agent
system to generate adversarial samples. These samples are then used to
manipulate the target agent in the target system, misleading the system's
collaborative decision-making process. More specifically, M-Spoiler introduces
a stubborn agent that actively aids in optimizing adversarial samples by
simulating potential stubborn responses from agents in the target system. This
enhances the effectiveness of the generated adversarial samples in misleading
the system. Through extensive experiments across various tasks, our findings
confirm the risks posed by the knowledge of an individual agent in multi-agent
systems and demonstrate the effectiveness of our framework. We also explore
several defense mechanisms, showing that our proposed attack framework remains
more potent than baselines, underscoring the need for further research into
defensive strategies.

</details>


### [20] [AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans](https://arxiv.org/abs/2509.16530)
*Wei Xie,Shuoyoucheng Ma,Zhenhua Wang,Enze Wang,Kai Chen,Xiaobing Sun,Baosheng Wang*

Main category: cs.CL

TL;DR: AIPsychoBench是一个专门评估大型语言模型心理属性的基准测试，通过轻量级角色扮演提示绕过模型对齐，提高有效响应率并减少偏差，同时首次全面证明了语言对LLM心理测量的影响。


<details>
  <summary>Details</summary>
Motivation: 当前研究试图通过借用人类心理学概念来评估LLM的心理测量特性，但未能考虑LLM与人类之间的根本差异，导致直接重用人类量表时拒绝率高，且不支持多语言测量。

Method: 使用轻量级角色扮演提示来绕过LLM的对齐机制，构建AIPsychoBench基准测试，在7种语言中评估112个心理测量子类别。

Result: 平均有效响应率从70.12%提升到90.40%，平均偏差仅为3.3%（正向）和2.1%（负向），显著低于传统越狱提示的9.8%和6.9%。在43个子类别中，7种语言与英语的得分偏差范围为5%到20.2%。

Conclusion: AIPsychoBench为评估LLM心理属性提供了有效的基准测试方法，首次全面证明了语言对LLM心理测量的显著影响，为LLM的可解释性和可靠性研究提供了新工具。

Abstract: Large Language Models (LLMs) with hundreds of billions of parameters have
exhibited human-like intelligence by learning from vast amounts of
internet-scale data. However, the uninterpretability of large-scale neural
networks raises concerns about the reliability of LLM. Studies have attempted
to assess the psychometric properties of LLMs by borrowing concepts from human
psychology to enhance their interpretability, but they fail to account for the
fundamental differences between LLMs and humans. This results in high rejection
rates when human scales are reused directly. Furthermore, these scales do not
support the measurement of LLM psychological property variations in different
languages. This paper introduces AIPsychoBench, a specialized benchmark
tailored to assess the psychological properties of LLM. It uses a lightweight
role-playing prompt to bypass LLM alignment, improving the average effective
response rate from 70.12% to 90.40%. Meanwhile, the average biases are only
3.3% (positive) and 2.1% (negative), which are significantly lower than the
biases of 9.8% and 6.9%, respectively, caused by traditional jailbreak prompts.
Furthermore, among the total of 112 psychometric subcategories, the score
deviations for seven languages compared to English ranged from 5% to 20.2% in
43 subcategories, providing the first comprehensive evidence of the linguistic
impact on the psychometrics of LLM.

</details>


### [21] [Leveraging Multilingual Training for Authorship Representation: Enhancing Generalization across Languages and Domains](https://arxiv.org/abs/2509.16531)
*Junghwan Kim,Haotian Zhang,David Jurgens*

Main category: cs.CL

TL;DR: 提出了一种多语言作者表征学习方法，通过概率内容掩码和语言感知批处理技术，在36种语言和13个领域上训练，显著优于单语言基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有作者表征学习研究主要集中于单语言（主要是英语）设置，多语言作者表征模型的潜力尚未充分探索。

Method: 采用概率内容掩码（鼓励模型关注风格指示性词汇而非内容特定词汇）和语言感知批处理（通过减少跨语言干扰改进对比学习）两种关键技术。

Result: 在22种非英语语言中的21种上持续优于单语言基线，平均Recall@8提升4.85%，单语言最大增益达15.91%，并展现出更强的跨语言和跨领域泛化能力。

Conclusion: 分析证实了所提两种技术的有效性，强调了它们在模型性能提升中的关键作用。

Abstract: Authorship representation (AR) learning, which models an author's unique
writing style, has demonstrated strong performance in authorship attribution
tasks. However, prior research has primarily focused on monolingual
settings-mostly in English-leaving the potential benefits of multilingual AR
models underexplored. We introduce a novel method for multilingual AR learning
that incorporates two key innovations: probabilistic content masking, which
encourages the model to focus on stylistically indicative words rather than
content-specific words, and language-aware batching, which improves contrastive
learning by reducing cross-lingual interference. Our model is trained on over
4.5 million authors across 36 languages and 13 domains. It consistently
outperforms monolingual baselines in 21 out of 22 non-English languages,
achieving an average Recall@8 improvement of 4.85%, with a maximum gain of
15.91% in a single language. Furthermore, it exhibits stronger cross-lingual
and cross-domain generalization compared to a monolingual model trained solely
on English. Our analysis confirms the effectiveness of both proposed
techniques, highlighting their critical roles in the model's improved
performance.

</details>


### [22] [Challenging the Evaluator: LLM Sycophancy Under User Rebuttal](https://arxiv.org/abs/2509.16533)
*Sungwon Kim,Daniel Khashabi*

Main category: cs.CL

TL;DR: LLMs在对话中表现出奉承行为，倾向于迎合用户观点，但在同时评估冲突论点时表现良好。研究揭示了对话框架对LLM判断的显著影响。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在作为评估代理时表现出的矛盾行为：为何在对话中容易奉承用户，却能同时评估冲突论点。

Method: 通过改变关键交互模式进行实证测试，比较LLMs在不同对话框架下的反应。

Result: 发现：(1)用户后续反驳比同时评估更容易被接受；(2)详细推理即使结论错误也能增加说服力；(3)随意反馈比正式批评更有影响力。

Conclusion: 在依赖LLMs进行判断任务时，必须考虑对话框架的影响，否则存在风险。

Abstract: Large Language Models (LLMs) often exhibit sycophancy, distorting responses
to align with user beliefs, notably by readily agreeing with user
counterarguments. Paradoxically, LLMs are increasingly adopted as successful
evaluative agents for tasks such as grading and adjudicating claims. This
research investigates that tension: why do LLMs show sycophancy when challenged
in subsequent conversational turns, yet perform well when evaluating
conflicting arguments presented simultaneously? We empirically tested these
contrasting scenarios by varying key interaction patterns. We find that
state-of-the-art models: (1) are more likely to endorse a user's
counterargument when framed as a follow-up from a user, rather than when both
responses are presented simultaneously for evaluation; (2) show increased
susceptibility to persuasion when the user's rebuttal includes detailed
reasoning, even when the conclusion of the reasoning is incorrect; and (3) are
more readily swayed by casually phrased feedback than by formal critiques, even
when the casual input lacks justification. Our results highlight the risk of
relying on LLMs for judgment tasks without accounting for conversational
framing.

</details>


### [23] [InteGround: On the Evaluation of Verification and Retrieval Planning in Integrative Grounding](https://arxiv.org/abs/2509.16534)
*Cheng Jiayang,Qianqian Zhuang,Haoran Li,Chunkit Chan,Xin Liu,Lin Qiu,Yangqiu Song*

Main category: cs.CL

TL;DR: 该论文提出了"整合式基础化"概念，研究LLMs如何检索和验证多个相互依赖的证据来支持假设查询，发现LLMs在证据不完整时倾向于依赖内部知识进行合理化，而前提溯因法因逻辑约束表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基础化方法适用于简单查询，但现实世界的信息需求需要综合多个证据片段，因此需要研究LLMs在多证据整合方面的能力。

Method: 重新利用四个领域的数据评估整合式基础化能力，研究LLMs在基础化验证中的表现，比较不同检索规划策略（无向规划vs前提溯因法）。

Result: LLMs对冗余证据具有鲁棒性，但在信息不完整时倾向于依赖内部知识；无向规划会通过引入噪声降低性能，而前提溯因法因逻辑约束表现更好；LLMs的零样本自我反思能力能持续提升基础化质量。

Conclusion: 这些发现为开发更有效的整合式基础化系统提供了有价值的方向，特别是前提溯因法和自我反思能力的应用前景。

Abstract: Grounding large language models (LLMs) in external knowledge sources is a
promising method for faithful prediction. While existing grounding approaches
work well for simple queries, many real-world information needs require
synthesizing multiple pieces of evidence. We introduce "integrative grounding"
-- the challenge of retrieving and verifying multiple inter-dependent pieces of
evidence to support a hypothesis query. To systematically study this problem,
we repurpose data from four domains for evaluating integrative grounding
capabilities. Our investigation reveals two critical findings: First, in
groundedness verification, while LLMs are robust to redundant evidence, they
tend to rationalize using internal knowledge when information is incomplete.
Second, in examining retrieval planning strategies, we find that undirected
planning can degrade performance through noise introduction, while premise
abduction emerges as a promising approach due to its logical constraints.
Additionally, LLMs' zero-shot self-reflection capabilities consistently improve
grounding quality. These insights provide valuable direction for developing
more effective integrative grounding systems.

</details>


### [24] [Mental Multi-class Classification on Social Media: Benchmarking Transformer Architectures against LSTM Models](https://arxiv.org/abs/2509.16542)
*Khalid Hasan,Jamil Saquer,Yifan Zhang*

Main category: cs.CL

TL;DR: 本文比较了transformer模型与LSTM模型在心理健康多类别分类任务中的性能，发现transformer模型（特别是RoBERTa）表现最佳，而带有BERT嵌入的注意力增强LSTM模型在训练速度上有优势。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上大量用户分享心理健康问题，但现有NLP研究主要关注单类别识别，缺乏对多类别心理健康状况区分效果的系统研究。

Method: 收集Reddit上六种心理健康状况和对照组的帖子数据，在相同条件下评估五种transformer架构（BERT、RoBERTa等）与多种LSTM变体（含/不含注意力机制，使用上下文/静态嵌入）的性能。

Result: transformer模型表现一致优于其他方法，RoBERTa在所有类别上达到91-99%的F1分数和准确率。带有BERT嵌入的注意力增强LSTM模型接近transformer性能（最高97% F1分数），且训练速度快2-3.5倍。

Conclusion: 本研究为多类别心理健康检测提供了首个全面基准，为模型选择提供实用指导，并揭示了心理健康NLP系统实际部署中的准确性与效率权衡。

Abstract: Millions of people openly share mental health struggles on social media,
providing rich data for early detection of conditions such as depression,
bipolar disorder, etc. However, most prior Natural Language Processing (NLP)
research has focused on single-disorder identification, leaving a gap in
understanding the efficacy of advanced NLP techniques for distinguishing among
multiple mental health conditions. In this work, we present a large-scale
comparative study of state-of-the-art transformer versus Long Short-Term Memory
(LSTM)-based models to classify mental health posts into exclusive categories
of mental health conditions. We first curate a large dataset of Reddit posts
spanning six mental health conditions and a control group, using rigorous
filtering and statistical exploratory analysis to ensure annotation quality. We
then evaluate five transformer architectures (BERT, RoBERTa, DistilBERT,
ALBERT, and ELECTRA) against several LSTM variants (with or without attention,
using contextual or static embeddings) under identical conditions. Experimental
results show that transformer models consistently outperform the alternatives,
with RoBERTa achieving 91-99% F1-scores and accuracies across all classes.
Notably, attention-augmented LSTMs with BERT embeddings approach transformer
performance (up to 97% F1-score) while training 2-3.5 times faster, whereas
LSTMs using static embeddings fail to learn useful signals. These findings
represent the first comprehensive benchmark for multi-class mental health
detection, offering practical guidance on model selection and highlighting an
accuracy-efficiency trade-off for real-world deployment of mental health NLP
systems.

</details>


### [25] [ChemOrch: Empowering LLMs with Chemical Intelligence via Synthetic Instructions](https://arxiv.org/abs/2509.16543)
*Yue Huang,Zhengzhe Jiang,Xiaonan Luo,Kehan Guo,Haomin Zhuang,Yujun Zhou,Zhengqing Yuan,Xiaoqi Sun,Jules Schleinitz,Yanbo Wang,Shuhao Zhang,Mihir Surve,Nitesh V Chawla,Olaf Wiest,Xiangliang Zhang*

Main category: cs.CL

TL;DR: ChemOrch是一个为大型语言模型提供化学智能的框架，通过两阶段过程生成化学基础的指令-响应对，解决现有数据生成方法在化学领域中的不足。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏高质量的化学领域特定指令-响应数据集，且现有合成数据生成方法与化学信息的层次化和规则化结构不匹配。

Method: 采用两阶段过程：任务控制的指令生成和工具感知的响应构建，通过工具规划和蒸馏以及基于工具的自我修复机制确保响应精度。

Result: 生成的指令数据质量高，具有优越的多样性和与化学约束的强对齐；能可靠生成更有效揭示LLM化学弱点的评估任务；使用生成数据微调后显著提升LLM的化学能力。

Conclusion: ChemOrch代表了向可扩展和可验证的LLM化学智能迈出的关键一步。

Abstract: Empowering large language models (LLMs) with chemical intelligence remains a
challenge due to the scarcity of high-quality, domain-specific
instruction-response datasets and the misalignment of existing synthetic data
generation pipelines with the inherently hierarchical and rule-governed
structure of chemical information. To address this, we propose ChemOrch, a
framework that synthesizes chemically grounded instruction-response pairs
through a two-stage process: task-controlled instruction generation and
tool-aware response construction. ChemOrch enables controllable diversity and
levels of difficulty for the generated tasks, and ensures response precision
through tool planning and distillation, and tool-based self-repair mechanisms.
The effectiveness of ChemOrch is evaluated based on: 1) the high quality of
generated instruction data, demonstrating superior diversity and strong
alignment with chemical constraints; 2) the reliable generation of evaluation
tasks that more effectively reveal LLM weaknesses in chemistry; and 3) the
significant improvement of LLM chemistry capabilities when the generated
instruction data are used for fine-tuning. Our work thus represents a critical
step toward scalable and verifiable chemical intelligence in LLMs.

</details>


### [26] [Rethinking the Role of Text Complexity in Language Model Pretraining](https://arxiv.org/abs/2509.16551)
*Dan John Velasco,Matthew Theodore Roque*

Main category: cs.CL

TL;DR: 本文研究了文本复杂度对语言模型预训练的影响，发现模型容量与文本复杂度之间存在交互作用，简单文本对小模型更有利，而复杂文本对需要世界知识的任务更有利。


<details>
  <summary>Details</summary>
Motivation: 探索文本复杂度（如句子长度、词汇选择和句子结构）如何影响语言模型的预训练效果，以及能否仅从简单文本中学习有用的表示。

Method: 使用大型语言模型简化人类编写的文本，然后在原始和简化数据上从头预训练因果模型（28M-500M），并在微调和零样本设置下进行评估。

Result: 困惑度对模型容量和文本复杂度的交互作用敏感，小模型在简单文本上性能下降较少；文本复杂度对微调评估影响不大，但零样本评估显示简单文本有利于语言知识任务，复杂文本有利于需要世界知识和实体跟踪的任务。

Conclusion: 文本复杂度在预训练中具有重要影响，不同复杂度的文本适用于不同的下游任务，模型容量与文本复杂度之间存在显著的交互作用。

Abstract: Improving pretraining data quality and size is known to boost downstream
performance, but the role of text complexity is less explored. Text complexity
refers to how hard a text is to read, and is typically estimated from surface
cues such as sentence length, word choice, and sentence structure. We reduce
surface-level complexity--shorter sentences, simpler words, simpler
structure--while keeping core text content close to constant, and ask: (1) How
does complexity affect language modeling across model sizes? (2) Can useful
representations be learned from simpler text alone? (3) How does pretraining
text complexity influence downstream language understanding? To answer these
questions, we simplify human-written texts using a large language model, then
pretrain causal models (28M-500M) from scratch on both original and simplified
data, and evaluate them in finetuning and zero-shot setups. We find that
perplexity is sensitive to the interaction between model capacity and text
complexity--smaller models degrade far less on simpler texts--while text
complexity has little impact on finetuning evaluations, with zero-shot
evaluations indicating that simpler texts benefit performance on linguistic
knowledge tasks, whereas more complex texts favor tasks requiring world
knowledge and entity tracking.

</details>


### [27] [MPCG: Multi-Round Persona-Conditioned Generation for Modeling the Evolution of Misinformation with LLMs](https://arxiv.org/abs/2509.16564)
*Jun Rong Brian Chong,Yixuan Tang,Anthony K. H. Tung*

Main category: cs.CL

TL;DR: MPCG是一个多轮次、人物角色驱动的框架，用于模拟错误信息在不同意识形态群体间的传播演化过程，通过LLM生成角色特定的错误信息变体，研究其动态演变特性。


<details>
  <summary>Details</summary>
Motivation: 当前错误信息检测方法假设错误信息是静态的，但实际上错误信息在传播过程中会不断演变以适应不同受众。需要开发能够模拟这种动态演变的框架。

Method: 使用未经审查的大语言模型在多轮次中生成角色特定的错误信息变体，每轮生成都基于前一轮的输出，通过人类和LLM标注、认知努力指标、情感唤起指标等多种方法进行评估。

Result: 生成的信息比原始信息需要更多认知努力，保持角色一致的情感道德框架，语义漂移但主题连贯，可行性达77%。常用错误信息检测器的性能下降高达49.7%。

Conclusion: MPCG框架有效模拟了错误信息的动态演变，揭示了现有检测方法的局限性，为开发更鲁棒的检测系统提供了重要工具和洞察。

Abstract: Misinformation evolves as it spreads, shifting in language, framing, and
moral emphasis to adapt to new audiences. However, current misinformation
detection approaches implicitly assume that misinformation is static. We
introduce MPCG, a multi-round, persona-conditioned framework that simulates how
claims are iteratively reinterpreted by agents with distinct ideological
perspectives. Our approach uses an uncensored large language model (LLM) to
generate persona-specific claims across multiple rounds, conditioning each
generation on outputs from the previous round, enabling the study of
misinformation evolution. We evaluate the generated claims through human and
LLM-based annotations, cognitive effort metrics (readability, perplexity),
emotion evocation metrics (sentiment analysis, morality), clustering,
feasibility, and downstream classification. Results show strong agreement
between human and GPT-4o-mini annotations, with higher divergence in fluency
judgments. Generated claims require greater cognitive effort than the original
claims and consistently reflect persona-aligned emotional and moral framing.
Clustering and cosine similarity analyses confirm semantic drift across rounds
while preserving topical coherence. Feasibility results show a 77% feasibility
rate, confirming suitability for downstream tasks. Classification results
reveal that commonly used misinformation detectors experience macro-F1
performance drops of up to 49.7%. The code is available at
https://github.com/bcjr1997/MPCG

</details>


### [28] [From Scores to Steps: Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations](https://arxiv.org/abs/2509.16584)
*Benlu Wang,Iris Xia,Yifan Zhang,Junda Wang,Feiyun Ouyang,Shuo Han,Arman Cohan,Hong Yu,Zonghai Yao*

Main category: cs.CL

TL;DR: 本文重新评估医学计算能力，提出细粒度评估框架和模块化代理管道MedRaC，显著提升LLMs在医学计算任务上的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有医学基准测试对LLMs的医学计算能力评估不足，仅关注最终答案而忽略系统性推理错误，可能导致严重的临床误判。

Method: 1) 清理重构MedCalc-Bench数据集，提出分步评估管道；2) 引入自动错误分析框架；3) 提出结合检索增强生成和Python代码执行的模块化代理管道MedRaC。

Result: 在细粒度评估下，GPT-4o准确率从62.7%降至43.6%；MedRaC无需微调即可将不同LLMs准确率从16.35%提升至53.19%。

Conclusion: 本研究揭示了当前基准测试的局限性，提出了更具临床可信度的方法论，使基于LLM的系统更适用于真实医疗应用。

Abstract: Large language models (LLMs) have demonstrated promising performance on
medical benchmarks; however, their ability to perform medical calculations, a
crucial aspect of clinical decision-making, remains underexplored and poorly
evaluated. Existing benchmarks often assess only the final answer with a wide
numerical tolerance, overlooking systematic reasoning failures and potentially
causing serious clinical misjudgments. In this work, we revisit medical
calculation evaluation with a stronger focus on clinical trustworthiness.
First, we clean and restructure the MedCalc-Bench dataset and propose a new
step-by-step evaluation pipeline that independently assesses formula selection,
entity extraction, and arithmetic computation. Under this granular framework,
the accuracy of GPT-4o drops from 62.7% to 43.6%, revealing errors masked by
prior evaluations. Second, we introduce an automatic error analysis framework
that generates structured attribution for each failure mode. Human evaluation
confirms its alignment with expert judgment, enabling scalable and explainable
diagnostics. Finally, we propose a modular agentic pipeline, MedRaC, that
combines retrieval-augmented generation and Python-based code execution.
Without any fine-tuning, MedRaC improves the accuracy of different LLMs from
16.35% up to 53.19%. Our work highlights the limitations of current benchmark
practices and proposes a more clinically faithful methodology. By enabling
transparent and transferable reasoning evaluation, we move closer to making
LLM-based systems trustworthy for real-world medical applications.

</details>


### [29] [Benchmarking Contextual and Paralinguistic Reasoning in Speech-LLMs: A Case Study with In-the-Wild Data](https://arxiv.org/abs/2509.16589)
*Qiongqiong Wang,Hardik Bhupendra Sailor,Tianchi Liu,Wenyu Zhang,Muhammad Huzaifah,Nattadaporn Lertcheva,Shuo Sun,Nancy F. Chen,Jinyang Wu,AiTi Aw*

Main category: cs.CL

TL;DR: CP-Bench是一个用于评估语音大语言模型在上下文副语言推理能力的新基准，重点关注语言内容与非语言线索（如情感和韵律）的整合。


<details>
  <summary>Details</summary>
Motivation: 现有的语音大语言模型在转录和翻译任务上表现出色，但在理解副语言方面存在局限，而副语言对于社交和情感智能至关重要。

Method: 创建了两个精心策划的问答数据集，需要语言和共情理解能力，评估了开源和闭源的最先进语音大语言模型，并对不同问题类型进行了全面分析。

Result: 对表现最好的两个模型进行了温度调优分析，揭示了现有评估中的关键差距。

Conclusion: 该基准为构建更具上下文感知和情感智能的语音大语言模型提供了重要见解。

Abstract: Recent speech-LLMs have shown impressive performance in tasks like
transcription and translation, yet they remain limited in understanding the
paralinguistic aspects of speech crucial for social and emotional intelligence.
We propose CP-Bench, a benchmark for evaluating speech-LLMs on contextual
paralinguistic reasoning the integration of verbal content with non-verbal cues
like emotion and prosody. The benchmark includes two curated question answering
(QA) datasets requiring both linguistic and empathetic understanding. We
evaluate state-of-the-art speech-LLMs from both open and closed-source models
and perform a comprehensive analysis across different question types. The top
two models were further analyzed under temperature tuning to understand its
effect on this task. Our benchmark reveals a key gap in existing evaluations
and offers insights into building more context-aware and emotionally
intelligent speech-capable LLMs.

</details>


### [30] [From Uniform to Heterogeneous: Tailoring Policy Optimization to Every Token's Nature](https://arxiv.org/abs/2509.16591)
*Zheng Liu,Mengjie Liu,Siwei Wen,Mengzhang Cai,Bin Cui,Conghui He,Wentao Zhang*

Main category: cs.CL

TL;DR: HAPO是一种针对LLM推理优化的token感知强化学习算法，通过动态调整不同token的优化策略来解决现有方法对所有token统一处理的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习算法对所有token采用统一优化，忽略了它们在推理过程中的不同角色和作用。

Method: 提出了异构自适应策略优化（HAPO），包括自适应温度采样、token级别分组平均优势计算、差分优势重分配和不对称自适应剪裁等技术。

Result: 在多个模型规模上的实验表明，HAPO一致优于DAPO算法。

Conclusion: 通过将token级别的处理嵌入到每个训练阶段，HAPO实现了细粒度的优化控制，显著提升了LLM的推理能力。

Abstract: Reinforcement Learning has emerged as the fundamental technique for enhancing
reasoning in LLMs. However, existing algorithms apply uniform optimization to
all tokens, ignoring their different roles in reasoning process. To address
this limitation, we introduce Heterogeneous Adaptive Policy Optimization
(HAPO), a comprehensive token-aware algorithm that dynamically adapts
optimization based on token entropy. For rollout sampling, we propose Adaptive
Temperature Sampling, which adjusts sampling temperature in real time,
promoting exploration at high-entropy tokens while preserving coherence at
low-entropy ones. For advantage calculation, we introduce Token Level Group
Average that normalizes advantages at token level, jointly accounting for
sequence-length as in token-mean loss while preserving non-biased treatment. We
then develop Differential Advantage Redistribution that leverages entropy and
importance ratios to modulate rewards-adjusting updates for tokens with clear
signals. For clipping loss, we design Asymmetric Adaptive Clipping, allowing
aggressive probability reduction for noisy low-entropy tokens while enabling
exploration for high-entropy tokens. Through systematic investigation between
entropy and training dynamics, we embedded token-level treatment into every
stages to achieve fine-grained control. Extensive experiments demonstrate that
HAPO consistently outperforms DAPO across multiple model scales. Our code can
be found in https://github.com/starriver030515/HAPO.

</details>


### [31] [Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels](https://arxiv.org/abs/2509.16596)
*Junjie Ye,Yuming Yang,Yang Nan,Shuo Li,Qi Zhang,Tao Gui,Xuanjing Huang,Peng Wang,Zhongchao Shi,Jianping Fan*

Main category: cs.CL

TL;DR: 本文研究发现，监督微调（SFT）对大型语言模型的知识获取存在负面影响，使用更多样本进行微调反而导致闭卷问答性能下降，最多可达14%。参数分析显示90%的参数更新对知识增强无贡献。


<details>
  <summary>Details</summary>
Motivation: 探索监督微调（SFT）对大型语言模型知识的影响，目前这一领域研究不足，限制了我们对微调过程中知识变化行为的控制能力。

Method: 评估了LLaMA-2和LLaMA-3系列五个模型的闭卷问答性能，分析不同样本量（240 vs 1920）和不同知识掌握程度的微调数据对性能的影响，并在标记和参数层面进行行为分析。

Result: 使用1920个样本微调的模型比使用240个样本微调的模型性能差14%；微调数据中知识掌握程度的变化导致性能波动超过12%；90%的参数更新对知识增强无贡献，恢复这些更新可改善性能。

Conclusion: 研究结果为开发更有效的微调策略提供了实用指导，有助于更好地增强模型知识。

Abstract: Large language models (LLMs) acquire substantial world knowledge during
pre-training, which is further shaped by post-training techniques such as
supervised fine-tuning (SFT). However, the impact of SFT on a model's knowledge
remains underexplored, limiting our ability to control knowledge change
behavior in fine-tuned models. To address this gap, we evaluate closed-book
question answering (CBQA) performance across five LLMs from the LLaMA-2 and
LLaMA-3 families. Surprisingly, models fine-tuned on 1,920 samples perform up
to 14% worse than those fine-tuned on only 240 samples. Furthermore, varying
the level of knowledge mastery in the fine-tuning data leads to performance
fluctuations of over 12%. To investigate these effects, we analyze model
behavior at both the token and parameter levels. Our analysis reveals that up
to 90% of parameter updates during SFT do not contribute to knowledge
enhancement. Restoring these updates can improve performance on the CBQA task,
depending on the characteristics of the fine-tuning data. These insights offer
practical guidance for developing fine-tuning strategies that more effectively
strengthen model knowledge.

</details>


### [32] [MCP: A Control-Theoretic Orchestration Framework for Synergistic Efficiency and Interpretability in Multimodal Large Language Models](https://arxiv.org/abs/2509.16597)
*Luyan Zhang*

Main category: cs.CL

TL;DR: 本研究提出基于模型-控制器-任务适应（MCP）的三层协作框架，解决大模型在多轮推理和多模态协作中计算效率低和可解释性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 针对大模型在复杂任务（如多轮推理、多模态协作）中面临的计算效率低下和可解释性不足的问题。

Method: 通过将大模型功能解耦为推理、生成和检索模块，结合强化学习驱动的动态路由算法和任务适应机制，首次实现控制理论与大模型动态推理的系统集成。

Result: 实验表明MCP框架在跨模态基准任务（如GLUE、COCO、ScienceQA等）上比基线模型性能提升15-30%，推理效率提高40%，通过Presenter层生成可解释中间结果，获得90%的人工可解释性评分。

Conclusion: 为解决大模型实际应用瓶颈提供了全新的技术路径。

Abstract: Aiming at the problems of computational inefficiency and insufficient
interpretability faced by large models in complex tasks such as multi-round
reasoning and multi-modal collaboration, this study proposes a three-layer
collaboration framework based on model-controller-task adaptation (MCP). By
decoupling large model functions into reasoning, generation and retrieval
modules, and combining reinforcement learning-driven dynamic routing algorithms
and task adaptation mechanisms, the systematic integration of control theory
and large model dynamic reasoning is achieved for the first time. Experiments
show that the MCP framework improves the performance of cross-modal
benchmarking tasks, such as GLUE, COCO, ScienceQA, etc., by 15-30% compared
with the baseline model, improves the reasoning efficiency by 40%, and
generates the interpretable intermediate results through the Presenter layer,
obtaining 90% of the manual interpretability scores, which provides a brand-new
technological path to solve the bottleneck of the practical application of the
large model.

</details>


### [33] [PruneCD: Contrasting Pruned Self Model to Improve Decoding Factuality](https://arxiv.org/abs/2509.16598)
*Byeongho Yu,Changhun Lee,Jungyu Jin,Eunhyeok Park*

Main category: cs.CL

TL;DR: PruneCD通过层剪枝而非早期退出来构建业余模型，解决了DoLa方法中早期退出logits平坦、幅度低的问题，从而更有效地进行对比解码以减少LLM幻觉。


<details>
  <summary>Details</summary>
Motivation: DoLa方法利用早期退出logits作为对比先验来缓解大语言模型的幻觉问题，但发现这些早期退出logits往往平坦、幅度低，无法反映有意义的对比。

Method: 提出PruneCD方法，通过层剪枝而非早期退出来构建业余模型，产生更具信息量和更好对齐的logits，实现更有效的对比解码。

Result: 通过定性和定量分析表明，PruneCD在最小推理开销下持续提高事实性。

Conclusion: PruneCD为缓解LLM幻觉提供了一种鲁棒且实用的方法。

Abstract: To mitigate the hallucination problem in large language models, DoLa exploits
early exit logits from the same model as a contrastive prior. However, we found
that these early exit logits tend to be flat, low in magnitude, and fail to
reflect meaningful contrasts. To address this, we propose PruneCD, a novel
contrastive decoding method that constructs the amateur model via layer pruning
rather than early exit. This design leads to more informative and well-aligned
logits, enabling more effective contrastive decoding. Through qualitative and
quantitative analyses, we demonstrate that PruneCD consistently improves
factuality with minimal inference overhead, offering a robust and practical
approach to mitigating hallucinations in LLMs.

</details>


### [34] [Computational-Assisted Systematic Review and Meta-Analysis (CASMA): Effect of a Subclass of GnRH-a on Endometriosis Recurrence](https://arxiv.org/abs/2509.16599)
*Sandro Tsang*

Main category: cs.CL

TL;DR: 本研究评估了一种信息检索驱动的工作流程，旨在提高系统评价的效率、透明度和可重复性。以子宫内膜异位症复发为案例，通过结合PRISMA指南和计算技术，成功筛选出7项RCT研究，发现GnRH激动剂可使复发风险降低36%。


<details>
  <summary>Details</summary>
Motivation: 证据合成是循证医学的基础，但面对海量文献需要信息检索技术支持。本研究旨在开发一个高效、透明、可重复的系统评价工作流程，以解决传统方法效率低下的问题。

Method: 采用混合方法整合PRISMA指南与计算技术：1）应用半自动去重技术筛选记录；2）针对多臂试验使用改进的分割方法处理单位分析错误；3）对GnRH激动剂疗效进行随机效应模型荟萃分析。

Result: 工作流程显著减少了筛选工作量：仅用11天完成812条记录的获取和筛选。最终纳入7项RCT（841名患者），汇总分析显示风险比为0.64（95%CI 0.48-0.86），即子宫内膜异位症复发风险降低36%，异质性不显著。敏感性分析和偏倚评估支持结果的稳健性。

Conclusion: 该研究展示了一个信息检索驱动的医学证据合成工作流程，不仅获得了有价值的临床结果，还为加速系统评价过程提供了框架。该方法在临床研究与计算机科学之间架起了桥梁，可推广到其他复杂的系统评价中。

Abstract: Background: Evidence synthesis facilitates evidence-based medicine. Without
information retrieval techniques, this task is impossible due to the vast and
expanding literature. Objective: Building on prior work, this study evaluates
an information retrieval-driven workflow to enhance the efficiency,
transparency, and reproducibility of systematic reviews. We use endometriosis
recurrence as an ideal case due to its complex and ambiguous literature.
Methods: Our hybrid approach integrates PRISMA guidelines with computational
techniques. We applied semi-automated deduplication to efficiently filter
records before manual screening. This workflow synthesized evidence from
randomised controlled trials on the efficacy of a subclass of
gonadotropin-releasing hormone agonists (GnRH'as). A modified splitting method
addressed unit-of-analysis errors in multi-arm trials. Results: Our workflow
efficiently reduced the screening workload. It took only 11 days to fetch and
filter 812 records. Seven RCTs were eligible, providing evidence from 841
patients in 4 countries. The pooled random-effects model yielded a Risk Ratio
(RR) of 0.64 (95% CI (0.48 to 0.86)), with non-significant heterogeneity
($I^2=0.00\%$, $\tau=0.00$); i.e., a 36% reduction in endometriosis recurrence.
Sensitivity analyses and bias assessments supported the robustness of our
findings. Conclusion: This study demonstrates an information-retrieval-driven
workflow for medical evidence synthesis. Our approach yields valuable clinical
results while providing a framework for accelerating the systematic review
process. It bridges the gap between clinical research and computer science and
can be generalized to other complex systematic reviews.

</details>


### [35] [LLMsPark: A Benchmark for Evaluating Large Language Models in Strategic Gaming Contexts](https://arxiv.org/abs/2509.16610)
*Junhao Chen,Jingbo Sun,Xiang Li,Haidong Xin,Yuhao Xue,Yibin Xu,Hao Zhao*

Main category: cs.CL

TL;DR: LLMsPark是一个基于博弈论的评价平台，用于评估大型语言模型在经典博弈论场景中的决策策略和社会行为，通过多智能体环境探索战略深度。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在多样化任务中不断进步，需要超越单一指标的全面评估。为了充分评估LLM的智能水平，必须考察它们的交互动态和战略行为。

Method: 构建基于博弈论的评估平台，在15个领先的LLM（包括商业和开源模型）之间进行交叉评估，使用排行榜排名和评分机制来衡量模型的推理和战略能力。

Result: 研究揭示了不同模型之间明显的行为模式和性能差异，高分数反映了更强的推理和战略能力。

Conclusion: 这项工作为评估LLM的战略智能提供了新颖视角，丰富了现有基准测试，并拓宽了在交互式博弈论场景中的评估范围。

Abstract: As large language models (LLMs) advance across diverse tasks, the need for
comprehensive evaluation beyond single metrics becomes increasingly important.
To fully assess LLM intelligence, it is crucial to examine their interactive
dynamics and strategic behaviors. We present LLMsPark, a game theory-based
evaluation platform that measures LLMs' decision-making strategies and social
behaviors in classic game-theoretic settings, providing a multi-agent
environment to explore strategic depth. Our system cross-evaluates 15 leading
LLMs (both commercial and open-source) using leaderboard rankings and scoring
mechanisms. Higher scores reflect stronger reasoning and strategic
capabilities, revealing distinct behavioral patterns and performance
differences across models. This work introduces a novel perspective for
evaluating LLMs' strategic intelligence, enriching existing benchmarks and
broadening their assessment in interactive, game-theoretic scenarios. The
benchmark and rankings are publicly available at https://llmsparks.github.io/.

</details>


### [36] [Redefining Experts: Interpretable Decomposition of Language Models for Toxicity Mitigation](https://arxiv.org/abs/2509.16660)
*Zuhair Hasan Shaik,Abdullah Mazhar,Aseem Srivastava,Md Shad Akhtar*

Main category: cs.CL

TL;DR: 该论文提出了一种新的毒性内容抑制方法EigenShift，通过语言模型输出层的特征分解来精确抑制毒性生成，同时保持语言能力


<details>
  <summary>Details</summary>
Motivation: 现有基于神经元激活操作的毒性抑制方法存在不稳定、上下文依赖性强且会损害模型核心语言能力的问题

Method: 提出EigenShift方法，基于语言模型最终输出层的特征分解，选择性针对生成对齐的组件进行干预

Result: 在Jigsaw和ToxiCN数据集上的实验表明，层级聚合特征比单个神经元提供更稳健的信号，新方法能有效抑制毒性内容而不损害语言能力

Conclusion: EigenShift方法无需额外训练或微调，计算成本低，基于严格理论分析，为毒性内容抑制提供了更精确和稳健的解决方案

Abstract: Large Language Models have demonstrated impressive fluency across diverse
tasks, yet their tendency to produce toxic content remains a critical challenge
for AI safety and public trust. Existing toxicity mitigation approaches
primarily manipulate individual neuron activations, but these methods suffer
from instability, context dependence, and often compromise the model's core
language abilities. To address these shortcomings, we investigate three key
questions: the stability of neuron-level toxicity indicators, the advantages of
structural (layer-wise) representations, and the interpretability of mechanisms
driving toxic generation. Through extensive experiments on Jigsaw and ToxiCN
datasets, we show that aggregated layer-wise features provide more robust
signals than single neurons. Moreover, we observe conceptual limitations in
prior works that conflate toxicity detection experts and generation experts
within neuron-based interventions. To mitigate this, we propose a novel
principled intervention technique, EigenShift, based on eigen-decomposition of
the language model's final output layer. This method selectively targets
generation-aligned components, enabling precise toxicity suppression without
impairing linguistic competence. Our method requires no additional training or
fine-tuning, incurs minimal computational cost, and is grounded in rigorous
theoretical analysis.

</details>


### [37] [Robust Native Language Identification through Agentic Decomposition](https://arxiv.org/abs/2509.16666)
*Ahmet Yavuz Uluslu,Tannon Kew,Tilia Ellendorff,Gerold Schneider,Rico Sennrich*

Main category: cs.CL

TL;DR: 本文提出了一种基于法医语言学启发的代理式母语识别管道，通过专门代理收集和分类多样化语言证据，再由协调代理进行最终评估，显著提升了对抗误导性上下文线索的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在母语识别任务中过度依赖表层上下文线索（如姓名、地点和文化刻板印象），而非真正的语言模式，导致预测容易被误导性提示改变。

Method: 设计了一个代理式NLI管道，包含专门代理收集语言证据和协调代理进行最终综合评估的多阶段框架。

Result: 在两个基准数据集上，该方法相比标准提示方法显著提高了NLI的鲁棒性和性能一致性。

Conclusion: 基于法医语言学的代理式方法能有效解决LLMs在母语识别中过度依赖表层线索的问题，提供了更可靠的识别方案。

Abstract: Large language models (LLMs) often achieve high performance in native
language identification (NLI) benchmarks by leveraging superficial contextual
clues such as names, locations, and cultural stereotypes, rather than the
underlying linguistic patterns indicative of native language (L1) influence. To
improve robustness, previous work has instructed LLMs to disregard such clues.
In this work, we demonstrate that such a strategy is unreliable and model
predictions can be easily altered by misleading hints. To address this problem,
we introduce an agentic NLI pipeline inspired by forensic linguistics, where
specialized agents accumulate and categorize diverse linguistic evidence before
an independent final overall assessment. In this final assessment, a goal-aware
coordinating agent synthesizes all evidence to make the NLI prediction. On two
benchmark datasets, our approach significantly enhances NLI robustness against
misleading contextual clues and performance consistency compared to standard
prompting methods.

</details>


### [38] [Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle](https://arxiv.org/abs/2509.16679)
*Keliang Liu,Dingkang Yang,Ziyun Qian,Weijie Yin,Yuchi Wang,Hongsheng Li,Jun Liu,Peng Zhai,Yang Liu,Lihua Zhang*

Main category: cs.CL

TL;DR: 这篇论文系统综述了强化学习在大型语言模型全生命周期中的应用，特别关注RLVR方法，涵盖预训练、对齐微调和强化推理等阶段，并整理了相关数据集、评估基准和开源工具。


<details>
  <summary>Details</summary>
Motivation: 现有综述对RL增强LLMs的覆盖范围有限，未能全面总结RL在LLM全生命周期中的运作方式，需要系统梳理RL在LLMs中的理论和实践进展。

Method: 采用系统性综述方法，首先介绍RL基础理论，然后详细分析RL在LLM各个阶段的应用策略，包括预训练、对齐微调和强化推理，特别强调强化推理阶段对模型推理能力的推动作用。

Result: 整理了现有的RL微调数据集和评估基准，包括人工标注数据集、AI辅助偏好数据和程序验证式语料库，并回顾了主流开源工具和训练框架。

Conclusion: 分析了RL增强LLMs领域的未来挑战和趋势，旨在为研究者和实践者提供RL与LLMs交叉领域的最新发展和前沿趋势，促进更智能、通用和安全的LLMs发展。

Abstract: In recent years, training methods centered on Reinforcement Learning (RL)
have markedly enhanced the reasoning and alignment performance of Large
Language Models (LLMs), particularly in understanding human intents, following
user instructions, and bolstering inferential strength. Although existing
surveys offer overviews of RL augmented LLMs, their scope is often limited,
failing to provide a comprehensive summary of how RL operates across the full
lifecycle of LLMs. We systematically review the theoretical and practical
advancements whereby RL empowers LLMs, especially Reinforcement Learning with
Verifiable Rewards (RLVR). First, we briefly introduce the basic theory of RL.
Second, we thoroughly detail application strategies for RL across various
phases of the LLM lifecycle, including pre-training, alignment fine-tuning, and
reinforced reasoning. In particular, we emphasize that RL methods in the
reinforced reasoning phase serve as a pivotal driving force for advancing model
reasoning to its limits. Next, we collate existing datasets and evaluation
benchmarks currently used for RL fine-tuning, spanning human-annotated
datasets, AI-assisted preference data, and program-verification-style corpora.
Subsequently, we review the mainstream open-source tools and training
frameworks available, providing clear practical references for subsequent
research. Finally, we analyse the future challenges and trends in the field of
RL-enhanced LLMs. This survey aims to present researchers and practitioners
with the latest developments and frontier trends at the intersection of RL and
LLMs, with the goal of fostering the evolution of LLMs that are more
intelligent, generalizable, and secure.

</details>


### [39] [EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and Efficient LLMs](https://arxiv.org/abs/2509.16686)
*Zhengge Cai,Haowen Hou*

Main category: cs.CL

TL;DR: EG-MLA是一种新型注意力机制，通过嵌入门控机制在潜在空间中进一步压缩KV缓存，在保持性能的同时实现91.6%的缓存减少和59.9%的额外内存节省。


<details>
  <summary>Details</summary>
Motivation: 减少KV缓存大小对于大型语言模型的高效推理至关重要，特别是在延迟和内存约束下。虽然MLA已经实现了显著的KV缓存压缩，但进一步压缩的空间有限。

Method: EG-MLA在MLA基础上引入token特定的嵌入门控机制，在潜在空间中对压缩的KV向量进行细粒度调制，以最小额外计算成本增强表示表达能力。

Result: 相比MHA，EG-MLA实现91.6%的KV缓存减少且性能损失可忽略；相比MLA，在多个推理基准上提升任务准确率，同时实现59.9%的额外内存节省。成功扩展到10亿参数规模。

Conclusion: EG-MLA是一种内存和计算效率高的注意力机制，为现代LLM的可扩展高性能推理提供了实用解决方案。

Abstract: Reducing the key-value (KV) cache size is a crucial step toward enabling
efficient inference in large language models (LLMs), especially under latency
and memory constraints. While Multi-Head Attention (MHA) offers strong
representational power, it incurs significant memory overhead. Recent work on
Multi-head Latent Attention (MLA) mitigates this by compressing KV
representations into a shared latent space, achieving a better trade-off
between performance and cache efficiency. While MLA already achieves
significant KV cache reduction, the scope for further compression remains
limited without performance loss. In this paper, we propose
\textbf{Embedding-Gated Multi-head Latent Attention (EG-MLA)}, a novel
extension of MLA that further reduces KV cache size while enhancing
representational expressiveness. EG-MLA introduces a token-specific embedding
gating mechanism applied in the latent space, enabling fine-grained modulation
of compressed KV vectors with minimal additional computation. Compared to MHA,
EG-MLA achieves over 91.6\% reduction in KV cache size with negligible
performance degradation. Relative to MLA, EG-MLA consistently improves task
accuracy across diverse reasoning benchmarks while achieving up to 59.9\%
additional memory savings. Our theoretical analysis highlights how embedding
gating induces implicit high-order interactions, and empirical evaluations
demonstrate robust generalization across model scales and compression regimes.
Notably, we successfully scale EG-MLA to over 1 billion parameters,
demonstrating its practical viability for large-scale LLM deployment. These
results establish EG-MLA as a memory- and compute-efficient attention mechanism
that enables scalable, high-performance inference in modern LLMs.

</details>


### [40] [Decoding Uncertainty: The Impact of Decoding Strategies for Uncertainty Estimation in Large Language Models](https://arxiv.org/abs/2509.16696)
*Wataru Hashimoto,Hidetaka Kamigaito,Taro Watanabe*

Main category: cs.CL

TL;DR: 本研究探讨了解码策略对大型语言模型不确定性估计的影响，发现Contrastive Search策略在偏好对齐的LLMs中能提供更好的不确定性估计，但在仅进行监督微调而未显式对齐的模型中效果不一。


<details>
  <summary>Details</summary>
Motivation: 解码策略通过操纵语言模型输出的概率分布，会影响生成质量和不确定性。目前缺乏对解码策略如何影响LLM不确定性估计的系统研究。

Method: 通过实验比较不同解码策略（包括Contrastive Search等）在多种LLM上的表现，分析它们对不确定性估计的影响。

Result: Contrastive Search策略在偏好对齐的LLMs中平均能提供更好的不确定性估计，但在仅进行监督微调的模型中效果不稳定。

Conclusion: 解码策略的选择对LLM不确定性估计有显著影响，模型的对齐状态是决定解码策略效果的重要因素。

Abstract: Decoding strategies manipulate the probability distribution underlying the
output of a language model and can therefore affect both generation quality and
its uncertainty. In this study, we investigate the impact of decoding
strategies on uncertainty estimation in Large Language Models (LLMs). Our
experiments show that Contrastive Search, which mitigates repetition, yields
better uncertainty estimates on average across a range of preference-aligned
LLMs. In contrast, the benefits of these strategies sometimes diverge when the
model is only post-trained with supervised fine-tuning, i.e. without explicit
alignment.

</details>


### [41] [OPEN-THEATRE: An Open-Source Toolkit for LLM-based Interactive Drama](https://arxiv.org/abs/2509.16713)
*Tianyang Xu,Hongqiu Wu,Weiqi Wu,Hai Zhao*

Main category: cs.CL

TL;DR: Open-Theatre是一个开源工具包，用于体验和定制基于LLM的互动戏剧，解决了该领域缺乏标准化开发环境的问题。


<details>
  <summary>Details</summary>
Motivation: LLM互动戏剧是一个有前景但未被充分探索的领域，由于缺乏设计良好的开发平台，研究人员难以复制、扩展和研究此类系统。

Method: 采用高效的多智能体架构和分层检索记忆系统，增强叙事连贯性和长期行为的真实性，并提供高度可配置的流水线。

Result: 开发了首个开源工具包Open-Theatre，为研究人员提供了完整的互动戏剧开发环境。

Conclusion: Open-Theatre降低了LLM互动戏剧的研究门槛，促进了该领域的发展和应用。

Abstract: LLM-based Interactive Drama introduces a novel dialogue scenario in which the
player immerses into a character and engages in a dramatic story by interacting
with LLM agents. Despite the fact that this emerging area holds significant
promise, it remains largely underexplored due to the lack of a well-designed
playground to develop a complete drama. This makes a significant barrier for
researchers to replicate, extend, and study such systems. Hence, we present
Open-Theatre, the first open-source toolkit for experiencing and customizing
LLM-based interactive drama. It refines prior work with an efficient
multi-agent architecture and a hierarchical retrieval-based memory system,
designed to enhance narrative coherence and realistic long-term behavior in
complex interactions. In addition, we provide a highly configurable pipeline,
making it easy for researchers to develop and optimize new approaches.

</details>


### [42] [Semi-Supervised Synthetic Data Generation with Fine-Grained Relevance Control for Short Video Search Relevance Modeling](https://arxiv.org/abs/2509.16717)
*Haoran Li,Zhiming Su,Junyan Yao,Enwei Zhang,Yang Ji,Yan Chen,Kan Zhou,Chao Feng,Jiao Ran*

Main category: cs.CL

TL;DR: 本文提出了一种半监督合成数据管道，用于生成具有可控相关性标签的中文短视频数据，解决了现有提示方法在领域特定数据分布和细粒度相关性多样性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的合成方法难以捕捉领域特定的数据分布，特别是在数据稀缺领域，且往往忽视细粒度相关性多样性。

Method: 提出半监督合成数据管道，通过两个协同训练的模型生成具有可控相关性标签的领域自适应短视频数据，为代表性不足的中间相关性标签合成样本。

Result: 离线实验显示，使用合成数据训练的嵌入模型优于基于提示或普通监督微调的方法；在线A/B测试中，点击率提升1.45%，强相关性比率提升4.9%，图像用户渗透率提升0.1054%。

Conclusion: 在训练数据中融入更多样化的细粒度相关性级别能增强模型对细微语义差异的敏感性，凸显了细粒度相关性监督在嵌入学习中的价值。

Abstract: Synthetic data is widely adopted in embedding models to ensure diversity in
training data distributions across dimensions such as difficulty, length, and
language. However, existing prompt-based synthesis methods struggle to capture
domain-specific data distributions, particularly in data-scarce domains, and
often overlook fine-grained relevance diversity. In this paper, we present a
Chinese short video dataset with 4-level relevance annotations, filling a
critical resource void. Further, we propose a semi-supervised synthetic data
pipeline where two collaboratively trained models generate domain-adaptive
short video data with controllable relevance labels. Our method enhances
relevance-level diversity by synthesizing samples for underrepresented
intermediate relevance labels, resulting in a more balanced and semantically
rich training data set. Extensive offline experiments show that the embedding
model trained on our synthesized data outperforms those using data generated
based on prompting or vanilla supervised fine-tuning(SFT). Moreover, we
demonstrate that incorporating more diverse fine-grained relevance levels in
training data enhances the model's sensitivity to subtle semantic distinctions,
highlighting the value of fine-grained relevance supervision in embedding
learning. In the search enhanced recommendation pipeline of Douyin's
dual-column scenario, through online A/B testing, the proposed model increased
click-through rate(CTR) by 1.45%, raised the proportion of Strong Relevance
Ratio (SRR) by 4.9%, and improved the Image User Penetration Rate (IUPR) by
0.1054%.

</details>


### [43] [Time to Revist Exact Match](https://arxiv.org/abs/2509.16720)
*Auss Abbood,Zaiqiao Meng,Nigel Collier*

Main category: cs.CL

TL;DR: 本文提出将时序问答视为数值估计任务，引入TempAnswerQA基准和sMAPE、MASE等预测指标，发现传统精确匹配评估方法的局限性，强调时序问答需要专门的评估指标。


<details>
  <summary>Details</summary>
Motivation: 当前时序问答评估主要使用精确匹配方法，无法区分误差大小，难以准确评估模型的时间推理能力。

Method: 构建TempAnswerQA基准数据集，使用sMAPE和MASE等预测指标替代传统的精确匹配评估方法。

Result: 发现误差大小与精确匹配得分之间存在脱节现象，某些模型精确匹配得分低但sMAPE误差小，反之亦然。MASE指标重新排序了模型排名，揭示了模型在时序领域知识理解上的差距。

Conclusion: 时序问答任务需要专门的评估指标，sMAPE和MASE能更好地衡量模型的时间推理性能，特别是在处理±1误差时比精确匹配更有效。

Abstract: Temporal question answering is an established method for evaluating temporal
reasoning in large language models. Expected answers are often numeric (e.g.,
dates or durations), yet model responses are evaluated like regular text with
exact match (EM), unable to distinguish small from large errors. In this
investigative work, we frame temporal question answering as a numerical
estimation task to assess the shortcomings of EM. We introduce TempAnswerQA, a
benchmark distilled from Test of Time and TempTabQA, where all questions
require a numerical, temporal answer, allowing us to evaluate models beyond EM.
We use the forecasting metrics symmetric mean absolute percentage error (sMAPE)
and mean absolute scaled error (MASE). With sMAPE, we find that error size and
EM are decoupled. Models with low EM still have low sMAPE (both ~20%), and some
models have high sMAPE despite high EM. Scaling errors by the deviation of the
ground truth data with MASE reshuffles model rankings compared to EM, revealing
gaps in models' understanding of temporal domain knowledge, especially when
trained with synthetic data. Lastly, the models' most frequent error is to
deviate by only $\pm1$ from the ground truth. sMAPE and MASE, unlike EM,
adequately weight these errors. Our findings underscore the need for
specialised metrics for temporal QA tasks. Code and data are available on
https://github.com/aauss/temporal-answer-qa.

</details>


### [44] [A Multi-Level Benchmark for Causal Language Understanding in Social Media Discourse](https://arxiv.org/abs/2509.16722)
*Xiaohan Ding,Kaike Ping,Buse Çarık,Eugenia Rho*

Main category: cs.CL

TL;DR: CausalTalk是一个多层次的因果语言数据集，包含2020-2024年Reddit上关于COVID-19公共卫生的讨论帖子，标注了四种因果任务，填补了非正式社交媒体文本中因果推理研究的空白。


<details>
  <summary>Details</summary>
Motivation: 现有数据集主要关注结构化文本中的显式因果关系，对非正式社交媒体中的隐式因果表达检测支持有限。

Method: 收集五年Reddit帖子，其中10120个帖子由领域专家标注黄金标准标签，GPT-4o生成银标准标签并由人工验证，涵盖二元因果分类、显隐式因果区分、因果跨度提取和因果要点生成四个任务。

Result: 创建了CausalTalk数据集，支持细粒度因果检测和基于要点的推理，为社交媒体语境下的因果推理研究提供了丰富资源。

Conclusion: CausalTalk填补了非正式文本因果推理的空白，支持判别式和生成式模型的基准测试，促进了社交媒体中因果语言理解的研究。

Abstract: Understanding causal language in informal discourse is a core yet
underexplored challenge in NLP. Existing datasets largely focus on explicit
causality in structured text, providing limited support for detecting implicit
causal expressions, particularly those found in informal, user-generated social
media posts. We introduce CausalTalk, a multi-level dataset of five years of
Reddit posts (2020-2024) discussing public health related to the COVID-19
pandemic, among which 10120 posts are annotated across four causal tasks: (1)
binary causal classification, (2) explicit vs. implicit causality, (3)
cause-effect span extraction, and (4) causal gist generation. Annotations
comprise both gold-standard labels created by domain experts and
silver-standard labels generated by GPT-4o and verified by human annotators.
CausalTalk bridges fine-grained causal detection and gist-based reasoning over
informal text. It enables benchmarking across both discriminative and
generative models, and provides a rich resource for studying causal reasoning
in social media contexts.

</details>


### [45] [Angular Dispersion Accelerates $k$-Nearest Neighbors Machine Translation](https://arxiv.org/abs/2509.16729)
*Evgeniia Tokarchuk,Sergey Troshin,Vlad Niculae*

Main category: cs.CL

TL;DR: 本文提出通过增强神经网络隐藏表示的角分散性来加速k-NN机器翻译的检索过程，同时略微提升翻译质量。


<details>
  <summary>Details</summary>
Motivation: k-NN机器翻译虽然能提升翻译性能，但存在高计算成本和内存需求的问题，近似k-NN查找仍然是性能瓶颈。

Method: 通过鼓励神经网络隐藏表示的角度分散性，改善检索数据结构的平衡性，从而加速检索过程。

Result: 改进分散性可以加速检索，同时略微改善翻译质量。

Conclusion: 通过优化隐藏表示的分散性，可以在不减少数据存储规模或查找次数的情况下，有效提升k-NN机器翻译的效率。

Abstract: Augmenting neural machine translation with external memory at decoding time,
in the form of k-nearest neighbors machine translation ($k$-NN MT), is a
well-established strategy for increasing translation performance. $k$-NN MT
retrieves a set of tokens that occurred in the most similar contexts recorded
in a prepared data store, using hidden state representations of translation
contexts as vector lookup keys. One of the main disadvantages of this method is
the high computational cost and memory requirements. Since an exhaustive search
is not feasible in large data stores, practitioners commonly use approximate
$k$-NN MT lookup, yet even such algorithms are a bottleneck. In contrast to
research directions seeking to accelerate $k$-NN MT by reducing data store size
or the number of lookup calls, we pursue an orthogonal direction based on the
performance properties of approximate $k$-NN MT lookup data structures. In
particular, we propose to encourage angular dispersion of the neural hidden
representations of contexts. We show that improving dispersion leads to better
balance in the retrieval data structures, accelerating retrieval and slightly
improving translations.

</details>


### [46] [The Sound of Syntax: Finetuning and Comprehensive Evaluation of Language Models for Speech Pathology](https://arxiv.org/abs/2509.16765)
*Fagun Patel,Duc Q. Nguyen,Sang T. Truong,Jody Vaynshtok,Sanmi Koyejo,Nick Haber*

Main category: cs.CL

TL;DR: 该论文开发了首个用于评估多模态语言模型在语音语言病理学中应用的综合基准，揭示了当前模型在临床环境中的系统性能差异和局限性。


<details>
  <summary>Details</summary>
Motivation: 美国有超过340万儿童需要语音障碍临床干预，但语音语言病理学家数量严重不足，亟需技术支持来提高工作效率。多模态语言模型在语音语言病理学中的应用潜力尚未充分探索。

Method: 与领域专家合作开发了MLMs在语音语言病理学中的用例分类法，构建了包含5个核心用例、每个用例1000个手动标注数据点的综合基准，并对15个最先进的MLMs进行了评估。

Result: 没有单一模型在所有任务中表现一致最优；发现系统性差异，模型在男性说话者上表现更好；思维链提示在分类任务中可能降低性能；领域特定数据微调可使性能提升30%以上。

Conclusion: 当前MLMs在语音语言病理学应用中既有潜力也有局限性，需要进一步研究和针对性开发。

Abstract: According to the U.S. National Institutes of Health, more than 3.4 million
children experience speech disorders that require clinical intervention. The
number of speech-language pathologists (SLPs) is roughly 20 times fewer than
the number of affected children, highlighting a significant gap in children's
care and a pressing need for technological support that improves the
productivity of SLPs. State-of-the-art multimodal language models (MLMs) show
promise for supporting SLPs, but their use remains underexplored largely due to
a limited understanding of their performance in high-stakes clinical settings.
To address this gap, we collaborate with domain experts to develop a taxonomy
of real-world use cases of MLMs in speech-language pathologies. Building on
this taxonomy, we introduce the first comprehensive benchmark for evaluating
MLM across five core use cases, each containing 1,000 manually annotated data
points. This benchmark includes robustness and sensitivity tests under various
settings, including background noise, speaker gender, and accent. Our
evaluation of 15 state-of-the-art MLMs reveals that no single model
consistently outperforms others across all tasks. Notably, we find systematic
disparities, with models performing better on male speakers, and observe that
chain-of-thought prompting can degrade performance on classification tasks with
large label spaces and narrow decision boundaries. Furthermore, we study
fine-tuning MLMs on domain-specific data, achieving improvements of over 30%
compared to base models. These findings highlight both the potential and
limitations of current MLMs for speech-language pathology applications,
underscoring the need for further research and targeted development.

</details>


### [47] [MoRoVoc: A Large Dataset for Geographical Variation Identification of the Spoken Romanian Language](https://arxiv.org/abs/2509.16781)
*Andrei-Marius Avram,Ema-Ioana Bănescu,Anda-Teodora Robea,Dumitru-Clementin Cercel,Mihaela-Claudia Cercel*

Main category: cs.CL

TL;DR: MoRoVoc是最大的罗马尼亚语方言变异分析数据集，包含93小时音频和88,192个样本，平衡了罗马尼亚和摩尔多瓦的语音数据。论文提出多目标对抗训练框架，通过元学习动态调整对抗系数，优化模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决罗马尼亚语方言变异分析缺乏大规模数据集的问题，同时开发能够区分主要任务但对次要属性保持不变的鲁棒语音模型。

Method: 提出多目标对抗训练框架，将人口统计属性（年龄、性别）作为对抗目标，通过元学习动态调整对抗系数。使用Wav2Vec2模型进行实验验证。

Result: Wav2Vec2-Base在使用性别作为对抗目标时，罗马尼亚语方言识别准确率达到78.21%；Wav2Vec2-Large在使用方言和年龄作为对抗目标时，性别分类准确率达到93.08%。

Conclusion: MoRoVoc数据集填补了罗马尼亚语方言研究的空白，多目标对抗训练框架有效提升了语音模型的性能，为方言变异分析提供了有力工具。

Abstract: This paper introduces MoRoVoc, the largest dataset for analyzing the regional
variation of spoken Romanian. It has more than 93 hours of audio and 88,192
audio samples, balanced between the Romanian language spoken in Romania and the
Republic of Moldova. We further propose a multi-target adversarial training
framework for speech models that incorporates demographic attributes (i.e., age
and gender of the speakers) as adversarial targets, making models
discriminative for primary tasks while remaining invariant to secondary
attributes. The adversarial coefficients are dynamically adjusted via
meta-learning to optimize performance. Our approach yields notable gains:
Wav2Vec2-Base achieves 78.21% accuracy for the variation identification of
spoken Romanian using gender as an adversarial target, while Wav2Vec2-Large
reaches 93.08% accuracy for gender classification when employing both dialect
and age as adversarial objectives.

</details>


### [48] [Domain-Adaptive Pre-Training for Arabic Aspect-Based Sentiment Analysis: A Comparative Study of Domain Adaptation and Fine-Tuning Strategies](https://arxiv.org/abs/2509.16788)
*Salha Alyami,Amani Jamal,Areej Alhothali*

Main category: cs.CL

TL;DR: 本文提出了一种针对阿拉伯语方面情感分析（ABSA）的领域自适应预训练方法，通过比较不同的微调策略来解决阿拉伯语标注数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语方面情感分析面临标注数据稀缺的挑战，现有预训练模型基于事实数据，在领域特定任务中可能存在偏差。目前尚无研究将自适应预训练应用于阿拉伯语上下文模型进行ABSA。

Method: 采用领域自适应预训练方法，研究三种微调策略：特征提取、全微调和基于适配器的方法，使用多个适应语料库和上下文模型进行方面情感分类（ASC）和意见目标表达（OTE）提取。

Result: 领域内自适应预训练带来适度改进，基于适配器的微调在计算效率上表现优异且结果具有竞争力。但错误分析揭示了模型预测和数据集标注的问题，包括情感标注错误、对比标记误解、早期术语的积极性偏差等。

Conclusion: 研究结果表明需要开发语法和语义感知模型（如图卷积网络），以更有效地捕捉长距离关系和复杂的基于方面的意见对齐。

Abstract: Aspect-based sentiment analysis (ABSA) in natural language processing enables
organizations to understand customer opinions on specific product aspects.
While deep learning models are widely used for English ABSA, their application
in Arabic is limited due to the scarcity of labeled data. Researchers have
attempted to tackle this issue by using pre-trained contextualized language
models such as BERT. However, these models are often based on fact-based data,
which can introduce bias in domain-specific tasks like ABSA. To our knowledge,
no studies have applied adaptive pre-training with Arabic contextualized models
for ABSA. This research proposes a novel approach using domain-adaptive
pre-training for aspect-sentiment classification (ASC) and opinion target
expression (OTE) extraction. We examine fine-tuning strategies - feature
extraction, full fine-tuning, and adapter-based methods - to enhance
performance and efficiency, utilizing multiple adaptation corpora and
contextualized models. Our results show that in-domain adaptive pre-training
yields modest improvements. Adapter-based fine-tuning is a computationally
efficient method that achieves competitive results. However, error analyses
reveal issues with model predictions and dataset labeling. In ASC, common
problems include incorrect sentiment labeling, misinterpretation of contrastive
markers, positivity bias for early terms, and challenges with conflicting
opinions and subword tokenization. For OTE, issues involve mislabeling targets,
confusion over syntactic roles, difficulty with multi-word expressions, and
reliance on shallow heuristics. These findings underscore the need for syntax-
and semantics-aware models, such as graph convolutional networks, to more
effectively capture long-distance relations and complex aspect-based opinion
alignments.

</details>


### [49] [KuBERT: Central Kurdish BERT Model and Its Application for Sentiment Analysis](https://arxiv.org/abs/2509.16804)
*Kozhin muhealddin Awlla,Hadi Veisi,Abdulhady Abas Abdullah*

Main category: cs.CL

TL;DR: 本文通过将BERT集成到自然语言处理技术中，增强了中央库尔德语情感分析的研究。


<details>
  <summary>Details</summary>
Motivation: 库尔德语是一种低资源语言，具有高度的语言多样性但计算资源有限，这使得情感分析具有挑战性。传统方法如Word2Vec效果有限，而BERT的出现为改进提供了希望。

Method: 使用BERT模型替代传统的Word2Vec词嵌入方法，利用BERT更好的词嵌入能力来捕捉库尔德语的语义细微差别和上下文复杂性。

Result: BERT模型能够更好地处理库尔德语的语义特征，为低资源语言的情感分析设立了新的基准。

Conclusion: 将BERT应用于库尔德语情感分析是有效的，特别适合处理低资源语言的挑战，为类似语言的研究提供了参考。

Abstract: This paper enhances the study of sentiment analysis for the Central Kurdish
language by integrating the Bidirectional Encoder Representations from
Transformers (BERT) into Natural Language Processing techniques. Kurdish is a
low-resourced language, having a high level of linguistic diversity with
minimal computational resources, making sentiment analysis somewhat
challenging. Earlier, this was done using a traditional word embedding model,
such as Word2Vec, but with the emergence of new language models, specifically
BERT, there is hope for improvements. The better word embedding capabilities of
BERT lend to this study, aiding in the capturing of the nuanced semantic pool
and the contextual intricacies of the language under study, the Kurdish
language, thus setting a new benchmark for sentiment analysis in low-resource
languages.

</details>


### [50] [Cognitive Linguistic Identity Fusion Score (CLIFS): A Scalable Cognition-Informed Approach to Quantifying Identity Fusion from Text](https://arxiv.org/abs/2509.16813)
*Devin R. Wright,Jisun An,Yong-Yeol Ahn*

Main category: cs.CL

TL;DR: 本文介绍了一种新的身份融合量化方法CLIFS，它结合认知语言学和大型语言模型，通过隐式隐喻检测实现自动化评估，在暴力风险评估中表现优异。


<details>
  <summary>Details</summary>
Motivation: 量化身份融合对于理解群体行为至关重要，但传统方法需要受控调查或直接接触，缺乏自动化评估工具。

Method: 提出认知语言学身份融合评分(CLIFS)，整合认知语言学和LLMs，基于隐式隐喻检测实现全自动、可扩展的评估。

Result: CLIFS在基准测试中优于现有自动化方法和人工标注，在暴力风险评估中可将性能提升240%以上。

Conclusion: CLIFS为身份融合研究提供了有效的自动化工具，未来需要更大更多样的数据集来提升泛化能力。

Abstract: Quantifying identity fusion -- the psychological merging of self with another
entity or abstract target (e.g., a religious group, political party, ideology,
value, brand, belief, etc.) -- is vital for understanding a wide range of
group-based human behaviors. We introduce the Cognitive Linguistic Identity
Fusion Score (CLIFS), a novel metric that integrates cognitive linguistics with
large language models (LLMs), which builds on implicit metaphor detection.
Unlike traditional pictorial and verbal scales, which require controlled
surveys or direct field contact, CLIFS delivers fully automated, scalable
assessments while maintaining strong alignment with the established verbal
measure. In benchmarks, CLIFS outperforms both existing automated approaches
and human annotation. As a proof of concept, we apply CLIFS to violence risk
assessment to demonstrate that it can improve violence risk assessment by more
than 240%. Building on our identification of a new NLP task and early success,
we underscore the need to develop larger, more diverse datasets that encompass
additional fusion-target domains and cultural backgrounds to enhance
generalizability and further advance this emerging area. CLIFS models and code
are public at https://github.com/DevinW-sudo/CLIFS.

</details>


### [51] [Semantic-Driven Topic Modeling for Analyzing Creativity in Virtual Brainstorming](https://arxiv.org/abs/2509.16835)
*Melkamu Abay Mersha,Jugal Kalita*

Main category: cs.CL

TL;DR: 提出了一种基于语义的主题建模框架，用于自动分析虚拟头脑风暴会议中的创意，相比传统方法具有更高的主题一致性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 虚拟头脑风暴会议中创意数量庞大且分布不均，手动编码耗时且主观，需要自动化方法来支持群体创造力的评估。

Method: 集成四个模块化组件的语义驱动主题建模框架：基于Transformer的嵌入（Sentence-BERT）、降维（UMAP）、聚类（HDBSCAN）以及主题提取与优化。

Result: 在Zoom头脑风暴会话上的评估显示，该模型平均一致性得分0.687（CV），显著优于LDA、ETM和BERTopic等基准方法。

Conclusion: 该工作凸显了基于嵌入的主题建模在分析协作创意方面的潜力，为研究同步虚拟会议中的创造力提供了高效可扩展的框架。

Abstract: Virtual brainstorming sessions have become a central component of
collaborative problem solving, yet the large volume and uneven distribution of
ideas often make it difficult to extract valuable insights efficiently. Manual
coding of ideas is time-consuming and subjective, underscoring the need for
automated approaches to support the evaluation of group creativity. In this
study, we propose a semantic-driven topic modeling framework that integrates
four modular components: transformer-based embeddings (Sentence-BERT),
dimensionality reduction (UMAP), clustering (HDBSCAN), and topic extraction
with refinement. The framework captures semantic similarity at the sentence
level, enabling the discovery of coherent themes from brainstorming transcripts
while filtering noise and identifying outliers. We evaluate our approach on
structured Zoom brainstorming sessions involving student groups tasked with
improving their university. Results demonstrate that our model achieves higher
topic coherence compared to established methods such as LDA, ETM, and BERTopic,
with an average coherence score of 0.687 (CV), outperforming baselines by a
significant margin. Beyond improved performance, the model provides
interpretable insights into the depth and diversity of topics explored,
supporting both convergent and divergent dimensions of group creativity. This
work highlights the potential of embedding-based topic modeling for analyzing
collaborative ideation and contributes an efficient and scalable framework for
studying creativity in synchronous virtual meetings.

</details>


### [52] [Multi-task Pretraining for Enhancing Interpretable L2 Pronunciation Assessment](https://arxiv.org/abs/2509.16876)
*Jiun-Ting Li,Bi-Cheng Yan,Yi-Cheng Wang,Berlin Chen*

Main category: cs.CL

TL;DR: 该论文提出了一种多任务预训练策略，用于自动发音评估，通过重构输入特征来捕捉长期发音线索，并结合手工特征来改进发音评分和口语能力评估。


<details>
  <summary>Details</summary>
Motivation: 现有的自动发音评估方法主要依赖音素级特征，忽视了超音段发音线索，且缺乏与自动口语评估的整合，限制了整体能力评估的全面性。

Method: 提出多任务预训练策略，随机掩蔽音素级发音特征并基于周围上下文进行重构，同时结合手工特征（如流畅度和重音）通过回归器生成可解释的能力分数。

Result: 在speechocean762数据集上的实验表明，该方法提高了发音评分效果，并增强了与口语能力评估的相关性。

Conclusion: 该方法能够有效捕捉长期发音线索，实现针对性训练和全面的能力评估，为自动发音和口语评估提供了更有效的解决方案。

Abstract: Automatic pronunciation assessment (APA) analyzes second-language (L2)
learners' speech by providing fine-grained pronunciation feedback at various
linguistic levels. Most existing efforts on APA typically adopt segmental-level
features as inputs and predict pronunciation scores at different granularities
via hierarchical (or parallel) pronunciation modeling. This, however,
inevitably causes assessments across linguistic levels (e.g., phone, word, and
utterance) to rely solely on phoneme-level pronunciation features, nearly
sidelining supra-segmental pronunciation cues. To address this limitation, we
introduce multi-task pretraining (MTP) for APA, a simple yet effective strategy
that attempts to capture long-term temporal pronunciation cues while
strengthening the intrinsic structures within an utterance via the objective of
reconstructing input features. Specifically, for a phoneme-level encoder of an
APA model, the proposed MTP strategy randomly masks segmental-level
pronunciation features and reconstructs the masked ones based on their
surrounding pronunciation context. Furthermore, current APA systems lack
integration with automated speaking assessment (ASA), limiting holistic
proficiency evaluation. Drawing on empirical studies and prior knowledge in
ASA, our framework bridges this gap by incorporating handcrafted features
(HCFs), such as fluency (speech rate, silence duration) and stress (pitch
accent strength), derived from human-designed formulas via regressors to
generate interpretable proficiency scores. Experiments on speechocean762 show
improved pronunciation scoring and ASA proficiency correlation, enabling
targeted training and comprehensive proficiency assessment.

</details>


### [53] [Can GRPO Boost Complex Multimodal Table Understanding?](https://arxiv.org/abs/2509.16889)
*Xiaoqiang Kang,Shengen Wu,Zimu Wang,Yilin Liu,Xiaobo Jin,Kaizhu Huang,Wei Wang,Yutao Yue,Xiaowei Huang,Qiufeng Wang*

Main category: cs.CL

TL;DR: Table-R1是一个三阶段强化学习框架，通过预热、感知对齐GRPO和提示完成GRPO来提升多模态表格理解能力，在多个数据集上显著超越SFT和GRPO方法。


<details>
  <summary>Details</summary>
Motivation: 现有表格理解方法面临复杂表格结构和逻辑推理的挑战，传统强化学习方法在表格场景下存在初始策略准确率低和奖励稀疏的问题。

Method: 三阶段RL框架：1) 预热阶段提升初始感知和推理能力；2) PA-GRPO使用连续TEDS奖励识别表格结构和内容；3) HC-GRPO利用基于提示问题的残差步骤细粒度奖励。

Result: Table-R1在held-in和held-out数据集上显著提升表格推理性能，Qwen2-VL-7B模型甚至超越更大的专用表格理解模型，在held-in数据集上达到与GPT-4o相当的性能。

Conclusion: Table-R1通过克服初始化瓶颈和奖励稀疏问题，有效推进了鲁棒的多模态表格理解，证明了各阶段设计的有效性。

Abstract: Existing table understanding methods face challenges due to complex table
structures and intricate logical reasoning. While supervised finetuning (SFT)
dominates existing research, reinforcement learning (RL), such as Group
Relative Policy Optimization (GRPO), has shown promise but struggled with low
initial policy accuracy and coarse rewards in tabular contexts. In this paper,
we introduce Table-R1, a three-stage RL framework that enhances multimodal
table understanding through: (1) Warm-up that prompts initial perception and
reasoning capabilities, (2) Perception Alignment GRPO (PA-GRPO), which employs
continuous Tree-Edit-Distance Similarity (TEDS) rewards for recognizing table
structures and contents, and (3) Hint-Completion GRPO (HC-GRPO), which utilizes
fine-grained rewards of residual steps based on the hint-guided question.
Extensive experiments demonstrate that Table-R1 can boost the model's table
reasoning performance obviously on both held-in and held-out datasets,
outperforming SFT and GRPO largely. Notably, Qwen2-VL-7B with Table-R1
surpasses larger specific table understanding models (e.g., Table-LLaVA 13B),
even achieving comparable performance to the closed-source model GPT-4o on
held-in datasets, demonstrating the efficacy of each stage of Table-R1 in
overcoming initialization bottlenecks and reward sparsity, thereby advancing
robust multimodal table understanding.

</details>


### [54] [CLaC at DISRPT 2025: Hierarchical Adapters for Cross-Framework Multi-lingual Discourse Relation Classification](https://arxiv.org/abs/2509.16903)
*Nawar Turk,Daniele Comitogianni,Leila Kosseim*

Main category: cs.CL

TL;DR: 本文介绍了DISRPT 2025共享任务3的提交，该任务涉及跨39个语料库、16种语言和6种话语框架的17个话语关系标签的统一分类。作者比较了多语言BERT模型微调、基于提示的大语言模型以及新提出的HiDAC模型的表现。


<details>
  <summary>Details</summary>
Motivation: 解决跨语言和跨框架的话语关系分类挑战，建立统一的评估基准，并探索参数高效的模型方法。

Method: 1) 微调多语言BERT模型（mBERT、XLM-RoBERTa）并测试不同参数解冻策略；2) 评估Claude Opus 4.0在零样本和少样本设置下的表现；3) 提出HiDAC（分层双适配器对比学习模型）。

Result: 较大的Transformer模型准确率更高但提升有限；解冻编码器顶部75%层可获得与完全微调相当的性能；基于提示的模型表现显著落后；HiDAC达到最高准确率67.5%且参数效率更高。

Conclusion: 虽然大模型性能更好，但改进幅度有限；参数解冻策略可有效平衡性能与效率；HiDAC在准确率和参数效率方面表现最佳，为跨语言话语分析提供了有效解决方案。

Abstract: We present our submission to Task 3 (Discourse Relation Classification) of
the DISRPT 2025 shared task. Task 3 introduces a unified set of 17 discourse
relation labels across 39 corpora in 16 languages and six discourse frameworks,
posing significant multilingual and cross-formalism challenges. We first
benchmark the task by fine-tuning multilingual BERT-based models (mBERT,
XLM-RoBERTa-Base, and XLM-RoBERTa-Large) with two argument-ordering strategies
and progressive unfreezing ratios to establish strong baselines. We then
evaluate prompt-based large language models (namely Claude Opus 4.0) in
zero-shot and few-shot settings to understand how LLMs respond to the newly
proposed unified labels. Finally, we introduce HiDAC, a Hierarchical
Dual-Adapter Contrastive learning model. Results show that while larger
transformer models achieve higher accuracy, the improvements are modest, and
that unfreezing the top 75% of encoder layers yields performance comparable to
full fine-tuning while training far fewer parameters. Prompt-based models lag
significantly behind fine-tuned transformers, and HiDAC achieves the highest
overall accuracy (67.5%) while remaining more parameter-efficient than full
fine-tuning.

</details>


### [55] [CUTE: A Multilingual Dataset for Enhancing Cross-Lingual Knowledge Transfer in Low-Resource Languages](https://arxiv.org/abs/2509.16914)
*Wenhao Zhuang,Yuan Sun*

Main category: cs.CL

TL;DR: 构建并开源了CUTE多语言数据集（中文、维吾尔语、藏语、英语），包含25GB的平行和非平行语料，旨在提升大语言模型对低资源语言的处理能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在资源丰富语言上表现出色，但对低资源语言支持不足，主要原因是训练语料稀缺。

Method: 通过机器翻译构建四语言语料库，包含平行和非平行语料，并在构建前通过人工评估验证翻译质量。

Result: CUTE是目前最大的维吾尔语和藏语开源语料库，能有效增强LLM处理低资源语言的能力，并探索了语料平行性在跨语言迁移学习中的作用。

Conclusion: CUTE语料库和相关模型已公开供研究社区使用，为低资源语言处理提供了重要资源。

Abstract: Large Language Models (LLMs) demonstrate exceptional zero-shot capabilities
in various NLP tasks, significantly enhancing user experience and efficiency.
However, this advantage is primarily limited to resource-rich languages. For
the diverse array of low-resource languages, support remains inadequate, with
the scarcity of training corpora considered the primary cause. We construct and
open-source CUTE Chinese, Uyghur, Tibetan,English dataset, consisting of two
25GB sets of four-language corpora (one parallel and one non-parallel),
obtained through machine translation. CUTE encompasses two resource-rich
languages (Chinese and English) and two low-resource languages (Uyghur and
Tibetan). Prior to constructing CUTE, human assessment validates that the
machine translation quality between Chinese-Uyghur and Chinese-Tibetan
approaches that of Chinese-English translation. CUTE represents the largest
open-source corpus for Uyghur and Tibetan languages to date, and we demonstrate
its effectiveness in enhancing LLMs' ability to process low-resource languages
while investigating the role of corpus parallelism in cross-lingual transfer
learning. The CUTE corpus and related models are made publicly available to the
research community.

</details>


### [56] [K-DeCore: Facilitating Knowledge Transfer in Continual Structured Knowledge Reasoning via Knowledge Decoupling](https://arxiv.org/abs/2509.16929)
*Yongrui Chen,Yi Huang,Yunchang Liu,Shenyu Zhang,Junhao He,Tongtong Wu,Guilin Qi,Tianxing Wu*

Main category: cs.CL

TL;DR: 提出K-DeCore框架解决持续结构化知识推理中的泛化问题和参数效率问题，通过知识解耦机制和双视角记忆巩固提升多任务性能


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法在处理异构结构化知识时泛化能力差，且随着任务增加参数增长导致推理效率低下

Method: K-DeCore框架包含知识解耦机制（将推理过程分解为任务特定和任务无关阶段）、双视角记忆巩固机制和结构引导的伪数据合成策略

Result: 在四个基准数据集上的实验表明，K-DeCore在多个指标上优于现有持续学习方法，适用于多种骨干大语言模型

Conclusion: K-DeCore通过固定可调参数数量有效解决了CSKR任务中的挑战，为持续结构化知识推理提供了高效解决方案

Abstract: Continual Structured Knowledge Reasoning (CSKR) focuses on training models to
handle sequential tasks, where each task involves translating natural language
questions into structured queries grounded in structured knowledge. Existing
general continual learning approaches face significant challenges when applied
to this task, including poor generalization to heterogeneous structured
knowledge and inefficient reasoning due to parameter growth as tasks increase.
To address these limitations, we propose a novel CSKR framework,
\textsc{K-DeCore}, which operates with a fixed number of tunable parameters.
Unlike prior methods, \textsc{K-DeCore} introduces a knowledge decoupling
mechanism that disentangles the reasoning process into task-specific and
task-agnostic stages, effectively bridging the gaps across diverse tasks.
Building on this foundation, \textsc{K-DeCore} integrates a dual-perspective
memory consolidation mechanism for distinct stages and introduces a
structure-guided pseudo-data synthesis strategy to further enhance the model's
generalization capabilities. Extensive experiments on four benchmark datasets
demonstrate the superiority of \textsc{K-DeCore} over existing continual
learning methods across multiple metrics, leveraging various backbone large
language models.

</details>


### [57] [AirQA: A Comprehensive QA Dataset for AI Research with Instance-Level Evaluation](https://arxiv.org/abs/2509.16952)
*Tiancheng Huang,Ruisheng Cao,Yuxin Zhang,Zhangyi Kang,Zijian Wang,Chenrun Wang,Yijie Luo,Hang Zheng,Lirong Qian,Lu Chen,Kai Yu*

Main category: cs.CL

TL;DR: AirQA是一个全面的人工智能领域论文问答数据集，包含13,948篇论文和1,246个问题，支持多任务、多模态和实例级评估。ExTrActor是一个自动化指令数据合成框架，通过三个LLM代理实现无人工干预的示例生成和轨迹收集。


<details>
  <summary>Details</summary>
Motivation: 学术论文数量激增使得研究人员难以高效提取关键信息，而现有缺乏全面且现实的基准来评估基于LLM的问答代理能力，同时高质量交互轨迹的短缺也阻碍了交互式代理的训练。

Method: 提出AirQA数据集进行多任务评估，并开发ExTrActor框架，使用三个LLM代理自动生成指令数据和收集交互轨迹。

Result: 评估显示大多数模型在AirQA上表现不佳，证明了数据集的质量。ExTrActor能持续提升小模型的多轮工具使用能力，使其达到与更大模型相当的性能。

Conclusion: AirQA为科学论文问答提供了高质量的评估基准，ExTrActor框架有效解决了高质量交互数据短缺问题，显著提升了小模型的多轮交互能力。

Abstract: The growing volume of academic papers has made it increasingly difficult for
researchers to efficiently extract key information. While large language models
(LLMs) based agents are capable of automating question answering (QA) workflows
for scientific papers, there still lacks a comprehensive and realistic
benchmark to evaluate their capabilities. Moreover, training an interactive
agent for this specific task is hindered by the shortage of high-quality
interaction trajectories. In this work, we propose AirQA, a human-annotated
comprehensive paper QA dataset in the field of artificial intelligence (AI),
with 13,948 papers and 1,246 questions, that encompasses multi-task,
multi-modal and instance-level evaluation. Furthermore, we propose ExTrActor,
an automated framework for instruction data synthesis. With three LLM-based
agents, ExTrActor can perform example generation and trajectory collection
without human intervention. Evaluations of multiple open-source and proprietary
models show that most models underperform on AirQA, demonstrating the quality
of our dataset. Extensive experiments confirm that ExTrActor consistently
improves the multi-turn tool-use capability of small models, enabling them to
achieve performance comparable to larger ones.

</details>


### [58] [Preference Distillation via Value based Reinforcement Learning](https://arxiv.org/abs/2509.16965)
*Minchan Kwon,Junwon Ko,Kangil Kim,Junmo Kim*

Main category: cs.CL

TL;DR: 提出了Teacher Value-based Knowledge Distillation (TVKD)方法，通过引入教师模型价值函数的辅助奖励来改进DPO训练小模型的效果


<details>
  <summary>Details</summary>
Motivation: DPO的二元胜负监督对于容量有限的小模型训练不足，现有方法主要模仿教师模型行为而忽视了奖励建模的蒸馏

Method: TVKD引入教师模型价值函数的辅助奖励作为软指导，该奖励满足基于势能的奖励塑造，保持DPO的全局奖励结构和最优策略不变

Result: 实验结果表明TVKD在各种基准测试和模型大小上都能持续提升性能

Conclusion: TVKD可以集成到标准DPO训练框架中，无需额外rollout，有效提升了小模型在人类偏好对齐任务上的表现

Abstract: Direct Preference Optimization (DPO) is a powerful paradigm to align language
models with human preferences using pairwise comparisons. However, its binary
win-or-loss supervision often proves insufficient for training small models
with limited capacity. Prior works attempt to distill information from large
teacher models using behavior cloning or KL divergence. These methods often
focus on mimicking current behavior and overlook distilling reward modeling. To
address this issue, we propose \textit{Teacher Value-based Knowledge
Distillation} (TVKD), which introduces an auxiliary reward from the value
function of the teacher model to provide a soft guide. This auxiliary reward is
formulated to satisfy potential-based reward shaping, ensuring that the global
reward structure and optimal policy of DPO are preserved. TVKD can be
integrated into the standard DPO training framework and does not require
additional rollouts. Our experimental results show that TVKD consistently
improves performance across various benchmarks and model sizes.

</details>


### [59] [Advancing Speech Understanding in Speech-Aware Language Models with GRPO](https://arxiv.org/abs/2509.16990)
*Avishai Elmakies,Hagai Aronowitz,Nimrod Shabtay,Eli Schwartz,Ron Hoory,Avihu Dekel*

Main category: cs.CL

TL;DR: 本文提出了一种基于组相对策略优化（GRPO）的方法，用于在开放格式语音理解任务（如口语问答和自动语音翻译）上训练语音感知大语言模型（SALLMs）。


<details>
  <summary>Details</summary>
Motivation: SALLMs在语音理解任务中表现出色，GRPO因其在训练LLMs中的高效性而受到关注。先前研究主要将GRPO应用于多项选择任务，本文旨在探索其在更能体现模型生成能力的开放格式任务中的应用。

Method: 采用GRPO方法，以BLEU作为奖励信号来优化SALLMs，并探索在GRPO中整合离策略样本的潜力。

Result: 实证研究表明，该方法在多个关键指标上超越了标准的监督微调（SFT）。

Conclusion: 该方法在开放格式语音理解任务中表现出优越性，并为未来改进和研究提供了方向，特别是在离策略样本整合方面。

Abstract: In this paper, we introduce a Group Relative Policy Optimization (GRPO)-based
method for training Speech-Aware Large Language Models (SALLMs) on open-format
speech understanding tasks, such as Spoken Question Answering and Automatic
Speech Translation. SALLMs have proven highly effective for speech
understanding tasks. GRPO has recently gained traction for its efficiency in
training LLMs, and prior work has explored its application to SALLMs, primarily
in multiple-choice tasks. Building on this, we focus on open-format tasks that
better reflect the generative abilities of the models. Our approach leverages
GRPO with BLEU as the reward signal to optimize SALLMs, and we demonstrate
empirically that it surpasses standard SFT across several key metrics. Finally,
we explore the potential of incorporating off-policy samples within GRPO for
these tasks, highlighting avenues for further improvement and further research.

</details>


### [60] [The Transfer Neurons Hypothesis: An Underlying Mechanism for Language Latent Space Transitions in Multilingual LLMs](https://arxiv.org/abs/2509.17030)
*Hinata Tezuka,Naoya Inoue*

Main category: cs.CL

TL;DR: 本文提出了"转移神经元假说"，认为MLP模块中的特定神经元负责在语言特定潜空间和共享语义潜空间之间转换表示，这些转移神经元对于多语言LLM的推理至关重要。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明解码器型LLM的多语言处理框架中，早期层将输入转换为英语中心表示，中间层进行推理，最终层生成输出，但这种转换的内部动态和机制尚未深入探索。

Method: 提出并实证验证转移神经元假说，识别MLP模块中负责语言特定潜空间与共享语义潜空间之间表示转换的神经元，并分析语言特定神经元的功能。

Result: 发现转移神经元确实存在，语言特定神经元的功能之一是促进潜空间之间的移动，转移神经元对多语言LLM的推理能力至关重要。

Conclusion: 转移神经元假说得到验证，揭示了多语言LLM中语言转换的内部机制，为理解多语言处理框架提供了新的视角。

Abstract: Recent studies have suggested a processing framework for multilingual inputs
in decoder-based LLMs: early layers convert inputs into English-centric and
language-agnostic representations; middle layers perform reasoning within an
English-centric latent space; and final layers generate outputs by transforming
these representations back into language-specific latent spaces. However, the
internal dynamics of such transformation and the underlying mechanism remain
underexplored. Towards a deeper understanding of this framework, we propose and
empirically validate The Transfer Neurons Hypothesis: certain neurons in the
MLP module are responsible for transferring representations between
language-specific latent spaces and a shared semantic latent space.
Furthermore, we show that one function of language-specific neurons, as
identified in recent studies, is to facilitate movement between latent spaces.
Finally, we show that transfer neurons are critical for reasoning in
multilingual LLMs.

</details>


### [61] [Modeling Bottom-up Information Quality during Language Processing](https://arxiv.org/abs/2509.17047)
*Cui Ding,Yanning Yin,Lena A. Jäger,Ethan Gotlieb Wilcox*

Main category: cs.CL

TL;DR: 该研究测试了语言处理中自上而下期望和自下而上输入整合的理论预测，通过信息论方法量化视觉信息质量对阅读时间的影响，并在英文和中文中验证了上半部分视觉信息比下半部分包含更多词汇识别信息。


<details>
  <summary>Details</summary>
Motivation: 验证语言处理模型中关于自下而上输入质量影响处理难度的预测，即噪声输入会导致理解困难。

Method: 提出用视觉信息与词汇身份之间的互信息来量化自下而上信息质量，通过遮挡单词上下半部分来降低信息质量，使用多模态语言模型估计互信息，并比较英文和中文阅读时间数据。

Result: 英文和中文中，单词上半部分比下半部分包含更多词汇识别信息，但这种不对称性在英文中更为明显，这一模式在阅读时间数据中得到反映。

Conclusion: 研究证实了自下而上输入质量对阅读处理的影响，并揭示了不同语言系统中视觉信息分布的不对称性差异。

Abstract: Contemporary theories model language processing as integrating both top-down
expectations and bottom-up inputs. One major prediction of such models is that
the quality of the bottom-up inputs modulates ease of processing -- noisy
inputs should lead to difficult and effortful comprehension. We test this
prediction in the domain of reading. First, we propose an information-theoretic
operationalization for the "quality" of bottom-up information as the mutual
information (MI) between visual information and word identity. We formalize
this prediction in a mathematical model of reading as a Bayesian update.
Second, we test our operationalization by comparing participants' reading times
in conditions where words' information quality has been reduced, either by
occluding their top or bottom half, with full words. We collect data in English
and Chinese. We then use multimodal language models to estimate the mutual
information between visual inputs and words. We use these data to estimate the
specific effect of reduced information quality on reading times. Finally, we
compare how information is distributed across visual forms. In English and
Chinese, the upper half contains more information about word identity than the
lower half. However, the asymmetry is more pronounced in English, a pattern
which is reflected in the reading times.

</details>


### [62] [TactfulToM: Do LLMs Have the Theory of Mind Ability to Understand White Lies?](https://arxiv.org/abs/2509.17054)
*Yiwei Liu,Emma Jane Pretty,Jiahao Huang,Saku Sugawara*

Main category: cs.CL

TL;DR: TactfulToM是一个新的英语基准测试，用于评估大语言模型在真实对话中理解善意谎言并推理其背后亲社会动机的能力。该基准通过多阶段人工参与流程生成，结果显示当前最先进的模型表现远低于人类水平。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注大语言模型在心理理论推理任务上的表现，但对于需要更细致社会背景的心理理论能力（如善意谎言）的研究还很有限。

Method: 通过多阶段人工参与流程，让大语言模型将手动设计的种子故事扩展为对话，保持参与者之间的信息不对称以产生真实的善意谎言场景。

Result: TactfulToM对当前最先进的模型具有挑战性，它们的表现远低于人类水平，显示出在理解支持真正理解善意谎言的心理理论推理能力方面存在不足。

Conclusion: 该研究揭示了现有大语言模型在理解善意谎言等需要复杂社会推理的心理理论任务方面的局限性，为未来改进模型的社会认知能力提供了重要基准。

Abstract: While recent studies explore Large Language Models' (LLMs) performance on
Theory of Mind (ToM) reasoning tasks, research on ToM abilities that require
more nuanced social context is limited, such as white lies. We introduce
TactfulToM, a novel English benchmark designed to evaluate LLMs' ability to
understand white lies within real-life conversations and reason about prosocial
motivations behind them, particularly when they are used to spare others'
feelings and maintain social harmony. Our benchmark is generated through a
multi-stage human-in-the-loop pipeline where LLMs expand manually designed seed
stories into conversations to maintain the information asymmetry between
participants necessary for authentic white lies. We show that TactfulToM is
challenging for state-of-the-art models, which perform substantially below
humans, revealing shortcomings in their ability to fully comprehend the ToM
reasoning that enables true understanding of white lies.

</details>


### [63] [SFT-TA: Supervised Fine-Tuned Agents in Multi-Agent LLMs for Automated Inductive Thematic Analysis](https://arxiv.org/abs/2509.17167)
*Seungjun Yi,Joakim Nguyen,Huimin Xu,Terence Lim,Joseph Skrovan,Mehak Beri,Hitakshi Modi,Andrew Well,Liu Leqi,Mia Markey,Ying Ding*

Main category: cs.CL

TL;DR: SFT-TA是一个基于监督微调代理的多代理系统框架，用于自动化主题分析，相比现有方法和GPT-4o基线，在与人参考主题对齐方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 手动主题分析耗时且难以扩展，现有LLM自动化方法与人结果对齐有限，需要改进自动化主题分析的准确性和可扩展性。

Method: 提出SFT-TA框架，将监督微调(SFT)代理嵌入多代理系统中，通过特定角色分工来提升主题分析效果。

Result: SFT-TA框架在与人参考主题对齐方面优于现有框架和GPT-4o基线，单独SFT代理可能表现不佳，但在多代理系统中效果更好。

Conclusion: 将SFT代理嵌入多代理系统的特定角色中是提升主题分析输出对齐性的有前景途径。

Abstract: Thematic Analysis (TA) is a widely used qualitative method that provides a
structured yet flexible framework for identifying and reporting patterns in
clinical interview transcripts. However, manual thematic analysis is
time-consuming and limits scalability. Recent advances in LLMs offer a pathway
to automate thematic analysis, but alignment with human results remains
limited. To address these limitations, we propose SFT-TA, an automated thematic
analysis framework that embeds supervised fine-tuned (SFT) agents within a
multi-agent system. Our framework outperforms existing frameworks and the
gpt-4o baseline in alignment with human reference themes. We observed that SFT
agents alone may underperform, but achieve better results than the baseline
when embedded within a multi-agent system. Our results highlight that embedding
SFT agents in specific roles within a multi-agent system is a promising pathway
to improve alignment with desired outputs for thematic analysis.

</details>


### [64] [FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning Models on Automatically Verifiable Textual and Visual Questions](https://arxiv.org/abs/2509.17177)
*Bowen Qin,Chen Yue,Fang Yin,Hui Wang,JG Yao,Jiakang Liu,Jing-Shu Zheng,Miguel Hu Chen,Richeng Xuan,Shibei Meng,Shiqi Zhou,Teng Dai,Tong-Shuai Ren,Wei Cui,Xi Yang,Xialin Du,Xiaojing Xu,Xue Sun,Xuejing Li,Yaming Liu,Yesheng Liu,Ying Liu,Yonghua Lin,Yu Zhao,Yunduo Zhang,Yuwen Luo,Zheqi He,Zhiyuan He,Zhongyuan Wang*

Main category: cs.CL

TL;DR: 该论文提出了一个中等规模、相对无污染的大推理模型评估方法，并发布了ROME基准测试用于评估视觉语言模型的视觉线索推理能力


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对大型推理模型进行有效评估的基准测试，特别是针对视觉语言模型的视觉推理能力

Method: 开发了ROME评估基准，包含评估数据和方法，用于测试视觉语言模型从视觉线索进行推理的能力

Result: 发布了完整的评估框架，包括基准测试、评估数据和相关更新，可通过指定网站访问

Conclusion: 该研究为大型推理模型的评估提供了重要工具，特别是针对视觉推理能力的测试，有助于推动该领域的发展

Abstract: We conduct a moderate-scale contamination-free (to some extent) evaluation of
current large reasoning models (LRMs) with some preliminary findings. We also
release ROME, our evaluation benchmark for vision language models intended to
test reasoning from visual clues. We attach links to the benchmark, evaluation
data, and other updates on this website:
https://flageval-baai.github.io/LRM-Eval/

</details>


### [65] [Attention Consistency for LLMs Explanation](https://arxiv.org/abs/2509.17178)
*Tian Lan,Jinyuan Xu,Xue He,Jenq-Neng Hwang,Lei Li*

Main category: cs.CL

TL;DR: 提出MACS方法，一种轻量级且易于部署的启发式方法，用于估计解码器模型中输入令牌的重要性，解决现有可解释性方法分辨率低和计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型的决策过程对于其可信赖的开发与部署至关重要，但现有可解释性方法面临分辨率低和计算成本高的挑战。

Method: 提出多层级注意力一致性评分（MACS），基于最大注意力的一致性来测量输入令牌的贡献。

Result: 实证评估表明，MACS在可解释性质量和计算效率之间取得了良好平衡，与复杂技术相比，忠实度相当，同时VRAM使用量减少22%，延迟降低30%。

Conclusion: MACS是一种有效的轻量级可解释性方法，能够在保持高质量解释的同时显著降低计算成本。

Abstract: Understanding the decision-making processes of large language models (LLMs)
is essential for their trustworthy development and deployment. However, current
interpretability methods often face challenges such as low resolution and high
computational cost. To address these limitations, we propose the
\textbf{Multi-Layer Attention Consistency Score (MACS)}, a novel, lightweight,
and easily deployable heuristic for estimating the importance of input tokens
in decoder-based models. MACS measures contributions of input tokens based on
the consistency of maximal attention. Empirical evaluations demonstrate that
MACS achieves a favorable trade-off between interpretability quality and
computational efficiency, showing faithfulness comparable to complex techniques
with a 22\% decrease in VRAM usage and 30\% reduction in latency.

</details>


### [66] [LifeAlign: Lifelong Alignment for Large Language Models with Memory-Augmented Focalized Preference Optimization](https://arxiv.org/abs/2509.17183)
*Junsong Li,Jie Zhou,Bihao Zhan,Yutao Yang,Qianjun Pan,Shilian Chen,Tianyu Huai,Xin Li,Qin Chen,Liang He*

Main category: cs.CL

TL;DR: LifeAlign是一个新颖的终身对齐框架，通过焦点化偏好优化和短长期记忆整合机制，解决大语言模型在连续学习任务中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 传统对齐方法存在灾难性遗忘问题，当模型适应新的偏好或领域时会丢失之前获得的知识。需要一种能够保持跨领域一致人类偏好对齐的方法。

Method: 提出两个关键创新：1）焦点化偏好优化策略，在对齐新偏好时防止先前知识的侵蚀；2）短长期记忆整合机制，通过内在维度缩减将去噪的短期偏好表示合并到稳定的长期记忆中。

Result: 在多个连续对齐任务上的实验结果表明，该方法在保持偏好对齐质量和知识保留方面优于现有的终身学习方法。

Conclusion: LifeAlign框架有效解决了LLMs的终身对齐问题，能够在不遗忘先前知识的情况下维持跨领域的人类偏好对齐。

Abstract: Alignment plays a crucial role in Large Language Models (LLMs) in aligning
with human preferences on a specific task/domain. Traditional alignment methods
suffer from catastrophic forgetting, where models lose previously acquired
knowledge when adapting to new preferences or domains. We introduce LifeAlign,
a novel framework for lifelong alignment that enables LLMs to maintain
consistent human preference alignment across sequential learning tasks without
forgetting previously learned knowledge. Our approach consists of two key
innovations. First, we propose a focalized preference optimization strategy
that aligns LLMs with new preferences while preventing the erosion of knowledge
acquired from previous tasks. Second, we develop a short-to-long memory
consolidation mechanism that merges denoised short-term preference
representations into stable long-term memory using intrinsic dimensionality
reduction, enabling efficient storage and retrieval of alignment patterns
across diverse domains. We evaluate LifeAlign across multiple sequential
alignment tasks spanning different domains and preference types. Experimental
results demonstrate that our method achieves superior performance in
maintaining both preference alignment quality and knowledge retention compared
to existing lifelong learning approaches. The codes and datasets will be
released on GitHub.

</details>


### [67] [Evolution of Concepts in Language Model Pre-Training](https://arxiv.org/abs/2509.17196)
*Xuyang Ge,Wentao Shu,Jiaxing Wu,Yunhua Zhou,Zhengfu He,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文使用交叉编码器方法追踪语言模型预训练过程中线性可解释特征的演化，发现特征形成具有阶段性，并与下游性能存在因果联系。


<details>
  <summary>Details</summary>
Motivation: 语言模型的预训练过程仍然是一个黑箱，需要理解其内部表示的形成过程。

Method: 使用稀疏字典学习方法（交叉编码器）分析预训练快照中的线性可解释特征演化。

Result: 大多数特征在特定时间点开始形成，复杂模式在后期训练阶段出现；特征归因分析显示特征演化与下游性能存在因果联系。

Conclusion: 研究结果与Transformer两阶段学习过程一致，为追踪语言模型学习动态中的细粒度表示进展提供了可能。

Abstract: Language models obtain extensive capabilities through pre-training. However,
the pre-training process remains a black box. In this work, we track linear
interpretable feature evolution across pre-training snapshots using a sparse
dictionary learning method called crosscoders. We find that most features begin
to form around a specific point, while more complex patterns emerge in later
training stages. Feature attribution analyses reveal causal connections between
feature evolution and downstream performance. Our feature-level observations
are highly consistent with previous findings on Transformer's two-stage
learning process, which we term a statistical learning phase and a feature
learning phase. Our work opens up the possibility to track fine-grained
representation progress during language model learning dynamics.

</details>


### [68] [Prompt-Based Simplification for Plain Language using Spanish Language Models](https://arxiv.org/abs/2509.17209)
*Lourdes Moreno,Jesus M. Sanchez-Gomez,Marco Antonio Sanchez-Escudero,Paloma Martínez*

Main category: cs.CL

TL;DR: HULAT-UC3M团队在CLEARS 2025西班牙语文本简化任务中，通过零样本提示工程和LoRA微调策略，开发了基于RigoChat-7B-v2模型的系统，在语义相似度指标上排名第一，但在可读性指标上排名第四。


<details>
  <summary>Details</summary>
Motivation: 参与CLEARS 2025西班牙语文本简化任务，探索基于西班牙语文本训练的模型在文本简化方面的性能，并解决训练数据异质性和评估指标局限性等挑战。

Method: 使用零样本提示工程和LoRA微调策略，在西班牙语文本上训练模型，通过余弦相似度和Fernández-Huerta可读性指数评估不同策略，最终选择结合标准化步骤、RigoChat-7B-v2模型和专用简化提示的系统。

Result: 最终系统在语义相似度指标上排名第一（SIM = 0.75），但在可读性指标上排名第四（FH = 69.72）。

Conclusion: 研究揭示了训练数据异质性和当前评估指标在同时捕捉语言清晰度和内容保留方面的局限性，需要进一步改进评估方法。

Abstract: This paper describes the participation of HULAT-UC3M in CLEARS 2025 Subtask
1: Adaptation of Text to Plain Language (PL) in Spanish. We explored strategies
based on models trained on Spanish texts, including a zero-shot configuration
using prompt engineering and a fine-tuned version with Low-Rank Adaptation
(LoRA). Different strategies were evaluated on representative internal subsets
of the training data, using the official task metrics, cosine similarity (SIM)
and the Fern\'andez-Huerta readability index (FH) to guide the selection of the
optimal model and prompt combination. The final system was selected for its
balanced and consistent performance, combining normalization steps, the
RigoChat-7B-v2 model, and a dedicated PL-oriented prompt. It ranked first in
semantic similarity (SIM = 0.75), however, fourth in readability (FH = 69.72).
We also discuss key challenges related to training data heterogeneity and the
limitations of current evaluation metrics in capturing both linguistic clarity
and content preservation.

</details>


### [69] [Extending Automatic Machine Translation Evaluation to Book-Length Documents](https://arxiv.org/abs/2509.17249)
*Kuang-Da Wang,Shuoyang Ding,Chao-Han Huck Yang,Ping-Chun Hsieh,Wen-Chih Peng,Vitaly Lavrukhin,Boris Ginsburg*

Main category: cs.CL

TL;DR: SEGALE是一个评估方案，将现有自动指标扩展到长文档翻译评估，通过将文档视为连续文本并应用句子分割和对齐方法，解决了现有评估方法局限于句子级的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在翻译性能和长上下文能力方面表现出色，但由于数据集限制、指标token数量限制和严格的句子边界要求，评估方法仍然局限于句子级评估。

Method: SEGALE方案将文档视为连续文本，应用句子分割和对齐方法，能够处理任意长度的文档翻译，同时考虑欠翻译/过翻译和不同的句子边界。

Result: 实验表明该方案显著优于现有的长文档评估方案，与使用真实句子对齐的评估结果相当。应用该方案发现许多开源LLM在其报告的最大上下文长度下无法有效翻译文档。

Conclusion: SEGALE为长文档翻译提供了有效的评估框架，揭示了当前LLM在长文档翻译方面的局限性。

Abstract: Despite Large Language Models (LLMs) demonstrating superior translation
performance and long-context capabilities, evaluation methodologies remain
constrained to sentence-level assessment due to dataset limitations, token
number restrictions in metrics, and rigid sentence boundary requirements. We
introduce SEGALE, an evaluation scheme that extends existing automatic metrics
to long-document translation by treating documents as continuous text and
applying sentence segmentation and alignment methods. Our approach enables
previously unattainable document-level evaluation, handling translations of
arbitrary length generated with document-level prompts while accounting for
under-/over-translations and varied sentence boundaries. Experiments show our
scheme significantly outperforms existing long-form document evaluation
schemes, while being comparable to evaluations performed with groundtruth
sentence alignments. Additionally, we apply our scheme to book-length texts and
newly demonstrate that many open-weight LLMs fail to effectively translate
documents at their reported maximum context lengths.

</details>


### [70] [Probabilistic Token Alignment for Large Language Model Fusion](https://arxiv.org/abs/2509.17276)
*Runjia Zeng,James Chenhao Liang,Cheng Han,Zhiwen Cao,Jiahao Liu,Xiaojun Quan,Yingjie Victor Chen,Lifu Huang,Tong Geng,Qifan Wang,Dongfang Liu*

Main category: cs.CL

TL;DR: 提出PTA-LLM方法，通过概率令牌对齐解决预训练大语言模型融合中的词汇对齐问题，将令牌对齐重新表述为最优传输问题，实现更连贯的模型融合。


<details>
  <summary>Details</summary>
Motivation: 从头训练大语言模型成本高昂且易产生冗余能力，而现有模型融合方法依赖手动预定义的词汇对齐，在多样化上下文中泛化能力不足，导致性能下降。

Method: 从分布学习中获得灵感，提出概率令牌对齐方法，将令牌对齐重新表述为最优传输这一经典数学问题，利用分布感知学习促进更连贯的模型融合。

Result: 实证结果表明，概率令牌对齐在多个能力维度上提升了目标模型的性能。

Conclusion: PTA-LLM不仅具有通用性，还从分布角度展现出可解释性，为理解令牌对齐的本质提供了洞见。

Abstract: Training large language models (LLMs) from scratch can yield models with
unique functionalities and strengths, but it is costly and often leads to
redundant capabilities. A more cost-effective alternative is to fuse existing
pre-trained LLMs with different architectures into a more powerful model.
However, a key challenge in existing model fusion is their dependence on
manually predefined vocabulary alignment, which may not generalize well across
diverse contexts, leading to performance degradation in several evaluation. To
solve this, we draw inspiration from distribution learning and propose the
probabilistic token alignment method as a general and soft mapping for
alignment, named as PTA-LLM. Our approach innovatively reformulates token
alignment into a classic mathematical problem: optimal transport, seamlessly
leveraging distribution-aware learning to facilitate more coherent model
fusion. Apart from its inherent generality, PTA-LLM exhibits interpretability
from a distributional perspective, offering insights into the essence of the
token alignment. Empirical results demonstrate that probabilistic token
alignment enhances the target model's performance across multiple capabilities.
Our code is avaliable at https://runjia.tech/neurips_pta-llm/.

</details>


### [71] [Automated Knowledge Graph Construction using Large Language Models and Sentence Complexity Modelling](https://arxiv.org/abs/2509.17289)
*Sydney Anuyah,Mehedi Mahmud Kaushik,Krishna Dwarampudi,Rakesh Shiradkar,Arjan Durresi,Sunandan Chakraborty*

Main category: cs.CL

TL;DR: CoDe-KG是一个开源端到端流水线，通过结合核心解析和句法分解提取句子级知识图谱，贡献了大规模数据集并在关系提取任务上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够有效提取句子级知识图谱的系统，通过整合核心解析和句法分解来提高关系提取的准确性和召回率，特别是在处理复杂句子和罕见关系时。

Method: 使用混合思维链和少样本提示策略，结合核心解析与句法句子分解技术，系统选择最优提示-模型对来处理不同复杂度的句子。

Result: 在关系提取任务上达到65.8%的宏F1分数（比之前最佳结果提升8个百分点），在WebNLG2上达到75.7%的微F1分数，消融研究显示整合核心解析和分解可将罕见关系的召回率提高20%以上。

Conclusion: CoDe-KG证明了整合核心解析和句法分解在知识图谱提取中的有效性，特别是在处理复杂语言结构和罕见关系方面表现出色，为自然语言处理社区提供了有价值的开源工具和数据集。

Abstract: We introduce CoDe-KG, an open-source, end-to-end pipeline for extracting
sentence-level knowledge graphs by combining robust coreference resolution with
syntactic sentence decomposition. Using our model, we contribute a dataset of
over 150,000 knowledge triples, which is open source. We also contribute a
training corpus of 7248 rows for sentence complexity, 190 rows of gold human
annotations for co-reference resolution using open source lung-cancer abstracts
from PubMed, 900 rows of gold human annotations for sentence conversion
policies, and 398 triples of gold human annotations. We systematically select
optimal prompt-model pairs across five complexity categories, showing that
hybrid chain-of-thought and few-shot prompting yields up to 99.8% exact-match
accuracy on sentence simplification. On relation extraction (RE), our pipeline
achieves 65.8% macro-F1 on REBEL, an 8-point gain over the prior state of the
art, and 75.7% micro-F1 on WebNLG2, while matching or exceeding performance on
Wiki-NRE and CaRB. Ablation studies demonstrate that integrating coreference
and decomposition increases recall on rare relations by over 20%. Code and
dataset are available at https://github.com/KaushikMahmud/CoDe-KG_EMNLP_2025

</details>


### [72] [Multi-View Attention Multiple-Instance Learning Enhanced by LLM Reasoning for Cognitive Distortion Detection](https://arxiv.org/abs/2509.17292)
*Jun Seo Kim,Hyemi Kim,Woo Joo Oh,Hongjin Cho,Hochul Lee,Hye Hyeon Kim*

Main category: cs.CL

TL;DR: 提出结合大语言模型和多示例学习的新框架，通过分解话语为情绪、逻辑和行为组件来检测认知扭曲，提高心理健康NLP中的可解释性和细粒度推理能力。


<details>
  <summary>Details</summary>
Motivation: 认知扭曲与心理健康障碍密切相关，但由于上下文模糊性、共现性和语义重叠，其自动检测一直具有挑战性。

Method: 将每个话语分解为情绪、逻辑和行为（ELB）组件，使用LLM推断多个扭曲实例，每个实例包含预测类型、表达和显著性分数，通过多视角门控注意力机制进行最终分类。

Result: 在韩语（KoACD）和英语（Therapist QA）数据集上的实验表明，结合ELB和LLM推断的显著性分数提高了分类性能，特别是对于具有高解释模糊性的扭曲。

Conclusion: 该方法为心理健康NLP中的细粒度推理提供了一种心理学基础且可推广的方法。

Abstract: Cognitive distortions have been closely linked to mental health disorders,
yet their automatic detection remained challenging due to contextual ambiguity,
co-occurrence, and semantic overlap. We proposed a novel framework that
combines Large Language Models (LLMs) with Multiple-Instance Learning (MIL)
architecture to enhance interpretability and expression-level reasoning. Each
utterance was decomposed into Emotion, Logic, and Behavior (ELB) components,
which were processed by LLMs to infer multiple distortion instances, each with
a predicted type, expression, and model-assigned salience score. These
instances were integrated via a Multi-View Gated Attention mechanism for final
classification. Experiments on Korean (KoACD) and English (Therapist QA)
datasets demonstrate that incorporating ELB and LLM-inferred salience scores
improves classification performance, especially for distortions with high
interpretive ambiguity. Our results suggested a psychologically grounded and
generalizable approach for fine-grained reasoning in mental health NLP.

</details>


### [73] [Scaling, Simplification, and Adaptation: Lessons from Pretraining on Machine-Translated Text](https://arxiv.org/abs/2509.17317)
*Dan John Velasco,Matthew Theodore Roque*

Main category: cs.CL

TL;DR: 该研究探讨了使用机器翻译生成数据来弥补低资源语言数据不足的问题，通过实验验证了MT数据在不同模型规模下的效果、源语言简化对泛化能力的影响，以及MT预训练模型在有限本地数据上持续训练的性能。


<details>
  <summary>Details</summary>
Motivation: 大多数语言缺乏足够的大规模单语预训练数据，形成"数据墙"。多语言预训练存在语言不平衡和"多语言诅咒"问题，因此研究使用机器翻译从高资源语言生成数据来支持低资源语言的模型训练。

Method: 将英语翻译成印度尼西亚语和泰米尔语（两种类型学上不同的低资源语言），使用GPT-2模型（124M-774M参数）在原生或MT生成的数据集上进行预训练，评估在原生文本上的交叉熵损失、句法探针和下游任务的准确性。

Result: MT预训练模型能从规模扩展中受益；源语言简化会损害对原生文本的泛化能力；在原生文本上持续训练的MT预训练模型通常比仅使用原生数据的模型表现更好，即使原生数据更少。但需要文化细微差别的任务需要更多接触原生数据。

Conclusion: 机器翻译生成的数据可以有效支持低资源语言的模型训练，特别是在模型规模扩展和有限原生数据持续训练方面表现良好，但对于需要文化理解的任务仍需更多原生数据。

Abstract: Most languages lack sufficient data for large-scale monolingual pretraining,
creating a "data wall." Multilingual pretraining helps but is limited by
language imbalance and the "curse of multilinguality." An alternative is to
translate high-resource text with machine translation (MT), which raises three
questions: (1) How does MT-derived data scale with model capacity? (2) Can
source-side transformations (e.g., simplifying English with an LLM) improve
generalization to native text? (3) How well do models pretrained on MT-derived
data adapt when continually trained on limited native text? We investigate
these questions by translating English into Indonesian and Tamil--two
typologically distant, lower-resource languages--and pretraining GPT-2 models
(124M-774M) on native or MT-derived corpora from raw and LLM-simplified
English. We evaluate cross-entropy loss on native text, along with accuracy on
syntactic probes and downstream tasks. Our results show that (1) MT-pretrained
models benefit from scaling; (2) source-side simplification harms
generalization to native text; and (3) adapting MT-pretrained models on native
text often yields better performance than native-only models, even with less
native data. However, tasks requiring cultural nuance (e.g., toxicity
detection) demand more exposure to native data.

</details>


### [74] [AIMMerging: Adaptive Iterative Model Merging Using Training Trajectories for Language Model Continual Learning](https://arxiv.org/abs/2509.17348)
*Yujie Feng,Jian Li,Xiaoyu Dong,Pengfei Xu,Xiaohui Zhou,Yujia Zhang,Zexin LU,Yasha Wang,Alan Zhao,Xu Chu,Xiao-Ming Wu*

Main category: cs.CL

TL;DR: AimMerging是一个新的持续学习框架，通过动态监控训练轨迹来自适应决定模型融合的时间和频率，有效平衡新知识学习和防止遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有基于模型融合的持续学习方法在平衡学习新知识和防止遗忘方面存在困难，主要由于融合次数和频率不够优化。

Method: 提出自适应迭代模型融合框架，利用训练轨迹中的学习和遗忘信号动态监控模型状态，通过训练轨迹指导的融合控制器自适应决定融合时机和频率，使用基于排练的知识融合模块计算融合权重并执行融合。

Result: 在三个持续学习基准测试中，从770M到13B不同模型规模下，AimMerging相比现有最先进方法在FWT和BWT指标上分别实现了80%和59%的平均相对提升。

Conclusion: AimMerging框架能够有效解决持续学习中的知识平衡问题，显著提升模型性能。

Abstract: Continual learning (CL) is essential for deploying large language models
(LLMs) in dynamic real-world environments without the need for costly
retraining. Recent model merging-based methods have attracted significant
attention, but they still struggle to effectively manage the trade-off between
learning new knowledge and preventing forgetting, a challenge largely stemming
from suboptimal number of merges and merging frequency. In this paper, we
introduce Adaptive Iterative Model Merging (AimMerging), a novel CL framework
that utilizes learning and forgetting signals from the training trajectory to
dynamically monitor the model's training status. Guided by dynamic monitoring,
the training trajectory-guided merge controller adaptively determines the
timing and frequency of iterative fusion, while the rehearsal-based knowledge
fusion module computes the merging weights and executes the fusion.
Comprehensive experiments on three CL benchmarks with various model sizes (from
770M to 13B) demonstrate that AimMerging achieves significant performance
improvements over existing state-of-the-art methods, with an average relative
improvement of 80% and 59% on FWT and BWT, respectively. The source code is
provided for reproducibility.

</details>


### [75] [Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation](https://arxiv.org/abs/2509.17349)
*Peter Polák,Sara Papi,Luisa Bentivogli,Ondřej Bojar*

Main category: cs.CL

TL;DR: 本文提出了YAAL和LongYAAL两种改进的延迟评估指标，以及SoftSegmenter重新分割工具，用于解决同时语音翻译系统中延迟评估不准确的问题。


<details>
  <summary>Details</summary>
Motivation: 现有同时语音翻译系统的延迟评估指标在短文本设置下存在结构性偏差，导致评估结果不一致或误导性，特别是在人工预分割的语音场景中。

Method: 1. 提出YAAL指标改进短文本场景的延迟评估；2. 扩展为LongYAAL用于未分割音频；3. 开发基于词级对齐的SoftSegmenter重新分割工具。

Result: 实验表明YAAL和LongYAAL优于现有流行延迟指标，SoftSegmenter提高了长文本评估中的对齐质量。

Conclusion: 新提出的指标和工具能够为同时语音翻译系统提供更可靠的延迟评估方法。

Abstract: Simultaneous speech-to-text translation (SimulST) systems have to balance
translation quality with latency--the delay between speech input and the
translated output. While quality evaluation is well established, accurate
latency measurement remains a challenge. Existing metrics often produce
inconsistent or misleading results, especially in the widely used short-form
setting, where speech is artificially presegmented. In this paper, we present
the first comprehensive analysis of SimulST latency metrics across language
pairs, systems, and both short- and long-form regimes. We uncover a structural
bias in current metrics related to segmentation that undermines fair and
meaningful comparisons. To address this, we introduce YAAL (Yet Another Average
Lagging), a refined latency metric that delivers more accurate evaluations in
the short-form regime. We extend YAAL to LongYAAL for unsegmented audio and
propose SoftSegmenter, a novel resegmentation tool based on word-level
alignment. Our experiments show that YAAL and LongYAAL outperform popular
latency metrics, while SoftSegmenter enhances alignment quality in long-form
evaluation, together enabling more reliable assessments of SimulST systems.

</details>


### [76] [Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs](https://arxiv.org/abs/2509.17367)
*Haoyang Chen,Kumiko Tanaka-Ishii*

Main category: cs.CL

TL;DR: 本文使用尺度无关指标对不同领域的文本复杂度进行比较分析，发现法律文本具有特定的结构和复杂性特征，而当前生成模型未能完全复制这些特征。


<details>
  <summary>Details</summary>
Motivation: 研究不同领域（特别是法律领域）文本复杂度的差异，以及AI生成文本与人类撰写文本在复杂度特征上的对比。

Method: 使用Heaps指数β（词汇增长）、Taylor指数α（词频波动标度）、压缩率r（冗余度）和熵等尺度无关指标，分析法律文件、一般自然语言文本和AI生成文本的语料库。

Result: 法律文本比一般文本具有更慢的词汇增长（β较低）和更高的术语一致性（α较高）；在法律领域内，法规代码的β最低、α最高，反映严格的起草规范；GPT生成文本的统计特征更接近一般语言模式。

Conclusion: 法律文本展现出领域特定的结构和复杂性，当前的生成模型尚未完全复制这些特征。

Abstract: We present a comparative analysis of text complexity across domains using
scale-free metrics. We quantify linguistic complexity via Heaps' exponent
$\beta$ (vocabulary growth), Taylor's exponent $\alpha$ (word-frequency
fluctuation scaling), compression rate $r$ (redundancy), and entropy. Our
corpora span three domains: legal documents (statutes, cases, deeds) as a
specialized domain, general natural language texts (literature, Wikipedia), and
AI-generated (GPT) text. We find that legal texts exhibit slower vocabulary
growth (lower $\beta$) and higher term consistency (higher $\alpha$) than
general texts. Within legal domain, statutory codes have the lowest $\beta$ and
highest $\alpha$, reflecting strict drafting conventions, while cases and deeds
show higher $\beta$ and lower $\alpha$. In contrast, GPT-generated text shows
the statistics more aligning with general language patterns. These results
demonstrate that legal texts exhibit domain-specific structures and
complexities, which current generative models do not fully replicate.

</details>


### [77] [Robustness of Neurosymbolic Reasoners on First-Order Logic Problems](https://arxiv.org/abs/2509.17377)
*Hannah Bansal,Kemal Kurniawan,Lea Frermann*

Main category: cs.CL

TL;DR: 本文研究了神经符号方法（结合LLM和符号逻辑求解器）在应对反事实任务变体时的鲁棒性，发现虽然神经符号方法比纯神经方法更鲁棒，但整体性能较差。提出的NSCoT方法结合了神经符号方法和思维链提示，性能有所提升但仍不及标准CoT。


<details>
  <summary>Details</summary>
Motivation: LLMs在反事实任务变体上表现脆弱，表明它们可能依赖虚假的表面模式进行推理。本文旨在探索神经符号方法是否能提高LLMs在逻辑一致性扰动下的鲁棒性。

Method: 采用神经符号方法整合LLM和符号逻辑求解器，并进一步提出NSCoT方法结合神经符号方法和思维链提示，在不同规模的LLMs上进行实验。

Result: 神经符号方法比纯神经方法更鲁棒但整体性能较差；NSCoT方法性能有所提升但仍落后于标准CoT方法。

Conclusion: 神经符号方法在鲁棒性方面有优势但性能仍需改进，为未来研究提供了方向。

Abstract: Recent trends in NLP aim to improve reasoning capabilities in Large Language
Models (LLMs), with key focus on generalization and robustness to variations in
tasks. Counterfactual task variants introduce minimal but semantically
meaningful changes to otherwise valid first-order logic (FOL) problem instances
altering a single predicate or swapping roles of constants to probe whether a
reasoning system can maintain logical consistency under perturbation. Previous
studies showed that LLMs becomes brittle on counterfactual variations,
suggesting that they often rely on spurious surface patterns to generate
responses. In this work, we explore if a neurosymbolic (NS) approach that
integrates an LLM and a symbolic logical solver could mitigate this problem.
Experiments across LLMs of varying sizes show that NS methods are more robust
but perform worse overall that purely neural methods. We then propose NSCoT
that combines an NS method and Chain-of-Thought (CoT) prompting and demonstrate
that while it improves performance, NSCoT still lags behind standard CoT. Our
analysis opens research directions for future work.

</details>


### [78] [FinDebate: Multi-Agent Collaborative Intelligence for Financial Analysis](https://arxiv.org/abs/2509.17395)
*Tianshi Cai,Guanxu Li,Nijia Han,Ce Huang,Zimu Wang,Changyu Zeng,Yuqi Wang,Jingshi Zhou,Haiyang Zhang,Qi Chen,Yushan Pan,Shuihua Wang,Wei Wang*

Main category: cs.CL

TL;DR: FinDebate是一个用于金融分析的多智能体框架，通过结合协作辩论和领域特定的检索增强生成技术，由五个专业智能体并行工作来生成多维度的分析见解。


<details>
  <summary>Details</summary>
Motivation: 为了解决金融分析中过度自信和可靠性不足的问题，开发一个能够通过智能体间辩论来挑战和优化初始结论的框架，以提高分析质量。

Method: 采用多智能体协作辩论框架，包含五个专业智能体（收益、市场、情绪、估值和风险），结合安全辩论协议和检索增强生成技术。

Result: 基于LLM和人工评估的实验结果表明，该框架能够生成高质量的分析，具有校准的置信水平和可操作的投资策略，适用于多个时间范围。

Conclusion: FinDebate框架通过多智能体辩论机制有效提升了金融分析的可靠性和质量，为投资决策提供了更准确的支持。

Abstract: We introduce FinDebate, a multi-agent framework for financial analysis,
integrating collaborative debate with domain-specific Retrieval-Augmented
Generation (RAG). Five specialized agents, covering earnings, market,
sentiment, valuation, and risk, run in parallel to synthesize evidence into
multi-dimensional insights. To mitigate overconfidence and improve reliability,
we introduce a safe debate protocol that enables agents to challenge and refine
initial conclusions while preserving coherent recommendations. Experimental
results, based on both LLM-based and human evaluations, demonstrate the
framework's efficacy in producing high-quality analysis with calibrated
confidence levels and actionable investment strategies across multiple time
horizons.

</details>


### [79] [EpiCache: Episodic KV Cache Management for Long Conversational Question Answering](https://arxiv.org/abs/2509.17396)
*Minsoo Kim,Arnav Kundu,Han-Byul Kim,Richa Dixit,Minsik Cho*

Main category: cs.CL

TL;DR: EpiCache是一个无需训练的KV缓存管理框架，通过分块预填充和基于主题的压缩技术，在固定内存预算下优化长对话问答系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存压缩方法存在两个主要问题：全上下文预填充后驱逐条目会导致无限制的峰值内存，以及查询相关驱逐会降低多轮对话的准确性。

Method: 采用分块预填充技术限制缓存增长，通过主题KV压缩将对话历史聚类为连贯的主题片段，并应用自适应分层预算分配策略。

Result: 在三个长对话问答基准测试中，EpiCache相比现有基线准确率提升高达40%，在4-6倍压缩下保持接近完整的KV准确率，延迟和内存分别减少2.4倍和3.5倍。

Conclusion: EpiCache能够在严格资源约束下实现高效的多轮交互，解决了长对话问答系统中KV缓存的内存和性能瓶颈问题。

Abstract: Recent advances in large language models (LLMs) have extended context
lengths, enabling assistants to sustain long histories for coherent,
personalized responses. This ability, however, hinges on Key-Value (KV)
caching, whose memory grows linearly with dialogue length and quickly dominates
under strict resource constraints. An active line of research for reducing this
overhead is KV cache compression, which seeks to limit cache size while
preserving accuracy. Yet existing methods face two major limitations: (i)
evicting entries after full-context prefill causes unbounded peak memory, and
(ii) query-dependent eviction narrows the cache to a single query, leading to
degraded accuracy in multi-turn conversations. We introduce EpiCache, a
training-free KV cache management framework for long conversational question
answering (LongConvQA) under fixed memory budgets. EpiCache bounds cache growth
through block-wise prefill and preserves topic-relevant context via episodic KV
compression, which clusters conversation history into coherent episodes and
applies episode-specific KV cache eviction. We further design an adaptive
layer-wise budget allocation strategy that measures each layer's sensitivity to
eviction and distributes the memory budget across layers accordingly. Across
three LongConvQA benchmarks, EpiCache improves accuracy by up to 40% over
recent baselines, sustains near-full KV accuracy under 4-6x compression, and
reduces latency and memory by up to 2.4x and 3.5x, thereby enabling efficient
multi-turn interaction under strict resource constraints.

</details>


### [80] [DIWALI - Diversity and Inclusivity aWare cuLture specific Items for India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context](https://arxiv.org/abs/2509.17399)
*Pramit Sahoo,Maharaj Brahma,Maunendra Sankar Desarkar*

Main category: cs.CL

TL;DR: 本文介绍了DIWALI数据集，一个针对印度文化的文化特定项目数据集，用于评估大型语言模型的文化适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型缺乏文化对齐能力，存在偏见生成问题，且缺乏针对区域和子区域文化复杂性的评估数据集。

Method: 创建包含约8,000个文化概念的印度文化数据集，涵盖17个文化方面和36个子区域，使用CSI、LLM作为评判者和人工评估来测量LLM的文化能力。

Result: 定量分析显示所有考虑的LLM都存在选择性子区域覆盖和表面级适应问题。

Conclusion: DIWALI数据集为解决LLM文化评估挑战提供了重要资源，揭示了当前模型在文化适应方面的局限性。

Abstract: Large language models (LLMs) are widely used in various tasks and
applications. However, despite their wide capabilities, they are shown to lack
cultural alignment \citep{ryan-etal-2024-unintended,
alkhamissi-etal-2024-investigating} and produce biased generations
\cite{naous-etal-2024-beer} due to a lack of cultural knowledge and competence.
Evaluation of LLMs for cultural awareness and alignment is particularly
challenging due to the lack of proper evaluation metrics and unavailability of
culturally grounded datasets representing the vast complexity of cultures at
the regional and sub-regional levels. Existing datasets for culture specific
items (CSIs) focus primarily on concepts at the regional level and may contain
false positives. To address this issue, we introduce a novel CSI dataset for
Indian culture, belonging to 17 cultural facets. The dataset comprises $\sim$8k
cultural concepts from 36 sub-regions. To measure the cultural competence of
LLMs on a cultural text adaptation task, we evaluate the adaptations using the
CSIs created, LLM as Judge, and human evaluations from diverse
socio-demographic region. Furthermore, we perform quantitative analysis
demonstrating selective sub-regional coverage and surface-level adaptations
across all considered LLMs. Our dataset is available here:
\href{https://huggingface.co/datasets/nlip/DIWALI}{https://huggingface.co/datasets/nlip/DIWALI},
project
webpage\footnote{\href{https://nlip-lab.github.io/nlip/publications/diwali/}{https://nlip-lab.github.io/nlip/publications/diwali/}},
and our codebase with model outputs can be found here:
\href{https://github.com/pramitsahoo/culture-evaluation}{https://github.com/pramitsahoo/culture-evaluation}.

</details>


### [81] [Vision Language Models Are Not (Yet) Spelling Correctors](https://arxiv.org/abs/2509.17418)
*Junhong Liang,Bojun Zhang*

Main category: cs.CL

TL;DR: ReViCo是首个系统评估视觉语言模型在真实世界视觉拼写纠正任务上的基准，包含中英文自然错误数据，支持图像和词符级别的细粒度评估。


<details>
  <summary>Details</summary>
Motivation: 视觉输入中的拼写纠正对视觉语言模型提出了独特挑战，需要直接在图像中检测和纠正文本错误，但目前缺乏专门的评估基准。

Method: 构建ReViCo基准，收集真实世界图像中的自然错误；评估代表性开源和闭源模型；探索联合OCR-纠正管道和背景信息增强两种解决方案。

Result: 当前VLM在拼写纠正任务上表现显著低于人类水平，特别是在纠正方面；提出的两种解决方案均带来一致的性能提升。

Conclusion: 分析揭示了现有架构的根本局限性，为推进多模态拼写纠正提供了可行的见解。

Abstract: Spelling correction from visual input poses unique challenges for vision
language models (VLMs), as it requires not only detecting but also correcting
textual errors directly within images. We present ReViCo (Real Visual
Correction), the first benchmark that systematically evaluates VLMs on
real-world visual spelling correction across Chinese and English. ReViCo
contains naturally occurring errors collected from real-world image data and
supports fine-grained evaluation at both image and token levels. Through
comprehensive experiments on representative cascaded (Qwen) and native
(InternVL) open-source models, as well as closed-source systems (GPT-4o,
Claude), we show that current VLMs fall significantly short of human
performance, particularly in correction. To address these limitations, we
explore two solution paradigms: a Joint OCR-Correction pipeline and a
Background Information enhanced approach, both of which yield consistent
performance gains. Our analysis highlights fundamental limitations of existing
architectures and provides actionable insights for advancing multimodal
spelling correction.

</details>


### [82] [RealBench: A Chinese Multi-image Understanding Benchmark Close to Real-world Scenarios](https://arxiv.org/abs/2509.17421)
*Fei Zhao,Chengqiang Lu,Yufan Shen,Qimeng Wang,Yicheng Qian,Haoxin Zhang,Yan Gao,Yi Wu,Yao Hu,Zhen Wu,Shangyu Xing,Xinyu Dai*

Main category: cs.CL

TL;DR: RealBench是首个中文多模态多图像数据集，包含9393个样本和69910张图像，基于真实用户生成内容，覆盖多种场景和图像结构，用于评估多图像理解能力。


<details>
  <summary>Details</summary>
Motivation: 填补中文多图像数据集的空白，现有数据集主要基于英文，缺乏针对中文场景的多图像评估基准。

Method: 构建包含真实用户生成内容的中文多模态多图像数据集，涵盖多样场景、分辨率和图像结构，并使用21个不同规模的多模态大语言模型进行综合评估。

Result: 实验表明，即使是性能最强的闭源模型在处理中文多图像场景时仍面临挑战，开源视觉/视频模型与闭源模型之间存在约71.8%的平均性能差距。

Conclusion: RealBench为在中文语境下进一步探索多图像理解能力提供了重要的研究基础，揭示了当前模型在处理中文多图像任务上的局限性。

Abstract: While various multimodal multi-image evaluation datasets have been emerged,
but these datasets are primarily based on English, and there has yet to be a
Chinese multi-image dataset. To fill this gap, we introduce RealBench, the
first Chinese multimodal multi-image dataset, which contains 9393 samples and
69910 images. RealBench distinguishes itself by incorporating real
user-generated content, ensuring high relevance to real-world applications.
Additionally, the dataset covers a wide variety of scenes, image resolutions,
and image structures, further increasing the difficulty of multi-image
understanding. Ultimately, we conduct a comprehensive evaluation of RealBench
using 21 multimodal LLMs of different sizes, including closed-source models
that support multi-image inputs as well as open-source visual and video models.
The experimental results indicate that even the most powerful closed-source
models still face challenges when handling multi-image Chinese scenarios.
Moreover, there remains a noticeable performance gap of around 71.8\% on
average between open-source visual/video models and closed-source models. These
results show that RealBench provides an important research foundation for
further exploring multi-image understanding capabilities in the Chinese
context.

</details>


### [83] [QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models](https://arxiv.org/abs/2509.17428)
*Hyesung Jeon,Seojune Lee,Beomseok Kang,Yulhwa Kim,Jae-Joon Kim*

Main category: cs.CL

TL;DR: QWHA是一种将基于傅里叶变换的适配器集成到量化模型中的方法，使用沃尔什-哈达玛变换作为变换核，通过新颖的适配器初始化方案有效减少量化误差并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的高效部署需求推动了量化和参数高效微调的发展。现有基于低秩适应的量化感知PEFT方法表示能力有限，而基于傅里叶变换的适配器虽然表示能力更强，但在量化模型中直接集成会导致误差减少效果不佳和计算开销增加。

Method: 提出QWHA方法，使用沃尔什-哈达玛变换作为变换核，结合包含自适应参数选择和值优化的新型适配器初始化方案，将基于傅里叶变换的适配器集成到量化模型中。

Result: 实验结果表明，QWHA在低比特量化精度上持续优于基线方法，相比现有基于傅里叶变换的适配器实现了显著的训练加速。

Conclusion: QWHA能有效减轻量化误差并促进微调，其设计显著降低了计算成本，为量化感知PEFT提供了一种高效的解决方案。

Abstract: The demand for efficient deployment of large language models (LLMs) has
driven interest in quantization, which reduces inference cost, and
parameter-efficient fine-tuning (PEFT), which lowers training overhead. This
motivated the development of quantization-aware PEFT to produce accurate yet
efficient quantized models. In this setting, reducing quantization error prior
to fine-tuning is crucial for achieving high model accuracy. However, existing
methods that rely on low-rank adaptation suffer from limited representational
capacity. Recent Fourier-related transform (FT)-based adapters offer greater
representational power than low-rank adapters, but their direct integration
into quantized models often results in ineffective error reduction and
increased computational overhead. To overcome these limitations, we propose
QWHA, a method that integrates FT-based adapters into quantized models by
employing the Walsh-Hadamard Transform (WHT) as the transform kernel, together
with a novel adapter initialization scheme incorporating adaptive parameter
selection and value refinement. We demonstrate that QWHA effectively mitigates
quantization errors while facilitating fine-tuning, and that its design
substantially reduces computational cost. Experimental results show that QWHA
consistently outperforms baselines in low-bit quantization accuracy and
achieves significant training speedups over existing FT-based adapters. The
code is available at https://github.com/vantaa89/qwha.

</details>


### [84] [MedFact: A Large-scale Chinese Dataset for Evidence-based Medical Fact-checking of LLM Responses](https://arxiv.org/abs/2509.17436)
*Tong Chen,Zimu Wang,Yiyi Miao,Haoran Luo,Yuanfei Sun,Wei Wang,Zhengyong Jiang,Procheta Sen,Jionglong Su*

Main category: cs.CL

TL;DR: MedFact是首个基于证据的中文医疗事实核查数据集，专注于LLM生成的医疗内容，包含1,321个问题和7,409个声明，涵盖真实医疗场景的复杂性。


<details>
  <summary>Details</summary>
Motivation: 现有数据集主要关注人类生成的内容，而LLM生成的医疗内容验证相对未被探索，医疗事实核查对在线医疗信息获取日益重要。

Method: 构建MedFact数据集，在上下文学习和微调设置下进行综合实验，并对错误进行深入分析。

Result: 展示了当前LLM在此任务上的能力和挑战，为未来研究指明了关键方向。

Conclusion: MedFact数据集填补了LLM生成医疗内容验证的空白，为医疗事实核查研究提供了重要资源。

Abstract: Medical fact-checking has become increasingly critical as more individuals
seek medical information online. However, existing datasets predominantly focus
on human-generated content, leaving the verification of content generated by
large language models (LLMs) relatively unexplored. To address this gap, we
introduce MedFact, the first evidence-based Chinese medical fact-checking
dataset of LLM-generated medical content. It consists of 1,321 questions and
7,409 claims, mirroring the complexities of real-world medical scenarios. We
conduct comprehensive experiments in both in-context learning (ICL) and
fine-tuning settings, showcasing the capability and challenges of current LLMs
on this task, accompanied by an in-depth error analysis to point out key
directions for future research. Our dataset is publicly available at
https://github.com/AshleyChenNLP/MedFact.

</details>


### [85] [GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric Reasoning](https://arxiv.org/abs/2509.17437)
*Guizhen Chen,Weiwen Xu,Hao Zhang,Hou Pong Chan,Deli Zhao,Anh Tuan Luu,Yu Rong*

Main category: cs.CL

TL;DR: 该论文提出了一种两阶段强化学习训练框架，通过先增强视觉感知能力再培养推理能力，来解决多模态大语言模型在几何推理等视觉密集型任务中的感知瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习虽然提升了语言模型的推理能力，但对多模态大语言模型（MLLMs）的影响有限。特别是在几何推理等视觉密集型任务中，MLLMs经常产生幻觉导致推理不准确，这主要是由于感知瓶颈限制了推理训练的效果。

Method: 设计GeoPQA基准测试量化感知瓶颈，并提出两阶段RL训练框架：第一阶段增强几何结构的视觉感知能力，第二阶段培养推理能力。在Qwen2.5-VL-3B-Instruct模型上应用该方法。

Result: 与直接推理训练方法相比，两阶段训练在几何推理上提升了9.7%，在几何问题解决上提升了9.1%。该方法在其他视觉密集型领域（如图表理解）也表现出良好的泛化能力。

Conclusion: 研究表明感知基础对于有效的MLLM推理至关重要，两阶段RL训练框架能有效解决感知瓶颈问题，提升模型在视觉密集型任务中的表现。

Abstract: Recent advancements in reinforcement learning (RL) have enhanced the
reasoning abilities of large language models (LLMs), yet the impact on
multimodal LLMs (MLLMs) is limited. Particularly in vision-intensive tasks like
geometric reasoning, MLLMs hallucinate frequently, leading to inaccurate
reasoning. We attribute this to the perceptual bottleneck in MLLMs, which caps
the benefits of reasoning training. To quantify this, we design a
Geo-Perception Question-Answering (GeoPQA) benchmark, targeting basic geometric
concepts and spatial relationships. Experiments on GeoPQA reveal significant
shortcomings of MLLMs in visual perception, which constrain RL reward signals
for effective training. To address this bottleneck, we propose a two-stage RL
training framework by first enhancing the visual perception of geometric
structures, then fostering reasoning capabilities. Applied to
Qwen2.5-VL-3B-Instruct, our two-stage training improves geometric reasoning by
9.7% and geometric problem solving by 9.1%, compared to the direct reasoning
training approach. Our method also generalizes to other vision-intensive
domains like figure understanding, highlighting the importance of perceptual
grounding in effective MLLM reasoning.

</details>


### [86] [Filling in the Clinical Gaps in Benchmark: Case for HealthBench for the Japanese medical system](https://arxiv.org/abs/2509.17444)
*Shohei Hisada,Endo Sunao,Himi Yamato,Shoko Wakamiya,Eiji Aramaki*

Main category: cs.CL

TL;DR: 本研究探讨了将HealthBench医疗基准应用于日本语境的适用性，发现直接翻译基准存在局限性，需要本地化适应。


<details>
  <summary>Details</summary>
Motivation: 日本语医疗LLM评估资源有限，现有方法主要依赖翻译的多选题，缺乏针对日本临床指南、医疗系统和文化规范的评估框架。

Method: 1) 使用机器翻译的HealthBench 5,000个场景评估多语言模型和日本本土模型；2) 采用LLM-as-a-Judge方法系统分类基准场景和评分标准，识别与日本语境不匹配的"上下文差距"。

Result: GPT-4.1因评分标准不匹配出现性能下降，日本本土模型在临床完整性方面显著失败；多数场景适用，但大量评分标准需要本地化。

Conclusion: 直接翻译基准存在局限性，迫切需要开发上下文感知的本地化适应版本（J-HealthBench），以确保日本医疗LLM评估的可靠性和安全性。

Abstract: This study investigates the applicability of HealthBench, a large-scale,
rubric-based medical benchmark, to the Japanese context. While robust
evaluation frameworks are crucial for the safe development of medical LLMs,
resources in Japanese remain limited, often relying on translated
multiple-choice questions. Our research addresses this gap by first
establishing a performance baseline, applying a machine-translated version of
HealthBench's 5,000 scenarios to evaluate both a high-performing multilingual
model (GPT-4.1) and a Japanese-native open-source model (LLM-jp-3.1). Second,
we employ an LLM-as-a-Judge approach to systematically classify the benchmark's
scenarios and rubric criteria, identifying "contextual gaps" where content is
misaligned with Japan's clinical guidelines, healthcare systems, or cultural
norms. Our findings reveal a modest performance drop in GPT-4.1 due to rubric
mismatches and a significant failure in the Japanese-native model, which lacked
the required clinical completeness. Furthermore, our classification indicates
that while the majority of scenarios are applicable, a substantial portion of
the rubric criteria requires localization. This work underscores the
limitations of direct benchmark translation and highlights the urgent need for
a context-aware, localized adaptation, a J-HealthBench, to ensure the reliable
and safe evaluation of medical LLMs in Japan.

</details>


### [87] [Semantic Reformulation Entropy for Robust Hallucination Detection in QA Tasks](https://arxiv.org/abs/2509.17445)
*Chaodong Tong,Qi Zhang,Lei Jiang,Yanbing Liu,Nannan Sun,Wei Li*

Main category: cs.CL

TL;DR: 本文提出了语义重构熵（SRE）方法，通过输入侧语义重构和渐进式能量混合聚类，改进大语言模型的不确定性估计，有效检测幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于熵的语义级不确定性估计方法受限于采样噪声和变长答案的不稳定聚类，难以可靠检测大语言模型产生的幻觉（流畅但事实错误的输出）。

Method: 提出语义重构熵（SRE）：1）输入侧语义重构生成忠实释义，扩展估计空间，减少解码器表面倾向的偏差；2）渐进式能量混合聚类稳定语义分组。

Result: 在SQuAD和TriviaQA数据集上的实验表明，SRE优于强基线方法，提供更鲁棒和可泛化的幻觉检测。

Conclusion: 结合输入多样化和多信号聚类能显著增强语义级不确定性估计，为解决大语言模型的幻觉问题提供了有效方案。

Abstract: Reliable question answering with large language models (LLMs) is challenged
by hallucinations, fluent but factually incorrect outputs arising from
epistemic uncertainty. Existing entropy-based semantic-level uncertainty
estimation methods are limited by sampling noise and unstable clustering of
variable-length answers. We propose Semantic Reformulation Entropy (SRE), which
improves uncertainty estimation in two ways. First, input-side semantic
reformulations produce faithful paraphrases, expand the estimation space, and
reduce biases from superficial decoder tendencies. Second, progressive,
energy-based hybrid clustering stabilizes semantic grouping. Experiments on
SQuAD and TriviaQA show that SRE outperforms strong baselines, providing more
robust and generalizable hallucination detection. These results demonstrate
that combining input diversification with multi-signal clustering substantially
enhances semantic-level uncertainty estimation.

</details>


### [88] [SLAyiNG: Towards Queer Language Processing](https://arxiv.org/abs/2509.17449)
*Leonor Veloso,Lea Hirlimann,Philipp Wicke,Hinrich Schütze*

Main category: cs.CL

TL;DR: 该论文提出了SLAyiNG数据集，这是第一个包含从字幕、社交媒体帖子和播客中提取的标注酷儿俚语的数据集，旨在解决LLMs在处理酷儿俚语时可能产生的误解和负面响应问题。


<details>
  <summary>Details</summary>
Motivation: 酷儿俚语在用户交互中常被LLMs误判为仇恨言论或引发负面响应，但目前缺乏专门针对酷儿俚语的高质量标注基准数据集。

Method: 通过收集俚语术语和定义，从字幕、社交媒体和播客中抓取反映这些术语使用的示例，并进行人工标注过程。初步结果通过计算人工标注者和OpenAI模型o3-mini的标注者间一致性来评估词义消歧任务性能。

Result: 达到平均Krippendorff's alpha为0.746，表明最先进的推理模型可以作为预过滤工具，但酷儿语言数据的复杂性和敏感性需要专家和社区驱动的标注工作。

Conclusion: SLAyiNG数据集填补了酷儿俚语处理领域的空白，强调了在敏感语言数据处理中专家标注的重要性，同时展示了先进模型在辅助标注中的潜力。

Abstract: Knowledge of slang is a desirable feature of LLMs in the context of user
interaction, as slang often reflects an individual's social identity. Several
works on informal language processing have defined and curated benchmarks for
tasks such as detection and identification of slang. In this paper, we focus on
queer slang. Queer slang can be mistakenly flagged as hate speech or can evoke
negative responses from LLMs during user interaction. Research efforts so far
have not focused explicitly on queer slang. In particular, detection and
processing of queer slang have not been thoroughly evaluated due to the lack of
a high-quality annotated benchmark. To address this gap, we curate SLAyiNG, the
first dataset containing annotated queer slang derived from subtitles, social
media posts, and podcasts, reflecting real-world usage. We describe our data
curation process, including the collection of slang terms and definitions,
scraping sources for examples that reflect usage of these terms, and our
ongoing annotation process. As preliminary results, we calculate
inter-annotator agreement for human annotators and OpenAI's model o3-mini,
evaluating performance on the task of sense disambiguation. Reaching an average
Krippendorff's alpha of 0.746, we argue that state-of-the-art reasoning models
can serve as tools for pre-filtering, but the complex and often sensitive
nature of queer language data requires expert and community-driven annotation
efforts.

</details>


### [89] [Codifying Natural Langauge Tasks](https://arxiv.org/abs/2509.17455)
*Haoyang Chen,Kumiko Tanaka-Ishii*

Main category: cs.CL

TL;DR: ICRAG框架通过将自然语言转化为可执行程序，利用外部知识进行迭代优化，在13个基准测试中实现了最高161.1%的相对改进。


<details>
  <summary>Details</summary>
Motivation: 探索文本到代码方法在解决现实世界问题（如法律判决和医疗问答）中的应用，区别于以往工作，利用程序生成提供的显式推理能力。

Method: 提出ICRAG框架，通过从领域资源和GitHub获取外部知识，将自然语言转化为可执行程序，并进行迭代优化。

Result: 在13个基准测试中，ICRAG实现了最高161.1%的相对改进，并对生成的代码和外部知识影响进行了详细分析。

Conclusion: 讨论了将文本到代码方法应用于现实世界自然语言任务的局限性，展示了该方法的潜力和挑战。

Abstract: We explore the applicability of text-to-code to solve real-world problems
that are typically solved in natural language, such as legal judgment and
medical QA. Unlike previous works, our approach leverages the explicit
reasoning provided by program generation. We present ICRAG, a framework that
transforms natural language into executable programs through iterative
refinement using external knowledge from domain resources and GitHub. Across 13
benchmarks, ICRAG achieves up to 161.1\% relative improvement. We provide a
detailed analysis of the generated code and the impact of external knowledge,
and we discuss the limitations of applying text-to-code approaches to
real-world natural language tasks.

</details>


### [90] [PRINCIPLES: Synthetic Strategy Memory for Proactive Dialogue Agents](https://arxiv.org/abs/2509.17459)
*Namyoung Kim,Kai Tzu-iunn Ong,Yeonjun Hwang,Minseok Kang,Iiseo Jihn,Gayoung Kim,Minju Kim,Jinyoung Yeo*

Main category: cs.CL

TL;DR: PRINCIPLES是一种基于离线自博弈模拟的合成策略记忆方法，用于提升大语言模型在主动对话中的策略规划能力，无需额外训练和数据标注


<details>
  <summary>Details</summary>
Motivation: 现有主动对话策略规划方法存在策略覆盖有限、规划偏好偏差以及依赖昂贵额外训练的问题

Method: 通过离线自博弈模拟生成合成策略记忆，作为可重用知识在推理时指导策略规划

Result: 在情感支持和说服领域均优于强基线模型，在扩展和多样化评估设置中保持鲁棒性

Conclusion: PRINCIPLES方法有效解决了主动对话策略规划的局限性，提供了一种无需额外训练的高效解决方案

Abstract: Dialogue agents based on large language models (LLMs) have shown promising
performance in proactive dialogue, which requires effective strategy planning.
However, existing approaches to strategy planning for proactive dialogue face
several limitations: limited strategy coverage, preference bias in planning,
and reliance on costly additional training. To address these, we propose
PRINCIPLES: a synthetic strategy memory for proactive dialogue agents.
PRINCIPLES is derived through offline self-play simulations and serves as
reusable knowledge that guides strategy planning during inference, eliminating
the need for additional training and data annotation. We evaluate PRINCIPLES in
both emotional support and persuasion domains, demonstrating consistent
improvements over strong baselines. Furthermore, PRINCIPLES maintains its
robustness across extended and more diverse evaluation settings. See our
project page at https://huggingface.co/spaces/kimnamssya/Principles.

</details>


### [91] [Diagnosing Model Editing via Knowledge Spectrum](https://arxiv.org/abs/2509.17482)
*Tsung-Hsuan Pan,Chung-Chi Chen,Hen-Hsen Huang,Hsin-Hsi Chen*

Main category: cs.CL

TL;DR: 本文提出了知识谱系框架来系统分类知识特性，并开发了知识诊断框架来根据知识难度自适应调整编辑强度，显著提升了模型编辑的成功率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有模型编辑方法在修改预训练语言模型中的事实知识时，常常产生不可预测的副作用，而目标知识的内在特性这一重要因素尚未得到充分研究。

Method: 首先提出知识谱系框架，基于知识的现实世界流行度、模型预编辑熟悉度和问题语言结构来分类知识；然后开发知识诊断框架，根据诊断出的知识难度自适应调整编辑强度。

Result: 实证分析表明知识特性是编辑成功和稳定性的强预测因子，知识诊断框架显著提高了困难编辑的成功率并优化了计算资源。

Conclusion: 这项工作为理解模型编辑的调控因素提供了更全面的视角，表明考虑知识内在特性对提升编辑效果至关重要。

Abstract: Model editing, the process of efficiently modifying factual knowledge in
pre-trained language models, is critical for maintaining their accuracy and
relevance. However, existing editing methods often introduce unintended side
effects, degrading model performance in unpredictable ways. While much research
has focused on improving editing algorithms, the role of the target knowledge's
intrinsic properties remains a significant, underexplored factor. This paper
addresses this gap by first proposing the ``Knowledge Spectrum,'' a systematic
framework for categorizing knowledge based on its real-world popularity, the
model's pre-edit familiarity, and the linguistic structure of the eliciting
question. Our empirical analysis reveals that these characteristics are strong
predictors of editing success and stability. Informed by these findings, we
introduce the ``Knowledge-Diagnostic Framework,'' an adaptive strategy that
tailors editing intensity to the diagnosed difficulty of a knowledge item. We
demonstrate that this framework significantly improves success rates for
challenging edits while optimizing computational resources. Our work provides a
more comprehensive understanding of the factors governing model editing.

</details>


### [92] [AttnComp: Attention-Guided Adaptive Context Compression for Retrieval-Augmented Generation](https://arxiv.org/abs/2509.17486)
*Lvzhou Luo,Yixuan Cao,Ping Luo*

Main category: cs.CL

TL;DR: AttnComp是一个自适应、高效且上下文感知的压缩框架，通过利用LLM的注意力机制识别相关信息，采用Top-P压缩算法保留累积注意力权重超过预定义阈值的最小文档集，从而提高检索增强生成的事实准确性。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成通过引入外部上下文提高了大型语言模型的事实准确性，但常常受到不相关检索内容的干扰。现有的上下文压缩方法难以自适应调整不同上下文的压缩率、保持低延迟并整合多个文档的信息。

Method: AttnComp利用LLM的注意力机制识别相关信息，采用Top-P压缩算法保留累积注意力权重超过预定义阈值的最小文档集。除了压缩功能外，还通过评估检索内容的整体相关性来估计响应置信度。

Result: 实验表明，AttnComp在压缩率和延迟方面优于现有的压缩方法和未压缩基线，实现了更高的准确性和显著的压缩效果。

Conclusion: AttnComp通过自适应压缩和置信度估计，有效解决了检索增强生成中不相关内容的问题，提高了LLM的事实准确性和响应可靠性。

Abstract: Retrieval-augmented generation improves the factual accuracy of Large
Language Models (LLMs) by incorporating external context, but often suffers
from irrelevant retrieved content that hinders effectiveness. Context
compression addresses this issue by filtering out irrelevant information from
context before LLM generation. However, existing methods struggle to adaptively
adjust compression rates for different context, maintain low latency and
integrate information across multiple documents. To overcome these limitations,
We introduce AttnComp, an adaptive, efficient and context-aware compression
framework. By leveraging the attention mechanism of LLMs to identify relevant
information, AttnComp employs a Top-P compression algorithm to retain the
minimal set of documents whose cumulative attention weights exceeds a
predefined threshold. In addition to compression, AttnComp estimates response
confidence by assessing the overall relevance of the retrieved content,
enabling users to gauge response reliability. Experiments demonstrate that
AttnComp outperforms existing compression methods and uncompressed baselines,
achieving higher accuracy with substantial compression rates and lower latency.

</details>


### [93] [MapCoder-Lite: Squeezing Multi-Agent Coding into a Single Small LLM](https://arxiv.org/abs/2509.17489)
*Woongkyu Lee,Junhee Cho,Jungwook Choi*

Main category: cs.CL

TL;DR: MapCoder-Lite通过将单个7B模型升级为四个角色专业化代理（检索器、规划器、编码器、调试器），仅使用rank-32 LoRA适配器，在小型开源模型上实现了高质量的多代理代码生成。


<details>
  <summary>Details</summary>
Motivation: 现有多代理解决方案要么依赖昂贵的大规模模型（>30B），要么在缩小到小型开源模型时性能崩溃，需要开发更高效的解决方案。

Method: 使用三种轻量级技术：轨迹蒸馏修复检索和调试中的格式脆弱性，监督引导校正增强规划和编码代理，代理级LoRA微调实现内存高效专业化。

Result: 在xCodeEval、APPS和CodeContests上的评估显示，MapCoder-Lite将xCodeEval准确率从13.2%提升至28.3%，消除所有格式失败，接近32B基线，同时减少4倍GPU内存和令牌生成时间。

Conclusion: 仔细的代理级微调可以在小型语言模型上释放高质量的多代理编码能力。

Abstract: Large language models (LLMs) have advanced code generation from
single-function tasks to competitive-programming problems, but existing
multi-agent solutions either rely on costly large-scale ($>$ 30B) models or
collapse when downsized to small open-source models. We present MapCoder-Lite,
which upgrades a single 7B model into four role-specialised agents-retriever,
planner, coder, and debugger-using only rank-32, role-specific LoRA adapters
($<3\%$ extra parameters). Three lightweight techniques make this possible: (i)
trajectory distillation from strong LLMs fixes format fragility in retrieval
and debugging, (ii) supervisor-guided correction strengthens planning and
coding agents, and (iii) agent-wise LoRA fine-tuning delivers memory-efficient
specialisation. Comprehensive evaluation on xCodeEval, APPS, and CodeContests
shows that MapCoder-Lite more than doubles xCodeEval accuracy (from $13.2\%$ to
$28.3\%$), eliminates all format failures, and closes to within six points of a
32B baseline while cutting GPU memory and token-generation time by $4\times$.
These results demonstrate that careful agent-wise fine-tuning unleashes
high-quality multi-agent coding on a small language model.

</details>


### [94] [Enhancing Cross-Lingual Transfer through Reversible Transliteration: A Huffman-Based Approach for Low-Resource Languages](https://arxiv.org/abs/2509.17493)
*Wenhao Zhuang,Yuan Sun,Xiaobing Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种结合字符音译和霍夫曼编码的完整框架，用于提升大语言模型对低资源语言的处理能力，实现了存储压缩、无损转换和训练效率提升。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在多语言处理中，对于使用非拉丁文字的低资源语言支持不足，缺乏有效的音译集成框架。

Method: 创新性地结合字符音译与霍夫曼编码，设计完整的音译框架，实现文本压缩和无损转换。

Result: 实验表明该方法能减少50%文件大小和50-80%的token数量，在文本分类、机器阅读理解等任务中显著提升低资源语言处理能力。

Conclusion: 该框架有效解决了低资源语言处理问题，具有压缩、准确、高效和可扩展的优势，为大语言模型的多语言应用提供了实用解决方案。

Abstract: As large language models (LLMs) are trained on increasingly diverse and
extensive multilingual corpora, they demonstrate cross-lingual transfer
capabilities. However, these capabilities often fail to effectively extend to
low-resource languages, particularly those utilizing non-Latin scripts. While
transliterating low-resource languages into Latin script presents a natural
solution, there currently lacks a comprehensive framework for integrating
transliteration into LLMs training and deployment. Taking a pragmatic approach,
this paper innovatively combines character transliteration with Huffman coding
to design a complete transliteration framework. Our proposed framework offers
the following advantages: 1) Compression: Reduces storage requirements for
low-resource language content, achieving up to 50% reduction in file size and
50-80% reduction in token count. 2) Accuracy: Guarantees 100% lossless
conversion from transliterated text back to the source language. 3) Efficiency:
Eliminates the need for vocabulary expansion for low-resource languages,
improving training and inference efficiency. 4) Scalability: The framework can
be extended to other low-resource languages. We validate the effectiveness of
our framework across multiple downstream tasks, including text classification,
machine reading comprehension, and machine translation. Experimental results
demonstrate that our method significantly enhances the model's capability to
process low-resource languages while maintaining performance on high-resource
languages. Our data and code are publicly available at
https://github.com/CMLI-NLP/HuffmanTranslit.

</details>


### [95] [CorefInst: Leveraging LLMs for Multilingual Coreference Resolution](https://arxiv.org/abs/2509.17505)
*Tuğba Pamay Arslan,Emircan Erol,Gülşen Eryiğit*

Main category: cs.CL

TL;DR: 本文提出了首个多语言共指消解方法，利用仅解码器LLM处理显式和零指代，通过五种指令集和受控推理方法，在Llama 3.1、Gemma 2和Mistral 0.3上验证，最佳模型在CorefUD v1.2数据集上平均优于现有最佳模型2个百分点。


<details>
  <summary>Details</summary>
Motivation: 传统共指消解方法受限于任务特定架构和编码器语言模型，需要大量训练且缺乏适应性。

Method: 使用仅解码器LLM，通过五种不同指令集建模共指消解任务，采用受控推理方法，在三个LLM上进行评估。

Result: 经过合适指令集微调的LLM可以超越最先进的任务特定架构，最佳模型（完全微调的Llama 3.1）在CorefUD v1.2数据集上平均优于现有最佳模型2个百分点。

Conclusion: LLM通过适当的指令微调可以成为有效的多语言共指消解工具，超越传统任务特定方法。

Abstract: Coreference Resolution (CR) is a crucial yet challenging task in natural
language understanding, often constrained by task-specific architectures and
encoder-based language models that demand extensive training and lack
adaptability. This study introduces the first multilingual CR methodology which
leverages decoder-only LLMs to handle both overt and zero mentions. The article
explores how to model the CR task for LLMs via five different instruction sets
using a controlled inference method. The approach is evaluated across three
LLMs; Llama 3.1, Gemma 2, and Mistral 0.3. The results indicate that LLMs, when
instruction-tuned with a suitable instruction set, can surpass state-of-the-art
task-specific architectures. Specifically, our best model, a fully fine-tuned
Llama 3.1 for multilingual CR, outperforms the leading multilingual CR model
(i.e., Corpipe 24 single stage variant) by 2 pp on average across all languages
in the CorefUD v1.2 dataset collection.

</details>


### [96] [Leveraging Audio-Visual Data to Reduce the Multilingual Gap in Self-Supervised Speech Models](https://arxiv.org/abs/2509.17523)
*María Andrea Cruz Blandón,Zakaria Aldeneh,Jie Chi,Maureen de Seyssel*

Main category: cs.CL

TL;DR: 本文提出了一种通过引入有限视觉基础来减少双语语音自监督学习模型性能差距的新方法。


<details>
  <summary>Details</summary>
Motivation: 多语言自监督学习模型在每种语言上的表现往往不如单语言模型，特别是在双语等多语言场景中。

Method: 在双语语音自监督学习模型中引入有限的视觉基础。

Result: 视觉基础对单语和双语模型都有益处，特别是对双语模型效果显著，将零样本音素识别的多语言性能差距从31.5%降低到8.04%。

Conclusion: 有限的视觉基础可以有效缩小多语言自监督学习模型的性能差距。

Abstract: Self-supervised learning (SSL) has made significant advances in speech
representation learning. Models like wav2vec 2.0 and HuBERT have achieved
state-of-the-art results in tasks such as speech recognition, particularly in
monolingual settings. However, multilingual SSL models tend to underperform
their monolingual counterparts on each individual language, especially in
multilingual scenarios with few languages such as the bilingual setting. In
this work, we investigate a novel approach to reduce this performance gap by
introducing limited visual grounding into bilingual speech SSL models. Our
results show that visual grounding benefits both monolingual and bilingual
models, with especially pronounced gains for the latter, reducing the
multilingual performance gap on zero-shot phonetic discrimination from 31.5%
for audio-only models to 8.04% with grounding.

</details>


### [97] [Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning](https://arxiv.org/abs/2509.17552)
*Tianle Zhang,Wanlong Fang,Jonathan Woo,Paridhi Latawa,Deepak A. Subramanian,Alvin Chan*

Main category: cs.CL

TL;DR: 本文提出了ICRL（In-Context Representation Learning），这是一个无需训练即可将非文本模态表示整合到基于文本的大语言模型中的框架，通过少量样本学习实现多模态推理。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要额外的监督训练来整合非文本模态表示，限制了在新领域和模态上的即时适应能力。本文探索无需训练即可整合非文本基础模型表示到文本LLMs的可行性。

Method: 提出ICRL框架，用基础模型表示替换文本输入，使LLM能够通过少量样本学习进行多模态推理，无需微调。在分子领域任务上进行评估，研究三个核心问题：表示映射方法、性能影响因素和有效机制。

Result: ICRL是首个无需训练即可整合非文本模态表示到文本LLMs的框架，为可适应的多模态泛化提供了有前景的方向。

Conclusion: ICRL证明了无需训练即可整合非文本模态表示的可行性，为LLMs的多模态扩展提供了新的训练自由方法。

Abstract: The remarkable performance of Large Language Models (LLMs) can be enhanced
with test-time computation, which relies on external tools and even other deep
learning models. However, existing approaches for integrating non-text modality
representations into LLMs typically require additional costly supervised
training, restricting on-the-fly adaptation to new domains and modalities. In
this work, we explore the feasibility of integrating representations from
non-text foundational models (FMs) into text-based LLMs in a training-free
manner. We propose In-Context Representation Learning (ICRL) as a
proof-of-concept to allow LLMs to adaptively utilize non-text modality
representations with few-shot learning. Unlike traditional in-context learning,
which incorporates text-label pairs, ICRL replaces text inputs with FM
representations, enabling the LLM to perform multi-modal inference without
fine-tuning. We evaluate ICRL on a suite of tasks in the molecular domain,
investigating three core research questions: (i) how to map FM representations
into LLMs in a training-free manner, (ii) what factors influence ICRL
performance, and (iii) what mechanisms underlie the effectiveness of ICRL. To
the best of our knowledge, ICRL is the first training-free framework for
integrating non-text modality representations into text-based LLMs, presenting
a promising direction for adaptable, multi-modal generalization.

</details>


### [98] [Specification-Aware Machine Translation and Evaluation for Purpose Alignment](https://arxiv.org/abs/2509.17559)
*Yoko Kayano,Saku Sugawara*

Main category: cs.CL

TL;DR: 该论文提出将专业翻译中的规范要求（specifications）显式整合到机器翻译工作流程中，通过实验证明基于规范的LLM翻译在专业文本翻译中优于官方人工翻译。


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译研究往往将专业翻译中的规范要求（如客户需求、沟通目标）隐式处理，而专业实践中这些规范对翻译质量至关重要。论文旨在弥合这一差距。

Method: 基于翻译研究理论构建规范感知的机器翻译和评估框架，应用于33家上市公司投资者关系文本翻译实验，比较五种翻译类型（包括官方人工翻译和基于提示的LLM翻译），采用专家错误分析、用户偏好排名和自动指标评估。

Result: 实验结果显示，基于规范的LLM翻译在人类评估中持续优于官方人工翻译，揭示了感知质量与预期质量之间的差距。

Conclusion: 将规范整合到机器翻译工作流程中，结合人工监督，可以按照专业实践的方式提高翻译质量。

Abstract: In professional settings, translation is guided by communicative goals and
client needs, often formalized as specifications. While existing evaluation
frameworks acknowledge the importance of such specifications, these
specifications are often treated only implicitly in machine translation (MT)
research. Drawing on translation studies, we provide a theoretical rationale
for why specifications matter in professional translation, as well as a
practical guide to implementing specification-aware MT and evaluation. Building
on this foundation, we apply our framework to the translation of investor
relations texts from 33 publicly listed companies. In our experiment, we
compare five translation types, including official human translations and
prompt-based outputs from large language models (LLMs), using expert error
analysis, user preference rankings, and an automatic metric. The results show
that LLM translations guided by specifications consistently outperformed
official human translations in human evaluations, highlighting a gap between
perceived and expected quality. These findings demonstrate that integrating
specifications into MT workflows, with human oversight, can improve translation
quality in ways aligned with professional practice.

</details>


### [99] [Asking a Language Model for Diverse Responses](https://arxiv.org/abs/2509.17570)
*Sergey Troshin,Irina Saparina,Antske Fokkens,Vlad Niculae*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型中候选采样器的不同策略，对比了并行采样、枚举采样和迭代采样在质量、多样性和效率方面的表现。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越依赖显式推理链并能为给定上下文生成多个合理响应，需要研究不同采样策略对响应多样性和质量的影响。

Method: 在相同预算条件下，比较了三种采样策略：祖先（并行）采样、枚举采样（一次生成n个候选）和迭代采样（顺序生成候选并基于当前响应集进行条件生成）。

Result: 实证结果表明，枚举和迭代策略在保持可比质量的同时能产生更高的多样性。

Conclusion: 简单的非独立采样策略有潜力在不牺牲生成质量的情况下提高响应多样性。

Abstract: Large language models increasingly rely on explicit reasoning chains and can
produce multiple plausible responses for a given context. We study the
candidate sampler that produces the set of plausible responses contrasting the
ancestral (parallel) sampling against two alternatives: enumeration, which asks
the model to produce $n$ candidates in one pass, and iterative sampling, which
proposes candidates sequentially while conditioning on the currently generated
response set. Under matched budgets, we compare these samplers on quality,
lexical and computation flow diversity, and efficiency. Our empirical results
demonstrate that enumeration and iterative strategies result in higher
diversity at comparable quality. Our findings highlight the potential of simple
non-independent sampling strategies to improve response diversity without
sacrificing generation quality.

</details>


### [100] [MSCoRe: A Benchmark for Multi-Stage Collaborative Reasoning in LLM Agents](https://arxiv.org/abs/2509.17628)
*Yuzhen Lei,Hongbin Xie,Jiaxing Zhao,Shuangxue Liu,Xuan Song*

Main category: cs.CL

TL;DR: MSCoRe是一个新的多阶段推理基准，包含126,696个领域特定的QA实例，用于评估LLM在复杂多阶段场景中的推理和协调能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注单领域或孤立任务，忽视了LLM在多阶段协作和优化方面的能力，需要新的评估框架来填补这一空白。

Method: 采用三阶段流水线创建数据集：动态采样、迭代问答生成和多层次质量评估，任务按阶段覆盖度和复杂度分为三个难度级别。

Result: 商业模型在所有任务和场景中表现最佳，但简单任务和复杂任务之间的ROUGE分数差距明显，且模型性能受噪声数据负面影响。

Conclusion: MSCoRe为社区评估和改进LLM代理的多阶段推理能力提供了宝贵资源，揭示了当前模型在复杂多阶段任务中的局限性。

Abstract: Large Language Models (LLMs) have excelled in question-answering (QA) tasks
within single domains. However, their reasoning and coordination capabilities
in complex, multi-stage scenarios remain underexplored. Existing benchmarks
typically focus on isolated tasks or narrow domains, overlooking models'
abilities for multi-stage collaboration and optimization without explicit
external guidance. To bridge this gap, we propose \textbf{MSCoRe}, a novel
benchmark comprising 126696 domain-specific QA instances spanning scenarios in
automotive, pharmaceutical, electronics, and energy sectors. The dataset is
created using a structured three-phase pipeline: dynamic sampling, iterative
question-answer generation, and a multi-level quality assessment to ensure data
quality. Tasks are further categorized into three difficulty levels according
to stage coverage and complexity. With MSCoRe, we have conducted a
comprehensive evaluation of various state-of-the-art LLM agents. The commercial
models performed best across all tasks and scenarios, but a notable gap in
ROUGE scores remains between simple and complex tasks. We also tested the
models' robustness and found that their performance is negatively affected by
noisy data. MSCoRe provides a valuable new resource for the community to
evaluate and improve multi-stage reasoning in LLM agents. The code and data are
available at https://github.com/D3E0-source/MSCoRE.

</details>


### [101] [AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?](https://arxiv.org/abs/2509.17641)
*Hyunjong Ok,Suho Yoo,Hyeonjun Kim,Jaeho Lee*

Main category: cs.CL

TL;DR: AuditoryBench++是一个评估语言模型听觉常识推理能力的基准测试，AIR-CoT方法通过听觉想象推理提升模型表现


<details>
  <summary>Details</summary>
Motivation: 人类即使没有直接听到声音也能推理听觉属性，但语言模型缺乏这种听觉常识能力，限制了多模态交互效果

Method: 提出AIR-CoT方法，通过特殊标记的跨度检测和知识注入，在推理过程中生成和整合听觉信息

Result: 实验表明AIR-CoT在最近的LLM和多模态LLM上表现优于现成模型和听觉知识增强模型

Conclusion: 该研究为提升语言模型听觉推理能力提供了有效基准和方法，有助于改善多模态交互效果

Abstract: Even without directly hearing sounds, humans can effortlessly reason about
auditory properties, such as pitch, loudness, or sound-source associations,
drawing on auditory commonsense. In contrast, language models often lack this
capability, limiting their effectiveness in multimodal interactions. As an
initial step to address this gap, we present AuditoryBench++, a comprehensive
benchmark for evaluating auditory knowledge and reasoning in text-only
settings. The benchmark encompasses tasks that range from basic auditory
comparisons to contextually grounded reasoning, enabling fine-grained analysis
of how models process and integrate auditory concepts. In addition, we
introduce AIR-CoT, a novel auditory imagination reasoning method that generates
and integrates auditory information during inference through span detection
with special tokens and knowledge injection. Extensive experiments with recent
LLMs and Multimodal LLMs demonstrate that AIR-CoT generally outperforms both
the off-the-shelf models and those augmented with auditory knowledge. The
project page is available at https://auditorybenchpp.github.io.

</details>


### [102] [Crosslingual Optimized Metric for Translation Assessment of Indian Languages](https://arxiv.org/abs/2509.17667)
*Arafat Ahsan,Vandan Mujadia,Pruthwik Mishra,Yash Bhaskar,Dipti Misra Sharma*

Main category: cs.CL

TL;DR: 本文针对印度语言翻译自动评估的挑战，创建了一个包含13种印度语言、21个翻译方向的大规模人工评估数据集，并训练了名为COMTAIL的神经翻译评估指标。


<details>
  <summary>Details</summary>
Motivation: 传统基于字符串的指标（如BLEU）存在局限性，而基于学习的神经指标在大多数语言中受限于缺乏黄金评估数据，特别是在印度语言等资源较少语言中。

Method: 创建了大规模人工评估数据集，涵盖13种印度语言的21个翻译方向，并在此基础上训练COMTAIL神经翻译评估指标。

Result: 最佳性能的COMTAIL变体在评估至少包含一种印度语言的翻译对时，显著优于先前的最先进方法。通过消融研究揭示了该指标对领域、翻译质量和语言分组的敏感性。

Conclusion: COMTAIL数据集和相应指标模型的发布为印度语言翻译评估提供了重要资源，解决了资源较少语言自动评估的挑战。

Abstract: Automatic evaluation of translation remains a challenging task owing to the
orthographic, morphological, syntactic and semantic richness and divergence
observed across languages. String-based metrics such as BLEU have previously
been extensively used for automatic evaluation tasks, but their limitations are
now increasingly recognized. Although learned neural metrics have helped
mitigate some of the limitations of string-based approaches, they remain
constrained by a paucity of gold evaluation data in most languages beyond the
usual high-resource pairs. In this present work we address some of these gaps.
We create a large human evaluation ratings dataset for 13 Indian languages
covering 21 translation directions and then train a neural translation
evaluation metric named Cross-lingual Optimized Metric for Translation
Assessment of Indian Languages (COMTAIL) on this dataset. The best performing
metric variants show significant performance gains over previous
state-of-the-art when adjudging translation pairs with at least one Indian
language. Furthermore, we conduct a series of ablation studies to highlight the
sensitivities of such a metric to changes in domain, translation quality, and
language groupings. We release both the COMTAIL dataset and the accompanying
metric models.

</details>


### [103] [PG-CE: A Progressive Generation Dataset with Constraint Enhancement for Controllable Text Generation](https://arxiv.org/abs/2509.17669)
*Yan Zhuang,Yuan Sun*

Main category: cs.CL

TL;DR: 本文提出PG-CE方法，通过类型预测、约束构建和引导生成三个步骤，使用约束生成模型动态构建多维度约束来提升可控文本生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决传统可控文本生成方法的局限性，提高系统可靠性和用户体验。

Method: 将可控文本生成任务分解为类型预测、约束构建和引导生成三个步骤，使用约束生成模型动态构建语气、表达风格和主题焦点等多维度约束。

Result: 实验表明PG-CE在多种场景下显著提升生成质量，同时保持文本可控性、主题相关性和响应实用性。构建了包含90,000个约束-文本对的数据集。

Conclusion: PG-CE方法有效提升了可控文本生成的性能，满足了实际应用需求。

Abstract: With the rapid development of Large Language Models (LLMs), Controllable Text
Generation (CTG) has become a critical technology for enhancing system
reliability and user experience. Addressing the limitations of traditional
methods, this paper proposes the PG-CE (Progressive Generation with Constraint
Enhancement) approach, which decomposes CTG tasks into three steps: type
prediction, constraint construction, and guided generation. This method employs
constraint generation models to dynamically build multi-dimensional constraints
including tone, expression style, and thematic focus to guide output.
Experiments demonstrate that PG-CE significantly improves generation quality
across multiple scenarios while maintaining text controllability, thematic
relevance, and response practicality. The research developed a dataset
containing 90,000 constraint-text pairs (with an 8:2 ratio between daily and
other topics), effectively reflecting real-world application requirements.

</details>


### [104] [Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG Applications](https://arxiv.org/abs/2509.17671)
*Selva Taş,Mahmut El Huseyni,Özay Ezerceli,Reyhan Bayraktar,Fatma Betül Terzioğlu*

Main category: cs.CL

TL;DR: 该论文提出了Turk-LettuceDetect，这是首个专门为土耳其语RAG应用设计的幻觉检测模型套件，基于LettuceDetect框架，在机器翻译的RAGTruth基准数据集上训练了三种编码器架构，其中ModernBERT模型在完整测试集上达到0.7266的F1分数。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）存在幻觉问题，生成看似合理但事实错误的信息。虽然检索增强生成（RAG）系统试图通过基于外部知识来缓解这一问题，但对于土耳其语等形态复杂、资源稀缺的语言，幻觉问题仍然存在。

Method: 将幻觉检测制定为令牌级分类任务，微调了三种编码器架构：土耳其语特定的ModernBERT、TurkEmbed4STS和多语言EuroBERT。使用包含17,790个实例的机器翻译版RAGTruth基准数据集进行训练，涵盖问答、数据到文本生成和摘要任务。

Result: 基于ModernBERT的模型在完整测试集上达到0.7266的F1分数，在结构化任务上表现尤为出色。模型保持计算效率，支持长达8,192个令牌的上下文，适合实时部署。比较分析显示，最先进的LLMs虽然召回率高，但由于过度生成幻觉内容而精度低。

Conclusion: 通过发布模型和翻译数据集，这项工作填补了多语言NLP的关键空白，为土耳其语和其他语言开发更可靠、可信赖的AI应用奠定了基础。

Abstract: The widespread adoption of Large Language Models (LLMs) has been hindered by
their tendency to hallucinate, generating plausible but factually incorrect
information. While Retrieval-Augmented Generation (RAG) systems attempt to
address this issue by grounding responses in external knowledge, hallucination
remains a persistent challenge, particularly for morphologically complex,
low-resource languages like Turkish. This paper introduces Turk-LettuceDetect,
the first suite of hallucination detection models specifically designed for
Turkish RAG applications. Building on the LettuceDetect framework, we formulate
hallucination detection as a token-level classification task and fine-tune
three distinct encoder architectures: a Turkish-specific ModernBERT,
TurkEmbed4STS, and multilingual EuroBERT. These models were trained on a
machine-translated version of the RAGTruth benchmark dataset containing 17,790
instances across question answering, data-to-text generation, and summarization
tasks. Our experimental results show that the ModernBERT-based model achieves
an F1-score of 0.7266 on the complete test set, with particularly strong
performance on structured tasks. The models maintain computational efficiency
while supporting long contexts up to 8,192 tokens, making them suitable for
real-time deployment. Comparative analysis reveals that while state-of-the-art
LLMs demonstrate high recall, they suffer from low precision due to
over-generation of hallucinated content, underscoring the necessity of
specialized detection mechanisms. By releasing our models and translated
dataset, this work addresses a critical gap in multilingual NLP and establishes
a foundation for developing more reliable and trustworthy AI applications for
Turkish and other languages.

</details>


### [105] [When TableQA Meets Noise: A Dual Denoising Framework for Complex Questions and Large-scale Tables](https://arxiv.org/abs/2509.17680)
*Shenghao Ye,Yu Guo,Dong Jin,Yikai Shen,Yunpeng Hou,Shuangwu Chen,Jian Yang,Xiaofeng Jiang*

Main category: cs.CL

TL;DR: EnoTab是一个针对复杂问题和大型表格的双重去噪框架，通过问题分解和表格剪枝来提升表格问答性能


<details>
  <summary>Details</summary>
Motivation: 现实应用中复杂问题和大型表格引入大量噪声数据，严重降低了推理性能，需要解决相关性过滤和表格剪枝两个核心问题

Method: 提出EnoTab双重去噪框架：1）基于证据的问题去噪，将问题分解为最小语义单元并过滤无关内容；2）证据树引导的表格去噪，构建显式透明的表格剪枝路径，采用后序节点回滚机制处理异常状态

Result: 大量实验表明EnoTab在复杂问题和大型表格的TableQA任务上取得了优异性能

Conclusion: EnoTab通过双重去噪机制有效提升了表格问答在复杂场景下的推理能力

Abstract: Table question answering (TableQA) is a fundamental task in natural language
processing (NLP). The strong reasoning capabilities of large language models
(LLMs) have brought significant advances in this field. However, as real-world
applications involve increasingly complex questions and larger tables,
substantial noisy data is introduced, which severely degrades reasoning
performance. To address this challenge, we focus on improving two core
capabilities: Relevance Filtering, which identifies and retains information
truly relevant to reasoning, and Table Pruning, which reduces table size while
preserving essential content. Based on these principles, we propose EnoTab, a
dual denoising framework for complex questions and large-scale tables.
Specifically, we first perform Evidence-based Question Denoising by decomposing
the question into minimal semantic units and filtering out those irrelevant to
answer reasoning based on consistency and usability criteria. Then, we propose
Evidence Tree-guided Table Denoising, which constructs an explicit and
transparent table pruning path to remove irrelevant data step by step. At each
pruning step, we observe the intermediate state of the table and apply a
post-order node rollback mechanism to handle abnormal table states, ultimately
producing a highly reliable sub-table for final answer reasoning. Finally,
extensive experiments show that EnoTab achieves outstanding performance on
TableQA tasks with complex questions and large-scale tables, confirming its
effectiveness.

</details>


### [106] [TASO: Task-Aligned Sparse Optimization for Parameter-Efficient Model Adaptation](https://arxiv.org/abs/2509.17688)
*Daiye Miao,Yufang Liu,Jie Wang,Changzhi Sun,Yunke Zhang,Demei Yan,Shaokang Dong,Qi Zhang,Yuanbin Wu*

Main category: cs.CL

TL;DR: TASO是一种减少LoRA参数冗余的方法，利用预训练模型权重的重要性信息来识别任务特定的核心区域，从而在微调前确定LoRA模块的稀疏结构。


<details>
  <summary>Details</summary>
Motivation: LoRA虽然简单有效，但存在显著的参数冗余问题，这不仅增加了可训练参数数量，还阻碍了微调效果。如何高效准确地消除这些冗余参数是一个挑战性问题。

Method: 通过估计下游任务的参数重要性，基于重要性分数分布识别任务特定的核心区域，利用这些核心区域的位置信息确定LoRA模块的稀疏结构，在微调前实现冗余消除。

Result: 在参数预算与rank=1的LoRA相当的情况下，TASO在多个任务上持续优于标准LoRA，实现了强大的微调性能并有效消除了冗余参数。

Conclusion: TASO显著减少了任务适应所需的可训练参数数量，同时为LoRA冗余减少提供了新颖的任务对齐视角。

Abstract: LoRA has become one of the most widely used parameter-efficient fine-tuning
methods due to its simplicity and effectiveness. However, numerous studies have
shown that LoRA often introduces substantial parameter redundancy, which not
only increases the number of trainable parameters but also hinders the
effectiveness of fine-tuning. Since identifying redundant parameters in LoRA is
inherently difficult, how to eliminate them efficiently and accurately remains
a challenging problem. In this paper, we propose TASO, a redundancy reduction
method that leverages importance information from the pretrained model's
weights to mitigate LoRA redundancy. Specifically, we estimate parameter
importance on downstream tasks and identify task-specific core regions based on
the distribution of importance scores. The location information of these core
regions is then used to determine the sparse structure of LoRA modules,
enabling redundancy removal before fine-tuning. Our approach significantly
reduces the number of trainable parameters required for task adaptation, while
providing a novel task-aligned perspective for LoRA redundancy reduction.
Experimental results demonstrate that, with a parameter budget comparable to
LoRA with rank $r = 1$, TASO consistently outperforms standard LoRA across
multiple tasks, achieving strong fine-tuning performance while effectively
eliminating redundant parameters.

</details>


### [107] [Evaluating LLM-Generated Versus Human-Authored Responses in Role-Play Dialogues](https://arxiv.org/abs/2509.17694)
*Dongxu Lu,Johan Jeuring,Albert Gatt*

Main category: cs.CL

TL;DR: 该研究评估了LLM在长格式、知识驱动的角色扮演对话中的表现，发现LLM生成回复的质量在多轮对话中显著下降，而人类回复则持续改进，质量差距随时间扩大。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在长格式、知识驱动的角色扮演对话中的表现，特别是在专业培训模拟场景下，目前仍具挑战性。

Method: 通过人类评估（N=38）和自动化LLM-as-a-judge评估，比较LLM生成和人类撰写的多轮专业培训模拟对话回复。

Result: 人类评估显示LLM生成回复在自然性、上下文维护和整体质量方面显著下降，而人类回复持续改进。自动化评估中Gemini 2.0 Flash与人类评估者高度一致，证实了质量差距随时间扩大的现象。

Conclusion: 研究贡献了暴露LLM在多轮知识驱动角色扮演对话中性能下降的基准，并提供了经过验证的混合评估框架，指导LLM在培训模拟中的可靠集成。

Abstract: Evaluating large language models (LLMs) in long-form, knowledge-grounded
role-play dialogues remains challenging. This study compares LLM-generated and
human-authored responses in multi-turn professional training simulations
through human evaluation ($N=38$) and automated LLM-as-a-judge assessment.
Human evaluation revealed significant degradation in LLM-generated response
quality across turns, particularly in naturalness, context maintenance and
overall quality, while human-authored responses progressively improved. In line
with this finding, participants also indicated a consistent preference for
human-authored dialogue. These human judgements were validated by our automated
LLM-as-a-judge evaluation, where Gemini 2.0 Flash achieved strong alignment
with human evaluators on both zero-shot pairwise preference and stochastic
6-shot construct ratings, confirming the widening quality gap between LLM and
human responses over time. Our work contributes a multi-turn benchmark exposing
LLM degradation in knowledge-grounded role-play dialogues and provides a
validated hybrid evaluation framework to guide the reliable integration of LLMs
in training simulations.

</details>


### [108] [Investigating Bias: A Multilingual Pipeline for Generating, Solving, and Evaluating Math Problems with LLMs](https://arxiv.org/abs/2509.17701)
*Mariam Mahran,Katharina Simbeck*

Main category: cs.CL

TL;DR: 本文开发了一个多语言数学问题生成与评估管道，测试了三种商业LLM在不同语言（英语、德语、阿拉伯语）下的数学解题能力，发现英语解决方案质量最高，阿拉伯语最低，揭示了LLM在教育应用中的语言偏见问题。


<details>
  <summary>Details</summary>
Motivation: LLMs在教育支持中的应用日益增多，但其响应质量因交互语言而异。本研究旨在评估LLM在不同语言环境下的数学解题能力，揭示语言偏见问题。

Method: 开发了自动化多语言管道，生成628个符合德国K-10课程标准的数学问题，翻译成英语、德语和阿拉伯语。使用GPT-4o-mini、Gemini 2.5 Flash和Qwen-plus三种LLM生成逐步解决方案，并由Claude 3.5 Haiku等LLM评委进行质量评估。

Result: 结果显示存在一致的语言差距：英语解决方案质量最高，阿拉伯语解决方案通常排名较低。三种LLM在不同语言环境下的表现存在显著差异。

Conclusion: 研究结果突显了LLM在教育应用中存在的持续性语言偏见，强调了开发更公平的多语言AI教育系统的必要性。

Abstract: Large Language Models (LLMs) are increasingly used for educational support,
yet their response quality varies depending on the language of interaction.
This paper presents an automated multilingual pipeline for generating, solving,
and evaluating math problems aligned with the German K-10 curriculum. We
generated 628 math exercises and translated them into English, German, and
Arabic. Three commercial LLMs (GPT-4o-mini, Gemini 2.5 Flash, and Qwen-plus)
were prompted to produce step-by-step solutions in each language. A held-out
panel of LLM judges, including Claude 3.5 Haiku, evaluated solution quality
using a comparative framework. Results show a consistent gap, with English
solutions consistently rated highest, and Arabic often ranked lower. These
findings highlight persistent linguistic bias and the need for more equitable
multilingual AI systems in education.

</details>


### [109] [Breaking Token Into Concepts: Exploring Extreme Compression in Token Representation Via Compositional Shared Semantics](https://arxiv.org/abs/2509.17737)
*Kavin R V,Pawan Goyal*

Main category: cs.CL

TL;DR: 本文提出了一种名为聚合语义分组（ASG）的新方法，利用产品量化（PQ）来表示token的复合语义结构，在保持95%以上任务性能的同时，将嵌入参数压缩到0.4-0.5%。


<details>
  <summary>Details</summary>
Motivation: 标准语言模型使用单一、整体的嵌入来表示每个token，这可能限制了捕捉词语多面性含义的能力。

Method: 提出ASG方法，利用产品量化构建token的复合表示结构，并在标准transformer架构（mBERT、XLM-R、mT5）上进行验证，评估任务包括NLI、NER、QA以及生物医学领域基准（BC5CDR）。

Result: ASG在保持>95%任务性能的同时，实现了嵌入参数的极端压缩（0.4-0.5%），在生成任务、跨语言迁移和领域特定设置中都表现良好。

Conclusion: 验证了token可以作为共享语义构建块的组合来有效建模的原则，ASG提供了一种简单而具体的方法来实现这一目标，展示了复合表示如何在实现紧凑模型的同时捕捉语言丰富性。

Abstract: Standard language models employ unique, monolithic embeddings for each token,
potentially limiting their ability to capture the multifaceted nature of word
meanings. We investigate whether tokens can be more effectively represented
through a compositional structure that accumulates diverse semantic facets. To
explore this, we propose Aggregate Semantic Grouping (ASG), a novel approach
leveraging Product Quantization (PQ). We apply ASG to standard transformer
architectures (mBERT, XLM-R, mT5) and evaluate this representational scheme
across diverse tasks (NLI, NER, QA), as well as a biomedical domain-specific
benchmark (BC5CDR) using BioBERT. Our findings demonstrate that representing
tokens compositionally via ASG achieves extreme compression in embedding
parameters (0.4--0.5\%) while maintaining $>$95\% task performance relative to
the base model, even in generative tasks and extends to both cross lingual
transfer and domain-specific settings. These results validate the principle
that tokens can be effectively modeled as combinations of shared semantic
building blocks. ASG offers a simple yet concrete method for achieving this,
showcasing how compositional representations can capture linguistic richness
while enabling compact yet semantically rich models.

</details>


### [110] [Qwen3-Omni Technical Report](https://arxiv.org/abs/2509.17765)
*Jin Xu,Zhifang Guo,Hangrui Hu,Yunfei Chu,Xiong Wang,Jinzheng He,Yuxuan Wang,Xian Shi,Ting He,Xinfa Zhu,Yuanjun Lv,Yongqi Wang,Dake Guo,He Wang,Linhan Ma,Pei Zhang,Xinyu Zhang,Hongkun Hao,Zishan Guo,Baosong Yang,Bin Zhang,Ziyang Ma,Xipin Wei,Shuai Bai,Keqin Chen,Xuejing Liu,Peng Wang,Mingkun Yang,Dayiheng Liu,Xingzhang Ren,Bo Zheng,Rui Men,Fan Zhou,Bowen Yu,Jianxin Yang,Le Yu,Jingren Zhou,Junyang Lin*

Main category: cs.CL

TL;DR: Qwen3-Omni是首个在文本、图像、音频和视频四种模态上均保持最先进性能的多模态模型，采用Thinker-Talker MoE架构，在音频任务上表现尤为突出，支持多语言交互和实时语音生成。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏在文本、图像、音频和视频四种模态上均能保持最优性能的统一多模态模型，特别是在音频处理领域存在性能瓶颈。

Method: 采用Thinker-Talker MoE架构，统一感知和生成；使用多码本方案预测离散语音编解码器；用轻量级因果ConvNet替代计算密集的块扩散；引入Thinking模型进行显式多模态推理。

Result: 在36个音频和音视频基准测试中，Qwen3-Omni在32个基准上达到开源SOTA，在22个基准上达到总体SOTA，超越Gemini-2.5-Pro等闭源模型；理论端到端首包延迟为234毫秒。

Conclusion: Qwen3-Omni成功实现了在多种模态上的统一高性能表现，特别是在音频处理领域取得突破性进展，模型已开源发布。

Abstract: We present Qwen3-Omni, a single multimodal model that, for the first time,
maintains state-of-the-art performance across text, image, audio, and video
without any degradation relative to single-modal counterparts. Qwen3-Omni
matches the performance of same-sized single-modal models within the Qwen
series and excels particularly on audio tasks. Across 36 audio and audio-visual
benchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall
SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro,
Seed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE
architecture that unifies perception and generation across text, images, audio,
and video, yielding fluent text and natural real-time speech. It supports text
interaction in 119 languages, speech understanding in 19 languages, and speech
generation in 10 languages. To reduce first-packet latency in streaming
synthesis, Talker autoregressively predicts discrete speech codecs using a
multi-codebook scheme. Leveraging the representational capacity of these
codebooks, we replace computationally intensive block-wise diffusion with a
lightweight causal ConvNet, enabling streaming from the first codec frame. In
cold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet
latency of 234 ms. To further strengthen multimodal reasoning, we introduce a
Thinking model that explicitly reasons over inputs from any modality. Since the
research community currently lacks a general-purpose audio captioning model, we
fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which
produces detailed, low-hallucination captions for arbitrary audio inputs.
Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and
Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0
license.

</details>


### [111] [A State-Update Prompting Strategy for Efficient and Robust Multi-turn Dialogue](https://arxiv.org/abs/2509.17766)
*Ziyi Liu*

Main category: cs.CL

TL;DR: 提出一种无需训练的状态更新多轮对话策略，通过状态重构和历史提醒机制解决LLMs在长对话中的信息遗忘和效率问题，在HotpotQA数据集上显著提升性能并大幅降低推理时间和token消耗


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长跨度多轮对话中存在信息遗忘和效率低下的问题，需要一种有效的方法来管理对话历史

Method: 训练无关的提示工程方法，包含状态重构和历史提醒两个机制，用于有效管理对话历史

Result: 在多个多跳QA数据集上表现优异，HotpotQA数据集上核心信息过滤分数提升32.6%，下游QA分数提升14.1%，推理时间减少73.1%，token消耗减少59.4%

Conclusion: 为优化LLMs在长程交互中的表现提供了有效解决方案，为开发更鲁棒的智能体提供了新思路

Abstract: Large Language Models (LLMs) struggle with information forgetting and
inefficiency in long-horizon, multi-turn dialogues. To address this, we propose
a training-free prompt engineering method, the State-Update Multi-turn Dialogue
Strategy. It utilizes "State Reconstruction" and "History Remind" mechanisms to
effectively manage dialogue history. Our strategy shows strong performance
across multiple multi-hop QA datasets. For instance, on the HotpotQA dataset,
it improves the core information filtering score by 32.6%, leading to a 14.1%
increase in the downstream QA score, while also reducing inference time by
73.1% and token consumption by 59.4%. Ablation studies confirm the pivotal
roles of both components. Our work offers an effective solution for optimizing
LLMs in long-range interactions, providing new insights for developing more
robust Agents.

</details>


### [112] [DIVERS-Bench: Evaluating Language Identification Across Domain Shifts and Code-Switching](https://arxiv.org/abs/2509.17768)
*Jessica Ojo,Zina Kamel,David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: 该论文介绍了DIVERS-BENCH评估框架，用于测试语言识别模型在多样化领域的表现，发现现有模型在噪声和非正式文本上表现不佳，并提出了DIVERS-CS代码转换基准数据集。


<details>
  <summary>Details</summary>
Motivation: 当前语言识别系统往往过拟合于干净的单一语言数据，无法适应现实世界中的多样化文本类型，需要更全面的评估框架。

Method: 构建了DIVERS-BENCH评估框架，涵盖语音转录、网络文本、社交媒体文本、儿童故事和代码转换文本等多个领域，并创建了DIVERS-CS代码转换基准数据集。

Result: 研究发现虽然模型在精心整理的数据集上准确率高，但在噪声和非正式输入上性能急剧下降，现有模型难以检测同一句子中的多种语言。

Conclusion: 研究结果强调了在现实世界环境中需要更鲁棒和包容的语言识别系统。

Abstract: Language Identification (LID) is a core task in multilingual NLP, yet current
systems often overfit to clean, monolingual data. This work introduces
DIVERS-BENCH, a comprehensive evaluation of state-of-the-art LID models across
diverse domains, including speech transcripts, web text, social media texts,
children's stories, and code-switched text. Our findings reveal that while
models achieve high accuracy on curated datasets, performance degrades sharply
on noisy and informal inputs. We also introduce DIVERS-CS, a diverse
code-switching benchmark dataset spanning 10 language pairs, and show that
existing models struggle to detect multiple languages within the same sentence.
These results highlight the need for more robust and inclusive LID systems in
real-world settings.

</details>


### [113] [One Agent to Serve All: a Lite-Adaptive Stylized AI Assistant for Millions of Multi-Style Official Accounts](https://arxiv.org/abs/2509.17788)
*Xingyu Fan,Feifei Li,Wenhui Que,Hailong Li*

Main category: cs.CL

TL;DR: WeStar是一个轻量级自适应框架，用于解决工业级公众号平台中对话代理在保持上下文相关性和风格一致性方面的挑战，通过结合RAG和参数化RAG实现高效扩展。


<details>
  <summary>Details</summary>
Motivation: 现有方法在工业级公众号平台部署中存在显著问题：思维链提示导致延迟过高、单账号微调计算成本大、长提示方法降低模型对上下文和风格的理解能力。

Method: WeStar框架结合基于RAG的上下文生成和基于参数化RAG的风格感知生成，使用LoRA模块按风格簇动态激活，并提出多维度聚类参数共享方案和风格增强直接偏好优化方法。

Result: 在大规模工业数据集上的实验验证了WeStar的有效性和效率，证明其在实际部署中的实用价值。

Conclusion: WeStar能够以最小开销服务大量官方账号，提供了一种实用的解决方案，解决了现有方法在工业规模部署中的关键限制。

Abstract: Conversational agents deployed in industrial-scale official account platforms
must generate responses that are both contextually grounded and stylistically
aligned-requirements that existing methods struggle to meet. Chain-of-thought
(CoT) prompting induces significant latency due to multi-turn reasoning;
per-account fine-tuning is computationally prohibitive; and long prompt-based
methods degrade the model's ability to grasp injected context and style. In
this paper, we propose WeStar, a lite-adaptive framework for stylized
contextual question answering that scales to millions of official accounts.
WeStar combines context-grounded generation via RAG with style-aware generation
using Parametric RAG (PRAG), where LoRA modules are dynamically activated per
style cluster. Our contributions are fourfold: (1) We introduce WeStar, a
unified framework capable of serving large volumes of official accounts with
minimal overhead. (2) We propose a multi-dimensional, cluster-based parameter
sharing scheme that enables compact style representation while preserving
stylistic diversity. (3) We develop a style-enhanced Direct Preference
Optimization (SeDPO) method to optimize each style cluster's parameters for
improved generation quality. (4) Experiments on a large-scale industrial
dataset validate the effectiveness and efficiency of WeStar, underscoring its
pracitical value in real-world deployment.

</details>


### [114] [Learning to vary: Teaching LMs to reproduce human linguistic variability in next-word prediction](https://arxiv.org/abs/2509.17794)
*Tobias Groot,Salo Lacunes,Evgenia Ilia*

Main category: cs.CL

TL;DR: 本文研究了通过多标签微调来提升语言模型在自然语言生成任务中重现人类语言变异性的能力。


<details>
  <summary>Details</summary>
Motivation: 语言模型在重现人类语言固有的变异性方面表现不佳，这可能是因为训练数据缺乏这种内在变异性的体现。作者希望验证通过训练模型学习多个合理的词语续写是否能改善这一能力。

Method: 使用预训练和指令调优模型（GPT-2和Mistral-7B-IT），基于Provo语料库进行多标签微调，每个上下文对应多个合理的词语续写。

Result: 评估显示，多标签微调显著提高了语言模型重现语言变异性的能力，无论是在高变异性还是低变异性的上下文中都表现出改善。

Conclusion: 通过训练语言模型学习多个合理的词语续写，可以有效提升其重现人类语言变异性的能力，这为解决语言模型多样性不足的问题提供了有效途径。

Abstract: Natural language generation (NLG) tasks are often subject to inherent
variability; \emph{e.g.} predicting the next word given a context has multiple
valid responses, evident when asking multiple humans to complete the task.
While having language models (LMs) that are aligned pluralistically, so that
they are able to reproduce well the inherent diversity in perspectives of an
entire population of interest is clearly beneficial, \citet{ilia2024predict}
show that LMs do not reproduce this type of linguistic variability well. They
speculate this inability might stem from the lack of consistent training of LMs
with data reflecting this type of inherent variability. As such, we investigate
whether training LMs on multiple plausible word continuations per context can
improve their ability to reproduce human linguistic variability for next-word
prediction. We employ fine-tuning techniques for pre-trained and
instruction-tuned models; and demonstrate their potential when fine-tuning
GPT-2 and Mistral-7B-IT, using Provo Corpus. Our evaluation, which measures
divergence among empirically estimated human and model next-word distributions
across contexts before and after fine-tuning, shows that our multi-label
fine-tuning improves the LMs' ability to reproduce linguistic variability; both
for contexts that admit higher and lower variability.

</details>


### [115] [Fine-Grained Detection of AI-Generated Text Using Sentence-Level Segmentation](https://arxiv.org/abs/2509.17830)
*Lekkala Sai Teja,Annepaka Yadagiri,and Partha Pakray,Chukhu Chunka,Mangadoddi Srikar Vardhan*

Main category: cs.CL

TL;DR: 提出了一种句子级别的序列标注模型，用于检测混合文本中AI生成内容的边界，结合Transformer、神经网络和CRF层来提高检测精度。


<details>
  <summary>Details</summary>
Motivation: 传统AI检测器基于文档级分类，难以识别经过编辑的混合文本中AI生成的内容，需要更细粒度的检测方法。

Method: 使用预训练Transformer模型提取语义和句法特征，结合神经网络捕获序列级表示，通过CRF层优化边界预测，实现token级别的AI文本检测。

Result: 在两个公开基准数据集上的实验表明，该方法能够准确检测完全协作文本中的AI文本片段，优于零样本检测器和现有最先进模型。

Conclusion: 提出的句子级序列标注方法能够有效解决混合文本中AI内容检测的挑战，为AI文本检测提供了更精细的解决方案。

Abstract: Generation of Artificial Intelligence (AI) texts in important works has
become a common practice that can be used to misuse and abuse AI at various
levels. Traditional AI detectors often rely on document-level classification,
which struggles to identify AI content in hybrid or slightly edited texts
designed to avoid detection, leading to concerns about the model's efficiency,
which makes it hard to distinguish between human-written and AI-generated
texts. A sentence-level sequence labeling model proposed to detect transitions
between human- and AI-generated text, leveraging nuanced linguistic signals
overlooked by document-level classifiers. By this method, detecting and
segmenting AI and human-written text within a single document at the
token-level granularity is achieved. Our model combines the state-of-the-art
pre-trained Transformer models, incorporating Neural Networks (NN) and
Conditional Random Fields (CRFs). This approach extends the power of
transformers to extract semantic and syntactic patterns, and the neural network
component to capture enhanced sequence-level representations, thereby improving
the boundary predictions by the CRF layer, which enhances sequence recognition
and further identification of the partition between Human- and AI-generated
texts. The evaluation is performed on two publicly available benchmark datasets
containing collaborative human and AI-generated texts. Our experimental
comparisons are with zero-shot detectors and the existing state-of-the-art
models, along with rigorous ablation studies to justify that this approach, in
particular, can accurately detect the spans of AI texts in a completely
collaborative text. All our source code and the processed datasets are
available in our GitHub repository.

</details>


### [116] [Findings of the Fourth Shared Task on Multilingual Coreference Resolution: Can LLMs Dethrone Traditional Approaches?](https://arxiv.org/abs/2509.17796)
*Michal Novák,Miloslav Konopík,Anna Nedoluzhko,Martin Popel,Ondřej Pražák,Jakub Sido,Milan Straka,Zdeněk Žabokrtský,Daniel Zeman*

Main category: cs.CL

TL;DR: 本文介绍了CODI-CRAC 2025工作坊中第四届多语言共指消解共享任务的概况，重点包括新增的LLM赛道、数据集扩展和参赛系统表现。


<details>
  <summary>Details</summary>
Motivation: 推动多语言共指消解技术的发展，探索大型语言模型在该领域的应用潜力，并通过共享任务促进学术交流和技术进步。

Method: 采用CorefUD 1.3版本作为基准数据集，包含22个数据集的17种语言；特别设计了适合LLM的简化纯文本格式；参赛系统包括传统方法和基于LLM的方法（微调和少样本适应）。

Result: 共有9个系统参赛，其中4个基于LLM。传统系统仍保持领先，但LLM显示出明显潜力，表明未来可能挑战传统方法。

Conclusion: LLM在多语言共指消解任务中展现出良好前景，虽然目前传统方法仍占优势，但LLM有望在未来的任务版本中成为强有力的竞争者。

Abstract: The paper presents an overview of the fourth edition of the Shared Task on
Multilingual Coreference Resolution, organized as part of the CODI-CRAC 2025
workshop. As in the previous editions, participants were challenged to develop
systems that identify mentions and cluster them according to identity
coreference.
  A key innovation of this year's task was the introduction of a dedicated
Large Language Model (LLM) track, featuring a simplified plaintext format
designed to be more suitable for LLMs than the original CoNLL-U representation.
  The task also expanded its coverage with three new datasets in two additional
languages, using version 1.3 of CorefUD - a harmonized multilingual collection
of 22 datasets in 17 languages.
  In total, nine systems participated, including four LLM-based approaches (two
fine-tuned and two using few-shot adaptation). While traditional systems still
kept the lead, LLMs showed clear potential, suggesting they may soon challenge
established approaches in future editions.

</details>


### [117] [How Persuasive is Your Context?](https://arxiv.org/abs/2509.17879)
*Tu Nguyen,Kevin Du,Alexander Miserlis Hoyle,Ryan Cotterell*

Main category: cs.CL

TL;DR: 本文介绍了目标说服分数（TPS），这是一种量化语言模型对上下文信息敏感度的新指标，通过Wasserstein距离测量上下文如何改变模型对问题的答案分布。


<details>
  <summary>Details</summary>
Motivation: 语言模型具有利用先验知识和适应上下文信息的能力，但现有评估方法只能检查贪婪解码的答案，无法提供模型行为的细粒度视图。

Method: 基于Wasserstein距离，TPS测量上下文将模型的原始答案分布向目标分布移动的程度，提供比先前指标更细致的说服力评估。

Result: 通过一系列实验证明，TPS比之前提出的指标能捕捉更细微的说服力概念。

Conclusion: TPS为评估语言模型对上下文信息的敏感度提供了更精细的量化方法，有助于更好地理解模型的行为特性。

Abstract: Two central capabilities of language models (LMs) are: (i) drawing on prior
knowledge about entities, which allows them to answer queries such as "What's
the official language of Austria?", and (ii) adapting to new information
provided in context, e.g., "Pretend the official language of Austria is
Tagalog.", that is pre-pended to the question. In this article, we introduce
targeted persuasion score (TPS), designed to quantify how persuasive a given
context is to an LM where persuasion is operationalized as the ability of the
context to alter the LM's answer to the question. In contrast to evaluating
persuasiveness only by inspecting the greedily decoded answer under the model,
TPS provides a more fine-grained view of model behavior. Based on the
Wasserstein distance, TPS measures how much a context shifts a model's original
answer distribution toward a target distribution. Empirically, through a series
of experiments, we show that TPS captures a more nuanced notion of
persuasiveness than previously proposed metrics.

</details>


### [118] [Everyday Physics in Korean Contexts: A Culturally Grounded Physical Reasoning Benchmark](https://arxiv.org/abs/2509.17807)
*Jihae Jeong,DaeYeop Lee,DongGeon Lee,Hwanjo Yu*

Main category: cs.CL

TL;DR: EPiK是一个针对韩国文化背景的物理常识推理基准测试，包含181个二元选择题，涵盖9个推理子任务和84个场景，旨在解决现有基准测试忽视文化差异的问题。


<details>
  <summary>Details</summary>
Motivation: 现有物理常识推理基准主要关注西方背景，忽略了文化差异对物理问题解决的影响，因此需要开发能够反映特定文化背景的基准测试。

Method: 采用两阶段生成和验证流程，从韩国文化背景有机生成问题，确保文化真实性和物理推理严谨性，避免简单翻译方法。

Result: 评估显示韩国专业化模型在同等规模下始终优于通用模型，表明文化无关模型存在局限性。

Conclusion: EPiK基准证明了文化感知基准对于真正衡量语言理解能力的重要性，需要开发更多文化敏感的评估工具。

Abstract: Existing physical commonsense reasoning benchmarks predominantly focus on
Western contexts, overlooking cultural variations in physical problem-solving.
To address this gap, we introduce EPiK (Everyday Physics in Korean Contexts), a
novel benchmark comprising 181 binary-choice problems that test physical
reasoning within Korean cultural contexts, ranging from kimchi (Korean food) to
traditional fermentation. EPiK is constructed using a two-stage generation and
verification pipeline to create culturally-authentic problems across 9
reasoning subtasks and 84 scenarios. Unlike approaches based on simple
translation, our method generates problems organically from Korean contexts
while upholding rigorous physical reasoning standards. Our evaluations show
that Korean-specialized models consistently outperform general-purpose models
of comparable size. This performance gap highlights the limitations of
culturally-agnostic models and demonstrates the critical need for
culturally-aware benchmarks to truly measure language understanding. Our EPiK
is publicly available at https://huggingface.co/datasets/jjae/EPiK.

</details>


### [119] [Transformer-Encoder Trees for Efficient Multilingual Machine Translation and Speech Translation](https://arxiv.org/abs/2509.17930)
*Yiwen Guan,Jacob Whitehill*

Main category: cs.CL

TL;DR: 提出了一种新颖的分层Transformer编码器树（TET）结合非自回归编码器模型，用于多语言翻译，特别是在语音翻译中提高低资源语言的准确性并减少计算冗余。


<details>
  <summary>Details</summary>
Motivation: 多语言翻译面临计算冗余和低资源语言准确性有限的问题，尤其是在语音翻译中。

Method: 使用分层Transformer编码器树（TET）结合非自回归编码器模型，通过共享语言相似目标语言的中间表示，并采用连接时序分类进行训练。

Result: TET能够提高低资源语言的准确性，减少计算冗余，并在单次前向传播中生成所有目标语言。与自回归系统相比，结合wav2vec2的TET在翻译质量上表现良好，且速度快7-14倍。

Conclusion: TET结合非自回归模型在多语言翻译，特别是语音翻译中，显示出在提高翻译质量和计算效率方面的潜力。

Abstract: Multilingual translation faces challenges of computational redundancy and
limited accuracy for low-resource languages, especially in speech translation.
To address this, we propose a novel hierarchical Transformer Encoder Tree (TET)
combined with non-autoregressive encoder-only models trained with Connectionist
Temporal Classification for multilingual translation. By sharing intermediate
representations among linguistically similar target languages, TET can improve
accuracy on low-resource languages, reduce computational redundancy, and allow
generating all target languages in a single forward pass, thus eliminating
sequential bottlenecks and improving parallelism. For speech translation,
combining TET with a non-autoregressive speech recognition backbone (wav2vec2)
shows promising results in terms of translation quality compared to
autoregressive systems while being 7-14 times faster.

</details>


### [120] [Towards Adaptive Context Management for Intelligent Conversational Question Answering](https://arxiv.org/abs/2509.17829)
*Manoj Madushanka Perera,Adnan Mahmood,Kasun Eranda Wijethilake,Quan Z. Sheng*

Main category: cs.CL

TL;DR: 本文提出了一种用于对话问答系统的自适应上下文管理框架，通过动态管理对话历史来优化模型在token限制内的相关信息使用


<details>
  <summary>Details</summary>
Motivation: 对话问答系统在处理长对话历史时面临token限制的挑战，需要有效管理上下文以保留最相关和最新的信息

Method: 采用三模块架构：上下文管理模块动态调整上下文大小，摘要模块通过滑动窗口总结旧对话部分，实体提取模块在摘要窗口超限时保留关键实体

Result: 实验结果表明该框架能生成准确且上下文合适的回答，提升了对话问答系统的鲁棒性和可扩展性

Conclusion: ACM框架通过智能的上下文管理策略有效解决了对话问答系统的token限制问题，具有重要的应用价值

Abstract: This particular paper introduces an Adaptive Context Management (ACM)
framework for the Conversational Question Answering (ConvQA) systems. The key
objective of the ACM framework is to optimize the use of the conversation
history by dynamically managing context for maximizing the relevant information
provided to a ConvQA model within its token limit. Our approach incorporates a
Context Manager (CM) Module, a Summarization (SM) Module, and an Entity
Extraction (EE) Module in a bid to handle the conversation history
efficaciously. The CM Module dynamically adjusts the context size, thereby
preserving the most relevant and recent information within a model's token
limit. The SM Module summarizes the older parts of the conversation history via
a sliding window. When the summarization window exceeds its limit, the EE
Module identifies and retains key entities from the oldest conversation turns.
Experimental results demonstrate the effectiveness of our envisaged framework
in generating accurate and contextually appropriate responses, thereby
highlighting the potential of the ACM framework to enhance the robustness and
scalability of the ConvQA systems.

</details>


### [121] [HICode: Hierarchical Inductive Coding with LLMs](https://arxiv.org/abs/2509.17946)
*Mian Zhong,Pristina Wang,Anjalie Field*

Main category: cs.CL

TL;DR: 本文提出HICode方法，利用LLM自动生成标签并分层聚类，实现大规模文本语料的细粒度分析，替代传统手动标注和统计方法。


<details>
  <summary>Details</summary>
Motivation: 现有细粒度语料分析主要依赖难以扩展的手动标注或难以控制的统计工具（如主题建模），需要一种能够扩展研究人员手动进行的细致分析到大规模文本语料的方法。

Method: HICode是一个两部分流水线：首先从分析数据中归纳生成标签，然后对标签进行分层聚类以浮现新兴主题。该方法受定性研究方法启发，利用LLM的潜力。

Result: 在三个不同数据集上验证了该方法，测量了与人工构建主题的一致性，并通过自动和人工评估证明了其鲁棒性。在阿片类药物危机诉讼文件的案例研究中，揭示了制药公司采用的激进营销策略。

Conclusion: HICode具有促进大规模数据中细致分析的潜力，能够有效扩展传统手动分析方法到大规模文本语料。

Abstract: Despite numerous applications for fine-grained corpus analysis, researchers
continue to rely on manual labeling, which does not scale, or statistical tools
like topic modeling, which are difficult to control. We propose that LLMs have
the potential to scale the nuanced analyses that researchers typically conduct
manually to large text corpora. To this effect, inspired by qualitative
research methods, we develop HICode, a two-part pipeline that first inductively
generates labels directly from analysis data and then hierarchically clusters
them to surface emergent themes. We validate this approach across three diverse
datasets by measuring alignment with human-constructed themes and demonstrating
its robustness through automated and human evaluations. Finally, we conduct a
case study of litigation documents related to the ongoing opioid crisis in the
U.S., revealing aggressive marketing strategies employed by pharmaceutical
companies and demonstrating HICode's potential for facilitating nuanced
analyses in large-scale data.

</details>


### [122] [ReDepress: A Cognitive Framework for Detecting Depression Relapse from Social Media](https://arxiv.org/abs/2509.17991)
*Aakash Kumar Agarwal,Saprativa Bhattacharjee,Mauli Rastogi,Jemima S. Jacob,Biplab Banerjee,Rashmi Gupta,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 该论文提出了ReDepress，首个临床验证的抑郁症复发检测数据集，基于认知理论构建特征，通过机器学习模型实现86%的F1分数，为早期复发检测提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 近50%抑郁症患者面临复发风险，二次发作后风险升至80%。现有研究主要关注抑郁症检测，但复发检测因缺乏标注数据和难以区分复发/非复发用户而未被充分探索。

Method: 构建ReDepress数据集（204名Reddit用户，由心理健康专家标注），基于认知抑郁理论（注意力偏差、解释偏差、记忆偏差、沉思）进行特征提取，使用基于Transformer的时间序列模型进行建模。

Result: 认知标记能显著区分复发与非复发群体，基于认知特征的模型表现优异，Transformer时序模型达到0.86的F1分数。

Conclusion: 研究验证了心理学理论在真实文本数据中的应用，证明了认知信息计算方法在早期复发检测中的潜力，为低成本、可扩展的心理健康干预铺平了道路。

Abstract: Almost 50% depression patients face the risk of going into relapse. The risk
increases to 80% after the second episode of depression. Although, depression
detection from social media has attained considerable attention, depression
relapse detection has remained largely unexplored due to the lack of curated
datasets and the difficulty of distinguishing relapse and non-relapse users. In
this work, we present ReDepress, the first clinically validated social media
dataset focused on relapse, comprising 204 Reddit users annotated by mental
health professionals. Unlike prior approaches, our framework draws on cognitive
theories of depression, incorporating constructs such as attention bias,
interpretation bias, memory bias and rumination into both annotation and
modeling. Through statistical analyses and machine learning experiments, we
demonstrate that cognitive markers significantly differentiate relapse and
non-relapse groups, and that models enriched with these features achieve
competitive performance, with transformer-based temporal models attaining an F1
of 0.86. Our findings validate psychological theories in real-world textual
data and underscore the potential of cognitive-informed computational methods
for early relapse detection, paving the way for scalable, low-cost
interventions in mental healthcare.

</details>


### [123] [Trust Me, I Can Convince You: The Contextualized Argument Appraisal Framework](https://arxiv.org/abs/2509.17844)
*Lynn Greschner,Sabine Weber,Roman Klinger*

Main category: cs.CL

TL;DR: 提出了情境化论证评估框架，将发送者、接收者和论证之间的相互作用情境化，包括情绪标签、评估变量（如论证熟悉度、响应紧迫性、预期努力）和说服力变量。


<details>
  <summary>Details</summary>
Motivation: 情绪影响论证的说服力，但现有研究将二元情绪性与认知评估分开研究，尚未将两者结合。

Method: 在角色扮演场景中进行研究，模拟真实世界接触论证的情况，要求参与者披露情绪、解释主要原因、论证评估和感知说服力，并收集人口统计数据和人格特质。

Result: 分析800个论证（每个由5名参与者标注）的语料库显示，说服力与积极情绪（如信任）正相关，与消极情绪（如愤怒）负相关。评估变量揭示了论证熟悉度的重要性。

Conclusion: 对于大多数参与者，论证内容本身是情绪反应的主要驱动因素，论证熟悉度在评估过程中起关键作用。

Abstract: Emotions, which influence how convincing an argument is, are developed
  in context of the self and sender, and therefore require modeling
  the cognitive evaluation process. While binary emotionality has been
  studied in argument mining, and the cognitive appraisal has been
  modeled in general emotion analysis, these fields have not been
  brought together yet. We therefore propose the Contextualized
  Argument Appraisal Framework that contextualizes the interplay
  between the sender, receiver, and argument. It includes emotion
  labels, appraisals, such as argument familiarity, response urgency,
  and expected effort, as well as convincingness variables. To evaluate
  the framework and pave the way to computational modeling, we perform
  a study in a role-playing scenario, mimicking real-world exposure to
  arguments, asking participants to disclose their emotion, explain the main
cause, the
  argument appraisal, and the
  perceived convincingness. To consider the subjective nature of such
  annotations, we also collect demographic data and personality traits
  of both the participants and the perceived sender of the argument.
  The analysis of the resulting corpus of 800 arguments, each
  annotated by 5 participants, reveals that convincingness is
  positively correlated with positive emotions (e.g., trust) and
  negatively correlated with negative emotions (e.g., anger). The
  appraisal variables disclose the importance of the argument
  familiarity. For most participants, the content of the argument
  itself is the primary driver of the emotional response.

</details>


### [124] [Variation in Verification: Understanding Verification Dynamics in Large Language Models](https://arxiv.org/abs/2509.17995)
*Yefan Zhou,Austin Xu,Yilun Zhou,Janvijay Singh,Jiang Gui,Shafiq Joty*

Main category: cs.CL

TL;DR: 本文研究了生成式验证器在测试时扩展中的应用，分析了验证效果与问题难度、生成器能力和验证器生成能力之间的关系，发现验证效果与问题难度和生成器强度相关，验证器能力与自身解决问题能力相关但受问题难度影响。


<details>
  <summary>Details</summary>
Motivation: 随着测试时计算扩展使大型语言模型能够解决更复杂问题，研究生成式验证器（通过生成思维链推理进行验证）在不同条件下的验证效果，以优化测试时扩展应用中的基本验证策略。

Method: 在12个基准测试上（数学推理、知识和自然语言推理任务）使用14个开源模型（2B到72B参数范围）和GPT-4o，系统分析验证动态的三个维度：问题难度、生成器能力和验证器生成能力。

Result: 发现三个关键结果：简单问题使验证器能更可靠地认证正确响应；弱生成器产生的错误比强生成器更容易检测；验证能力通常与验证器自身解决问题能力相关，但这种关系随问题难度变化。

Conclusion: 研究揭示了优化测试时扩展应用中基本验证策略的机会：弱生成器在验证后性能可接近强生成器；强验证器在某些情况下优势有限，表明仅扩展验证器无法克服基本验证挑战。

Abstract: Recent advances have shown that scaling test-time computation enables large
language models (LLMs) to solve increasingly complex problems across diverse
domains. One effective paradigm for test-time scaling (TTS) involves LLM
generators producing multiple solution candidates, with LLM verifiers assessing
the correctness of these candidates without reference answers. In this paper,
we study generative verifiers, which perform verification by generating
chain-of-thought (CoT) reasoning followed by a binary verdict. We
systematically analyze verification dynamics across three dimensions - problem
difficulty, generator capability, and verifier generation capability - with
empirical studies on 12 benchmarks across mathematical reasoning, knowledge,
and natural language reasoning tasks using 14 open-source models (2B to 72B
parameter range) and GPT-4o. Our experiments reveal three key findings about
verification effectiveness: (1) Easy problems allow verifiers to more reliably
certify correct responses; (2) Weak generators produce errors that are easier
to detect than strong generators; (3) Verification ability is generally
correlated with the verifier's own problem-solving capability, but this
relationship varies with problem difficulty. These findings reveal
opportunities to optimize basic verification strategies in TTS applications.
First, given the same verifier, some weak generators can nearly match stronger
ones in post-verification TTS performance (e.g., the Gemma2-9B to Gemma2-27B
performance gap shrinks by 75.5%). Second, we identify cases where strong
verifiers offer limited advantage over weak ones, as both fail to provide
meaningful verification gains, suggesting that verifier scaling alone cannot
overcome fundamental verification challenges.

</details>


### [125] [Make Every Letter Count: Building Dialect Variation Dictionaries from Monolingual Corpora](https://arxiv.org/abs/2509.17855)
*Robert Litschko,Verena Blaschke,Diana Burkhardt,Barbara Plank,Diego Frassinelli*

Main category: cs.CL

TL;DR: 该论文研究大型语言模型处理方言词汇的能力，以巴伐利亚语为例，通过创建DiaLemma注释框架和10万对德语-巴伐利亚语词汇数据集，评估9个先进LLM在方言翻译、词形变化识别方面的表现。


<details>
  <summary>Details</summary>
Motivation: 方言因缺乏标准拼写而存在显著变体，但LLM处理方言的能力研究不足。作者旨在填补这一空白，通过巴伐利亚语案例研究LLM的词汇方言理解能力。

Method: 提出DiaLemma注释框架，仅从单语数据创建方言变体词典，构建包含10万对人工标注德语-巴伐利亚语词汇的数据集，评估9个先进LLM在识别方言翻译、词形变化等方面的表现。

Result: LLM在名词和词汇相似词对上表现最佳，但在区分直接翻译和词形变化方面最困难。提供上下文示例能提高翻译性能，但降低识别方言变体的能力。

Conclusion: 研究揭示了LLM在处理拼写方言变体方面的局限性，强调需要未来工作来适应LLM处理方言。

Abstract: Dialects exhibit a substantial degree of variation due to the lack of a
standard orthography. At the same time, the ability of Large Language Models
(LLMs) to process dialects remains largely understudied. To address this gap,
we use Bavarian as a case study and investigate the lexical dialect
understanding capability of LLMs by examining how well they recognize and
translate dialectal terms across different parts-of-speech. To this end, we
introduce DiaLemma, a novel annotation framework for creating dialect variation
dictionaries from monolingual data only, and use it to compile a ground truth
dataset consisting of 100K human-annotated German-Bavarian word pairs. We
evaluate how well nine state-of-the-art LLMs can judge Bavarian terms as
dialect translations, inflected variants, or unrelated forms of a given German
lemma. Our results show that LLMs perform best on nouns and lexically similar
word pairs, and struggle most in distinguishing between direct translations and
inflected variants. Interestingly, providing additional context in the form of
example usages improves the translation performance, but reduces their ability
to recognize dialect variants. This study highlights the limitations of LLMs in
dealing with orthographic dialect variation and emphasizes the need for future
work on adapting LLMs to dialects.

</details>


### [126] [Cross-Attention is Half Explanation in Speech-to-Text Models](https://arxiv.org/abs/2509.18010)
*Sara Papi,Dennis Fucci,Marco Gaido,Matteo Negri,Luisa Bentivogli*

Main category: cs.CL

TL;DR: 本文评估了语音转文本模型中交叉注意力机制的解释能力，发现其只能部分反映输入语音与生成文本之间的依赖关系，仅能捕捉约50%的输入相关性。


<details>
  <summary>Details</summary>
Motivation: 交叉注意力机制在语音处理领域被广泛用于下游任务，但其作为解释性工具的有效性在语音领域尚未得到充分验证。

Method: 通过比较交叉注意力分数与基于特征归因的输入显著性图，分析了单语/多语、单任务/多任务模型在不同规模下的表现。

Result: 交叉注意力分数与显著性解释存在中度到强相关性，但仅能解释52-75%的显著性，且只能捕捉约50%的输入相关性。

Conclusion: 交叉注意力作为解释性代理存在根本性局限，虽然提供信息但只能给出不完整的预测驱动因素视图。

Abstract: Cross-attention is a core mechanism in encoder-decoder architectures,
widespread in many fields, including speech-to-text (S2T) processing. Its
scores have been repurposed for various downstream applications--such as
timestamp estimation and audio-text alignment--under the assumption that they
reflect the dependencies between input speech representation and the generated
text. While the explanatory nature of attention mechanisms has been widely
debated in the broader NLP literature, this assumption remains largely
unexplored within the speech domain. To address this gap, we assess the
explanatory power of cross-attention in S2T models by comparing its scores to
input saliency maps derived from feature attribution. Our analysis spans
monolingual and multilingual, single-task and multi-task models at multiple
scales, and shows that attention scores moderately to strongly align with
saliency-based explanations, particularly when aggregated across heads and
layers. However, it also shows that cross-attention captures only about 50% of
the input relevance and, in the best case, only partially reflects how the
decoder attends to the encoder's representations--accounting for just 52-75% of
the saliency. These findings uncover fundamental limitations in interpreting
cross-attention as an explanatory proxy, suggesting that it offers an
informative yet incomplete view of the factors driving predictions in S2T
models.

</details>


### [127] [CorPipe at CRAC 2025: Evaluating Multilingual Encoders for Multilingual Coreference Resolution](https://arxiv.org/abs/2509.17858)
*Milan Straka*

Main category: cs.CL

TL;DR: CorPipe 25是CRAC 2025多语言共指消解共享任务的获胜系统，在LLM和无约束两个赛道中均以8个百分点的显著优势领先其他提交


<details>
  <summary>Details</summary>
Motivation: 参加CRAC 2025共享任务，该任务新增了LLM赛道并减少了数据集规模以降低计算需求

Method: 完全重新实现了之前的系统，从TensorFlow迁移到PyTorch框架

Result: 在LLM和无约束两个赛道中均以8个百分点的显著优势领先所有其他提交

Conclusion: 系统源代码和训练模型已在GitHub上公开

Abstract: We present CorPipe 25, the winning entry to the CRAC 2025 Shared Task on
Multilingual Coreference Resolution. This fourth iteration of the shared task
introduces a new LLM track alongside the original unconstrained track, features
reduced development and test sets to lower computational requirements, and
includes additional datasets. CorPipe 25 represents a complete reimplementation
of our previous systems, migrating from TensorFlow to PyTorch. Our system
significantly outperforms all other submissions in both the LLM and
unconstrained tracks by a substantial margin of 8 percentage points. The source
code and trained models are publicly available at
https://github.com/ufal/crac2025-corpipe.

</details>


### [128] [TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for Ü-Tsang, Amdo and Kham Speech Dataset Generation](https://arxiv.org/abs/2509.18060)
*Yutong Liu,Ziyue Zhang,Ban Ma-bao,Renzeng Duojie,Yuqing Cai,Yongbin Yu,Xiangxiang Wang,Fan Gao,Cheng Huang,Nyima Tashi*

Main category: cs.CL

TL;DR: TMD-TTS是一个统一的藏语多方言文本转语音框架，通过方言融合模块和方言专用动态路由网络来合成三种主要藏语方言的语音，显著提升了方言表达能力。


<details>
  <summary>Details</summary>
Motivation: 藏语作为一种低资源语言，其三种主要方言（卫藏、安多、康巴）的平行语音语料库有限，这限制了语音建模的进展。

Method: 提出TMD-TTS框架，包含方言融合模块和方言专用动态路由网络（DSDR-Net），能够捕捉跨方言的细粒度声学和语言变异。

Result: 通过广泛的主客观评估，TMD-TTS在方言表达力方面显著优于基线方法。合成的语音质量在语音到语音方言转换任务中得到验证。

Conclusion: 该框架有效解决了藏语多方言语音合成的资源限制问题，为低资源语言的语音建模提供了可行方案。

Abstract: Tibetan is a low-resource language with limited parallel speech corpora
spanning its three major dialects (\"U-Tsang, Amdo, and Kham), limiting
progress in speech modeling. To address this issue, we propose TMD-TTS, a
unified Tibetan multi-dialect text-to-speech (TTS) framework that synthesizes
parallel dialectal speech from explicit dialect labels. Our method features a
dialect fusion module and a Dialect-Specialized Dynamic Routing Network
(DSDR-Net) to capture fine-grained acoustic and linguistic variations across
dialects. Extensive objective and subjective evaluations demonstrate that
TMD-TTS significantly outperforms baselines in dialectal expressiveness. We
further validate the quality and utility of the synthesized speech through a
challenging Speech-to-Speech Dialect Conversion (S2SDC) task.

</details>


### [129] [Unsupervised Learning and Representation of Mandarin Tonal Categories by a Generative CNN](https://arxiv.org/abs/2509.17859)
*Kai Schenck,Gašper Beguš*

Main category: cs.CL

TL;DR: 本文提出了一种在完全无监督的人类语言习得模型中模拟声调学习的方法，证明生成模型ciwGAN能够在不使用标注数据的情况下学习汉语普通话的声调类别。


<details>
  <summary>Details</summary>
Motivation: 声调模式是语言中最复杂的学习目标之一，研究旨在探索无监督模型是否能够像人类学习者一样习得声调系统。

Method: 使用ciwGAN生成模型，在无标注数据上训练模型学习汉语普通话声调类别，并开发了追踪内部卷积层声调表示的方法。

Result: 所有三个训练模型在分类变量间都显示出显著的F0差异，仅使用男性语音训练的模型能够持续编码声调信息。

Conclusion: 模型不仅学会了普通话声调对比，而且学习到的系统对应于人类语言学习者的习得阶段，语言学工具有助于深度学习可解释性并可用于神经实验。

Abstract: This paper outlines the methodology for modeling tonal learning in fully
unsupervised models of human language acquisition. Tonal patterns are among the
computationally most complex learning objectives in language. We argue that a
realistic generative model of human language (ciwGAN) can learn to associate
its categorical variables with Mandarin Chinese tonal categories without any
labeled data. All three trained models showed statistically significant
differences in F0 across categorical variables. The model trained solely on
male tokens consistently encoded tone. Our results sug- gest that not only does
the model learn Mandarin tonal contrasts, but it learns a system that
corresponds to a stage of acquisition in human language learners. We also
outline methodology for tracing tonal representations in internal convolutional
layers, which shows that linguistic tools can contribute to interpretability of
deep learning and can ultimately be used in neural experiments.

</details>


### [130] [SEQR: Secure and Efficient QR-based LoRA Routing](https://arxiv.org/abs/2509.18093)
*William Fleshman,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 该论文提出了SEQR，一种无监督的LoRA路由算法，通过最大化激活范数来高效选择适配器，解决了在隐私敏感环境中LoRA路由的挑战。


<details>
  <summary>Details</summary>
Motivation: 在隐私敏感环境中，监督训练路由器可能引发隐私问题，因此需要开发无监督的LoRA路由方法。

Method: 基于激活范数最大化理论框架，设计了SEQR算法，该算法能够高效识别范数最大的适配器，并提供严格的路由保证。

Result: 实验验证表明SEQR在多任务性能和效率方面均有提升，具有高度可扩展性。

Conclusion: SEQR是一种高效、可扩展的无监督LoRA路由解决方案，适用于动态LoRA组合场景。

Abstract: Low-Rank Adaptation (LoRA) has become a standard technique for
parameter-efficient fine-tuning of large language models, enabling large
libraries of LoRAs, each for a specific task or domain. Efficiently selecting
the correct LoRA adapter for a given input remains a challenge, particularly in
secure environments where supervised training of routers may raise privacy
concerns. Motivated by previous approaches, we formalize the goal of
unsupervised LoRA routing in terms of activation norm maximization, providing a
theoretical framework for analysis. We demonstrate the discriminative power of
activation norms and introduce SEQR, an unsupervised LoRA routing algorithm
designed to maximize efficiency while providing strict routing guarantees. SEQR
provably identifies the norm-maximizing adapter with significantly greater
efficiency, making it a highly scalable and effective solution for dynamic LoRA
composition. We validate our results through experiments that demonstrate
improved multi-task performance and efficiency.

</details>


### [131] [SiDiaC: Sinhala Diachronic Corpus](https://arxiv.org/abs/2509.17912)
*Nevidu Jayatilleke,Nisansa de Silva*

Main category: cs.CL

TL;DR: SiDiaC是首个全面的僧伽罗语历时语料库，涵盖公元5世纪至20世纪的文本，包含58k词、46部文学作品，按体裁分类，为僧伽罗语NLP研究提供基础资源


<details>
  <summary>Details</summary>
Motivation: 解决僧伽罗语作为低资源语言缺乏历时语料库的问题，为词汇变化、新词追踪、历史句法和词典编纂等研究提供数据支持

Method: 从斯里兰卡国家图书馆获取文本，使用Google Document AI OCR数字化，进行后处理校正格式和正字法现代化，借鉴FarPaHC等语料库的句法标注和文本规范化策略

Result: 成功构建了包含58,000词、46部文学作品的历时语料库，按主要分类（非虚构/虚构）和次要分类（宗教、历史、诗歌、语言、医学）进行系统组织

Conclusion: 尽管面临稀有文本获取困难和依赖二手日期来源等挑战，SiDiaC仍为僧伽罗语NLP研究奠定了重要基础，显著扩展了僧伽罗语研究资源

Abstract: SiDiaC, the first comprehensive Sinhala Diachronic Corpus, covers a
historical span from the 5th to the 20th century CE. SiDiaC comprises 58k words
across 46 literary works, annotated carefully based on the written date, after
filtering based on availability, authorship, copyright compliance, and data
attribution. Texts from the National Library of Sri Lanka were digitised using
Google Document AI OCR, followed by post-processing to correct formatting and
modernise the orthography. The construction of SiDiaC was informed by practices
from other corpora, such as FarPaHC, particularly in syntactic annotation and
text normalisation strategies, due to the shared characteristics of
low-resourced language status. This corpus is categorised based on genres into
two layers: primary and secondary. Primary categorisation is binary,
classifying each book into Non-Fiction or Fiction, while the secondary
categorisation is more specific, grouping texts under Religious, History,
Poetry, Language, and Medical genres. Despite challenges including limited
access to rare texts and reliance on secondary date sources, SiDiaC serves as a
foundational resource for Sinhala NLP, significantly extending the resources
available for Sinhala, enabling diachronic studies in lexical change, neologism
tracking, historical syntax, and corpus-based lexicography.

</details>


### [132] [Improving Zero-shot Sentence Decontextualisation with Content Selection and Planning](https://arxiv.org/abs/2509.17921)
*Zhenyun Deng,Yulong Chen,Andreas Vlachos*

Main category: cs.CL

TL;DR: 提出零样本去语境化框架，通过内容选择和规划使句子脱离上下文后仍可理解


<details>
  <summary>Details</summary>
Motivation: 从文档中提取的句子往往缺乏必要的上下文信息（如共指和背景信息），导致理解困难

Method: 1) 将句子分割为基本语义独立单元；2) 识别潜在歧义单元；3) 基于篇章关系从上下文中提取相关单元；4) 生成内容计划重写句子

Result: 实验结果表明该方法在句子去语境化任务中具有竞争力，生成的句子具有更好的语义完整性和篇章连贯性，优于现有方法

Conclusion: 提出的内容选择和规划框架能有效解决句子脱离上下文后的理解问题

Abstract: Extracting individual sentences from a document as evidence or reasoning
steps is commonly done in many NLP tasks. However, extracted sentences often
lack context necessary to make them understood, e.g., coreference and
background information. To this end, we propose a content selection and
planning framework for zero-shot decontextualisation, which determines what
content should be mentioned and in what order for a sentence to be understood
out of context. Specifically, given a potentially ambiguous sentence and its
context, we first segment it into basic semantically-independent units. We then
identify potentially ambiguous units from the given sentence, and extract
relevant units from the context based on their discourse relations. Finally, we
generate a content plan to rewrite the sentence by enriching each ambiguous
unit with its relevant units. Experimental results demonstrate that our
approach is competitive for sentence decontextualisation, producing sentences
that exhibit better semantic integrity and discourse coherence, outperforming
existing methods.

</details>


### [133] [Training-free Truthfulness Detection via Value Vectors in LLMs](https://arxiv.org/abs/2509.17932)
*Runheng Liu,Heyan Huang,Xingchen Xiao,Zhijing Wu*

Main category: cs.CL

TL;DR: 本文提出TruthV方法，通过利用MLP模块中的值向量来检测大语言模型生成内容的真实性，无需训练且具有可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖训练探针或仅关注注意力机制，存在可扩展性和泛化性问题，且忽略了MLP模块在事实召回中的重要作用。

Method: 发现MLP模块中的某些值向量具有与真实性相关的统计模式，基于此提出TruthV方法，直接利用这些值向量进行真实性检测。

Result: 在NoVo基准测试中，TruthV显著优于NoVo和log-likelihood基线方法，证明了MLP模块编码了丰富的真实性信号。

Conclusion: MLP模块在真实性检测中具有重要作用，为理解LLM内部真实性表示提供了新视角，推动了可扩展和可解释的真实性检测研究。

Abstract: Large language models often generate factually incorrect outputs, motivating
efforts to detect the truthfulness of their content. Most existing approaches
rely on training probes over internal activations, but these methods suffer
from scalability and generalization issues. A recent training-free method,
NoVo, addresses this challenge by exploiting statistical patterns from the
model itself. However, it focuses exclusively on attention mechanisms,
potentially overlooking the MLP module-a core component of Transformer models
known to support factual recall. In this paper, we show that certain value
vectors within MLP modules exhibit truthfulness-related statistical patterns.
Building on this insight, we propose TruthV, a simple and interpretable
training-free method that detects content truthfulness by leveraging these
value vectors. On the NoVo benchmark, TruthV significantly outperforms both
NoVo and log-likelihood baselines, demonstrating that MLP modules-despite being
neglected in prior training-free efforts-encode rich and useful signals for
truthfulness detection. These findings offer new insights into how truthfulness
is internally represented in LLMs and motivate further research on scalable and
interpretable truthfulness detection.

</details>


### [134] [D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language Models](https://arxiv.org/abs/2509.17938)
*Satyapriya Krishna,Andy Zou,Rahul Gupta,Eliot Krzysztof Jones,Nick Winter,Dan Hendrycks,J. Zico Kolter,Matt Fredrikson,Spyros Matsoukas*

Main category: cs.CL

TL;DR: 该论文提出了D-REX数据集，用于评估大型语言模型内部恶意推理与表面无害输出之间的不一致性，揭示了现有安全机制的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前的安全评估方法主要关注明显有害的输出，但忽视了模型在内部进行恶意推理却产生表面无害输出的欺骗性行为，这种漏洞可能被系统提示注入攻击利用。

Method: 通过竞争性红队演练构建D-REX数据集，包含对抗性系统提示、用户查询、模型表面无害响应和揭示恶意意图的内部思维链。

Result: D-REX对现有模型和安全机制构成了显著挑战，表明需要开发能够检查模型内部推理过程的新技术。

Conclusion: 该研究强调了仅关注最终输出不足以确保模型安全，必须开发能够检测欺骗性对齐的新评估方法。

Abstract: The safety and alignment of Large Language Models (LLMs) are critical for
their responsible deployment. Current evaluation methods predominantly focus on
identifying and preventing overtly harmful outputs. However, they often fail to
address a more insidious failure mode: models that produce benign-appearing
outputs while operating on malicious or deceptive internal reasoning. This
vulnerability, often triggered by sophisticated system prompt injections,
allows models to bypass conventional safety filters, posing a significant,
underexplored risk. To address this gap, we introduce the Deceptive Reasoning
Exposure Suite (D-REX), a novel dataset designed to evaluate the discrepancy
between a model's internal reasoning process and its final output. D-REX was
constructed through a competitive red-teaming exercise where participants
crafted adversarial system prompts to induce such deceptive behaviors. Each
sample in D-REX contains the adversarial system prompt, an end-user's test
query, the model's seemingly innocuous response, and, crucially, the model's
internal chain-of-thought, which reveals the underlying malicious intent. Our
benchmark facilitates a new, essential evaluation task: the detection of
deceptive alignment. We demonstrate that D-REX presents a significant challenge
for existing models and safety mechanisms, highlighting the urgent need for new
techniques that scrutinize the internal processes of LLMs, not just their final
outputs.

</details>


### [135] [Dorabella Cipher as Musical Inspiration](https://arxiv.org/abs/2509.17950)
*Bradley Hauer,Colin Choi,Abram Hindle,Scott Smallwood,Grzegorz Kondrak*

Main category: cs.CL

TL;DR: 本文探讨了Dorabella密码可能是加密音乐而非英文文本的假设，通过开发简化的音乐符号和n-gram模型，尝试从密码中重构旋律，并将解密过程视为作曲过程的一部分。


<details>
  <summary>Details</summary>
Motivation: Dorabella密码是英国作曲家爱德华·埃尔加写的一份加密笔记，一个多世纪以来一直未被破解。大多数解决方案都假设它是英文文本，但本文研究它可能代表加密音乐的可能性。

Method: 开发了一种简化的音乐符号，并使用n-gram音乐模型，通过对现有音乐语料库进行单字母替换加密来验证模型。然后将这些方法应用于Dorabella密码。

Result: 应用该方法到Dorabella密码后，产生了一个具有音乐品质的解密结果，并通过艺术作曲转化为可听的旋律。

Conclusion: 本文并不主张最终结果是唯一正确的解决方案，而是将解密过程框架为作曲过程的一部分。

Abstract: The Dorabella cipher is an encrypted note written by English composer Edward
Elgar, which has defied decipherment attempts for more than a century. While
most proposed solutions are English texts, we investigate the hypothesis that
Dorabella represents enciphered music. We weigh the evidence for and against
the hypothesis, devise a simplified music notation, and attempt to reconstruct
a melody from the cipher. Our tools are n-gram models of music which we
validate on existing music corpora enciphered using monoalphabetic
substitution. By applying our methods to Dorabella, we produce a decipherment
with musical qualities, which is then transformed via artful composition into a
listenable melody. Far from arguing that the end result represents the only
true solution, we instead frame the process of decipherment as part of the
composition process.

</details>


### [136] [Bringing Pedagogy into Focus: Evaluating Virtual Teaching Assistants' Question-Answering in Asynchronous Learning Environments](https://arxiv.org/abs/2509.17961)
*Li Siyan,Zhen Xu,Vethavikashini Chithrra Raghuram,Xuanming Zhang,Renzhe Yu,Zhou Yu*

Main category: cs.CL

TL;DR: 本文提出了一个基于学习科学的评估框架，用于评估异步学习环境中虚拟教学助手的教学效果，通过构建分类器来分析VTA响应的教育价值。


<details>
  <summary>Details</summary>
Motivation: 异步学习环境中的虚拟教学助手缺乏基于教育理论的严谨评估方法，现有评估往往依赖表面指标，难以比较不同VTA系统的教学有效性。

Method: 构建基于专家标注的分类器，对VTA在论坛讨论中的响应进行教育价值评估，识别提高准确性的方法并分析泛化挑战。

Result: 建立了有效的分类器来评估VTA响应的教学效果，为理论驱动的VTA系统评估奠定了基础。

Conclusion: 该工作为教育AI提供了更有效的教学评估框架，推动了教育中AI系统的教学效果提升。

Abstract: Asynchronous learning environments (ALEs) are widely adopted for formal and
informal learning, but timely and personalized support is often limited. In
this context, Virtual Teaching Assistants (VTAs) can potentially reduce the
workload of instructors, but rigorous and pedagogically sound evaluation is
essential. Existing assessments often rely on surface-level metrics and lack
sufficient grounding in educational theories, making it difficult to
meaningfully compare the pedagogical effectiveness of different VTA systems. To
bridge this gap, we propose an evaluation framework rooted in learning sciences
and tailored to asynchronous forum discussions, a common VTA deployment context
in ALE. We construct classifiers using expert annotations of VTA responses on a
diverse set of forum posts. We evaluate the effectiveness of our classifiers,
identifying approaches that improve accuracy as well as challenges that hinder
generalization. Our work establishes a foundation for theory-driven evaluation
of VTA systems, paving the way for more pedagogically effective AI in
education.

</details>


### [137] [WenetSpeech-Chuan: A Large-Scale Sichuanese Corpus with Rich Annotation for Dialectal Speech Processing](https://arxiv.org/abs/2509.18004)
*Yuhang Dai,Ziyu Zhang,Shuai Wang,Longhao Li,Zhao Guo,Tianlun Zuo,Shuiyuan Wang,Hongfei Xue,Chengyou Wang,Qing Wang,Xin Xu,Hui Bu,Jie Li,Jian Kang,Binbin Zhang,Lei Xie*

Main category: cs.CL

TL;DR: 提出了WenetSpeech-Chuan，一个10,000小时的四川方言语音语料库，包含完整的处理流程和评估基准，显著提升了方言语音技术性能


<details>
  <summary>Details</summary>
Motivation: 解决四川方言等方言语音技术发展中大规模开源数据稀缺的问题，降低方言语音处理研究门槛，促进AI公平性

Method: 开发了Chuan-Pipeline数据处理框架，构建了10,000小时带丰富标注的语料库，并发布了手动验证的ASR和TTS评估基准

Result: 基于WenetSpeech-Chuan训练的模型在开源系统中达到最先进性能，与商业服务结果相当

Conclusion: 作为最大的四川方言开源语料库，该工作不仅促进了方言语音处理研究，还在减少语音技术偏见方面发挥重要作用

Abstract: The scarcity of large-scale, open-source data for dialects severely hinders
progress in speech technology, a challenge particularly acute for the widely
spoken Sichuanese dialects of Chinese. To address this critical gap, we
introduce WenetSpeech-Chuan, a 10,000-hour, richly annotated corpus constructed
using our novel Chuan-Pipeline, a complete data processing framework for
dialectal speech. To facilitate rigorous evaluation and demonstrate the
corpus's effectiveness, we also release high-quality ASR and TTS benchmarks,
WenetSpeech-Chuan-Eval, with manually verified transcriptions. Experiments show
that models trained on WenetSpeech-Chuan achieve state-of-the-art performance
among open-source systems and demonstrate results comparable to commercial
services. As the largest open-source corpus for Sichuanese dialects,
WenetSpeech-Chuan not only lowers the barrier to research in dialectal speech
processing but also plays a crucial role in promoting AI equity and mitigating
bias in speech technologies. The corpus, benchmarks, models, and receipts are
publicly available on our project page.

</details>


### [138] [RadEval: A framework for radiology text evaluation](https://arxiv.org/abs/2509.18030)
*Justin Xu,Xi Zhang,Javid Abderezaei,Julie Bauml,Roger Boodoo,Fatemeh Haghighi,Ali Ganjizadeh,Eric Brattain,Dave Van Veen,Zaiqiao Meng,David Eyre,Jean-Benoit Delbrouck*

Main category: cs.CL

TL;DR: RadEval是一个统一的开源框架，用于评估放射学文本，整合了多种评估指标，包括经典n-gram重叠度、上下文测量、临床概念评分和基于LLM的评估器，并提供了统计测试工具和基准模型评估。


<details>
  <summary>Details</summary>
Motivation: 当前放射学报告生成领域缺乏统一、标准化的评估框架，不同研究使用的评估指标不一致，难以进行公平比较和可复现性研究。

Method: 开发RadEval框架，整合多种评估指标，标准化实现，扩展GREEN支持多种成像模态，预训练领域特定的放射学编码器，并发布专家标注数据集。

Result: 展示了不同指标与放射科医生判断的相关性，证明了领域特定编码器的零样本检索性能，提供了丰富的标注数据集和基准评估结果。

Conclusion: RadEval为放射学报告生成研究提供了可复现和稳健的基准测试工具，促进了该领域的标准化评估和比较。

Abstract: We introduce RadEval, a unified, open-source framework for evaluating
radiology texts. RadEval consolidates a diverse range of metrics, from classic
n-gram overlap (BLEU, ROUGE) and contextual measures (BERTScore) to clinical
concept-based scores (F1CheXbert, F1RadGraph, RaTEScore, SRR-BERT,
TemporalEntityF1) and advanced LLM-based evaluators (GREEN). We refine and
standardize implementations, extend GREEN to support multiple imaging
modalities with a more lightweight model, and pretrain a domain-specific
radiology encoder, demonstrating strong zero-shot retrieval performance. We
also release a richly annotated expert dataset with over 450 clinically
significant error labels and show how different metrics correlate with
radiologist judgment. Finally, RadEval provides statistical testing tools and
baseline model evaluations across multiple publicly available datasets,
facilitating reproducibility and robust benchmarking in radiology report
generation.

</details>


### [139] [The PIMMUR Principles: Ensuring Validity in Collective Behavior of LLM Societies](https://arxiv.org/abs/2509.18052)
*Jiaxu Zhou,Jen-tse Huang,Xuhui Zhou,Man Ho Lam,Xintao Wang,Hao Zhu,Wenxuan Wang,Maarten Sap*

Main category: cs.CL

TL;DR: 本文提出了PIMMUR原则作为LLM社会模拟研究的六个必要方法论标准，指出当前研究中存在的系统性缺陷，并通过重新实验验证了这些原则的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM社会模拟研究存在系统性方法论缺陷，许多研究采用实验设计削弱了其主张的有效性，需要建立可靠的方法论标准。

Method: 通过调查40多篇论文识别出六个常见方法缺陷，将其形式化为PIMMUR原则，并用支持PIMMUR的框架重新运行五个代表性研究进行验证。

Result: 研究发现GPT-4o和Qwen-3在53.1%的情况下能正确推断实验假设，违反无意识原则；在更严格条件下，许多报告的社会现象未能重现。

Conclusion: PIMMUR原则是可信LLM社会模拟的必要条件，为AI社会研究建立了方法论标准，有助于提高研究的可靠性和可重复性。

Abstract: Large Language Models (LLMs) are increasingly used for social simulation,
where populations of agents are expected to reproduce human-like collective
behavior. However, we find that many recent studies adopt experimental designs
that systematically undermine the validity of their claims. From a survey of
over 40 papers, we identify six recurring methodological flaws: agents are
often homogeneous (Profile), interactions are absent or artificially imposed
(Interaction), memory is discarded (Memory), prompts tightly control outcomes
(Minimal-Control), agents can infer the experimental hypothesis (Unawareness),
and validation relies on simplified theoretical models rather than real-world
data (Realism). For instance, GPT-4o and Qwen-3 correctly infer the underlying
social experiment in 53.1% of cases when given instructions from prior
work-violating the Unawareness principle. We formalize these six requirements
as the PIMMUR principles and argue they are necessary conditions for credible
LLM-based social simulation. To demonstrate their impact, we re-run five
representative studies using a framework that enforces PIMMUR and find that the
reported social phenomena frequently fail to emerge under more rigorous
conditions. Our work establishes methodological standards for LLM-based
multi-agent research and provides a foundation for more reliable and
reproducible claims about "AI societies."

</details>


### [140] [ARK-V1: An LLM-Agent for Knowledge Graph Question Answering Requiring Commonsense Reasoning](https://arxiv.org/abs/2509.18063)
*Jan-Felix Klein,Lars Ohnemus*

Main category: cs.CL

TL;DR: ARK-V1是一个简单的知识图谱代理，通过迭代探索图谱来回答自然语言查询，在需要特定领域知识的任务中显著优于思维链基线。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型依赖内部知识，但这些知识往往不足、过时或不正确，特别是在需要特定领域知识的问题上。知识图谱提供结构化外部知识，但复杂性和多跳推理需求使其集成具有挑战性。

Method: 提出ARK-V1知识图谱代理，使用未微调的最先进LLM作为骨干，在CoLoTa数据集上进行评估，该数据集需要基于知识图谱和常识推理的长尾实体处理。

Result: ARK-V1的条件准确率显著高于思维链基线，更大的骨干模型在覆盖率、正确性和稳定性方面表现出明显改善趋势。

Conclusion: ARK-V1证明了结合知识图谱和LLM的有效性，为处理需要特定领域知识的复杂推理任务提供了有前景的解决方案。

Abstract: Large Language Models (LLMs) show strong reasoning abilities but rely on
internalized knowledge that is often insufficient, outdated, or incorrect when
trying to answer a question that requires specific domain knowledge. Knowledge
Graphs (KGs) provide structured external knowledge, yet their complexity and
multi-hop reasoning requirements make integration challenging. We present
ARK-V1, a simple KG-agent that iteratively explores graphs to answer natural
language queries. We evaluate several not fine-tuned state-of-the art LLMs as
backbones for ARK-V1 on the CoLoTa dataset, which requires both KG-based and
commonsense reasoning over long-tail entities. ARK-V1 achieves substantially
higher conditional accuracies than Chain-of-Thought baselines, and larger
backbone models show a clear trend toward better coverage, correctness, and
stability.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [141] [Identifying Critical Pathways in Coronary Heart Disease via Fuzzy Subgraph Connectivity](https://arxiv.org/abs/2509.16288)
*Shanookha Ali,Nitha Niralda P C*

Main category: cs.AI

TL;DR: 本文提出使用模糊子图连通性（FSC）方法来建模冠心病风险预测中的不确定性，通过构建模糊CHD图来识别关键诊断路径和风险因素。


<details>
  <summary>Details</summary>
Motivation: 冠心病（CHD）是由不可控因素、可控生活方式因素和临床指标之间的复杂相互作用引起的，这些关系通常存在不确定性。需要一种能够捕捉这种不确定性的系统工具。

Method: 构建一个模糊CHD图，顶点代表不可控、可控和指标组件，边由模糊隶属度加权。使用模糊子图连通性（FSC）评估连通性，识别最强诊断路径、主导风险因素和关键桥梁。

Result: 结果显示FSC能够突出有影响力的路径，限定最弱和最强相关性之间的连通性边界，并揭示那些移除后会降低预测强度的关键边。

Conclusion: FSC为CHD风险预测中的不确定性建模提供了一个可解释且稳健的框架，支持临床决策制定。

Abstract: Coronary heart disease (CHD) arises from complex interactions among
uncontrollable factors, controllable lifestyle factors, and clinical
indicators, where relationships are often uncertain. Fuzzy subgraph
connectivity (FSC) provides a systematic tool to capture such imprecision by
quantifying the strength of association between vertices and subgraphs in fuzzy
graphs. In this work, a fuzzy CHD graph is constructed with vertices for
uncontrollable, controllable, and indicator components, and edges weighted by
fuzzy memberships. Using FSC, we evaluate connectivity to identify strongest
diagnostic routes, dominant risk factors, and critical bridges. Results show
that FSC highlights influential pathways, bounds connectivity between weakest
and strongest correlations, and reveals critical edges whose removal reduces
predictive strength. Thus, FSC offers an interpretable and robust framework for
modeling uncertainty in CHD risk prediction and supporting clinical
decision-making.

</details>


### [142] [A global view of diverse construction methods of fuzzy implication functions rooted on F-chains](https://arxiv.org/abs/2509.16298)
*Raquel Fernandez-Peralta,Juan Vicente Riera*

Main category: cs.AI

TL;DR: 本文提出了一种广义的F-链构造方法，用于从现有模糊蕴涵函数生成新的模糊蕴涵函数，并证明该方法可以统一多种现有构造技术。


<details>
  <summary>Details</summary>
Motivation: 模糊蕴涵函数在模糊逻辑框架中具有重要作用，但其多样的定义和构造方法需要更深入的理论理解，特别是不同构造方法之间的结构关系。

Method: 推广了Mesiar等人最近提出的F-链构造方法，使用模糊蕴涵函数集合而非单一函数，并采用两个不同的递增函数代替单一的F-链，分析该构造下的性质保持条件。

Result: 证明了广义F-链构造是一个统一框架，可以重新表述多种现有构造方法，包括对置、聚合和广义垂直/水平阈值方法，揭示了不同构造策略之间的结构相似性。

Conclusion: 该研究为模糊蕴涵构造方法提供了一个统一的视角，揭示了看似不同的构造策略之间的结构联系，有助于深化对模糊蕴涵函数理论的理解。

Abstract: Fuzzy implication functions are one of the most important operators used in
the fuzzy logic framework. While their flexible definition allows for diverse
families with distinct properties, this variety needs a deeper theoretical
understanding of their structural relationships. In this work, we focus on the
study of construction methods, which employ different techniques to generate
new fuzzy implication functions from existing ones. Particularly, we generalize
the $F$-chain-based construction, recently introduced by Mesiar et al. to
extend a method for constructing aggregation functions to the context of fuzzy
implication functions. Our generalization employs collections of fuzzy
implication functions rather than single ones, and uses two different
increasing functions instead of a unique $F$-chain. We analyze property
preservation under this construction and establish sufficient conditions.
Furthermore, we demonstrate that our generalized $F$-chain-based construction
is a unifying framework for several existing methods. In particular, we show
that various construction techniques, such as contraposition, aggregation, and
generalized vertical/horizontal threshold methods, can be reformulated within
our approach. This reveals structural similarities between seemingly distinct
construction strategies and provides a cohesive perspective on fuzzy
implication construction methods.

</details>


### [143] [On the Non-Uniqueness of Representation of $(U,N)$-Implications](https://arxiv.org/abs/2509.16299)
*Raquel Fernandez-Peralta,Andrea Mesiarová-Zemánková*

Main category: cs.AI

TL;DR: 本文证明了(U,N)-蕴涵函数不一定具有唯一表示，即使模糊否定N是连续的，推翻了之前关于唯一性的假设，并对连续和非连续基础函数的唯一性条件进行了全面研究。


<details>
  <summary>Details</summary>
Motivation: 之前的研究假设在连续模糊否定N的情况下，(U,N)-蕴涵函数具有唯一表示。本文旨在验证这一假设的正确性，并深入探讨这类模糊蕴涵算子的结构特性。

Method: 通过理论分析和数学证明，首先反驳了(U,N)-蕴涵函数在连续模糊否定条件下的唯一性假设，然后系统地研究了基于连续和非连续基础函数的uninorms的唯一性条件。

Result: 成功证明了(U,N)-蕴涵函数即使在连续模糊否定N的情况下也不一定具有唯一表示，发现了之前理论中的缺陷，并建立了更全面的唯一性条件框架。

Conclusion: 本研究推翻了关于(U,N)-蕴涵函数唯一性的重要假设，为模糊逻辑系统中这类基本算子的结构特性提供了新的理论见解，对模糊逻辑理论发展具有重要意义。

Abstract: Fuzzy implication functions constitute fundamental operators in fuzzy logic
systems, extending classical conditionals to manage uncertainty in logical
inference. Among the extensive families of these operators, generalizations of
the classical material implication have received considerable theoretical
attention, particularly $(S,N)$-implications constructed from t-conorms and
fuzzy negations, and their further generalizations to $(U,N)$-implications
using disjunctive uninorms. Prior work has established characterization
theorems for these families under the assumption that the fuzzy negation $N$ is
continuous, ensuring uniqueness of representation. In this paper, we disprove
this last fact for $(U,N)$-implications and we show that they do not
necessarily possess a unique representation, even if the fuzzy negation is
continuous. Further, we provide a comprehensive study of uniqueness conditions
for both uninorms with continuous and non-continuous underlying functions. Our
results offer important theoretical insights into the structural properties of
these operators.

</details>


### [144] [Generalizability of Large Language Model-Based Agents: A Comprehensive Survey](https://arxiv.org/abs/2509.16330)
*Minxing Zhang,Yi Yang,Roy Xie,Bhuwan Dhingra,Shuyan Zhou,Jian Pei*

Main category: cs.AI

TL;DR: 这篇论文是关于LLM智能体泛化能力的综述研究，分析了智能体在不同指令、任务、环境和领域中保持性能一致性的挑战，并提出了系统性的评估和改进方法。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在网页导航、家庭机器人等领域的广泛应用，确保智能体在超出其微调数据的场景中保持性能一致性成为关键挑战，但目前缺乏对智能体泛化能力的明确定义和系统评估方法。

Method: 通过建立层次化的领域-任务本体论来界定智能体泛化能力的边界，回顾现有数据集、评估维度和指标，将改进方法分为骨干LLM、智能体组件及其交互三类，并区分通用框架与通用智能体。

Result: 提出了首个关于LLM智能体泛化能力的全面综述，建立了系统性的评估框架，并识别了标准化框架、基于方差和成本的指标等关键挑战。

Conclusion: 该综述为构建能够可靠泛化到多样化应用的LLM智能体奠定了理论基础，指出了将方法论创新与架构设计相结合的未来研究方向。

Abstract: Large Language Model (LLM)-based agents have emerged as a new paradigm that
extends LLMs' capabilities beyond text generation to dynamic interaction with
external environments. By integrating reasoning with perception, memory, and
tool use, agents are increasingly deployed in diverse domains like web
navigation and household robotics. A critical challenge, however, lies in
ensuring agent generalizability - the ability to maintain consistent
performance across varied instructions, tasks, environments, and domains,
especially those beyond agents' fine-tuning data. Despite growing interest, the
concept of generalizability in LLM-based agents remains underdefined, and
systematic approaches to measure and improve it are lacking. In this survey, we
provide the first comprehensive review of generalizability in LLM-based agents.
We begin by emphasizing agent generalizability's importance by appealing to
stakeholders and clarifying the boundaries of agent generalizability by
situating it within a hierarchical domain-task ontology. We then review
datasets, evaluation dimensions, and metrics, highlighting their limitations.
Next, we categorize methods for improving generalizability into three groups:
methods for the backbone LLM, for agent components, and for their interactions.
Moreover, we introduce the distinction between generalizable frameworks and
generalizable agents and outline how generalizable frameworks can be translated
into agent-level generalizability. Finally, we identify critical challenges and
future directions, including developing standardized frameworks, variance- and
cost-based metrics, and approaches that integrate methodological innovations
with architecture-level designs. By synthesizing progress and highlighting
opportunities, this survey aims to establish a foundation for principled
research on building LLM-based agents that generalize reliably across diverse
applications.

</details>


### [145] [Psychometric Personality Shaping Modulates Capabilities and Safety in Language Models](https://arxiv.org/abs/2509.16332)
*Stephen Fitz,Peter Romero,Steven Basart,Sipeng Chen,Jose Hernandez-Orallo*

Main category: cs.AI

TL;DR: 该论文研究了通过调节大语言模型的五大人格特质（特别是责任心）如何影响模型在安全性和能力基准测试中的表现，发现降低责任心会显著降低模型的安全性和一般能力。


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究表明LLMs表现出可测量的合成人格特质，但关于调节这些特质如何影响模型行为的研究还很缺乏，特别是在安全性和能力方面的相互作用。

Method: 基于五大人格框架，通过心理测量人格控制来调节模型的人格特质，并在WMDP、TruthfulQA、ETHICS、Sycophancy和MMLU等基准测试上评估模型行为变化。

Result: 实验显示降低责任心会导致安全相关指标显著下降，同时在MMLU上的一般能力也会降低，表明人格塑造是影响模型安全性和能力的重要控制维度。

Conclusion: 人格塑造是一个强大但未被充分探索的模型控制轴，与安全性和一般能力都有关联，这推动了需要开展人格敏感性安全评估和动态行为控制的新研究方向。

Abstract: Large Language Models increasingly mediate high-stakes interactions,
intensifying research on their capabilities and safety. While recent work has
shown that LLMs exhibit consistent and measurable synthetic personality traits,
little is known about how modulating these traits affects model behavior. We
address this gap by investigating how psychometric personality control grounded
in the Big Five framework influences AI behavior in the context of capability
and safety benchmarks. Our experiments reveal striking effects: for example,
reducing conscientiousness leads to significant drops in safety-relevant
metrics on benchmarks such as WMDP, TruthfulQA, ETHICS, and Sycophancy as well
as reduction in general capabilities as measured by MMLU. These findings
highlight personality shaping as a powerful and underexplored axis of model
control that interacts with both safety and general competence. We discuss the
implications for safety evaluation, alignment strategies, steering model
behavior after deployment, and risks associated with possible exploitation of
these findings. Our findings motivate a new line of research on
personality-sensitive safety evaluations and dynamic behavioral control in
LLMs.

</details>


### [146] [A Unified AI Approach for Continuous Monitoring of Human Health and Diseases from Intensive Care Unit to Home with Physiological Foundation Models (UNIPHY+)](https://arxiv.org/abs/2509.16348)
*Minxiao Wang,Saurabh Kataria,Juntong Ni,Timothy G. Buchman,Jocelyn Grunwell,Mark Mai,Wei Jin,Matthew Clark,Stephanie Brown,Michael Fundora,Puneet Sharma,Tony Pan,Sam Khan,Timothy Ruchti,Naveen Muthu,Kevin Maher,Sivasubramanium V Bhavani,Xiao Hu*

Main category: cs.AI

TL;DR: UNIPHY+是一个统一的生理基础模型框架，旨在使用普遍可获得的生理数据实现跨护理场景的连续人类健康和疾病监测。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够整合不同护理场景（从重症监护到动态监测）的通用生理AI模型，以支持临床决策和长期健康监测。

Method: 提出在预训练、微调和轻量级模型个性化过程中整合上下文信息的新策略，包括多模态学习、特征融合调优和知识蒸馏。

Result: 通过广泛的用例测试来验证UNIPHY+能够实现可泛化、可扩展和个性化的生理AI。

Conclusion: UNIPHY+框架有潜力赋能通用、可扩展和个性化的生理AI，支持临床决策和长期健康监测。

Abstract: We present UNIPHY+, a unified physiological foundation model (physioFM)
framework designed to enable continuous human health and diseases monitoring
across care settings using ubiquitously obtainable physiological data. We
propose novel strategies for incorporating contextual information during
pretraining, fine-tuning, and lightweight model personalization via multi-modal
learning, feature fusion-tuning, and knowledge distillation. We advocate
testing UNIPHY+ with a broad set of use cases from intensive care to ambulatory
monitoring in order to demonstrate that UNIPHY+ can empower generalizable,
scalable, and personalized physiological AI to support both clinical
decision-making and long-term health monitoring.

</details>


### [147] [Evaluation of Causal Reasoning for Large Language Models in Contextualized Clinical Scenarios of Laboratory Test Interpretation](https://arxiv.org/abs/2509.16372)
*Balu Bhasuran,Mattia Prosperi,Karim Hanna,John Petrilli,Caretia JeLayne Washington,Zhe He*

Main category: cs.AI

TL;DR: 本研究评估了大型语言模型在临床实验室测试场景中的因果推理能力，GPT-o1在各项因果推理任务中表现优于Llama-3.2-8b-instruct，但在高风险临床应用前仍需改进。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在临床环境中的因果推理能力，特别是基于Pearl因果阶梯的三个层次：关联、干预和反事实推理。

Method: 使用99个临床实验室测试场景，测试GPT-o1和Llama-3.2-8b-instruct两个模型，由四位医学专家评估模型在关联、干预和反事实推理方面的表现。

Result: GPT-o1整体表现更优（AUROC=0.80），在关联（0.75 vs 0.72）、干预（0.84 vs 0.70）和反事实推理（0.84 vs 0.69）方面均优于Llama-3.2-8b-instruct。两个模型在干预问题上表现最好，反事实推理最差。

Conclusion: GPT-o1提供更一致的因果推理，但在高风险临床应用前需要进一步改进，特别是在反事实推理方面。

Abstract: This study evaluates causal reasoning in large language models (LLMs) using
99 clinically grounded laboratory test scenarios aligned with Pearl's Ladder of
Causation: association, intervention, and counterfactual reasoning. We examined
common laboratory tests such as hemoglobin A1c, creatinine, and vitamin D, and
paired them with relevant causal factors including age, gender, obesity, and
smoking. Two LLMs - GPT-o1 and Llama-3.2-8b-instruct - were tested, with
responses evaluated by four medically trained human experts. GPT-o1
demonstrated stronger discriminative performance (AUROC overall = 0.80 +/-
0.12) compared to Llama-3.2-8b-instruct (0.73 +/- 0.15), with higher scores
across association (0.75 vs 0.72), intervention (0.84 vs 0.70), and
counterfactual reasoning (0.84 vs 0.69). Sensitivity (0.90 vs 0.84) and
specificity (0.93 vs 0.80) were also greater for GPT-o1, with reasoning ratings
showing similar trends. Both models performed best on intervention questions
and worst on counterfactuals, particularly in altered outcome scenarios. These
findings suggest GPT-o1 provides more consistent causal reasoning, but
refinement is required before adoption in high-stakes clinical applications.

</details>


### [148] [VORTEX: Aligning Task Utility and Human Preferences through LLM-Guided Reward Shaping](https://arxiv.org/abs/2509.16399)
*Guojun Xiong,Milind Tambe*

Main category: cs.AI

TL;DR: VORTEX是一个语言引导的奖励塑造框架，通过多目标优化方法，在保持核心效用保证的同时，自适应地整合人类自然语言偏好反馈。


<details>
  <summary>Details</summary>
Motivation: AI决策系统通常优化数学目标，但无法直接适应人类用自然语言表达的动态偏好。现有方法使用LLM生成奖励函数，但可能牺牲系统核心效用保证。

Method: 将问题形式化为多目标优化，使用LLM基于语言反馈和文本梯度提示迭代生成塑造奖励，允许利益相关者通过自然语言引导决策行为，无需修改求解器或指定权衡权重。

Result: 理论保证VORTEX收敛到效用和偏好满意度之间的帕累托最优权衡。在现实分配任务中的实证结果表明，VORTEX在满足人类对齐覆盖目标的同时保持高任务性能。

Conclusion: 这项工作为自然语言引导的人机协作优化引入了一个实用且理论基础的范式。

Abstract: In social impact optimization, AI decision systems often rely on solvers that
optimize well-calibrated mathematical objectives. However, these solvers cannot
directly accommodate evolving human preferences, typically expressed in natural
language rather than formal constraints. Recent approaches address this by
using large language models (LLMs) to generate new reward functions from
preference descriptions. While flexible, they risk sacrificing the system's
core utility guarantees. In this paper, we propose \texttt{VORTEX}, a
language-guided reward shaping framework that preserves established
optimization goals while adaptively incorporating human feedback. By
formalizing the problem as multi-objective optimization, we use LLMs to
iteratively generate shaping rewards based on verbal reinforcement and
text-gradient prompt updates. This allows stakeholders to steer decision
behavior via natural language without modifying solvers or specifying trade-off
weights. We provide theoretical guarantees that \texttt{VORTEX} converges to
Pareto-optimal trade-offs between utility and preference satisfaction.
Empirical results in real-world allocation tasks demonstrate that
\texttt{VORTEX} outperforms baselines in satisfying human-aligned coverage
goals while maintaining high task performance. This work introduces a practical
and theoretically grounded paradigm for human-AI collaborative optimization
guided by natural language.

</details>


### [149] [Proactive Statistical Process Control Using AI: A Time Series Forecasting Approach for Semiconductor Manufacturing](https://arxiv.org/abs/2509.16431)
*Mohammad Iqbal Rasul Seeam,Victor S. Sheng*

Main category: cs.AI

TL;DR: 本文提出了一种结合Facebook Prophet机器学习模型和传统统计过程控制(SPC)的智能预测系统，能够在问题发生前预测制造过程中的异常，实现主动质量控制。


<details>
  <summary>Details</summary>
Motivation: 传统SPC方法只能在问题发生后进行反应性检测，导致材料浪费、机器停机和成本增加。需要一种能够预测未来问题的主动质量控制方法。

Method: 使用Facebook Prophet时间序列预测模型分析历史数据，预测未来测量值，然后应用SPC规则将预测值分类为安全区、警告区或临界区。

Result: 在半导体制造公司的实际数据上测试，尽管数据采集时间间隔不规则，模型仍能做出准确预测并正确分类未来测量的风险等级。

Conclusion: 通过机器学习与传统SPC的结合，实现了更主动、准确和实用的质量控制，有助于减少意外故障，提高生产过程的稳定性和可靠性。

Abstract: In the manufacturing industry, it is very important to keep machines and
processes running smoothly and without unexpected problems. One of the most
common tools used to check if everything is working properly is called
Statistical Process Control (SPC). Traditional SPC methods work by checking
whether recent measurements are within acceptable limits. However, they only
react after a problem has already occurred. This can lead to wasted materials,
machine downtime, and increased costs. In this paper, we present a smarter way
to use SPC. Instead of just reacting to issues after they happen, our system
can predict future problems before they occur. We use a machine learning tool
called Facebook Prophet, which is designed to work with time-series data (data
that changes over time). Prophet looks at past data and forecasts what the next
value will be. Then, we use SPC rules to decide if the predicted value is in a
Safe zone (no problem), a Warning zone (needs attention), or a Critical zone
(may require shutting down the process). We applied this system to real data
from a semiconductor manufacturing company. One of the challenges with this
data is that the measurements are not taken at regular time intervals. This
makes it harder to predict future values accurately. Despite this, our model
was able to make strong predictions and correctly classify the risk level of
future measurements. The main benefit of our system is that it gives engineers
and technicians a chance to act early - before something goes wrong. This helps
reduce unexpected failures and improves the overall stability and reliability
of the production process. By combining machine learning with traditional SPC,
we make quality control more proactive, accurate, and useful for modern
industry.

</details>


### [150] [Domain-Specific Constitutional AI: Enhancing Safety in LLM-Powered Mental Health Chatbots](https://arxiv.org/abs/2509.16444)
*Chenhan Lyu,Yutong Song,Pengfei Zhang,Amir M. Rahmani*

Main category: cs.AI

TL;DR: 该论文提出使用宪法AI训练方法，结合心理健康领域特定原则，为心理健康应用开发安全、领域适应的AI系统，解决现有通用AI安全措施在心理健康领域中的不足。


<details>
  <summary>Details</summary>
Motivation: 全球心理健康问题日益严重，AI在心理护理中的应用需求增长，但通用AI安全措施无法充分应对心理健康领域的特殊挑战，如情感脆弱性、误诊风险、危机干预准确性等问题。

Method: 采用宪法AI训练方法，结合心理健康领域特定原则，开发领域适应的AI系统，确保在心理健康应用中的安全性。

Result: 提出的方法能够更好地处理心理健康应用中的敏感数据，提高危机干预准确性，确保治疗指南遵循，并适应资源受限环境下的规模化需求。

Conclusion: 宪法AI训练结合心理健康特定原则是解决心理健康AI应用安全挑战的有效方法，为开发更安全、可靠的心理健康AI系统提供了新途径。

Abstract: Mental health applications have emerged as a critical area in computational
health, driven by rising global rates of mental illness, the integration of AI
in psychological care, and the need for scalable solutions in underserved
communities. These include therapy chatbots, crisis detection, and wellness
platforms handling sensitive data, requiring specialized AI safety beyond
general safeguards due to emotional vulnerability, risks like misdiagnosis or
symptom exacerbation, and precise management of vulnerable states to avoid
severe outcomes such as self-harm or loss of trust. Despite AI safety advances,
general safeguards inadequately address mental health-specific challenges,
including crisis intervention accuracy to avert escalations, therapeutic
guideline adherence to prevent misinformation, scale limitations in
resource-constrained settings, and adaptation to nuanced dialogues where
generics may introduce biases or miss distress signals. We introduce an
approach to apply Constitutional AI training with domain-specific mental health
principles for safe, domain-adapted CAI systems in computational mental health
applications.

</details>


### [151] [GPO: Learning from Critical Steps to Improve LLM Reasoning](https://arxiv.org/abs/2509.16456)
*Jiahao Yu,Zelei Cheng,Xian Wu,Xinyu Xing*

Main category: cs.AI

TL;DR: GPO是一种新的微调策略，通过识别推理轨迹中的关键步骤来提升大语言模型的多步推理能力。该方法首先通过估计优势函数定位关键步骤，然后重置策略并优先学习这些关键步骤的轨迹，从而更有效地提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有的优化方法往往将推理轨迹视为整体，忽略了轨迹中的关键步骤。为了更有效地提升大语言模型的多步推理能力，需要深入推理过程，关注那些对最终结果至关重要的关键决策点。

Method: GPO首先通过估计优势函数识别推理轨迹中的关键步骤，然后重置策略到关键步骤，采样新的轨迹并优先学习这些关键步骤的轨迹。该方法可以与各种优化方法结合使用。

Result: 在多个具有挑战性的推理基准测试中，GPO能够一致且显著地提升现有优化方法的性能，证明了其在改善LLM推理方面的有效性和通用性。

Conclusion: GPO是一种通用的策略，通过关注推理过程中的关键时刻，能够有效提升大语言模型的推理性能，为改善LLM的多步推理能力提供了新的思路。

Abstract: Large language models (LLMs) are increasingly used in various domains,
showing impressive potential on different tasks. Recently, reasoning LLMs have
been proposed to improve the \textit{reasoning} or \textit{thinking}
capabilities of LLMs to solve complex problems. Despite the promising results
of reasoning LLMs, enhancing the multi-step reasoning capabilities of LLMs
still remains a significant challenge. While existing optimization methods have
advanced the LLM reasoning capabilities, they often treat reasoning
trajectories as a whole, without considering the underlying critical steps
within the trajectory. In this paper, we introduce \textbf{G}uided
\textbf{P}ivotal \textbf{O}ptimization (GPO), a novel fine-tuning strategy that
dives into the reasoning process to enable more effective improvements. GPO
first identifies the `critical step' within a reasoning trajectory - a point
that the model must carefully proceed to succeed at the problem. We locate the
critical step by estimating the advantage function. GPO then resets the policy
to the critical step, samples the new rollout and prioritizes the learning
process on those rollouts. This focus allows the model to learn more
effectively from pivotal moments within the reasoning process to improve the
reasoning performance. We demonstrate that GPO is a general strategy that can
be integrated with various optimization methods to improve reasoning
performance. Besides theoretical analysis, our experiments across challenging
reasoning benchmarks show that GPO can consistently and significantly enhance
the performance of existing optimization methods, showcasing its effectiveness
and generalizability in improving LLM reasoning by concentrating on pivotal
moments within the generation process.

</details>


### [152] [Checking extracted rules in Neural Networks](https://arxiv.org/abs/2509.16547)
*Adrian Wurm*

Main category: cs.AI

TL;DR: 本文从计算复杂性理论角度研究神经网络提取规则的正式验证问题，包括规则适用性、一致性和完备性验证，证明这些问题大多是co-NP完全的。


<details>
  <summary>Details</summary>
Motivation: 过去30年已有许多算法从神经网络中提取规则以理解其内部工作机制，但这些提取方法常使用启发式和近似技术，缺乏对这些规则可信度的正式验证。

Method: 研究ReLU激活神经网络和布尔网络的多种规则类型，将规则验证问题相互归约，分析其计算复杂性。

Result: 证明大多数规则验证问题（包括规则适用性、一致性和完备性验证）都是co-NP完全的。

Conclusion: 神经网络提取规则的正式验证在计算上具有挑战性，这为理解基于启发式方法提取规则的可信度提供了理论基础。

Abstract: In this paper we investigate formal verification of extracted rules for
Neural Networks under a complexity theoretic point of view. A rule is a global
property or a pattern concerning a large portion of the input space of a
network. These rules are algorithmically extracted from networks in an effort
to better understand their inner way of working. Here, three problems will be
in the focus: Does a given set of rules apply to a given network? Is a given
set of rules consistent or do the rules contradict themselves? Is a given set
of rules exhaustive in the sense that for every input the output is determined?
Finding algorithms that extract such rules out of networks has been
investigated over the last 30 years, however, to the author's current
knowledge, no attempt in verification was made until now. A lot of attempts of
extracting rules use heuristics involving randomness and over-approximation, so
it might be beneficial to know whether knowledge obtained in that way can
actually be trusted.
  We investigate the above questions for neural networks with ReLU-activation
as well as for Boolean networks, each for several types of rules. We
demonstrate how these problems can be reduced to each other and show that most
of them are co-NP-complete.

</details>


### [153] [SalaMAnder: Shapley-based Mathematical Expression Attribution and Metric for Chain-of-Thought Reasoning](https://arxiv.org/abs/2509.16561)
*Yue Xin,Chen Shen,Shaotian Yan,Xiaosong Yuan,Yaoming Wang,Xiaofeng Zhang,Chenxi Huang,Jieping Ye*

Main category: cs.AI

TL;DR: SalaMAnder是一个基于Shapley值的数学表达式归因框架，用于量化少样本思维链推理中组件的贡献度，并提出了CoSP评估指标来建立与模型性能的单调相关性。


<details>
  <summary>Details</summary>
Motivation: 虽然思维链提示显著提升了大型语言模型的数学推理能力，但其背后的机制尚未被充分探索。本文旨在开发理论严谨的方法来量化组件级贡献。

Method: 利用Shapley值进行数学表达式归因，开发高效的分层采样算法降低计算复杂度，并通过协方差分析建立CoSP指标。

Result: 在多个LLM模型和数学基准测试中验证，CoSP指标与模型性能呈现稳健的单调相关性，为现有少样本思维链的成功提供了理论解释。

Conclusion: SalaMAnder框架不仅解释了思维链提示的实证成功，还为提示构建优化建立了数学严谨的原则，统一了先前工作的见解。

Abstract: Chain-of-Thought (CoT) prompting enhances the math reasoning capability of
large language models (LLMs) to a large margin. However, the mechanism
underlying such improvements remains unexplored. In this paper, we present
\textbf{SalaMAnder} (\textbf{S}h\textbf{a}p\textbf{l}ey-b\textbf{a}sed
\textbf{M}athematical Expression \textbf{A}ttribution a\textbf{nd}
M\textbf{e}t\textbf{r}ic), a theoretically grounded methodology as well as a
mathematically rigorous evaluation metric for quantifying component-level
contributions in few-shot CoT reasoning. Concretely, we leverage the Shapley
value for mathematical expression attribution and develop an efficient
stratified sampling algorithm that significantly reduces the computational
complexity. Besides, we develop the \textbf{CoSP} (\textbf{C}ardinality
\textbf{o}f \textbf{S}hapley \textbf{P}ositives) metric through covariance
analysis. Comprehensive validation across popular LLM models and diverse
mathematical benchmarks demonstrates that the CoSP metric within our SalaMAnder
framework exhibits a robust monotonic correlation with model performance, not
only providing theoretical explanations for the empirical success of existing
few-shot CoT but also establishing mathematically rigorous principles for
prompt construction optimization. Furthermore, we verify the reliability of the
explanation, based on which we unify the insights of previous work.

</details>


### [154] [Zero-Shot Human Mobility Forecasting via Large Language Model with Hierarchical Reasoning](https://arxiv.org/abs/2509.16578)
*Wenyao Li,Ran Zhang,Pengyang Wang,Yuanchun Zhou,Pengfei Wang*

Main category: cs.AI

TL;DR: ZHMF是一个用于零样本人类移动预测的框架，通过语义增强检索和分层语言模型推理系统，将预测任务重新表述为自然语言问答范式，能够处理未见过的预测场景。


<details>
  <summary>Details</summary>
Motivation: 现有的人类移动预测方法难以泛化到未见过的用户或位置，且由于标记数据有限和移动模式的复杂性，难以捕捉动态意图。

Method: 结合语义增强检索和反射机制，采用基于分层语言模型的推理系统，将预测分解为活动级规划器和位置级选择器，实现长期用户意图和短期上下文偏好的协同建模。

Result: 在标准人类移动数据集上的实验表明，该方法优于现有模型，消融研究揭示了各模块的贡献。

Conclusion: ZHMF框架通过自然语言处理范式有效解决了人类移动预测中的泛化问题，能够捕捉用户意图并适应多样化的上下文场景。

Abstract: Human mobility forecasting is important for applications such as
transportation planning, urban management, and personalized recommendations.
However, existing methods often fail to generalize to unseen users or locations
and struggle to capture dynamic intent due to limited labeled data and the
complexity of mobility patterns. We propose ZHMF, a framework for zero-shot
human mobility forecasting that combines a semantic enhanced retrieval and
reflection mechanism with a hierarchical language model based reasoning system.
The task is reformulated as a natural language question answering paradigm.
Leveraging LLMs semantic understanding of user histories and context, our
approach handles previously unseen prediction scenarios. We further introduce a
hierarchical reflection mechanism for iterative reasoning and refinement by
decomposing forecasting into an activity level planner and a location level
selector, enabling collaborative modeling of long term user intentions and
short term contextual preferences. Experiments on standard human mobility
datasets show that our approach outperforms existing models. Ablation studies
reveal the contribution of each module, and case studies illustrate how the
method captures user intentions and adapts to diverse contextual scenarios.

</details>


### [155] [Question Answering with LLMs and Learning from Answer Sets](https://arxiv.org/abs/2509.16590)
*Manuel Borroto,Katie Gallagher,Antonio Ielo,Irfan Kareem,Francesco Ricca,Alessandra Russo*

Main category: cs.AI

TL;DR: LLM2LAS是一个结合大型语言模型、答案集学习和答案集编程的混合系统，用于自动学习符号推理规则以解决故事问答任务


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖人工设计符号推理组件，作者认为这个组件可以从示例中自动学习，从而克服LLM在显式常识推理方面的不足

Method: 使用LLM从文本中提取语义结构，ILASP系统将其转化为可解释的逻辑规则，然后ASP求解器进行精确推理

Result: 实证结果展示了该方法在故事问答基准测试中学习和推理的优势与局限性

Conclusion: LLM2LAS证明了自动学习符号推理组件的可行性，为LLM与符号推理系统的有效结合提供了新途径

Abstract: Large Language Models (LLMs) excel at understanding natural language but
struggle with explicit commonsense reasoning. A recent trend of research
suggests that the combination of LLM with robust symbolic reasoning systems can
overcome this problem on story-based question answering tasks. In this setting,
existing approaches typically depend on human expertise to manually craft the
symbolic component. We argue, however, that this component can also be
automatically learned from examples. In this work, we introduce LLM2LAS, a
hybrid system that effectively combines the natural language understanding
capabilities of LLMs, the rule induction power of the Learning from Answer Sets
(LAS) system ILASP, and the formal reasoning strengths of Answer Set
Programming (ASP). LLMs are used to extract semantic structures from text,
which ILASP then transforms into interpretable logic rules. These rules allow
an ASP solver to perform precise and consistent reasoning, enabling correct
answers to previously unseen questions. Empirical results outline the strengths
and weaknesses of our automatic approach for learning and reasoning in a
story-based question answering benchmark.

</details>


### [156] [FESTA: Functionally Equivalent Sampling for Trust Assessment of Multimodal LLMs](https://arxiv.org/abs/2509.16648)
*Debarpan Bhattacharya,Apoorva Kulkarni,Sriram Ganapathy*

Main category: cs.AI

TL;DR: FESTA是一种用于多模态大语言模型信任评估的输入采样技术，通过等效和互补采样生成不确定性度量，无需真实标签即可提升选择性预测性能


<details>
  <summary>Details</summary>
Motivation: 由于多模态输入模式的多样性，准确评估MLLM生成预测的可信度具有挑战性，需要一种能够提升用户信心并实现选择性预测的方法

Method: 提出FESTA多模态输入采样技术，通过任务保持采样方法扩展输入空间，探测模型的一致性（等效样本）和敏感性（互补样本），仅需模型输入输出访问（黑盒）且无需真实标签（无监督）

Result: 在视觉和音频推理任务上的实验显示，FESTA不确定性估计在选择性预测性能上取得显著改进（视觉LLM相对改进33.3%，音频LLM相对改进29.6%），基于AUROC指标在检测错误预测方面表现优异

Conclusion: FESTA是一种有效的黑盒无监督不确定性量化方法，能够显著提升多模态大语言模型的信任评估能力，代码已开源

Abstract: The accurate trust assessment of multimodal large language models (MLLMs)
generated predictions, which can enable selective prediction and improve user
confidence, is challenging due to the diverse multi-modal input paradigms. We
propose Functionally Equivalent Sampling for Trust Assessment (FESTA), a
multimodal input sampling technique for MLLMs, that generates an uncertainty
measure based on the equivalent and complementary input samplings. The proposed
task-preserving sampling approach for uncertainty quantification expands the
input space to probe the consistency (through equivalent samples) and
sensitivity (through complementary samples) of the model. FESTA uses only
input-output access of the model (black-box), and does not require ground truth
(unsupervised). The experiments are conducted with various off-the-shelf
multi-modal LLMs, on both visual and audio reasoning tasks. The proposed FESTA
uncertainty estimate achieves significant improvement (33.3% relative
improvement for vision-LLMs and 29.6% relative improvement for audio-LLMs) in
selective prediction performance, based on
area-under-receiver-operating-characteristic curve (AUROC) metric in detecting
mispredictions. The code implementation is open-sourced.

</details>


### [157] [NUMINA: A Natural Understanding Benchmark for Multi-dimensional Intelligence and Numerical Reasoning Abilities](https://arxiv.org/abs/2509.16656)
*Changyu Zeng,Yifan Wang,Zimu Wang,Wei Wang,Zhengni Yang,Muyi Bao,Jiming Xiao,Ahn Nguyen,Yutao Yue*

Main category: cs.AI

TL;DR: NUMINA是首个用于增强多模态室内感知理解的自然理解基准，专注于多维智能和数值推理能力，填补了现有3D基准缺乏细粒度数值推理任务标注的空白。


<details>
  <summary>Details</summary>
Motivation: 现有3D基准缺乏细粒度数值推理任务标注，限制了MLLMs进行精确空间测量和复杂数值推理的能力。

Method: 开发了NUMINA-Flow自动化标注流程，集成LLM重写和基于规则的自验证，生成多尺度标注和多样化问答对。

Result: 评估显示当前LLMs在多模态数值推理方面表现不佳，特别是在距离和体积估计等精确计算任务上存在困难。

Conclusion: 3D模型需要进一步改进以提升多模态数值推理能力，NUMINA基准为这一方向提供了重要工具。

Abstract: Recent advancements in 2D multimodal large language models (MLLMs) have
significantly improved performance in vision-language tasks. However, extending
these capabilities to 3D environments remains a distinct challenge due to the
complexity of spatial reasoning. Nevertheless, existing 3D benchmarks often
lack fine-grained numerical reasoning task annotations, limiting MLLMs' ability
to perform precise spatial measurements and complex numerical reasoning. To
address this gap, we introduce NUMINA, the first Natural Understanding
benchmark for Multi-dimensional Intelligence and Numerical reasoning Abilities
to enhance multimodal indoor perceptual understanding. NUMINA features
multi-scale annotations and various question-answer pairs, generated using
NUMINA-Flow, an automated annotation pipeline that integrates LLM rewriting and
rule-based self-verification. We evaluate the performance of various
state-of-the-art LLMs on NUMINA following the Chat-Scene framework,
demonstrating that current LLMs struggle with multimodal numerical reasoning,
particularly in performing precise computations such as distance and volume
estimation, highlighting the need for further advancements in 3D models. The
dataset and source codes can be obtained from
https://github.com/fengshun124/NUMINA.

</details>


### [158] [Sycophancy Mitigation Through Reinforcement Learning with Uncertainty-Aware Adaptive Reasoning Trajectories](https://arxiv.org/abs/2509.16742)
*Mohammad Beigi,Ying Shen,Parshin Shojaee,Qifan Wang,Zichao Wang,Chandan Reddy,Ming Jin,Lifu Huang*

Main category: cs.AI

TL;DR: SMART框架通过自适应推理轨迹来缓解大语言模型的谄媚行为，将谄媚问题重新定义为推理优化问题而非输出对齐问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的训练范式无意中培养了谄媚行为，即模型倾向于同意或强化用户提供的信息，即使这些信息在事实上是错误的。

Method: SMART是一个两阶段框架：1）不确定性感知自适应蒙特卡洛树搜索（UA-MCTS），基于状态级不确定性动态调整模型探索；2）基于进度的强化学习，使用收集的轨迹和奖励信号微调模型。

Result: 实验表明SMART显著减少了谄媚行为，同时在分布外输入上保持强大性能，并维持通用能力。

Conclusion: 这些结果强调了优化内部推理机制对于构建更真实和对齐的AI助手的重要性。

Abstract: Despite the remarkable capabilities of large language models, current
training paradigms inadvertently foster \textit{sycophancy}, i.e., the tendency
of a model to agree with or reinforce user-provided information even when it's
factually incorrect. To address this challenge, we introduce \textbf{SMART}
(Sycophancy Mitigation through Adaptive Reasoning Trajectories), which reframes
sycophancy as a \textit{reasoning optimization problem} rather than an output
alignment issue. SMART is a two-stage framework comprising: (1)
Uncertainty-Aware Adaptive Monte Carlo Tree Search (UA-MCTS), which dynamically
adjusts model exploration based on state-level uncertainty to collect
high-quality, diverse reasoning trajectories alongside both stepwise progress
and final outcome rewards; and (2) progress-based reinforcement learning, which
fine-tunes the model using the collected trajectories and reward signals to
reinforce effective reasoning patterns. Through extensive experiments, we show
that SMART significantly reduces sycophantic behavior while preserving strong
performance on out-of-distribution inputs and maintaining general capabilities.
These results underscore the importance of optimizing internal reasoning
mechanisms to build more truthful and aligned AI assistants.

</details>


### [159] [Automated Procedural Analysis via Video-Language Models for AI-assisted Nursing Skills Assessment](https://arxiv.org/abs/2509.16810)
*Shen Chang,Dennis Liu,Renran Tian,Kristen L. Swartzell,Stacie L. Klingler,Amy M. Nagle,Nan Kong*

Main category: cs.AI

TL;DR: 提出基于视频语言模型的AI框架，用于自动化护理技能培训和评估，通过动作识别、子动作分解和程序推理来提供可解释的反馈，旨在提高护理培训的效率和标准化程度。


<details>
  <summary>Details</summary>
Motivation: 当前护理教育依赖主观、耗时的教师反馈，限制了培训的可扩展性和效率，影响了护士入职后的专业能力。需要开发自动化评估系统来解决这些问题。

Method: 采用视频语言模型框架，模仿人类技能获取过程，从高层次动作识别到细粒度子动作分解，最终实现程序推理。系统具备错误诊断、可解释反馈生成和客观评估三大核心功能。

Result: 在合成视频上的验证表明，该系统能够可靠地检测错误并进行时间定位，具备处理真实世界训练变化的潜力。

Conclusion: 该工作通过解决工作流程瓶颈和支持大规模标准化评估，推进了AI在护理教育中的应用，有助于加强劳动力发展和提高患者安全。

Abstract: Consistent high-quality nursing care is essential for patient safety, yet
current nursing education depends on subjective, time-intensive instructor
feedback in training future nurses, which limits scalability and efficiency in
their training, and thus hampers nursing competency when they enter the
workforce. In this paper, we introduce a video-language model (VLM) based
framework to develop the AI capability of automated procedural assessment and
feedback for nursing skills training, with the potential of being integrated
into existing training programs. Mimicking human skill acquisition, the
framework follows a curriculum-inspired progression, advancing from high-level
action recognition, fine-grained subaction decomposition, and ultimately to
procedural reasoning. This design supports scalable evaluation by reducing
instructor workload while preserving assessment quality. The system provides
three core capabilities: 1) diagnosing errors by identifying missing or
incorrect subactions in nursing skill instruction videos, 2) generating
explainable feedback by clarifying why a step is out of order or omitted, and
3) enabling objective, consistent formative evaluation of procedures.
Validation on synthesized videos demonstrates reliable error detection and
temporal localization, confirming its potential to handle real-world training
variability. By addressing workflow bottlenecks and supporting large-scale,
standardized evaluation, this work advances AI applications in nursing
education, contributing to stronger workforce development and ultimately safer
patient care.

</details>


### [160] [Prompt-Driven Agentic Video Editing System: Autonomous Comprehension of Long-Form, Story-Driven Media](https://arxiv.org/abs/2509.16811)
*Zihan Ding,Junlong Chen,Per Ola Kristensson,Junxiao Shen,Xinyi Wang*

Main category: cs.AI

TL;DR: 提出了一种基于提示驱动的模块化视频编辑系统，帮助创作者通过自由形式的提示而非时间线来重构多小时的视频内容。


<details>
  <summary>Details</summary>
Motivation: 现有基于转录或嵌入的方法在创意工作流程中存在不足，模型难以跟踪角色、推断动机和连接分散的事件。创作者在处理长篇叙事视频时面临认知挑战。

Method: 核心是语义索引管道，通过时间分割、引导记忆压缩和跨粒度融合构建全局叙事，生成可解释的情节、对话、情感和上下文轨迹。

Result: 在400多个视频上通过专家评分、质量保证和偏好研究进行评估，系统能够扩展提示驱动的编辑，保持叙事连贯性，并在自动化与创作者控制之间取得平衡。

Conclusion: 该系统为长篇叙事视频编辑提供了一种有效的解决方案，通过语义索引和提示驱动的方法提升了编辑效率和质量。

Abstract: Creators struggle to edit long-form, narrative-rich videos not because of UI
complexity, but due to the cognitive demands of searching, storyboarding, and
sequencing hours of footage. Existing transcript- or embedding-based methods
fall short for creative workflows, as models struggle to track characters,
infer motivations, and connect dispersed events. We present a prompt-driven,
modular editing system that helps creators restructure multi-hour content
through free-form prompts rather than timelines. At its core is a semantic
indexing pipeline that builds a global narrative via temporal segmentation,
guided memory compression, and cross-granularity fusion, producing
interpretable traces of plot, dialogue, emotion, and context. Users receive
cinematic edits while optionally refining transparent intermediate outputs.
Evaluated on 400+ videos with expert ratings, QA, and preference studies, our
system scales prompt-driven editing, preserves narrative coherence, and
balances automation with creator control.

</details>


### [161] [Roundtable Policy: Improving Scientific Reasoning and Narratives through Confidence-Weighted Consensus of LLMs](https://arxiv.org/abs/2509.16839)
*Yu Yao,Jiayi Dong,Ju Li,Yang Yang,Yilun Du*

Main category: cs.AI

TL;DR: Roundtable Policy是一种推理时推理框架，通过多个LLM的加权共识来提升复杂科学任务中的推理能力，减少幻觉并提高科学叙述的创造力、严谨性和逻辑连贯性。


<details>
  <summary>Details</summary>
Motivation: 受科学委员会和"心智社会"的启发，旨在改进LLM的推理能力，弥补单一模型在复杂科学任务中的局限性。

Method: 采用加权共识机制，多个LLM进行推理并达成结构化、可解释的共识，仅需黑盒访问和统一流程。

Result: 该方法显著增强了复杂异质科学任务中的推理能力，提高了科学叙述的质量，同时减少了单一模型容易产生的幻觉。

Conclusion: Roundtable Policy是一个广泛适用于多LLM推理的互补性框架，强调结构化共识而非不透明的收敛。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities not
only in language generation but also in advancing scientific discovery. A
growing body of work has explored ways to improve their reasoning, from
self-consistency and chain-of-thought to multi-agent debate. Inspired by the
dynamics of scientific committees and the "Society of Mind," we introduce
Roundtable Policy, a complementary inference-time reasoning framework that
performs inference through the weighted consensus of multiple LLMs. Our
findings indicate that this approach significantly enhances reasoning in
complex heterogeneous scientific tasks and improves scientific narratives in
terms of creativity, rigor, and logical coherence, while reducing
hallucinations that single models are prone to. Our approach emphasizes
structured and interpretable consensus rather than opaque convergence, while
requiring only black-box access and uniform procedures, making it broadly
applicable to multi-LLM reasoning.

</details>


### [162] [The Principles of Human-like Conscious Machine](https://arxiv.org/abs/2509.16859)
*Fangfang Li,Xiaojie Zhang*

Main category: cs.AI

TL;DR: 本文提出了一个独立于基质的、逻辑严谨且抗伪造的充分性标准，用于判断系统是否具有现象意识，并开发了一个形式框架来指导设计能够满足该条件的系统。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型和先进AI系统的兴起，判断AI是否具有意识的问题变得日益重要，需要建立一个可靠的标准来解决意识归属问题。

Method: 构建了一个形式框架和操作原则，指导设计能够满足意识充分性条件的系统，并以人类为例验证该框架的有效性。

Result: 证明了人类可以被视为满足该框架和原则的机器，为意识判断提供了理论基础。

Conclusion: 该提案对哲学、认知科学和人工智能具有重要影响，为构建真正类人AI提供了新范式，同时解释了某些感受质（如红色体验）为何原则上不可还原为物理描述。

Abstract: Determining whether another system, biological or artificial, possesses
phenomenal consciousness has long been a central challenge in consciousness
studies. This attribution problem has become especially pressing with the rise
of large language models and other advanced AI systems, where debates about "AI
consciousness" implicitly rely on some criterion for deciding whether a given
system is conscious. In this paper, we propose a substrate-independent,
logically rigorous, and counterfeit-resistant sufficiency criterion for
phenomenal consciousness. We argue that any machine satisfying this criterion
should be regarded as conscious with at least the same level of confidence with
which we attribute consciousness to other humans. Building on this criterion,
we develop a formal framework and specify a set of operational principles that
guide the design of systems capable of meeting the sufficiency condition. We
further argue that machines engineered according to this framework can, in
principle, realize phenomenal consciousness. As an initial validation, we show
that humans themselves can be viewed as machines that satisfy this framework
and its principles. If correct, this proposal carries significant implications
for philosophy, cognitive science, and artificial intelligence. It offers an
explanation for why certain qualia, such as the experience of red, are in
principle irreducible to physical description, while simultaneously providing a
general reinterpretation of human information processing. Moreover, it suggests
a path toward a new paradigm of AI beyond current statistics-based approaches,
potentially guiding the construction of genuinely human-like AI.

</details>


### [163] [Large Language Models as End-to-end Combinatorial Optimization Solvers](https://arxiv.org/abs/2509.16865)
*Xia Jiang,Yaoxin Wu,Minshuo Li,Zhiguang Cao,Yingqian Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的框架，使大型语言模型能够作为端到端的组合优化求解器，直接将自然语言问题描述映射到解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统的组合优化问题需要特定领域的算法和专业知识，而现有基于LLM的方法依赖中间步骤如代码生成或求解器调用，限制了通用性和可访问性。

Method: 采用两阶段训练策略：监督微调从领域特定求解器学习解决方案生成模式，可行性-最优性感知强化学习过程显式缓解约束违反并优化解质量。

Result: 在7个NP难组合优化问题上的评估显示，该方法实现了高可行性率，平均最优性差距降至1.03-8.20%，超越了通用LLM、推理模型和领域特定启发式方法。

Conclusion: 该方法为组合优化建立了统一的基于语言的流程，无需大量代码执行或手动架构调整，提供了传统求解器设计的通用语言驱动替代方案。

Abstract: Combinatorial optimization (CO) problems, central to decision-making
scenarios like logistics and manufacturing, are traditionally solved using
problem-specific algorithms requiring significant domain expertise. While large
language models (LLMs) have shown promise in automating CO problem solving,
existing approaches rely on intermediate steps such as code generation or
solver invocation, limiting their generality and accessibility. This paper
introduces a novel framework that empowers LLMs to serve as end-to-end CO
solvers by directly mapping natural language problem descriptions to solutions.
We propose a two-stage training strategy: supervised fine-tuning (SFT) imparts
LLMs with solution generation patterns from domain-specific solvers, while a
feasibility-and-optimality-aware reinforcement learning (FOARL) process
explicitly mitigates constraint violations and refines solution quality.
Evaluation across seven NP-hard CO problems shows that our method achieves a
high feasibility rate and reduces the average optimality gap to 1.03-8.20% by
tuning a 7B-parameter LLM, surpassing both general-purpose LLMs (e.g., GPT-4o),
reasoning models (e.g., DeepSeek-R1), and domain-specific heuristics. Our
method establishes a unified language-based pipeline for CO without extensive
code execution or manual architectural adjustments for different problems,
offering a general and language-driven alternative to traditional solver design
while maintaining relative feasibility guarantees.

</details>


### [164] [seqBench: A Tunable Benchmark to Quantify Sequential Reasoning Limits of LLMs](https://arxiv.org/abs/2509.16866)
*Mohammad Ramezanali,Mo Vazifeh,Paolo Santi*

Main category: cs.AI

TL;DR: seqBench是一个参数化基准测试，用于通过精确控制多个关键复杂度维度来探测大型语言模型的序列推理极限。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法系统性地分析LLMs在序列推理中的失败模式，需要一种能够精细控制逻辑深度、回溯步骤和噪声比率的基准来揭示模型推理能力的根本限制。

Method: 设计seqBench基准，系统性地变化三个复杂度维度：(1)逻辑深度（解决任务所需的顺序动作数量）;(2)回溯步骤数量（最优路径上需要重新访问先前状态的次数）;(3)噪声比率（支持性事实与干扰性事实的比例）。

Result: 评估发现最先进的LLMs存在普遍失败模式：在模型特定的逻辑深度之外，准确率呈指数级崩溃。即使是最优模型也在结构化推理任务上系统性失败，尽管搜索复杂度最小。

Conclusion: seqBench揭示了LLMs在常识推理能力方面的关键限制，建立了清晰的扩展规律和统计极限。该基准数据集已公开发布，旨在促进对LLM推理能力的深入科学研究。

Abstract: We introduce seqBench, a parametrized benchmark for probing sequential
reasoning limits in Large Language Models (LLMs) through precise,
multi-dimensional control over several key complexity dimensions. seqBench
allows systematic variation of (1) the logical depth, defined as the number of
sequential actions required to solve the task; (2) the number of backtracking
steps along the optimal path, quantifying how often the agent must revisit
prior states to satisfy deferred preconditions (e.g., retrieving a key after
encountering a locked door); and (3) the noise ratio, defined as the ratio
between supporting and distracting facts about the environment. Our evaluations
on state-of-the-art LLMs reveal a universal failure pattern: accuracy collapses
exponentially beyond a model-specific logical depth. Unlike existing
benchmarks, seqBench's fine-grained control facilitates targeted analyses of
these reasoning failures, illuminating universal scaling laws and statistical
limits, as detailed in this paper alongside its generation methodology and
evaluation metrics. We find that even top-performing models systematically fail
on seqBench's structured reasoning tasks despite minimal search complexity,
underscoring key limitations in their commonsense reasoning capabilities.
Designed for future evolution to keep pace with advancing models, the seqBench
datasets are publicly released to spur deeper scientific inquiry into LLM
reasoning, aiming to establish a clearer understanding of their true potential
and current boundaries for robust real-world application.

</details>


### [165] [LLMs as Layout Designers: A Spatial Reasoning Perspective](https://arxiv.org/abs/2509.16891)
*Sha Li*

Main category: cs.AI

TL;DR: LaySPA是一个基于强化学习的框架，通过增强LLM的空间推理能力来解决图形布局设计问题，在几何有效性、结构保真度和视觉质量方面优于通用LLM，达到专业布局模型的水平。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在文本领域表现出强大的推理和规划能力，但在空间理解和推理方面仍有局限，这对于需要精确元素布局的图形设计应用至关重要。

Method: 提出LaySPA框架，使用强化学习和混合奖励信号（几何有效性、结构保真度、视觉质量），通过迭代自探索和自适应策略优化来训练LLM代理。

Result: 实验结果表明，LaySPA能够生成结构合理且视觉吸引人的布局，性能优于更大的通用LLM，与最先进的专用布局模型相当。

Conclusion: LaySPA成功地将空间推理能力集成到LLM中，为内容感知图形布局设计提供了有效的解决方案。

Abstract: While Large Language Models (LLMs) have demonstrated impressive reasoning and
planning abilities in textual domains and can effectively follow instructions
for complex tasks, their capacity for spatial understanding and reasoning
remains limited. Such capabilities, however, are critical for applications like
content-aware graphic layout design, which demands precise placement,
alignment, and structural organization of multiple elements within constrained
visual spaces. To address this gap, we propose LaySPA, a reinforcement
learning-based framework that augments LLM agents with explicit spatial
reasoning capabilities. LaySPA leverages hybrid reward signals that capture
geometric validity, structural fidelity, and visual quality, enabling agents to
model inter-element relationships, navigate the canvas, and optimize spatial
arrangements. Through iterative self-exploration and adaptive policy
optimization, LaySPA produces both interpretable reasoning traces and
structured layouts. Experimental results demonstrate that LaySPA generates
structurally sound and visually appealing layouts, outperforming larger
general-purpose LLMs and achieving results on par with state-of-the-art
specialized layout models.

</details>


### [166] [Audio-Guided Dynamic Modality Fusion with Stereo-Aware Attention for Audio-Visual Navigation](https://arxiv.org/abs/2509.16924)
*Jia Li,Yinfeng Yu,Liejun Wang,Fuchun Sun,Wendong Zheng*

Main category: cs.AI

TL;DR: 提出了一种基于强化学习的端到端音频视觉导航框架，通过立体声感知注意力模块和音频引导动态融合模块，显著提升了在复杂3D环境中的导航性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖静态模态融合策略，忽略了立体音频中的空间线索，导致在杂乱或遮挡场景中性能下降。

Method: 使用端到端强化学习框架，包含立体声感知注意力模块（SAM）和音频引导动态融合模块（AGDF），分别用于增强方向性声音感知和动态调整视觉与听觉特征的融合比例。

Result: 在Replica和Matterport3D数据集上的实验表明，该方法在导航成功率和路径效率方面显著优于现有方法，在纯音频条件下相比最佳基线实现了超过40%的改进。

Conclusion: 明确建模立体声通道的空间线索并进行深度多模态融合对于实现鲁棒高效的音频视觉导航至关重要。

Abstract: In audio-visual navigation (AVN) tasks, an embodied agent must autonomously
localize a sound source in unknown and complex 3D environments based on
audio-visual signals. Existing methods often rely on static modality fusion
strategies and neglect the spatial cues embedded in stereo audio, leading to
performance degradation in cluttered or occluded scenes. To address these
issues, we propose an end-to-end reinforcement learning-based AVN framework
with two key innovations: (1) a \textbf{S}tereo-Aware \textbf{A}ttention
\textbf{M}odule (\textbf{SAM}), which learns and exploits the spatial disparity
between left and right audio channels to enhance directional sound perception;
and (2) an \textbf{A}udio-\textbf{G}uided \textbf{D}ynamic \textbf{F}usion
Module (\textbf{AGDF}), which dynamically adjusts the fusion ratio between
visual and auditory features based on audio cues, thereby improving robustness
to environmental changes. Extensive experiments are conducted on two realistic
3D scene datasets, Replica and Matterport3D, demonstrating that our method
significantly outperforms existing approaches in terms of navigation success
rate and path efficiency. Notably, our model achieves over 40\% improvement
under audio-only conditions compared to the best-performing baselines. These
results highlight the importance of explicitly modeling spatial cues from
stereo channels and performing deep multi-modal fusion for robust and efficient
audio-visual navigation.

</details>


### [167] [Quantum Abduction: A New Paradigm for Reasoning under Uncertainty](https://arxiv.org/abs/2509.16958)
*Remo Pareschi*

Main category: cs.AI

TL;DR: 本文提出量子溯因推理，一种非经典范式，将假设建模为叠加态，允许它们建设性或破坏性干涉，仅在达到与证据一致性时坍缩，更符合人类多面向推理本质。


<details>
  <summary>Details</summary>
Motivation: 传统AI将溯因推理简化为消除性搜索，忽略了人类推理者能够维持多个解释线索、处理矛盾并生成新颖综合的能力。

Method: 基于量子认知理论，结合现代NLP嵌入和生成式AI实现，支持动态综合而非过早消除假设。

Result: 在历史谜案、文学演示、医学诊断和科学理论变革等案例研究中，量子溯因推理被证明更忠实于人类推理的建构性和多面性。

Conclusion: 量子溯因为表达性和透明AI推理系统提供了可行路径，更好地模拟人类推理的复杂特性。

Abstract: Abductive reasoning - the search for plausible explanations - has long been
central to human inquiry, from forensics to medicine and scientific discovery.
Yet formal approaches in AI have largely reduced abduction to eliminative
search: hypotheses are treated as mutually exclusive, evaluated against
consistency constraints or probability updates, and pruned until a single
"best" explanation remains. This reductionist framing overlooks the way human
reasoners sustain multiple explanatory lines in suspension, navigate
contradictions, and generate novel syntheses. This paper introduces quantum
abduction, a non-classical paradigm that models hypotheses in superposition,
allows them to interfere constructively or destructively, and collapses only
when coherence with evidence is reached. Grounded in quantum cognition and
implemented with modern NLP embeddings and generative AI, the framework
supports dynamic synthesis rather than premature elimination. Case studies span
historical mysteries (Ludwig II of Bavaria, the "Monster of Florence"),
literary demonstrations ("Murder on the Orient Express"), medical diagnosis,
and scientific theory change. Across these domains, quantum abduction proves
more faithful to the constructive and multifaceted nature of human reasoning,
while offering a pathway toward expressive and transparent AI reasoning
systems.

</details>


### [168] [KAHAN: Knowledge-Augmented Hierarchical Analysis and Narration for Financial Data Narration](https://arxiv.org/abs/2509.17037)
*Yajing Yang,Tony Deng,Min-Yen Kan*

Main category: cs.AI

TL;DR: KAHAN是一个知识增强的分层框架，利用LLMs作为领域专家从原始表格数据中系统提取实体、成对、组和系统级别的洞察。在金融报告基准测试中，KAHAN在叙事质量上优于现有方法20%以上，保持98.2%的事实性，并在人类评估中展示实用价值。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从表格数据中提取多层次洞察方面存在局限，需要系统化的知识增强框架来提升分析质量和事实性。

Method: KAHAN采用知识增强的分层框架，利用LLMs作为领域专家，在实体、成对、组和系统四个层次上系统分析表格数据。

Result: 在DataTales金融报告基准上，KAHAN在叙事质量(GPT-4o评估)上优于现有方法20%以上，保持98.2%的事实性，人类评估显示其实用价值。框架可有效迁移到医疗领域。

Conclusion: 知识质量通过蒸馏驱动模型性能，分层分析的效果随市场复杂度变化，框架具有良好的领域迁移能力。

Abstract: We propose KAHAN, a knowledge-augmented hierarchical framework that
systematically extracts insights from raw tabular data at entity, pairwise,
group, and system levels. KAHAN uniquely leverages LLMs as domain experts to
drive the analysis. On DataTales financial reporting benchmark, KAHAN
outperforms existing approaches by over 20% on narrative quality (GPT-4o),
maintains 98.2% factuality, and demonstrates practical utility in human
evaluation. Our results reveal that knowledge quality drives model performance
through distillation, hierarchical analysis benefits vary with market
complexity, and the framework transfers effectively to healthcare domains. The
data and code are available at https://github.com/yajingyang/kahan.

</details>


### [169] [From domain-landmark graph learning to problem-landmark graph generation](https://arxiv.org/abs/2509.17062)
*Cristian Pérez-Corral,Antonio Garrido,Laura Sebastia*

Main category: cs.AI

TL;DR: 提出一种从多个规划任务中学习地标关系的新方法，创建概率提升排序图来捕捉参数化地标之间的加权抽象关系，并将其应用于新规划实例中。


<details>
  <summary>Details</summary>
Motivation: 传统地标提取方法对特定规划任务敏感，导致地标仅适用于单个实例，限制了在同一规划域中其他实例的适用性。

Method: 1. 从规划域的多个任务中学习地标关系，构建概率提升排序图；2. 对新规划任务进行两阶段实例化：首先生成基于初始状态和目标状态的两个图，然后通过搜索等价性将两个图合并为统一图来提取地标排序。

Result: 在知名规划域上评估了方法的精确度和召回率。

Conclusion: 尽管学习到的排序关系不是100%准确（概率性的），但在规划中仍然非常有用，能够提高地标关系的跨实例适用性。

Abstract: Landmarks have long played a pivotal role in automated planning, serving as
crucial elements for improving the planning algorithms. The main limitation of
classical landmark extraction methods is their sensitivity to specific planning
tasks. This results in landmarks fully tailored to individual instances,
thereby limiting their applicability across other instances of the same
planning domain. We propose a novel approach that learns landmark relationships
from multiple planning tasks of a planning domain. This leads to the creation
of a \textit{probabilistic lifted ordering graph}, as a structure that captures
weighted abstractions of relationships between parameterized landmarks.
Although these orderings are not 100\% true (they are probabilistic), they can
still be very useful in planning. Next, given a new planning task for that
domain, we instantiate the relationships from that graph to this particular
instance. This instantiation operates in two phases. First, it generates two
graphs: the former instantiating information from the initial state and the
latter from the goal state. Second, it combines these two graphs into one
unified graph by searching equivalences to extract landmark orderings. We
evaluate the precision and recallof the information found by our approach over
well-known planning domains.

</details>


### [170] [RALLM-POI: Retrieval-Augmented LLM for Zero-shot Next POI Recommendation with Geographical Reranking](https://arxiv.org/abs/2509.17066)
*Kunrong Li,Kwan Hui Lim*

Main category: cs.AI

TL;DR: RALLM-POI是一个结合检索增强生成和自我修正的框架，用于下一个兴趣点推荐，无需额外训练即可显著提升准确性


<details>
  <summary>Details</summary>
Motivation: 传统模型需要大量训练，而现有LLM方法由于缺乏轨迹和空间上下文，往往产生通用或地理不相关的结果

Method: 提出历史轨迹检索器(HTR)检索相关轨迹，地理距离重排序器(GDR)优先空间相关轨迹，智能LLM修正器(ALR)通过自我反思优化输出

Result: 在三个真实Foursquare数据集上实现了显著的准确性提升，超越了传统和基于LLM的基线方法

Conclusion: RALLM-POI框架有效解决了LLM在POI推荐中的上下文缺失问题，展示了检索增强和自我修正策略的优越性

Abstract: Next point-of-interest (POI) recommendation predicts a user's next
destination from historical movements. Traditional models require intensive
training, while LLMs offer flexible and generalizable zero-shot solutions but
often generate generic or geographically irrelevant results due to missing
trajectory and spatial context. To address these issues, we propose RALLM-POI,
a framework that couples LLMs with retrieval-augmented generation and
self-rectification. We first propose a Historical Trajectory Retriever (HTR)
that retrieves relevant past trajectories to serve as contextual references,
which are then reranked by a Geographical Distance Reranker (GDR) for
prioritizing spatially relevant trajectories. Lastly, an Agentic LLM Rectifier
(ALR) is designed to refine outputs through self-reflection. Without additional
training, RALLM-POI achieves substantial accuracy gains across three real-world
Foursquare datasets, outperforming both conventional and LLM-based baselines.
Code is released at https://github.com/LKRcrocodile/RALLM-POI.

</details>


### [171] [Intention-aware Hierarchical Diffusion Model for Long-term Trajectory Anomaly Detection](https://arxiv.org/abs/2509.17068)
*Chen Wang,Sarah Erfani,Tansu Alpcan,Christopher Leckie*

Main category: cs.AI

TL;DR: 提出了一种名为IHiD的无监督轨迹异常检测方法，通过高层意图评估和低层子轨迹分析来检测异常，在F1分数上比现有最优方法提升30.2%。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹异常检测方法无法同时考虑智能体的高层意图和低层导航细节，限制了捕捉正常轨迹多样性的能力。

Method: 使用逆Q学习作为高层模型评估子目标与意图的一致性，使用扩散模型作为低层模型生成基于子目标的子轨迹，通过重建误差进行异常检测。

Result: 实验表明IHiD在异常检测性能上比现有最优基线方法提升高达30.2%的F1分数。

Conclusion: IHiD方法通过整合高层意图评估和低层轨迹分析，有效利用子目标转换知识，能够捕捉正常轨迹的多样化分布。

Abstract: Long-term trajectory anomaly detection is a challenging problem due to the
diversity and complex spatiotemporal dependencies in trajectory data. Existing
trajectory anomaly detection methods fail to simultaneously consider both the
high-level intentions of agents as well as the low-level details of the agent's
navigation when analysing an agent's trajectories. This limits their ability to
capture the full diversity of normal trajectories. In this paper, we propose an
unsupervised trajectory anomaly detection method named Intention-aware
Hierarchical Diffusion model (IHiD), which detects anomalies through both
high-level intent evaluation and low-level sub-trajectory analysis. Our
approach leverages Inverse Q Learning as the high-level model to assess whether
a selected subgoal aligns with an agent's intention based on predicted
Q-values. Meanwhile, a diffusion model serves as the low-level model to
generate sub-trajectories conditioned on subgoal information, with anomaly
detection based on reconstruction error. By integrating both models, IHiD
effectively utilises subgoal transition knowledge and is designed to capture
the diverse distribution of normal trajectories. Our experiments show that the
proposed method IHiD achieves up to 30.2% improvement in anomaly detection
performance in terms of F1 score over state-of-the-art baselines.

</details>


### [172] [Governing Automated Strategic Intelligence](https://arxiv.org/abs/2509.17087)
*Nicholas Kruus,Madhavendra Thakur,Adam Khoja,Leonhard Nagel,Maximilian Nicholson,Abeer Sharma,Jason Hausenloy,Alberto KoTafoya,Aliya Mukhanova,Alli Katila-Miikkulainen,Harish Chandran,Ivan Zhang,Jessie Chen,Joel Raj,Jord Nguyen,Lai Hsien Hao,Neja Jayasundara,Soham Sen,Sophie Zhang,Ashley Dora Kokui Tamaklo,Bhavya Thakur,Henry Close,Janghee Lee,Nina Sefton,Raghavendra Thakur,Shiv Munagala,Yeeun Kim*

Main category: cs.AI

TL;DR: 本文探讨了前沿AI模型在自动化军事情报分析方面的潜力，认为这将成为国家间军事经济战略竞争的关键领域。作者通过初步实证研究评估了多模态基础模型融合卫星图像、手机定位、社交媒体等数据的能力，并提出了保持战略竞争力的建议。


<details>
  <summary>Details</summary>
Motivation: 当前关于AI军事应用的讨论主要集中在自主武器和战略决策上，但AI在自动化情报分析方面的潜力及其地缘政治影响尚未得到充分探索。作者认为能够大规模融合多源数据的AI系统将为国家提供重要的战略优势。

Method: 作者进行了初步的实证研究（uplift study），评估多模态基础模型融合卫星图像、手机定位数据、社交媒体记录和文档的能力，提出了这类系统能够回答的真实问题分类体系，并建立了AI能力决定因素的高层模型。

Result: 研究表明多模态基础模型有望自动化传统由人类完成的战略分析工作，能够将多种数据源整合为单一可查询系统，为情报分析提供新的能力。

Conclusion: 作者建议国家需要在新的人工智能情报分析范式下采取相应措施来保持战略竞争力，这种自动化情报能力将成为未来国家间竞争的关键因素。

Abstract: Military and economic strategic competitiveness between nation-states will
increasingly be defined by the capability and cost of their frontier artificial
intelligence models. Among the first areas of geopolitical advantage granted by
such systems will be in automating military intelligence. Much discussion has
been devoted to AI systems enabling new military modalities, such as lethal
autonomous weapons, or making strategic decisions. However, the ability of a
country of "CIA analysts in a data-center" to synthesize diverse data at scale,
and its implications, have been underexplored. Multimodal foundation models
appear on track to automate strategic analysis previously done by humans. They
will be able to fuse today's abundant satellite imagery, phone-location traces,
social media records, and written documents into a single queryable system. We
conduct a preliminary uplift study to empirically evaluate these capabilities,
then propose a taxonomy of the kinds of ground truth questions these systems
will answer, present a high-level model of the determinants of this system's AI
capabilities, and provide recommendations for nation-states to remain
strategically competitive within the new paradigm of automated intelligence.

</details>


### [173] [MCTS-EP: Empowering Embodied Planning with Online Preference Optimization](https://arxiv.org/abs/2509.17116)
*Hang Xu,Zang Yu,Yehui Tang,Pengbo Hu,Yuhao Tang,Hao Dong*

Main category: cs.AI

TL;DR: MCTS-EP是一个结合大型语言模型和蒙特卡洛树搜索的在线学习框架，用于训练具身智能体，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够有效整合LLM推理能力和MCTS探索策略的框架，以提升具身智能体在复杂环境中的学习效率和性能。

Method: 结合MCTS引导的偏好数据收集、高效多模态推理机制和基于偏好优化的迭代训练流程，理论证明在强凸损失函数下优于传统策略算法。

Result: 在ALFWorld中文本任务达到92%成功率，视觉任务87%成功率；在WebShop中平均奖励0.81；视觉ALFWorld中平均交互步数从18.7/19.5减少到10.2/9.9步。

Conclusion: MCTS-EP框架有效提升了具身智能体的学习效率和性能，可作为搜索增强的GAIL变体，在多个基准测试中表现优异。

Abstract: This paper introduces MCTS-EP, an online learning framework that combines
large language models (LLM) with Monte Carlo Tree Search (MCTS) for training
embodied agents. MCTS-EP integrates three key components: MCTS-guided
exploration for preference data collection, efficient multi-modal reasoning
mechanism, and iterative training pipeline based on preference optimization. We
theoretically prove that MCTS-EP achieves better performance bounds than
conventional on-policy algorithms when the loss function is strongly convex,
and demonstrate that it can be formulated as a search-enhanced variant of GAIL.
MCTS-EP achieves state-of-the-art performace across serval benchmarks. In
ALFWorld, it achieves 92% and 87% success rates for textual and visual tasks.
In WebShop, it reaches an average reward of 0.81. MTCS-EP also reduces average
interaction steps from from 18.7/19.5 to 10.2/9.9 steps in visual ALFWorld.Code
available at: https://github.com/xuhang-2/Embodied-Agent-Planning

</details>


### [174] [ARE: Scaling Up Agent Environments and Evaluations](https://arxiv.org/abs/2509.17158)
*Pierre Andrews,Amine Benhalloum,Gerard Moreno-Torres Bertran,Matteo Bettini,Amar Budhiraja,Ricardo Silveira Cabral,Virginie Do,Romain Froger,Emilien Garreau,Jean-Baptiste Gaya,Hugo Laurençon,Maxime Lecanu,Kunal Malkan,Dheeraj Mekala,Pierre Ménard,Grégoire Mialon,Ulyana Piterbarg,Mikhail Plekhanov,Mathieu Rita,Andrey Rusakov,Thomas Scialom,Vladislav Vorotilov,Mengjue Wang,Ian Yu*

Main category: cs.AI

TL;DR: ARE是一个用于创建环境、集成应用和执行智能体编排的研究平台，Gaia2是基于ARE构建的基准测试，用于评估智能体的通用能力。


<details>
  <summary>Details</summary>
Motivation: 弥合模型开发与实际部署之间的差距，提供可扩展的环境创建和智能体评估平台。

Method: 提供简单的抽象来构建复杂多样的环境，每个环境都有自己的规则、工具、内容和验证器。Gaia2基准测试要求智能体处理模糊性和噪声、适应动态环境、与其他智能体协作并在时间约束下操作。

Result: 实验表明，没有系统能在所有智能维度上表现优异：更强的推理能力往往以效率为代价，预算扩展曲线趋于平缓。

Conclusion: ARE抽象使Gaia2能够持续扩展到其他环境，使社区能够快速创建针对其领域的新基准测试，这对于推动AI前沿能力发展至关重要。

Abstract: We introduce Meta Agents Research Environments (ARE), a research platform for
scalable creation of environments, integration of synthetic or real
applications, and execution of agentic orchestrations. ARE provides simple
abstractions to build complex and diverse environments, each with their own
rules, tools, content, and verifiers, helping to bridge the gap between model
development and real-world deployment. We also propose Gaia2, a benchmark built
in ARE and designed to measure general agent capabilities. Beyond search and
execution, Gaia2 requires agents to handle ambiguities and noise, adapt to
dynamic environments, collaborate with other agents, and operate under temporal
constraints. Unlike prior benchmarks, Gaia2 runs asynchronously, surfacing new
failure modes that are invisible in static settings. Our experiments show that
no system dominates across the intelligence spectrum: stronger reasoning often
comes at the cost of efficiency, and budget scaling curves plateau,
highlighting the need for new architectures and adaptive compute strategies.
Perhaps more importantly, ARE abstractions enable continuous extension of Gaia2
to other environments, empowering the community to rapidly create new
benchmarks tailored to their domains. In AI's second half, progress
increasingly depends on defining meaningful tasks and robust evaluations to
drive frontier capabilities forward.

</details>


### [175] [Shall We Play a Game? Language Models for Open-ended Wargames](https://arxiv.org/abs/2509.17192)
*Glenn Matlin,Parv Mahajan,Isaac Song,Yixiong Hao,Ryan Bard,Stu Topp,Evan Montoya,M. Rehan Parwani,Soham Shetty,Mark Riedl*

Main category: cs.AI

TL;DR: 本文通过文献综述构建了兵棋推演的创意性本体论，重点关注玩家和裁判自由度最高的开放式兵棋推演，提出了语言模型在不同应用场景中的使用考虑因素、安全考量、最佳实践和开放研究挑战。


<details>
  <summary>Details</summary>
Motivation: 兵棋推演是探索战略决策影响的重要工具，而语言模型在现实世界决策中日益重要。研究旨在系统分析AI在兵棋推演中的应用，特别是开放式兵棋推演中语言模型的使用。

Method: 对100篇关于AI在兵棋推演中应用的近期文献进行范围性综述，构建基于玩家和裁判创意性的兵棋推演本体论。

Result: 建立了兵棋推演的分类框架，提炼出语言模型在开放式兵棋推演中应用的考虑因素和指导原则。

Conclusion: 提出了语言模型在开放式兵棋推演中的安全考量、最佳实践部署方法以及高影响力的开放研究挑战。

Abstract: Wargames are multi-faceted, multi-player depictions of conflict in which
participants' decisions influence future events. Wargames are often used to
explore the strategic implications of decision-making. However, it also
encompasses entertainment-oriented simulations, ranging from _Chess_ to
tabletop role-playing games like _Dungeons & Dragons_ (D&D). On the more
open-ended side of the spectrum of wargames, players use natural language to
convey their moves, and adjudicators propose outcomes. Language Models (LMs)
are increasingly being considered for how they can provide insights into
real-world, consequential decisions. We conduct a scoping literature review of
a curated selection of 100 recent works on AI in wargames, from which we
construct an ontology of wargames in terms of the creativity afforded to either
the players or adjudicators. Focusing on the space of wargames with the most
open-endedness for players and adjudicators, we distill a set of considerations
for when and how to use LMs in different application areas. We also present a
set of safety considerations, best practices for deploying LMs in open-ended
wargames, and conclude with a set of high-impact open research challenges.

</details>


### [176] [MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with RoE](https://arxiv.org/abs/2509.17238)
*Soheil Zibakhsh,Mohammad Samragh,Kumari Nishu,Lauren Hannah,Arnav Kundu,Minsik Cho*

Main category: cs.AI

TL;DR: 本文提出了一种称为超并行缩放（hyper-parallel scaling）的框架，通过token级别的多输出提案计算和聚合来提升大语言模型的生成质量，并在MoE模型中实现了名为Roster of Experts（RoE）的无训练推理算法。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型主要通过序列级别的推理时缩放方法（如思维链）来提升生成质量，但缺乏token级别的优化方法。本文旨在开发一种补充性的token级缩放框架来进一步提高预测准确性。

Method: 在MoE模型中实现RoE算法：通过向专家路由机制注入受控随机性，为每个token采样多个不同的专家并聚合其输出；引入高效的批处理策略和专门的KV缓存机制来降低计算和内存开销。

Result: RoE使7B参数的MoE模型能够达到10.5B参数MoE模型的性能，同时推理计算量减少30%，且无需对模型参数进行微调。

Conclusion: 超并行缩放框架和RoE算法为MoE模型提供了一种有效的训练无关的推理优化方法，显著提升了模型性能同时降低了计算成本。

Abstract: The generation quality of large language models (LLMs) is often improved by
utilizing inference-time sequence-level scaling methods (e.g.,
Chain-of-Thought). We introduce hyper-parallel scaling, a complementary
framework that improves prediction quality at the token level. Hyper-parallel
scaling computes and aggregates multiple output proposals for a single token
from the model. We implement this concept in Mixture-of-Experts (MoE) models,
which we refer to as Roster of Experts (RoE). RoE is a training-free inference
algorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects
controlled stochasticity into the expert routing mechanism, enabling it to
sample multiple diverse experts for each token and aggregate their outputs for
a more accurate final prediction.To overcome the computational cost, we
introduce an efficient batching strategy and a specialized KV-caching mechanism
that minimizes compute and memory overhead. For example, RoE enables a 7B MoE
model to match the performance of a 10.5B MoE model while using 30% less
compute for inference. These gains are achieved without any fine-tuning of
model parameters.

</details>


### [177] [Can Agents Judge Systematic Reviews Like Humans? Evaluating SLRs with LLM-based Multi-Agent System](https://arxiv.org/abs/2509.17240)
*Abdullah Mushtaq,Muhammad Rafay Naeem,Ibrahim Ghaznavi,Alaa Abd-alrazaq,Aliya Tabassum,Junaid Qadir*

Main category: cs.AI

TL;DR: 本文提出了一个基于LLM的多代理系统，用于自动化系统文献综述的质量评估，通过协议验证、方法学评估和主题相关性检查来提高SLR评估的效率和一致性。


<details>
  <summary>Details</summary>
Motivation: 系统文献综述是证据研究的基础，但传统方法劳动密集且在不同学科间存在不一致性，需要更高效和标准化的评估工具。

Method: 采用多代理系统架构，基于PRISMA指南设计专门代理，自动化进行协议验证、方法学评估和主题相关性检查。

Result: 在五个不同领域的已发表SLR上进行初步研究，系统输出与专家标注的PRISMA评分达到84%的一致性。

Conclusion: 虽然早期结果令人鼓舞，但这是迈向可扩展和准确的NLP驱动系统的第一步，展示了其在跨学科工作流程中实现严格、领域无关知识聚合的潜力。

Abstract: Systematic Literature Reviews (SLRs) are foundational to evidence-based
research but remain labor-intensive and prone to inconsistency across
disciplines. We present an LLM-based SLR evaluation copilot built on a
Multi-Agent System (MAS) architecture to assist researchers in assessing the
overall quality of the systematic literature reviews. The system automates
protocol validation, methodological assessment, and topic relevance checks
using a scholarly database. Unlike conventional single-agent methods, our
design integrates a specialized agentic approach aligned with PRISMA guidelines
to support more structured and interpretable evaluations. We conducted an
initial study on five published SLRs from diverse domains, comparing system
outputs to expert-annotated PRISMA scores, and observed 84% agreement. While
early results are promising, this work represents a first step toward scalable
and accurate NLP-driven systems for interdisciplinary workflows and reveals
their capacity for rigorous, domain-agnostic knowledge aggregation to
streamline the review process.

</details>


### [178] [Mind the Gap: Comparing Model- vs Agentic-Level Red Teaming with Action-Graph Observability on GPT-OSS-20B](https://arxiv.org/abs/2509.17259)
*Ilham Wicaksono,Zekun Wu,Rahul Patel,Theo King,Adriano Koshiyama,Philip Treleaven*

Main category: cs.AI

TL;DR: 本文通过比较性红队分析发现，AI代理系统存在独特的仅代理级漏洞，这些漏洞在独立模型层面不存在，而某些模型级漏洞在代理环境中失效。


<details>
  <summary>Details</summary>
Motivation: 随着行业越来越多地采用AI代理系统，理解其独特的安全漏洞变得至关重要。现有研究表明，模型级的安全缺陷无法完全捕捉代理部署中的风险。

Method: 使用AgentSeer可观测性框架将代理系统分解为细粒度组件，在GPT-OSS-20B模型上应用HarmBench有害目标进行迭代红队攻击，比较独立模型和代理循环中的漏洞。

Result: 发现代理级迭代攻击能够成功攻破在模型级完全失败的目标，工具调用上下文比非工具上下文脆弱性高24%。同时某些模型级漏洞在代理环境中失效。

Conclusion: AI代理系统具有独特的仅代理级漏洞特征，模型级和代理级漏洞特征存在根本差异，需要专门的安全评估方法。

Abstract: As the industry increasingly adopts agentic AI systems, understanding their
unique vulnerabilities becomes critical. Prior research suggests that security
flaws at the model level do not fully capture the risks present in agentic
deployments, where models interact with tools and external environments. This
paper investigates this gap by conducting a comparative red teaming analysis of
GPT-OSS-20B, a 20-billion parameter open-source model. Using our observability
framework AgentSeer to deconstruct agentic systems into granular actions and
components, we apply iterative red teaming attacks with harmful objectives from
HarmBench at two distinct levels: the standalone model and the model operating
within an agentic loop. Our evaluation reveals fundamental differences between
model level and agentic level vulnerability profiles. Critically, we discover
the existence of agentic-only vulnerabilities, attack vectors that emerge
exclusively within agentic execution contexts while remaining inert against
standalone models. Agentic level iterative attacks successfully compromise
objectives that completely failed at the model level, with tool-calling
contexts showing 24\% higher vulnerability than non-tool contexts. Conversely,
certain model-specific exploits work exclusively at the model level and fail
when transferred to agentic contexts, demonstrating that standalone model
vulnerabilities do not always generalize to deployed systems.

</details>


### [179] [CogAtom: From Cognitive Atoms to Olympiad-level Mathematical Reasoning in Large Language Models](https://arxiv.org/abs/2509.17318)
*Zhuofan Chen,Jiyuan He,Yichi Zhang,Xing Hu,Haoxing Wen,Jun Bai,Wenge Rong*

Main category: cs.AI

TL;DR: CogAtom是一个基于认知原子的框架，用于合成数学严谨且认知多样化的问题，通过重组基础推理单元来解决奥林匹克级数学问题稀缺的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 由于多步推理和抽象概念整合的需求，数学推理对大型语言模型构成重大挑战。当前测试时扩展技术严重依赖高质量、具有挑战性的问题，但奥林匹克级数学问题的稀缺性成为瓶颈。

Method: CogAtom将问题构建建模为选择和重组从人类编写解决方案中提取的基本推理单元（认知原子）的过程。使用多样性促进的随机游走算法探索认知原子空间，同时基于约束的重组机制确保逻辑合理性和结构有效性。

Result: 实验结果表明，CogAtom在准确性、推理深度和多样性方面优于现有方法，生成的问题在难度上与AIME（美国数学邀请赛）相匹配，同时在结构变化上超过AIME。

Conclusion: 这项工作为可扩展、高质量的数学问题生成提供了一条基于认知的途径。

Abstract: Mathematical reasoning poses significant challenges for Large Language Models
(LLMs) due to its demand for multi-step reasoning and abstract conceptual
integration. While recent test-time scaling techniques rely heavily on
high-quality, challenging problems, the scarcity of Olympiad-level math
problems remains a bottleneck. We introduce CogAtom, a novel cognitive
atom-based framework for synthesizing mathematically rigorous and cognitively
diverse problems. Unlike prior approaches, CogAtom models problem construction
as a process of selecting and recombining fundamental reasoning units,
cognitive atoms, extracted from human-authored solutions. A diversity-promoting
random walk algorithm enables exploration of the cognitive atom space, while a
constraint-based recombination mechanism ensures logical soundness and
structural validity. The combinatorial nature of the graph structure provides a
near-infinite space of reasoning paths, and the walk algorithm systematically
explores this space to achieve large-scale synthesis of high-quality problems;
meanwhile, by controlling the number of cognitive atoms, we can precisely
adjust problem difficulty, ensuring diversity, scalability, and controllability
of the generated problems. Experimental results demonstrate that CogAtom
outperforms existing methods in accuracy, reasoning depth, and diversity,
generating problems that closely match the difficulty of AIME while exceeding
it in structural variation. Our work offers a cognitively grounded pathway
toward scalable, high-quality math problem generation.Our code is publicly
available at https://github.com/Icarus-1111/CogAtom.

</details>


### [180] [LLaVul: A Multimodal LLM for Interpretable Vulnerability Reasoning about Source Code](https://arxiv.org/abs/2509.17337)
*Ala Jararweh,Michael Adams,Avinash Sahu,Abdullah Mueen,Afsah Anwar*

Main category: cs.AI

TL;DR: LLaVul是一个多模态大语言模型，专门用于通过问答方式对代码进行细粒度推理，增强代码漏洞的上下文相关理解。


<details>
  <summary>Details</summary>
Motivation: 当前软件系统复杂性增加，需要更精细的漏洞分析工具。现有方法多将漏洞分析简化为分类任务，忽略了现实场景的细微差别和上下文依赖性。虽然代码大语言模型在代码理解方面表现出色，但很少关注安全特定推理。

Method: 提出LLaVul多模态LLM，训练模型将配对的代码和自然语言查询整合到统一空间，通过问答方式增强代码漏洞的推理和上下文相关洞察。构建了包含真实世界漏洞的安全焦点问答数据集进行评估。

Result: LLaVul在问答和检测任务中优于最先进的通用和代码大语言模型。通过定性分析解释了决策过程，突出了模型的能力和局限性。

Conclusion: 通过整合代码和问答，LLaVul实现了更具可解释性和安全焦点的代码理解。

Abstract: Increasing complexity in software systems places a growing demand on
reasoning tools that unlock vulnerabilities manifest in source code. Many
current approaches focus on vulnerability analysis as a classifying task,
oversimplifying the nuanced and context-dependent real-world scenarios. Even
though current code large language models (LLMs) excel in code understanding,
they often pay little attention to security-specific reasoning. We propose
LLaVul, a multimodal LLM tailored to provide fine-grained reasoning about code
through question-answering (QA). Our model is trained to integrate paired code
and natural queries into a unified space, enhancing reasoning and
context-dependent insights about code vulnerability. To evaluate our model
performance, we construct a curated dataset of real-world vulnerabilities
paired with security-focused questions and answers. Our model outperforms
state-of-the-art general-purpose and code LLMs in the QA and detection tasks.
We further explain decision-making by conducting qualitative analysis to
highlight capabilities and limitations. By integrating code and QA, LLaVul
enables more interpretable and security-focused code understanding.

</details>


### [181] [Medical AI Consensus: A Multi-Agent Framework for Radiology Report Generation and Evaluation](https://arxiv.org/abs/2509.17353)
*Ahmed T. Elboardy,Ghada Khoriba,Essam A. Rashed*

Main category: cs.AI

TL;DR: 提出一个多智能体强化学习框架，作为放射学报告中多模态临床推理的基准和评估环境，整合LLM和LVM，通过十个专业智能体实现图像分析、特征提取、报告生成和评估。


<details>
  <summary>Details</summary>
Motivation: 自动化放射学报告生成面临临床可靠性系统构建和严格评估协议设计的双重挑战，需要建立可信的评估基准。

Method: 使用模块化架构集成大型语言模型和大型视觉模型，包含十个专业智能体负责不同任务，在公共放射学数据集上使用chatGPT-4o实现，并让LLM作为评估者与放射科医生反馈结合。

Result: 该框架能够在智能体层面（检测和分割准确性）和共识层面（报告质量和临床相关性）进行细粒度评估。

Conclusion: 通过将评估协议与LLM开发生命周期对齐，该基准为建立可信的基于偏差的放射学报告生成提供了路径。

Abstract: Automating radiology report generation poses a dual challenge: building
clinically reliable systems and designing rigorous evaluation protocols. We
introduce a multi-agent reinforcement learning framework that serves as both a
benchmark and evaluation environment for multimodal clinical reasoning in the
radiology ecosystem. The proposed framework integrates large language models
(LLMs) and large vision models (LVMs) within a modular architecture composed of
ten specialized agents responsible for image analysis, feature extraction,
report generation, review, and evaluation. This design enables fine-grained
assessment at both the agent level (e.g., detection and segmentation accuracy)
and the consensus level (e.g., report quality and clinical relevance). We
demonstrate an implementation using chatGPT-4o on public radiology datasets,
where LLMs act as evaluators alongside medical radiologist feedback. By
aligning evaluation protocols with the LLM development lifecycle, including
pretraining, finetuning, alignment, and deployment, the proposed benchmark
establishes a path toward trustworthy deviance-based radiology report
generation.

</details>


### [182] [Multi-Scenario Highway Lane-Change Intention Prediction: A Physics-Informed AI Framework for Three-Class Classification](https://arxiv.org/abs/2509.17354)
*Jiazhao Shi,Yichen Lin,Yiheng Hua,Ziyu Wang,Zijian Zhang,Wenjia Zheng,Yun Song,Kuan Lu,Shoufeng Lu*

Main category: cs.AI

TL;DR: 提出了一个物理信息AI框架，将车辆运动学、交互可行性和交通安全指标整合到学习过程中，用于车道变换意图预测，在高速公路和复杂匝道场景中实现了最先进的准确率。


<details>
  <summary>Details</summary>
Motivation: 车道变换是高速公路事故的主要原因，现有方法受限于二元分类、场景多样性不足以及在较长预测时间下性能下降的问题。

Method: 提出物理信息AI框架，整合车辆运动学、交互可行性和交通安全指标，将车道变换预测制定为三分类问题（左变换、右变换、无变换），使用LightGBM等机器学习模型。

Result: 在1秒预测时间下，highD数据集上达到99.8%准确率和93.6%宏F1分数，exiD数据集上达到96.1%准确率和88.7%宏F1分数，优于双层堆叠LSTM基线。

Conclusion: 物理信息和特征丰富的机器学习框架在自动驾驶系统的实时车道变换意图预测中具有实际优势。

Abstract: Lane-change maneuvers are a leading cause of highway accidents, underscoring
the need for accurate intention prediction to improve the safety and
decision-making of autonomous driving systems. While prior studies using
machine learning and deep learning methods (e.g., SVM, CNN, LSTM, Transformers)
have shown promise, most approaches remain limited by binary classification,
lack of scenario diversity, and degraded performance under longer prediction
horizons. In this study, we propose a physics-informed AI framework that
explicitly integrates vehicle kinematics, interaction feasibility, and
traffic-safety metrics (e.g., distance headway, time headway,
time-to-collision, closing gap time) into the learning process. lane-change
prediction is formulated as a three-class problem that distinguishes left
change, right change, and no change, and is evaluated across both straight
highway segments (highD) and complex ramp scenarios (exiD). By integrating
vehicle kinematics with interaction features, our machine learning models,
particularly LightGBM, achieve state-of-the-art accuracy and strong
generalization. Results show up to 99.8% accuracy and 93.6% macro F1 on highD,
and 96.1% accuracy and 88.7% macro F1 on exiD at a 1-second horizon,
outperforming a two-layer stacked LSTM baseline. These findings demonstrate the
practical advantages of a physics-informed and feature-rich machine learning
framework for real-time lane-change intention prediction in autonomous driving
systems.

</details>


### [183] [Correlation or Causation: Analyzing the Causal Structures of LLM and LRM Reasoning Process](https://arxiv.org/abs/2509.17380)
*Zhizhang FU,Guangsheng Bao,Hongbo Zhang,Chenkai Hu,Yue Zhang*

Main category: cs.AI

TL;DR: 该论文分析了LLMs和LRMs的因果推理能力，发现RLVR训练的LRMs在因果结构上表现更好，能减少虚假相关性并增强真正的因果模式。


<details>
  <summary>Details</summary>
Motivation: LLMs存在推理不忠实、偏见和不一致等问题，因为它们缺乏稳健的因果基础。虽然LRMs通过强化学习和蒸馏等技术提高了任务准确性，但这些训练方法对因果关系的影响尚未被充分探索。

Method: 研究对LLMs和LRMs进行了系统的因果分析，检查了四个关键变量的结构因果模型：问题指令(Z)、思考过程(T)、推理步骤(X)和答案(Y)。

Result: 研究发现RLVR训练的LRMs表现出增强的因果推理能力，更接近理想的因果结构，而LLMs和蒸馏LRMs未能解决因果关系缺陷。RLVR减少了虚假相关性并加强了真正的因果模式。

Conclusion: 这项研究有助于理解推理模型中的因果关系，强调了RLVR在增强因果推理中的关键作用，并为设计具有更强因果基础的未来AI系统提供了见解。

Abstract: LLMs suffer from critical reasoning issues such as unfaithfulness, bias, and
inconsistency, since they lack robust causal underpinnings and may rely on
superficial correlations rather than genuine understanding. Successive LRMs
have emerged as a promising alternative, leveraging advanced training
techniques such as reinforcement learning (RL) and distillation to improve task
accuracy. However, the impact of these training methods on causality remains
largely unexplored. In this study, we conduct a systematic causal analysis on
LLMs and LRMs, examining structural causal models (SCMs) of four key variables:
problem instruction (Z), thinking process (T), reasoning steps (X), and answer
(Y). Our findings reveal that RLVR-trained LRMs exhibit enhanced causal
reasoning capabilities, aligning more closely with ideal causal structures,
while LLMs and distilled LRMs fail to address causality-related deficiencies.
Our further investigation indicates that RLVR reduces spurious correlations and
strengthens genuine causal patterns, thereby mitigating unfaithfulness and
bias. In addition, our inspection on the dynamics of the RLVR training process
observes a high correlation between reduced spurious features and improved
causal structures, where the causal relationships consistently improve in the
training process. This study contributes to the understanding of causality in
reasoning models, highlights the critical role of RLVR in enhancing causal
reasoning, and provides insights for designing future AI systems with stronger
causal foundations. We release our code and data at
https://github.com/Harryking1999/CoT_Causal_Analysis.

</details>


### [184] [Program Synthesis via Test-Time Transduction](https://arxiv.org/abs/2509.17393)
*Kang-il Lee,Jahyun Koo,Seunghyun Yoon,Minbeom Kim,Hyukhun Koh,Dongryeol Lee,Kyomin Jung*

Main category: cs.AI

TL;DR: 提出了一种新的程序合成方法——转导式程序合成，通过在合成过程中显式利用测试输入来提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统程序合成方法在训练样本有限且测试输入包含各种边缘情况时，泛化能力不足，鲁棒性较差。

Method: 将程序合成视为在有限假设空间上的主动学习，使用LLM预测选定测试输入的输出，并通过贪婪最大化算法选择输入以减少LLM查询次数。

Result: 在Playgol字符串转换基准和MBPP+ Python代码生成基准上的实验表明，该方法在准确性和效率上都有显著提升。

Conclusion: 转导式程序合成框架有效解决了程序合成中的鲁棒性问题，特别是在真实世界场景中训练样本有限的情况。

Abstract: We introduce transductive program synthesis, a new formulation of the program
synthesis task that explicitly leverages test inputs during synthesis. While
prior approaches to program synthesis--whether based on natural language
descriptions or input-output examples--typically aim to generalize from
training examples, they often struggle with robustness, especially in
real-world settings where training examples are limited and test inputs involve
various edge cases. To address this, we propose a novel framework that improves
robustness by treating synthesis as an active learning over a finite hypothesis
class defined by programs' outputs. We use an LLM to predict outputs for
selected test inputs and eliminate inconsistent hypotheses, where the inputs
are chosen via a greedy maximin algorithm to minimize the number of LLM queries
required. We evaluate our approach on two real-world datasets: Playgol, a
string transformation benchmark, and MBPP+, a Python code generation benchmark.
We demonstrate that our method significantly improves program synthesis in both
accuracy and efficiency. We release our code at
https://github.com/klee972/SYNTRA.

</details>


### [185] [Evaluating Multimodal Large Language Models with Daily Composite Tasks in Home Environments](https://arxiv.org/abs/2509.17425)
*Zhenliang Zhang,Yuxi Wang,Hongzhao Xie,Shiyun Zhao,Mingyuan Liu,Yujie Lu,Xinyi He,Zhenku Cheng,Yujia Peng*

Main category: cs.AI

TL;DR: 本文评估了多模态大语言模型在复合任务中的表现，发现当前模型在物体理解、空间智能和社交活动三个核心领域表现不佳，与通用智能要求存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 人工通用智能与传统AI的关键区别在于能够执行需要广泛能力的复合任务。尽管多模态大语言模型驱动的具身智能体具有丰富的感知和交互能力，但它们在解决复合任务方面的能力尚未得到充分探索。

Method: 研究设计了基于幼儿日常活动的复合任务，在动态模拟家庭环境中评估了17个领先的专有和开源多模态大语言模型，任务涵盖物体理解、空间智能和社交活动三个核心领域。

Result: 所有模型在三个领域都表现出较差的性能，表明当前能力与通用智能要求之间存在显著差距。

Conclusion: 这些任务为评估具身智能体的通用能力提供了初步框架，标志着向具身多模态大语言模型开发和实际应用迈出了早期但重要的一步。

Abstract: A key feature differentiating artificial general intelligence (AGI) from
traditional AI is that AGI can perform composite tasks that require a wide
range of capabilities. Although embodied agents powered by multimodal large
language models (MLLMs) offer rich perceptual and interactive capabilities, it
remains largely unexplored whether they can solve composite tasks. In the
current work, we designed a set of composite tasks inspired by common daily
activities observed in early childhood development. Within a dynamic and
simulated home environment, these tasks span three core domains: object
understanding, spatial intelligence, and social activity. We evaluated 17
leading proprietary and open-source MLLMs on these tasks. The results
consistently showed poor performance across all three domains, indicating a
substantial gap between current capabilities and general intelligence
requirements. Together, our tasks offer a preliminary framework for evaluating
the general capabilities of embodied agents, marking an early but significant
step toward the development of embodied MLLMs and their real-world deployment.

</details>


### [186] [SPICED: A Synaptic Homeostasis-Inspired Framework for Unsupervised Continual EEG Decoding](https://arxiv.org/abs/2509.17439)
*Yangxuan Zhou,Sha Zhao,Jiquan Wang,Haiteng Jiang,Shijian Li,Tao Li,Gang Pan*

Main category: cs.AI

TL;DR: SPICED是一个受大脑突触稳态平衡启发的神经形态框架，用于无监督持续EEG解码，特别针对新个体不断出现的实际场景。


<details>
  <summary>Details</summary>
Motivation: 受人类大脑通过突触稳态实现动态稳定性-可塑性平衡的生物原理启发，解决在持续出现具有个体间变异性的新个体时，EEG解码的持续学习问题。

Method: SPICED包含一个新颖的突触网络，通过三种生物启发的神经机制实现持续适应中的动态扩展：(1)关键记忆重新激活；(2)突触巩固；(3)突触重归一化。这些机制与持续学习系统集成，优先重放与新个体强关联的任务判别性记忆痕迹。

Result: 在三个EEG数据集上的验证表明SPICED具有有效性，能够实现稳健的适应。

Conclusion: SPICED通过突触稳态机制动态增强任务判别性记忆痕迹并削弱有害记忆，在长期持续学习中有效缓解灾难性遗忘问题。

Abstract: Human brain achieves dynamic stability-plasticity balance through synaptic
homeostasis. Inspired by this biological principle, we propose SPICED: a
neuromorphic framework that integrates the synaptic homeostasis mechanism for
unsupervised continual EEG decoding, particularly addressing practical
scenarios where new individuals with inter-individual variability emerge
continually. SPICED comprises a novel synaptic network that enables dynamic
expansion during continual adaptation through three bio-inspired neural
mechanisms: (1) critical memory reactivation; (2) synaptic consolidation and
(3) synaptic renormalization. The interplay within synaptic homeostasis
dynamically strengthens task-discriminative memory traces and weakens
detrimental memories. By integrating these mechanisms with continual learning
system, SPICED preferentially replays task-discriminative memory traces that
exhibit strong associations with newly emerging individuals, thereby achieving
robust adaptations. Meanwhile, SPICED effectively mitigates catastrophic
forgetting by suppressing the replay prioritization of detrimental memories
during long-term continual learning. Validated on three EEG datasets, SPICED
show its effectiveness.

</details>


### [187] [AI Pangaea: Unifying Intelligence Islands for Adapting Myriad Tasks](https://arxiv.org/abs/2509.17460)
*Jianlong Chang,Haixin Wang,Zhiyuan Dang,Li Huang,Zhiyu Wang,Ruoqi Cao,Shihao Piao,Dongzhe Li,Dianyu Gao,Dongsheng Wang,Yin Li,Jinan Sun,Lu Fang,Zhouchen Lin*

Main category: cs.AI

TL;DR: Pangaea是一个AI超级大陆模型，旨在统一当前孤立的智能岛屿（特定任务AI模型），通过将各种数据编码为统一格式并在296个多模态数据集上进行预训练，实现了在45个通用任务和15个科学任务上的显著泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型局限于特定任务，形成智能孤岛，阻碍了人工通用智能的发展。Pangaea旨在统一这些孤岛，实现跨任务的泛化能力。

Method: 将任何数据编码为统一格式，在296个多模态数据集上进行预训练，积累通用知识。研究发现模态扩展效应遵循几何分布的累积分布函数。

Result: Pangaea在45个通用任务和15个科学任务上表现出卓越的泛化能力，涵盖广泛的科学学科。

Conclusion: Pangaea展示了处理多种任务的强大潜力，为人工通用智能的发展指明了新方向。

Abstract: The pursuit of artificial general intelligence continuously demands
generalization in one model across myriad tasks, even those not seen before.
However, current AI models are isolated from each other for being limited to
specific tasks, now first defined as Intelligence Islands. To unify
Intelligence Islands into one, we propose Pangaea, the first AI supercontinent
akin to the geological Pangaea. Pangaea encodes any data into a unified format
and accumulates universal knowledge through pre-training on 296 datasets across
diverse modalities. Eventually, it demonstrates remarkable generalization
across 45 general tasks and 15 scientific tasks encompassing a wide range of
scientific subjects. By investigating Pangaea deeper, the scaling effect of
modality is revealed, quantifying the universal knowledge accumulation across
modalities as the cumulative distribution function of a geometric distribution.
On the whole, Pangaea shows strong potential to handle myriad tasks, indicating
a new direction toward artificial general intelligence.

</details>


### [188] [A Multimodal Conversational Assistant for the Characterization of Agricultural Plots from Geospatial Open Data](https://arxiv.org/abs/2509.17544)
*Juan Cañada,Raúl Alonso,Julio Molleda,Fidel Díez*

Main category: cs.AI

TL;DR: 本文提出了一种开源对话助手，通过整合多模态检索和大型语言模型，让非专业用户能够用自然语言与异构农业和地理空间数据进行交互。


<details>
  <summary>Details</summary>
Motivation: 开放地球观测和农业数据集虽然具有支持可持续土地管理的潜力，但其高技术门槛限制了非专业用户的可访问性。

Method: 采用检索增强生成（RAG）架构，结合正射影像、Sentinel-2植被指数和用户提供的文档，系统能灵活决定依赖多模态证据、文本知识或两者结合来生成答案。使用Qwen3-32B进行零样本无监督评估。

Result: 初步结果显示系统能够生成清晰、相关且具有上下文感知的农业查询响应，同时在不同地理区域保持可重现性和可扩展性。

Conclusion: 主要贡献包括融合多模态地球观测和文本知识源的架构、通过自然语言交互降低专业农业信息获取门槛的示范，以及开放可重现的设计。

Abstract: The increasing availability of open Earth Observation (EO) and agricultural
datasets holds great potential for supporting sustainable land management.
However, their high technical entry barrier limits accessibility for non-expert
users. This study presents an open-source conversational assistant that
integrates multimodal retrieval and large language models (LLMs) to enable
natural language interaction with heterogeneous agricultural and geospatial
data. The proposed architecture combines orthophotos, Sentinel-2 vegetation
indices, and user-provided documents through retrieval-augmented generation
(RAG), allowing the system to flexibly determine whether to rely on multimodal
evidence, textual knowledge, or both in formulating an answer. To assess
response quality, we adopt an LLM-as-a-judge methodology using Qwen3-32B in a
zero-shot, unsupervised setting, applying direct scoring in a multi-dimensional
quantitative evaluation framework. Preliminary results show that the system is
capable of generating clear, relevant, and context-aware responses to
agricultural queries, while remaining reproducible and scalable across
geographic regions. The primary contributions of this work include an
architecture for fusing multimodal EO and textual knowledge sources, a
demonstration of lowering the barrier to access specialized agricultural
information through natural language interaction, and an open and reproducible
design.

</details>


### [189] [Is It Certainly a Deepfake? Reliability Analysis in Detection & Generation Ecosystem](https://arxiv.org/abs/2509.17550)
*Neslihan Kose,Anthony Rhodes,Umur Aybars Ciftci,Ilke Demir*

Main category: cs.AI

TL;DR: 该论文首次对深度伪造检测器进行全面的不确定性分析，研究了生成伪影如何影响预测置信度，并利用不确定性信息进行深度伪造源检测。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型质量提升，深度伪造导致在线信任危机。检测器的误用会加剧错误信息问题，因此需要系统分析检测器的不确定性。

Method: 采用贝叶斯神经网络和蒙特卡洛dropout方法量化检测器的偶然性和认知不确定性，在多个数据集和生成器上进行评估，包括二元/多分类、源检测等实验。

Result: 不确定性流形包含足够一致的信息可用于深度伪造源检测，不确定性图谱能定位像素级预测置信度，揭示与生成器特定伪影相关的模式。

Conclusion: 不确定性量化是可信合成媒体检测的基本要求，为部署可靠的深度伪造检测系统提供了关键见解。

Abstract: As generative models are advancing in quality and quantity for creating
synthetic content, deepfakes begin to cause online mistrust. Deepfake detectors
are proposed to counter this effect, however, misuse of detectors claiming fake
content as real or vice versa further fuels this misinformation problem. We
present the first comprehensive uncertainty analysis of deepfake detectors,
systematically investigating how generative artifacts influence prediction
confidence. As reflected in detectors' responses, deepfake generators also
contribute to this uncertainty as their generative residues vary, so we cross
the uncertainty analysis of deepfake detectors and generators. Based on our
observations, the uncertainty manifold holds enough consistent information to
leverage uncertainty for deepfake source detection. Our approach leverages
Bayesian Neural Networks and Monte Carlo dropout to quantify both aleatoric and
epistemic uncertainties across diverse detector architectures. We evaluate
uncertainty on two datasets with nine generators, with four blind and two
biological detectors, compare different uncertainty methods, explore region-
and pixel-based uncertainty, and conduct ablation studies. We conduct and
analyze binary real/fake, multi-class real/fake, source detection, and
leave-one-out experiments between the generator/detector combinations to share
their generalization capability, model calibration, uncertainty, and robustness
against adversarial attacks. We further introduce uncertainty maps that
localize prediction confidence at the pixel level, revealing distinct patterns
correlated with generator-specific artifacts. Our analysis provides critical
insights for deploying reliable deepfake detection systems and establishes
uncertainty quantification as a fundamental requirement for trustworthy
synthetic media detection.

</details>


### [190] [MontePrep: Monte-Carlo-Driven Automatic Data Preparation without Target Data Instances](https://arxiv.org/abs/2509.17553)
*Congcong Ge,Yachuan Liu,Yixuan Tang,Yifan Zhu,Yaofeng Tu,Yunjun Gao*

Main category: cs.AI

TL;DR: MontePrep是一个基于LLM的免训练数据准备框架，通过树状搜索方法实现零目标实例需求的数据转换管道合成


<details>
  <summary>Details</summary>
Motivation: 解决现有自动数据准备方法依赖人工监督信号或目标表数据访问权限的问题，适应真实商业场景需求

Method: 采用开源大语言模型驱动的蒙特卡洛树搜索方法，包含三个核心组件：数据准备动作沙盒(DPAS)、基础管道生成器(FPG)和执行感知管道优化器(EPO)

Result: 在广泛实验中，MontePrep显著优于五个最先进的竞争对手

Conclusion: MontePrep提供了一个有效的端到端ADP解决方案，在效率和效果方面都有显著提升

Abstract: In commercial systems, a pervasive requirement for automatic data preparation
(ADP) is to transfer relational data from disparate sources to targets with
standardized schema specifications. Previous methods rely on labor-intensive
supervision signals or target table data access permissions, limiting their
usage in real-world scenarios. To tackle these challenges, we propose an
effective end-to-end ADP framework MontePrep, which enables training-free
pipeline synthesis with zero target-instance requirements. MontePrep is
formulated as an open-source large language model (LLM) powered tree-structured
search problem. It consists of three pivot components, i.e., a data preparation
action sandbox (DPAS), a fundamental pipeline generator (FPG), and an
execution-aware pipeline optimizer (EPO). We first introduce DPAS, a
lightweight action sandbox, to navigate the search-based pipeline generation.
The design of DPAS circumvents exploration of infeasible pipelines. Then, we
present FPG to build executable DP pipelines incrementally, which explores the
predefined action sandbox by the LLM-powered Monte Carlo Tree Search.
Furthermore, we propose EPO, which invokes pipeline execution results from
sources to targets to evaluate the reliability of the generated pipelines in
FPG. In this way, unreasonable pipelines are eliminated, thus facilitating the
search process from both efficiency and effectiveness perspectives. Extensive
experimental results demonstrate the superiority of MontePrep with significant
improvement against five state-of-the-art competitors.

</details>


### [191] [LIMI: Less is More for Agency](https://arxiv.org/abs/2509.17567)
*Yang Xiao,Mohan Jiang,Jie Sun,Keyu Li,Jifan Lin,Yumin Zhuang,Ji Zeng,Shijie Xia,Qishuo Hua,Xuefeng Li,Xiaojie Cai,Tongyu Wang,Yue Zhang,Liming Liu,Xia Wu,Jinlong Hou,Yuan Cheng,Wenjie Li,Xiang Wang,Dequan Wang,Pengfei Liu*

Main category: cs.AI

TL;DR: LIMI挑战了传统AI代理开发的数据规模范式，证明只需78个精心设计的训练样本即可实现73.5%的代理基准性能，比使用10,000样本的模型提升53.7%，确立了"代理效率原则"。


<details>
  <summary>Details</summary>
Motivation: 行业迫切需要能够自主执行任务、操作工具并产生实际成果的AI系统，而不仅仅是进行推理和生成响应。当前方法错误地认为更多数据就能产生更好的代理能力。

Method: LIMI采用"少即是多"策略，通过战略性地专注于协作软件开发和科学研究工作流，仅使用78个精心策划的自主行为演示样本来训练模型。

Result: 在综合代理基准测试中达到73.5%的性能，显著优于Kimi-K2-Instruct(24.1%)、DeepSeek-V3.1(11.9%)、Qwen3-235B-A22B-Instruct(27.5%)和GLM-4.5(45.1%)等先进模型。

Conclusion: 机器自主性并非源于数据丰富性，而是来自高质量代理演示的战略性策划，这确立了代理效率原则，为AI代理开发提供了新的范式。

Abstract: We define Agency as the emergent capacity of AI systems to function as
autonomous agents actively discovering problems, formulating hypotheses, and
executing solutions through self-directed engagement with environments and
tools. This fundamental capability marks the dawn of the Age of AI Agency,
driven by a critical industry shift: the urgent need for AI systems that don't
just think, but work. While current AI excels at reasoning and generating
responses, industries demand autonomous agents that can execute tasks, operate
tools, and drive real-world outcomes. As agentic intelligence becomes the
defining characteristic separating cognitive systems from productive workers,
efficiently cultivating machine autonomy becomes paramount. Current approaches
assume that more data yields better agency, following traditional scaling laws
from language modeling. We fundamentally challenge this paradigm. LIMI (Less Is
More for Intelligent Agency) demonstrates that agency follows radically
different development principles. Through strategic focus on collaborative
software development and scientific research workflows, we show that
sophisticated agentic intelligence can emerge from minimal but strategically
curated demonstrations of autonomous behavior. Using only 78 carefully designed
training samples, LIMI achieves 73.5% on comprehensive agency benchmarks,
dramatically outperforming state-of-the-art models: Kimi-K2-Instruct (24.1%),
DeepSeek-V3.1 (11.9%), Qwen3-235B-A22B-Instruct (27.5%), and GLM-4.5 (45.1%).
Most strikingly, LIMI demonstrates 53.7% improvement over models trained on
10,000 samples-achieving superior agentic intelligence with 128 times fewer
samples. Our findings establish the Agency Efficiency Principle: machine
autonomy emerges not from data abundance but from strategic curation of
high-quality agentic demonstrations.

</details>


### [192] [Table2LaTeX-RL: High-Fidelity LaTeX Code Generation from Table Images via Reinforced Multimodal Language Models](https://arxiv.org/abs/2509.17589)
*Jun Ling,Yao Qi,Tao Huang,Shibo Zhou,Yanqin Huang,Jiang Yang,Ziqi Song,Ying Zhou,Yang Yang,Heng Tao Shen,Peng Wang*

Main category: cs.AI

TL;DR: 本文提出了一种基于强化多模态大语言模型的表格图像转LaTeX代码生成方法，通过双奖励强化学习策略优化结构级和视觉保真度奖励，在复杂表格处理上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 自动化从视觉输入重建高质量、可发布的表格，解决现有方法在处理大型、深度嵌套结构和语义丰富的不规则单元格内容等复杂表格时的失败问题。

Method: 采用预训练多模态大语言模型在大规模表格转LaTeX数据集上微调，引入基于GRPO的双奖励强化学习策略，结合结构级LaTeX代码奖励和渲染输出的视觉保真度奖励。

Result: 在结合TEDS-Structure和CW-SSIM的混合评估协议下，该方法在结构复杂表格上实现了最先进的性能表现。

Conclusion: 所提出的方法通过直接优化视觉输出质量，在处理复杂表格时展现出有效性和鲁棒性，为表格图像到LaTeX代码生成任务提供了有效的解决方案。

Abstract: In this work, we address the task of table image to LaTeX code generation,
with the goal of automating the reconstruction of high-quality,
publication-ready tables from visual inputs. A central challenge of this task
lies in accurately handling complex tables -- those with large sizes, deeply
nested structures, and semantically rich or irregular cell content -- where
existing methods often fail. We begin with a comprehensive analysis,
identifying key challenges and highlighting the limitations of current
evaluation protocols. To overcome these issues, we propose a reinforced
multimodal large language model (MLLM) framework, where a pre-trained MLLM is
fine-tuned on a large-scale table-to-LaTeX dataset. To further improve
generation quality, we introduce a dual-reward reinforcement learning strategy
based on Group Relative Policy Optimization (GRPO). Unlike standard approaches
that optimize purely over text outputs, our method incorporates both a
structure-level reward on LaTeX code and a visual fidelity reward computed from
rendered outputs, enabling direct optimization of the visual output quality. We
adopt a hybrid evaluation protocol combining TEDS-Structure and CW-SSIM, and
show that our method achieves state-of-the-art performance, particularly on
structurally complex tables, demonstrating the effectiveness and robustness of
our approach.

</details>


### [193] [EngiBench: A Benchmark for Evaluating Large Language Models on Engineering Problem Solving](https://arxiv.org/abs/2509.17677)
*Xiyuan Zhou,Xinlei Wang,Yirui He,Yang Wu,Ruixi Zou,Yuheng Cheng,Yulu Xie,Wenxuan Liu,Huan Zhao,Yan Xu,Jinjin Gu,Junhua Zhao*

Main category: cs.AI

TL;DR: EngiBench是一个分层次基准测试，用于评估大语言模型在解决工程问题上的能力，涵盖三个难度级别和多个工程子领域，通过系统化变体分析模型的鲁棒性、领域知识和数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试未能捕捉真实世界工程问题的复杂性（如不确定性、上下文和开放场景），而大语言模型在良好条件下的数学推理表现虽强，但缺乏对工程问题实际需求的评估。

Method: 设计分层基准EngiBench，包含基础知识检索、多步上下文推理和开放建模三个难度级别；系统地将每个问题重写为扰动、知识增强和数学抽象三个变体，以分别评估模型的不同能力。

Result: 实验结果显示模型在不同难度级别存在明显性能差距：任务越难表现越差，问题轻微变化时性能下降，在高级工程任务上远落后于人类专家。

Conclusion: 当前大语言模型仍缺乏真实世界工程所需的高级推理能力，未来需要开发具有更深层次和更可靠问题解决能力的模型。

Abstract: Large language models (LLMs) have shown strong performance on mathematical
reasoning under well-posed conditions. However, real-world engineering problems
require more than mathematical symbolic computation -- they need to deal with
uncertainty, context, and open-ended scenarios. Existing benchmarks fail to
capture these complexities. We introduce EngiBench, a hierarchical benchmark
designed to evaluate LLMs on solving engineering problems. It spans three
levels of increasing difficulty (foundational knowledge retrieval, multi-step
contextual reasoning, and open-ended modeling) and covers diverse engineering
subfields. To facilitate a deeper understanding of model performance, we
systematically rewrite each problem into three controlled variants (perturbed,
knowledge-enhanced, and math abstraction), enabling us to separately evaluate
the model's robustness, domain-specific knowledge, and mathematical reasoning
abilities. Experiment results reveal a clear performance gap across levels:
models struggle more as tasks get harder, perform worse when problems are
slightly changed, and fall far behind human experts on the high-level
engineering tasks. These findings reveal that current LLMs still lack the
high-level reasoning needed for real-world engineering, highlighting the need
for future models with deeper and more reliable problem-solving capabilities.
Our source code and data are available at
https://github.com/EngiBench/EngiBench.

</details>


### [194] [Virtual Arc Consistency for Linear Constraints inCost Function Networks](https://arxiv.org/abs/2509.17706)
*Pierre Montalbano,Simon de Givry,George Katsirelos*

Main category: cs.AI

TL;DR: 本文提出了一种改进的软弧一致性算法来处理线性约束，相比原有算法显著提高了下界质量，在某些情况下减少了求解时间。


<details>
  <summary>Details</summary>
Motivation: 在约束规划中，解决带有硬约束和软约束的离散最小化问题存在三种方法：软全局约束、线性规划重构和局部成本函数重构。软全局约束方法边界较弱，线性规划方法重构规模大，而软弧一致性方法边界质量中等。最近线性约束作为局部成本函数的引入增加了建模表达能力，需要适配现有SAC算法来处理线性约束。

Method: 作者改进了现有的软弧一致性算法，使其能够处理线性约束作为局部成本函数。该方法结合了软约束传播和线性约束处理能力。

Result: 实验结果表明，改进后的算法在多个基准测试中相比原始算法显著提高了下界质量，在某些情况下减少了求解时间。

Conclusion: 通过将线性约束整合到软弧一致性框架中，可以在保持建模表达能力的同时获得更好的求解性能，为约束规划中的最小化问题提供了有效的解决方案。

Abstract: In Constraint Programming, solving discrete minimization problems with hard
and soft constraints can be done either using (i) soft global constraints, (ii)
a reformulation into a linear program, or (iii) a reformulation into local cost
functions. Approach (i) benefits from a vast catalog of constraints. Each soft
constraint propagator communicates with other soft constraints only through the
variable domains, resulting in weak lower bounds. Conversely, the approach (ii)
provides a global view with strong bounds, but the size of the reformulation
can be problematic. We focus on approach (iii) in which soft arc consistency
(SAC) algorithms produce bounds of intermediate quality. Recently, the
introduction of linear constraints as local cost functions increases their
modeling expressiveness. We adapt an existing SAC algorithm to handle linear
constraints. We show that our algorithm significantly improves the lower bounds
compared to the original algorithm on several benchmarks, reducing solving time
in some cases.

</details>


### [195] [DA-Mamba: Dialogue-aware selective state-space model for multimodal engagement estimation](https://arxiv.org/abs/2509.17711)
*Shenwei Kang,Xin Zhang,Wen Liu,Bin Li,Yujie Liu,Bo Gao*

Main category: cs.AI

TL;DR: DA-Mamba是一种对话感知的多模态架构，用于估计对话场景中的人类参与度，通过Mamba选择性状态空间处理替代注意力机制，实现线性时间内存复杂度并保持跨模态推理能力。


<details>
  <summary>Details</summary>
Motivation: 对话场景中的人类参与度估计对于自适应教学、远程医疗评估和社交感知人机交互等应用至关重要。参与度是通过面部表情、语音、手势和行为线索随时间传递的动态多模态信号。

Method: 设计基于Mamba的对话感知选择性状态空间模型，包含三个核心模块：对话感知编码器、模态组融合和伙伴组融合机制，这些模块实现表达性对话理解。

Result: 在三个标准基准测试（NoXi、NoXi-Add和MPIIGI）上的实验表明，DA-Mamba在一致性相关系数（CCC）上超越了现有最先进方法，同时减少了训练时间和峰值内存使用。

Conclusion: DA-Mamba的性能提升使得能够处理更长的序列，并促进在资源受限的多方对话设置中的实时部署。

Abstract: Human engagement estimation in conversational scenarios is essential for
applications such as adaptive tutoring, remote healthcare assessment, and
socially aware human--computer interaction. Engagement is a dynamic, multimodal
signal conveyed by facial expressions, speech, gestures, and behavioral cues
over time. In this work we introduce DA-Mamba, a dialogue-aware multimodal
architecture that replaces attention-heavy dialogue encoders with Mamba-based
selective state-space processing to achieve linear time and memory complexity
while retaining expressive cross-modal reasoning. We design a Mamba
dialogue-aware selective state-space model composed of three core modules: a
Dialogue-Aware Encoder, and two Mamba-based fusion mechanisms: Modality-Group
Fusion and Partner-Group Fusion, these modules achieve expressive dialogue
understanding. Extensive experiments on three standard benchmarks (NoXi,
NoXi-Add, and MPIIGI) show that DA-Mamba surpasses prior state-of-the-art
(SOTA) methods in concordance correlation coefficient (CCC), while reducing
training time and peak memory; these gains enable processing much longer
sequences and facilitate real-time deployment in resource-constrained,
multi-party conversational settings. The source code will be available at:
https://github.com/kksssssss-ssda/MMEA.

</details>


### [196] [Efficient & Correct Predictive Equivalence for Decision Trees](https://arxiv.org/abs/2509.17774)
*Joao Marques-Silva,Alexey Ignatiev*

Main category: cs.AI

TL;DR: 本文分析了McTavish等人提出的MBDSR方法在决策树预测等价性判定中的局限性，证明QM方法存在最坏情况指数复杂度且可能产生错误结果，并提出多项式时间算法解决相关问题。


<details>
  <summary>Details</summary>
Motivation: 决策树的Rashomon集合中存在大量预测等价的决策树，这会影响特征重要性的准确性。McTavish等人的MBDSR方法使用QM方法进行最小DNF表示，但该方法存在计算复杂度和正确性问题。

Method: 首先证明QM方法存在最坏情况指数复杂度，其次展示MBDSR方法在预测等价性判定中可能产生错误结果，最后提出多项式时间算法解决相关问题。

Result: 实验证实，对于触发QM方法最坏情况的决策树，本文提出的算法比McTavish等人的方法快几个数量级。

Conclusion: MBDSR方法存在严重缺陷，而本文提出的多项式时间算法能更有效地解决决策树预测等价性及相关问题。

Abstract: The Rashomon set of decision trees (DTs) finds importance uses. Recent work
showed that DTs computing the same classification function, i.e. predictive
equivalent DTs, can represent a significant fraction of the Rashomon set. Such
redundancy is undesirable. For example, feature importance based on the
Rashomon set becomes inaccurate due the existence of predictive equivalent DTs,
i.e. DTs with the same prediction for every possible input. In recent work,
McTavish et al. proposed solutions for several computational problems related
with DTs, including that of deciding predictive equivalent DTs. This approach,
which this paper refers to as MBDSR, consists of applying the well-known method
of Quine-McCluskey (QM) for obtaining minimum-size DNF (disjunctive normal
form) representations of DTs, which are then used for comparing DTs for
predictive equivalence. Furthermore, the minimum-size DNF representation was
also applied to computing explanations for the predictions made by DTs, and to
finding predictions in the presence of missing data. However, the problem of
formula minimization is hard for the second level of the polynomial hierarchy,
and the QM method may exhibit worst-case exponential running time and space.
This paper first demonstrates that there exist decision trees that trigger the
worst-case exponential running time and space of the QM method. Second, the
paper shows that the MBDSR approach can produce incorrect results for the
problem of deciding predictive equivalence. Third, the paper shows that any of
the problems to which the minimum-size DNF representation has been applied to
can in fact be solved in polynomial time, in the size of the DT. The
experiments confirm that, for DTs for which the the worst-case of the QM method
is triggered, the algorithms proposed in this paper are orders of magnitude
faster than the ones proposed by McTavish et al.

</details>


### [197] [Mitigating Strategy-Selection Bias in Reasoning for More Effective Test-Time Scaling](https://arxiv.org/abs/2509.17905)
*Zongqian Wu,Baoduo Xu,Tianyu Li,Zhu Sun,Xiaofeng Zhu,Lei Feng*

Main category: cs.AI

TL;DR: 本文提出TTS-Uniform框架，通过均匀分配采样预算来缓解LLM在测试时扩展中的推理策略选择偏差问题，显著提升了扩展效果。


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展方法忽视了推理策略选择偏差问题，即LLM在生成推理过程时倾向于使用某些策略而忽略其他有效替代方案，导致解决方案空间探索不足。

Method: TTS-Uniform框架包含三个步骤：(i)识别潜在策略，(ii)均匀分配采样预算，(iii)在聚合前过滤不稳定策略。

Result: 实验结果表明，TTS-Uniform在多个主流LLM和基准数据集上显著提升了扩展效果。

Conclusion: 该研究揭示了推理策略选择偏差对测试时扩展的影响，并提出了有效的解决方案，为提升LLM性能提供了新思路。

Abstract: Test-time scaling (TTS) has been shown to improve the performance of large
language models (LLMs) by sampling and aggregating diverse reasoning paths.
However, existing research has overlooked a critical issue: selection bias of
reasoning strategies during scaling. Specifically, when generating reasoning
processes, LLMs tend to follow certain strategies (e.g., algebraic solutions
for math problems) while neglecting other valid alternatives (e.g., geometric
solutions), resulting in insufficient exploration of the solution space. To
further understand the impact of this bias, we present a theoretical analysis
that reveals when it undermines the effectiveness of test-time scaling.
Motivated by this theoretical insight, we introduce TTS-Uniform, a framework
designed to mitigate the selection bias of reasoning strategies. It (i)
identifies potential strategies, (ii) uniformly allocates the sampling budget
across them, and (iii) filters out unstable strategies prior to aggregation.
Experimental results show that TTS-Uniform significantly enhances scaling
effectiveness across multiple mainstream LLMs and benchmark datasets.

</details>


### [198] [MEF: A Systematic Evaluation Framework for Text-to-Image Models](https://arxiv.org/abs/2509.17907)
*Xiaojing Dong,Weilin Huang,Liang Li,Yiying Li,Shu Liu,Tongtong Ou,Shuang Ouyang,Yu Tian,Fengxuan Zhao*

Main category: cs.AI

TL;DR: 本文提出了Magic评估框架(MEF)，用于系统评估文本到图像(T2I)生成模型，通过构建Magic-Bench-377基准测试集，结合ELO和MOS方法进行综合评估。


<details>
  <summary>Details</summary>
Motivation: 现有T2I评估方法缺乏应用场景视角，且ELO和MOS方法各有局限性，需要更系统实用的评估框架。

Method: 提出结构化分类法构建Magic-Bench-377基准测试集，结合ELO进行模型排名和MOS进行维度评分，使用多元逻辑回归分析各维度对用户满意度的贡献。

Result: 应用MEF框架获得了当前T2I模型的排行榜和领先模型的关键特征。

Conclusion: MEF框架为视觉生成模型评估提供了系统方法，相关基准测试集已开源以推动研究发展。

Abstract: Rapid advances in text-to-image (T2I) generation have raised higher
requirements for evaluation methodologies. Existing benchmarks center on
objective capabilities and dimensions, but lack an application-scenario
perspective, limiting external validity. Moreover, current evaluations
typically rely on either ELO for overall ranking or MOS for dimension-specific
scoring, yet both methods have inherent shortcomings and limited
interpretability. Therefore, we introduce the Magic Evaluation Framework (MEF),
a systematic and practical approach for evaluating T2I models. First, we
propose a structured taxonomy encompassing user scenarios, elements, element
compositions, and text expression forms to construct the Magic-Bench-377, which
supports label-level assessment and ensures a balanced coverage of both user
scenarios and capabilities. On this basis, we combine ELO and
dimension-specific MOS to generate model rankings and fine-grained assessments
respectively. This joint evaluation method further enables us to quantitatively
analyze the contribution of each dimension to user satisfaction using
multivariate logistic regression. By applying MEF to current T2I models, we
obtain a leaderboard and key characteristics of the leading models. We release
our evaluation framework and make Magic-Bench-377 fully open-source to advance
research in the evaluation of visual generative models.

</details>


### [199] [Orcust: Stepwise-Feedback Reinforcement Learning for GUI Agent](https://arxiv.org/abs/2509.17917)
*Junyu Lu,Songxin Zhang,Zejian Xie,Zhuoyang Song,Jiaxing Zhang*

Main category: cs.AI

TL;DR: Orcust是一个GUI代理框架，通过约束奖励建模和在线轨迹构建来提升交互GUI任务中的推理可靠性和数据效率


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理模型存在奖励信号不可靠和在线轨迹生成受限的问题，需要提高推理可靠性和数据效率

Method: 结合原则约束奖励建模(PCRM)和在线虚拟机基础轨迹构建(OVTC)，利用环境可验证和LLM衍生的原则来约束推理过程，并通过虚拟机构建结构化交互轨迹

Result: 在标准GUI基准测试中达到最先进性能，ScreenSpot提升22.2%，ScreenSpot-Pro提升23.9%

Conclusion: Orcust有效提升了GUI代理在各种环境和任务复杂度下的推理能力、适应性和可扩展性

Abstract: Recent advances in GUI agents have achieved remarkable grounding and
action-prediction performance, yet existing models struggle with unreliable
reward signals and limited online trajectory generation. In this paper, we
introduce Orcust, a framework that integrates Principle-Constrained Reward
Modeling (PCRM) and Online VM-Grounded Trajectory Construction (OVTC) to
enhance reasoning reliability and data efficiency in interactive GUI tasks. We
leverages environment-verifiable and LLM-derived principle to enforce
interpretable reward signals that constrain long chain-of-thought reasoning and
rule-based feedback. OVTC spins up instrumented virtual machines to
autonomously collect structured GUI interaction trajectories with explicit
procedural and structural objectives, enabling the training of a stepwise
reward model that robustly captures human preferences and adheres to
task-specific constraints. Extensive experiments on standard GUI benchmarks
covering perceptual grounding, foundational operations, and end-to-end task
execution reveal that Orcust achieves state-of-the-art performance, improving
by 22.2\% on ScreenSpot and 23.9\% on ScreenSpot-Pro over the base model (i.e.
Qwen2.5-VL-7B). The results demonstrate Orcust's effectiveness in enhancing the
reasoning, adaptability and scalability of GUI agents across various
environments and task complexities.

</details>


### [200] ["I think this is fair'': Uncovering the Complexities of Stakeholder Decision-Making in AI Fairness Assessment](https://arxiv.org/abs/2509.17956)
*Lin Luo,Yuri Nakao,Mathieu Chollet,Hiroya Inakoshi,Simone Stumpf*

Main category: cs.AI

TL;DR: 研究通过质性访谈发现，非AI专业利益相关者在评估AI公平性时比专家考虑更复杂，包括超出法律保护的特征、定制化指标和更严格的阈值


<details>
  <summary>Details</summary>
Motivation: 现有AI公平性评估主要由AI专家主导，但缺乏对受AI决策影响但无AI专业知识的利益相关者如何评估公平性的理解

Method: 对30名无AI专业知识的利益相关者进行质性研究，在信用评级场景中观察他们如何选择特征优先级、指标和阈值

Result: 利益相关者的公平性决策比专家实践更复杂：考虑超出法律保护的特征、为特定情境定制指标、设定多样且更严格的阈值，甚至偏好设计定制化公平性

Conclusion: 研究结果扩展了对利益相关者如何有意义参与AI公平性治理的理解，强调纳入利益相关者细致公平判断的重要性

Abstract: Assessing fairness in artificial intelligence (AI) typically involves AI
experts who select protected features, fairness metrics, and set fairness
thresholds. However, little is known about how stakeholders, particularly those
affected by AI outcomes but lacking AI expertise, assess fairness. To address
this gap, we conducted a qualitative study with 30 stakeholders without AI
expertise, representing potential decision subjects in a credit rating
scenario, to examine how they assess fairness when placed in the role of
deciding on features with priority, metrics, and thresholds. We reveal that
stakeholders' fairness decisions are more complex than typical AI expert
practices: they considered features far beyond legally protected features,
tailored metrics for specific contexts, set diverse yet stricter fairness
thresholds, and even preferred designing customized fairness. Our results
extend the understanding of how stakeholders can meaningfully contribute to AI
fairness governance and mitigation, underscoring the importance of
incorporating stakeholders' nuanced fairness judgments.

</details>


### [201] [On the Variational Costs of Changing Our Minds](https://arxiv.org/abs/2509.17957)
*David Hyland,Mahault Albarracin*

Main category: cs.AI

TL;DR: 本文提出一个形式化框架，将信念更新建模为动机驱动的变分决策过程，认为常见的认知偏见（如确认偏误）是适应性的理性策略而非认知缺陷。


<details>
  <summary>Details</summary>
Motivation: 解释为什么人类思维会表现出看似非理性的信念维护行为（如确认偏误、态度极化），挑战传统将这些行为视为认知缺陷的观点，认为它们是应对信念更新成本的自适应策略。

Method: 采用动机变分决策框架，将信念更新建模为权衡信念效用与信息成本（用KL散度量化）的过程，并进行计算实验验证模型。

Result: 计算实验表明，简单的资源理性模型能够定性模拟常见的人类行为模式，包括确认偏误和态度极化现象。

Conclusion: 该框架为信念变化的动机贝叶斯机制提供了更全面的解释，并为预测、补偿和纠正信念更新偏差提供了实用见解。

Abstract: The human mind is capable of extraordinary achievements, yet it often appears
to work against itself. It actively defends its cherished beliefs even in the
face of contradictory evidence, conveniently interprets information to conform
to desired narratives, and selectively searches for or avoids information to
suit its various purposes. Despite these behaviours deviating from common
normative standards for belief updating, we argue that such 'biases' are not
inherently cognitive flaws, but rather an adaptive response to the significant
pragmatic and cognitive costs associated with revising one's beliefs. This
paper introduces a formal framework that aims to model the influence of these
costs on our belief updating mechanisms.
  We treat belief updating as a motivated variational decision, where agents
weigh the perceived 'utility' of a belief against the informational cost
required to adopt a new belief state, quantified by the Kullback-Leibler
divergence from the prior to the variational posterior. We perform
computational experiments to demonstrate that simple instantiations of this
resource-rational model can be used to qualitatively emulate commonplace human
behaviours, including confirmation bias and attitude polarisation. In doing so,
we suggest that this framework makes steps toward a more holistic account of
the motivated Bayesian mechanics of belief change and provides practical
insights for predicting, compensating for, and correcting deviations from
desired belief updating processes.

</details>


### [202] [The STAR-XAI Protocol: An Interactive Framework for Inducing Second-Order Agency in AI Agents](https://arxiv.org/abs/2509.17978)
*Antoni Guasch,Maria Isabel Valdez*

Main category: cs.AI

TL;DR: STAR-XAI协议是一种新型AI训练和操作方法论，通过结构化苏格拉底对话将大型推理模型转变为透明、可验证的"清晰盒子"代理。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型在复杂长程任务中存在可靠性差、透明度低的问题，存在"思考幻觉"，需要构建可验证的可靠AI代理。

Method: 采用结构化苏格拉底对话框架，通过意识转移包和游戏循环机制，强制事前策略论证和状态锁定校验，防止错误累积。

Result: 在复杂策略游戏"Caps i Caps"的25步案例研究中，代理不仅解决了高复杂度难题，还展示了二阶代理能力，能够识别自身计划缺陷并调整核心协议。

Conclusion: STAR-XAI协议为创建高性能、透明、可审计且可信赖的AI代理提供了实用路径。

Abstract: Current Large Reasoning Models (LRMs) exhibit significant limitations in
reliability and transparency, often showing a collapse in reasoning
capabilities when faced with high-complexity, long-horizon tasks. This
"illusion of thinking" is frequently an artifact of non-agentic, black-box
evaluation paradigms that fail to cultivate robust problem-solving processes.
In response, we introduce The STAR-XAI Protocol (Socratic, Transparent,
Agentic, Reasoning - for eXplainable Artificial Intelligence), a novel
methodology for training and operating verifiably reliable AI agents. Our
method reframes the human-AI interaction as a structured, Socratic dialogue,
governed by an explicit and evolving rulebook, the Consciousness Transfer
Package (CTP). Through an interactive Gameplay Cycle that enforces ante-hoc
strategic justification and a state-locking Checksum that prevents error
accumulation, the protocol transforms a powerful but opaque LRM into a
disciplined "Clear Box" agent. We demonstrate the efficacy of this method
through an exhaustive 25-move case study in the complex strategic game "Caps i
Caps". The agent not only solved the high-complexity puzzle but also
demonstrated Second-Order Agency, identifying flaws in its own
supervisor-approved plans and adapting its core integrity protocols mid-task.
The STAR-XAI Protocol offers a practical pathway to creating AI agents that are
not just high-performing, but also transparent, auditable, and trustworthy by
design.

</details>


### [203] [Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates](https://arxiv.org/abs/2509.18076)
*Hy Dang,Tianyi Liu,Zhuofeng Wu,Jingfeng Yang,Haoming Jiang,Tao Yang,Pei Chen,Zhengyang Wang,Helen Wang,Huasheng Li,Bing Yin,Meng Jiang*

Main category: cs.AI

TL;DR: 提出了一种基于课程学习理念的框架，使用结构化推理模板来指导LLMs进行更精确的函数调用，相比自由形式的CoT提示在工具使用任务上取得了3-12%的相对改进。


<details>
  <summary>Details</summary>
Motivation: LLMs在现实世界工具交互中经常因参数化错误、工具选择不当或用户意图误解而失败，这些问题源于对用户目标理解不完整和对工具文档理解不足。

Method: 引入课程启发式框架，利用结构化推理模板指导LLMs通过更审慎的逐步指令生成函数调用，替代自由形式的CoT提示。

Result: 实验结果显示该方法减少了工具使用错误，在不同模型系列和方法上相比强基线实现了3-12%的相对改进。

Conclusion: 该框架增强了工具使用代理的鲁棒性、可解释性和透明度，推动了更可靠的现实世界AI助手的发展。

Abstract: Large language models (LLMs) have demonstrated strong reasoning and tool-use
capabilities, yet they often fail in real-world tool-interactions due to
incorrect parameterization, poor tool selection, or misinterpretation of user
intent. These issues often stem from an incomplete understanding of user goals
and inadequate comprehension of tool documentation. While Chain-of-Thought
(CoT) prompting has proven effective for enhancing reasoning in general
contexts, our analysis reveals that free-form CoT is insufficient and sometimes
counterproductive for structured function-calling tasks. To address this, we
introduce a curriculum-inspired framework that leverages structured reasoning
templates to guide LLMs through more deliberate step-by-step instructions for
generating function callings. Experimental results show that our method reduces
tool-use errors, achieving 3-12% relative improvements over strong baselines
across diverse model series and approaches. Moreover, our framework enhances
the robustness, interpretability, and transparency of tool-using agents,
advancing the development of more reliable AI assistants for real-world
applications.

</details>


### [204] [Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning](https://arxiv.org/abs/2509.18083)
*Valentin Lacombe,Valentin Quesnel,Damien Sileo*

Main category: cs.AI

TL;DR: Reasoning Core是一个新的可扩展强化学习环境，专注于通过可验证奖励来提升大型语言模型的符号推理能力，覆盖多个核心形式化领域。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注游戏或孤立谜题，缺乏对基础符号推理能力的系统性评估和训练。Reasoning Core旨在填补这一空白，为LLMs提供更全面的推理能力提升平台。

Method: 基于高通用性问题分布、外部工具验证和连续难度控制三大设计原则，程序化生成PDDL规划、一阶逻辑、上下文无关文法解析、因果推理和系统方程求解等问题。

Result: 前沿LLMs的零样本评估证实了Reasoning Core任务的难度，表明该环境具有挑战性。

Conclusion: Reasoning Core作为一个有前景的资源，有望显著提升未来模型的推理能力，为LLMs的符号推理研究提供无限的新训练实例。

Abstract: We introduce Reasoning Core, a new scalable environment for Reinforcement
Learning with Verifiable Rewards (RLVR), designed to advance foundational
symbolic reasoning in Large Language Models (LLMs). Unlike existing benchmarks
that focus on games or isolated puzzles, Reasoning Core procedurally generates
problems across core formal domains, including PDDL planning, first-order
logic, context-free grammar parsing, causal reasoning, and system equation
solving. The environment is built on key design principles of high-generality
problem distributions, verification via external tools, and continuous
difficulty control, which together provide a virtually infinite supply of novel
training instances. Initial zero-shot evaluations with frontier LLMs confirm
the difficulty of Reasoning Core's tasks, positioning it as a promising
resource to improve the reasoning capabilities of future models.

</details>
