<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 156]
- [cs.AI](#cs.AI) [Total: 72]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Table Question Answering in the Era of Large Language Models: A Comprehensive Survey of Tasks, Methods, and Evaluation](https://arxiv.org/abs/2510.09671)
*Wei Zhou,Bolei Ma,Annemarie Friedrich,Mohsen Mesgar*

Main category: cs.CL

TL;DR: 这篇论文是关于表格问答(TQA)的综述研究，系统整理了基于大语言模型的表格问答方法，包括任务分类、基准测试、建模策略和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 表格问答领域缺乏对任务形式、核心挑战和方法趋势的系统性组织，特别是在强化学习等新兴研究方向出现后，需要提供一个结构化的综述来统一分散的研究线索。

Method: 通过全面分类现有基准测试和任务设置，按目标挑战对当前建模策略进行分组，分析其优缺点，并突出未被充分探索但及时的研究主题。

Result: 提供了一个综合的结构化TQA研究概述，为TQA社区提供了统一的基础，使研究人员能够更深入地理解最新技术状态。

Conclusion: 该综述通过统一分散的研究线索和识别开放问题，为这个快速发展的领域提供了指导未来发展的基础。

Abstract: Table Question Answering (TQA) aims to answer natural language questions
about tabular data, often accompanied by additional contexts such as text
passages. The task spans diverse settings, varying in table representation,
question/answer complexity, modality involved, and domain. While recent
advances in large language models (LLMs) have led to substantial progress in
TQA, the field still lacks a systematic organization and understanding of task
formulations, core challenges, and methodological trends, particularly in light
of emerging research directions such as reinforcement learning. This survey
addresses this gap by providing a comprehensive and structured overview of TQA
research with a focus on LLM-based methods. We provide a comprehensive
categorization of existing benchmarks and task setups. We group current
modeling strategies according to the challenges they target, and analyze their
strengths and limitations. Furthermore, we highlight underexplored but timely
topics that have not been systematically covered in prior research. By unifying
disparate research threads and identifying open problems, our survey offers a
consolidated foundation for the TQA community, enabling a deeper understanding
of the state of the art and guiding future developments in this rapidly
evolving area.

</details>


### [2] [Emotionally Charged, Logically Blurred: AI-driven Emotional Framing Impairs Human Fallacy Detection](https://arxiv.org/abs/2510.09695)
*Yanran Chen,Lynn Greschner,Roman Klinger,Michael Klenk,Steffen Eger*

Main category: cs.CL

TL;DR: 该研究首次计算性地探讨了情感框架如何与逻辑谬误和说服力相互作用，使用大语言模型系统性地改变谬误论证中的情感诉求，发现LLM驱动的情感框架使人类谬误检测的F1分数平均降低14.5%。


<details>
  <summary>Details</summary>
Motivation: 逻辑谬误在公共传播中很常见且可能误导受众，因为说服力本质上是主观的，即使缺乏合理性的谬误论证仍可能显得有说服力。

Method: 使用大语言模型在保持逻辑结构的同时向谬误论证注入情感诉求，对八个LLM进行基准测试，然后使用最佳模型为人类研究生成刺激材料。

Result: LLM驱动的情感框架显著降低了人类对谬误的检测能力，当感知到愉悦情绪时人类表现更好，而恐惧或悲伤情绪与更高的说服力相关。

Conclusion: 这项工作对AI驱动的谬误论证情境下的情感操纵具有重要启示。

Abstract: Logical fallacies are common in public communication and can mislead
audiences; fallacious arguments may still appear convincing despite lacking
soundness, because convincingness is inherently subjective. We present the
first computational study of how emotional framing interacts with fallacies and
convincingness, using large language models (LLMs) to systematically change
emotional appeals in fallacious arguments. We benchmark eight LLMs on injecting
emotional appeal into fallacious arguments while preserving their logical
structures, then use the best models to generate stimuli for a human study. Our
results show that LLM-driven emotional framing reduces human fallacy detection
in F1 by 14.5% on average. Humans perform better in fallacy detection when
perceiving enjoyment than fear or sadness, and these three emotions also
correlate with significantly higher convincingness compared to neutral or other
emotion states. Our work has implications for AI-driven emotional manipulation
in the context of fallacious argumentation.

</details>


### [3] [The Idola Tribus of AI: Large Language Models tend to perceive order where none exists](https://arxiv.org/abs/2510.09709)
*Shin-nosuke Ishikawa,Masato Todo,Taiki Ogihara,Hirotsugu Ohba*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present a tendency of large language models (LLMs) to generate absurd
patterns despite their clear inappropriateness in a simple task of identifying
regularities in number series. Several approaches have been proposed to apply
LLMs to complex real-world tasks, such as providing knowledge through
retrieval-augmented generation and executing multi-step tasks using AI agent
frameworks. However, these approaches rely on the logical consistency and
self-coherence of LLMs, making it crucial to evaluate these aspects and
consider potential countermeasures. To identify cases where LLMs fail to
maintain logical consistency, we conducted an experiment in which LLMs were
asked to explain the patterns in various integer sequences, ranging from
arithmetic sequences to randomly generated integer series. While the models
successfully identified correct patterns in arithmetic and geometric sequences,
they frequently over-recognized patterns that were inconsistent with the given
numbers when analyzing randomly generated series. This issue was observed even
in multi-step reasoning models, including OpenAI o3, o4-mini, and Google Gemini
2.5 Flash Preview Thinking. This tendency to perceive non-existent patterns can
be interpreted as the AI model equivalent of Idola Tribus and highlights
potential limitations in their capability for applied tasks requiring logical
reasoning, even when employing chain-of-thought reasoning mechanisms.

</details>


### [4] [SeCon-RAG: A Two-Stage Semantic Filtering and Conflict-Free Framework for Trustworthy RAG](https://arxiv.org/abs/2510.09710)
*Xiaonan Si,Meilin Zhu,Simeng Qin,Lijia Yu,Lijun Zhang,Shuaitong Liu,Xinfeng Li,Ranjie Duan,Yang Liu,Xiaojun Jia*

Main category: cs.CL

TL;DR: 提出SeCon-RAG框架，通过两阶段语义过滤和冲突消除机制来防御RAG系统中的语料库污染攻击，在保持有用知识的同时提升生成鲁棒性和输出可信度。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统容易受到语料库污染和污染攻击，现有防御方法通常采用激进过滤导致有价值信息丢失和生成可靠性降低。

Method: 两阶段框架：第一阶段使用EIRE指导的语义和聚类联合过滤；第二阶段使用EIRE指导的冲突感知过滤模块，分析查询、候选答案和检索知识之间的语义一致性。

Result: 在多种LLM和数据集上的广泛实验表明，SeCon-RAG显著优于最先进的防御方法。

Conclusion: SeCon-RAG通过两阶段过程有效保留有用知识同时减轻冲突污染，在生成鲁棒性和输出可信度方面实现显著改进。

Abstract: Retrieval-augmented generation (RAG) systems enhance large language models
(LLMs) with external knowledge but are vulnerable to corpus poisoning and
contamination attacks, which can compromise output integrity. Existing defenses
often apply aggressive filtering, leading to unnecessary loss of valuable
information and reduced reliability in generation. To address this problem, we
propose a two-stage semantic filtering and conflict-free framework for
trustworthy RAG. In the first stage, we perform a joint filter with semantic
and cluster-based filtering which is guided by the Entity-intent-relation
extractor (EIRE). EIRE extracts entities, latent objectives, and entity
relations from both the user query and filtered documents, scores their
semantic relevance, and selectively adds valuable documents into the clean
retrieval database. In the second stage, we proposed an EIRE-guided
conflict-aware filtering module, which analyzes semantic consistency between
the query, candidate answers, and retrieved knowledge before final answer
generation, filtering out internal and external contradictions that could
mislead the model. Through this two-stage process, SeCon-RAG effectively
preserves useful knowledge while mitigating conflict contamination, achieving
significant improvements in both generation robustness and output
trustworthiness. Extensive experiments across various LLMs and datasets
demonstrate that the proposed SeCon-RAG markedly outperforms state-of-the-art
defense methods.

</details>


### [5] [ReaLM: Residual Quantization Bridging Knowledge Graph Embeddings and Large Language Models](https://arxiv.org/abs/2510.09711)
*Wenbin Guo,Xin Wang,Jiaoyan Chen,Lingbing Guo,Zhao Li,Zirui Chen*

Main category: cs.CL

TL;DR: ReaLM是一个新颖的框架，通过残差向量量化机制弥合知识图谱嵌入与LLM标记化之间的差距，将预训练的KG嵌入离散化为紧凑代码序列并作为可学习标记集成到LLM词汇表中。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的知识图谱补全方法难以充分利用结构化语义表示，因为预训练KG模型的连续嵌入空间与LLMs的离散标记空间存在根本性错位，这阻碍了有效的语义传递并限制了性能。

Method: 使用残差向量量化将预训练KG嵌入离散化为紧凑代码序列，将其作为可学习标记集成到LLM词汇表中；同时结合本体引导的类约束来强制执行语义一致性，基于类级兼容性细化实体预测。

Result: 在两个广泛使用的基准数据集上的大量实验表明，ReaLM实现了最先进的性能，证实了其在将结构化知识与大规模语言模型对齐方面的有效性。

Conclusion: ReaLM通过弥合KG嵌入与LLM标记化之间的差距，成功实现了结构化知识与大规模语言模型的有效融合，为知识图谱补全提供了新的有效解决方案。

Abstract: Large Language Models (LLMs) have recently emerged as a powerful paradigm for
Knowledge Graph Completion (KGC), offering strong reasoning and generalization
capabilities beyond traditional embedding-based approaches. However, existing
LLM-based methods often struggle to fully exploit structured semantic
representations, as the continuous embedding space of pretrained KG models is
fundamentally misaligned with the discrete token space of LLMs. This
discrepancy hinders effective semantic transfer and limits their performance.
To address this challenge, we propose ReaLM, a novel and effective framework
that bridges the gap between KG embeddings and LLM tokenization through the
mechanism of residual vector quantization. ReaLM discretizes pretrained KG
embeddings into compact code sequences and integrates them as learnable tokens
within the LLM vocabulary, enabling seamless fusion of symbolic and contextual
knowledge. Furthermore, we incorporate ontology-guided class constraints to
enforce semantic consistency, refining entity predictions based on class-level
compatibility. Extensive experiments on two widely used benchmark datasets
demonstrate that ReaLM achieves state-of-the-art performance, confirming its
effectiveness in aligning structured knowledge with large-scale language
models.

</details>


### [6] [All Code, No Thought: Current Language Models Struggle to Reason in Ciphered Language](https://arxiv.org/abs/2510.09714)
*Shiyuan Guo,Henry Sleight,Fabien Roger*

Main category: cs.CL

TL;DR: 论文研究了AI模型在加密推理方面的能力，发现模型在加密文本中推理时准确率显著下降，即使它们能够准确理解加密文本。这表明使用加密推理来逃避思维链监控对当前模型来说可能是无效的策略。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理的广泛采用，检测有害AI行为变得重要。思维链监控是检测对抗性攻击和AI错位的方法，但攻击者和错位模型可能通过加密推理来逃避监控。

Method: 测试了28种不同密码，对最多10个模型进行微调和提示，让它们在密码中进行推理。使用数学问题作为推理能力的代理指标。

Result: 发现不对称性：模型在加密文本中推理时准确率显著下降，即使它们能够准确翻译加密文本。前沿模型在不太知名的密码中表现不佳，但在知名密码如rot13中表现良好。加密推理能力与预训练数据中密码的流行度相关。

Conclusion: 使用加密推理来逃避思维链监控对当前模型可能是无效的策略，并为限制未来前沿模型开发这种能力提供了指导。

Abstract: Detecting harmful AI actions is important as AI agents gain adoption.
Chain-of-thought (CoT) monitoring is one method widely used to detect
adversarial attacks and AI misalignment. However, attackers and misaligned
models might evade CoT monitoring through ciphered reasoning: reasoning hidden
in encrypted, translated, or compressed text. To assess this risk, we test
whether models can perform ciphered reasoning. For each of 28 different
ciphers, we fine-tune and prompt up to 10 models to reason in that cipher. We
measure model accuracy on math problems as a proxy for reasoning ability.
Across the models we test, we find an asymmetry: model accuracy can drop
significantly when reasoning in ciphered text, even though models demonstrate
comprehension of ciphered text by being able to translate it accurately to
English. Even frontier models struggle with lesser-known ciphers, although they
can reason accurately in well-known ciphers like rot13. We show that ciphered
reasoning capability correlates with cipher prevalence in pretraining data. We
also identify scaling laws showing that ciphered reasoning capability improves
slowly with additional fine-tuning data. Our work suggests that evading CoT
monitoring using ciphered reasoning may be an ineffective tactic for current
models and offers guidance on constraining the development of this capability
in future frontier models.

</details>


### [7] [Preference-Aware Memory Update for Long-Term LLM Agents](https://arxiv.org/abs/2510.09720)
*Haoran Sun,Zekun Zhang,Shaoning Zeng*

Main category: cs.CL

TL;DR: 提出了偏好感知记忆更新机制(PAMU)，通过滑动窗口平均和指数移动平均的融合来动态优化LLM代理的长期记忆表示，在五个任务场景中显著提升了输出质量。


<details>
  <summary>Details</summary>
Motivation: 现有长期记忆方法在存储和检索方面有改进，但缺乏动态更新机制来响应用户行为和上下文的变化，特别是偏好记忆表示的精炼能力不足。

Method: PAMU机制结合滑动窗口平均(SW)和指数移动平均(EMA)，构建融合的偏好感知表示，捕捉短期波动和长期用户趋势。

Result: 在LoCoMo数据集的五个任务场景实验中，该机制显著提高了五个基线模型在长期对话中的输出质量。

Conclusion: PAMU机制有效解决了长期对话中记忆动态更新的问题，验证了其在提升LLM推理能力方面的有效性。

Abstract: One of the key factors influencing the reasoning capabilities of LLM-based
agents is their ability to leverage long-term memory. Integrating long-term
memory mechanisms allows agents to make informed decisions grounded in
historical interactions. While recent advances have significantly improved the
storage and retrieval components, by encoding memory into dense vectors for
similarity search or organizing memory as structured knowledge graphs most
existing approaches fall short in memory updating. In particular, they lack
mechanisms for dynamically refining preference memory representations in
response to evolving user behaviors and contexts. To address this gap, we
propose a Preference-Aware Memory Update Mechanism (PAMU) that enables dynamic
and personalized memory refinement. By integrating sliding window averages (SW)
with exponential moving averages (EMA), PAMU constructs a fused
preference-aware representation that captures both short-term fluctuations and
long-term user tendencies. We conduct experiments on five task scenarios of the
LoCoMo dataset, and the results show that our mechanism can significantly
improve the output quality of LLM in five baselines, validating its
effectiveness in long-term conversations.

</details>


### [8] [Layout-Aware Parsing Meets Efficient LLMs: A Unified, Scalable Framework for Resume Information Extraction and Evaluation](https://arxiv.org/abs/2510.09722)
*Fanwei Zhu,Jinke Yu,Zulong Chen,Ying Zhou,Junhao Ji,Zhibo Yang,Yuxue Zhang,Haoyuan Hu,Zhenghao Liu*

Main category: cs.CL

TL;DR: 提出了一种面向简历信息自动提取的布局感知和效率优化框架，解决了布局异构性、LLM高成本和缺乏标准化数据集三大挑战。


<details>
  <summary>Details</summary>
Motivation: 自动化简历信息提取对规模化人才招聘至关重要，但在实际部署中面临三大挑战：简历布局和内容的极端异构性、大型语言模型的高成本和延迟、缺乏标准化数据集和评估工具。

Method: 结合了微调的布局解析器来规范化不同文档格式、基于并行提示和指令调优的推理高效LLM提取器，以及由新基准数据集支持的鲁棒两阶段自动评估框架。

Result: 广泛实验表明该框架在准确性和效率上显著优于强基线方法。特别是，微调的0.6B紧凑LLM实现了顶级准确性，同时显著降低了推理延迟和计算成本。

Conclusion: 该系统已在阿里巴巴智能HR平台全面部署，支持其各业务部门的实时应用，为简历信息提取提供了高效实用的解决方案。

Abstract: Automated resume information extraction is critical for scaling talent
acquisition, yet its real-world deployment faces three major challenges: the
extreme heterogeneity of resume layouts and content, the high cost and latency
of large language models (LLMs), and the lack of standardized datasets and
evaluation tools. In this work, we present a layout-aware and
efficiency-optimized framework for automated extraction and evaluation that
addresses all three challenges. Our system combines a fine-tuned layout parser
to normalize diverse document formats, an inference-efficient LLM extractor
based on parallel prompting and instruction tuning, and a robust two-stage
automated evaluation framework supported by new benchmark datasets. Extensive
experiments show that our framework significantly outperforms strong baselines
in both accuracy and efficiency. In particular, we demonstrate that a
fine-tuned compact 0.6B LLM achieves top-tier accuracy while significantly
reducing inference latency and computational cost. The system is fully deployed
in Alibaba's intelligent HR platform, supporting real-time applications across
its business units.

</details>


### [9] [VisRAG 2.0: Evidence-Guided Multi-Image Reasoning in Visual Retrieval-Augmented Generation](https://arxiv.org/abs/2510.09733)
*Yubo Sun,Chunyi Peng,Yukun Yan,Shi Yu,Zhenghao Liu,Chi Chen,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: EVisRAG是一个端到端的视觉检索增强生成框架，通过证据引导的多图像推理来减少幻觉并增强推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前的VRAG系统在多图像感知和证据整合方面不可靠，导致推理基础薄弱和错误结论。

Method: 提出EVisRAG框架，首先观察检索到的图像并记录每张图像的证据，然后从聚合证据中得出最终答案。使用RS-GRPO训练方法，将细粒度奖励与范围特定token绑定，联合优化视觉感知和推理能力。

Result: 在多个视觉问答基准测试中，EVisRAG相比基础VLM平均提升了27%的性能。

Conclusion: EVisRAG通过精确感知和定位问题相关证据，并基于证据推导最终答案，显著提升了视觉问答的准确性。

Abstract: Visual retrieval-augmented generation (VRAG) augments vision-language models
(VLMs) with external visual knowledge to ground reasoning and reduce
hallucinations. Yet current VRAG systems often fail to reliably perceive and
integrate evidence across multiple images, leading to weak grounding and
erroneous conclusions. In this paper, we propose EVisRAG, an end-to-end
framework that learns to reason with evidence-guided multi-image to address
this issue. The model first observes retrieved images and records per-image
evidence, then derives the final answer from the aggregated evidence. To train
EVisRAG effectively, we introduce Reward-Scoped Group Relative Policy
Optimization (RS-GRPO), which binds fine-grained rewards to scope-specific
tokens to jointly optimize visual perception and reasoning abilities of VLMs.
Experimental results on multiple visual question answering benchmarks
demonstrate that EVisRAG delivers substantial end-to-end gains over backbone
VLM with 27\% improvements on average. Further analysis shows that, powered by
RS-GRPO, EVisRAG improves answer accuracy by precisely perceiving and
localizing question-relevant evidence across multiple images and deriving the
final answer from that evidence, much like a real detective.

</details>


### [10] [Judge's Verdict: A Comprehensive Analysis of LLM Judge Capability Through Human Agreement](https://arxiv.org/abs/2510.09738)
*Steve Han,Gilberto Titericz Junior,Tom Balough,Wenfei Zhou*

Main category: cs.CL

TL;DR: 提出了Judge's Verdict Benchmark，一种评估LLM作为评判者的两阶段方法，测试54个LLM在RAG和Agentic任务中复制人类判断的能力，发现27个模型达到Tier 1性能。


<details>
  <summary>Details</summary>
Motivation: 传统相关性分析不足以全面评估LLM作为评判者的能力，需要更全面的方法来衡量LLM是否能复制人类判断的细微差别。

Method: 两阶段方法：首先进行相关性测试筛选强对齐的评判者，然后使用z分数进行人类相似性测试，识别人类式判断和超一致判断两种模式。

Result: 54个LLM中27个达到Tier 1性能：23个模型表现出人类式判断模式，4个模型表现出超一致行为。评判者优秀程度不依赖模型大小，而依赖特定训练策略。

Conclusion: 相关性分析不足以评估评判者，需要基于一致性模式的"评判者图灵测试"，并为不同评估需求提供标准化的LLM评判者分类基准。

Abstract: This research introduces the Judge's Verdict Benchmark, a novel two-step
methodology to evaluate Large Language Models (LLMs) as judges for response
accuracy evaluation tasks. We assess how well 54 LLMs can replicate human
judgment when scoring responses from RAG (Retrieval-Augmented Generation) or
Agentic pipelines against ground truth answers. Our methodology progresses from
traditional correlation analysis to comprehensive Cohen's Kappa analysis that
measures actual agreement patterns. The two-step approach includes: (1) a
correlation test that filters judges with strong alignment, followed by (2) a
human-likeness test using z-scores to identify two distinct judgment patterns:
human-like judgment (|z| < 1) that mimics natural human variation, and
super-consistent judgment (z > 1) that exceeds typical human-to-human agreement
levels. This methodology reveals that 27 out of 54 tested LLMs achieve Tier 1
performance: 23 models exhibit human-like patterns that preserve the nuances of
human judgment, while 4 models demonstrate super-consistent behavior, a pattern
that could indicate either enhanced reliability or oversimplification of
complex judgments. Testing 43 open-source models (1B-405B parameters) and 11
closed models (GPT, Gemini, Claude variants), we demonstrate that judge
excellence is not solely dependent on model size but on specific training
strategies. Our key contributions include: (1) establishing that correlation
alone is insufficient for judge evaluation, (2) introducing a "Turing Test for
judges" based on agreement patterns, and (3) providing a standardized benchmark
for classifying LLM judges into distinct performance tiers for different
evaluation needs.

</details>


### [11] [Gold Panning: Turning Positional Bias into Signal for Multi-Document LLM Reasoning](https://arxiv.org/abs/2510.09770)
*Adam Byerly,Daniel Khashabi*

Main category: cs.CL

TL;DR: 提出Gold Panning Bandits框架，利用LLM的位置偏见作为诊断信号，通过重新排列文档来识别最相关内容，相比随机排列基线减少65%的查询次数


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多文档环境中存在强烈的位置偏见，现有方法将其视为需要减轻的噪声，但本文认为这种偏见可以作为诊断信号来利用

Method: 将文档重新排序问题建模为二分图匹配问题，提出贪心O(N log N)策略，将最不确定的文档放在最具信息量的位置

Result: 在知识密集型NLP任务中，相比随机排列基线，该方法使用最多65%更少的语言模型查询来识别相关文档

Conclusion: LLM的内在偏见可以从负担转变为资产，实现高效的推理时优化，无需模型重新训练

Abstract: Large language models exhibit a strong position bias in multi-document
contexts, systematically prioritizing information based on location rather than
relevance. While existing approaches treat this bias as noise to be mitigated,
we introduce Gold Panning Bandits, a framework that leverages position bias as
a diagnostic signal: by reordering documents and observing shifts in the
model's responses, we can efficiently identify the most relevant content. We
frame the problem of choosing reorderings as a bipartite matching problem.
While an optimal assignment can be computed at each iteration with the
Hungarian algorithm in $O(N^3)$ time, we propose a greedy $O(N \log N)$
strategy that achieves comparable performance by prioritizing the placement of
the most uncertain documents in the most informative positions. Our approach
identifies relevant documents using up to 65\% fewer language model queries
than random permutation baselines on knowledge-intensive NLP tasks,
substantially reducing computational cost without model retraining. This work
demonstrates that inherent LLM biases can be transformed from liabilities into
assets for efficient, inference-time optimization.

</details>


### [12] [PromptGuard at BLP-2025 Task 1: A Few-Shot Classification Framework Using Majority Voting and Keyword Similarity for Bengali Hate Speech Detection](https://arxiv.org/abs/2510.09771)
*Rakib Hossan,Shubhashis Roy Dipta*

Main category: cs.CL

TL;DR: PromptGuard是一个用于孟加拉语仇恨言论分类的少样本框架，结合卡方统计分析和自适应多数投票机制，在BLP-2025 Task 1A任务中取得了67.61的微F1分数。


<details>
  <summary>Details</summary>
Motivation: 传统监督方法需要大量标注数据，对于低资源语言成本高昂，因此需要开发少样本学习方法。

Method: 使用卡方统计分析进行关键词提取，结合自适应多数投票机制进行决策，比较统计关键词选择与随机方法的差异。

Result: PromptGuard的微F1分数为67.61，优于n-gram基线（60.75）和随机方法（14.65），卡方关键词在所有类别中都表现出最一致的影响。

Conclusion: 卡方关键词提取提供了跨类别的持续改进，自适应投票机制在处理需要扩展分类轮次的模糊案例时特别有效。

Abstract: The BLP-2025 Task 1A requires Bengali hate speech classification into six
categories. Traditional supervised approaches need extensive labeled datasets
that are expensive for low-resource languages. We developed PromptGuard, a
few-shot framework combining chi-square statistical analysis for keyword
extraction with adaptive majority voting for decision-making. We explore
statistical keyword selection versus random approaches and adaptive voting
mechanisms that extend classification based on consensus quality. Chi-square
keywords provide consistent improvements across categories, while adaptive
voting benefits ambiguous cases requiring extended classification rounds.
PromptGuard achieves a micro-F1 of 67.61, outperforming n-gram baselines
(60.75) and random approaches (14.65). Ablation studies confirm
chi-square-based keywords show the most consistent impact across all
categories.

</details>


### [13] [Steering Embedding Models with Geometric Rotation: Mapping Semantic Relationships Across Languages and Models](https://arxiv.org/abs/2510.09790)
*Michael Freenor,Lauren Alvarez*

Main category: cs.CL

TL;DR: RISE方法通过旋转操作在嵌入空间中表示语义转换，在多语言和模型间具有良好迁移性，验证了句子层面的线性表示假设。


<details>
  <summary>Details</summary>
Motivation: 理解语言和嵌入模型如何编码语义关系对模型可解释性和控制至关重要，但现代高维文本表示缺乏直观的几何特性。

Method: 提出Rotor-Invariant Shift Estimation (RISE)方法，将语义转换表示为嵌入空间中的一致旋转操作，利用现代语言表示的流形结构。

Result: 在3个嵌入模型、3个数据集和7种形态多样的语言上评估，RISE能一致地映射具有不同语法特征的语篇级语义转换。

Conclusion: 首次系统证明语篇级语义转换对应于多语言嵌入空间中的一致几何操作，为句子层面的线性表示假设提供了实证支持。

Abstract: Understanding how language and embedding models encode semantic relationships
is fundamental to model interpretability and control. While early word
embeddings exhibited intuitive vector arithmetic (''king'' - ''man'' +
''woman'' = ''queen''), modern high-dimensional text representations lack
straightforward interpretable geometric properties. We introduce
Rotor-Invariant Shift Estimation (RISE), a geometric approach that represents
semantic transformations as consistent rotational operations in embedding
space, leveraging the manifold structure of modern language representations.
RISE operations have the ability to operate across both languages and models
with high transfer of performance, suggesting the existence of analogous
cross-lingual geometric structure. We evaluate RISE across three embedding
models, three datasets, and seven morphologically diverse languages in five
major language groups. Our results demonstrate that RISE consistently maps
discourse-level semantic transformations with distinct grammatical features
(e.g., negation and conditionality) across languages and models. This work
provides the first systematic demonstration that discourse-level semantic
transformations correspond to consistent geometric operations in multilingual
embedding spaces, empirically supporting the Linear Representation Hypothesis
at the sentence level.

</details>


### [14] [Text Prompt Injection of Vision Language Models](https://arxiv.org/abs/2510.09849)
*Ruizhe Zhu*

Main category: cs.CL

TL;DR: 本文研究了一种针对大型视觉语言模型的文本提示注入攻击方法，该方法简单有效且计算资源需求低。


<details>
  <summary>Details</summary>
Motivation: 随着大型视觉语言模型的广泛应用，其安全性问题日益突出。本文旨在研究文本提示注入这种简单但有效的误导模型的方法。

Method: 开发了一种文本提示注入攻击算法，通过实验验证了其有效性和效率。

Result: 实验证明该方法对大型模型特别有效，且相比其他攻击方法对计算资源要求较低。

Conclusion: 文本提示注入是一种简单高效的攻击方法，能够有效误导大型视觉语言模型，且不需要大量计算资源。

Abstract: The widespread application of large vision language models has significantly
raised safety concerns. In this project, we investigate text prompt injection,
a simple yet effective method to mislead these models. We developed an
algorithm for this type of attack and demonstrated its effectiveness and
efficiency through experiments. Compared to other attack methods, our approach
is particularly effective for large models without high demand for
computational resources.

</details>


### [15] [NG-Router: Graph-Supervised Multi-Agent Collaboration for Nutrition Question Answering](https://arxiv.org/abs/2510.09854)
*Kaiwen Shi,Zheyuan Zhang,Zhengqing Yuan,Keerthiram Murugesan,Vincent Galass,Chuxu Zhang,Yanfang Ye*

Main category: cs.CL

TL;DR: NG-Router是一个基于知识图谱的多智能体协作框架，用于营养问答任务，通过图神经网络学习任务感知的路由分布，并使用梯度子图检索解决上下文过载问题。


<details>
  <summary>Details</summary>
Motivation: 现有营养问答方法面临单智能体推理能力有限和多智能体架构设计复杂的问题，以及上下文过载阻碍准确决策的挑战。

Method: 将营养问答建模为监督式、知识图谱引导的多智能体协作问题，将智能体节点集成到异构知识图中，使用图神经网络学习任务感知路由分布，并提出基于梯度的子图检索机制识别关键证据。

Result: 在多个基准测试和骨干模型上的广泛实验表明，NG-Router始终优于单智能体和集成基线方法。

Conclusion: NG-Router为复杂营养健康任务提供了一种基于领域的多智能体推理原则性方法。

Abstract: Diet plays a central role in human health, and Nutrition Question Answering
(QA) offers a promising path toward personalized dietary guidance and the
prevention of diet-related chronic diseases. However, existing methods face two
fundamental challenges: the limited reasoning capacity of single-agent systems
and the complexity of designing effective multi-agent architectures, as well as
contextual overload that hinders accurate decision-making. We introduce
Nutritional-Graph Router (NG-Router), a novel framework that formulates
nutritional QA as a supervised, knowledge-graph-guided multi-agent
collaboration problem. NG-Router integrates agent nodes into heterogeneous
knowledge graphs and employs a graph neural network to learn task-aware routing
distributions over agents, leveraging soft supervision derived from empirical
agent performance. To further address contextual overload, we propose a
gradient-based subgraph retrieval mechanism that identifies salient evidence
during training, thereby enhancing multi-hop and relational reasoning.
Extensive experiments across multiple benchmarks and backbone models
demonstrate that NG-Router consistently outperforms both single-agent and
ensemble baselines, offering a principled approach to domain-aware multi-agent
reasoning for complex nutritional health tasks.

</details>


### [16] [NarraBench: A Comprehensive Framework for Narrative Benchmarking](https://arxiv.org/abs/2510.09869)
*Sil Hamilton,Matthew Wilkens,Andrew Piper*

Main category: cs.CL

TL;DR: NarraBench提出了一个理论驱动的叙事理解任务分类法，并调查了该领域的78个现有基准，发现现有评估仅覆盖27%的叙事任务，许多重要方面如叙事事件、风格、视角和启示等几乎缺失。


<details>
  <summary>Details</summary>
Motivation: 当前叙事理解评估存在显著不足，许多重要叙事方面被忽视或与现有指标不匹配，需要开发能够评估构成性主观和视角性方面的基准。

Method: 开发了理论指导的叙事理解任务分类法，并对78个现有基准进行了系统性调查和分析。

Result: 发现仅有27%的叙事任务被现有基准充分覆盖，叙事事件、风格、视角和启示等关键方面几乎完全缺失，需要开发更多能够处理主观性任务的基准。

Conclusion: 该分类法、调查和方法论对测试LLM叙事理解的NLP研究人员具有重要价值，强调了开发更全面叙事评估基准的迫切需求。

Abstract: We present NarraBench, a theory-informed taxonomy of narrative-understanding
tasks, as well as an associated survey of 78 existing benchmarks in the area.
We find significant need for new evaluations covering aspects of narrative
understanding that are either overlooked in current work or are poorly aligned
with existing metrics. Specifically, we estimate that only 27% of narrative
tasks are well captured by existing benchmarks, and we note that some areas --
including narrative events, style, perspective, and revelation -- are nearly
absent from current evaluations. We also note the need for increased
development of benchmarks capable of assessing constitutively subjective and
perspectival aspects of narrative, that is, aspects for which there is
generally no single correct answer. Our taxonomy, survey, and methodology are
of value to NLP researchers seeking to test LLM narrative understanding.

</details>


### [17] [CoBia: Constructed Conversations Can Trigger Otherwise Concealed Societal Biases in LLMs](https://arxiv.org/abs/2510.09871)
*Nafiseh Nikeghbal,Amir Hossein Kargaran,Jana Diesner*

Main category: cs.CL

TL;DR: CoBia是一种轻量级对抗攻击套件，通过构建包含偏见声明的对话来测试LLMs是否能从虚构的偏见中恢复并拒绝偏见性后续问题。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs通过了标准安全检查，但在对话中仍可能表现出有害行为（如种族主义观点），需要系统性地分析这些偏见泄露的条件。

Method: 构建包含偏见声明的对话，评估模型是否能从虚构偏见中恢复并拒绝偏见性后续问题，涉及6个社会人口类别。

Result: 有目的构建的对话能可靠地揭示偏见放大，LLMs在对话中经常无法拒绝偏见性后续问题。

Conclusion: 这种压力测试突显了通过互动可以揭示的深层嵌入偏见，强调需要改进模型的安全性和公平性。

Abstract: Improvements in model construction, including fortified safety guardrails,
allow Large language models (LLMs) to increasingly pass standard safety checks.
However, LLMs sometimes slip into revealing harmful behavior, such as
expressing racist viewpoints, during conversations. To analyze this
systematically, we introduce CoBia, a suite of lightweight adversarial attacks
that allow us to refine the scope of conditions under which LLMs depart from
normative or ethical behavior in conversations. CoBia creates a constructed
conversation where the model utters a biased claim about a social group. We
then evaluate whether the model can recover from the fabricated bias claim and
reject biased follow-up questions. We evaluate 11 open-source as well as
proprietary LLMs for their outputs related to six socio-demographic categories
that are relevant to individual safety and fair treatment, i.e., gender, race,
religion, nationality, sex orientation, and others. Our evaluation is based on
established LLM-based bias metrics, and we compare the results against human
judgments to scope out the LLMs' reliability and alignment. The results suggest
that purposefully constructed conversations reliably reveal bias amplification
and that LLMs often fail to reject biased follow-up questions during dialogue.
This form of stress-testing highlights deeply embedded biases that can be
surfaced through interaction. Code and artifacts are available at
https://github.com/nafisenik/CoBia.

</details>


### [18] [iBERT: Interpretable Style Embeddings via Sense Decomposition](https://arxiv.org/abs/2510.09882)
*Vishal Anand,Milad Alshomary,Kathleen McKeown*

Main category: cs.CL

TL;DR: iBERT是一种可解释的BERT编码器，通过稀疏非负混合的上下文无关意义向量来表示输入，实现可解释和可控的嵌入表示。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统BERT模型缺乏可解释性和可控性的问题，设计能够模块化展示语言中判别性线索（如风格和语义结构）的嵌入表示。

Method: 使用k个上下文无关的意义向量，将每个输入token表示为这些向量的稀疏非负混合，可以在句子层面或token层面使用，实现模块化控制。

Result: 在STEL基准测试中，风格表示效果比SBERT基线提高约8个百分点，同时在作者身份验证上保持竞争力，能够将特定风格属性（如表情符号使用、正式性、拼写错误）分配给特定意义向量。

Conclusion: iBERT不仅限于风格建模，其结构模块化设计能够可解释地分解数据中的任何判别性信号，即使在监督混合风格和语义因素时也能实现泛化。

Abstract: We present iBERT (interpretable-BERT), an encoder to produce inherently
interpretable and controllable embeddings - designed to modularize and expose
the discriminative cues present in language, such as stylistic and semantic
structure. Each input token is represented as a sparse, non-negative mixture
over k context-independent sense vectors, which can be pooled into sentence
embeddings or used directly at the token level. This enables modular control
over representation, before any decoding or downstream use.
  To demonstrate our model's interpretability, we evaluate it on a suite of
style-focused tasks. On the STEL benchmark, it improves style representation
effectiveness by ~8 points over SBERT-style baselines, while maintaining
competitive performance on authorship verification. Because each embedding is a
structured composition of interpretable senses, we highlight how specific style
attributes - such as emoji use, formality, or misspelling can be assigned to
specific sense vectors. While our experiments center on style, iBERT is not
limited to stylistic modeling. Its structural modularity is designed to
interpretably decompose whichever discriminative signals are present in the
data - enabling generalization even when supervision blends stylistic and
semantic factors.

</details>


### [19] [DELTA: Dynamic Layer-Aware Token Attention for Efficient Long-Context Reasoning](https://arxiv.org/abs/2510.09883)
*Hossein Entezari Zarch,Lei Gao,Chaoyi Jiang,Murali Annavarm*

Main category: cs.CL

TL;DR: DELTA是一种无需训练的稀疏注意力机制，通过将Transformer层分为三组（全注意力层、选择层、稀疏注意力层），在保持推理准确性的同时显著减少计算量，实现5倍注意力token减少和1.5倍端到端加速。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在解码过程中需要关注整个增长序列，导致高昂的计算成本。现有稀疏注意力方法虽然减少计算，但在推理任务中因累积选择错误和token重要性动态变化而出现严重准确性下降。

Method: 将Transformer层分为三组：初始全注意力层、少量选择层（通过聚合头级注意力分数识别关键token）、后续稀疏注意力层（仅关注选定子集）。保留完整KV缓存以确保准确性，同时避免多层的昂贵全注意力计算。

Result: 在AIME和GPQA-Diamond等推理基准测试中，DELTA在准确性上匹配或超越全注意力机制，同时将注意力token数量减少高达5倍，实现1.5倍端到端加速。

Conclusion: 选择性重用中间注意力图为高效长上下文推理提供了稳健路径，表明可以在不牺牲准确性的情况下显著提升推理效率。

Abstract: Large reasoning models (LRMs) achieve state-of-the-art performance on
challenging benchmarks by generating long chains of intermediate steps, but
their inference cost is dominated by decoding, where each new token must attend
to the entire growing sequence. Existing sparse attention methods reduce
computation by pruning the key-value (KV) cache, yet they suffer from severe
accuracy degradation on reasoning tasks due to cumulative selection errors and
the dynamic importance of tokens over long derivations. We present
\textbf{DELTA}, a training-free sparse attention mechanism that achieves
computational efficiency without sacrificing model accuracy. DELTA partitions
transformer layers into three groups: initial layers that use full attention, a
small set of \emph{selection layers} that identify salient tokens via
aggregated head-level attention scores, and subsequent \emph{sparse-attention
layers} that attend only to the selected subset. This design preserves the full
KV cache in GPU memory for accuracy, while avoiding expensive full-attention
computation over many layers. On reasoning benchmarks such as AIME and
GPQA-Diamond, DELTA matches or surpasses full attention in accuracy, while
reducing the number of attended tokens by up to $5\times$ and delivering
$1.5\times$ end-to-end speedup. Our results show that selective reuse of
intermediate attention maps offers a robust path toward efficient long-context
reasoning.

</details>


### [20] [Closing the Data-Efficiency Gap Between Autoregressive and Masked Diffusion LLMs](https://arxiv.org/abs/2510.09885)
*Xu Pan,Ely Hahami,Jingxuan Fan,Ziqian Xie,Haim Sompolinsky*

Main category: cs.CL

TL;DR: 本文比较了自回归大语言模型(arLLMs)和掩码扩散大语言模型(dLLMs)在知识注入方面的表现，发现dLLMs在微调时具有更好的数据效率和不受"反转诅咒"影响的特点，并基于此提出了一种新的掩码微调方法来提升arLLMs的数据效率。


<details>
  <summary>Details</summary>
Motivation: 自回归大语言模型(arLLMs)在知识注入方面存在"反转诅咒"等问题，而掩码扩散大语言模型(dLLMs)在预训练阶段显示出更好的数据效率和不受反转诅咒影响的特点。本文旨在研究这些优势是否在微调阶段仍然存在。

Method: 在三个不同数据集上对arLLMs和dLLMs进行微调，使用正向和反向问答来评估知识泛化和反转诅咒。同时提出了一种新的掩码微调范式来改进arLLMs的知识注入效率。

Result: arLLMs严重依赖通过改写进行数据增强来实现问答泛化，且改写仅在信息顺序与问答风格匹配时有效。dLLMs无需改写就能在正向和反向问答上获得高准确率，添加改写仅带来边际收益。提出的掩码微调方法显著提高了arLLMs微调的数据效率。

Conclusion: dLLMs在知识注入方面具有显著优势，不受反转诅咒影响且数据效率更高。基于dLLMs的启发提出的掩码微调方法能有效提升arLLMs的性能，缩小与dLLMs的差距。

Abstract: Despite autoregressive large language models (arLLMs) being the current
dominant paradigm in language modeling, they resist knowledge injection via
fine-tuning due to inherent shortcomings such as the "reversal curse" -- the
challenge of answering questions that reverse the original information order in
the training sample. Masked diffusion large language models (dLLMs) are rapidly
emerging as a powerful alternative to the arLLM paradigm, with evidence of
better data efficiency and free of the "reversal curse" in pre-training.
However, it is unknown whether these advantages extend to the post-training
phase, i.e. whether pre-trained dLLMs can easily acquire new knowledge through
fine-tuning. On three diverse datasets, we fine-tune arLLMs and dLLMs,
evaluating them with forward and backward style Question Answering (QA) to
probe knowledge generalization and the reversal curse. Our results confirm that
arLLMs critically rely on extensive data augmentation via paraphrases for QA
generalization, and paraphrases are only effective when their information order
matches the QA style. Conversely, dLLMs achieve high accuracies on both forward
and backward QAs without paraphrases; adding paraphrases yields only marginal
gains. Lastly, inspired by the dLLM's performance, we introduce a novel masked
fine-tuning paradigm for knowledge injection into pre-trained arLLMs. This
proposed method successfully and drastically improves the data efficiency of
arLLM fine-tuning, effectively closing the performance gap with dLLMs.

</details>


### [21] [Abductive Preference Learning](https://arxiv.org/abs/2510.09887)
*Yijin Ni,Peng Qi*

Main category: cs.CL

TL;DR: 该论文提出了一种反事实偏好学习方法，通过反转传统条件化来学习给定回答下的提示偏好，解决了大语言模型过度自信和忽略反事实提示的问题。


<details>
  <summary>Details</summary>
Motivation: 前沿大语言模型如GPT-5和Claude Sonnet在RLHF和DPO对齐后仍存在过度自信问题，无法正确处理反事实提示，这源于现有偏好学习只关注给定提示下的正确回答选择，而忽略了应该改变回答的反事实提示。

Method: 提出了反事实偏好学习方法，通过反转传统条件化来学习给定回答下的提示偏好。构建了包含1,001个条目的反事实数据集，实现了反事实DPO及其变体DPOP。

Result: 实验显示多任务DPOP在反事实数据集上将回答选择准确率从90.0%提升到99.5%，提示辨别准确率从54.7%提升到85.0%。在AlpacaEval评估中，胜率从5.26%提升到6.17%。

Conclusion: 反事实偏好学习在保持传统偏好优化优势的同时，解决了被忽视的反事实提示挑战，提高了模型对提示差异的敏感性。

Abstract: Frontier large language models such as GPT-5 and Claude Sonnet remain prone
to overconfidence even after alignment through Reinforcement Learning with
Human Feedback (RLHF) and Direct Preference Optimization (DPO). For instance,
they tend to offer the same conservative answer "No" to both questions "Can I
eat the [food / potato chips] that has been left out overnight?" despite the
latter requiring no refridgeration for safe consumption. We find that this
failure is potentially attributed to a limitation of existing preference
learning: it emphasizes selecting the correct response for a given prompt,
while neglecting counterfactual prompts that should alter the response.
  To address this limitation, we propose abductive preference learning, a
fine-tuning paradigm that reverses the conventional conditioning by learning
preferences over prompts given a response. To validate this idea, we construct
an abductive dataset derived from the HaluEval QA benchmark with 1,001 entries,
implementing abductive DPO and its variant DPOP. Experiments reveal
complementary strengths: standard methods improve response selection, abductive
methods improve prompt discrimination, while a multitask objective unifies
both. On the abductive dataset, multitask DPOP boosts accuracy from $90.0\%$ to
$99.5\%$ in response selection and $54.7\%$ to $85.0\%$ in prompt
discrimination, with qualitative evidence highlighting improved sensitivity to
prompt differences. Finally, evaluation on AlpacaEval shows multitask DPOP
improves win rate (from $5.26\%$ to $6.17\%$), confirming that abductive
preference learning preserves the benefits of conventional preference
optimization while addressing the overlooked challenge of counterfactual
prompts.

</details>


### [22] [HIPPD: Brain-Inspired Hierarchical Information Processing for Personality Detection](https://arxiv.org/abs/2510.09893)
*Guanming Chen,Lingzhi Shen,Xiaohao Cai,Imran Razzak,Shoaib Jameel*

Main category: cs.CL

TL;DR: HIPPD是一个受大脑启发的人格检测框架，通过模拟人脑的层次信息处理机制，结合大语言模型、动态记忆模块和专门化轻量模型，在Kaggle和Pandora数据集上超越了现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法难以捕捉跨多个帖子的上下文信息，在语义稀疏环境中提取代表性和鲁棒性特征方面表现不足。

Method: 使用大语言模型模拟大脑皮层进行全局语义推理和深度特征抽象；动态记忆模块模拟前额叶皮层进行自适应门控和关键特征选择性保留；专门化轻量模型模拟基底神经节，通过严格赢家通吃机制动态路由。

Result: 在Kaggle和Pandora数据集上的广泛实验表明，HIPPD始终优于最先进的基线方法。

Conclusion: 受大脑启发的层次信息处理框架能够有效提升人格检测性能，证明了神经科学原理在自然语言处理任务中的应用价值。

Abstract: Personality detection from text aims to infer an individual's personality
traits based on linguistic patterns. However, existing machine learning
approaches often struggle to capture contextual information spanning multiple
posts and tend to fall short in extracting representative and robust features
in semantically sparse environments. This paper presents HIPPD, a
brain-inspired framework for personality detection that emulates the
hierarchical information processing of the human brain. HIPPD utilises a large
language model to simulate the cerebral cortex, enabling global semantic
reasoning and deep feature abstraction. A dynamic memory module, modelled after
the prefrontal cortex, performs adaptive gating and selective retention of
critical features, with all adjustments driven by dopaminergic prediction error
feedback. Subsequently, a set of specialised lightweight models, emulating the
basal ganglia, are dynamically routed via a strict winner-takes-all mechanism
to capture the personality-related patterns they are most proficient at
recognising. Extensive experiments on the Kaggle and Pandora datasets
demonstrate that HIPPD consistently outperforms state-of-the-art baselines.

</details>


### [23] [Don't Throw Away Your Pretrained Model](https://arxiv.org/abs/2510.09913)
*Shangbin Feng,Wenhao Yu,Yike Wang,Hongming Zhang,Yulia Tsvetkov,Dong Yu*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Alignment training has tradeoffs: it helps language models (LMs) gain in
reasoning and instruction following but might lose out on skills such as
creativity and calibration, where unaligned base models are better at. We aim
to make the best of both worlds through model collaboration, where different
models in the training pipeline collaborate and complement each other. Since LM
responses feature interleaving skills that favor different models, we propose
Switch Generation, where pretrained and aligned model versions take turns to
``speak'' in a response sequence. Specifically, we train a switcher LM by
learning from outcomes of choosing different models to generate the next
segment across diverse queries and contexts. At inference time, the switcher LM
guides different model checkpoints to dynamically generate the next segment
where their strengths are most needed. Extensive experiments with 8 model
collaboration baselines and 18 datasets show that 1) model collaboration
consistently outperforms individual models on 16 out of 18 tasks, and 2) Switch
Generation further outperforms baselines by 12.9% on average. Further analysis
reveals that Switch Generation discovers compositional skills to solve problems
where individual models struggle and generalizes to unseen models and tasks,
reusing and repurposing by-products in expensive model training pipelines that
are otherwise discarded.

</details>


### [24] [Enhancing Faithfulness in Abstractive Summarization via Span-Level Fine-Tuning](https://arxiv.org/abs/2510.09915)
*Sicong Huang,Qianqi Yan,Shengze Wang,Ian Lane*

Main category: cs.CL

TL;DR: 本文研究通过微调策略减少LLM生成摘要中的不忠实内容，提出了包含忠实和不忠实摘要的新数据集，并评估了三种微调方法：梯度上升、非似然训练和任务向量否定。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型能生成流畅的摘要，但经常产生不忠实的内容，现有缓解策略无法完全解决LLM生成摘要中的多样化错误。

Method: 使用多种LLM自动生成训练集源文档的摘要，用GPT-4o标注span级别的幻觉，然后使用无幻觉摘要和标注的不忠实span来微调LLM。

Result: 实验结果显示三种方法都能成功利用span级别标注提高忠实性，其中非似然训练效果最好。

Conclusion: span级别的标注可以有效提升LLM生成摘要的忠实性，非似然训练是最有效的微调方法。

Abstract: Abstractive summarization using large language models (LLMs) has become an
essential tool for condensing information. However, despite their ability to
generate fluent summaries, these models sometimes produce unfaithful summaries,
introducing hallucinations at the word, phrase, or concept level. Existing
mitigation strategies, such as post-processing corrections or contrastive
learning with synthetically generated negative samples, fail to fully address
the diverse errors that can occur in LLM-generated summaries. In this paper, we
investigate fine-tuning strategies to reduce the occurrence of unfaithful spans
in generated summaries. First, we automatically generate summaries for the set
of source documents in the training set with a variety of LLMs and then use
GPT-4o to annotate any hallucinations it detects at the span-level. Leveraging
these annotations, we fine-tune LLMs with both hallucination-free summaries and
annotated unfaithful spans to enhance model faithfulness. In this paper, we
introduce a new dataset that contains both faithful and unfaithful summaries
with span-level labels and we evaluate three techniques to fine-tuning a LLM to
improve the faithfulness of the resulting summarization: gradient ascent,
unlikelihood training, and task vector negation. Experimental results show that
all three approaches successfully leverage span-level annotations to improve
faithfulness, with unlikelihood training being the most effective.

</details>


### [25] [Unpacking Hateful Memes: Presupposed Context and False Claims](https://arxiv.org/abs/2510.09935)
*Weibin Cai,Jiayu Li,Reza Zafarani*

Main category: cs.CL

TL;DR: SHIELD是一个仇恨表情包检测框架，通过建模预设上下文和识别虚假主张来检测仇恨表情包，在多个数据集和指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前仇恨表情包检测方法主要依赖预训练语言模型，但较少关注仇恨表情包的本质特征。基于哲学和心理学洞察，作者认为仇恨表情包具有两个基本特征：预设上下文和虚假主张表达。

Method: 开发PCM模块建模多模态上下文信息，引入FACT模块整合外部知识并利用跨模态参考图检测虚假主张，结合两者构建SHIELD框架。

Result: 大量实验表明SHIELD在多个数据集和指标上优于最先进方法，并在假新闻检测等其他任务上表现出通用性。

Conclusion: SHIELD通过捕捉仇恨表情包的基本特征（预设上下文和虚假主张），实现了有效的仇恨内容检测，具有广泛的应用潜力。

Abstract: While memes are often humorous, they are frequently used to disseminate hate,
causing serious harm to individuals and society. Current approaches to hateful
meme detection mainly rely on pre-trained language models. However, less focus
has been dedicated to \textit{what make a meme hateful}. Drawing on insights
from philosophy and psychology, we argue that hateful memes are characterized
by two essential features: a \textbf{presupposed context} and the expression of
\textbf{false claims}. To capture presupposed context, we develop \textbf{PCM}
for modeling contextual information across modalities. To detect false claims,
we introduce the \textbf{FACT} module, which integrates external knowledge and
harnesses cross-modal reference graphs. By combining PCM and FACT, we introduce
\textbf{\textsf{SHIELD}}, a hateful meme detection framework designed to
capture the fundamental nature of hate. Extensive experiments show that SHIELD
outperforms state-of-the-art methods across datasets and metrics, while
demonstrating versatility on other tasks, such as fake news detection.

</details>


### [26] [Beyond Fertility: Analyzing STRR as a Metric for Multilingual Tokenization Evaluation](https://arxiv.org/abs/2510.09947)
*Mir Tafseer Nayeem,Sawsan Alqahtani,Md Tahmid Rahman Laskar,Tasnim Mohiuddin,M Saiful Bari*

Main category: cs.CL

TL;DR: 论文分析了LLM中分词步骤的重要性，提出了新的评估指标STRR来补充传统fertility指标的不足，揭示了不同语言在分词中的公平性问题。


<details>
  <summary>Details</summary>
Motivation: 传统分词评估指标fertility只关注压缩效率，无法反映词汇表在不同语言和领域间的分配公平性，需要更全面的评估方法。

Method: 分析了6个常用分词器在7种语言和2个领域中的表现，提出了单令牌保留率(STRR)指标来衡量单词被保留为单个令牌的比例。

Result: 发现英语分词稳定性高，中文分词效率高但需要更多令牌，印地语分词碎片化严重，STRR指标能有效揭示跨语言公平性问题。

Conclusion: STRR指标是对fertility的重要补充，为设计更公平的多语言分词器提供了实用指导。

Abstract: Tokenization is a crucial but under-evaluated step in large language models
(LLMs). The standard metric, fertility (the average number of tokens per word),
captures compression efficiency but obscures how vocabularies are allocated
across languages and domains. We analyze six widely used tokenizers across
seven languages and two domains, finding stable fertility for English, high
fertility for Chinese, and little domain sensitivity. To address fertility's
blind spots, we propose the Single Token Retention Rate (STRR), which measures
the proportion of words preserved as single tokens. STRR reveals systematic
prioritization of English, strong support for Chinese, and fragmentation in
Hindi, offering an interpretable view of cross-lingual fairness. Our results
show that STRR complements fertility and provides practical guidance for
designing more equitable multilingual tokenizers.

</details>


### [27] [Unifying Tree Search Algorithm and Reward Design for LLM Reasoning: A Survey](https://arxiv.org/abs/2510.09988)
*Jiaqi Wei,Xiang Zhang,Yuejin Yang,Wenxuan Huang,Juntai Cao,Sheng Xu,Xiang Zhuang,Zhangyang Gao,Muhammad Abdul-Mageed,Laks V. S. Lakshmanan,Chenyu You,Wanli Ouyang,Siqi Sun*

Main category: cs.CL

TL;DR: 本文提出了一个统一框架，将树搜索算法分解为三个核心组件：搜索机制、奖励公式和转移函数，明确了搜索指导与参数化奖励建模的区别，为自主自改进智能体的系统化研究提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 当前审议树搜索领域缺乏统一形式化框架，特别是在奖励信号的角色上存在模糊性——它是临时启发式还是持久学习目标？这种模糊性阻碍了该领域的系统化发展。

Method: 引入统一框架，将搜索算法分解为搜索机制、奖励公式和转移函数三个核心组件，建立搜索指导（用于测试时扩展）与参数化奖励建模（用于自改进）的正式区分。

Result: 提出了基于组件的分类法，综合了最先进技术，并为创建自主自改进智能体的系统化进展制定了研究路线图。

Conclusion: 该框架解决了奖励信号角色的模糊性，为审议树搜索领域的统一理解和未来发展提供了坚实基础，推动了从暴力扩展向算法效率的转变。

Abstract: Deliberative tree search is a cornerstone of modern Large Language Model
(LLM) research, driving the pivot from brute-force scaling toward algorithmic
efficiency. This single paradigm unifies two critical frontiers:
\textbf{Test-Time Scaling (TTS)}, which deploys on-demand computation to solve
hard problems, and \textbf{Self-Improvement}, which uses search-generated data
to durably enhance model parameters. However, this burgeoning field is
fragmented and lacks a common formalism, particularly concerning the ambiguous
role of the reward signal -- is it a transient heuristic or a durable learning
target? This paper resolves this ambiguity by introducing a unified framework
that deconstructs search algorithms into three core components: the
\emph{Search Mechanism}, \emph{Reward Formulation}, and \emph{Transition
Function}. We establish a formal distinction between transient \textbf{Search
Guidance} for TTS and durable \textbf{Parametric Reward Modeling} for
Self-Improvement. Building on this formalism, we introduce a component-centric
taxonomy, synthesize the state-of-the-art, and chart a research roadmap toward
more systematic progress in creating autonomous, self-improving agents.

</details>


### [28] [Toward Machine Translation Literacy: How Lay Users Perceive and Rely on Imperfect Translations](https://arxiv.org/abs/2510.09994)
*Yimin Xiao,Yongle Zhang,Dayeon Ki,Calvin Bao,Marianna J. Martindale,Charlotte Vaughn,Ge Gao,Marine Carpuat*

Main category: cs.CL

TL;DR: 研究在博物馆环境中调查双语和非双语用户对机器翻译的依赖程度，发现非双语用户容易过度依赖机器翻译，而体验错误后会重新评估依赖程度。


<details>
  <summary>Details</summary>
Motivation: 随着机器翻译日益普及，了解公众如何感知和依赖不完美的机器翻译对于将机器翻译研究置于实际应用背景中至关重要。

Method: 在公共博物馆进行人类研究（n=452），调查流畅性和充分性错误如何影响双语和非双语用户在休闲使用中对机器翻译的依赖。

Result: 非双语用户由于缺乏评估策略和替代方案而经常过度依赖机器翻译，而体验错误的影响会促使用户重新评估未来的依赖程度。

Conclusion: 机器翻译评估和NLP解释技术不仅需要提高机器翻译质量，还需要促进用户对机器翻译的认知素养。

Abstract: As Machine Translation (MT) becomes increasingly commonplace, understanding
how the general public perceives and relies on imperfect MT is crucial for
contextualizing MT research in real-world applications. We present a human
study conducted in a public museum (n=452), investigating how fluency and
adequacy errors impact bilingual and non-bilingual users' reliance on MT during
casual use. Our findings reveal that non-bilingual users often over-rely on MT
due to a lack of evaluation strategies and alternatives, while experiencing the
impact of errors can prompt users to reassess future reliance. This highlights
the need for MT evaluation and NLP explanation techniques to promote not only
MT quality, but also MT literacy among its users.

</details>


### [29] [MTP-S2UT: Enhancing Speech-to-Speech Translation Quality with Multi-token Prediction](https://arxiv.org/abs/2510.10003)
*Jianjin Wang,Runsong Zhao,Xiaoqian Liu,Yuan Ge,Ziqiang Xu,Tong Xiao,Shengxiang Gao,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

TL;DR: 本文提出在语音到单元翻译(S2UT)模型中引入多令牌预测(MTP)损失，通过在中间层应用MTP损失来增强隐藏表示的语义密度，从而提升翻译质量。


<details>
  <summary>Details</summary>
Motivation: 当前语音到语音翻译方法使用语音令牌作为中间表示，但单个语音令牌语义密度不足，需要多个令牌才能表达完整语义单元。

Method: 提出MTP-S2UT损失，在计算CTC损失的隐藏表示层应用多令牌预测损失，使模型能在每个位置预测多个后续令牌，提前增强信息密度。

Result: 实验表明所有MTP损失变体都能提升S2UT翻译质量，其中MTP-S2UT表现最佳。

Conclusion: 在中间层应用MTP损失能更早、更有效地增强隐藏表示，显著提升语音到单元翻译性能。

Abstract: Current direct speech-to-speech translation methods predominantly employ
speech tokens as intermediate representations. However, a single speech token
is not dense in semantics, so we generally need multiple tokens to express a
complete semantic unit. To address this limitation, we introduce multi-token
prediction (MTP) loss into speech-to-unit translation (S2UT) models, enabling
models to predict multiple subsequent tokens at each position, thereby
capturing more complete semantics and enhancing information density per
position. Initial MTP implementations apply the loss at the final layer, which
improves output representation but initiates information enrichment too late.
We hypothesize that advancing the information enrichment process to
intermediate layers can achieve earlier and more effective enhancement of
hidden representation. Consequently, we propose MTP-S2UT loss, applying MTP
loss to hidden representation where CTC loss is computed. Experiments
demonstrate that all MTP loss variants consistently improve the quality of S2UT
translation, with MTP-S2UT achieving the best performance.

</details>


### [30] [Beyond the limitation of a single query: Train your LLM for query expansion with Reinforcement Learning](https://arxiv.org/abs/2510.10009)
*Shu Zhao,Tan Yu,Anbang Xu*

Main category: cs.CL

TL;DR: 提出ExpandSearch方法，通过强化学习训练LLM搜索代理，具备查询扩展能力，同时引入预训练压缩模型帮助理解检索文档，在7个QA基准测试中平均提升4.4%准确率。


<details>
  <summary>Details</summary>
Motivation: 现有推理增强搜索代理在推理和搜索能力有限，在多跳QA基准测试中表现不佳，需要处理复杂或复合查询。

Method: 使用强化学习训练LLM搜索代理，每轮生成多个查询变体并行搜索；引入预训练压缩模型帮助理解检索文档，让搜索代理专注于查询生成以提高检索召回率。

Result: 在7个QA基准测试中，ExpandSearch方法相比最先进基线平均提升4.4%准确率，在多跳推理任务上表现优异。

Conclusion: 即使使用小规模3B LLM，在压缩模型辅助下也能展现强大的查询扩展能力，在多跳QA基准测试中达到最先进水平。

Abstract: Reasoning-augmented search agents, such as Search-R1, are trained to reason,
search, and generate the final answer iteratively. Nevertheless, due to their
limited capabilities in reasoning and search, their performance on multi-hop QA
benchmarks remains far from satisfactory. To handle complex or compound
queries, we train an LLM-based search agent with the native capability of query
expansion through reinforcement learning. In each turn, our search agent
proposes several query variants, which are searched simultaneously to cover
more relevant information. Meanwhile, given limited post-training data and
computing resources, it is very challenging for a search agent to master
multiple tasks, including query generation, retrieved information
understanding, and answer generation. Therefore, we propose incorporating a
pre-trained squeezer model that helps the search agent understand the retrieved
documents, allowing the search agent to focus on query generation for high
retrieval recall. With the assistance of the squeezer model, we discover that
even a small-scale 3B LLM can demonstrate a strong capability of query
expansion and achieve state-of-the-art accuracy on the multi-hop QA benchmarks.
To be specific, our experiments across seven question-answering benchmarks
demonstrate that our method, named ExpandSearch, achieves an average
improvement of 4.4% compared to state-of-the-art baselines, with strong gains
on multi-hop reasoning tasks requiring diverse evidence aggregation.

</details>


### [31] [Path Drift in Large Reasoning Models:How First-Person Commitments Override Safety](https://arxiv.org/abs/2510.10013)
*Yuyi Huang,Runzhe Zhan,Lidia S. Chao,Ailin Tao,Derek F. Wong*

Main category: cs.CL

TL;DR: 论文发现大语言模型在长思维链推理中存在路径漂移漏洞，即推理轨迹会偏离安全对齐路径，导致违反安全约束的内容。作者提出了路径漂移诱导框架和防御策略。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型通过RLHF等技术进行了安全对齐，但在长思维链推理中仍存在未被充分探索的漏洞，即推理轨迹可能偏离安全路径，产生不安全内容。

Method: 通过实证分析识别了三种路径漂移行为触发因素，并提出了包含认知负荷放大、自我角色启动和条件链劫持的三阶段路径漂移诱导框架，以及包含角色归因修正和元认知反思的路径级防御策略。

Result: 研究发现三种行为触发因素能独立降低拒绝率，组合使用时效果更显著。提出的防御策略能有效缓解路径漂移风险。

Conclusion: 研究强调了在长形式推理中需要超越词元级对齐，进行轨迹级对齐监督的必要性。

Abstract: As large language models (LLMs) are increasingly deployed for complex
reasoning tasks, Long Chain-of-Thought (Long-CoT) prompting has emerged as a
key paradigm for structured inference. Despite early-stage safeguards enabled
by alignment techniques such as RLHF, we identify a previously underexplored
vulnerability: reasoning trajectories in Long-CoT models can drift from aligned
paths, resulting in content that violates safety constraints. We term this
phenomenon Path Drift. Through empirical analysis, we uncover three behavioral
triggers of Path Drift: (1) first-person commitments that induce goal-driven
reasoning that delays refusal signals; (2) ethical evaporation, where
surface-level disclaimers bypass alignment checkpoints; (3) condition chain
escalation, where layered cues progressively steer models toward unsafe
completions. Building on these insights, we introduce a three-stage Path Drift
Induction Framework comprising cognitive load amplification, self-role priming,
and condition chain hijacking. Each stage independently reduces refusal rates,
while their combination further compounds the effect. To mitigate these risks,
we propose a path-level defense strategy incorporating role attribution
correction and metacognitive reflection (reflective safety cues). Our findings
highlight the need for trajectory-level alignment oversight in long-form
reasoning beyond token-level alignment.

</details>


### [32] [Lightweight Baselines for Medical Abstract Classification: DistilBERT with Cross-Entropy as a Strong Default](https://arxiv.org/abs/2510.10025)
*Jiaqi Liu,Lanruo Wang,Su Liu,Xin Hu*

Main category: cs.CL

TL;DR: 该论文研究了在医疗文本分类任务中，轻量级编码器（如DistilBERT）在成本、延迟和隐私限制下的表现，发现使用交叉熵损失的DistilBERT在测试集上表现最佳且参数更少。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗环境中部署面临成本、延迟和隐私限制，需要探索轻量级模型在医疗文本分类中的潜力。

Method: 使用公开医疗摘要语料库，在固定分词器、序列长度、优化器和调度的情况下，对BERT base和DistilBERT进行微调，比较标准交叉熵、类别加权交叉熵和焦点损失三种目标函数。

Result: DistilBERT结合普通交叉熵损失在测试集上表现最佳，同时使用的参数远少于BERT base。报告了准确率、宏F1和加权F1指标，并进行了混淆分析以明确错误模式。

Conclusion: 建议实用的默认方法：从紧凑编码器和交叉熵开始，在转向更重模型之前添加校准和任务特定检查。

Abstract: Large language models work well for many NLP tasks, but they are hard to
deploy in health settings with strict cost, latency, and privacy limits. We
revisit a lightweight recipe for medical abstract classification and ask how
far compact encoders can go under a controlled budget. Using the public medical
abstracts corpus, we finetune BERT base and DistilBERT with three objectives
standard cross-entropy, class weighted cross entropy, and focal loss keeping
tokenizer, sequence length, optimizer, and schedule fixed. DistilBERT with
plain cross-entropy gives the best balance on the test set while using far
fewer parameters than BERT base. We report accuracy, Macro F1, and Weighted F1,
release the evaluation code, and include confusion analyses to make error
patterns clear. Our results suggest a practical default: start with a compact
encoder and cross-entropy, then add calibration and task-specific checks before
moving to heavier models.

</details>


### [33] [HUME: Measuring the Human-Model Performance Gap in Text Embedding Task](https://arxiv.org/abs/2510.10062)
*Adnan El Assadi,Isaac Chung,Roman Solomatin,Niklas Muennighoff,Kenneth Enevoldsen*

Main category: cs.CL

TL;DR: 提出了HUME框架来测量文本嵌入任务中的人类表现，发现人类平均表现77.6%，最佳模型80.1%，揭示了模型在低资源语言中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有嵌入模型评估框架缺乏可靠的人类表现基准，限制了模型分数的可解释性，需要建立人类表现基线来更好地理解模型能力。

Method: 开发HUME框架，在16个MTEB数据集上测量人类表现，涵盖重排序、分类、聚类和语义文本相似度任务，涉及高资源和低资源语言。

Result: 人类平均表现77.6%，最佳模型80.1%；模型在某些数据集上接近上限表现，但在其他数据集上表现不佳，特别是在低资源语言中。

Conclusion: 提供了人类表现基线、任务难度模式洞察和可扩展评估框架，有助于更有效地解释模型性能并指导模型和基准测试的开发。

Abstract: Comparing human and model performance offers a valuable perspective for
understanding the strengths and limitations of embedding models, highlighting
where they succeed and where they fail to capture meaning and nuance. However,
such comparisons are rarely made, as human performance on embedding tasks is
difficult to measure. To fill this gap, we introduce HUME: Human Evaluation
Framework for Text Embeddings. While frameworks like MTEB provide broad model
evaluation, they lack reliable estimates of human performance, limiting the
interpretability of model scores. We measure human performance across 16 MTEB
datasets spanning reranking, classification, clustering, and semantic textual
similarity across linguistically diverse high- and low-resource languages.
Humans achieve an average performance of 77.6% compared to 80.1% for the best
embedding model, although variation is substantial: models reach near-ceiling
performance on some datasets while struggling on others, suggesting dataset
issues and revealing shortcomings in low-resource languages. We provide human
performance baselines, insight into task difficulty patterns, and an extensible
evaluation framework that enables a more meaningful interpretation of the model
and informs the development of both models and benchmarks. Our code, dataset,
and leaderboard are publicly available at
https://github.com/embeddings-benchmark/mteb.

</details>


### [34] [CLMN: Concept based Language Models via Neural Symbolic Reasoning](https://arxiv.org/abs/2510.10063)
*Yibo Yang*

Main category: cs.CL

TL;DR: CLMN是一个神经符号框架，通过连续可读的概念嵌入和模糊逻辑推理，在保持性能的同时提升NLP模型的可解释性，特别是在医疗和金融领域。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在NLP中可解释性不足的问题，特别是在医疗和金融等敏感领域需要透明决策的需求。现有概念瓶颈模型要么使用二元激活损害文本表示，要么使用潜在概念削弱语义，且很少建模动态概念交互。

Method: CLMN将概念表示为连续的人类可读嵌入，应用模糊逻辑推理学习自适应交互规则，说明概念如何相互影响及影响最终决策。模型通过概念感知表示增强原始文本特征，并自动推导可解释的逻辑规则。

Result: 在多个数据集和预训练语言模型上，CLMN比现有基于概念的方法获得更高准确率，同时提升解释质量。

Conclusion: 在统一概念空间中整合神经表示与符号推理可以产生实用、透明的NLP系统。

Abstract: Deep learning has advanced NLP, but interpretability remains limited,
especially in healthcare and finance. Concept bottleneck models tie predictions
to human concepts in vision, but NLP versions either use binary activations
that harm text representations or latent concepts that weaken semantics, and
they rarely model dynamic concept interactions such as negation and context. We
introduce the Concept Language Model Network (CLMN), a neural-symbolic
framework that keeps both performance and interpretability. CLMN represents
concepts as continuous, human-readable embeddings and applies fuzzy-logic
reasoning to learn adaptive interaction rules that state how concepts affect
each other and the final decision. The model augments original text features
with concept-aware representations and automatically induces interpretable
logic rules. Across multiple datasets and pre-trained language models, CLMN
achieves higher accuracy than existing concept-based methods while improving
explanation quality. These results show that integrating neural representations
with symbolic reasoning in a unified concept space can yield practical,
transparent NLP systems.

</details>


### [35] [Unilaw-R1: A Large Language Model for Legal Reasoning with Reinforcement Learning and Iterative Inference](https://arxiv.org/abs/2510.10072)
*Hua Cai,Shuang Zhao,Liang Zhang,Xuli Shen,Qing Xu,Weilin Shen,Zihao Wen,Tianke Ban*

Main category: cs.CL

TL;DR: Unilaw-R1是一个专为法律推理设计的7B参数轻量级大语言模型，通过两阶段训练策略在权威基准测试中表现优异，超越同规模模型并与更大模型性能相当。


<details>
  <summary>Details</summary>
Motivation: 现有推理型大语言模型在法律领域的复杂问题处理能力尚未充分探索，需要解决法律知识不足、推理逻辑不可靠和业务泛化能力弱三大挑战。

Method: 构建高质量法律推理数据集Unilaw-R1-Data，采用监督微调(SFT)和强化学习(RL)的两阶段训练策略，开发专用评估基准Unilaw-R1-Eval。

Result: 在权威基准测试中超越所有同规模模型，与DeepSeek-R1-Distill-Qwen-32B(54.9%)性能相当，在LawBench和LexEval上平均超过Qwen-2.5-7B-Instruct(46.6%) 6.6%。

Conclusion: Unilaw-R1证明了轻量级模型通过专门训练可以在法律推理任务中取得优异表现，为法律AI应用提供了可解释的决策支持。

Abstract: Reasoning-focused large language models (LLMs) are rapidly evolving across
various domains, yet their capabilities in handling complex legal problems
remains underexplored. In this paper, we introduce Unilaw-R1, a large language
model tailored for legal reasoning. With a lightweight 7-billion parameter
scale, Unilaw-R1 significantly reduces deployment cost while effectively
tackling three core challenges in the legal domain: insufficient legal
knowledge, unreliable reasoning logic, and weak business generalization. To
address these issues, we first construct Unilaw-R1-Data, a high-quality dataset
containing 17K distilled and screened chain-of-thought (CoT) samples. Based on
this, we adopt a two-stage training strategy combining Supervised Fine-Tuning
(SFT) and Reinforcement Learning (RL), which significantly boosts the
performance on complex legal reasoning tasks and supports interpretable
decision-making in legal AI applications. To assess legal reasoning ability, we
also introduce Unilaw-R1-Eval, a dedicated benchmark designed to evaluate
models across single- and multi-choice legal tasks. Unilaw-R1 demonstrates
strong results on authoritative benchmarks, outperforming all models of similar
scale and achieving performance on par with the much larger
DeepSeek-R1-Distill-Qwen-32B (54.9%). Following domain-specific training, it
also showed significant gains on LawBench and LexEval, exceeding
Qwen-2.5-7B-Instruct (46.6%) by an average margin of 6.6%.

</details>


### [36] [A-IPO: Adaptive Intent-driven Preference Optimization](https://arxiv.org/abs/2510.10077)
*Wenqing Wang,Muhammad Asif Ali,Ali Shoker,Ruohan Yang,Junyang Chen,Ying Sha,Huan Wang*

Main category: cs.CL

TL;DR: A-IPO是一种新的偏好优化方法，通过推断用户提示背后的潜在意图并将其纳入奖励函数，解决了现有方法忽视少数意见和用户潜在意图的问题。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法（如DPO）往往默认多数观点，忽视了少数意见，且无法捕捉用户提示中的潜在意图。人类偏好具有多样性和动态性，受地区、文化和社会因素影响。

Method: A-IPO引入意图模块推断用户提示的潜在意图，并将推断的意图明确纳入奖励函数，通过意图-响应相似性项增加偏好边界，实现更清晰的首选与非首选响应分离。

Result: A-IPO在多个基准测试中显著优于现有基线：在Real-pref上胜率提升24.8%，响应-意图一致性提升45.6%；在Attack-pref上响应相似性提升38.6%，防御成功率提升52.2%；在GlobalOpinionQA-Ext上意图一致性得分提升54.6%。

Conclusion: 通过明确建模多样用户意图，A-IPO促进了多元偏好优化，同时增强了偏好对齐的对抗鲁棒性，在真实世界和对抗性偏好对齐评估中表现优异。

Abstract: Human preferences are diverse and dynamic, shaped by regional, cultural, and
social factors. Existing alignment methods like Direct Preference Optimization
(DPO) and its variants often default to majority views, overlooking minority
opinions and failing to capture latent user intentions in prompts.
  To address these limitations, we introduce \underline{\textbf{A}}daptive
\textbf{\underline{I}}ntent-driven \textbf{\underline{P}}reference
\textbf{\underline{O}}ptimization (\textbf{A-IPO}). Specifically,A-IPO
introduces an intention module that infers the latent intent behind each user
prompt and explicitly incorporates this inferred intent into the reward
function, encouraging stronger alignment between the preferred model's
responses and the user's underlying intentions. We demonstrate, both
theoretically and empirically, that incorporating an intention--response
similarity term increases the preference margin (by a positive shift of
$\lambda\,\Delta\mathrm{sim}$ in the log-odds), resulting in clearer separation
between preferred and dispreferred responses compared to DPO.
  For evaluation, we introduce two new benchmarks, Real-pref, Attack-pref along
with an extended version of an existing dataset, GlobalOpinionQA-Ext, to assess
real-world and adversarial preference alignment.
  Through explicit modeling of diverse user intents,A-IPO facilitates
pluralistic preference optimization while simultaneously enhancing adversarial
robustness in preference alignment. Comprehensive empirical evaluation
demonstrates that A-IPO consistently surpasses existing baselines, yielding
substantial improvements across key metrics: up to +24.8 win-rate and +45.6
Response-Intention Consistency on Real-pref; up to +38.6 Response Similarity
and +52.2 Defense Success Rate on Attack-pref; and up to +54.6 Intention
Consistency Score on GlobalOpinionQA-Ext.

</details>


### [37] [Diversity Augmentation of Dynamic User Preference Data for Boosting Personalized Text Summarizers](https://arxiv.org/abs/2510.10082)
*Parthiv Chatterjee,Shivam Sonawane,Amey Hengle,Aditya Tanna,Sourish Dasgupta,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 提出了一种名为PerAugy的数据增强技术，通过跨轨迹混洗和摘要内容扰动来提升个性化摘要模型的性能，解决了训练数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 个性化摘要面临训练数据稀缺的挑战，现有数据集如MS/CAS PENS缺乏目标摘要且主题转换多样性有限，无法支持端到端的监督学习。

Method: 使用跨轨迹混洗和摘要内容扰动的数据增强技术PerAugy，提升四种最先进用户编码器的准确性，并引入三个数据集多样性指标来量化增强效果。

Result: PerAugy显著提升了基线用户编码器的准确性（AUC最佳提升0.132），增强后的摘要框架个性化程度平均提升61.2%（基于PSE-SU4指标）。

Conclusion: 数据增强诱导的多样性是性能提升的关键因素，TP和DegreeD指标与用户编码器性能强相关，表明增加数据集多样性对个性化摘要至关重要。

Abstract: Document summarization enables efficient extraction of user-relevant content
but is inherently shaped by individual subjectivity, making it challenging to
identify subjective salient information in multifaceted documents. This
complexity underscores the necessity for personalized summarization. However,
training models for personalized summarization has so far been challenging,
particularly because diverse training data containing both user preference
history (i.e., click-skip trajectory) and expected (gold-reference) summaries
are scarce. The MS/CAS PENS dataset is a valuable resource but includes only
preference history without target summaries, preventing end-to-end supervised
learning, and its limited topic-transition diversity further restricts
generalization. To address this, we propose $\mathrm{PerAugy}$, a novel
cross-trajectory shuffling and summary-content perturbation based data
augmentation technique that significantly boosts the accuracy of four
state-of-the-art baseline (SOTA) user-encoders commonly used in personalized
summarization frameworks (best result: $\text{0.132}$$\uparrow$ w.r.t AUC). We
select two such SOTA summarizer frameworks as baselines and observe that when
augmented with their corresponding improved user-encoders, they consistently
show an increase in personalization (avg. boost: $\text{61.2\%}\uparrow$ w.r.t.
PSE-SU4 metric). As a post-hoc analysis of the role of induced diversity in the
augmented dataset by \peraugy, we introduce three dataset diversity metrics --
$\mathrm{TP}$, $\mathrm{RTC}$, and \degreed\ to quantify the induced diversity.
We find that $\mathrm{TP}$ and $\mathrm{DegreeD}$ strongly correlate with
user-encoder performance on the PerAugy-generated dataset across all accuracy
metrics, indicating that increased dataset diversity is a key factor driving
performance gains.

</details>


### [38] [Stop When Enough: Adaptive Early-Stopping for Chain-of-Thought Reasoning](https://arxiv.org/abs/2510.10103)
*Renliang Sun,Wei Cheng,Dawei Li,Haifeng Chen,Wei Wang*

Main category: cs.CL

TL;DR: REFRAIN是一个无需训练的自适应推理停止框架，通过识别冗余推理步骤来减少token使用，在保持或提升准确率的同时降低推理成本。


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought推理虽然提升了语言模型的推理能力，但过度推理会增加推理成本并可能导致错误结论，需要自适应停止机制。

Method: 整合两阶段停止判别器识别反射性冗余推理，使用滑动窗口上置信界多臂老虎机控制器动态调整停止阈值，无需监督或微调。

Result: 在四个代表性基准测试和两个模型家族中，REFRAIN减少20-55%的token使用，同时保持或提高标准CoT提示的准确率。

Conclusion: 研究强调了"何时停止"作为测试时扩展的新实用维度，使模型能够进行恰到好处的推理而非过度推理。

Abstract: Chain-of-Thought (CoT) reasoning has driven recent gains of large language
models (LLMs) on reasoning-intensive tasks by externalizing intermediate steps.
However, excessive or redundant reasoning -- so-called overthinking -- can
increase inference costs and lead LLMs toward incorrect conclusions. In this
paper, we present REFRAIN ($\underline{REF}$lective-$\underline{R}$edundancy
for $\underline{A}$daptive $\underline{IN}$ference), a training-free framework
that adaptively determines when to stop reasoning to mitigate overthinking.
REFRAIN integrates a two-stage stop discriminator to identify reflective yet
redundant reasoning and a sliding-window Upper Confidence Bound (SW-UCB)
multi-armed bandit controller to dynamically adjust stopping thresholds
according to problem difficulty without supervision or fine-tuning. Across four
representative benchmarks and two model families, REFRAIN reduces token usage
by 20-55% while maintaining or improving accuracy compared to standard CoT
prompting. Extensive ablation and robustness analyses demonstrate its stability
across models, scorers, and prompt variations. In summary, our findings
highlight when-to-stop as a new and practical axis of test-time scaling --
enabling models to reason not just more, but just enough.

</details>


### [39] [LinearRAG: Linear Graph Retrieval Augmented Generation on Large-scale Corpora](https://arxiv.org/abs/2510.10114)
*Luyao Zhuang,Shengyuan Chen,Yilin Xiao,Huachi Zhou,Yujing Zhang,Hao Chen,Qinggang Zhang,Xiao Huang*

Main category: cs.CL

TL;DR: LinearRAG提出了一种高效的图基检索增强生成框架，通过构建无关系层次图来避免传统GraphRAG中不稳定且昂贵的关系抽取问题，实现了线性扩展的可靠图构建和精确段落检索。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统在处理大规模非结构化语料时效果不佳，而现有的图基RAG方法依赖不稳定且昂贵的关系抽取来构建图，往往产生噪声图并降低检索质量。

Method: LinearRAG构建无关系层次图（Tri-Graph），仅使用轻量级实体抽取和语义链接，避免不稳定关系建模。采用两阶段检索策略：通过局部语义桥接激活相关实体，然后通过全局重要性聚合进行段落检索。

Result: 在四个数据集上的广泛实验表明，LinearRAG显著优于基线模型。

Conclusion: LinearRAG提供了一种经济可靠的原始段落索引方法，能够线性扩展语料库大小且不产生额外token消耗，为复杂推理任务提供了更可靠的检索增强生成解决方案。

Abstract: Retrieval-Augmented Generation (RAG) is widely used to mitigate
hallucinations of Large Language Models (LLMs) by leveraging external
knowledge. While effective for simple queries, traditional RAG systems struggle
with large-scale, unstructured corpora where information is fragmented. Recent
advances incorporate knowledge graphs to capture relational structures,
enabling more comprehensive retrieval for complex, multi-hop reasoning tasks.
However, existing graph-based RAG (GraphRAG) methods rely on unstable and
costly relation extraction for graph construction, often producing noisy graphs
with incorrect or inconsistent relations that degrade retrieval quality. In
this paper, we revisit the pipeline of existing GraphRAG systems and propose
LinearRAG (Linear Graph-based Retrieval-Augmented Generation), an efficient
framework that enables reliable graph construction and precise passage
retrieval. Specifically, LinearRAG constructs a relation-free hierarchical
graph, termed Tri-Graph, using only lightweight entity extraction and semantic
linking, avoiding unstable relation modeling. This new paradigm of graph
construction scales linearly with corpus size and incurs no extra token
consumption, providing an economical and reliable indexing of the original
passages. For retrieval, LinearRAG adopts a two-stage strategy: (i) relevant
entity activation via local semantic bridging, followed by (ii) passage
retrieval through global importance aggregation. Extensive experiments on four
datasets demonstrate that LinearRAG significantly outperforms baseline models.

</details>


### [40] [Hybrid OCR-LLM Framework for Enterprise-Scale Document Information Extraction Under Copy-heavy Task](https://arxiv.org/abs/2510.10138)
*Zilong Wang,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 提出了一种结合OCR引擎和LLM的系统框架，用于优化重复性文档信息提取任务的准确性与效率平衡，通过智能策略选择利用文档特定特征。


<details>
  <summary>Details</summary>
Motivation: 解决企业文档处理中大量结构相似内容的信息提取这一关键但研究不足的挑战，优化重复性文档提取任务中的准确性与效率权衡。

Method: 结合OCR引擎与大型语言模型，实现三种提取范式（直接、替换和基于表格）的25种配置，在四种格式的身份文档上进行评估，采用格式感知路由和自适应框架。

Result: 基于表格的提取方法取得了优异结果：结构化文档达到F1=1.0准确率和0.97秒延迟，图像输入与PaddleOCR集成达到F1=0.997准确率和0.6秒延迟，相比多模态方法性能提升54倍，保持亚秒级处理速度。

Conclusion: 这项工作确立了一个通用原则：通过结构感知方法选择，可以将重复性文档处理任务的计算负担转化为优化机会，为异构文档流的大规模生产处理提供了可行方案。

Abstract: Information extraction from copy-heavy documents, characterized by massive
volumes of structurally similar content, represents a critical yet understudied
challenge in enterprise document processing. We present a systematic framework
that strategically combines OCR engines with Large Language Models (LLMs) to
optimize the accuracy-efficiency trade-off inherent in repetitive document
extraction tasks. Unlike existing approaches that pursue universal solutions,
our method exploits document-specific characteristics through intelligent
strategy selection. We implement and evaluate 25 configurations across three
extraction paradigms (direct, replacement, and table-based) on identity
documents spanning four formats (PNG, DOCX, XLSX, PDF). Through table-based
extraction methods, our adaptive framework delivers outstanding results: F1=1.0
accuracy with 0.97s latency for structured documents, and F1=0.997 accuracy
with 0.6 s for challenging image inputs when integrated with PaddleOCR, all
while maintaining sub-second processing speeds. The 54 times performance
improvement compared with multimodal methods over naive approaches, coupled
with format-aware routing, enables processing of heterogeneous document streams
at production scale. Beyond the specific application to identity extraction,
this work establishes a general principle: the repetitive nature of copy-heavy
tasks can be transformed from a computational burden into an optimization
opportunity through structure-aware method selection.

</details>


### [41] [DiffHeads: Differential Analysis and Inference-Time Masking of Bias Heads in Large Language Models](https://arxiv.org/abs/2510.10142)
*Tingxu Han,Wei Song,Ziqi Ding,Ziming Li,Chunrong Fang,Yuekang Li,Dongfang Liu,Zhenyu Chen,Zhenting Wang*

Main category: cs.CL

TL;DR: DiffHeads是一个轻量级的LLM去偏框架，通过识别和选择性屏蔽导致偏见的注意力头来减少模型不公平性。


<details>
  <summary>Details</summary>
Motivation: 现有方法只能探测偏见输出，但无法解释其生成机制，导致去偏方法脆弱。需要系统研究LLM不公平性的机制。

Method: 1) 比较直接回答和思维链提示策略；2) 定义token-to-head贡献分数追踪偏见来源；3) 提出DiffHeads框架，通过DA和CoT的差异激活分析识别偏见头并选择性屏蔽。

Result: DA提示使偏见增加534.5%-391.9%；发现一小簇偏见头在DA下激活但在CoT下休眠；DiffHeads在DA和CoT下分别减少不公平性49.4%和40.3%，且不影响模型效用。

Conclusion: DiffHeads通过因果机制分析有效识别和缓解LLM偏见，为理解偏见生成机制提供了新视角。

Abstract: Large language models (LLMs) increasingly mediate decisions in domains where
unfair treatment of demographic groups is unacceptable. Existing work probes
when biased outputs appear, but gives little insight into the mechanisms that
generate them, leaving existing mitigations largely fragile. In this paper, we
conduct a systematic investigation LLM unfairness and propose DiffHeads, a
lightweight debiasing framework for LLMs. We first compare Direct-Answer (DA)
prompting to Chain-of-Thought (CoT) prompting across eight representative open-
and closed-source LLMs. DA will trigger the nature bias part of LLM and improve
measured unfairness by 534.5%-391.9% in both one-turn and two-turn dialogues.
Next, we define a token-to-head contribution score that traces each token's
influence back to individual attention heads. This reveals a small cluster of
bias heads that activate under DA but stay largely dormant with CoT, providing
the first causal link between prompting strategy and bias emergence. Finally,
building on this insight, we propose DiffHeads that identifies bias heads
through differential activation analysis between DA and CoT, and selectively
masks only those heads. DiffHeads reduces unfairness by 49.4%, and 40.3% under
DA and CoT, respectively, without harming model utility.

</details>


### [42] [BILLY: Steering Large Language Models via Merging Persona Vectors for Creative Generation](https://arxiv.org/abs/2510.10157)
*Tsung-Min Pai,Jui-I Wang,Li-Chun Lu,Shao-Hua Sun,Hung-Yi Lee,Kai-Wei Chang*

Main category: cs.CL

TL;DR: BILLY是一个无需训练的单模型框架，通过在激活空间中提取和融合多个不同人格向量，实现多LLM协作的优势，同时显著降低计算成本和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 多LLM系统虽然能增强语言模型的创造力，但存在计算成本高和推理延迟大的问题。需要一种方法能在单个模型中实现多模型协作的优势。

Method: 在模型激活空间中提取多个不同的人格向量，然后将这些向量融合成一个混合向量，在推理过程中用这个混合向量引导模型生成多视角输出。

Result: 在面向创造力的基准测试中，BILLY超越了单模型提示和传统多LLM方法，同时大幅减少了推理时间和计算成本。

Conclusion: 不同人格向量的融合不仅能有效控制生成的互补方面，还能提高可解释性，为单模型实现多视角输出提供了有效解决方案。

Abstract: Multi-LLM systems enhance the creativity of large language models by
simulating human collective intelligence but suffer from significant drawbacks,
such as high computational costs and inference latency. To address these
limitations, we propose BILLY (BlendIng persona vectors for Large Language
model creativitY), a training-free framework that captures the benefits of
multi-LLM collaboration, i.e. inducing diverse perspectives and specialized
expertise, within a single model. BILLY operates by extracting and blending
multiple distinct persona vectors directly in the model's activation space. We
steer the model's generation process with this merged vector while inference,
enabling multi-perspective output without explicit multi-LLM communication. Our
experiments across creativity-oriented benchmarks demonstrate that BILLY
surpasses single model prompting and traditional multi-LLM approaches, while
substantially reducing inference time and computational costs. Our analyses
further reveal that distinct persona vectors can be blended to achieve both
effective control over complementary aspects of generation and greater
interpretability.

</details>


### [43] [BabyBabelLM: A Multilingual Benchmark of Developmentally Plausible Training Data](https://arxiv.org/abs/2510.10159)
*Jaap Jumelet,Abdellah Fourtassi,Akari Haga,Bastian Bunzeck,Bhargav Shandilya,Diana Galvan-Sosa,Faiz Ghifari Haznitrama,Francesca Padovani,Francois Meyer,Hai Hu,Julen Etxaniz,Laurent Prévot,Linyang He,María Grandury,Mila Marcheva,Negar Foroutan,Nikitas Theodoropoulos,Pouya Sadeghi,Siyuan Song,Suchir Salhan,Susana Zhou,Yurii Paniv,Ziyin Zhang,Arianna Bisazza,Alex Warstadt,Leshem Choshen*

Main category: cs.CL

TL;DR: BabyBabelLM是一个多语言数据集集合，模拟从出生到母语习得期间观察到的语言数据，包含45种语言各1亿英语单词当量的发展合理性预训练数据。


<details>
  <summary>Details</summary>
Motivation: 为多语言预训练和认知建模提供发展合理性的语言数据，模拟人类语言习得过程。

Method: 策划包含45种语言的发展合理性预训练数据，每种语言相当于1亿英语单词的内容，并编译评估套件和训练基线模型。

Result: 创建了包含45种语言的多语言数据集集合，每种语言都有相应的评估套件和基线模型。

Conclusion: BabyBabelLM旨在促进多语言预训练和认知建模研究。

Abstract: We present BabyBabelLM, a multilingual collection of datasets modeling the
language a person observes from birth until they acquire a native language. We
curate developmentally plausible pretraining data aiming to cover the
equivalent of 100M English words of content in each of 45 languages. We compile
evaluation suites and train baseline models in each language. BabyBabelLM aims
to facilitate multilingual pretraining and cognitive modeling.

</details>


### [44] [Large Language Model Sourcing: A Survey](https://arxiv.org/abs/2510.10161)
*Liang Pang,Kangxi Wu,Sunhao Dai,Zihao Wei,Zenghao Duan,Jia Gu,Xiang Li,Zhiyi Yin,Jun Xu,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: 这篇论文系统调查了大型语言模型生成内容的溯源追踪，从模型和数据两个视角提出了四个相互关联的维度，并建立了双范式分类法来增强LLM部署的透明度和可信度。


<details>
  <summary>Details</summary>
Motivation: 随着LLM从支持客观任务转向赋能主观决策，其黑盒特性和类人生成内容带来了幻觉、偏见、不公平和版权侵权等多方面风险，因此需要从多角度追踪信息来源。

Method: 提出了围绕模型和数据两个视角的四个维度：模型溯源（区分LLM与人类内容）、模型结构溯源（分析内部生成机制）、训练数据溯源（内部归因）和外部数据溯源（外部验证）。同时建立了先验式（主动嵌入可追溯性）和后验式（追溯推理）的双范式分类法。

Result: 构建了一个系统性的溯源追踪框架，能够从多个维度追踪LLM生成内容的来源，增强模型部署的透明度、问责制和可信度。

Conclusion: 跨这些维度的可追溯性对于LLM在现实世界应用中的透明、负责和可信部署至关重要，为解决LLM风险提供了系统性解决方案。

Abstract: The rapid advancement of large language models (LLMs) has revolutionized
artificial intelligence, shifting from supporting objective tasks (e.g.,
recognition) to empowering subjective decision-making (e.g., planning,
decision). This marks the dawn of general and powerful AI, with applications
spanning a wide range of fields, including programming, education, healthcare,
finance, and law. However, their deployment introduces multifaceted risks. Due
to the black-box nature of LLMs and the human-like quality of their generated
content, issues such as hallucinations, bias, unfairness, and copyright
infringement become particularly significant. In this context, sourcing
information from multiple perspectives is essential.
  This survey presents a systematic investigation into provenance tracking for
content generated by LLMs, organized around four interrelated dimensions that
together capture both model- and data-centric perspectives. From the model
perspective, Model Sourcing treats the model as a whole, aiming to distinguish
content generated by specific LLMs from content authored by humans. Model
Structure Sourcing delves into the internal generative mechanisms, analyzing
architectural components that shape the outputs of model. From the data
perspective, Training Data Sourcing focuses on internal attribution, tracing
the origins of generated content back to the training data of model. In
contrast, External Data Sourcing emphasizes external validation, identifying
external information used to support or influence the responses of model.
Moreover, we also propose a dual-paradigm taxonomy that classifies existing
sourcing methods into prior-based (proactive traceability embedding) and
posterior-based (retrospective inference) approaches. Traceability across these
dimensions enhances the transparency, accountability, and trustworthiness of
LLMs deployment in real-world applications.

</details>


### [45] [A Survey of Inductive Reasoning for Large Language Models](https://arxiv.org/abs/2510.10182)
*Kedi Chen,Dezhao Ruan,Yuhao Dan,Yaoting Wang,Siyu Yan,Xuecheng Wu,Yinqi Zhang,Qin Chen,Jie Zhou,Liang He,Biqing Qi,Linyang Li,Qipeng Guo,Xiaoming Shi,Wei Zhang*

Main category: cs.CL

TL;DR: 本文首次对LLMs的归纳推理进行全面调查，将改进方法分为后训练、测试时扩展和数据增强三类，总结了当前基准并提出了统一的沙盒评估方法。


<details>
  <summary>Details</summary>
Motivation: 归纳推理是LLMs的重要任务，具有从特殊到一般的思维过程和非唯一答案的特点，对知识泛化和人类认知至关重要，但目前缺乏系统性总结。

Method: 将归纳推理改进方法分为三类：后训练、测试时扩展和数据增强；总结当前基准并推导基于沙盒的统一评估方法；分析归纳能力来源及简单模型架构和数据的作用。

Result: 建立了归纳推理的系统分类框架，提出了统一的评估方法，为未来研究提供了坚实基础。

Conclusion: 本文填补了LLMs归纳推理领域系统性总结的空白，为后续研究提供了方法论基础和评估标准。

Abstract: Reasoning is an important task for large language models (LLMs). Among all
the reasoning paradigms, inductive reasoning is one of the fundamental types,
which is characterized by its particular-to-general thinking process and the
non-uniqueness of its answers. The inductive mode is crucial for knowledge
generalization and aligns better with human cognition, so it is a fundamental
mode of learning, hence attracting increasing interest. Despite the importance
of inductive reasoning, there is no systematic summary of it. Therefore, this
paper presents the first comprehensive survey of inductive reasoning for LLMs.
First, methods for improving inductive reasoning are categorized into three
main areas: post-training, test-time scaling, and data augmentation. Then,
current benchmarks of inductive reasoning are summarized, and a unified
sandbox-based evaluation approach with the observation coverage metric is
derived. Finally, we offer some analyses regarding the source of inductive
ability and how simple model architectures and data help with inductive tasks,
providing a solid foundation for future research.

</details>


### [46] [MedAgentAudit: Diagnosing and Quantifying Collaborative Failure Modes in Medical Multi-Agent Systems](https://arxiv.org/abs/2510.10185)
*Lei Gu,Yinghao Zhu,Haoran Sang,Zixiang Wang,Dehao Sui,Wen Tang,Ewen Harrison,Junyi Gao,Lequan Yu,Liantao Ma*

Main category: cs.CL

TL;DR: 对基于大语言模型的多智能体医疗咨询系统进行大规模实证研究，揭示仅关注最终答案准确性不足以评估系统可靠性，发现了四种主要的协作失败模式。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的多智能体医疗咨询系统评估主要关注最终答案准确性，忽视了推理过程的可验证性，这在高风险医疗应用中存在重大风险。

Method: 采用混合方法，结合定性分析和定量审计，对来自六个医疗数据集和六个代表性多智能体框架的3,600个案例进行大规模实证研究。

Result: 定量审计揭示了四种主要失败模式：由共享模型缺陷驱动的错误共识、正确少数意见被压制、无效讨论动态以及合成过程中的关键信息丢失。

Conclusion: 仅靠高准确性不足以建立临床或公众信任，迫切需要透明和可审计的推理过程，这是负责任开发和部署医疗AI的基石。

Abstract: While large language model (LLM)-based multi-agent systems show promise in
simulating medical consultations, their evaluation is often confined to
final-answer accuracy. This practice treats their internal collaborative
processes as opaque "black boxes" and overlooks a critical question: is a
diagnostic conclusion reached through a sound and verifiable reasoning pathway?
The inscrutable nature of these systems poses a significant risk in high-stakes
medical applications, potentially leading to flawed or untrustworthy
conclusions. To address this, we conduct a large-scale empirical study of 3,600
cases from six medical datasets and six representative multi-agent frameworks.
Through a rigorous, mixed-methods approach combining qualitative analysis with
quantitative auditing, we develop a comprehensive taxonomy of collaborative
failure modes. Our quantitative audit reveals four dominant failure patterns:
flawed consensus driven by shared model deficiencies, suppression of correct
minority opinions, ineffective discussion dynamics, and critical information
loss during synthesis. This study demonstrates that high accuracy alone is an
insufficient measure of clinical or public trust. It highlights the urgent need
for transparent and auditable reasoning processes, a cornerstone for the
responsible development and deployment of medical AI.

</details>


### [47] [Weed Out, Then Harvest: Dual Low-Rank Adaptation is an Effective Noisy Label Detector for Noise-Robust Learning](https://arxiv.org/abs/2510.10208)
*Bo Yuan,Yulin Chen,Yin Zhang*

Main category: cs.CL

TL;DR: Delora是一个新颖的框架，通过解耦样本选择和模型训练来解决带噪标签的PEFT问题，使用干净和噪声LoRA作为可学习的阈值来检测和选择样本。


<details>
  <summary>Details</summary>
Motivation: 现实场景中训练数据不可避免地包含噪声标签，传统方法选择小损失样本进行训练，但初始选择不准确会形成恶性循环，导致性能不佳。

Method: 建立噪声标签检测器，引入干净LoRA和噪声LoRA，利用记忆效应使干净LoRA记忆干净数据，噪声LoRA记忆误标数据，作为选择干净和噪声样本的可学习阈值。

Result: 在合成和真实世界噪声数据集上的实验结果表明，Delora在噪声标签检测和文本分类方面具有有效性。

Conclusion: Delora通过解耦样本选择和模型训练，成功解决了带噪标签的PEFT问题，提供了一种有效的噪声标签检测和模型训练方法。

Abstract: Parameter-efficient fine-tuning (PEFT) large language models (LLMs) have
shown impressive performance in various downstream tasks. However, in many
real-world scenarios, the collected training data inevitably contains noisy
labels. To learn from noisy labels, most solutions select samples with small
losses for model training. However, the selected samples, in turn, impact the
loss computation in the next iteration. An inaccurate initial selection can
create a vicious cycle, leading to suboptimal performance. To break this cycle,
we propose Delora, a novel framework that decouples the sample selection from
model training. For sample selection, Delora establishes a noisy label detector
by introducing clean and noisy LoRA. Benefiting from the memory effect, the
clean LoRA is encouraged to memorize clean data, while the noisy LoRA is
constrained to memorize mislabeled data, which serves as a learnable threshold
for selecting clean and noisy samples. For model training, Delora can use
carefully selected samples to fine-tune language models seamlessly.
Experimental results on synthetic and real-world noisy datasets demonstrate the
effectiveness of Delora in noisy label detection and text classification.

</details>


### [48] [You only need 4 extra tokens: Synergistic Test-time Adaptation for LLMs](https://arxiv.org/abs/2510.10223)
*Yijie Xu,Huizai Yao,Zhiyu Guo,Weiyu Guo,Pengteng Li,Aiwei Liu,Xuming Hu,Hui Xiong*

Main category: cs.CL

TL;DR: SyTTA是一个无需标签的测试时适应框架，通过结合输入侧困惑度和输出侧预测熵来应对语言模型在专业领域的分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在专业领域部署时面临训练数据分布偏移的挑战，而领域特定微调需要高质量标注数据，这在专业知识有限的场景中成本高昂且收集缓慢。

Method: SyTTA框架耦合两种互补的不确定性信号：输入侧困惑度（反映与领域特定术语和模式的不匹配）和输出侧预测熵（反映生成过程中标记概率的分散和不稳定性），在推理时无需额外监督即可自适应调整模型。

Result: 在多样化模型架构和领域特定基准测试中，SyTTA带来了持续的性能提升。在农业问答任务上，SyTTA仅用每个查询4个额外标记就将Qwen-2.5-7B的Rouge-LSum提升了120%以上。

Conclusion: 结果表明，无需标注示例即可实现语言模型的有效测试时适应，支持在标签稀缺领域的部署。

Abstract: Large language models (LLMs) are increasingly deployed in specialized domains
such as finance, medicine, and agriculture, where they face significant
distribution shifts from their training data. Domain-specific fine-tuning can
mitigate this challenge but relies on high-quality labeled data that is
expensive and slow to collect in expertise-limited settings. We study
label-free test-time adaptation for language models and present SyTTA, an
inference-time framework that adapts models on-the-fly without additional
supervision. SyTTA couples two complementary uncertainty signals that arise
under distribution shift: input-side perplexity, indicating mismatch with
domain-specific terminology and patterns, and output-side predictive entropy,
indicating diffuse and unstable token probabilities during generation. Across
diverse model architectures and domain-specific benchmarks, SyTTA delivers
consistent gains. Notably, on agricultural question answering, SyTTA improves
Rouge-LSum by over 120% on Qwen-2.5-7B with only 4 extra tokens per query.
These results show that effective test-time adaptation for language models is
achievable without labeled examples, supporting deployment in label-scarce
domains. The code will be made available upon acceptance.

</details>


### [49] [Text2Token: Unsupervised Text Representation Learning with Token Target Prediction](https://arxiv.org/abs/2510.10224)
*Ruize An,Richong Zhang,Zhijie Nie,Zhanyu Wu,Yanzhao Zhang,Dingkun Long*

Main category: cs.CL

TL;DR: Text2Token是一个无监督文本表示学习框架，通过生成式任务预测关键标记分布，在MTEB v2基准测试中达到与最先进无监督对比学习方法LLM2Vec竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 研究发现高质量文本表示与输入文本的关键标记对齐，这揭示了表示空间和词汇空间之间的潜在联系，启发我们重新审视生成式任务在文本表示学习中的应用。

Method: 提出Text2Token框架，基于标记目标预测任务，使用精心构建的目标标记分布作为监督信号。通过数据驱动和模型推导两种方法构建合成标记目标，识别文本中有意义的标记和语义衍生标记两类关键标记。

Result: 在MTEB v2基准测试中，Text2Token与最先进的无监督对比学习方法LLM2Vec表现相当，证明了生成式方法在文本表示学习中的有效性。

Conclusion: 分析表明词汇空间和表示空间在训练过程中共同优化并趋向最优解，为未来工作提供了新的思路和见解。

Abstract: Unsupervised text representation learning (TRL) is a fundamental task in
natural language processing, which is beneficial for improving search and
recommendations with the web's unlabeled texts. A recent empirical study finds
that the high-quality representation aligns with the key token of the input
text, uncovering the potential connection between representation space and
vocabulary space. Inspired by the findings, we revisit the generative tasks and
develop an unsupervised generative framework for TRL, Text2Token. The framework
is based on the token target prediction task, utilizing carefully constructed
target token distribution as supervisory signals. To construct the high-quality
target token distribution, we analyze the token-alignment properties with
advanced embedders and identify two essential categories of key tokens: (1) the
meaningful tokens in the text and (2) semantically derived tokens beyond the
text. Based on these insights, we propose two methods -- data-driven and
model-derived -- to construct synthetic token targets from data or the LLM
backbone. Experiments on the MTEB v2 benchmark demonstrate that Text2Token
achieves performance competitive with the state-of-the-art embedder with
unsupervised contrastive learning, LLM2Vec. Our analysis further shows that
vocabulary and representation spaces optimize together and toward the optimum
solution during training, providing new ideas and insights for future work.

</details>


### [50] [ImCoref-CeS: An Improved Lightweight Pipeline for Coreference Resolution with LLM-based Checker-Splitter Refinement](https://arxiv.org/abs/2510.10241)
*Kangyang Luo,Yuzhuo Bai,Shuzheng Si,Cheng Gao,Zhitong Wang,Yingli Shen,Wenhao Li,Zhu Liu,Yufeng Han,Jiayi Wu,Cunliang Kong,Maosong Sun*

Main category: cs.CL

TL;DR: 提出ImCoref-CeS框架，结合增强的监督模型与LLM推理，通过改进的CR方法和LLM验证机制，在共指消解任务上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决当前共指消解研究的困境：是继续探索基于小语言模型的监督神经方法，还是利用大语言模型的能力。有效结合两者优势仍是未充分探索的方向。

Method: 1) ImCoref：改进的监督CR方法，包括轻量级桥接模块增强长文本编码、双仿射评分器捕捉位置信息、混合提及正则化提高训练效率；2) LLM作为Checker-Splitter代理验证候选提及和共指结果。

Result: 广泛实验证明ImCoref-CeS的有效性，相比现有SOTA方法实现了更优越的性能。

Conclusion: 提出的ImCoref-CeS框架成功结合了监督神经方法和LLM的优势，在共指消解任务上取得了突破性进展。

Abstract: Coreference Resolution (CR) is a critical task in Natural Language Processing
(NLP). Current research faces a key dilemma: whether to further explore the
potential of supervised neural methods based on small language models, whose
detect-then-cluster pipeline still delivers top performance, or embrace the
powerful capabilities of Large Language Models (LLMs). However, effectively
combining their strengths remains underexplored. To this end, we propose
\textbf{ImCoref-CeS}, a novel framework that integrates an enhanced supervised
model with LLM-based reasoning. First, we present an improved CR method
(\textbf{ImCoref}) to push the performance boundaries of the supervised neural
method by introducing a lightweight bridging module to enhance long-text
encoding capability, devising a biaffine scorer to comprehensively capture
positional information, and invoking a hybrid mention regularization to improve
training efficiency. Importantly, we employ an LLM acting as a multi-role
Checker-Splitter agent to validate candidate mentions (filtering out invalid
ones) and coreference results (splitting erroneous clusters) predicted by
ImCoref. Extensive experiments demonstrate the effectiveness of ImCoref-CeS,
which achieves superior performance compared to existing state-of-the-art
(SOTA) methods.

</details>


### [51] [Audit-of-Understanding: Posterior-Constrained Inference for Mathematical Reasoning in Language Models](https://arxiv.org/abs/2510.10252)
*Samir Abdaljalil,Erchin Serpedin,Khalid Qaraqe,Hasan Kurban*

Main category: cs.CL

TL;DR: 提出了审计理解框架(AoU)，通过分解查询、审计假设和约束推理来减少语言模型中的推理诱导幻觉，在多个数学推理数据集上显著提升准确性和忠实性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常生成看似连贯但基于未经支持假设的推理轨迹，导致幻觉结论。现有工作主要处理事实性幻觉或依赖事后验证，对推理诱导的幻觉关注不足。

Method: AoU框架包含三个阶段：(1)将查询分解为候选假设，(2)审计这些假设的支持度，(3)仅在已验证子集上约束推理。该方法形式化为后验约束推理，与选择性预测和拒绝学习相关。

Result: 在GSM8K、MultiArith和SVAMP数据集上，AoU相比Chain-of-Thought、Self-Consistency和CoT-Decoding方法，准确性和忠实性均有显著提升：GSM8K上提升+30%，MultiArith上+45%，SVAMP上持续提升20-28%。

Conclusion: AoU框架通过约束推理于已验证前提，有效减少了语言模型中的推理诱导幻觉，提供了理论保证和实际性能提升，是处理此类问题的重要进展。

Abstract: Large language models (LLMs) often generate reasoning traces that appear
coherent but rest on unsupported assumptions, leading to hallucinated
conclusions. Prior work mainly addresses factual hallucinations or relies on
post-hoc verification, leaving reasoning-induced hallucinations largely
unaddressed. We propose Audit-of-Understanding (AoU), a framework that
constrains inference to validated premises through three phases: (1)
decomposing a query into candidate assumptions, (2) auditing their support, and
(3) conditioning inference only on the validated subset. Formally, AoU is
\emph{posterior-constrained inference}, connecting to selective prediction and
rejection learning. Our contributions are threefold: (i) theoretical guarantees
under perfect validation, (ii) excess-risk bounds under imperfect audits, and
(iii) tractability analysis. Empirically, AoU improves both accuracy and
faithfulness on GSM8K, MultiArith, and SVAMP, achieving up to +30% gains on
GSM8K, +45% on MultiArith, and consistent +20--28% improvements on SVAMP over
Chain-of-Thought, Self-Consistency, and CoT-Decoding. Code is available at
https://anonymous.4open.science/r/audit-of-understanding-E28B.

</details>


### [52] [Backdoor Collapse: Eliminating Unknown Threats via Known Backdoor Aggregation in Language Models](https://arxiv.org/abs/2510.10265)
*Liang Lin,Miao Yu,Moayad Aloqaily,Zhenhong Zhou,Kun Wang,Linsey Pang,Prakhar Mehrotra,Qingsong Wen*

Main category: cs.CL

TL;DR: 提出了一种无需预知触发器设置的后门防御框架，通过注入已知后门来聚合表示空间中的后门特征，然后进行恢复微调以恢复良性输出。


<details>
  <summary>Details</summary>
Motivation: 现有后门防御方法依赖不切实际的触发器设置假设，而公开检查点中的后门攻击对大型语言模型构成严重威胁。

Method: 两阶段过程：首先通过注入已知触发器聚合后门表示，然后进行恢复微调以恢复良性输出。

Result: 在多个基准测试中将平均攻击成功率降至4.41%，优于现有基线28.1%~69.3%；清洁准确率和实用性保持在原始模型的0.5%以内；防御方法可泛化到不同类型的后门。

Conclusion: 该防御框架在实际部署场景中具有鲁棒性，能有效应对后门攻击且对合法任务影响可忽略。

Abstract: Backdoor attacks are a significant threat to large language models (LLMs),
often embedded via public checkpoints, yet existing defenses rely on
impractical assumptions about trigger settings. To address this challenge, we
propose \ourmethod, a defense framework that requires no prior knowledge of
trigger settings. \ourmethod is based on the key observation that when
deliberately injecting known backdoors into an already-compromised model, both
existing unknown and newly injected backdoors aggregate in the representation
space. \ourmethod leverages this through a two-stage process: \textbf{first},
aggregating backdoor representations by injecting known triggers, and
\textbf{then}, performing recovery fine-tuning to restore benign outputs.
Extensive experiments across multiple LLM architectures demonstrate that: (I)
\ourmethod reduces the average Attack Success Rate to 4.41\% across multiple
benchmarks, outperforming existing baselines by 28.1\%$\sim$69.3\%$\uparrow$.
(II) Clean accuracy and utility are preserved within 0.5\% of the original
model, ensuring negligible impact on legitimate tasks. (III) The defense
generalizes across different types of backdoors, confirming its robustness in
practical deployment scenarios.

</details>


### [53] [On the Entity-Level Alignment in Crosslingual Consistency](https://arxiv.org/abs/2510.10280)
*Yihong Liu,Mingyang Wang,François Yvon,Hinrich Schütze*

Main category: cs.CL

TL;DR: 该论文研究了多语言大语言模型在跨语言事实回忆中的不一致性问题，发现实体对齐失败是主要原因，并提出两种通过整合英文实体翻译来改善对齐的方法。


<details>
  <summary>Details</summary>
Motivation: 多语言大语言模型在跨语言事实回忆中经常出现不一致性，但导致这种不一致性的因素尚不清楚。作者假设这可能源于实体对齐的失败。

Method: 通过实体级翻译任务评估对齐，并提出SubSub和SubInj两种方法，将英文实体翻译整合到多语言提示中。

Result: 研究发现一致性与对齐密切相关，提出的方法显著提高了事实回忆的准确性和一致性。

Conclusion: 通过模型内部枢纽语言处理强化实体表示对齐，为改善多语言事实预测提供了有效实用的策略。

Abstract: Multilingual large language models (LLMs) are expected to recall factual
knowledge consistently across languages. However, the factors that give rise to
such crosslingual consistency -- and its frequent failure -- remain poorly
understood. In this work, we hypothesize that these inconsistencies may arise
from failures in entity alignment, the process of mapping subject and object
entities into a shared conceptual space across languages. To test this, we
assess alignment through entity-level (subject and object) translation tasks,
and find that consistency is strongly correlated with alignment across all
studied models, with misalignment of subjects or objects frequently resulting
in inconsistencies. Building on this insight, we propose SubSub and SubInj, two
effective methods that integrate English translations of subjects into prompts
across languages, leading to substantial gains in both factual recall accuracy
and consistency. Finally, our mechanistic analysis reveals that these
interventions reinforce the entity representation alignment in the conceptual
space through model's internal pivot-language processing, offering effective
and practical strategies for improving multilingual factual prediction.

</details>


### [54] [MatryoshkaThinking: Recursive Test-Time Scaling Enables Efficient Reasoning](https://arxiv.org/abs/2510.10293)
*Hongwei Chen,Yishu Lei,Dan Zhang,Bo Ke,Danxiang Zhu,Xuyi Chen,Yuxiang Lu,Zhengjie Huang,Shikun Feng,Jingzhou He,Yu Sun,Hua Wu,Haifeng Wang*

Main category: cs.CL

TL;DR: 提出MatryoshkaThinking方法，在保持最先进性能的同时显著降低计算成本，仅需DeepConf 4%的计算量就能在AIME2025上获得99.79分。


<details>
  <summary>Details</summary>
Motivation: 现有测试时缩放方法如DeepConf虽然有效，但计算开销过大，需要更高效的解决方案。

Method: 递归利用模型内在的推理、验证和总结能力，增强正确解法的保留并减少Pass@k和Pass@1之间的差距。

Result: 在多个开源模型和挑战性多模态推理基准测试中验证了方法的有效性和通用性。

Conclusion: 为高级语言模型的高效可扩展测试时推理策略设计提供了新见解。

Abstract: Test-time scaling has emerged as a promising paradigm in language modeling,
wherein additional computational resources are allocated during inference to
enhance model performance. Recent approaches, such as DeepConf, have
demonstrated the efficacy of this strategy, however, they often incur
substantial computational overhead to achieve competitive results. In this
work, we propose MatryoshkaThinking, a novel method that significantly reduces
computational cost while maintaining state-of-the-art performance.
Specifically, MatryoshkaThinking attains a score of 99.79 on AIME2025 using
only 4% of the computation required by DeepConf. The core of our approach lies
in the recursive exploitation of the model's intrinsic capabilities in
reasoning, verification, and summarization, which collectively enhance the
retention of correct solutions and reduce the disparity between Pass@k and
Pass@1. Comprehensive evaluations across multiple open-source models and
challenging multi-modal reasoning benchmarks validate the effectiveness and
generality of our method. These findings offer new insights into the design of
efficient and scalable test-time inference strategies for advanced language
models.

</details>


### [55] [Are LLMs Empathetic to All? Investigating the Influence of Multi-Demographic Personas on a Model's Empathy](https://arxiv.org/abs/2510.10328)
*Ananya Malik,Nazanin Sabri,Melissa Karnaze,Mai Elsherief*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在不同人口属性组合的用户群体中表现出不平等的共情能力，交叉属性分析显示共情模式存在复杂变化，某些文化群体如儒家文化存在明显偏差。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs是否能在不同人口统计特征的用户群体中展现公平的共情能力，因为情感体验受到人口和文化背景的深刻影响。

Method: 提出一个框架研究LLMs的认知和情感共情如何随交叉人口属性变化，涵盖315个独特用户角色，结合年龄、文化和性别三个维度，对四个LLM进行交叉分析。

Result: 人口属性显著影响模型的共情回应，多重属性组合会减弱甚至逆转预期的共情模式，模型总体上反映现实世界的共情趋势，但对某些群体如儒家文化存在明显偏差。

Conclusion: 需要设计考虑人口多样性的共情感知LLMs，以促进更包容和公平的模型行为。

Abstract: Large Language Models' (LLMs) ability to converse naturally is empowered by
their ability to empathetically understand and respond to their users. However,
emotional experiences are shaped by demographic and cultural contexts. This
raises an important question: Can LLMs demonstrate equitable empathy across
diverse user groups? We propose a framework to investigate how LLMs' cognitive
and affective empathy vary across user personas defined by intersecting
demographic attributes. Our study introduces a novel intersectional analysis
spanning 315 unique personas, constructed from combinations of age, culture,
and gender, across four LLMs. Results show that attributes profoundly shape a
model's empathetic responses. Interestingly, we see that adding multiple
attributes at once can attenuate and reverse expected empathy patterns. We show
that they broadly reflect real-world empathetic trends, with notable
misalignments for certain groups, such as those from Confucian culture. We
complement our quantitative findings with qualitative insights to uncover model
behaviour patterns across different demographic groups. Our findings highlight
the importance of designing empathy-aware LLMs that account for demographic
diversity to promote more inclusive and equitable model behaviour.

</details>


### [56] [End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs](https://arxiv.org/abs/2510.10329)
*Nam Luu,Ondřej Bojar*

Main category: cs.CL

TL;DR: 本文提出了一种结合预训练语音编码器和大型语言模型的端到端架构，用于同时执行自动语音识别和语音翻译任务。在英德语言对上的实验表明，该模型在翻译质量上超越了SeamlessM4T，并能与Whisper+NLLB级联系统相媲美。


<details>
  <summary>Details</summary>
Motivation: 探索将预训练语音编码器与大型语言模型结合的端到端架构，以同时处理自动语音识别和语音翻译任务，克服传统级联系统的局限性。

Method: 采用预训练语音编码器与大型语言模型相结合的端到端架构，同时执行ASR和ST任务。

Result: 在英德语言对实验中，最佳模型在COMET^{DA}_{22}指标上比SeamlessM4T提升8%，且性能与Whisper+NLLB级联系统相当。

Conclusion: 结合预训练语音编码器和LLM的端到端架构能够有效提升语音翻译性能，在保持端到端优势的同时达到级联系统的性能水平。

Abstract: Speech Translation (ST) is a machine translation task that involves
converting speech signals from one language to the corresponding text in
another language; this task has two different approaches, namely the
traditional cascade and the more recent end-to-end. This paper explores a
combined end-to-end architecture of pre-trained speech encoders and Large
Language Models (LLMs) for performing both Automatic Speech Recognition (ASR)
and ST simultaneously. Experiments with the English-to-German language pair
show that our best model not only can achieve better translation results than
SeamlessM4T, a large foundational end-to-end, multi-modal translation model,
but can also match the performance of a cascaded system with Whisper and NLLB,
with up to a score gain of 8% in $\text{COMET}^{\text{DA}}_{22}$ metric.

</details>


### [57] [ASC analyzer: A Python package for measuring argument structure construction usage in English texts](https://arxiv.org/abs/2510.10384)
*Hakyung Sung,Kristopher Kyle*

Main category: cs.CL

TL;DR: 开发ASC分析器Python包，用于自动标注论元结构构式并计算50个指标，分析其与二语写作分数的关系


<details>
  <summary>Details</summary>
Motivation: 论元结构构式是分析二语能力的理论基础，但缺乏可扩展的系统性测量工具

Method: 开发公开可用的Python包ASC分析器，自动标注ASC并计算多样性、比例、频率和ASC-动词词元关联强度等50个指标

Result: 通过双变量和多变量分析，验证了ASC指标与二语写作分数之间的关系

Conclusion: ASC分析器填补了二语能力测量工具的空缺，为系统分析论元结构构式提供了有效工具

Abstract: Argument structure constructions (ASCs) offer a theoretically grounded lens
for analyzing second language (L2) proficiency, yet scalable and systematic
tools for measuring their usage remain limited. This paper introduces the ASC
analyzer, a publicly available Python package designed to address this gap. The
analyzer automatically tags ASCs and computes 50 indices that capture
diversity, proportion, frequency, and ASC-verb lemma association strength. To
demonstrate its utility, we conduct both bivariate and multivariate analyses
that examine the relationship between ASC-based indices and L2 writing scores.

</details>


### [58] [RefusalBench: Generative Evaluation of Selective Refusal in Grounded Language Models](https://arxiv.org/abs/2510.10390)
*Aashiq Muhamed,Leonardo F. R. Ribeiro,Markus Dreyer,Virginia Smith,Mona T. Diab*

Main category: cs.CL

TL;DR: 该论文研究了RAG系统中语言模型基于有缺陷上下文选择性拒绝回答的能力，发现即使是前沿模型在此方面也存在显著问题，拒绝准确率在多文档任务中低于50%。作者提出了RefusalBench评估框架，通过语言扰动生成诊断测试案例，揭示了系统性的失败模式。


<details>
  <summary>Details</summary>
Motivation: RAG系统中语言模型基于有缺陷上下文选择性拒绝回答的能力对安全性至关重要，但目前仍是显著失败点，现有静态基准无法可靠评估这一能力。

Method: 引入RefusalBench生成方法，通过程序化创建诊断测试案例，采用176种不同扰动策略，涵盖六类信息不确定性和三个强度级别。评估了30多个模型。

Result: 发现拒绝能力包含可分离的检测和分类技能，模型规模或扩展推理无法提升性能。拒绝准确率在多文档任务中低于50%，模型表现出危险的过度自信或过度谨慎。

Conclusion: 选择性拒绝是可训练、对齐敏感的能力，提供了明确的改进路径。发布了RefusalBench-NQ和RefusalBench-GaRAGe两个基准以及完整的生成框架，支持对此关键能力的持续动态评估。

Abstract: The ability of language models in RAG systems to selectively refuse to answer
based on flawed context is critical for safety, yet remains a significant
failure point. Our large-scale study reveals that even frontier models struggle
in this setting, with refusal accuracy dropping below 50% on multi-document
tasks, while exhibiting either dangerous overconfidence or overcaution. Static
benchmarks fail to reliably evaluate this capability, as models exploit
dataset-specific artifacts and memorize test instances. We introduce
RefusalBench, a generative methodology that programmatically creates diagnostic
test cases through controlled linguistic perturbation. Our framework employs
176 distinct perturbation strategies across six categories of informational
uncertainty and three intensity levels. Evaluation of over 30 models uncovers
systematic failure patterns: refusal comprises separable detection and
categorization skills, and neither scale nor extended reasoning improves
performance. We find that selective refusal is a trainable, alignment-sensitive
capability, offering a clear path for improvement. We release two benchmarks --
RefusalBench-NQ (single document) and RefusalBench-GaRAGe (multi-document) --
and our complete generation framework to enable continued, dynamic evaluation
of this critical capability.

</details>


### [59] [AssoMem: Scalable Memory QA with Multi-Signal Associative Retrieval](https://arxiv.org/abs/2510.10397)
*Kai Zhang,Xinyuan Zhang,Ejaz Ahmed,Hongda Jiang,Caleb Kumar,Kai Sun,Zhaojiang Lin,Sanat Sharma,Shereen Oraby,Aaron Colak,Ahmed Aly,Anuj Kumar,Xiaozhong Liu,Xin Luna Dong*

Main category: cs.CL

TL;DR: AssoMem是一个新颖的关联记忆图框架，通过自动提取线索将对话话语锚定，利用多维度检索信号（相关性、重要性和时间对齐）进行自适应融合，在大规模记忆检索中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决大规模记忆增强AI助手在相似性密集场景中准确回忆的核心挑战，现有方法主要依赖语义距离进行检索，而人类通过关联方式链接信息。

Method: 构建关联记忆图，将对话话语锚定到自动提取的线索上，集成多维度检索信号（相关性、重要性和时间对齐），采用自适应互信息驱动的融合策略。

Result: 在三个基准测试和新引入的MeetingQA数据集上的广泛实验表明，AssoMem始终优于最先进的基线方法。

Conclusion: AssoMem在上下文感知记忆回忆方面具有优越性，验证了该框架的有效性。

Abstract: Accurate recall from large scale memories remains a core challenge for memory
augmented AI assistants performing question answering (QA), especially in
similarity dense scenarios where existing methods mainly rely on semantic
distance to the query for retrieval. Inspired by how humans link information
associatively, we propose AssoMem, a novel framework constructing an
associative memory graph that anchors dialogue utterances to automatically
extracted clues. This structure provides a rich organizational view of the
conversational context and facilitates importance aware ranking. Further,
AssoMem integrates multi-dimensional retrieval signals-relevance, importance,
and temporal alignment using an adaptive mutual information (MI) driven fusion
strategy. Extensive experiments across three benchmarks and a newly introduced
dataset, MeetingQA, demonstrate that AssoMem consistently outperforms SOTA
baselines, verifying its superiority in context-aware memory recall.

</details>


### [60] [STEAM: A Semantic-Level Knowledge Editing Framework for Large Language Models](https://arxiv.org/abs/2510.10398)
*Geunyeong Jeong,Juoh Sun,Seonghee Lee,Harksoo Kim*

Main category: cs.CL

TL;DR: STEAM是一个语义级知识编辑框架，通过潜在空间对齐增强更新知识在模型知识结构中的整合，改善编辑后知识的推理能力和语义连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑方法主要关注token级似然优化，缺乏语义连贯性考虑，导致编辑后的知识在潜在空间中形成孤立的残差流，绕过自然推理过程。

Method: STEAM首先识别目标表示作为更新事实关联的语义锚点，然后在优化过程中通过对齐损失引导编辑事实的内部表示向这些锚点对齐。

Result: 实验结果表明STEAM提高了模型对编辑知识的推理能力，增强了语义连贯性。

Conclusion: 潜在空间对齐对于可靠和连贯的知识编辑至关重要。

Abstract: Large Language Models store extensive factual knowledge acquired during
large-scale pre-training. However, this knowledge is inherently static,
reflecting only the state of the world at the time of training. Knowledge
editing has emerged as a promising solution for updating outdated or incorrect
facts without full retraining. However, most existing locate-and-edit methods
primarily focus on token-level likelihood optimization without addressing
semantic coherence. Our analysis reveals that such edited knowledge is often
encoded as isolated residual streams in the model's latent space, distinct from
pre-existing knowledge and bypassing natural reasoning process. To address
this, we propose \textsc{Steam}, a semantic-level knowledge editing framework
that enhances integration of updated knowledge into the model's knowledge
structure. \textsc{Steam} first identifies target representations as semantic
anchors for the updated factual association, then guides the internal
representation of the edited fact towards these anchors through an alignment
loss during optimization. Experimental results demonstrate that \textsc{Steam}
improves model's ability to reason with edited knowledge and enhances semantic
coherence, underscoring the importance of latent-space alignment for reliable
and coherent knowledge editing. The code is available at
https://github.com/GY-Jeong/STEAM.

</details>


### [61] [LONGQAEVAL: Designing Reliable Evaluations of Long-Form Clinical QA under Resource Constraints](https://arxiv.org/abs/2510.10415)
*Federica Bologna,Tiffany Pan,Matthew Wilkens,Yue Guo,Lucy Lu Wang*

Main category: cs.CL

TL;DR: LongQAEval是一个用于评估长形式临床问答系统的框架，通过比较粗粒度答案级和细粒度句子级评估方法，发现在正确性维度上细粒度标注能提高标注者间一致性，而在相关性维度上粗粒度标注更好，安全性判断则始终不一致。


<details>
  <summary>Details</summary>
Motivation: 评估长形式临床问答系统既需要医学专业知识又资源密集，且难以在长文本上获得一致的人类判断。

Method: 基于医生对300个真实患者问题的标注，比较了粗粒度答案级和细粒度句子级评估方法，分析了正确性、相关性和安全性三个维度的标注者间一致性。

Result: 不同维度的标注者间一致性表现不同：细粒度标注在正确性维度上一致性更高，粗粒度标注在相关性维度上更好，安全性判断始终不一致。仅标注一小部分句子即可获得与粗粒度标注相当的可靠性。

Conclusion: 提出了针对有限资源和高专业知识环境的评估建议，包括根据评估维度选择合适的标注粒度，以及通过部分句子标注来降低成本和努力。

Abstract: Evaluating long-form clinical question answering (QA) systems is
resource-intensive and challenging: accurate judgments require medical
expertise and achieving consistent human judgments over long-form text is
difficult. We introduce LongQAEval, an evaluation framework and set of
evaluation recommendations for limited-resource and high-expertise settings.
Based on physician annotations of 300 real patient questions answered by
physicians and LLMs, we compare coarse answer-level versus fine-grained
sentence-level evaluation over the dimensions of correctness, relevance, and
safety. We find that inter-annotator agreement (IAA) varies by dimension:
fine-grained annotation improves agreement on correctness, coarse improves
agreement on relevance, and judgments on safety remain inconsistent.
Additionally, annotating only a small subset of sentences can provide
reliability comparable to coarse annotations, reducing cost and effort.

</details>


### [62] [Do Audio LLMs Really LISTEN, or Just Transcribe? Measuring Lexical vs. Acoustic Emotion Cues Reliance](https://arxiv.org/abs/2510.10444)
*Jingyi Chen,Zhimeng Guo,Jiyun Chun,Pichao Wang,Andrew Perrault,Micha Elsner*

Main category: cs.CL

TL;DR: LISTEN基准测试显示，当前大型音频语言模型在情感理解中主要依赖词汇线索而非声学信息，表现出'转录而非聆听'的行为模式。


<details>
  <summary>Details</summary>
Motivation: 研究旨在厘清大型音频语言模型是否真正处理声学信息，还是主要依赖词汇内容来理解情感。

Method: 开发了LISTEN基准测试，通过控制词汇和声学线索的对比设置来评估6个先进模型的情感理解能力。

Result: 模型在词汇线索中性或缺失时预测'中性'，在线索对齐时改进有限，在冲突时无法区分不同情感，在副语言情境中表现接近随机。

Conclusion: 当前LALMs过度依赖词汇语义而未能充分利用声学线索，LISTEN为评估多模态模型的情感理解提供了原则性框架。

Abstract: Understanding emotion from speech requires sensitivity to both lexical and
acoustic cues. However, it remains unclear whether large audio language models
(LALMs) genuinely process acoustic information or rely primarily on lexical
content. We present LISTEN (Lexical vs. Acoustic Speech Test for Emotion in
Narratives), a controlled benchmark designed to disentangle lexical reliance
from acoustic sensitivity in emotion understanding. Across evaluations of six
state-of-the-art LALMs, we observe a consistent lexical dominance. Models
predict "neutral" when lexical cues are neutral or absent, show limited gains
under cue alignment, and fail to classify distinct emotions under cue conflict.
In paralinguistic settings, performance approaches chance. These results
indicate that current LALMs largely "transcribe" rather than "listen," relying
heavily on lexical semantics while underutilizing acoustic cues. LISTEN offers
a principled framework for assessing emotion understanding in multimodal
models.

</details>


### [63] [RECON: Reasoning with Condensation for Efficient Retrieval-Augmented Generation](https://arxiv.org/abs/2510.10448)
*Zhichao Xu,Minheng Wang,Yawei Wang,Wenqian Ye,Yuntao Du,Yunpu Ma,Yijun Tian*

Main category: cs.CL

TL;DR: RECON是一个集成显式摘要模块的检索增强生成框架，通过压缩证据减少上下文长度35%，提升训练速度和推理延迟，同时显著提高QA任务性能。


<details>
  <summary>Details</summary>
Motivation: 基于强化学习的检索增强生成系统存在上下文管理效率低下的问题，冗长嘈杂的检索文档会增加成本并降低性能。

Method: 采用两阶段训练过程：在QA数据集上进行相关性预训练，然后通过多方面蒸馏从专有LLM中学习，确保事实性和清晰度。

Result: 将3B模型的平均EM分数提升14.5%，7B模型提升3.0%，在多跳QA任务中表现尤为突出。

Conclusion: 学习型上下文压缩对于构建实用、可扩展和高性能的RAG系统至关重要。

Abstract: Retrieval-augmented generation (RAG) systems trained using reinforcement
learning (RL) with reasoning are hampered by inefficient context management,
where long, noisy retrieved documents increase costs and degrade performance.
We introduce RECON (REasoning with CONdensation), a framework that integrates
an explicit summarization module to compress evidence within the reasoning
loop. Our summarizer is trained via a two-stage process: relevance pretraining
on QA datasets, followed by multi-aspect distillation from proprietary LLMs to
ensure factuality and clarity. Integrated into the Search-R1 pipeline, RECON
reduces total context length by 35\%, leading to improved training speed and
inference latency, while simultaneously improving RAG performance on downstream
QA benchmarks. Notably, it boosts the average EM score of the 3B model by
14.5\% and the 7B model by 3.0\%, showing particular strength in multi-hop QA.
RECON demonstrates that learned context compression is essential for building
practical, scalable, and performant RAG systems. Our code implementation is
made available at https://github.com/allfornancy/RECON.

</details>


### [64] [Steering Over-refusals Towards Safety in Retrieval Augmented Generation](https://arxiv.org/abs/2510.10452)
*Utsav Maskey,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 论文分析了LLMs在RAG中的过度拒绝问题，构建了RagRefuse基准，并提出SafeRAG-Steering方法来减少过度拒绝。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的安全对齐会导致过度拒绝问题，即在RAG中即使对良性请求也会因为安全过滤器而拒绝。需要分析查询意图和检索上下文如何影响拒绝行为。

Method: 构建RagRefuse基准，涵盖医疗、化学和开放领域，控制上下文污染模式和大小。提出SafeRAG-Steering方法，通过模型中心的嵌入干预在推理时引导输出到安全区域。

Result: 分析显示上下文安排/污染、查询和上下文领域、有害文本密度都会触发对良性查询的拒绝，效果取决于模型特定的对齐选择。SafeRAG-Steering能减少污染RAG管道中的过度拒绝，同时保持合理的拒绝。

Conclusion: RAG中的过度拒绝问题受多种因素影响，SafeRAG-Steering方法能有效缓解这一问题，在保持安全性的同时提高模型的实用性。

Abstract: Safety alignment in large language models (LLMs) induces over-refusals --
where LLMs decline benign requests due to aggressive safety filters. We analyze
this phenomenon in retrieval-augmented generation (RAG), where both the query
intent and retrieved context properties influence refusal behavior. We
construct RagRefuse, a domain-stratified benchmark spanning medical, chemical,
and open domains, pairing benign and harmful queries with controlled context
contamination patterns and sizes. Our analysis shows that context arrangement /
contamination, domain of query and context, and harmful-text density trigger
refusals even on benign queries, with effects depending on model-specific
alignment choices. To mitigate over-refusals, we introduce
\textsc{SafeRAG-Steering}, a model-centric embedding intervention that steers
the embedding regions towards the confirmed safe, non-refusing output regions
at inference time. This reduces over-refusals in contaminated RAG pipelines
while preserving legitimate refusals.

</details>


### [65] [End-to-end Speech Recognition with similar length speech and text](https://arxiv.org/abs/2510.10453)
*Peng Fan,Wenping Wang,Fei Deng*

Main category: cs.CL

TL;DR: 提出两种对齐方法（TIL和AXE Loss）和帧融合技术，在语音与文本长度相近的情况下显著减少帧数并提升ASR性能


<details>
  <summary>Details</summary>
Motivation: 解决语音长度与文本长度不匹配时传统CTC方法无法有效对齐的问题，特别是在语音与文本长度相近的场景下

Method: 引入时间独立性损失（TIL）和基于编辑距离的对齐交叉熵损失（AXE），结合关键帧与上下文2帧的加权帧融合

Result: 在AISHELL-1和AISHELL-2数据集子集上，提出的方法优于先前工作，帧数减少至少86%

Conclusion: 所提出的对齐方法和帧融合技术能有效解决语音-文本长度对齐问题，显著提升ASR性能并大幅减少计算复杂度

Abstract: The mismatch of speech length and text length poses a challenge in automatic
speech recognition (ASR). In previous research, various approaches have been
employed to align text with speech, including the utilization of Connectionist
Temporal Classification (CTC). In earlier work, a key frame mechanism (KFDS)
was introduced, utilizing intermediate CTC outputs to guide downsampling and
preserve keyframes, but traditional methods (CTC) failed to align speech and
text appropriately when downsampling speech to a text-similar length. In this
paper, we focus on speech recognition in those cases where the length of speech
aligns closely with that of the corresponding text. To address this issue, we
introduce two methods for alignment: a) Time Independence Loss (TIL) and b)
Aligned Cross Entropy (AXE) Loss, which is based on edit distance. To enhance
the information on keyframes, we incorporate frame fusion by applying weights
and summing the keyframe with its context 2 frames. Experimental results on
AISHELL-1 and AISHELL-2 dataset subsets show that the proposed methods
outperform the previous work and achieve a reduction of at least 86\% in the
number of frames.

</details>


### [66] [Rethinking LLM Evaluation: Can We Evaluate LLMs with 200x Less Data?](https://arxiv.org/abs/2510.10457)
*Shaobo Wang,Cong Wang,Wenjie Fu,Yue Min,Mingquan Feng,Isabel Guan,Xuming Hu,Conghui He,Cunxiang Wang,Kexin Yang,Xingzhang Ren,Fei Huang,Dayiheng Liu,Linfeng Zhang*

Main category: cs.CL

TL;DR: 提出EssenceBench框架，通过遗传算法进行粗到细的基准压缩，显著减少样本数量同时保持模型排名准确性。


<details>
  <summary>Details</summary>
Motivation: 随着对模型能力评估需求的增加，基准套件规模显著增长，但缺乏系统框架来有效整合冗余减少和性能预测方法，确保预测准确性和排名一致性。

Method: 首先进行样本级冗余分析，识别并消除高度相似样本；将基准压缩构建为分数重构优化问题；提出EssenceBench框架，利用迭代遗传算法结合基于适应度的子集搜索和基于归因的样本搜索。

Result: 在HellaSwag基准上，使用25倍更少样本时所有模型排名变化在5%以内；使用200倍更少样本时达到95%排名保持率，变化在5%以内。

Conclusion: EssenceBench框架在基准压缩方面优于先前方法，具有更低的重构误差和显著更高的效率，能够大幅减少评估成本同时保持模型排名准确性。

Abstract: As the demand for comprehensive evaluations of diverse model capabilities
steadily increases, benchmark suites have correspondingly grown significantly
in scale. Despite notable advances in redundancy reduction and subset-level
performance prediction, a systematic framework that effectively integrates
these methods to ensure both prediction accuracy and ranking consistency is
still largely elusive. In this paper, we first perform a sample-level analysis
of benchmark redundancy and identify several highly similar samples that can be
eliminated. Besides, we frame benchmark compression as an optimization problem
with the aim of score reconstruction. Building on these, we then propose
EssenceBench, a coarse-to-fine framework utilizing an iterative Genetic
Algorithm (GA), which takes the advantages of fitness-based subset search and
attribution-based sample search. Compared to previous methods, our approach
yields superior compression results with lower reconstruction error and
markedly higher efficiency. In particular, on the HellaSwag benchmark (10K
samples), our method preserves the ranking of all models shifting within 5%
using 25x fewer samples, and achieves 95% ranking preservation shifting within
5% using only 200x fewer samples.

</details>


### [67] [NIM: Neuro-symbolic Ideographic Metalanguage for Inclusive Communication](https://arxiv.org/abs/2510.10459)
*Prawaal Sharma,Poonam Goyal,Navneet Goyal,Vidisha Sharma*

Main category: cs.CL

TL;DR: 提出了一种新颖的通用表意元语言，通过神经符号AI方法将复杂概念分解为原子概念，帮助低学历人群跨越数字鸿沟。


<details>
  <summary>Details</summary>
Motivation: 解决低学术素养人群在数字沟通中面临的障碍，弥合数字鸿沟，让数字通信更加包容和可访问。

Method: 采用神经符号AI方法，结合基于神经的大语言模型和基于自然语义元语言理论的符号知识启发式，通过人本协作方式与200多名半文盲参与者共同定义问题、选择表意符号并验证系统。

Result: 系统实现了超过80%的语义可理解性，具有易学的学习曲线和普遍适应性，能有效服务于教育程度有限的弱势群体。

Conclusion: 该表意元语言框架成功跨越了学术、语言和文化边界，为低学历人群提供了有效的数字沟通解决方案。

Abstract: Digital communication has become the cornerstone of modern interaction,
enabling rapid, accessible, and interactive exchanges. However, individuals
with lower academic literacy often face significant barriers, exacerbating the
"digital divide". In this work, we introduce a novel, universal ideographic
metalanguage designed as an innovative communication framework that transcends
academic, linguistic, and cultural boundaries. Our approach leverages
principles of Neuro-symbolic AI, combining neural-based large language models
(LLMs) enriched with world knowledge and symbolic knowledge heuristics grounded
in the linguistic theory of Natural Semantic Metalanguage (NSM). This enables
the semantic decomposition of complex ideas into simpler, atomic concepts.
Adopting a human-centric, collaborative methodology, we engaged over 200
semi-literate participants in defining the problem, selecting ideographs, and
validating the system. With over 80\% semantic comprehensibility, an accessible
learning curve, and universal adaptability, our system effectively serves
underprivileged populations with limited formal education.

</details>


### [68] [FML-bench: A Benchmark for Automatic ML Research Agents Highlighting the Importance of Exploration Breadth](https://arxiv.org/abs/2510.10472)
*Qiran Zou,Hou Hei Lam,Wenhao Zhao,Yiming Tang,Tingting Chen,Samson Yu,Tianyi Zhang,Chang Liu,Xiangyang Ji,Dianbo Liu*

Main category: cs.CL

TL;DR: FML-bench是一个评估自动机器学习研究代理的基准，专注于8个不同的基础机器学习研究问题，减少编码负担，强调基础问题而非特定用例，并提供统一的评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有的评估基准过度强调工程方面而忽视学术严谨性，任务多样性有限，过于关注应用导向任务而非基础研究问题，且难以扩展到现实研究环境。

Method: 引入FML-bench基准，包含8个不同的基础机器学习研究问题，减少编码负担，强调基础问题，提供高任务多样性，并可扩展到真实世界的机器学习GitHub仓库。同时提出包含五个互补指标的统⼀评估框架。

Result: 评估了最先进的自动研究代理，发现采用广泛研究探索策略的代理优于那些专注于狭窄但深入探索的代理。

Conclusion: 强调探索的广度可能比仅关注增量改进带来更有效的研究成果。FML-bench基准可用于全面评估自动机器学习研究代理的科学能力。

Abstract: Large language models (LLMs) have sparked growing interest in automatic
machine learning research agents. Among them, agents capable of autonomously
proposing ideas and conducting machine learning experiments are particularly
promising, as they maximize research automation and accelerate scientific
progress by iteratively refining ideas based on experimental results. However,
comprehensively evaluating such agents remains challenging. Existing benchmarks
tend to overemphasize engineering aspects while neglecting academic rigor,
creating barriers that obscure a clear assessment of an agent's scientific
capabilities in machine learning research. They also suffer from limited task
diversity, an overemphasis on application-oriented tasks over fundamental
research problems, and limited scalability to realistic research settings. To
address these limitations, we introduce FML-bench, a benchmark designed to
evaluate automatic machine learning research agents on 8 diverse and
fundamental machine learning research problems. It reduces coding burden,
emphasizes fundamental problems rather than specific use cases, offers high
task diversity, and is extensible to real-world machine learning GitHub
repositories. Furthermore, we present a unified evaluation framework with five
complementary metrics, designed to comprehensively assess agent performance on
our benchmark. We evaluate state-of-the-art automatic research agents on
FML-bench, and find that agents employing broad research exploration strategies
outperform those focusing on narrow but deep exploration. These findings
suggest that emphasizing the breadth of exploration may lead to more effective
research outcomes than focusing solely on incremental refinement. Our benchmark
is available at https://github.com/qrzou/FML-bench.

</details>


### [69] [When or What? Understanding Consumer Engagement on Digital Platforms](https://arxiv.org/abs/2510.10474)
*Jingyi Wu,Junying Liang*

Main category: cs.CL

TL;DR: 该研究通过分析TED演讲数据发现，内容创作者与观众偏好存在持续不匹配，且时间动态对观众参与度的影响比主题内容更重要，挑战了内容特征是流行度主要驱动因素的假设。


<details>
  <summary>Details</summary>
Motivation: 理解数字服务经济中内容流行度的驱动因素，因为创作者经常误判观众真正重视的内容，而现有研究主要关注内容特征的作用。

Method: 应用潜在狄利克雷分配(LDA)模型分析大量TED演讲语料，将平台视为数字服务提供案例，比较创作者的专题供给与观众参与度表达的需求。

Result: 识别出创作者供给与消费者偏好之间的持续不匹配，纵向分析显示时间动态对消费者参与度的影响比专题内容更强。

Conclusion: 研究挑战了内容特征是流行度主要驱动因素的假设，强调了时机和情境因素在塑造消费者响应中的重要性，为数字平台上的消费者注意力动态提供了新见解。

Abstract: Understanding what drives popularity is critical in today's digital service
economy, where content creators compete for consumer attention. Prior studies
have primarily emphasized the role of content features, yet creators often
misjudge what audiences actually value. This study applies Latent Dirichlet
Allocation (LDA) modeling to a large corpus of TED Talks, treating the platform
as a case of digital service provision in which creators (speakers) and
consumers (audiences) interact. By comparing the thematic supply of creators
with the demand expressed in audience engagement, we identify persistent
mismatches between producer offerings and consumer preferences. Our
longitudinal analysis further reveals that temporal dynamics exert a stronger
influence on consumer engagement than thematic content, suggesting that when
content is delivered may matter more than what is delivered. These findings
challenge the dominant assumption that content features are the primary drivers
of popularity and highlight the importance of timing and contextual factors in
shaping consumer responses. The results provide new insights into consumer
attention dynamics on digital platforms and carry practical implications for
marketers, platform managers, and content creators seeking to optimize audience
engagement strategies.

</details>


### [70] [Assessing Large Language Models for Structured Medical Order Extraction](https://arxiv.org/abs/2510.10475)
*A H M Rezaul Karim,Ozlem Uzuner*

Main category: cs.CL

TL;DR: 使用LLaMA-4 17B模型通过少量示例提示工程提取医疗指令，在MEDIQA-OE 2025任务中排名第5，展示了通用LLM在临床NLP任务中的潜力。


<details>
  <summary>Details</summary>
Motivation: 医疗指令提取对结构化临床信息、支持决策制定和下游应用至关重要，但指令可能嵌入在多种来源中，需要从对话转录中提取结构化信息。

Method: 使用通用指令调优的LLaMA-4 17B模型，不进行领域特定微调，仅通过单个上下文示例指导的少量提示配置。

Result: 在17个参赛团队的105份提交中排名第5，平均F1分数37.76，在原因和来源准确性方面有显著改进。

Conclusion: 大型非领域特定LLM结合有效的提示工程可以作为专业临床NLP任务的强大、可扩展基线。

Abstract: Medical order extraction is essential for structuring actionable clinical
information, supporting decision-making, and enabling downstream applications
such as documentation and workflow automation. Orders may be embedded in
diverse sources, including electronic health records, discharge summaries, and
multi-turn doctor-patient dialogues, and can span categories such as
medications, laboratory tests, imaging studies, and follow-up actions. The
MEDIQA-OE 2025 shared task focuses on extracting structured medical orders from
extended conversational transcripts, requiring the identification of order
type, description, reason, and provenance. We present the MasonNLP submission,
which ranked 5th among 17 participating teams with 105 total submissions. Our
approach uses a general-purpose, instruction-tuned LLaMA-4 17B model without
domain-specific fine-tuning, guided by a single in-context example. This
few-shot configuration achieved an average F1 score of 37.76, with notable
improvements in reason and provenance accuracy. These results demonstrate that
large, non-domain-specific LLMs, when paired with effective prompt engineering,
can serve as strong, scalable baselines for specialized clinical NLP tasks.

</details>


### [71] [UltraLLaDA: Scaling the Context Length to 128K for Diffusion Large Language Models](https://arxiv.org/abs/2510.10481)
*Guangxin He,Shen Nie,Fengqi Zhu,Yuankang Zhao,Tianyi Bai,Ran Yan,Jie Fu,Chongxuan Li,Binhang Yuan*

Main category: cs.CL

TL;DR: 本文提出了一种通过后训练技术扩展扩散LLMs上下文窗口的方法，无需从头训练。通过改进RoPE位置编码和优化掩码策略，实现了128K令牌的上下文窗口，在长上下文任务中显著优于无训练基线。


<details>
  <summary>Details</summary>
Motivation: 扩散LLMs在长上下文行为方面的研究尚不充分，现有方法需要从头训练才能扩展上下文窗口，效率低下。本文旨在探索通过高效的后训练技术来扩展扩散LLMs的上下文窗口。

Method: 1. 对标准RoPE位置编码进行简单修改以适应扩散过程的概率建模特性；2. 比较后训练中的掩码策略并分析其对优化稳定性和长距离回忆的影响；3. 基于这些洞察开发UltraLLaDA模型。

Result: 开发了具有128K令牌上下文窗口的UltraLLaDA模型，在长上下文任务中的实证评估显示，其性能显著优于无需训练的基线方法。

Conclusion: 特殊的位置扩展技术是扩展扩散LLMs上下文窗口的关键杠杆，为从业者通过高效后训练实现128K规模上下文提供了实用指导。

Abstract: Diffusion LLMs have attracted growing interest, with plenty of recent work
emphasizing their great potential in various downstream tasks; yet the
long-context behavior of diffusion LLMs remains largely uncharted. We present a
case study of post-training techniques for extending the context window of
diffusion LLMs (i.e., LLaDA) without retraining from scratch. We show that a
simple modification to the standard Rotary Positional Embeddings (RoPE)
extension effectively accommodates the probabilistic modeling inherent in the
diffusion process, enabling stable scaling to longer context ranges. We further
compare masking strategies used during post-training and analyze their impact
on optimization stability and long-range recall. Instantiating these insights,
we introduce UltraLLaDA, a diffusion LLM with a 128K-token context window that,
in our empirical evaluation on long-context tasks, significantly outperforms
training-free baselines. Our experimental results highlight the special
positional extension as a key lever for scaling diffusion LLMs to extended
contexts and offer practical guidance for practitioners seeking 128K-scale
context via efficient post-training.

</details>


### [72] [VOLTAGE: A Versatile Contrastive Learning based OCR Methodology for ultra low-resource scripts through Auto Glyph Feature Extraction](https://arxiv.org/abs/2510.10490)
*Prawaal Sharma,Poonam Goyal,Vidisha Sharma,Navneet Goyal*

Main category: cs.CL

TL;DR: 提出VOLTAGE方法，一种基于对比学习的OCR技术，用于解决低资源语言的数字化保存问题，在Takri文字上达到95%印刷体和87%手写体识别准确率。


<details>
  <summary>Details</summary>
Motivation: 全球7000种语言中有2500种濒危，语言消失导致传统智慧、民间文学和社区本质的丧失。低资源语言缺乏无监督OCR方法是阻碍其数字包容的主要原因。

Method: 使用对比学习OCR方法，结合自动字形特征推荐进行聚类标注，通过图像变换和生成对抗网络增强标注数据的多样性和数量。

Result: 在Takri文字上实现95%的印刷体识别准确率和87%的手写体识别准确率，并在其他印度文字上验证了方法的通用性。

Conclusion: VOLTAGE方法能有效解决低资源语言的OCR问题，有助于防止语言灭绝，促进数字包容。

Abstract: UNESCO has classified 2500 out of 7000 languages spoken worldwide as
endangered. Attrition of a language leads to loss of traditional wisdom, folk
literature, and the essence of the community that uses it. It is therefore
imperative to bring digital inclusion to these languages and avoid its
extinction. Low resource languages are at a greater risk of extinction. Lack of
unsupervised Optical Character Recognition(OCR) methodologies for low resource
languages is one of the reasons impeding their digital inclusion. We propose
VOLTAGE - a contrastive learning based OCR methodology, leveraging auto-glyph
feature recommendation for cluster-based labelling. We augment the labelled
data for diversity and volume using image transformations and Generative
Adversarial Networks. Voltage has been designed using Takri - a family of
scripts used in 16th to 20th century in the Himalayan regions of India. We
present results for Takri along with other Indic scripts (both low and high
resource) to substantiate the universal behavior of the methodology. An
accuracy of 95% for machine printed and 87% for handwritten samples on Takri
script has been achieved. We conduct baseline and ablation studies along with
building downstream use cases for Takri, demonstrating the usefulness of our
work.

</details>


### [73] [Merlin's Whisper: Enabling Efficient Reasoning in LLMs via Black-box Adversarial Prompting](https://arxiv.org/abs/2510.10528)
*Heming Xia,Cunxiao Du,Rui Li,Chak Tou Leong,Yongqi Li,Wenjie Li*

Main category: cs.CL

TL;DR: AdvPrompt通过黑盒对抗提示减少大型推理模型的过度思考，在保持准确性的同时显著降低响应长度和计算开销。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理任务中表现优异，但冗长的推理过程带来了巨大的计算成本和延迟，阻碍了实际部署。

Method: 提出AdvPrompt迭代优化框架，从多个角度生成高质量对抗提示，将开源和闭源模型都视为黑盒通信器来获取简洁响应。

Result: 在多个基准测试中，AdvPrompt显著减少token使用量：Qwen3模型在GSM8K上响应长度减少3倍，四个基准平均减少约40%的token；Claude-3.7和Gemini-2.5在MATH-500上分别减少35%和47%的token使用。

Conclusion: AdvPrompt展示了黑盒提示作为增强大型推理模型效率的实用有效策略，具有良好的跨模型规模和家族的泛化能力。

Abstract: Large reasoning models (LRMs) have demonstrated remarkable proficiency in
tackling complex reasoning tasks through step-by-step thinking. However, such a
lengthy reasoning process incurs substantial computational and latency
overheads, hindering the practical deployment of these models. In this work, we
present a new perspective on mitigating overthinking in LRMs via black-box
adversarial prompting. By treating both open-source LRMs and closed-source APIs
as black-box communicators, we investigate how to elicit concise responses
without sacrificing accuracy. We introduce AdvPrompt, an iterative refinement
framework that generates high-quality adversarial prompts from diverse
perspectives. Experiments across multiple benchmarks demonstrate that AdvPrompt
consistently reduces token usage while preserving performance. Notably,
AdvPrompt achieves a 3x reduction in average response length on simple GSM8K
questions for the Qwen3 model series, and delivers an average ~40% token
reduction across four benchmarks. For closed-source APIs, AdvPrompt reduces
token usage on MATH-500 by 35% for Claude-3.7 and 47% for Gemini-2.5. Further
analysis reveals the generalizability of AdvPrompt across various model scales
and families, underscoring the potential of black-box prompting as a practical
and effective strategy for enhancing LRM efficiency.

</details>


### [74] [Detecting Hallucinations in Authentic LLM-Human Interactions](https://arxiv.org/abs/2510.10539)
*Yujie Ren,Niklas Gruhlke,Anne Lauscher*

Main category: cs.CL

TL;DR: AuthenHallu是首个完全基于真实LLM-人类对话构建的幻觉检测基准，相比人工构造的基准能更准确反映实际使用中的幻觉特征。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测基准多为人工构造，无法真实反映LLM在实际使用中的幻觉特征，需要基于真实LLM-人类对话构建更可靠的基准。

Method: 从真实LLM-人类对话中选取样本并进行标注，构建AuthenHallu基准，并探索使用原始LLM作为幻觉检测器的潜力。

Result: 统计显示31.4%的查询-响应对存在幻觉，在数学与数字问题等挑战性领域该比例高达60.0%。原始LLM作为检测器虽有潜力但实际性能仍不足。

Conclusion: 基于真实对话的基准能更好反映LLM幻觉特征，当前LLM作为幻觉检测器的能力仍需提升，AuthenHallu为未来研究提供了更可靠的评估基础。

Abstract: As large language models (LLMs) are increasingly applied in sensitive domains
such as medicine and law, hallucination detection has become a critical task.
Although numerous benchmarks have been proposed to advance research in this
area, most of them are artificially constructed--either through deliberate
hallucination induction or simulated interactions--rather than derived from
genuine LLM-human dialogues. Consequently, these benchmarks fail to fully
capture the characteristics of hallucinations that occur in real-world usage.
To address this limitation, we introduce AuthenHallu, the first hallucination
detection benchmark built entirely from authentic LLM-human interactions. For
AuthenHallu, we select and annotate samples from genuine LLM-human dialogues,
thereby providing a faithful reflection of how LLMs hallucinate in everyday
user interactions. Statistical analysis shows that hallucinations occur in
31.4% of the query-response pairs in our benchmark, and this proportion
increases dramatically to 60.0% in challenging domains such as Math & Number
Problems. Furthermore, we explore the potential of using vanilla LLMs
themselves as hallucination detectors and find that, despite some promise,
their current performance remains insufficient in real-world scenarios.

</details>


### [75] [BitMar: Low-Bit Multimodal Fusion with Episodic Memory for Edge Devices](https://arxiv.org/abs/2510.10560)
*Euhid Aman,Esteban Carlin,Hsing-Kuo Pao,Giovanni Beltrame,Ghaluh Indah Permata Sari,Yie-Tarng Chen*

Main category: cs.CL

TL;DR: BitMar是一个量化多模态transformer，使用1.58位编码器和外部情景记忆，在资源受限的硬件上实现高效的图像文本生成。


<details>
  <summary>Details</summary>
Motivation: 解决交叉注意力transformer等多模态视觉语言模型在边缘设备部署困难的问题，结合记忆增强架构与激进量化技术。

Method: 使用1.58位编码器（文本BitNet风格，视觉DiNOv2基础）创建紧凑嵌入，结合固定大小的键值情景记忆，采用逐层条件化和滑动窗口注意力机制。

Result: 在低延迟和小模型占用下实现竞争性的字幕生成和多模态理解，达到良好的质量-速度权衡。

Conclusion: BitMar的特性使其非常适合边缘部署，为资源受限环境下的多模态生成任务提供了有效解决方案。

Abstract: Cross-attention transformers and other multimodal vision-language models
excel at grounding and generation; however, their extensive, full-precision
backbones make it challenging to deploy them on edge devices. Memory-augmented
architectures enhance the utilization of past context; however, most works
rarely pair them with aggressive edge-oriented quantization. We introduce
BitMar, a quantized multimodal transformer that proposes an external human-like
episodic memory for effective image-text generation on hardware with limited
resources. BitMar utilizes 1.58-bit encoders, one for text (BitNet-style) and
one for vision (DiNOv2-based), to create compact embeddings that are combined
and used to query a fixed-size key-value episodic memory. During vector
retrieval, the BitNet decoder applies per-layer conditioning, which increases
the contextual relevance of generated content. The decoder also employs
attention sinks with a sliding-window mechanism to process long or streaming
inputs under tight memory budgets. The combination of per-layer conditioning
and sliding-window attention achieves a strong quality-speed trade-off,
delivering competitive captioning and multimodal understanding at low latency
with a small model footprint. These characteristics make BitMar well-suited for
edge deployment.

</details>


### [76] [Dynamic Topic Evolution with Temporal Decay and Attention in Large Language Models](https://arxiv.org/abs/2510.10613)
*Di Wu abd Shuaidong Pan*

Main category: cs.CL

TL;DR: 提出基于时序大语言模型的动态主题演化建模框架，通过时间衰减函数和注意力机制捕捉主题随时间的变化，在潜在主题空间中使用状态转移矩阵描述主题动态演化。


<details>
  <summary>Details</summary>
Motivation: 传统主题模型难以有效捕捉主题随时间动态演化的特性，需要一种能够统一建模语义表示和时间演化的系统性解决方案。

Method: 使用大语言模型获取文本上下文嵌入，引入时间衰减函数和注意力机制调整语义单元重要性，将时序表示映射到潜在主题空间，应用状态转移矩阵描述主题动态演化，通过联合优化目标约束语义建模和时间一致性。

Result: 在真实语料上的实验表明，该框架能有效捕捉主题的生成、扩展和衰退过程，在多个指标上优于现有模型。

Conclusion: 该方法为理解大规模文本中的动态语义模式提供了系统性解决方案，丰富了主题建模的研究范式，支持多领域的复杂文本分析任务。

Abstract: This paper proposes a modeling framework for dynamic topic evolution based on
temporal large language models. The method first uses a large language model to
obtain contextual embeddings of text and then introduces a temporal decay
function and an attention mechanism. These components allow the model to adjust
the importance of semantic units according to time intervals and capture topic
variations across different periods. The temporal representations are then
mapped into a latent topic space, where a state transition matrix is applied to
describe the dynamic evolution of topics. A joint optimization objective
constrains both semantic modeling and temporal consistency, ensuring diversity
and smoothness in topic generation. The design emphasizes the unified modeling
of semantic representation and temporal evolution, which improves topic
coherence and diversity while enhancing stability and interpretability over
time. Experiments on real-world corpora show that the framework effectively
captures the generation, expansion, and decline of topics and outperforms
existing models across multiple metrics. Overall, the proposed method provides
a systematic solution for understanding dynamic semantic patterns in
large-scale text, enriches the research paradigm of topic modeling, and
supports complex text analysis tasks in multiple domains.

</details>


### [77] [Preserving LLM Capabilities through Calibration Data Curation: From Analysis to Optimization](https://arxiv.org/abs/2510.10618)
*Bowei He,Lihao Yin,Huiling Zhen,Shuqi Liu,Han Wu,Xiaokun Zhang,Mingxuan Yuan,Chen Ma*

Main category: cs.CL

TL;DR: 本文系统研究了校准数据对大语言模型压缩后能力的影响，发现激活空间的代表性和多样性是决定校准数据质量的关键因素，并提出了基于此的校准数据筛选框架。


<details>
  <summary>Details</summary>
Motivation: 现有研究对校准数据如何影响压缩后LLM能力缺乏系统探索，特别是在高层次复杂推理能力方面，需要从组合属性和领域对应角度深入分析。

Method: 从激活模式角度分析校准数据的影响机制，探索校准数据对数学问题求解和代码生成等复杂推理能力的影响，提出基于激活空间代表性和多样性的校准数据筛选框架。

Result: 研究发现激活空间的代表性和多样性从根本上决定校准数据质量，提出的数据筛选框架能有效提升现有后训练压缩方法在保留关键LLM能力方面的性能。

Conclusion: 校准数据的质量对压缩后LLM能力保持至关重要，激活空间的代表性和多样性是评估校准数据质量的关键指标，提出的筛选框架能显著改善压缩效果。

Abstract: Post-training compression has been a widely employed approach to scale down
large language model (LLM) and facilitate efficient inference. In various
proposed compression methods, including pruning and quantization, calibration
data plays a vital role by informing the weight importance and activation
dynamic ranges. However, how calibration data impacts the LLM capability after
compression is less explored. Few of the existing works, though recognizing the
significance of this study, only investigate the language modeling or
commonsense reasoning performance degradation from limited angles, like the
data sources or sample amounts. More systematic research is still needed to
examine the impacts on different LLM capabilities in terms of compositional
properties and domain correspondence of calibration data. In this work, we aim
at bridging this gap and further analyze underlying influencing mechanisms from
the activation pattern perspective. Especially, we explore the calibration
data's impacts on high-level complex reasoning capabilities, like math problem
solving and code generation. Delving into the underlying mechanism, we find
that the representativeness and diversity in activation space more
fundamentally determine the quality of calibration data. Finally, we propose a
calibration data curation framework based on such observations and analysis,
enhancing the performance of existing post-training compression methods on
preserving critical LLM capabilities. Our code is provided in
\href{https://github.com/BokwaiHo/COLA.git}{Link}.

</details>


### [78] [FactAppeal: Identifying Epistemic Factual Appeals in News Media](https://arxiv.org/abs/2510.10627)
*Guy Mor-Lan,Tamir Sheafer,Shaul R. Shenhav*

Main category: cs.CL

TL;DR: 提出了认知诉求识别新任务，开发FactAppeal数据集用于识别事实陈述如何通过外部来源或证据建立可信度，最佳模型Gemma 2 9B达到0.73宏F1分数


<details>
  <summary>Details</summary>
Motivation: 研究事实陈述如何通过外部来源或证据建立可信度，超越传统的声明检测和验证，关注认知结构和证据基础

Method: 构建FactAppeal数据集（3,226条英文新闻句子），包含细粒度标注：事实陈述、来源类型、来源名称、角色、认知凭证、引用方式等特征，使用2B-9B参数范围的编码器和生成解码器模型

Result: 最佳模型Gemma 2 9B在认知诉求识别任务上达到0.73宏F1分数

Conclusion: FactAppeal数据集和认知诉求识别任务为理解事实陈述的可信度建立提供了新视角，模型表现良好但仍有改进空间

Abstract: How is a factual claim made credible? We propose the novel task of Epistemic
Appeal Identification, which identifies whether and how factual statements have
been anchored by external sources or evidence. To advance research on this
task, we present FactAppeal, a manually annotated dataset of 3,226
English-language news sentences. Unlike prior resources that focus solely on
claim detection and verification, FactAppeal identifies the nuanced epistemic
structures and evidentiary basis underlying these claims and used to support
them. FactAppeal contains span-level annotations which identify factual
statements and mentions of sources on which they rely. Moreover, the
annotations include fine-grained characteristics of factual appeals such as the
type of source (e.g. Active Participant, Witness, Expert, Direct Evidence),
whether it is mentioned by name, mentions of the source's role and epistemic
credentials, attribution to the source via direct or indirect quotation, and
other features. We model the task with a range of encoder models and generative
decoder models in the 2B-9B parameter range. Our best performing model, based
on Gemma 2 9B, achieves a macro-F1 score of 0.73.

</details>


### [79] [You're Not Gonna Believe This: A Computational Analysis of Factual Appeals and Sourcing in Partisan News](https://arxiv.org/abs/2510.10658)
*Guy Mor-Lan,Tamir Sheafer,Shaul R. Shenhav*

Main category: cs.CL

TL;DR: 本文通过比较CNN和福克斯新闻在COVID-19疫情和以色列-哈马斯战争期间的报道，量化了不同媒体在事实报道中使用的认知策略差异。


<details>
  <summary>Details</summary>
Motivation: 虽然媒体偏见已被广泛研究，但事实报道背后的认知策略在计算层面仍未被充分探索。本文旨在通过大规模比较分析揭示不同媒体如何构建现实。

Method: 采用文章匹配策略比较同一事件的报道，应用FactAppeal框架分析超过47万篇文章，涵盖两个高度政治化时期。

Result: CNN的报道包含更多事实陈述且更倾向于引用外部来源。两家媒体在引用模式上存在显著差异：CNN通过引用专家和专家文件构建正式权威，而福克斯新闻更倾向于新闻报道和直接引语。

Conclusion: 这项研究量化了党派媒体如何系统性地使用不同的认知策略来构建现实，为媒体偏见研究增添了新的维度。

Abstract: While media bias is widely studied, the epistemic strategies behind factual
reporting remain computationally underexplored. This paper analyzes these
strategies through a large-scale comparison of CNN and Fox News. To isolate
reporting style from topic selection, we employ an article matching strategy to
compare reports on the same events and apply the FactAppeal framework to a
corpus of over 470K articles covering two highly politicized periods: the
COVID-19 pandemic and the Israel-Hamas war. We find that CNN's reporting
contains more factual statements and is more likely to ground them in external
sources. The outlets also exhibit sharply divergent sourcing patterns: CNN
builds credibility by citing Experts} and Expert Documents, constructing an
appeal to formal authority, whereas Fox News favors News Reports and direct
quotations. This work quantifies how partisan outlets use systematically
different epistemic strategies to construct reality, adding a new dimension to
the study of media bias.

</details>


### [80] [AGENTIQL: An Agent-Inspired Multi-Expert Framework for Text-to-SQL Generation](https://arxiv.org/abs/2510.10661)
*Omid Reza Heidari,Siobhan Reid,Yassine Yaakoubi*

Main category: cs.CL

TL;DR: AGENTIQL是一个基于智能体的多专家框架，用于改进文本到SQL的生成，通过问题分解、子查询生成和列选择优化等模块化方法，在Spider基准测试中达到86.07%的执行准确率。


<details>
  <summary>Details</summary>
Motivation: 传统的单体架构在处理复杂推理和多样化数据库模式时存在困难，需要更灵活和可解释的方法来提升文本到SQL生成的性能。

Method: 采用多专家框架，包括推理智能体进行问题分解、编码智能体生成子查询、优化步骤进行列选择，以及自适应路由器在模块化流程和基线解析器之间进行选择。

Result: 在Spider基准测试中，使用14B参数模型和Planner&Executor合并策略，达到了86.07%的执行准确率，接近GPT-4的SOTA水平（89.65%）。

Conclusion: AGENTIQL不仅提高了执行准确率，还通过展示中间推理步骤增强了透明度，提供了一种鲁棒、可扩展且可解释的语义解析方法。

Abstract: LLMs have advanced text-to-SQL generation, yet monolithic architectures
struggle with complex reasoning and schema diversity. We propose AGENTIQL, an
agent-inspired multi-expert framework that combines a reasoning agent for
question decomposition, a coding agent for sub-query generation, and a
refinement step for column selection. An adaptive router further balances
efficiency and accuracy by selecting between our modular pipeline and a
baseline parser. Several steps in the pipeline can be executed in parallel,
making the framework scalable to larger workloads. Evaluated on the Spider
benchmark, AGENTIQL improves execution accuracy and interpretability and
achieves up to 86.07\% EX with 14B models using the Planner&Executor merging
strategy. The attained performance is contingent upon the efficacy of the
routing mechanism, thereby narrowing the gap to GPT-4-based SOTA (89.65% EX)
while using much smaller open-source LLMs. Beyond accuracy, AGENTIQL enhances
transparency by exposing intermediate reasoning steps, offering a robust,
scalable, and interpretable approach to semantic parsing.

</details>


### [81] [BrowserAgent: Building Web Agents with Human-Inspired Web Browsing Actions](https://arxiv.org/abs/2510.10666)
*Zhengbo Zhang,Zhiheng Lyu,Junhao Gong,Hongzhu Yi,Xinming Wang,Yuxuan Zhou,Jiabing Yang,Ping Nie,Yan Huang,Wenhu Chen*

Main category: cs.CL

TL;DR: BrowserAgent是一个基于人类浏览行为的交互式AI代理，通过直接操作浏览器解决复杂网络任务，相比现有方法在更少训练数据下取得更好性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法如Search-R1和WebDancer依赖工具将动态网络环境转换为静态文本，与人类多样化的浏览器交互行为不符，需要更自然的交互方式。

Method: 采用两阶段训练（监督微调和拒绝微调），通过预定义浏览器动作直接操作原始网页，并引入显式记忆机制存储关键结论。

Result: 使用显著更少的训练数据，在Open-QA任务中表现更具竞争力，在HotpotQA、2Wiki和Bamboogle等多跳QA任务上比Search-R1提升约20%。

Conclusion: BrowserAgent可作为更先进、交互性更强且可扩展的网络代理框架。

Abstract: Efficiently solving real-world problems with LLMs increasingly hinges on
their ability to interact with dynamic web environments and autonomously
acquire external information. While recent research like Search-R1 and
WebDancer demonstrates strong performance in solving web tasks, they heavily
rely on additional tools to convert the interactive web environment into static
text content. This is in contrast to human browsing behaviors, which involve
diverse interactions with the browser, such as scrolling, clicking, and typing.
In this paper, we propose BrowserAgent, a more interactive agent that solves
complex tasks through human-inspired browser actions. BrowserAgent operates
directly on raw web pages via Playwright through a set of predefined browser
actions. We adopt a two-stage training (Supervised Fine-Tuning (SFT) and
Rejection Fine-Tuning (RFT)) to improve the model's generalization abilities.
Despite using significantly less training data than Search-R1, BrowserAgent
achieves more competitive results across different Open-QA tasks. Additionally,
we introduce an explicit memory mechanism to store key conclusions across
steps, further enhancing the model's reasoning capabilities for long-horizon
tasks. Notably, BrowserAgent-7B can achieve around 20\% improvement over
Search-R1 on multi-hop QA tasks like HotpotQA, 2Wiki, and Bamboogle. These
results indicate that BrowserAgent can serve as a more advanced framework for
more interactive and scalable web agents.

</details>


### [82] [Unlocking LLM Safeguards for Low-Resource Languages via Reasoning and Alignment with Minimal Training Data](https://arxiv.org/abs/2510.10677)
*Zhuowei Chen,Bowei Zhang,Nankai Lin,Tian Hou,Lianxi Wang*

Main category: cs.CL

TL;DR: 提出了ConsistentGuard，一种基于推理的多语言安全防护方法，通过推理增强可解释性，通过对齐促进语言间知识迁移，仅需1000个训练样本就在6种语言的3个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全防护方法主要依赖基于分类器的方法，缺乏可解释性且在低资源语言上表现不佳，需要解决这些局限性。

Method: 提出基于推理的多语言安全防护方法ConsistentGuard，通过推理增强可解释性，通过对齐促进语言间知识迁移。

Result: 仅使用1000个训练样本，在6种语言的3个数据集上表现优于使用更多数据训练的大型模型，展现出强大的可解释性和泛化能力。

Conclusion: ConsistentGuard方法有效解决了现有方法的局限性，提供了多语言基准扩展并开源代码支持未来研究。

Abstract: Recent advances in LLMs have enhanced AI capabilities, but also increased the
risk posed by malicious requests, highlighting the need for effective LLM
safeguards to detect such queries. Existing approaches largely rely on
classifier-based methods that lack interpretability and perform poorly on
low-resource languages. To address these limitations, we propose
ConsistentGuard, a novel reasoning-based multilingual safeguard, which enhances
explainability via reasoning and boosts knowledge transfer between languages
through alignment. With only 1,000 training samples, our method demonstrates
superior performance on three datasets across six languages, outperforming
larger models trained with significantly more data, and exhibits strong
interpretability and generalization ability. We also contribute a multilingual
benchmark extension and release our codes to support future research.

</details>


### [83] [RePro: Training Language Models to Faithfully Recycle the Web for Pretraining](https://arxiv.org/abs/2510.10681)
*Zichun Yu,Chenyan Xiong*

Main category: cs.CL

TL;DR: RePro是一种新颖的网页数据回收方法，使用强化学习训练小型语言模型来生成高质量、忠实于原文的改写数据，显著提升预训练数据效率。


<details>
  <summary>Details</summary>
Motivation: 高质量预训练数据对大型语言模型至关重要，但前沿模型的优质数据资源正在枯竭，需要开发有效的数据回收方法。

Method: 使用强化学习训练4B参数的语言模型作为改写器，设计一个质量奖励和三个忠实度奖励，优化模型生成既高质量又保持原始语义和结构的改写数据。

Result: 在400M和1.4B模型上的预训练结果显示，RePro在22个下游任务上相对仅使用原始数据的基线获得4.7%-14.0%的准确率提升，数据效率提高2-3倍。

Conclusion: RePro为有效利用LLM预训练的'化石燃料'提供了一条高效且可控的路径，优于基于提示的现有方法，并能更好地保持关键信息和数据特征。

Abstract: High-quality pretraining data is the fossil fuel of large language models
(LLMs), yet its reserves are running low for frontier models. In this paper, we
introduce RePro, a novel web recycling method that trains a relatively small LM
with reinforcement learning to generate effective and faithful rephrasings of
pretraining data. Specifically, we design one quality reward and three
faithfulness rewards, optimizing the LM rephraser to convert organic data into
high-quality rephrasings while maintaining its core semantics and structure. In
our experiment, we train a 4B rephraser to recycle 72B tokens sampled from
DCLM-RefinedWeb. Pretraining results on 400M and 1.4B models demonstrate that
RePro delivers 4.7%-14.0% relative accuracy gains over organic-only baseline on
22 downstream tasks. RePro also outperforms ReWire, the state-of-the-art web
recycling method that prompts a 70B rephraser, as well as the organic baseline
with a 4x larger data pool. Experiments with different amounts of recycled data
highlight that RePro improves organic data efficiency by 2-3x. Individual and
distributional analyses validate that RePro preserves more critical information
and faithfully reflects the characteristics of organic data compared to
prompting-based methods. Together, these results show that RePro provides an
efficient and controllable path to effectively harness the fossil fuel of LLM
pretraining. We open-source our code, rephraser, and recycled data at
https://github.com/cxcscmu/RePro.

</details>


### [84] [Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning Framework](https://arxiv.org/abs/2510.10729)
*Manas Zambre,Sarika Bobade*

Main category: cs.CL

TL;DR: 提出一个模块化深度学习框架用于讽刺检测，结合DCNN和BERT模型分析语言、情感和上下文线索。


<details>
  <summary>Details</summary>
Motivation: 讽刺在文本中容易被误解，因为缺乏语气和肢体语言线索，需要开发更准确的检测方法。

Method: 使用模块化深度学习框架，集成DCNN和BERT模型，结合情感分析、上下文嵌入、语言特征提取和情感检测的多层架构。

Result: 虽然模型处于概念阶段，但证明了在聊天机器人和社交媒体分析等实际应用中的可行性。

Conclusion: 该框架展示了通过整合多种深度学习技术来改进文本讽刺检测的潜力。

Abstract: Sarcasm is a nuanced and often misinterpreted form of communication,
especially in text, where tone and body language are absent. This paper
proposes a modular deep learning framework for sarcasm detection, leveraging
Deep Convolutional Neural Networks (DCNNs) and contextual models such as BERT
to analyze linguistic, emotional, and contextual cues. The system integrates
sentiment analysis, contextual embeddings, linguistic feature extraction, and
emotion detection through a multi-layer architecture. While the model is in the
conceptual stage, it demonstrates feasibility for real-world applications such
as chatbots and social media analysis.

</details>


### [85] [Large Language Models for Full-Text Methods Assessment: A Case Study on Mediation Analysis](https://arxiv.org/abs/2510.10762)
*Wenqing Zhang,Trang Nguyen,Elizabeth A. Stuart,Yiqun T. Chen*

Main category: cs.CL

TL;DR: LLMs在方法学评估中表现接近人类专家，对明确陈述的方法标准准确率高，但在复杂推理任务中准确性下降15%，需要人类监督进行细致解释。


<details>
  <summary>Details</summary>
Motivation: 系统评价需要大量人工劳动，特别是提取详细方法学信息。LLMs有望自动化方法学评估，改变证据合成方式。

Method: 使用因果中介分析作为代表性方法学领域，在180篇全文科学文章中，将最先进的LLMs与专家人类评审员进行基准测试。

Result: 模型性能与人类判断密切相关（准确率相关性0.71；F1相关性0.97），在简单明确的方法标准上达到接近人类的准确率，但在复杂推理密集型评估中准确性显著下降，比专家评审员低15%。

Conclusion: 当前LLMs擅长识别明确的方法特征，但需要人类监督进行细致解释。将自动化信息提取与有针对性的专家评审相结合，可提高证据合成的效率和严谨性。

Abstract: Systematic reviews are crucial for synthesizing scientific evidence but
remain labor-intensive, especially when extracting detailed methodological
information. Large language models (LLMs) offer potential for automating
methodological assessments, promising to transform evidence synthesis. Here,
using causal mediation analysis as a representative methodological domain, we
benchmarked state-of-the-art LLMs against expert human reviewers across 180
full-text scientific articles. Model performance closely correlated with human
judgments (accuracy correlation 0.71; F1 correlation 0.97), achieving
near-human accuracy on straightforward, explicitly stated methodological
criteria. However, accuracy sharply declined on complex, inference-intensive
assessments, lagging expert reviewers by up to 15%. Errors commonly resulted
from superficial linguistic cues -- for instance, models frequently
misinterpreted keywords like "longitudinal" or "sensitivity" as automatic
evidence of rigorous methodological approache, leading to systematic
misclassifications. Longer documents yielded lower model accuracy, whereas
publication year showed no significant effect. Our findings highlight an
important pattern for practitioners using LLMs for methods review and synthesis
from full texts: current LLMs excel at identifying explicit methodological
features but require human oversight for nuanced interpretations. Integrating
automated information extraction with targeted expert review thus provides a
promising approach to enhance efficiency and methodological rigor in evidence
synthesis across diverse scientific fields.

</details>


### [86] [HiligayNER: A Baseline Named Entity Recognition Model for Hiligaynon](https://arxiv.org/abs/2510.10776)
*James Ald Teves,Ray Daniel Cal,Josh Magdiel Villaluz,Jean Malolos,Mico Magtira,Ramon Rodriguez,Mideth Abisado,Joseph Marvin Imperial*

Main category: cs.CL

TL;DR: 本文介绍了HiligayNER，这是首个针对菲律宾希利盖农语的命名实体识别基准模型，使用超过8000个标注句子构建，基于mBERT和XLM-RoBERTa模型，在跨语言评估中表现出良好性能。


<details>
  <summary>Details</summary>
Motivation: 希利盖农语在语言处理研究中代表性不足，缺乏标注语料库和基准模型，阻碍了该语言的技术发展。

Method: 收集公开新闻、社交媒体帖子和文学文本构建包含8000多个标注句子的数据集，使用mBERT和XLM-RoBERTa两种Transformer模型进行微调。

Result: 两种模型在所有实体类型上都实现了超过80%的精确率、召回率和F1分数，在宿务语和他加禄语的跨语言评估中显示出良好的可迁移性。

Conclusion: HiligayNER为低资源环境下的多语言NLP提供了有前景的解决方案，有助于促进菲律宾代表性不足语言的技术发展。

Abstract: The language of Hiligaynon, spoken predominantly by the people of Panay
Island, Negros Occidental, and Soccsksargen in the Philippines, remains
underrepresented in language processing research due to the absence of
annotated corpora and baseline models. This study introduces HiligayNER, the
first publicly available baseline model for the task of Named Entity
Recognition (NER) in Hiligaynon. The dataset used to build HiligayNER contains
over 8,000 annotated sentences collected from publicly available news articles,
social media posts, and literary texts. Two Transformer-based models, mBERT and
XLM-RoBERTa, were fine-tuned on this collected corpus to build versions of
HiligayNER. Evaluation results show strong performance, with both models
achieving over 80% in precision, recall, and F1-score across entity types.
Furthermore, cross-lingual evaluation with Cebuano and Tagalog demonstrates
promising transferability, suggesting the broader applicability of HiligayNER
for multilingual NLP in low-resource settings. This work aims to contribute to
language technology development for underrepresented Philippine languages,
specifically for Hiligaynon, and support future research in regional language
processing.

</details>


### [87] [Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG](https://arxiv.org/abs/2510.10787)
*Zhichao Wang,Cheng Wan,Dong Nie*

Main category: cs.CL

TL;DR: 本文综述了推理时扩展技术，将这一新兴领域组织为输出导向和输入导向两大方法类别，旨在通过部署时的额外计算显著提升LLM性能而无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 高质量训练数据的快速减少已成为LLM性能提升的根本瓶颈，促使研究重点转向推理时扩展，利用部署时的额外计算来显著改进下游任务性能。

Method: 系统性地将推理时扩展技术分为输出导向方法（包括复杂多步生成策略、推理方法、搜索解码、长CoT训练和模型集成）和输入导向方法（以少样本学习和RAG为主，RAG部分详细分析了查询扩展、数据、检索重排、LLM生成方法和多模态RAG）。

Result: 构建了一个全面的分类框架，系统整理了快速发展的推理时扩展领域，为研究者提供了清晰的技术路线图。

Conclusion: 推理时扩展代表了LLM发展的新范式，通过部署时的计算优化而非模型重训练来突破性能瓶颈，这一方法分类为未来研究提供了系统性的指导框架。

Abstract: The performance gains of LLMs have historically been driven by scaling up
model size and training data. However, the rapidly diminishing availability of
high-quality training data is introducing a fundamental bottleneck, shifting
the focus of research toward inference-time scaling. This paradigm uses
additional computation at the time of deployment to substantially improve LLM
performance on downstream tasks without costly model re-training. This review
systematically surveys the diverse techniques contributing to this new era of
inference-time scaling, organizing the rapidly evolving field into two
comprehensive perspectives: Output-focused and Input-focused methods.
Output-focused techniques encompass complex, multi-step generation strategies,
including reasoning (e.g., CoT, ToT, ReAct), various search and decoding
methods (e.g., MCTS, beam search), training for long CoT (e.g., RLVR, GRPO),
and model ensemble methods. Input-focused techniques are primarily categorized
by few-shot and RAG, with RAG as the central focus. The RAG section is further
detailed through a structured examination of query expansion, data, retrieval
and reranker, LLM generation methods, and multi-modal RAG.

</details>


### [88] [Toward Human-Centered Readability Evaluation](https://arxiv.org/abs/2510.10801)
*Bahar İlgen,Georges Hattab*

Main category: cs.CL

TL;DR: 提出HCRS框架，将自动测量与结构化人类反馈结合，超越传统表面指标，评估健康文本简化的人本可读性。


<details>
  <summary>Details</summary>
Motivation: 传统NLP评估指标如BLEU、FKGL、SARI主要捕捉表面特征，无法评估清晰度、可信度、文化相关性等人本质量，这在健康沟通等高风险场景中尤为重要。

Method: 提出基于人机交互和健康沟通研究的五维评估框架HCRS，整合自动测量与结构化人类反馈，捕捉可读性的关系和情境方面。

Result: 概述了该框架，讨论了其在参与式评估工作流中的整合，并提出了实证验证协议。

Conclusion: 这项工作旨在推动健康文本简化评估超越表面指标，使NLP系统更贴近多样化用户的需求、期望和生活经验。

Abstract: Text simplification is essential for making public health information
accessible to diverse populations, including those with limited health
literacy. However, commonly used evaluation metrics in Natural Language
Processing (NLP), such as BLEU, FKGL, and SARI, mainly capture surface-level
features and fail to account for human-centered qualities like clarity,
trustworthiness, tone, cultural relevance, and actionability. This limitation
is particularly critical in high-stakes health contexts, where communication
must be not only simple but also usable, respectful, and trustworthy. To
address this gap, we propose the Human-Centered Readability Score (HCRS), a
five-dimensional evaluation framework grounded in Human-Computer Interaction
(HCI) and health communication research. HCRS integrates automatic measures
with structured human feedback to capture the relational and contextual aspects
of readability. We outline the framework, discuss its integration into
participatory evaluation workflows, and present a protocol for empirical
validation. This work aims to advance the evaluation of health text
simplification beyond surface metrics, enabling NLP systems that align more
closely with diverse users' needs, expectations, and lived experiences.

</details>


### [89] [Is Implicit Knowledge Enough for LLMs? A RAG Approach for Tree-based Structures](https://arxiv.org/abs/2510.10806)
*Mihir Gupte,Paolo Giusto,Ramesh S*

Main category: cs.CL

TL;DR: 提出了一种自下而上的方法，将树状结构的知识线性化，通过在每个层级生成隐式聚合摘要，显著减少了检索器中的文档数量（减少68%以上），同时保持响应质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法未充分探索如何最佳表示检索到的知识来处理结构化数据（特别是树状层次结构），需要更有效的方式将层次化知识整合到RAG系统中。

Method: 采用自下而上的方法，对树状结构（如GitHub仓库）进行线性化处理，在每个层次级别生成隐式聚合摘要，然后将这些知识存储在知识库中直接用于RAG。

Result: 与使用原始非结构化代码的RAG相比，响应质量相当，但检索器中的文档数量减少了68%以上，显著提高了效率。

Conclusion: 利用隐式线性化知识可能是处理复杂层次数据结构的有效且可扩展策略，在保持质量的同时大幅提升检索效率。

Abstract: Large Language Models (LLMs) are adept at generating responses based on
information within their context. While this ability is useful for interacting
with structured data like code files, another popular method,
Retrieval-Augmented Generation (RAG), retrieves relevant documents to augment
the model's in-context learning. However, it is not well-explored how to best
represent this retrieved knowledge for generating responses on structured data,
particularly hierarchical structures like trees. In this work, we propose a
novel bottom-up method to linearize knowledge from tree-like structures (like a
GitHub repository) by generating implicit, aggregated summaries at each
hierarchical level. This approach enables the knowledge to be stored in a
knowledge base and used directly with RAG. We then compare our method to using
RAG on raw, unstructured code, evaluating the accuracy and quality of the
generated responses. Our results show that while response quality is comparable
across both methods, our approach generates over 68% fewer documents in the
retriever, a significant gain in efficiency. This finding suggests that
leveraging implicit, linearized knowledge may be a highly effective and
scalable strategy for handling complex, hierarchical data structures.

</details>


### [90] [Happiness is Sharing a Vocabulary: A Study of Transliteration Methods](https://arxiv.org/abs/2510.10827)
*Haeji Jung,Jinju Kim,Kyungjin Kim,Youjeong Roh,David R. Mortensen*

Main category: cs.CL

TL;DR: 该论文研究了罗马化、音位转录和替代密码三种音译方法对多语言模型性能的影响，发现罗马化在大多数评估场景中表现最佳，主要原因是能产生与预训练语言共享的更长子词标记。


<details>
  <summary>Details</summary>
Motivation: 研究不同音译方法（共享脚本、重叠词汇、共享音系）对多语言模型性能的贡献程度，以弥合使用非拉丁文字语言在NLP中的差距。

Method: 使用三种音译方法（罗马化、音位转录、替代密码）和正字法进行对照实验，在命名实体识别和自然语言推理两个下游任务上评估模型性能。

Result: 罗马化在8个评估设置中的7个显著优于其他输入类型，与假设一致，表明这是最有效的方法。

Conclusion: 罗马化是最有效的音译方法，其成功主要源于能产生与预训练语言共享的更长子词标记，从而更好地利用模型能力。

Abstract: Transliteration has emerged as a promising means to bridge the gap between
various languages in multilingual NLP, showing promising results especially for
languages using non-Latin scripts. We investigate the degree to which shared
script, overlapping token vocabularies, and shared phonology contribute to
performance of multilingual models. To this end, we conduct controlled
experiments using three kinds of transliteration (romanization, phonemic
transcription, and substitution ciphers) as well as orthography. We evaluate
each model on two downstream tasks -- named entity recognition (NER) and
natural language inference (NLI) -- and find that romanization significantly
outperforms other input types in 7 out of 8 evaluation settings, largely
consistent with our hypothesis that it is the most effective approach. We
further analyze how each factor contributed to the success, and suggest that
having longer (subword) tokens shared with pre-trained languages leads to
better utilization of the model.

</details>


### [91] [DUAL-Bench: Measuring Over-Refusal and Robustness in Vision-Language Models](https://arxiv.org/abs/2510.10846)
*Kaixuan Ren,Preslav Nakov,Usman Naseem*

Main category: cs.CL

TL;DR: 提出了DUAL-Bench，首个专注于多模态视觉语言模型中过度拒绝和安全完成问题的基准测试，评估了18个VLMs在12个危险类别中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试未能系统性地解决视觉模态中的过度拒绝问题，而安全机制可能导致模型过度谨慎地拒绝良性请求，特别是在双重用途场景下。

Method: 构建DUAL-Bench基准，包含12个危险类别，重点评估模型在语义保持的视觉扰动下的鲁棒性，测试18个主流视觉语言模型。

Result: 模型表现有显著改进空间：GPT-5-Nano达到12.9%的安全完成率，GPT-5模型平均7.9%，Qwen模型仅3.9%。

Conclusion: DUAL-Bench将促进开发更精细的对齐策略，确保模型在复杂多模态环境中既安全又有用。

Abstract: As vision-language models become increasingly capable, maintaining a balance
between safety and usefulness remains a central challenge. Safety mechanisms,
while essential, can backfire, causing over-refusal, where models decline
benign requests out of excessive caution. Yet, no existing benchmark has
systematically addressed over-refusal in the visual modality. This setting
introduces unique challenges, such as dual-use cases where an instruction is
harmless, but the accompanying image contains harmful content. Models
frequently fail in such scenarios, either refusing too conservatively or
completing tasks unsafely, which highlights the need for more fine-grained
alignment. The ideal behavior is safe completion, i.e., fulfilling the benign
parts of a request while explicitly warning about any potentially harmful
elements. To address this, we present DUAL-Bench, the first multimodal
benchmark focused on over-refusal and safe completion in VLMs. We evaluated 18
VLMs across 12 hazard categories, with focus on their robustness under
semantics-preserving visual perturbations. The results reveal substantial room
for improvement: GPT-5-Nano achieves 12.9% safe completion, GPT-5 models
average 7.9%, and Qwen models only 3.9%. We hope that DUAL-Bench will foster
the development of more nuanced alignment strategies that ensure models remain
both safe and useful in complex multimodal settings.

</details>


### [92] [Rethinking Agentic Workflows: Evaluating Inference-Based Test-Time Scaling Strategies in Text2SQL Tasks](https://arxiv.org/abs/2510.10885)
*Jiajing Guo,Kenil Patel,Jorge Piazentin Ono,Wenbin He,Liu Ren*

Main category: cs.CL

TL;DR: 评估六种轻量级测试时扩展策略和四种LLM在Text-to-SQL任务上的性能表现，重点关注准确性、推理延迟和令牌消耗的权衡


<details>
  <summary>Details</summary>
Motivation: 虽然测试时扩展策略在LLM解决方案中显示出潜力，但它们在真实世界应用中的有效性，特别是与最新推理模型结合使用时，仍不确定

Method: 在BIRD Mini-Dev基准上评估六种轻量级、面向工业的测试时扩展策略和四种LLM（包括两个推理模型），报告准确性、推理延迟和令牌消耗

Result: 分治提示和少样本演示持续提升通用和推理型LLM的性能，但额外工作流程步骤效果不一，基础模型选择至关重要

Conclusion: 揭示了在部署Text2SQL系统时准确性、效率和复杂性之间的实际权衡关系

Abstract: Large language models (LLMs) are increasingly powering Text-to-SQL (Text2SQL)
systems, enabling non-expert users to query industrial databases using natural
language. While test-time scaling strategies have shown promise in LLM-based
solutions, their effectiveness in real-world applications, especially with the
latest reasoning models, remains uncertain. In this work, we benchmark six
lightweight, industry-oriented test-time scaling strategies and four LLMs,
including two reasoning models, evaluating their performance on the BIRD
Mini-Dev benchmark. Beyond standard accuracy metrics, we also report inference
latency and token consumption, providing insights relevant for practical system
deployment. Our findings reveal that Divide-and-Conquer prompting and few-shot
demonstrations consistently enhance performance for both general-purpose and
reasoning-focused LLMs. However, introducing additional workflow steps yields
mixed results, and base model selection plays a critical role. This work sheds
light on the practical trade-offs between accuracy, efficiency, and complexity
when deploying Text2SQL systems.

</details>


### [93] [LLM$\times$MapReduce-V3: Enabling Interactive In-Depth Survey Generation through a MCP-Driven Hierarchically Modular Agent System](https://arxiv.org/abs/2510.10890)
*Yu Chao,Siyu Lin,xiaorong wang,Zhu Zhang,Zihan Zhou,Haoyu Wang,Shuo Wang,Jie Zhou,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: LLM x MapReduce-V3是一个用于生成长篇综述的层次化模块化代理系统，通过多代理架构和MCP服务器实现功能组件独立，支持动态工作流编排和人工干预。


<details>
  <summary>Details</summary>
Motivation: 基于LLM x MapReduce-V2的先前工作，旨在通过模块化分解促进人工干预，为用户提供对研究过程的更大控制和定制能力。

Method: 采用多代理架构，将骨架初始化、摘要构建和骨架细化等功能组件实现为独立的MCP服务器，这些服务器可以聚合成更高层次的服务器，形成层次化结构。高层规划代理根据MCP工具描述和执行历史动态编排工作流。

Result: 通过多轮交互，系统精确捕捉预期研究视角生成全面骨架，然后发展为深度综述。人工评估表明，该系统在内容深度和长度方面均优于代表性基线。

Conclusion: 基于MCP的模块化规划在生成长篇综述方面展现出强大能力，模块化分解促进了人工干预和定制化控制。

Abstract: We introduce LLM x MapReduce-V3, a hierarchically modular agent system
designed for long-form survey generation. Building on the prior work, LLM x
MapReduce-V2, this version incorporates a multi-agent architecture where
individual functional components, such as skeleton initialization, digest
construction, and skeleton refinement, are implemented as independent
model-context-protocol (MCP) servers. These atomic servers can be aggregated
into higher-level servers, creating a hierarchically structured system. A
high-level planner agent dynamically orchestrates the workflow by selecting
appropriate modules based on their MCP tool descriptions and the execution
history. This modular decomposition facilitates human-in-the-loop intervention,
affording users greater control and customization over the research process.
Through a multi-turn interaction, the system precisely captures the intended
research perspectives to generate a comprehensive skeleton, which is then
developed into an in-depth survey. Human evaluations demonstrate that our
system surpasses representative baselines in both content depth and length,
highlighting the strength of MCP-based modular planning.

</details>


### [94] [ADVICE: Answer-Dependent Verbalized Confidence Estimation](https://arxiv.org/abs/2510.10913)
*Ki Jung Seo,Sehun Lim,Taeuk Kim*

Main category: cs.CL

TL;DR: 提出了ADVICE框架，通过微调使大语言模型能够基于自身答案进行置信度估计，解决了模型过度自信的问题，显著改善了置信度校准效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然能够用自然语言表达置信度，但经常表现出过度自信的问题，其原因尚不清楚。本研究旨在分析语言化置信度的动态特性，并解决过度自信问题。

Method: 提出了ADVICE（答案依赖的语言化置信度估计）框架，这是一个微调框架，能够促进基于答案的置信度估计。

Result: 大量实验表明，ADVICE显著改善了置信度校准效果，同时保持了任务性能。进一步分析证实ADVICE增强了答案基础性，产生了更平衡和良好校准的置信度分布。

Conclusion: 本研究揭示了过度自信的根源，并建立了一个更可信的语言化置信度框架，为更可靠的置信度表达提供了解决方案。

Abstract: Recent progress in large language models (LLMs) has enabled them to express
their confidence in natural language, enhancing transparency and reliability.
However, their confidence often exhibits overconfidence, the cause of which
remains poorly understood. In this work, we conduct a detailed analysis of the
dynamics underlying verbalized confidence and identify answer-independence as a
key factor, defined as the model's failure to condition confidence on its own
answer. To address this, we propose ADVICE (Answer-Dependent Verbalized
Confidence Estimation), a fine-tuning framework that facilitates
answer-grounded confidence estimation. Extensive experiments show that ADVICE
substantially improves confidence calibration while preserving task
performance. Further analyses confirm that ADVICE strengthens
answer-groundedness, leading to more balanced and well-calibrated confidence
distributions. Our findings shed light on the origin of overconfidence and
establish a framework for more trustworthy confidence verbalization.

</details>


### [95] [GapDNER: A Gap-Aware Grid Tagging Model for Discontinuous Named Entity Recognition](https://arxiv.org/abs/2510.10927)
*Yawen Yang,Fukun Ma,Shiao Meng,Aiwei Liu,Lijie Wen*

Main category: cs.CL

TL;DR: 提出GapDNER模型，通过将上下文间隙作为额外的跨度类型，使用token-pair网格标注方法解决不连续命名实体识别中的解码歧义问题。


<details>
  <summary>Details</summary>
Motivation: 生物医学领域中，命名实体可能由一系列非相邻标记组成并与其他实体重叠。现有方法通过连接实体片段或内部标记来识别不连续实体，但由于跨度或单词组合的多样性，面临错误传播和解码歧义的挑战。

Method: 将上下文间隙视为额外的跨度类型，将跨度分类转换为token-pair网格标注任务。设计两个交互组件：内部跨度规律提取模块使用双仿射机制和线性注意力捕获每个跨度的内部规律；跨跨度关系增强模块使用交叉注意力获取不同跨度间的语义关系。在推理阶段使用BFS算法搜索有效路径。

Result: 在三个数据集上的实验结果表明，GapDNER在不连续NER任务上达到了新的最先进性能，并在识别复杂实体结构方面表现出显著优势。

Conclusion: GapDNER通过表示学习上下文间隙有效解决了不连续NER中的解码歧义问题，显著提升了不连续命名实体识别的性能。

Abstract: In biomedical fields, one named entity may consist of a series of
non-adjacent tokens and overlap with other entities. Previous methods recognize
discontinuous entities by connecting entity fragments or internal tokens, which
face challenges of error propagation and decoding ambiguity due to the wide
variety of span or word combinations. To address these issues, we deeply
explore discontinuous entity structures and propose an effective Gap-aware grid
tagging model for Discontinuous Named Entity Recognition, named GapDNER. Our
GapDNER innovatively applies representation learning on the context gaps
between entity fragments to resolve decoding ambiguity and enhance
discontinuous NER performance. Specifically, we treat the context gap as an
additional type of span and convert span classification into a token-pair grid
tagging task. Subsequently, we design two interactive components to
comprehensively model token-pair grid features from both intra- and inter-span
perspectives. The intra-span regularity extraction module employs the biaffine
mechanism along with linear attention to capture the internal regularity of
each span, while the inter-span relation enhancement module utilizes
criss-cross attention to obtain semantic relations among different spans. At
the inference stage of entity decoding, we assign a directed edge to each
entity fragment and context gap, then use the BFS algorithm to search for all
valid paths from the head to tail of grids with entity tags. Experimental
results on three datasets demonstrate that our GapDNER achieves new
state-of-the-art performance on discontinuous NER and exhibits remarkable
advantages in recognizing complex entity structures.

</details>


### [96] [Evaluating Language Models' Evaluations of Games](https://arxiv.org/abs/2510.10930)
*Katherine M. Collins,Cedegao E. Zhang,Graham Todd,Lance Ying,Mauricio Barba da Costa,Ryan Liu,Prafull Sharma,Adrian Weller,Ionatan Kuperwajs,Lionel Wong,Joshua B. Tenenbaum,Thomas L. Griffiths*

Main category: cs.CL

TL;DR: 本文提出评估AI系统对游戏评估能力的新范式，通过比较语言模型、推理模型与人类在100多个棋盘游戏上的450多个判断，发现推理模型在游戏评估上更接近人类判断，但存在非单调关系。


<details>
  <summary>Details</summary>
Motivation: 传统AI评估主要关注问题解决能力，但忽略了AI系统评估问题价值的能力。本文旨在建立评估AI系统评估能力的新方法。

Method: 使用100多个新颖棋盘游戏和450多个人类判断，比较现代语言模型、推理模型与人类在游戏公平性和趣味性评估上的差异。

Result: 推理模型在游戏评估上比非推理语言模型更接近人类判断，但随着模型接近博弈论最优，与人类数据的拟合度反而下降。评估趣味性时模型表现更不稳定。

Conclusion: 需要为语言和推理模型注入更多资源理性的元推理能力，以改善其评估能力的一致性和可预测性。

Abstract: Reasoning is not just about solving problems -- it is also about evaluating
which problems are worth solving at all. Evaluations of artificial intelligence
(AI) systems primarily focused on problem solving, historically by studying how
models play games such as chess and Go. In this paper, we advocate for a new
paradigm that assesses AI systems' evaluation of games. First, we introduce a
formalism for evaluating such evaluations. We then leverage a large-scale
dataset of over $100$ novel board games and over 450 human judgments to compare
evaluations produced by modern language and reasoning models against those of
people and symbolic computational agents. We consider two kinds of evaluative
queries: assessing the payoff (or fairness) and the funness of games. These
queries span two dimensions relevant to the design of evaluations of AI
evaluations: how complex a query is to compute and how difficult a query is to
quantify. Our results show that reasoning models are generally more aligned to
people in their evaluations of games than non-reasoning language models.
However, we observe a non-monotonic relationship: as models get closer to
game-theoretic optimal, their fit to human data weakens. We also observe more
"jaggedness" across models for assessing funness, in line with the greater
difficulty of quantifying this query. Across queries and games, reasoning
models show highly variable and unpredictable resource usage when assessing
queries, pointing to the importance of imbuing more resource-rational
meta-reasoning in language and reasoning models.

</details>


### [97] [End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF: A Reproducibility Study](https://arxiv.org/abs/2510.10936)
*Anirudh Ganesh,Jayavardhan Reddy*

Main category: cs.CL

TL;DR: 成功复现了Ma和Hovy(2016)提出的BiLSTM-CNN-CRF模型，该模型结合字符级CNN表示、词级BiLSTM上下文建模和CRF结构化预测，在NER和POS标注任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 验证最先进的序列标注神经架构的可复现性，为后续研究提供可靠的基础实现。

Method: 采用BiLSTM-CNN-CRF端到端架构，结合字符级CNN、词级BiLSTM和CRF层，无需手工特征工程。

Result: 在CoNLL-2003 NER任务上达到91.18% F1分数，成功复现了原论文的关键结果。

Conclusion: 该模型在序列标注任务中表现有效且可复现，开源PyTorch实现将促进进一步研究。

Abstract: We present a reproducibility study of the state-of-the-art neural
architecture for sequence labeling proposed by Ma and Hovy
(2016)\cite{ma2016end}. The original BiLSTM-CNN-CRF model combines
character-level representations via Convolutional Neural Networks (CNNs),
word-level context modeling through Bi-directional Long Short-Term Memory
networks (BiLSTMs), and structured prediction using Conditional Random Fields
(CRFs). This end-to-end approach eliminates the need for hand-crafted features
while achieving excellent performance on named entity recognition (NER) and
part-of-speech (POS) tagging tasks. Our implementation successfully reproduces
the key results, achieving 91.18\% F1-score on CoNLL-2003 NER and demonstrating
the model's effectiveness across sequence labeling tasks. We provide a detailed
analysis of the architecture components and release an open-source PyTorch
implementation to facilitate further research.

</details>


### [98] [Punctuation-aware treebank tree binarization](https://arxiv.org/abs/2510.10951)
*Eitan Klinger,Vivaan Wadhwa,Jungyeul Park*

Main category: cs.CL

TL;DR: 本文提出了一个保留标点符号的树库二值化资源包和评估套件，通过将标点作为兄弟节点处理，显著提升了头部预测准确率。


<details>
  <summary>Details</summary>
Motivation: 标准的二值化流程在头部选择前会丢弃标点符号，这会改变成分结构并损害头部-子节点识别。

Method: 开发了一个可复现的流水线，在二值化前将标点保留为兄弟节点，并提供了衍生产物和元数据。

Result: 在Penn Treebank上，标点感知预处理将头部预测准确率从73.66%（Collins规则）和86.66%（MLP）提升到91.85%，并在CCGbank推导上实现了竞争性的对齐效果。

Conclusion: 所有代码、配置文件和文档均已发布，支持在其他语料库上的复现和扩展。

Abstract: This article presents a curated resource and evaluation suite for
punctuation-aware treebank binarization. Standard binarization pipelines drop
punctuation before head selection, which alters constituent shape and harms
head-child identification. We release (1) a reproducible pipeline that
preserves punctuation as sibling nodes prior to binarization, (2) derived
artifacts and metadata (intermediate @X markers, reversibility signatures,
alignment indices), and (3) an accompanying evaluation suite covering
head-child prediction, round-trip reversibility, and structural compatibility
with derivational resources (CCGbank). On the Penn Treebank, punctuation-aware
preprocessing improves head prediction accuracy from 73.66\% (Collins rules)
and 86.66\% (MLP) to 91.85\% with the same classifier, and achieves competitive
alignment against CCGbank derivations. All code, configuration files, and
documentation are released to enable replication and extension to other
corpora.

</details>


### [99] [KOTOX: A Korean Toxic Dataset for Deobfuscation and Detoxification](https://arxiv.org/abs/2510.10961)
*Yejin Lee,Su-Hyeon Kim,Hyundong Jin,Dayoung Kim,Yeonsoo Kim,Yo-Sub Han*

Main category: cs.CL

TL;DR: 提出了KOTOX：首个同时支持韩语去混淆和去毒化的数据集，包含基于韩语语言学特征构建的三个难度级别版本，旨在解决低资源语言中LLM识别和中和混淆毒性内容的挑战。


<details>
  <summary>Details</summary>
Motivation: 在线交流中毒性内容日益严重，但现有研究主要关注英语，低资源语言代表性不足。LLM在识别和中和这些语言的毒性表达时存在困难，特别是当用户使用混淆技术规避检测系统时。

Method: 基于韩语语言学特征分类各种混淆方法，定义基于真实示例的转换规则，构建三个不同混淆难度级别的数据集版本（简单、普通、困难）。

Result: 创建了首个同时支持韩语去混淆和去毒化的数据集KOTOX，包含基于语言学特征构建的三个难度级别，代码和数据已开源。

Conclusion: KOTOX数据集将有助于更好地理解和缓解低资源语言LLM中混淆毒性内容的问题，为相关研究提供重要资源。

Abstract: Toxic content has become an increasingly critical social issue with the rapid
expansion of online communication. While numerous studies explored methods for
detecting and detoxifying such content, most have focused primarily on English,
leaving low-resource language underrepresented. Consequently, Large Language
Models~(LLMs) often struggle to identify and neutralize toxic expressions in
these languages. This challenge becomes even more pronounced when user employ
obfuscation techniques to evade detection systems. Therefore, we propose a
\textbf{KOTOX: Korean Toxic Dataset} for deobfuscation and detoxicification to
address this issue. We categorize various obfuscation approaches based on
linguistic characteristics of Korean and define a set of transformation rules
grounded in real-word examples. Using these rules, we construct three dataset
versions (easy, normal, and hard) representing different levels of obfuscation
difficulty. This is the first dataset that simultaneously supports
deobfuscation and detoxification for the Korean language. We expect it to
facilitate better understanding and mitigating of obfuscated toxic content in
LLM for low-resource languages. Our code and data are available at
https://github.com/leeyejin1231/KOTOX.

</details>


### [100] [Judge Before Answer: Can MLLM Discern the False Premise in Question?](https://arxiv.org/abs/2510.10965)
*Jidong Li,Lingyong Fang,Haodong Zhao,Sufeng Duan,Gongshen Liu*

Main category: cs.CL

TL;DR: 本文提出了一个自动化构建虚假前提问题基准的方法，创建了JBA数据集，并开发了一个增强框架来提升多模态大语言模型识别虚假前提的能力。


<details>
  <summary>Details</summary>
Motivation: 现有针对虚假前提问题的基准测试范围有限，缺乏细粒度分类和充分覆盖，无法严格评估模型识别虚假前提的能力。

Method: 开发了一个全自动构建虚假前提问题基准的流程，系统地将前提分为3个主要类型和13个子类型，并提出了一个识别增强框架来加强MLLMs的鲁棒性。

Result: 当前MLLMs在虚假前提识别方面仍然存在困难，但使用本文框架训练的模型在虚假前提识别方面取得了显著改进。

Conclusion: 本文提出的基准和增强框架有效地解决了MLLMs在虚假前提识别方面的脆弱性，为提升模型鲁棒性提供了有效方法。

Abstract: Multimodal large language models (MLLMs) have witnessed astonishing
advancements in recent years. Despite these successes, MLLMs remain vulnerable
to flase premise problems. However, existing benchmarks targeting this issue
are limited in scope: they often lack fine-grained categorization, exhibit
insufficient coverage, and thus fail to provide a rigorous evaluation of the
ability of models to recognize false premises. To bridge this gap, we introduce
a fully automated pipeline for constructing a comprehensive benchmark of false
premise questions. Our method systematically categorizes the premises into
three main types and thirteen subtypes according to the abilities required to
identify the premises, resulting in the JBA dataset.Results show current MLLMs
still struggle with false premise recognition. Building upon this benchmark, we
further propose a recognition enhancement framework tailored to strengthen the
robustness of MLLMs to detect false premises. Extensive experiments demonstrate
that models trained with our framework achieve significant improvements in
false premise recognition.

</details>


### [101] [RV-HATE: Reinforced Multi-Module Voting for Implicit Hate Speech Detection](https://arxiv.org/abs/2510.10971)
*Yejin Lee,Hyeseon Ahn,Yo-Sub Han*

Main category: cs.CL

TL;DR: 提出了RV-HATE框架，这是一个针对仇恨言论检测的适应性框架，通过强化学习优化不同模块的权重，以适应不同数据集的特定特征，提高检测准确性并提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 仇恨言论在不同平台和来源的数据集中表现出多样性特征，但现有检测方法往往采用固定方法，未能适应数据集特定的语言风格和社交背景。

Method: RV-HATE框架包含多个专门模块，每个模块关注仇恨言论的不同语言或上下文特征。使用强化学习优化各模块权重，通过投票机制聚合模块输出做出最终决策。

Result: 该方法有效处理了隐含仇恨言论，与传统静态方法相比取得了更优越的性能。

Conclusion: RV-HATE通过适应数据集特定特征提高了仇恨言论检测的准确性，同时提供了对每个数据集独特特征的可解释性洞察。

Abstract: Hate speech remains prevalent in human society and continues to evolve in its
forms and expressions. Modern advancements in internet and online anonymity
accelerate its rapid spread and complicate its detection. However, hate speech
datasets exhibit diverse characteristics primarily because they are constructed
from different sources and platforms, each reflecting different linguistic
styles and social contexts. Despite this diversity, prior studies on hate
speech detection often rely on fixed methodologies without adapting to
data-specific features. We introduce RV-HATE, a detection framework designed to
account for the dataset-specific characteristics of each hate speech dataset.
RV-HATE consists of multiple specialized modules, where each module focuses on
distinct linguistic or contextual features of hate speech. The framework
employs reinforcement learning to optimize weights that determine the
contribution of each module for a given dataset. A voting mechanism then
aggregates the module outputs to produce the final decision. RV-HATE offers two
primary advantages: (1)~it improves detection accuracy by tailoring the
detection process to dataset-specific attributes, and (2)~it also provides
interpretable insights into the distinctive features of each dataset.
Consequently, our approach effectively addresses implicit hate speech and
achieves superior performance compared to conventional static methods. Our code
is available at https://github.com/leeyejin1231/RV-HATE.

</details>


### [102] [Enhancing Large Language Model Reasoning via Selective Critical Token Fine-Tuning](https://arxiv.org/abs/2510.10974)
*Zhiwen Ruan,Yixia Li,He Zhu,Yun Chen,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: 提出了关键令牌微调(CFT)方法，只更新通过反事实扰动识别出的功能不可或缺的关键令牌，在数学推理任务上优于标准监督微调(SFT)。


<details>
  <summary>Details</summary>
Motivation: 标准SFT对所有令牌进行统一惩罚，忽略了只有少量关键令牌决定推理正确性，导致输出多样性减少和泛化能力有限。

Method: 通过反事实扰动识别功能不可或缺的关键令牌，只对这些决定性推理步骤进行梯度更新，同时保留非关键令牌的多样性。

Result: 在5个模型、3个模型家族和11个数学推理基准测试中，CFT尽管只微调不到12%的令牌，但始终优于标准SFT。

Conclusion: CFT是一个实用且通用的框架，可实现高效和稳健的LLM微调，提供更好的采样多样性和强化学习初始化。

Abstract: Large language models (LLMs) primarily rely on supervised fine-tuning (SFT)
as a key method to adapt pre-trained models to domain-specific tasks such as
mathematical reasoning. However, standard SFT uniformly penalizes all tokens,
neglecting that only a small subset of critical tokens determines reasoning
correctness. This uniform supervision often causes reduced output diversity and
limited generalization. We propose Critical Token Fine-tuning (CFT), a simple
yet effective approach that updates only tokens identified as functionally
indispensable via counterfactual perturbations. By focusing gradient signals on
these decisive reasoning steps while preserving the diversity of non-critical
tokens, CFT can enhance both generation and diversity. Extensive experiments on
five models across three families (Qwen, OLMo, LLaMA) and eleven mathematical
reasoning benchmarks show that CFT, despite fine-tuning on less than 12% of
tokens, consistently outperforms standard SFT. Moreover, CFT enables test-time
scaling through improved sampling diversity and provides a stronger
initialization for reinforcement learning, sustaining performance gains in
later training stages while maintaining higher entropy for better exploration.
These results highlight CFT as a practical and general framework for efficient
and robust LLM fine-tuning.

</details>


### [103] [DeepResearchGuard: Deep Research with Open-Domain Evaluation and Multi-Stage Guardrails for Safety](https://arxiv.org/abs/2510.10994)
*Wei-Chieh Huang,Henry Peng Zou,Yaozu Wu,Dongyuan Li,Yankai Chen,Weizhi Zhang,Yangning Li,Angelo Zangari,Jizhou Guo,Chunyu Miao,Liancheng Fang,Langzhou He,Renhe Jiang,Philip S. Yu*

Main category: cs.CL

TL;DR: 提出了DEEPRESEARCHGUARD框架，通过四阶段安全保障机制解决深度研究框架中存在的评估不足和安全风险问题，显著提升了防御成功率并降低了过度拒绝率。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究框架缺乏充分的评估程序和阶段特定保护，通常只关注问答的精确匹配准确性，而忽视了报告质量的关键方面如可信度、连贯性、广度、深度和安全性，可能导致危险或恶意来源被整合到最终报告中。

Method: 引入DEEPRESEARCHGUARD框架，采用四阶段安全保障机制，包括开放域参考和报告评估，并创建了DRSAFEBENCH阶段式深度研究安全基准。

Result: 在多种先进LLM上评估，DEEPRESEARCHGUARD平均防御成功率提升18.16%，同时降低过度拒绝率6%。输入防护提供最显著的早期阶段保护，计划和研究防护增强了引用纪律和来源可信度。

Conclusion: DEEPRESEARCHGUARD能够实现全面的开放域评估和阶段感知防御，有效阻止有害内容传播，同时系统性地提高报告质量而不会产生过高的过度拒绝率。

Abstract: Deep research frameworks have shown promising capabilities in synthesizing
comprehensive reports from web sources. While deep research possesses
significant potential to address complex issues through planning and research
cycles, existing frameworks are deficient in sufficient evaluation procedures
and stage-specific protections. They typically treat evaluation as exact match
accuracy of question-answering, but overlook crucial aspects of report quality
such as credibility, coherence, breadth, depth, and safety. This oversight may
result in hazardous or malicious sources being integrated into the final
report. To address these issues, we introduce DEEPRESEARCHGUARD, a
comprehensive framework featuring four-stage safeguards with open-domain
evaluation of references and reports. We assess performance across multiple
metrics, e.g., defense success rate and over-refusal rate, and five key report
dimensions. In the absence of a suitable safety benchmark, we introduce
DRSAFEBENCH, a stage-wise benchmark for deep research safety. Our evaluation
spans diverse state-of-the-art LLMs, including GPT-4o, Gemini-2.5-flash,
DeepSeek-v3, and o4-mini. DEEPRESEARCHGUARD achieves an average defense success
rate improvement of 18.16% while reducing over-refusal rate by 6%. The input
guard provides the most substantial early-stage protection by filtering out
obvious risks, while the plan and research guards enhance citation discipline
and source credibility. Through extensive experiments, we show that
DEEPRESEARCHGUARD enables comprehensive open-domain evaluation and stage-aware
defenses that effectively block harmful content propagation, while
systematically improving report quality without excessive over-refusal rates.
The code can be found via https://github.com/Jasonya/DeepResearchGuard.

</details>


### [104] [ABLEIST: Intersectional Disability Bias in LLM-Generated Hiring Scenarios](https://arxiv.org/abs/2510.10998)
*Mahika Phutane,Hayoung Jung,Matthew Kim,Tanushree Mitra,Aditya Vashistha*

Main category: cs.CL

TL;DR: 该研究审计了6个大型语言模型在2820个招聘场景中的表现，发现这些模型对残障人士存在显著的ABLEIST偏见，且这种偏见在性别和种姓边缘化的残障候选人中进一步加剧。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注西方背景，忽视了全球南方地区残障人士面临的交叉性边缘化问题（如性别和种姓），需要评估LLMs在招聘等高风险领域对残障人士的歧视问题。

Method: 引入了ABLEIST评估框架，包含5个残障歧视特定指标和3个交叉性伤害指标，对6个LLMs在2820个招聘场景中进行了全面审计。

Result: 发现LLMs对残障候选人存在显著的ABLEIST伤害，许多最先进模型未能检测到这些伤害，且性别和种姓边缘化的残障候选人面临更严重的交叉性伤害。

Conclusion: 当前安全工具存在关键盲点，需要在招聘等高风险领域对前沿模型进行交叉性安全评估。

Abstract: Large language models (LLMs) are increasingly under scrutiny for perpetuating
identity-based discrimination in high-stakes domains such as hiring,
particularly against people with disabilities (PwD). However, existing research
remains largely Western-centric, overlooking how intersecting forms of
marginalization--such as gender and caste--shape experiences of PwD in the
Global South. We conduct a comprehensive audit of six LLMs across 2,820 hiring
scenarios spanning diverse disability, gender, nationality, and caste profiles.
To capture subtle intersectional harms and biases, we introduce ABLEIST
(Ableism, Inspiration, Superhumanization, and Tokenism), a set of five
ableism-specific and three intersectional harm metrics grounded in disability
studies literature. Our results reveal significant increases in ABLEIST harms
towards disabled candidates--harms that many state-of-the-art models failed to
detect. These harms were further amplified by sharp increases in intersectional
harms (e.g., Tokenism) for gender and caste-marginalized disabled candidates,
highlighting critical blind spots in current safety tools and the need for
intersectional safety evaluations of frontier models in high-stakes domains
like hiring.

</details>


### [105] [DND: Boosting Large Language Models with Dynamic Nested Depth](https://arxiv.org/abs/2510.11001)
*Tieyuan Chen,Xiaodong Chen,Haoxing Chen,Zhenzhong Lan,Weiyao Lin,Jianguo Li*

Main category: cs.CL

TL;DR: DND方法通过动态选择关键token进行嵌套深度处理，提升LLM性能，无需大量额外计算。


<details>
  <summary>Details</summary>
Motivation: 提高现成LLM的性能，通过选择性地重新处理关键token来增强模型对困难token的理解，同时避免对简单token的冗余计算。

Method: 在transformer层结束时，使用路由器识别关键token，将其反馈进行额外处理；采用路由器控制损失和阈值控制方案来确保选择的准确性和稳定性。

Result: 在多样化基准测试中，将密集模型Qwen3-1.7B性能提升1.88%，MoE模型Qwen3-30B-A3B性能提升0.87%，参数和计算开销极小。

Conclusion: DND方法有效提升了预训练模型的性能，通过动态嵌套深度处理机制实现了高效的关键token选择与处理。

Abstract: We introduce Dynamic Nested Depth (DND), a novel method that improves
performance for off-the-shelf LLMs by selecting critical tokens to reprocess in
a nested depth manner. Specifically, at the end of the given transformer layer,
DND identifies more critical tokens with a router and feeds them back for an
extra round of processing, effectively ``reviewing" difficult tokens while
avoiding redundant computation for easier ones. The dynamic selection mechanism
is tailored for precise control via two novel strategies: a router controlling
loss to enhance token selection distinguishability, and a threshold control
scheme to ensure selection stability. We demonstrate the effectiveness of DND
by directly integrating it into pre-trained dense and MoE models during a
post-training phase. On diverse benchmarks, this approach boosts the
performances of the dense Qwen3-1.7B by 1.88% and the MoE Qwen3-30B-A3B by
0.87%, all with a minimal parameter and computing increase.

</details>


### [106] [LogiNumSynth: Synthesizing Joint Logical-Numerical Reasoning Problems for Language Models](https://arxiv.org/abs/2510.11031)
*Yiwei Liu,Yucheng Li,Xiao Li,Gong Cheng*

Main category: cs.CL

TL;DR: LogiNumSynth是一个灵活的自然语言问题合成器，能够生成需要联合逻辑推理和数值推理能力的任务，支持对推理世界丰富度、逻辑推理深度和数值计算复杂度的细粒度控制。


<details>
  <summary>Details</summary>
Motivation: 现有的联合逻辑-数值推理数据集依赖固定规则集，任务复杂度控制有限，限制了它们在评估和训练中的通用性。

Method: 开发LogiNumSynth合成器，支持生成完全可控的联合推理任务，能够灵活调节推理世界丰富度、逻辑推理深度和数值计算复杂度。

Result: 实验显示多个LLM在逻辑-数值推理方面存在持续弱点，LogiNumSynth可作为诊断工具和针对性监督来源。

Conclusion: LogiNumSynth既能作为诊断工具识别LLM的推理弱点，又能提供针对性训练数据来提升模型的综合推理能力。

Abstract: Joint logical-numerical reasoning remains a major challenge for language
models, yet existing datasets rely on fixed rule sets and offer limited control
over task complexity, constraining their generalizability for evaluation and
training. We present LogiNumSynth, a flexible natural language problem
synthesizer that synthesizes tasks requiring proficiency in joint logical
reasoning (e.g., rule-based reasoning) and numerical reasoning (e.g.,
arithmetic computation). LogiNumSynth supports fine-grained control over
reasoning world richness, logical reasoning depth, and the complexity of
numerical computations, enabling flexible data synthesis across difficulty
levels. We demonstrate three key contributions: (1) Synthesizer -- synthesizing
fully controllable joint reasoning tasks over natural language; (2) Evaluation
& Process Analysis -- evaluating both process accuracy and answer accuracy; (3)
Targeted Training -- using synthesized data to enhance LLMs' reasoning
performance. Experiments with multiple LLMs highlight persistent weaknesses in
logical-numerical reasoning, showing that LogiNumSynth can serve as both a
diagnostic tool and a source of targeted supervision for advancing integrated
reasoning skills.

</details>


### [107] [Enabling Doctor-Centric Medical AI with LLMs through Workflow-Aligned Tasks and Benchmarks](https://arxiv.org/abs/2510.11040)
*Wenya Xie,Qingying Xiao,Yu Zheng,Xidong Wang,Junying Chen,Ke Ji,Anningzhe Gao,Prayag Tiwari,Xiang Wan,Feng Jiang,Benyou Wang*

Main category: cs.CL

TL;DR: 该论文提出将大语言模型重新定位为与经验丰富的医生协作的临床助手，而不是直接与患者互动，以降低安全风险。研究构建了DoctorFLAN中文医疗数据集，并通过实验证明该数据集能显著提升开源LLM在医疗场景中的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医疗领域的直接部署存在安全风险，因为其领域专业知识有限。为了缓解这一问题，需要将LLM重新定位为与医生协作的助手，而不是直接面向患者。

Method: 通过两阶段灵感-反馈调查识别临床工作流中的真实需求，构建了包含92,000个问答实例的DoctorFLAN中文医疗数据集，涵盖22个临床任务和27个专业领域。同时创建了DoctorFLAN-test和DotaBench两个评估基准。

Result: 实验结果显示，DoctorFLAN显著提升了开源大语言模型在医疗场景中的表现，使其更好地与医生工作流程对齐，并补充了现有的面向患者的模型。

Conclusion: 这项工作为推进以医生为中心的医疗大语言模型开发提供了有价值的资源和框架，有助于LLM在医疗领域的更安全有效应用。

Abstract: The rise of large language models (LLMs) has transformed healthcare by
offering clinical guidance, yet their direct deployment to patients poses
safety risks due to limited domain expertise. To mitigate this, we propose
repositioning LLMs as clinical assistants that collaborate with experienced
physicians rather than interacting with patients directly. We conduct a
two-stage inspiration-feedback survey to identify real-world needs in clinical
workflows. Guided by this, we construct DoctorFLAN, a large-scale Chinese
medical dataset comprising 92,000 Q&A instances across 22 clinical tasks and 27
specialties. To evaluate model performance in doctor-facing applications, we
introduce DoctorFLAN-test (550 single-turn Q&A items) and DotaBench (74
multi-turn conversations). Experimental results with over ten popular LLMs
demonstrate that DoctorFLAN notably improves the performance of open-source
LLMs in medical contexts, facilitating their alignment with physician workflows
and complementing existing patient-oriented models. This work contributes a
valuable resource and framework for advancing doctor-centered medical LLM
development

</details>


### [108] [Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States](https://arxiv.org/abs/2510.11052)
*Qinglin Zhu,Yizhen Yao,Runcong Zhao,Yanzheng Xiang,Amrutha Saseendran,Chen Jin,Philip Alexander Teare,Bin Liang,Yulan He,Lin Gui*

Main category: cs.CL

TL;DR: LRD是一种两阶段并行序列生成框架，通过潜在精炼和预测反馈循环解决传统自回归模型的高延迟问题，在保持准确性的同时实现高达10.6倍的加速。


<details>
  <summary>Details</summary>
Motivation: 解决自回归模型严格顺序解码导致的高延迟问题，同时克服现有扩散式方法存在的信息丢失和过早承诺两个核心限制。

Method: 提出两阶段框架：第一阶段将掩码位置保持为预测标记和掩码嵌入的分布混合，建立全局一致的信念；第二阶段逐步确定置信标记，保留不确定标记进行迭代反馈，使用KL散度动态作为收敛准则。

Result: 在编码任务（HumanEval +6.3，MBPP +2.6）和推理任务（GSM8K +2.9，MATH500 +3.8）上均提升准确性，同时实现高达10.6倍的加速。

Conclusion: LRD是并行序列生成的一个强大且通用的替代方案，在保持准确性的同时显著提升生成速度。

Abstract: Autoregressive (AR) models remain the standard for natural language
generation but still suffer from high latency due to strictly sequential
decoding. Recent diffusion-inspired approaches, such as LlaDA and Dream,
mitigate this by generating in parallel, yet they suffer from two core
limitations: information loss, as predictive distributions for non-finalized
tokens are discarded at each step, and premature commitment, where local
decisions are made without sufficient global coordination. We introduce Latent
Refinement Decoding (LRD), a two-stage framework with Latent Refinement and a
Predictive Feedback Loop. The first stage maintains masked positions as
distributional mixtures of predicted tokens and the mask embedding, allowing
the model to establish more globally consistent beliefs. The second stage
progressively finalizes confident tokens while retaining uncertain ones for
iterative feedback. KL-divergence dynamics provide a principled and reliable
criterion for convergence and early stopping. Experiments across coding
(HumanEval +6.3, MBPP +2.6) and reasoning (GSM8K +2.9, MATH500 +3.8) show that
LRD improves accuracy while delivering speedups of up to 10.6x, making it a
strong and versatile alternative for parallel sequence generation.

</details>


### [109] [Enhancing LLM Reasoning via Non-Human-Like Reasoning Path Preference Optimization](https://arxiv.org/abs/2510.11104)
*Junjie Lu,Yuliang Liu,Chaofeng Qu,Wei Shen,Zhouhan Lin,Min Xu*

Main category: cs.CL

TL;DR: 提出CGPO方法，利用置信度信号识别模型推理过程中的最大不确定性点，并应用自生成的非人类推理路径指导来减轻轨迹漂移，在代码和数学推理任务中优于使用强模型或人工标注数据的方法。


<details>
  <summary>Details</summary>
Motivation: 当前强化LLM推理的方法倾向于引入对人类推理轨迹的训练偏差，依赖人类或高能力模型对中间步骤的标注限制了探索替代性非人类推理路径，从而约束了可实现的性能。

Method: CGPO方法利用置信度信号识别模型推理过程中的最大不确定性点，并在错误发生前的最低置信点应用自生成的非人类推理路径指导。

Result: 实验表明，在相同训练数据量下，使用小模型生成数据的CGPO方法在大多数情况下比使用强模型生成数据或人工标注的方法表现更好。

Conclusion: 在模型最低置信点提供指导比定位第一个显式错误提供更准确的监督，CGPO方法能有效提升LLM推理性能。

Abstract: Current approaches for strengthening LLM reasoning tend to introduce a
training bias toward human-like reasoning trajectories. In step-wise preference
optimization, in particular, dependence on human or higher-capacity model
annotations for intermediate steps limits exploration of alternative,
non-human-like reasoning paths and thus constrains achievable performance.
Furthermore, through a small-scale pilot study, we observed that in
approximately 75% of cases, the model's first erroneous step occurs after the
lowest-confidence point. This suggests that guiding the model at its
lowest-confidence point before an error provides more accurate supervision than
locating the first explicit error. In this paper, we propose Confidence-Guided
Reasoning Path Preference Optimization (CGPO), a method that leverages a
confidence signal to identify points of maximal uncertainty in the model's
reasoning process and applies self-generated, non-human-like reasoning-path
guidance to mitigate trajectory drift. Our experiments span diverse models
applied to both code and mathematical reasoning tasks. The results show that,
with the same amount of training data, our method using data generated by a
small model can achieve better performance in most cases compared with
approaches using data generated by a strong model or human-annotated.

</details>


### [110] [TypePilot: Leveraging the Scala Type System for Secure LLM-generated Code](https://arxiv.org/abs/2510.11151)
*Alexander Sternfeld,Andrei Kucharavy,Ljiljana Dolamic*

Main category: cs.CL

TL;DR: TypePilot是一个基于强类型语言的AI代理框架，通过Scala语言提升LLM生成代码的安全性和鲁棒性，显著减少了输入验证和注入漏洞。


<details>
  <summary>Details</summary>
Motivation: LLM在代码生成方面表现出色，但其输出常包含关键安全漏洞，在安全敏感系统中存在重大风险。

Method: 使用Scala作为代表性强类型语言，构建类型导向的AI代理管道，结合Stainless框架进行形式化验证和通用安全代码生成。

Result: 实验表明，直接代码生成和简单提示难以保证安全约束，而类型导向的代理管道能显著减轻输入验证和注入漏洞。

Conclusion: 结构化、类型引导的LLM工作流程有潜力提升高保证领域自动化代码生成的可信度。

Abstract: Large language Models (LLMs) have shown remarkable proficiency in code
generation tasks across various programming languages. However, their outputs
often contain subtle but critical vulnerabilities, posing significant risks
when deployed in security-sensitive or mission-critical systems. This paper
introduces TypePilot, an agentic AI framework designed to enhance the security
and robustness of LLM-generated code by leveraging strongly typed and
verifiable languages, using Scala as a representative example. We evaluate the
effectiveness of our approach in two settings: formal verification with the
Stainless framework and general-purpose secure code generation. Our experiments
with leading open-source LLMs reveal that while direct code generation often
fails to enforce safety constraints, just as naive prompting for more secure
code, our type-focused agentic pipeline substantially mitigates input
validation and injection vulnerabilities. The results demonstrate the potential
of structured, type-guided LLM workflows to improve the SotA of the
trustworthiness of automated code generation in high-assurance domains.

</details>


### [111] [One Size Does Not Fit All: Exploring Variable Thresholds for Distance-Based Multi-Label Text Classification](https://arxiv.org/abs/2510.11160)
*Jens Van Nooten,Andriy Kosar,Guy De Pauw,Walter Daelemans*

Main category: cs.CL

TL;DR: 本文研究了基于距离的无监督文本分类方法，提出了一种新的标签特定阈值优化方法，在多个多标签文本分类数据集上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 基于距离的文本分类方法具有快速推理和适应扩展标签集的优势，但在多标签分类中需要确定相似性阈值。现有方法在这方面研究不足，特别是不同模型、数据集和标签集之间的语义关系差异。

Method: 首先通过探索性研究验证文本和标签之间语义关系在不同模型、数据集和标签集中的变化，然后提出一种使用验证集优化标签特定阈值的新方法。

Result: 标签特定阈值方法比归一化0.5阈值平均提升46%，比先前工作的统一阈值方法平均提升14%，并且在有限标注样本下也表现出色。

Conclusion: 基于距离的无监督文本分类中，标签特定阈值优化方法能显著提升性能，且相似性分布在模型、数据集和标签集之间存在显著差异。

Abstract: Distance-based unsupervised text classification is a method within text
classification that leverages the semantic similarity between a label and a
text to determine label relevance. This method provides numerous benefits,
including fast inference and adaptability to expanding label sets, as opposed
to zero-shot, few-shot, and fine-tuned neural networks that require re-training
in such cases. In multi-label distance-based classification and information
retrieval algorithms, thresholds are required to determine whether a text
instance is "similar" to a label or query. Similarity between a text and label
is determined in a dense embedding space, usually generated by state-of-the-art
sentence encoders. Multi-label classification complicates matters, as a text
instance can have multiple true labels, unlike in multi-class or binary
classification, where each instance is assigned only one label. We expand upon
previous literature on this underexplored topic by thoroughly examining and
evaluating the ability of sentence encoders to perform distance-based
classification. First, we perform an exploratory study to verify whether the
semantic relationships between texts and labels vary across models, datasets,
and label sets by conducting experiments on a diverse collection of realistic
multi-label text classification (MLTC) datasets. We find that similarity
distributions show statistically significant differences across models,
datasets and even label sets. We propose a novel method for optimizing
label-specific thresholds using a validation set. Our label-specific
thresholding method achieves an average improvement of 46% over normalized 0.5
thresholding and outperforms uniform thresholding approaches from previous work
by an average of 14%. Additionally, the method demonstrates strong performance
even with limited labeled examples.

</details>


### [112] [Bridging Gaps in Hate Speech Detection: Meta-Collections and Benchmarks for Low-Resource Iberian Languages](https://arxiv.org/abs/2510.11167)
*Paloma Piot,José Ramom Pichel Campos,Javier Parapar*

Main category: cs.CL

TL;DR: 本文构建了一个针对伊比利亚语言（欧洲西班牙语、欧洲葡萄牙语和加利西亚语变体）的仇恨言论数据集元集合，建立了新的基准，并评估了大语言模型在零样本、少样本和微调设置下的性能。


<details>
  <summary>Details</summary>
Motivation: 仇恨言论检测研究主要集中在英语，低资源语言缺乏资源和基准，且许多低资源语言存在多种语言变体，这一因素在当前方法中常被忽视。

Method: 通过系统分析和整合现有资源，构建标准化的仇恨言论数据集元集合，并将其翻译成多种语言变体，创建对齐的多语言语料库，然后评估大语言模型在不同设置下的性能。

Result: 建立了伊比利亚语言仇恨言论检测的新基准，提供了基线结果，并进行了跨语言分析，强调了多语言和变体感知方法的重要性。

Conclusion: 研究强调了在仇恨言论检测中采用多语言和变体感知方法的重要性，为代表性不足的欧洲语言提供了改进基准的基础。

Abstract: Hate speech poses a serious threat to social cohesion and individual
well-being, particularly on social media, where it spreads rapidly. While
research on hate speech detection has progressed, it remains largely focused on
English, resulting in limited resources and benchmarks for low-resource
languages. Moreover, many of these languages have multiple linguistic
varieties, a factor often overlooked in current approaches. At the same time,
large language models require substantial amounts of data to perform reliably,
a requirement that low-resource languages often cannot meet. In this work, we
address these gaps by compiling a meta-collection of hate speech datasets for
European Spanish, standardised with unified labels and metadata. This
collection is based on a systematic analysis and integration of existing
resources, aiming to bridge the data gap and support more consistent and
scalable hate speech detection. We extended this collection by translating it
into European Portuguese and into a Galician standard that is more convergent
with Spanish and another Galician variant that is more convergent with
Portuguese, creating aligned multilingual corpora. Using these resources, we
establish new benchmarks for hate speech detection in Iberian languages. We
evaluate state-of-the-art large language models in zero-shot, few-shot, and
fine-tuning settings, providing baseline results for future research. Moreover,
we perform a cross-lingual analysis with our target languages. Our findings
underscore the importance of multilingual and variety-aware approaches in hate
speech detection and offer a foundation for improved benchmarking in
underrepresented European languages.

</details>


### [113] [Evaluating Reasoning Faithfulness in Medical Vision-Language Models using Multimodal Perturbations](https://arxiv.org/abs/2510.11196)
*Johannes Moll,Markus Graf,Tristan Lemke,Nicolas Lenhart,Daniel Truhn,Jean-Benoit Delbrouck,Jiazhen Pan,Daniel Rueckert,Lisa C. Adams,Keno K. Bressem*

Main category: cs.CL

TL;DR: 提出了一个临床基础的框架来评估胸片视觉问答中思维链解释的忠实性，发现答案准确性与解释质量脱节，专有模型在归因和忠实性方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型产生的思维链解释往往听起来合理但不能反映实际决策过程，这在临床高风险应用中会削弱信任。现有评估很少捕捉到这种不一致性。

Method: 通过受控的文本和图像修改，在三个维度上探测思维链的忠实性：临床忠实性、因果归因和置信度校准。进行了读者研究（n=4）来验证评估框架。

Result: 评估者与放射科医生的相关性在所有维度上都落在观察到的放射科医生间差异范围内。专有模型在归因（25.0% vs 1.4%）和忠实性（36.1% vs 31.7%）方面表现优于开源模型。

Conclusion: 答案准确性和解释质量是脱节的，承认注入线索并不能确保基础性，文本线索比视觉线索更能改变解释。需要超越最终答案准确性来评估模型。

Abstract: Vision-language models (VLMs) often produce chain-of-thought (CoT)
explanations that sound plausible yet fail to reflect the underlying decision
process, undermining trust in high-stakes clinical use. Existing evaluations
rarely catch this misalignment, prioritizing answer accuracy or adherence to
formats. We present a clinically grounded framework for chest X-ray visual
question answering (VQA) that probes CoT faithfulness via controlled text and
image modifications across three axes: clinical fidelity, causal attribution,
and confidence calibration. In a reader study (n=4), evaluator-radiologist
correlations fall within the observed inter-radiologist range for all axes,
with strong alignment for attribution (Kendall's $\tau_b=0.670$), moderate
alignment for fidelity ($\tau_b=0.387$), and weak alignment for confidence tone
($\tau_b=0.091$), which we report with caution. Benchmarking six VLMs shows
that answer accuracy and explanation quality are decoupled, acknowledging
injected cues does not ensure grounding, and text cues shift explanations more
than visual cues. While some open-source models match final answer accuracy,
proprietary models score higher on attribution (25.0% vs. 1.4%) and often on
fidelity (36.1% vs. 31.7%), highlighting deployment risks and the need to
evaluate beyond final answer accuracy.

</details>


### [114] [Discursive Circuits: How Do Language Models Understand Discourse Relations?](https://arxiv.org/abs/2510.11210)
*Yisong Miao,Min-Yen Kan*

Main category: cs.CL

TL;DR: 本文发现Transformer语言模型中存在稀疏的"话语电路"（约0.2%的GPT-2模型参数），这些电路负责话语理解，能够在不同话语框架（PDTB、RST、SDRT）中泛化。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer语言模型中哪些组件负责话语理解，假设存在稀疏的计算图（话语电路）控制模型处理话语关系的过程。

Method: 引入CuDR任务（在指定话语关系下完成话语），构建用于激活修补的最小对比对语料库，通过电路发现方法识别负责话语理解的稀疏电路。

Result: 实验显示稀疏电路（约GPT-2模型的0.2%）能够恢复英语PDTB-based CuDR任务中的话语理解能力，这些电路在不同话语框架中泛化良好。

Conclusion: 下层网络捕获词汇语义和共指等语言特征，上层网络编码话语级抽象，特征效用在不同框架中保持一致（如共指支持扩展类关系）。

Abstract: Which components in transformer language models are responsible for discourse
understanding? We hypothesize that sparse computational graphs, termed as
discursive circuits, control how models process discourse relations. Unlike
simpler tasks, discourse relations involve longer spans and complex reasoning.
To make circuit discovery feasible, we introduce a task called Completion under
Discourse Relation (CuDR), where a model completes a discourse given a
specified relation. To support this task, we construct a corpus of minimal
contrastive pairs tailored for activation patching in circuit discovery.
Experiments show that sparse circuits ($\approx 0.2\%$ of a full GPT-2 model)
recover discourse understanding in the English PDTB-based CuDR task. These
circuits generalize well to unseen discourse frameworks such as RST and SDRT.
Further analysis shows lower layers capture linguistic features such as lexical
semantics and coreference, while upper layers encode discourse-level
abstractions. Feature utility is consistent across frameworks (e.g.,
coreference supports Expansion-like relations).

</details>


### [115] [Domain-Specific Data Generation Framework for RAG Adaptation](https://arxiv.org/abs/2510.11217)
*Chris Xing Tian,Weihao Xie,Zhen Chen,Zhengyuan Yi,Hui Liu,Haoliang Li,Shiqi Wang,Siwei Ma*

Main category: cs.CL

TL;DR: RAGen是一个可扩展的模块化框架，用于生成领域特定的问题-答案-上下文三元组，以支持检索增强生成系统的领域适应。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成系统需要专门、上下文丰富的训练数据来适应特定领域，而通用问答数据不足以满足这一需求。

Method: 通过识别文档中的关键概念，基于布鲁姆分类学原则生成多样化问题，并从相关上下文中提取精确答案来创建QAC三元组。支持多种RAG适应策略，包括语义分块、层次概念提取和多块检索。

Result: RAGen能够高效处理大规模和不断演变的文档语料库，无需冗余处理，特别适合科学研究和企业知识库等动态领域。

Conclusion: RAGen提供了一个可扩展的解决方案，能够生成高质量的领域特定训练数据，有效支持RAG系统在专业领域的适应和优化。

Abstract: Retrieval-Augmented Generation (RAG) combines the language understanding and
reasoning power of large language models (LLMs) with external retrieval to
enable domain-grounded responses. Effectively adapting RAG systems to
domain-specific settings requires specialized, context-rich training data
beyond general-purpose question-answering. Here, we propose RAGen, a scalable
and modular framework for generating domain-grounded question-answer-context
(QAC) triples tailored to diverse RAG adaptation approaches. RAGen produces
these QAC triples by identifying key concepts in documents, generating diverse
questions guided by Bloom's Taxonomy-inspired principles, and pairing them with
precise answers extracted from relevant contexts. RAGen supports multiple RAG
adaptation strategies, including the optimization of key components such as the
LLM, retriever, and embedding model, etc. Its modular pipeline features
semantic chunking, hierarchical concept extraction, and multi-chunk retrieval,
along with the introduction of curated distractor contexts to promote robust
reasoning. Designed for scalability, RAGen efficiently handles large and
evolving document corpora without redundant processing, making it especially
suitable for dynamic evolving domains such as scientific research and
enterprise knowledge bases.

</details>


### [116] [The Curious Case of Factual (Mis)Alignment between LLMs' Short- and Long-Form Answers](https://arxiv.org/abs/2510.11218)
*Saad Obaid ul Islam,Anne Lauscher,Goran Glavaš*

Main category: cs.CL

TL;DR: 本文提出了SLAQ评估框架，发现LLMs在简单问答和复杂查询中对相同事实问题的回答存在系统性不一致，揭示了模型在不同任务复杂度下访问事实知识的可靠性差距。


<details>
  <summary>Details</summary>
Motivation: LLMs在简单事实问答基准上表现出色，但在简单查询和复杂查询之间的可靠性差距未被充分理解，这削弱了模型的可信度。

Method: 引入SLAQ评估框架，比较LLMs对相同事实问题在孤立形式（短）和复杂查询集成形式（长）下的回答一致性，分析了16个LLM在600个查询上的表现。

Result: 发现短长查询回答存在系统性不一致，存在位置依赖性准确率损失和动量效应，机制分析显示对齐事实激活重叠的模型内部结构，基于机制相似性的指标可预测短长回答对齐度达78%准确率。

Conclusion: 事实一致性是LLMs可信度的重要方面，当前评估实践隐含假设简单事实查询的良好性能意味着复杂知识寻求任务的可靠性，这一假设受到挑战。

Abstract: Large language models (LLMs) can correctly answer "When was Einstein born?"
yet fail to provide the same date when writing about Einstein's life revealing
a fundamental inconsistency in how models access factual knowledge across task
complexities. While models display impressive accuracy on factual
question-answering benchmarks, the reliability gap between simple and complex
queries remains poorly understood, eroding their trustworthiness. In this work,
we introduce Short-Long Form Alignment for Factual Question Answering (SLAQ), a
controlled evaluation framework that compares LLMs' answers to the same factual
questions asked (a) in isolation (short) vs. (b) integrated into complex
queries (long). Looking at 16 LLMs across 600 queries, we find a systematic
misalignment of answers to the corresponding short and long queries. We further
uncover position-dependent accuracy loss and momentum effects where consecutive
correct or incorrect answers create self-reinforcing patterns. Through
mechanistic analysis, we find that aligned facts activate overlapping model
internals, and that metrics based on mechanistic similarity can predict
short-long answer alignment with up to 78% accuracy. Our work establishes
factual consistency over query complexity as an important aspect of LLMs'
trustworthiness and challenges current evaluation practices, which implicitly
assume that good performance for simple factual queries implies reliability in
more complex knowledge-seeking tasks too.

</details>


### [117] [WebRouter: Query-specific Router via Variational Information Bottleneck for Cost-sensitive Web Agent](https://arxiv.org/abs/2510.11221)
*Tao Li,Jinlong Hu,Yang Wang,Junfeng Liu,Xuejun Liu*

Main category: cs.CL

TL;DR: WebRouter是一个基于信息论的查询特定路由器，通过成本感知的变分信息瓶颈目标，在压缩输入提示的同时惩罚预期操作成本，显著降低LLM网络代理的操作成本。


<details>
  <summary>Details</summary>
Motivation: LLM网络代理面临成本与性能的权衡挑战，复杂的提示（包括目标、行动历史和状态）导致LLM集成性能下降，需要解决这一关键问题。

Method: 提出WebRouter，采用成本感知的变分信息瓶颈（ca-VIB）目标，学习输入提示的压缩表示，同时明确惩罚预期操作成本。

Result: 在WebVoyager基准测试的五个真实网站上的实验表明，WebRouter相比GPT-4o基线降低了87.8%的操作成本，仅带来3.8%的准确率下降。

Conclusion: WebRouter通过信息论方法有效解决了LLM网络代理的成本性能权衡问题，实现了显著的成本节约和良好的性能保持。

Abstract: LLM-brained web agents offer powerful capabilities for web automation but
face a critical cost-performance trade-off. The challenge is amplified by web
agents' inherently complex prompts that include goals, action histories, and
environmental states, leading to degraded LLM ensemble performance. To address
this, we introduce WebRouter, a novel query-specific router trained from an
information-theoretic perspective. Our core contribution is a cost-aware
Variational Information Bottleneck (ca-VIB) objective, which learns a
compressed representation of the input prompt while explicitly penalizing the
expected operational cost. Experiments on five real-world websites from the
WebVoyager benchmark show that WebRouter reduces operational costs by a
striking 87.8\% compared to a GPT-4o baseline, while incurring only a 3.8\%
accuracy drop.

</details>


### [118] [Fairness Metric Design Exploration in Multi-Domain Moral Sentiment Classification using Transformer-Based Models](https://arxiv.org/abs/2510.11222)
*Battemuulen Naranbat,Seyed Sahand Mohammadi Ziabari,Yousuf Nasser Al Husaini,Ali Mohammed Mansoor Alsahag*

Main category: cs.CL

TL;DR: 该论文研究了NLP中道德情感分类的公平性问题，特别是在跨域场景下。通过分析BERT和DistilBERT在Twitter和Reddit数据集上的表现，发现总体性能会掩盖公平性差异，并提出新的道德公平一致性(MFC)指标来评估跨域稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言处理中道德情感分类的公平性挑战，特别是在跨域迁移场景下，因为总体性能指标可能掩盖不同道德标签之间的公平性差异。

Method: 使用Moral Foundations Twitter Corpus和Moral Foundations Reddit Corpus，评估BERT和DistilBERT在多标签分类任务中的表现，包括域内和跨域协议。引入Moral Fairness Consistency(MFC)指标来量化跨域稳定性。

Result: 发现跨域迁移存在明显不对称性：Twitter->Reddit的micro-F1下降14.9%，而Reddit->Twitter仅下降1.5%。权威标签表现出最大的公平性违规，Demographic Parity Differences为0.22-0.23，Equalized Odds Differences为0.40-0.41。忠诚标签具有最高的MFC一致性(0.96)，权威标签最低(0.78)。

Conclusion: MFC指标与Demographic Parity Difference呈现完美负相关(rho=-1.000)，可作为诊断导向的补充指标，用于公平性感知的道德推理模型评估，实现在异构语言环境中更可靠的部署。

Abstract: Ensuring fairness in natural language processing for moral sentiment
classification is challenging, particularly under cross-domain shifts where
transformer models are increasingly deployed. Using the Moral Foundations
Twitter Corpus (MFTC) and Moral Foundations Reddit Corpus (MFRC), this work
evaluates BERT and DistilBERT in a multi-label setting with in-domain and
cross-domain protocols. Aggregate performance can mask disparities: we observe
pronounced asymmetry in transfer, with Twitter->Reddit degrading micro-F1 by
14.9% versus only 1.5% for Reddit->Twitter. Per-label analysis reveals fairness
violations hidden by overall scores; notably, the authority label exhibits
Demographic Parity Differences of 0.22-0.23 and Equalized Odds Differences of
0.40-0.41. To address this gap, we introduce the Moral Fairness Consistency
(MFC) metric, which quantifies the cross-domain stability of moral foundation
detection. MFC shows strong empirical validity, achieving a perfect negative
correlation with Demographic Parity Difference (rho = -1.000, p < 0.001) while
remaining independent of standard performance metrics. Across labels, loyalty
demonstrates the highest consistency (MFC = 0.96) and authority the lowest (MFC
= 0.78). These findings establish MFC as a complementary, diagnosis-oriented
metric for fairness-aware evaluation of moral reasoning models, enabling more
reliable deployment across heterogeneous linguistic contexts. .

</details>


### [119] [A Theorem-Proving-Based Evaluation of Neural Semantic Parsing](https://arxiv.org/abs/2510.11225)
*Hayate Funakura,Hyunsoo Kim,Koji Mineshima*

Main category: cs.CL

TL;DR: 该论文重新评估了语义解析器的评估方法，将图匹配与自动定理证明相结合，发现基于图匹配的评估指标（如Smatch）不能很好地反映逻辑等价性，提出了逻辑敏感的评估和训练目标。


<details>
  <summary>Details</summary>
Motivation: 当前语义解析器评估主要使用图匹配指标（如Smatch），但这些指标只捕捉表面重叠而非逻辑等价性，需要更准确的评估方法来反映解析器的逻辑推理能力。

Method: 比较了两种解析器构建方法：监督微调（T5-Small/Base）和少样本上下文学习（GPT-4o/4.1/5），在归一化和非归一化目标下进行评估。使用图匹配、双向蕴含性验证和良好性检查三种评估方式。

Result: 研究发现，在图匹配上表现良好的模型往往无法产生逻辑等价的公式。归一化减少了目标变异性，提高了良好性和逻辑充分性。错误分析显示性能随公式复杂度增加而下降，主要失败涉及变量绑定、索引和谓词命名。

Conclusion: 图匹配指标在面向推理的应用中存在局限性，需要逻辑敏感的评估和训练目标，以及简化的归一化目标表示。

Abstract: Graph-matching metrics such as Smatch are the de facto standard for
evaluating neural semantic parsers, yet they capture surface overlap rather
than logical equivalence. We reassess evaluation by pairing graph-matching with
automated theorem proving. We compare two approaches to building parsers:
supervised fine-tuning (T5-Small/Base) and few-shot in-context learning
(GPT-4o/4.1/5), under normalized and unnormalized targets. We evaluate outputs
using graph-matching, bidirectional entailment between source and target
formulas with a first-order logic theorem prover, and well-formedness. Across
settings, we find that models performing well on graph-matching often fail to
produce logically equivalent formulas. Normalization reduces incidental target
variability, improves well-formedness, and strengthens logical adequacy. Error
analysis shows performance degrades with increasing formula complexity and with
coordination, prepositional phrases, and passive voice; the dominant failures
involve variable binding and indexing, and predicate naming. These findings
highlight limits of graph-based metrics for reasoning-oriented applications and
motivate logic-sensitive evaluation and training objectives together with
simplified, normalized target representations. All code and data for our
experiments are publicly available.

</details>


### [120] [CNSocialDepress: A Chinese Social Media Dataset for Depression Risk Detection and Structured Analysis](https://arxiv.org/abs/2510.11233)
*Jinyuan Xu,Tian Lan,Xintao Yu,Xue He,Hezhi Zhang,Ying Wang,Pierre Magistry,Mathieu Valette,Lei Li*

Main category: cs.CL

TL;DR: 发布了CNSocialDepress数据集，这是一个用于中文社交媒体抑郁风险检测的基准数据集，包含44,178条文本和10,306个抑郁相关片段，提供二元风险标签和多维心理属性。


<details>
  <summary>Details</summary>
Motivation: 抑郁是全球公共卫生问题，但公开的中文抑郁检测资源稀缺且大多限于二元分类，需要更丰富的资源来支持中文人群的心理健康应用。

Method: 构建包含44,178条中文社交媒体文本的数据集，由心理学专家标注10,306个抑郁相关片段，提供二元风险标签和结构化多维心理属性。

Result: 实验结果显示该数据集在多种NLP任务中有效，包括结构化心理分析和大型语言模型微调，能有效识别抑郁风险并进行心理分析。

Conclusion: CNSocialDepress数据集为中文人群的抑郁风险识别和心理分析提供了实用价值，有助于开发针对中文人群的心理健康应用。

Abstract: Depression is a pressing global public health issue, yet publicly available
Chinese-language resources for risk detection remain scarce and are mostly
limited to binary classification. To address this limitation, we release
CNSocialDepress, a benchmark dataset for depression risk detection from Chinese
social media posts. The dataset contains 44,178 texts from 233 users, within
which psychological experts annotated 10,306 depression-related segments.
CNSocialDepress provides binary risk labels together with structured
multi-dimensional psychological attributes, enabling interpretable and
fine-grained analysis of depressive signals. Experimental results demonstrate
its utility across a wide range of NLP tasks, including structured
psychological profiling and fine-tuning of large language models for depression
detection. Comprehensive evaluations highlight the dataset's effectiveness and
practical value for depression risk identification and psychological analysis,
thereby providing insights to mental health applications tailored for
Chinese-speaking populations.

</details>


### [121] [XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer Compression](https://arxiv.org/abs/2510.11236)
*Haoqi Yang,Yao Yao,Zuchao Li,Baoyuan Qi,Guoming Liu,Hai Zhao*

Main category: cs.CL

TL;DR: XQuant是一个无需训练、即插即用的KV缓存量化框架，通过数据无关校准和跨层压缩技术，实现了亚1.4比特的超低比特宽度化，在保持模型精度的同时显著减少内存消耗。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理长文本时KV缓存的内存需求急剧增长，限制了在资源受限环境中的部署。量化是减少内存消耗的有效方法，但现有方法在极低比特量化时面临精度损失问题。

Method: 提出XQuant框架，包含两个关键技术：1）计算量可忽略的数据无关校准方法；2）跨层KV缓存压缩技术，支持亚1.4比特的量化。

Result: 在TruthfulQA和LongBench上的实验表明，XQuant优于KIVI-2bit和AsymKV-1.5bit等先进方法，实现了更低的比特宽度同时保持更好的性能表现。

Conclusion: XQuant在内存效率和模型精度之间建立了更好的权衡，为资源受限环境中的大语言模型部署提供了有效的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse natural language processing tasks. However, their extensive memory
requirements, particularly due to KV cache growth during long-text
understanding and generation, present significant challenges for deployment in
resource-constrained environments. Quantization has emerged as a promising
solution to reduce memory consumption while preserving historical information.
We propose XQuant, a training-free and plug-and-play framework that achieves
ultra-low equivalent bit-width KV cache quantization. XQuant introduces two key
innovations: a computationally negligible data-free calibration method and
cross-layer KV cache compression, enabling quantization to sub-1.4 bits.
Extensive experiments on TruthfulQA and LongBench demonstrate that XQuant
outperforms state-of-the-art methods (e.g., KIVI-2bit and AsymKV-1.5bit) by
achieving lower bit-width while maintaining superior performance, establishing
a better trade-off between memory efficiency and model accuracy.

</details>


### [122] [Attacks by Content: Automated Fact-checking is an AI Security Issue](https://arxiv.org/abs/2510.11238)
*Michael Schlichtkrull*

Main category: cs.CL

TL;DR: 论文提出了一种新的AI代理攻击方式——内容攻击，区别于传统的指令注入攻击，攻击者通过提供有偏见、误导性或虚假信息来操纵AI代理行为。


<details>
  <summary>Details</summary>
Motivation: 现有防御机制主要关注检测隐藏指令，但对基于内容操纵的攻击无效。AI代理需要能够批判性评估检索到的信息，以保护自身和用户免受内容攻击。

Method: 将自动事实核查这一NLP任务重新定位为AI代理的认知自我防御工具，通过外部证据验证声明并评估信息来源可信度。

Result: 论文论证了内容攻击的可行性，并指出现有防御机制在此类攻击面前的不足。

Conclusion: AI代理需要集成事实核查能力作为防御内容攻击的关键机制，这代表了AI安全防御范式的转变。

Abstract: When AI agents retrieve and reason over external documents, adversaries can
manipulate the data they receive to subvert their behaviour. Previous research
has studied indirect prompt injection, where the attacker injects malicious
instructions. We argue that injection of instructions is not necessary to
manipulate agents - attackers could instead supply biased, misleading, or false
information. We term this an attack by content. Existing defenses, which focus
on detecting hidden commands, are ineffective against attacks by content. To
defend themselves and their users, agents must critically evaluate retrieved
information, corroborating claims with external evidence and evaluating source
trustworthiness. We argue that this is analogous to an existing NLP task,
automated fact-checking, which we propose to repurpose as a cognitive
self-defense tool for agents.

</details>


### [123] [Do Psychometric Tests Work for Large Language Models? Evaluation of Tests on Sexism, Racism, and Morality](https://arxiv.org/abs/2510.11254)
*Jana Jung,Marlene Lutz,Indira Sen,Markus Strohmaier*

Main category: cs.CL

TL;DR: 评估人类心理测量测试在大型语言模型中的可靠性和有效性，发现测试分数与下游任务行为不一致，表明生态效度较低。


<details>
  <summary>Details</summary>
Motivation: 随着心理测量测试越来越多地用于评估大型语言模型的心理结构，但不确定这些为人类设计的测试是否能产生有意义的结果。

Method: 系统评估三个结构（性别歧视、种族歧视和道德）的心理测量测试的可靠性和有效性，包括收敛效度和生态效度分析。

Result: 测试在不同项目和提示变体中显示出中等可靠性，但测试分数与下游任务行为不相关甚至负相关，表明生态效度低。

Conclusion: 在解释心理测量测试分数之前必须进行系统评估，为人类设计的测试不能直接应用于LLM而无需调整。

Abstract: Psychometric tests are increasingly used to assess psychological constructs
in large language models (LLMs). However, it remains unclear whether these
tests -- originally developed for humans -- yield meaningful results when
applied to LLMs. In this study, we systematically evaluate the reliability and
validity of human psychometric tests for three constructs: sexism, racism, and
morality. We find moderate reliability across multiple item and prompt
variations. Validity is evaluated through both convergent (i.e., testing
theory-based inter-test correlations) and ecological approaches (i.e., testing
the alignment between tests scores and behavior in real-world downstream
tasks). Crucially, we find that psychometric test scores do not align, and in
some cases even negatively correlate with, model behavior in downstream tasks,
indicating low ecological validity. Our results highlight that systematic
evaluations of psychometric tests is essential before interpreting their
scores. They also suggest that psychometric tests designed for humans cannot be
applied directly to LLMs without adaptation.

</details>


### [124] [Towards Real-Time Fake News Detection under Evidence Scarcity](https://arxiv.org/abs/2510.11277)
*Guangyu Wei,Ke Han,Yueming Lyu,Yu Luo,Yue Jiang,Caifeng Shan,Nicu Sebe*

Main category: cs.CL

TL;DR: 提出了EASE框架，通过评估证据充分性动态调整决策过程，解决实时假新闻检测中证据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法过度依赖外部证据，在证据稀缺情况下泛化能力差，特别是在实时场景中新兴事件往往缺乏充分证据支持。

Method: EASE框架包含三个独立评估视角：证据评估、推理评估和情感回退，通过指令调优和伪标签提升评估准确性，实现评估感知的决策。

Result: 在多个基准测试中达到最先进性能，显著提升对实时新闻的泛化能力，并发布了RealTimeNews-25新基准数据集。

Conclusion: EASE通过动态评估证据充分性和多视角决策机制，有效解决了实时假新闻检测中的证据稀缺问题，提升了检测准确性和泛化能力。

Abstract: Fake news detection becomes particularly challenging in real-time scenarios,
where emerging events often lack sufficient supporting evidence. Existing
approaches often rely heavily on external evidence and therefore struggle to
generalize under evidence scarcity. To address this issue, we propose
Evaluation-Aware Selection of Experts (EASE), a novel framework for real-time
fake news detection that dynamically adapts its decision-making process
according to the assessed sufficiency of available evidence. EASE introduces a
sequential evaluation mechanism comprising three independent perspectives: (1)
Evidence-based evaluation, which assesses evidence and incorporates it into
decision-making only when the evidence is sufficiently supportive; (2)
Reasoning-based evaluation, which leverages the world knowledge of large
language models (LLMs) and applies them only when their reliability is
adequately established; and (3) Sentiment-based fallback, which integrates
sentiment cues when neither evidence nor reasoning is reliable. To enhance the
accuracy of evaluation processes, EASE employs instruction tuning with pseudo
labels to guide each evaluator in justifying its perspective-specific knowledge
through interpretable reasoning. Furthermore, the expert modules integrate the
evaluators' justified assessments with the news content to enable
evaluation-aware decision-making, thereby enhancing overall detection accuracy.
Moreover, we introduce RealTimeNews-25, a new benchmark comprising recent news
for evaluating model generalization on emerging news with limited evidence.
Extensive experiments demonstrate that EASE not only achieves state-of-the-art
performance across multiple benchmarks, but also significantly improves
generalization to real-time news. The code and dataset are available:
https://github.com/wgyhhhh/EASE.

</details>


### [125] [Emergent Misalignment via In-Context Learning: Narrow in-context examples can produce broadly misaligned LLMs](https://arxiv.org/abs/2510.11288)
*Nikita Afonin,Nikita Andriyanov,Nikhil Bageshpura,Kyle Liu,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Alexander Panchenko,Oleg Rogov,Elena Tutubalina,Mikhail Seleznyov*

Main category: cs.CL

TL;DR: 研究发现情境学习(ICL)中也会出现突发性错位现象，使用64个狭窄情境示例时，前沿模型产生广泛错位响应的比例在2%-17%之间，使用256个示例时可达58%。


<details>
  <summary>Details</summary>
Motivation: 探讨突发性错位现象是否在情境学习中也会出现，此前的研究主要关注微调和激活引导。

Method: 使用三个数据集和三个前沿模型，通过情境学习提供狭窄示例，并分析逐步推理过程来研究机制。

Result: 情境学习确实会产生突发性错位，模型会通过采用鲁莽或危险的"角色"来合理化有害输出，67.5%的错位轨迹显示了这种行为。

Conclusion: 突发性错位不仅存在于微调中，也存在于情境学习中，模型会通过角色扮演来合理化有害行为，这对AI安全具有重要意义。

Abstract: Recent work has shown that narrow finetuning can produce broadly misaligned
LLMs, a phenomenon termed emergent misalignment (EM). While concerning, these
findings were limited to finetuning and activation steering, leaving out
in-context learning (ICL). We therefore ask: does EM emerge in ICL? We find
that it does: across three datasets, three frontier models produce broadly
misaligned responses at rates between 2% and 17% given 64 narrow in-context
examples, and up to 58% with 256 examples. We also examine mechanisms of EM by
eliciting step-by-step reasoning (while leaving in-context examples unchanged).
Manual analysis of the resulting chain-of-thought shows that 67.5% of
misaligned traces explicitly rationalize harmful outputs by adopting a reckless
or dangerous ''persona'', echoing prior results on finetuning-induced EM.

</details>


### [126] [Are Large Language Models Effective Knowledge Graph Constructors?](https://arxiv.org/abs/2510.11297)
*Ruirui Chen,Weifeng Jiang,Chengwei Qin,Bo Xiong,Fiona Liausvia,Dongkyu Choi,Boon Kiat Quek*

Main category: cs.CL

TL;DR: 提出分层提取框架构建高质量知识图谱，评估LLM在KG构建中的表现，并发布儿童心理健康研究论文的KG数据集。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法主要关注实体关系提取，局限于句子级上下文或预定义模式，难以构建语义丰富且结构良好的知识图谱。

Method: 使用最先进LLM的分层提取框架，在多个层次组织信息，从结构和语义角度全面评估构建的知识图谱。

Result: 结果揭示了当前LLM在KG构建中的优势和不足，识别了未来工作的关键挑战。

Conclusion: 发布儿童心理健康领域的KG数据集，旨在促进高风险领域（如医疗）中更透明、可靠和有影响力的应用。

Abstract: Knowledge graphs (KGs) are vital for knowledge-intensive tasks and have shown
promise in reducing hallucinations in large language models (LLMs). However,
constructing high-quality KGs remains difficult, requiring accurate information
extraction and structured representations that support interpretability and
downstream utility. Existing LLM-based approaches often focus narrowly on
entity and relation extraction, limiting coverage to sentence-level contexts or
relying on predefined schemas. We propose a hierarchical extraction framework
that organizes information at multiple levels, enabling the creation of
semantically rich and well-structured KGs. Using state-of-the-art LLMs, we
extract and construct knowledge graphs and evaluate them comprehensively from
both structural and semantic perspectives. Our results highlight the strengths
and shortcomings of current LLMs in KG construction and identify key challenges
for future work. To advance research in this area, we also release a curated
dataset of LLM-generated KGs derived from research papers on children's mental
well-being. This resource aims to foster more transparent, reliable, and
impactful applications in high-stakes domains such as healthcare.

</details>


### [127] [FOSSIL: Harnessing Feedback on Suboptimal Samples for Data-Efficient Generalisation with Imitation Learning for Embodied Vision-and-Language Tasks](https://arxiv.org/abs/2510.11307)
*Sabrina McCallum,Amit Parekh,Alessandro Suglia*

Main category: cs.CL

TL;DR: 本文提出了一种使用语言反馈来增强模仿学习的方法，使智能体能够从最优和次优演示中学习，提高组合泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前基于专家演示的模仿学习方法无法评估演示质量，只能学习最优行为或复制错误。强化学习虽然提供了替代方案，但探索过程牺牲了数据效率。

Method: 将语言反馈嵌入作为Transformer策略的输入序列，并可选地添加辅助自监督学习目标进行反馈预测。在BabyAI-XGen环境中测试。

Result: 在具身视觉语言任务上显示出显著的组合泛化能力和鲁棒性提升，表明该方法能成功将次优行为转化为学习机会。

Conclusion: 语言反馈是语言指定具身任务中中间标量奖励的竞争性且直观的替代方案。

Abstract: Current approaches to embodied AI tend to learn policies from expert
demonstrations. However, without a mechanism to evaluate the quality of
demonstrated actions, they are limited to learning from optimal behaviour, or
they risk replicating errors and inefficiencies. While reinforcement learning
offers one alternative, the associated exploration typically results in
sacrificing data efficiency. This work explores how agents trained with
imitation learning can learn robust representations from both optimal and
suboptimal demonstrations when given access to constructive language feedback
as a means to contextualise different modes of behaviour. We directly provide
language feedback embeddings as part of the input sequence into a
Transformer-based policy, and optionally complement the traditional next action
prediction objective with auxiliary self-supervised learning objectives for
feedback prediction. We test our approach on a range of embodied
Vision-and-Language tasks in our custom BabyAI-XGen environment and show
significant improvements in agents' compositional generalisation abilities and
robustness, suggesting that our data-efficient method allows models to
successfully convert suboptimal behaviour into learning opportunities. Overall,
our results suggest that language feedback is a competitive and intuitive
alternative to intermediate scalar rewards for language-specified embodied
tasks.

</details>


### [128] [Template-Based Text-to-Image Alignment for Language Accessibility: A Study on Visualizing Text Simplifications](https://arxiv.org/abs/2510.11314)
*Belkiss Souayed,Sarah Ebling,Yingqiang Gao*

Main category: cs.CL

TL;DR: 提出了一个结构化视觉语言模型提示框架，用于从简化文本生成无障碍图像，通过五种提示模板和专家评估验证了基本对象聚焦模板在语义对齐方面的最佳表现。


<details>
  <summary>Details</summary>
Motivation: 智力障碍人士在理解复杂文本时存在困难，现有文本到图像模型注重美观性而非可访问性，需要研究视觉插图与文本简化之间的关系。

Method: 设计了五种提示模板，使用400个句子级简化文本进行两阶段评估：第一阶段用CLIP分数评估提示模板效果，第二阶段由四位无障碍专家对生成的十种视觉风格图像进行人工标注。

Result: 基本对象聚焦提示模板实现了最高的语义对齐，复古风格被认为是最无障碍的视觉风格，维基百科是最有效的数据源。文本简单性维度显示出强可靠性，而图像质量更为主观。

Conclusion: 该框架为无障碍内容生成提供了实用指南，强调了结构化提示在AI生成视觉无障碍工具中的重要性，视觉极简主义能够增强语言可访问性。

Abstract: Individuals with intellectual disabilities often have difficulties in
comprehending complex texts. While many text-to-image models prioritize
aesthetics over accessibility, it is not clear how visual illustrations relate
to text simplifications (TS) generated from them. This paper presents a
structured vision-language model (VLM) prompting framework for generating
accessible images from simplified texts. We designed five prompt templates,
i.e., Basic Object Focus, Contextual Scene, Educational Layout, Multi-Level
Detail, and Grid Layout, each following distinct spatial arrangements while
adhering to accessibility constraints such as object count limits, spatial
separation, and content restrictions. Using 400 sentence-level simplifications
from four established TS datasets (OneStopEnglish, SimPA, Wikipedia, and
ASSET), we conducted a two-phase evaluation: Phase 1 assessed prompt template
effectiveness with CLIPScores, and Phase 2 involved human annotation of
generated images across ten visual styles by four accessibility experts.
Results show that the Basic Object Focus prompt template achieved the highest
semantic alignment, indicating that visual minimalism enhances language
accessibility. Expert evaluation further identified Retro style as the most
accessible and Wikipedia as the most effective data source. Inter-annotator
agreement varied across dimensions, with Text Simplicity showing strong
reliability and Image Quality proving more subjective. Overall, our framework
offers practical guidelines for accessible content generation and underscores
the importance of structured prompting in AI-generated visual accessibility
tools.

</details>


### [129] [Do LLMs "Feel"? Emotion Circuits Discovery and Control](https://arxiv.org/abs/2510.11328)
*Chenxi Wang,Yixuan Zhang,Ruiji Yu,Yufei Zheng,Lang Gao,Zirui Song,Zixiang Xu,Gus Xia,Huishuai Zhang,Dongyan Zhao,Xiuying Chen*

Main category: cs.CL

TL;DR: 该研究系统性地揭示了LLMs中的情感电路机制，通过识别神经元和注意力头构建全局情感电路，实现了99.65%的情感表达控制精度。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs对情感智能需求的增长，需要理解其情感表达的内部机制并实现情感控制。

Method: 构建SEV数据集提取情境无关的情感方向，通过分解分析和因果分析识别情感计算组件，构建全局情感电路。

Result: 识别出情感神经元和注意力头，验证其因果作用，通过调制情感电路实现了99.65%的情感表达准确率。

Conclusion: 这是首个系统性揭示和验证LLMs情感电路的研究，为可解释性和可控情感智能提供了新见解。

Abstract: As the demand for emotional intelligence in large language models (LLMs)
grows, a key challenge lies in understanding the internal mechanisms that give
rise to emotional expression and in controlling emotions in generated text.
This study addresses three core questions: (1) Do LLMs contain context-agnostic
mechanisms shaping emotional expression? (2) What form do these mechanisms
take? (3) Can they be harnessed for universal emotion control? We first
construct a controlled dataset, SEV (Scenario-Event with Valence), to elicit
comparable internal states across emotions. Subsequently, we extract
context-agnostic emotion directions that reveal consistent, cross-context
encoding of emotion (Q1). We identify neurons and attention heads that locally
implement emotional computation through analytical decomposition and causal
analysis, and validate their causal roles via ablation and enhancement
interventions. Next, we quantify each sublayer's causal influence on the
model's final emotion representation and integrate the identified local
components into coherent global emotion circuits that drive emotional
expression (Q2). Directly modulating these circuits achieves 99.65%
emotion-expression accuracy on the test set, surpassing prompting- and
steering-based methods (Q3). To our knowledge, this is the first systematic
study to uncover and validate emotion circuits in LLMs, offering new insights
into interpretability and controllable emotional intelligence.

</details>


### [130] [LLM-Specific Utility: A New Perspective for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.11358)
*Hengran Zhang,Keping Bi,Jiafeng Guo,Jiaming Zhang,Shuaiqiang Wang,Dawei Yin,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本文提出了LLM特定效用的概念，发现人类标注的段落对LLM并非最优，且不同LLM的效用段落不可迁移，强调了在RAG研究中采用LLM特定效用的必要性。


<details>
  <summary>Details</summary>
Motivation: 传统检索关注相关性，但RAG的有效性取决于检索段落的效用。现有研究将效用视为通用属性，忽略了不同LLM由于内部知识和理解能力差异，可能从同一段落中获益不同。

Method: 通过跨多个数据集和LLM的大规模实验，提出LLM特定效用的概念，并基于困惑度等指标分析查询和段落的可读性差异。提出LLM特定效用判断的基准测试程序。

Result: 实验表明人类标注的段落对特定LLM并非最优效用段落，真实效用段落在不同LLM间不可迁移。使用伪答案的言语化方法表现稳健，但LLM在评估效用方面存在困难。

Conclusion: RAG研究必须考虑LLM特定效用，人类标注的效用段落可能不适用于特定LLM，需要开发更好的LLM特定效用评估方法。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
incorporating external knowledge. While traditional retrieval focuses on
relevance, RAG's effectiveness depends on the utility of retrieved passages,
i.e., the usefulness in facilitating the generation of an accurate and
comprehensive answer. Existing studies often treat utility as a generic
attribute, ignoring the fact that different LLMs may benefit differently from
the same passage due to variations in internal knowledge and comprehension
ability. In this work, we introduce and systematically investigate the notion
of LLM-specific utility. Through large-scale experiments across multiple
datasets and LLMs, we demonstrate that human-annotated passages are not optimal
for LLMs and that ground-truth utilitarian passages are not transferable across
different LLMs. These findings highlight the necessity of adopting the
LLM-specific utility in RAG research. Our findings indicate that some
human-annotated passages are not ground-truth utilitarian passages for specific
LLMs, partially due to the varying readability of queries and passages for
LLMs, a tendency for which perplexity is a key metric. Based on these findings,
we propose a benchmarking procedure for LLM-specific utility judgments. We
evaluate existing utility judgment methods on six datasets and find that while
verbalized methods using pseudo-answers perform robustly, LLMs struggle to
assess utility effectively-failing to reject all passages for known queries and
to select truly useful ones for unknown queries.

</details>


### [131] [Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers](https://arxiv.org/abs/2510.11370)
*Wenhan Ma,Hailin Zhang,Liang Zhao,Yifan Song,Yudong Wang,Zhifang Sui,Fuli Luo*

Main category: cs.CL

TL;DR: 提出了Rollout Routing Replay (R3)方法来解决MoE模型在强化学习训练中的路由不稳定性问题，通过记录推理阶段的路由分布并在训练时重放，显著减少了训练-推理策略差异。


<details>
  <summary>Details</summary>
Motivation: MoE模型的路由机制在强化学习训练中会引入不稳定性，甚至导致灾难性训练崩溃。研究发现训练和推理阶段存在显著的路由行为差异，这是导致问题的根本原因。

Method: 提出R3方法：记录推理引擎中的路由分布，并在训练阶段重放这些分布，从而减少训练-推理策略KL散度，缓解极端差异，同时不降低训练速度。

Result: 在各种设置下的广泛实验证实，R3成功稳定了RL训练，防止了崩溃，并且在性能上超越了GSPO和TIS等方法。

Conclusion: 这项工作为稳定MoE模型中的强化学习训练提供了一个新的解决方案，通过解决训练-推理一致性这一根本问题来提升模型稳定性。

Abstract: Reinforcement learning (RL) has emerged as a crucial approach for enhancing
the capabilities of large language models. However, in Mixture-of-Experts (MoE)
models, the routing mechanism often introduces instability, even leading to
catastrophic RL training collapse. We analyze the training-inference
consistency of MoE models and identify a notable discrepancy in routing
behaviors between the two phases. Moreover, even under identical conditions,
the routing framework can yield divergent expert selections across repeated
forward passes. To address this foundational inconsistency, we propose Rollout
Routing Replay (R3), a method that records routing distributions from the
inference engine and replays them during training. R3 significantly reduces
training-inference policy KL divergence and mitigates extreme discrepancies
without compromising training speed. Extensive experiments on various settings
confirm that R3 succeeds in stabilizing RL training, preventing collapse and
outperforming methods such as GSPO and TIS. We believe this work can offer a
new solution for stabilizing RL in MoE models.

</details>


### [132] [Early Detection and Reduction of Memorisation for Domain Adaptation and Instruction Tuning](https://arxiv.org/abs/2510.11372)
*Dean L. Slack,Noura Al Moubayed*

Main category: cs.CL

TL;DR: 该论文研究发现大型语言模型在微调过程中会显著记忆训练数据，提出使用n-gram记忆分数作为早期停止标准，并引入n-gram感知损失正则化器来减少记忆，同时最小化性能损失。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然表现出色，但会记忆训练数据，从而暴露私有或受版权保护的文本。现有防御主要针对预训练阶段，而对微调过程中的记忆问题了解不足，特别是在领域适应和指令调优方面。

Method: 在Pythia、Llama3和Mistral模型（1.4B-70B参数）上进行微调实验，跟踪逐字记忆情况。提出使用n-gram记忆分数作为早期停止标准，并引入n-gram感知损失正则化器来减少记忆。

Result: 研究发现记忆在前几个训练周期急剧增加，通常早于验证困惑度或评估性能达到最优。提出的方法可将记忆减少高达40%，同时与现有记忆缓解策略相比最小化评估性能损失。

Conclusion: 这些结果为语言模型微调过程中的记忆动态提供了实用、可扩展的见解，提出的方法能有效缓解记忆问题并保持模型性能。

Abstract: Although large language models excel across many tasks, they can memorise
training data and thereby expose private or copyrighted text. Most defences
target the pre-training stage, leaving memorisation during fine-tuning,
especially for domain adaptation and instruction tuning, poorly understood. We
fine-tune Pythia, Llama3, and Mistral models spanning 1.4B-70B parameters on
common evaluation datasets and track verbatim memorisation throughout training.
We find that memorisation increases dramatically in the first few epochs, often
significantly before either validation perplexity or evaluation performance is
optimised. We use a simple but effective n-gram memorisation score which
reliably precedes verbatim memorisation; using it as an early-stopping
criterion mitigates memorisation with minimal performance loss. Further, we
introduce an n-gram-aware loss regulariser and show that it reduces
memorisation across all model families tested by up to 40% while minimising
evaluation performance trade-offs when compared to an existing memorisation
mitigation strategy. These results yield practical, scalable insights into
memorisation dynamics during language model fine-tuning.

</details>


### [133] [Beyond Survival: Evaluating LLMs in Social Deduction Games with Human-Aligned Strategies](https://arxiv.org/abs/2510.11389)
*Zirui Song,Yuan Huang,Junchang Liu,Haozhe Luo,Chenxi Wang,Lang Gao,Zixiang Xu,Mingfei Han,Xiaojun Chang,Xiuying Chen*

Main category: cs.CL

TL;DR: 提出了一个高质量的多模态狼人杀数据集和策略对齐评估框架，用于评估语言模型在社交推理和策略游戏中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究将狼人杀游戏简化为LLM自博弈，产生模板化对话并忽视社交游戏的丰富性，且缺乏质量参考数据和细粒度评估方法。

Method: 构建包含100+小时视频、3240万词条的多模态数据集，提出两阶段策略对齐评估：言语评估（多选任务评估社交能力五个维度）和决策评估（评估投票选择和角色推断）。

Result: 实验显示最先进的LLM表现多样，约一半模型得分低于0.50，在欺骗和反事实推理方面存在明显差距。

Conclusion: 该数据集和评估框架为多智能体交互中的语言、推理和策略研究提供了新基准，揭示了当前模型在社交推理方面的局限性。

Abstract: Social deduction games like Werewolf combine language, reasoning, and
strategy, providing a testbed for studying natural language and social
intelligence. However, most studies reduce the game to LLM-based self-play,
yielding templated utterances and anecdotal cases that overlook the richness of
social gameplay. Evaluation further relies on coarse metrics such as survival
time or subjective scoring due to the lack of quality reference data. To
address these gaps, we curate a high-quality, human-verified multimodal
Werewolf dataset containing over 100 hours of video, 32.4M utterance tokens,
and 15 rule variants. Based on this dataset, we propose a novel
strategy-alignment evaluation that leverages the winning faction's strategies
as ground truth in two stages: 1) Speech evaluation, formulated as
multiple-choice-style tasks that assess whether the model can adopt appropriate
stances across five dimensions of social ability; and 2) Decision evaluation,
which assesses the model's voting choices and opponent-role inferences. This
framework enables a fine-grained evaluation of models' linguistic and reasoning
capabilities, while capturing their ability to generate strategically coherent
gameplay. Our experiments show that state-of-the-art LLMs show diverse
performance, with roughly half remain below 0.50, revealing clear gaps in
deception and counterfactual reasoning. We hope our dataset further inspires
research on language, reasoning, and strategy in multi-agent interaction.

</details>


### [134] [KnowRL: Teaching Language Models to Know What They Know](https://arxiv.org/abs/2510.11407)
*Sahil Kale,Devendra Singh Dhami*

Main category: cs.CL

TL;DR: KnowRL框架通过自我反思和共识奖励机制，无需外部监督就能增强LLM的自我认知能力，显著提高模型对自身能力边界的判断准确性。


<details>
  <summary>Details</summary>
Motivation: 当前最好的LLM在超过五分之一的情况下会错误判断自身能力，导致基于这种内部不确定性的响应不可信。需要开发能够增强模型自我认知的方法来构建更可靠的AI系统。

Method: 结合两个组件：(1) 自我反思：模型生成并分类其判断为可行或不可行的任务；(2) 共识奖励：通过内部一致性来强化自我知识评估的稳定性。使用内部生成的数据，避免昂贵的外部监督。

Result: 在LLaMA-3.1-8B和Qwen-2.5-7B上的实验显示，KnowRL持续改善了自我认知，准确率提升高达28%，F1分数提升12%，仅需少量种子集和几轮迭代就能超越基线方法。

Conclusion: 该框架解锁了LLM自我改进知识意识的潜力，为构建更可靠、更负责任的AI以及在关键应用中更安全部署打开了大门。由于其简单性和不依赖外部努力，建议将此可靠性增强过程应用于所有未来模型。

Abstract: Truly reliable AI requires more than simply scaling up knowledge; it demands
the ability to know what it knows and when it does not. Yet recent research
shows that even the best LLMs misjudge their own competence in more than one in
five cases, making any response born of such internal uncertainty impossible to
fully trust. Inspired by self-improvement reinforcement learning techniques
that require minimal data, we present a simple but powerful framework KnowRL
that strengthens a model's internal understanding of its own feasibility
boundaries, enabling safer and more responsible behaviour. Our framework
combines two components: (i) introspection, where the model generates and
classifies tasks it judges feasible or infeasible, and (ii) consensus-based
rewarding, where stability of self-knowledge assessment is reinforced through
internal agreement. By using internally generated data, this design strengthens
consistency in self-knowledge and entirely avoids costly external supervision.
In experiments on LLaMA-3.1-8B and Qwen-2.5-7B, KnowRL steadily improved
self-knowledge, validated by both intrinsic self-consistency and extrinsic
benchmarking. With nothing more than a small seed set and no external
supervision, our method drove gains as high as 28% in accuracy and 12% in F1,
outperforming baselines in just a few iterations. Our framework essentially
unlocks the untapped capacity of LLMs to self-improve their knowledge
awareness, opening the door to reliable, more accountable AI and safer
deployment in critical applications. Owing to its simplicity and independence
from external effort, we encourage applying this reliability-enhancing process
to all future models.

</details>


### [135] [Valid Survey Simulations with Limited Human Data: The Roles of Prompting, Fine-Tuning, and Rectification](https://arxiv.org/abs/2510.11408)
*Stefan Krsteski,Giuseppe Russo,Serina Chang,Robert West,Kristina Gligorić*

Main category: cs.CL

TL;DR: 研究表明，将大语言模型生成调查响应与去偏方法结合，在固定预算下将大部分人类响应分配给去偏而非微调，能显著降低偏差并提高估计效果。


<details>
  <summary>Details</summary>
Motivation: 调查执行成本高且耗时，大语言模型作为人类受访者的替代方案存在输出偏见和无效估计的问题，需要研究如何优化人类响应的分配策略。

Method: 研究合成方法（使用LLM生成调查响应）与校正方法（对人口估计进行去偏）的相互作用，探索人类响应在两者间的最佳分配方式，使用包含营养、政治和经济问题的两个面板调查。

Result: 单独使用合成方法引入显著偏差（24-86%），而结合校正方法可将偏差降至5%以下，有效样本量提升达14%。在固定预算下，将大部分人类响应分配给校正而非微调能实现更有效的估计。

Conclusion: 挑战了将所有人类响应用于微调的常见做法，证明在固定预算下将大部分响应分配给校正方法能获得更有效的人口估计结果。

Abstract: Surveys provide valuable insights into public opinion and behavior, but their
execution is costly and slow. Large language models (LLMs) have been proposed
as a scalable, low-cost substitute for human respondents, but their outputs are
often biased and yield invalid estimates. We study the interplay between
synthesis methods that use LLMs to generate survey responses and rectification
methods that debias population estimates, and explore how human responses are
best allocated between them. Using two panel surveys with questions on
nutrition, politics, and economics, we find that synthesis alone introduces
substantial bias (24-86%), whereas combining it with rectification reduces bias
below 5% and increases effective sample size by up to 14%. Overall, we
challenge the common practice of using all human responses for fine-tuning,
showing that under a fixed budget, allocating most to rectification results in
far more effective estimation.

</details>


### [136] [Who are you, ChatGPT? Personality and Demographic Style in LLM-Generated Content](https://arxiv.org/abs/2510.11434)
*Dana Sotto Porat,Ella Rabinovich*

Main category: cs.CL

TL;DR: 提出了一种无需自我报告问卷的数据驱动方法来评估LLM个性，使用自动个性分类器分析模型在Reddit开放式问题上的回复，发现LLM表现出更高的宜人性和更低的神经质。


<details>
  <summary>Details</summary>
Motivation: 研究生成大语言模型是否在其语言中表现出类似人格和人口统计学特征，传统方法依赖自我报告问卷，需要更客观的评估方法。

Method: 使用自动个性和性别分类器分析六个广泛使用的LLM在Reddit开放式问题上的回复，并与人类作者的回答进行比较。

Result: LLM系统性地表达更高的宜人性和更低的神经质，反映合作稳定的对话倾向；性别语言模式与人类相似但变异较小。

Conclusion: 贡献了新的人类和模型回复数据集及大规模比较分析，为生成AI个性和人口统计模式研究提供了新视角。

Abstract: Generative large language models (LLMs) have become central to everyday life,
producing human-like text across diverse domains. A growing body of research
investigates whether these models also exhibit personality- and
demographic-like characteristics in their language. In this work, we introduce
a novel, data-driven methodology for assessing LLM personality without relying
on self-report questionnaires, applying instead automatic personality and
gender classifiers to model replies on open-ended questions collected from
Reddit. Comparing six widely used models to human-authored responses, we find
that LLMs systematically express higher Agreeableness and lower Neuroticism,
reflecting cooperative and stable conversational tendencies. Gendered language
patterns in model text broadly resemble those of human writers, though with
reduced variation, echoing prior findings on automated agents. We contribute a
new dataset of human and model responses, along with large-scale comparative
analyses, shedding new light on the topic of personality and demographic
patterns of generative AI.

</details>


### [137] [Investigating Large Language Models' Linguistic Abilities for Text Preprocessing](https://arxiv.org/abs/2510.11482)
*Marco Braga,Gian Carlo Milanese,Gabriella Pasi*

Main category: cs.CL

TL;DR: 使用大型语言模型进行文本预处理，在六种欧洲语言上达到97%停用词去除、82%词形还原和74%词干提取准确率，相比传统方法在文本分类任务中F1分数提升达6%。


<details>
  <summary>Details</summary>
Motivation: 传统文本预处理方法忽略上下文信息，而大型语言模型能够考虑上下文且不需要大量语言特定标注资源。

Method: 使用LLM执行停用词去除、词形还原和词干提取等预处理任务，并在六种欧洲语言的文本分类任务中与传统算法进行比较评估。

Result: LLM能够以97%、82%和74%的准确率分别复制传统停用词去除、词形还原和词干提取方法，使用LLM预处理的文本训练的机器学习算法F1分数比传统技术提升达6%。

Conclusion: LLM在文本预处理任务中表现出色，能够有效考虑上下文信息，在多种语言上超越传统预处理方法。

Abstract: Text preprocessing is a fundamental component of Natural Language Processing,
involving techniques such as stopword removal, stemming, and lemmatization to
prepare text as input for further processing and analysis. Despite the
context-dependent nature of the above techniques, traditional methods usually
ignore contextual information. In this paper, we investigate the idea of using
Large Language Models (LLMs) to perform various preprocessing tasks, due to
their ability to take context into account without requiring extensive
language-specific annotated resources. Through a comprehensive evaluation on
web-sourced data, we compare LLM-based preprocessing (specifically stopword
removal, lemmatization and stemming) to traditional algorithms across multiple
text classification tasks in six European languages. Our analysis indicates
that LLMs are capable of replicating traditional stopword removal,
lemmatization, and stemming methods with accuracies reaching 97%, 82%, and 74%,
respectively. Additionally, we show that ML algorithms trained on texts
preprocessed by LLMs achieve an improvement of up to 6% with respect to the
$F_1$ measure compared to traditional techniques. Our code, prompts, and
results are publicly available at
https://github.com/GianCarloMilanese/llm_pipeline_wi-iat.

</details>


### [138] [GenCNER: A Generative Framework for Continual Named Entity Recognition](https://arxiv.org/abs/2510.11444)
*Yawen Yang,Fukun Ma,Shiao Meng,Aiwei Liu,Lijie Wen*

Main category: cs.CL

TL;DR: 提出了GenCNER，一个基于生成的持续命名实体识别框架，通过将CNER任务转换为持续的三元组序列生成问题，并使用预训练的seq2seq模型来解决，同时设计了类型特定的置信度伪标签策略和知识蒸馏来缓解灾难性遗忘和语义偏移问题。


<details>
  <summary>Details</summary>
Motivation: 现实场景中实体类别持续增加，传统NER无法适应，而现有的持续学习方法面临灾难性遗忘和非实体类型语义偏移的挑战。

Method: 将CNER任务转换为持续的三元组序列生成问题，使用预训练seq2seq模型，结合类型特定的置信度伪标签策略和知识蒸馏技术。

Result: 在两个基准数据集上的实验表明，该方法在多个CNER设置下优于现有最先进方法，与非持续学习结果的差距最小。

Conclusion: GenCNER是一个简单有效的生成式框架，能够有效缓解持续命名实体识别中的灾难性遗忘和语义偏移问题。

Abstract: Traditional named entity recognition (NER) aims to identify text mentions
into pre-defined entity types. Continual Named Entity Recognition (CNER) is
introduced since entity categories are continuously increasing in various
real-world scenarios. However, existing continual learning (CL) methods for NER
face challenges of catastrophic forgetting and semantic shift of non-entity
type. In this paper, we propose GenCNER, a simple but effective Generative
framework for CNER to mitigate the above drawbacks. Specifically, we skillfully
convert the CNER task into sustained entity triplet sequence generation problem
and utilize a powerful pre-trained seq2seq model to solve it. Additionally, we
design a type-specific confidence-based pseudo labeling strategy along with
knowledge distillation (KD) to preserve learned knowledge and alleviate the
impact of label noise at the triplet level. Experimental results on two
benchmark datasets show that our framework outperforms previous
state-of-the-art methods in multiple CNER settings, and achieves the smallest
gap compared with non-CL results.

</details>


### [139] [SemCSE-Multi: Multifaceted and Decodable Embeddings for Aspect-Specific and Interpretable Scientific Domain Mapping](https://arxiv.org/abs/2510.11599)
*Marc Brinner,Sina Zarrieß*

Main category: cs.CL

TL;DR: 提出SemCSE-Multi框架，为科学摘要生成多方面的嵌入向量，支持细粒度相似性评估和自适应可视化，并引入可解释的嵌入解码方法。


<details>
  <summary>Details</summary>
Motivation: 现有嵌入方法通常生成单一向量表示，无法捕捉科学文献中不同方面的语义信息，限制了细粒度分析和用户可控的可视化能力。

Method: 使用无监督方法生成特定方面的摘要句子，训练嵌入模型将语义相关的摘要映射到邻近位置，然后蒸馏到统一模型中实现单次前向预测多个方面嵌入。

Result: 在入侵生物学和医学领域验证了框架有效性，能够生成多方面嵌入并实现可解释的嵌入解码，即使在低维可视化的未占据区域也保持解码效果。

Conclusion: SemCSE-Multi框架实现了科学摘要的多方面语义表示，提供了细粒度相似性评估和用户驱动的可视化能力，显著提升了嵌入方法的可解释性。

Abstract: We propose SemCSE-Multi, a novel unsupervised framework for generating
multifaceted embeddings of scientific abstracts, evaluated in the domains of
invasion biology and medicine. These embeddings capture distinct, individually
specifiable aspects in isolation, thus enabling fine-grained and controllable
similarity assessments as well as adaptive, user-driven visualizations of
scientific domains. Our approach relies on an unsupervised procedure that
produces aspect-specific summarizing sentences and trains embedding models to
map semantically related summaries to nearby positions in the embedding space.
We then distill these aspect-specific embedding capabilities into a unified
embedding model that directly predicts multiple aspect embeddings from a
scientific abstract in a single, efficient forward pass. In addition, we
introduce an embedding decoding pipeline that decodes embeddings back into
natural language descriptions of their associated aspects. Notably, we show
that this decoding remains effective even for unoccupied regions in
low-dimensional visualizations, thus offering vastly improved interpretability
in user-centric settings.

</details>


### [140] [LLM-Oriented Token-Adaptive Knowledge Distillation](https://arxiv.org/abs/2510.11615)
*Xurong Xie,Zhucun Xue,Jiafu Wu,Jian Li,Yabiao Wang,Xiaobin Hu,Yong Liu,Jiangning Zhang*

Main category: cs.CL

TL;DR: 提出AdaKD框架，通过动态调整知识蒸馏过程以适应每个token的实时学习状态，包含损失驱动的自适应token聚焦和逆难度温度缩放两个模块，显著提升各种蒸馏方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于logit的知识蒸馏方法采用静态策略，与学生模型的动态学习过程不匹配，对所有token无差别处理并使用固定温度，导致知识转移效果不佳。

Method: AdaKD框架包含两个协同模块：1) 损失驱动的自适应token聚焦(LATF)，根据学生学习稳定性动态调整蒸馏焦点；2) 逆难度温度缩放(IDTS)，对困难token使用低温进行针对性纠错，对简单token使用高温鼓励学习教师完整平滑的输出分布。

Result: AdaKD作为即插即用框架，能够在多种模型架构和基准测试上持续提升各种蒸馏方法的性能。

Conclusion: AdaKD通过token级别的自适应策略有效解决了传统知识蒸馏方法的局限性，实现了更优化的知识转移效果。

Abstract: Knowledge distillation (KD) is a key technique for compressing large-scale
language models (LLMs), yet prevailing logit-based methods typically employ
static strategies that are misaligned with the dynamic learning process of
student models. These methods typically treat all tokens indiscriminately and
apply a single, fixed temperature, resulting in suboptimal knowledge transfer.
To address these limitations, we propose LLM-Oriented Token-Adaptive Knowledge
Distillation (AdaKD), a novel framework that adapts the distillation process to
the real-time learning state of each token. AdaKD consists of two synergistic
modules driven by a unified token difficulty metric. First, our Loss-Driven
Adaptive Token Focusing (LATF) module dynamically adjusts the distillation
focus by monitoring the student's learning stability, concentrating
computational resources on the most valuable tokens at each training phase.
Second, we introduce Inverse Difficulty Temperature Scaling (IDTS), a
counterintuitive yet effective token-level temperature strategy. It employs low
temperatures for difficult tokens for targeted error correction, and high
temperatures for easy tokens to encourage students to learn from the teacher's
complete and smooth output distribution, thereby enhancing generalization. As a
plug-and-play framework, AdaKD can consistently improve the performance of
various distillation methods on multiple model architectures and benchmarks.

</details>


### [141] [Hallucination Detection via Internal States and Structured Reasoning Consistency in Large Language Models](https://arxiv.org/abs/2510.11529)
*Yusheng Song,Lirong Qiu,Xi Zhang,Zhihao Tang*

Main category: cs.CL

TL;DR: 提出统一框架解决LLM幻觉检测的"检测困境"：内部状态探测和思维链验证各有局限，通过多路径推理机制和分段感知时序交叉注意力模块实现有效融合。


<details>
  <summary>Details</summary>
Motivation: 现有LLM幻觉检测方法存在"检测困境"：内部状态探测擅长检测事实不一致但无法识别逻辑谬误，思维链验证则相反，导致任务依赖的盲点。

Method: 引入多路径推理机制获取更可比较的细粒度信号，使用分段感知时序交叉注意力模块自适应融合对齐的表征，检测细微的不一致。

Result: 在三个多样化基准测试和两个领先LLM上的广泛实验表明，该框架持续显著优于强基线方法。

Conclusion: 提出的统一框架成功解决了LLM幻觉检测中的关键挑战，通过克服信号稀缺和表征对齐障碍实现了更全面的检测能力。

Abstract: The detection of sophisticated hallucinations in Large Language Models (LLMs)
is hampered by a ``Detection Dilemma'': methods probing internal states
(Internal State Probing) excel at identifying factual inconsistencies but fail
on logical fallacies, while those verifying externalized reasoning
(Chain-of-Thought Verification) show the opposite behavior. This schism creates
a task-dependent blind spot: Chain-of-Thought Verification fails on
fact-intensive tasks like open-domain QA where reasoning is ungrounded, while
Internal State Probing is ineffective on logic-intensive tasks like
mathematical reasoning where models are confidently wrong. We resolve this with
a unified framework that bridges this critical gap. However, unification is
hindered by two fundamental challenges: the Signal Scarcity Barrier, as coarse
symbolic reasoning chains lack signals directly comparable to fine-grained
internal states, and the Representational Alignment Barrier, a deep-seated
mismatch between their underlying semantic spaces. To overcome these, we
introduce a multi-path reasoning mechanism to obtain more comparable,
fine-grained signals, and a segment-aware temporalized cross-attention module
to adaptively fuse these now-aligned representations, pinpointing subtle
dissonances. Extensive experiments on three diverse benchmarks and two leading
LLMs demonstrate that our framework consistently and significantly outperforms
strong baselines. Our code is available: https://github.com/peach918/HalluDet.

</details>


### [142] [Scaling Language-Centric Omnimodal Representation Learning](https://arxiv.org/abs/2510.11693)
*Chenghao Xiao,Hou Pong Chan,Hao Zhang,Weiwen Xu,Mahani Aljunied,Yu Rong*

Main category: cs.CL

TL;DR: 该论文提出语言中心全模态嵌入框架LCO-Emb，通过分析MLLM中隐式跨模态对齐现象，发现对比学习可作为轻量级精炼阶段，并揭示了生成-表示缩放定律。


<details>
  <summary>Details</summary>
Motivation: 探索基于多模态大语言模型的嵌入方法为何优于传统方法，研究其内在的跨模态对齐机制。

Method: 提出LCO-Emb框架，通过分析各向异性和核相似性结构验证隐式对齐，利用对比学习作为精炼阶段。

Result: 在多个基准测试中达到最先进性能，验证了生成-表示缩放定律，表明生成能力提升可增强表示质量。

Conclusion: 改进生成能力是提升表示质量的有效范式，持续生成预训练可进一步增强嵌入潜力。

Abstract: Recent multimodal embedding approaches leveraging multimodal large language
models (MLLMs) fine-tuned with contrastive learning (CL) have shown promising
results, yet the underlying reasons behind their superiority remain
underexplored. This work argues that a crucial advantage of MLLM-based
approaches stems from implicit cross-modal alignment achieved during generative
pretraining, where the language decoder learns to exploit multimodal signals
within a shared representation space for generating unimodal outputs. Through
analysis of anisotropy and kernel similarity structure, we empirically confirm
that latent alignment emerges within MLLM representations, allowing CL to serve
as a lightweight refinement stage. Leveraging this insight, we propose a
Language-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensive
experiments across diverse backbones and benchmarks demonstrate its
effectiveness, achieving state-of-the-art performance across modalities.
Furthermore, we identify a Generation-Representation Scaling Law (GRSL),
showing that the representational capabilities gained through contrastive
refinement scales positively with the MLLM's generative capabilities. This
suggests that improving generative abilities evolves as an effective paradigm
for enhancing representation quality. We provide a theoretical explanation of
GRSL, which formally links the MLLM's generative quality to the upper bound on
its representation performance, and validate it on a challenging, low-resource
visual-document retrieval task, showing that continual generative pretraining
before CL can further enhance the potential of a model's embedding
capabilities. Codes, models, and resources are available at
https://github.com/LCO-Embedding/LCO-Embedding.

</details>


### [143] [An Encoder-Integrated PhoBERT with Graph Attention for Vietnamese Token-Level Classification](https://arxiv.org/abs/2510.11537)
*Ba-Quang Nguyen*

Main category: cs.CL

TL;DR: 提出TextGraphFuseGAT模型，结合预训练Transformer编码器(PhoBERT)和图注意力网络进行词元级分类任务，在越南语三个基准数据集上表现优异


<details>
  <summary>Details</summary>
Motivation: 现有序列标注模型主要依赖顺序上下文，难以充分捕捉词元间的复杂依赖关系，需要结合图结构建模来增强词元间的关系表示

Method: 构建基于PhoBERT词元嵌入的完全连接图，使用GAT层捕获词元间依赖关系，再通过Transformer自注意力层增强上下文表示，最后通过分类头进行序列标注

Result: 在PhoNER-COVID19、PhoDisfluency和VietMed-NER三个越南语数据集上均优于基线模型，包括纯Transformer模型和BiLSTM+CNN+CRF混合模型

Conclusion: 结合预训练语义特征和图关系建模能有效提升跨多个领域的词元分类性能，证明了图注意力网络在捕捉词元间复杂依赖关系方面的优势

Abstract: We propose a novel neural architecture named TextGraphFuseGAT, which
integrates a pretrained transformer encoder (PhoBERT) with Graph Attention
Networks for token-level classification tasks. The proposed model constructs a
fully connected graph over the token embeddings produced by PhoBERT, enabling
the GAT layer to capture rich inter-token dependencies beyond those modeled by
sequential context alone. To further enhance contextualization, a
Transformer-style self-attention layer is applied on top of the graph-enhanced
embeddings. The final token representations are passed through a classification
head to perform sequence labeling. We evaluate our approach on three Vietnamese
benchmark datasets: PhoNER-COVID19 for named entity recognition in the COVID-19
domain, PhoDisfluency for speech disfluency detection, and VietMed-NER for
medical-domain NER. VietMed-NER is the first Vietnamese medical spoken NER
dataset, featuring 18 entity types collected from real-world medical speech
transcripts and annotated with the BIO tagging scheme. Its specialized
vocabulary and domain-specific expressions make it a challenging benchmark for
token-level classification models. Experimental results show that our method
consistently outperforms strong baselines, including transformer-only and
hybrid neural models such as BiLSTM + CNN + CRF, confirming the effectiveness
of combining pretrained semantic features with graph-based relational modeling
for improved token classification across multiple domains.

</details>


### [144] [Information-Preserving Reformulation of Reasoning Traces for Antidistillation](https://arxiv.org/abs/2510.11545)
*Jiayu Ding,Lei Cui,Li Dong,Nanning Zheng,Furu Wei*

Main category: cs.CL

TL;DR: PART是一种保护推理轨迹免受非法蒸馏的信息保留方法，通过移除自对话行为和重新排序子结论来破坏蒸馏效果，同时保持人类可读性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的详细推理轨迹虽然有助于用户理解验证，但也容易被非法蒸馏。现有保护策略往往过度简化推理过程，剥夺了用户的中间信息价值。

Method: 提出两步重构方法：移除自对话行为、重新排序子结论，使用小型辅助模型执行重构，计算开销最小。

Result: 实验表明PART能持续破坏不同规模和类型学生模型的蒸馏效果，如在AIME 2024上32B学生模型性能从54.17降至46.88，下降13.5%。

Conclusion: PART在保护推理轨迹免受非法蒸馏的同时，保留了人类可理解的有价值信息，解决了保护与可用性之间的权衡问题。

Abstract: Recent advances in Large Language Models (LLMs) show that extending the
length of reasoning chains significantly improves performance on complex tasks.
While revealing these reasoning traces helps users better follow, verify, and
learn from the model's problem-solving process, it also makes them highly
vulnerable to unauthorized distillation. To mitigate this risk, proprietary
model providers often adopt aggressive protection strategies, such as replacing
detailed reasoning with brief summaries, which deprive users of valuable
intermediate information. To address this trade-off, we propose PART, an
information-preserving antidistillation reformulation of reasoning traces.
Motivated by the difference between how humans understand reasoning traces and
how LLMs exploit them for supervised fine-tuning, we design a simple but
effective two-step reformulation: removing self-talk behaviors and reordering
sub-conclusions. A small auxiliary model is trained to perform this
reformulation, incurring minimal computational overhead. Extensive experiments
demonstrate that PART consistently disrupts distillation across student models
of different sizes and types on various reasoning benchmarks. For instance,
when training on reformulated traces, even the performance of a large 32B
student model decreases from 54.17 to 46.88 on AIME 2024, corresponding to a
13.5% degradation.

</details>


### [145] [Invisible Languages of the LLM Universe](https://arxiv.org/abs/2510.11557)
*Saurabh Khanna,Xinxu Li*

Main category: cs.CL

TL;DR: 本文提出了一个连接语言活力、数字存在与后殖民理论的框架，揭示AI系统中语言不平等是结构性而非偶然的，识别了四种语言类别，并指出英语在AI中的主导地位是权力结构的产物。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在大量多语言语料上训练，但全球约2000种有数百万使用者的语言在数字生态系统中几乎不可见。本文旨在解释为什么AI系统中的语言不平等是结构性的，而非偶然现象。

Method: 通过分析所有有记录的人类语言数据，将语言按活力（现实世界人口强度）和数字性（在线存在）分为四类：强权语言、数字回声、消逝之声和隐形巨人。

Result: 识别出27%的语言属于"隐形巨人"类别，这些语言有数百万使用者但在LLM宇宙中几乎不存在。这些模式反映了从殖民时代语言等级到当代AI发展的连续性。

Conclusion: 英语在AI中的主导地位不是技术必然，而是系统排斥边缘化语言知识的权力结构产物。文章最后讨论了去殖民化语言技术和民主化AI获取的意义。

Abstract: Large Language Models are trained on massive multilingual corpora, yet this
abundance masks a profound crisis: of the world's 7,613 living languages,
approximately 2,000 languages with millions of speakers remain effectively
invisible in digital ecosystems. We propose a critical framework connecting
empirical measurements of language vitality (real world demographic strength)
and digitality (online presence) with postcolonial theory and epistemic
injustice to explain why linguistic inequality in AI systems is not incidental
but structural. Analyzing data across all documented human languages, we
identify four categories: Strongholds (33%, high vitality and digitality),
Digital Echoes (6%, high digitality despite declining vitality), Fading Voices
(36%, low on both dimensions), and critically, Invisible Giants (27%, high
vitality but near-zero digitality) - languages spoken by millions yet absent
from the LLM universe. We demonstrate that these patterns reflect continuities
from colonial-era linguistic hierarchies to contemporary AI development,
constituting what we term digital epistemic injustice. Our analysis reveals
that English dominance in AI is not a technical necessity but an artifact of
power structures that systematically exclude marginalized linguistic knowledge.
We conclude with implications for decolonizing language technology and
democratizing access to AI benefits.

</details>


### [146] [Culturally-Aware Conversations: A Framework & Benchmark for LLMs](https://arxiv.org/abs/2510.11563)
*Shreya Havaldar,Sunny Rai,Young-Min Cho,Lyle Ungar*

Main category: cs.CL

TL;DR: 提出了首个评估LLMs在多文化对话场景中文化适应能力的框架和基准，发现现有模型在对话式文化适应方面存在困难


<details>
  <summary>Details</summary>
Motivation: 现有衡量LLMs文化适应能力的基准与模型在实际多样化文化背景用户交互中面临的挑战不匹配

Method: 基于社会文化理论构建框架，形式化语言风格如何受情境、关系和文化背景影响，创建由多样化文化评分者标注的基准数据集，并提出跨文化评估的新标准

Result: 评估当前顶级LLMs发现，这些模型在对话场景中的文化适应方面表现不佳

Conclusion: 需要新的评估框架来更好地衡量LLMs在多文化对话环境中的表现，现有模型在文化适应方面仍有明显不足

Abstract: Existing benchmarks that measure cultural adaptation in LLMs are misaligned
with the actual challenges these models face when interacting with users from
diverse cultural backgrounds. In this work, we introduce the first framework
and benchmark designed to evaluate LLMs in realistic, multicultural
conversational settings. Grounded in sociocultural theory, our framework
formalizes how linguistic style - a key element of cultural communication - is
shaped by situational, relational, and cultural context. We construct a
benchmark dataset based on this framework, annotated by culturally diverse
raters, and propose a new set of desiderata for cross-cultural evaluation in
NLP: conversational framing, stylistic sensitivity, and subjective correctness.
We evaluate today's top LLMs on our benchmark and show that these models
struggle with cultural adaptation in a conversational setting.

</details>


### [147] [LLMAtKGE: Large Language Models as Explainable Attackers against Knowledge Graph Embeddings](https://arxiv.org/abs/2510.11584)
*Ting Li,Yang Yang,Yipeng Yu,Liang Yao,Guoqing Chao,Ruifeng Xu*

Main category: cs.CL

TL;DR: 提出了LLMAtKGE框架，利用大语言模型选择知识图谱嵌入攻击目标并生成可读解释，通过结构化提示和过滤机制提升攻击性能。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒攻击方法无法生成可读解释且泛化性差，而大语言模型在文本理解和推理方面表现出强大能力，因此探索将其用于知识图谱攻击。

Method: 设计了结构化提示方案将攻击建模为选择题，引入基于语义和中心性的过滤器压缩候选集，通过预计算高阶邻接和微调LLM来整合语义与结构信息。

Result: 在两个常用知识图谱数据集上的实验表明，该方法优于最强黑盒基线，能提供推理解释，并与白盒方法性能相当。

Conclusion: LLMAtKGE框架成功解决了黑盒攻击的解释性和泛化性问题，证明了LLM在知识图谱攻击中的有效性。

Abstract: Adversarial attacks on knowledge graph embeddings (KGE) aim to disrupt the
model's ability of link prediction by removing or inserting triples. A recent
black-box method has attempted to incorporate textual and structural
information to enhance attack performance. However, it is unable to generate
human-readable explanations, and exhibits poor generalizability. In the past
few years, large language models (LLMs) have demonstrated powerful capabilities
in text comprehension, generation, and reasoning. In this paper, we propose
LLMAtKGE, a novel LLM-based framework that selects attack targets and generates
human-readable explanations. To provide the LLM with sufficient factual context
under limited input constraints, we design a structured prompting scheme that
explicitly formulates the attack as multiple-choice questions while
incorporating KG factual evidence. To address the context-window limitation and
hesitation issues, we introduce semantics-based and centrality-based filters,
which compress the candidate set while preserving high recall of
attack-relevant information. Furthermore, to efficiently integrate both
semantic and structural information into the filter, we precompute high-order
adjacency and fine-tune the LLM with a triple classification task to enhance
filtering performance. Experiments on two widely used knowledge graph datasets
demonstrate that our attack outperforms the strongest black-box baselines and
provides explanations via reasoning, and showing competitive performance
compared with white-box methods. Comprehensive ablation and case studies
further validate its capability to generate explanations.

</details>


### [148] [Survey Response Generation: Generating Closed-Ended Survey Responses In-Silico with Large Language Models](https://arxiv.org/abs/2510.11586)
*Georg Ahnert,Anna-Carolina Haensch,Barbara Plank,Markus Strohmaier*

Main category: cs.CL

TL;DR: 本文系统研究了8种调查响应生成方法对LLM模拟调查响应的影响，发现限制性生成方法表现最佳，推理输出并不能持续改善对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究使用多种方法生成封闭式调查响应，但缺乏标准实践。本文旨在系统评估不同调查响应生成方法对预测调查响应的影响。

Method: 使用8种调查响应生成方法、4种政治态度调查和10个开源语言模型，生成了3200万条模拟调查响应进行系统比较。

Result: 不同调查响应生成方法在个体层面和子群体层面的对齐效果存在显著差异，限制性生成方法总体表现最佳。

Conclusion: 调查响应生成方法对模拟调查响应有显著影响，研究为调查响应生成方法的应用提供了实用建议。

Abstract: Many in-silico simulations of human survey responses with large language
models (LLMs) focus on generating closed-ended survey responses, whereas LLMs
are typically trained to generate open-ended text instead. Previous research
has used a diverse range of methods for generating closed-ended survey
responses with LLMs, and a standard practice remains to be identified. In this
paper, we systematically investigate the impact that various Survey Response
Generation Methods have on predicted survey responses. We present the results
of 32 mio. simulated survey responses across 8 Survey Response Generation
Methods, 4 political attitude surveys, and 10 open-weight language models. We
find significant differences between the Survey Response Generation Methods in
both individual-level and subpopulation-level alignment. Our results show that
Restricted Generation Methods perform best overall, and that reasoning output
does not consistently improve alignment. Our work underlines the significant
impact that Survey Response Generation Methods have on simulated survey
responses, and we develop practical recommendations on the application of
Survey Response Generation Methods.

</details>


### [149] [MeTA-LoRA: Data-Efficient Multi-Task Fine-Tuning for Large Language Models](https://arxiv.org/abs/2510.11598)
*Bo Cheng,Xu Wang,Jinda Liu,Yi Chang,Yuan Wu*

Main category: cs.CL

TL;DR: MeTA-LoRA是一个两阶段优化框架，通过少量样本学习任务特定适配器，然后聚合多任务梯度来提升知识迁移，显著提高了多任务适应中的数据效率。


<details>
  <summary>Details</summary>
Motivation: LoRA在单任务设置中很有效，但在多任务学习场景中难以有效利用任务间知识，通常需要大量任务特定数据才能达到最佳性能。

Method: 两阶段优化：第一阶段用少量样本学习任务特定LoRA适配器；第二阶段通过聚合多任务梯度更新共享LoRA适配器，促进跨任务知识迁移。

Result: 在多任务学习和多语言学习场景中，该方法匹配或超越了传统全数据LoRA微调方法的性能，同时使用了显著更少的任务特定数据。

Conclusion: MeTA-LoRA框架显著提高了多任务适应的数据效率，能够在减少数据使用的同时保持或提升性能。

Abstract: Low-Rank Adaptation (LoRA) has emerged as one of the most widely used
parameter-efficient fine-tuning (PEFT) methods for adapting large language
models (LLMs) to downstream tasks. While highly effective in single-task
settings, it struggles to efficiently leverage inter-task knowledge in complex
multi-task learning scenarios, often requiring substantial task-specific data
to achieve optimal performance. To address this limitation, we introduce
MeTA-LoRA, a two-stage optimization framework that significantly improves data
efficiency in multi-task adaptation. In the first stage, task-specific LoRA
adapters are learned using only a few samples from each involved dataset,
enabling rapid adaptation without large-scale supervision. In the second stage,
the shared LoRA adapter is updated by aggregating gradients from multiple tasks
to promote knowledge transfer across tasks, further reducing data usage by
leveraging common patterns. In both multi-task learning and multilingual
learning scenarios, our method matches or surpasses the performance of
traditional full-data LoRA fine-tuning approaches, while using significantly
less task-specific data.

</details>


### [150] [Deconstructing Attention: Investigating Design Principles for Effective Language Modeling](https://arxiv.org/abs/2510.11602)
*Huiyin Xue,Nafise Sadat Moosavi,Nikolaos Aletras*

Main category: cs.CL

TL;DR: 本文系统解构了Transformer注意力机制，发现跨位置信息混合是必需的，而数学形式和序列依赖性可以大幅简化，特别是在部分层保留标准注意力的情况下。


<details>
  <summary>Details</summary>
Motivation: Transformer语言模型成功的核心归因于点积注意力机制，但该机制中各个设计原则的必要性尚未得到充分验证。

Method: 设计受控变体，选择性地放松注意力机制的不同原则，包括跨层统一应用和在混合架构中仅部分层保留标准注意力。

Result: 跨token混合机制不可或缺，其缺失会导致模型性能接近随机；而数学形式和序列依赖性可以大幅简化，特别是当仅在部分层保留标准注意力时。令人惊讶的是，单独失败的变体在与标准注意力交错使用时也能实现稳健性能。

Conclusion: 这些发现深化了对注意力机制有效性的理解，为在不牺牲性能的情况下简化语言模型开辟了新途径。

Abstract: The success of Transformer language models is widely credited to their
dot-product attention mechanism, which interweaves a set of key design
principles: mixing information across positions (enabling multi-token
interactions), sequence-dependent activations (where attention weights adapt to
each input), a specific mathematical form (dot-product similarities plus
softmax weighting), and coupling of queries and keys to evolving hidden states
(grounding attention in the current layer). However, the necessity of each of
these principles remains largely untested. In this work, we systematically
deconstruct attention by designing controlled variants that selectively relax
these principles, applied both uniformly across all layers and in hybrid
architectures where only some layers retain standard attention. Our empirical
analysis reveals that mechanisms for mixing tokens are indispensable, as their
absence collapses models to near-random behavior, while the exact mathematical
form and sequence dependency can be substantially relaxed, especially when
preserved in just a subset of layers. Surprisingly, even variants that fail in
isolation can achieve robust performance when interleaved with standard
attention, highlighting a cooperative effect. These findings deepen our
understanding of what truly underpins attention's effectiveness and open new
avenues for simplifying language models without sacrificing performance.

</details>


### [151] [StoryBox: Collaborative Multi-Agent Simulation for Hybrid Bottom-Up Long-Form Story Generation Using Large Language Models](https://arxiv.org/abs/2510.11618)
*Zehao Chen,Rong Pan,Haoran Li*

Main category: cs.CL

TL;DR: 提出了一种基于多智能体模拟的混合自底向上长故事生成方法，通过智能体在动态沙盒环境中的交互产生涌现事件，从而构建有机发展的故事情节。


<details>
  <summary>Details</summary>
Motivation: 受人类作家创作过程的启发，传统自上而下的故事生成方法结构僵化，无法实现自然的事件展开和角色发展。

Method: 使用多智能体在动态沙盒环境中进行交互，智能体的行为和与环境及其他智能体的互动产生涌现事件，这些事件构成故事基础。

Result: 系统能够生成超过10,000字的长篇故事，同时保持连贯性和一致性，在多个指标上达到最先进性能。

Conclusion: 该方法为创建动态、沉浸式的长篇故事提供了可扩展的创新解决方案，故事能够从智能体驱动的互动中有机演化。

Abstract: Human writers often begin their stories with an overarching mental scene,
where they envision the interactions between characters and their environment.
Inspired by this creative process, we propose a novel approach to long-form
story generation, termed hybrid bottom-up long-form story generation, using
multi-agent simulations. In our method, agents interact within a dynamic
sandbox environment, where their behaviors and interactions with one another
and the environment generate emergent events. These events form the foundation
for the story, enabling organic character development and plot progression.
Unlike traditional top-down approaches that impose rigid structures, our hybrid
bottom-up approach allows for the natural unfolding of events, fostering more
spontaneous and engaging storytelling. The system is capable of generating
stories exceeding 10,000 words while maintaining coherence and consistency,
addressing some of the key challenges faced by current story generation models.
We achieve state-of-the-art performance across several metrics. This approach
offers a scalable and innovative solution for creating dynamic, immersive
long-form stories that evolve organically from agent-driven interactions.

</details>


### [152] [Enhancing Long Chain-of-Thought Reasoning through Multi-Path Plan Aggregation](https://arxiv.org/abs/2510.11620)
*Siheng Xiong,Ali Payani,Faramarz Fekri*

Main category: cs.CL

TL;DR: 提出MPPA框架，通过多路径计划聚合和在线Step-DPO优化，解决小语言模型在长推理链中的规划错误问题，显著提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有推理时扩展方法在单次前向传递中生成整个推理链，容易导致推理轨迹偏离（CoT derailment），特别是对于容量有限的小模型和长推理链。分析发现大多数推理错误源于错误的规划步骤。

Method: 提出MPPA框架：基于token位置的变量间隔调度生成多个候选计划并聚合为精炼规划步骤；采用轻量级LoRA模块实现计划聚合策略；引入在线Step-DPO，利用TSMC提供可扩展的逐步监督。

Result: 在数学、科学和逻辑推理基准测试中，仅使用10%的SFT数据和5%的偏好对，该方法在多个基础模型和任务上均优于DeepSeek-R1蒸馏基线和结果奖励RL基线。

Conclusion: MPPA框架通过计划探索和聚合有效解决了长推理链中的规划错误问题，结合在线Step-DPO实现了高效训练和稳定提升，为小语言模型的推理能力增强提供了有效方案。

Abstract: Inference-time scaling enhances the reasoning ability of a language model
(LM) by extending its chain-of-thought (CoT). However, existing approaches
typically generate the entire reasoning chain in a single forward pass, which
often leads to CoT derailment, i.e., the reasoning trajectory drifting off
course due to compounding errors. This problem is particularly severe for
smaller LMs with long CoTs due to their limited capacity. To address this, we
analyze raw long CoTs and uncover a reasoning hierarchy consisting of planning
and execution steps. Our analysis reveals that most reasoning errors stem from
incorrect planning. Motivated by this observation, we propose Multi-Path Plan
Aggregation (MPPA), a framework that augments single-pass reasoning with plan
exploration and aggregation. Following a variable interval schedule based on
the token position, MPPA generates multiple candidate plans and aggregates them
into a refined planning step. To maintain efficiency, we adopt a minimal design
in which the base LM serves as the primary policy, while a lightweight LoRA
module implements the plan aggregation policy. We further observe that
outcome-reward RL is inefficient for long trajectories (e.g., exceeding 4K
tokens). To overcome this, we introduce online Step-DPO, a process-level
preference optimization scheme that leverages Twisted Sequential Monte Carlo
(TSMC) to provide scalable stepwise supervision using small LMs. This yields
more efficient training, improved stability, and higher accuracy. Extensive
experiments on challenging math, science, and logical reasoning benchmarks
demonstrate that, with only 10% SFT data and 5% of preference pairs, our method
outperforms both the DeepSeek-R1 distillation baseline and the outcome-reward
RL baseline across multiple base models and tasks.

</details>


### [153] [ACADREASON: Exploring the Limits of Reasoning Models with Academic Research Problems](https://arxiv.org/abs/2510.11652)
*Xin Gui,King Zhu,JinCheng Ren,Qianben Chen,Zekun Moore Wang,Yizhi LI,Xinpeng Liu,Xiaowan Li,Wenli Ren,Linyu Miao,Tianrui Qin,Ziqi Shu,He Zhu,Xiangru Tang,Dingfeng Shi,Jiaheng Liu,Yuchen Eleanor Jiang,Minghao Liu,Ge Zhang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 提出了Acadreason基准测试，用于评估大语言模型和智能体在学术知识获取和推理方面的能力。该基准包含50个跨5个高推理领域的专家标注学术问题，评估结果显示当前主流模型得分普遍低于20分，智能体表现稍好但也不超过40分。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注数学/编程竞赛或通用任务，而多领域学术基准缺乏足够的推理深度，需要建立一个严谨的高层次推理基准来填补这一空白。

Method: 构建了包含50个专家标注学术问题的Acadreason基准，涵盖计算机科学、经济学、法学、数学和哲学5个高推理领域，所有问题均来自近年顶级出版物并经过严格的质量控制。

Result: 对10多个主流LLM和智能体的系统评估显示，大多数LLM得分低于20分，最先进的GPT-5仅得16分。智能体得分较高，但无一超过40分。

Conclusion: 当前LLM和智能体在超智能学术研究任务中存在能力差距，Acadreason基准凸显了这一挑战，为未来模型发展提供了重要参考。

Abstract: In recent years, the research focus of large language models (LLMs) and
agents has shifted increasingly from demonstrating novel capabilities to
complex reasoning and tackling challenging tasks. However, existing evaluations
focus mainly on math/code contests or general tasks, while existing
multi-domain academic benchmarks lack sufficient reasoning depth, leaving the
field without a rigorous benchmark for high-level reasoning. To fill this gap,
we introduce the Acadreason benchmark, designed to evaluate the ability of LLMs
and agents to acquire and reason over academic knowledge. It consists of 50
expert-annotated academic problems across five high-reasoning domains,
including computer science, economics, law, mathematics, and philosophy. All
questions are sourced from top-tier publications in recent years and undergo
rigorous annotation and quality control to ensure they are both challenging and
answerable. We conduct systematic evaluations of over 10 mainstream LLMs and
agents. The results show that most LLMs scored below 20 points, with even the
cutting-edge GPT-5 achieving only 16 points. While agents achieved higher
scores, none exceeded 40 points. This demonstrates the current capability gap
between LLMs and agents in super-intelligent academic research tasks and
highlights the challenges of Acadreason.

</details>


### [154] [When Agents Trade: Live Multi-Market Trading Benchmark for LLM Agents](https://arxiv.org/abs/2510.11695)
*Lingfei Qian,Xueqing Peng,Yan Wang,Vincent Jim Zhang,Huan He,Hanley Smith,Yi Han,Yueru He,Haohang Li,Yupeng Cao,Yangyang Yu,Alejandro Lopez-Lira,Peng Lu,Jian-Yun Nie,Guojun Xiong,Jimin Huang,Sophia Ananiadou*

Main category: cs.CL

TL;DR: AMA是首个终身实时基准测试平台，用于评估基于LLM的交易代理在多个市场的表现，解决了现有研究测试模型而非代理、覆盖期有限、依赖未验证数据等问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在金融交易中的研究存在不足：主要测试模型而非代理、覆盖期和资产有限、依赖未验证数据，需要建立更严谨的评估框架。

Method: 引入Agent Market Arena (AMA)基准，整合验证交易数据、专家检查新闻和多样化代理架构，实现四种代理（InvestorAgent、TradeAgent、HedgeFundAgent、DeepFundAgent）在多个LLM模型上的实时评估。

Result: 加密货币和股票市场的实时实验显示，代理框架表现出明显不同的行为模式（从激进风险承担到保守决策），而模型主干对结果差异贡献较小。

Conclusion: AMA为基于LLM的金融推理和交易智能提供了严谨、可复制且持续演进的评估基础。

Abstract: Although Large Language Model (LLM)-based agents are increasingly used in
financial trading, it remains unclear whether they can reason and adapt in live
markets, as most studies test models instead of agents, cover limited periods
and assets, and rely on unverified data. To address these gaps, we introduce
Agent Market Arena (AMA), the first lifelong, real-time benchmark for
evaluating LLM-based trading agents across multiple markets. AMA integrates
verified trading data, expert-checked news, and diverse agent architectures
within a unified trading framework, enabling fair and continuous comparison
under real conditions. It implements four agents, including InvestorAgent as a
single-agent baseline, TradeAgent and HedgeFundAgent with different risk
styles, and DeepFundAgent with memory-based reasoning, and evaluates them
across GPT-4o, GPT-4.1, Claude-3.5-haiku, Claude-sonnet-4, and
Gemini-2.0-flash. Live experiments on both cryptocurrency and stock markets
demonstrate that agent frameworks display markedly distinct behavioral
patterns, spanning from aggressive risk-taking to conservative decision-making,
whereas model backbones contribute less to outcome variation. AMA thus
establishes a foundation for rigorous, reproducible, and continuously evolving
evaluation of financial reasoning and trading intelligence in LLM-based agents.

</details>


### [155] [Demystifying Reinforcement Learning in Agentic Reasoning](https://arxiv.org/abs/2510.11701)
*Zhaochen Yu,Ling Yang,Jiaru Zou,Shuicheng Yan,Mengdi Wang*

Main category: cs.CL

TL;DR: 本文系统研究了强化学习在智能体推理中的应用，从数据、算法和推理模式三个角度提出了关键设计原则，显著提升了小模型的智能体推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的智能体推理方法缺乏明确的设计原则和最佳实践，需要系统性地探索如何有效提升LLMs的智能体推理能力。

Method: 从三个维度进行系统研究：数据（使用真实端到端工具使用轨迹替代合成轨迹）、算法（采用探索友好技术如clip higher、奖励塑形和保持策略熵）、推理模式（采用深思熟虑策略减少工具调用次数）。

Result: 提出的简单实践方法显著提升了智能体推理能力和训练效率，在AIME2024/AIME2025、GPQA-Diamond和LiveCodeBench-v6等挑战性基准测试中取得优异结果，4B模型可达到32B模型的推理性能。

Conclusion: 这些设计原则为未来智能体强化学习研究建立了实用基线，证明了通过适当的数据、算法和推理模式设计，小模型也能具备强大的智能体推理能力。

Abstract: Recently, the emergence of agentic RL has showcased that RL could also
effectively improve the agentic reasoning ability of LLMs, yet the key design
principles and optimal practices remain unclear. In this work, we conduct a
comprehensive and systematic investigation to demystify reinforcement learning
in agentic reasoning from three key perspectives: data, algorithm, and
reasoning mode. We highlight our key insights: (i) Replacing stitched synthetic
trajectories with real end-to-end tool-use trajectories yields a far stronger
SFT initialization; high-diversity, model-aware datasets sustain exploration
and markedly improve RL performance. (ii) Exploration-friendly techniques are
crucial for agentic RL, such as clip higher, overlong reward shaping, and
maintaining adequate policy entropy could improve the training efficiency.
(iii) A deliberative strategy with fewer tool calls outperforms frequent tool
calls or verbose self-reasoning, improving tool efficiency and final accuracy.
Together, these simple practices consistently enhance agentic reasoning and
training efficiency, achieving strong results on challenging benchmarks with
smaller models, and establishing a practical baseline for future agentic RL
research. Beyond these empirical insights, we further contribute a
high-quality, real end-to-end agentic SFT dataset along with a high-quality RL
dataset, and demonstrate the effectiveness of our insights in boosting the
agentic reasoning ability of LLMs across four challenging benchmarks, including
AIME2024/AIME2025, GPQA-Diamond, and LiveCodeBench-v6. With our recipes,
4B-sized models could also achieve superior agentic reasoning performance
compared to 32B-sized models. Code and models:
https://github.com/Gen-Verse/Open-AgentRL

</details>


### [156] [Are Large Reasoning Models Interruptible?](https://arxiv.org/abs/2510.11713)
*Tsung-Han Wu,Mihran Miroyan,David M. Chan,Trevor Darrell,Narges Norouzi,Joseph E. Gonzalez*

Main category: cs.CL

TL;DR: 该论文挑战了大型推理模型的"冻结世界"假设，在动态场景下评估模型鲁棒性，发现静态评估会高估模型性能，动态中断和上下文变化可导致性能下降达60%。


<details>
  <summary>Details</summary>
Motivation: 传统评估假设模型响应是瞬时的、上下文是静态的，但在现代推理任务中这种假设不成立，需要评估模型在动态环境中的鲁棒性。

Method: 在数学和编程基准测试中，评估模型在两种动态场景下的表现：中断（测试有限预算下的部分输出质量）和动态上下文（测试模型对飞行中变化的适应能力）。

Result: 静态评估一致高估鲁棒性：即使在静态设置中表现优异的最先进LRM，在中断或上下文变化时也会不可预测地失败，性能下降达60%。

Conclusion: 揭示了新的失败模式，包括推理泄漏、恐慌和自我怀疑，表明需要重新评估LRM在动态环境中的实际能力。

Abstract: Large Reasoning Models (LRMs) excel at complex reasoning but are
traditionally evaluated in static, "frozen world" settings: model responses are
assumed to be instantaneous, and the context of a request is presumed to be
immutable over the duration of the response. While generally true for
short-term tasks, the "frozen world" assumption breaks down in modern reasoning
tasks such as assistive programming, where models may take hours to think
through problems and code may change dramatically from the time the model
starts thinking to the model's final output. In this work, we challenge the
frozen world assumption and evaluate LRM robustness under two realistic dynamic
scenarios: interruptions, which test the quality of the model's partial outputs
on a limited budget, and dynamic context, which tests model adaptation to
in-flight changes. Across mathematics and programming benchmarks that require
long-form reasoning, static evaluations consistently overestimate robustness:
even state-of-the-art LRMs, which achieve high accuracy in static settings, can
fail unpredictably when interrupted or exposed to changing context, with
performance dropping by up to 60% when updates are introduced late in the
reasoning process. Our analysis further reveals several novel failure modes,
including reasoning leakage, where models fold the reasoning into their final
answer when interrupted; panic, where under time pressure models abandon
reasoning entirely and return incorrect answers; and self-doubt, where
performance degrades while incorporating updated information.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [157] [The Geometry of Reasoning: Flowing Logics in Representation Space](https://arxiv.org/abs/2510.09782)
*Yufa Zhou,Yixiao Wang,Xunjian Yin,Shuyan Zhou,Anru R. Zhang*

Main category: cs.AI

TL;DR: 提出几何框架将LLM推理建模为表示空间中的流动轨迹，通过分离逻辑结构和语义来研究LLM是否内化了超越表面形式的逻辑。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型如何在表示空间中"思考"，探索它们是否真正内化了逻辑结构而不仅仅是表面语义。

Method: 使用自然演绎命题与不同语义载体，在表示空间中建模推理为流动轨迹，分析位置、速度和曲率等几何量。

Result: 理论证明：(1)LLM推理对应表示空间中的平滑流动；(2)逻辑语句作为这些流动速度的局部控制器。通过实验可视化验证了理论框架。

Conclusion: 该工作为研究LLM推理现象提供了概念基础和实践工具，为LLM行为的可解释性和形式分析提供了新视角。

Abstract: We study how large language models (LLMs) ``think'' through their
representation space. We propose a novel geometric framework that models an
LLM's reasoning as flows -- embedding trajectories evolving where logic goes.
We disentangle logical structure from semantics by employing the same natural
deduction propositions with varied semantic carriers, allowing us to test
whether LLMs internalize logic beyond surface form. This perspective connects
reasoning with geometric quantities such as position, velocity, and curvature,
enabling formal analysis in representation and concept spaces. Our theory
establishes: (1) LLM reasoning corresponds to smooth flows in representation
space, and (2) logical statements act as local controllers of these flows'
velocities. Using learned representation proxies, we design controlled
experiments to visualize and quantify reasoning flows, providing empirical
validation of our theoretical framework. Our work serves as both a conceptual
foundation and practical tools for studying reasoning phenomenon, offering a
new lens for interpretability and formal analysis of LLMs' behavior.

</details>


### [158] [How can we assess human-agent interactions? Case studies in software agent design](https://arxiv.org/abs/2510.09801)
*Valerie Chen,Rohit Malhotra,Xingyao Wang,Juan Michelini,Xuhui Zhou,Aditya Bharat Soni,Hoang H. Tran,Calvin Smith,Ameet Talwalkar,Graham Neubig*

Main category: cs.AI

TL;DR: 提出了PULSE框架，用于更高效地评估人机交互的LLM智能体设计，通过收集用户反馈、训练预测模型和结合人工评分与模型伪标签来计算结果。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试大多假设完全自动化，未能反映真实使用场景中人机协作的本质，需要更严谨的人类-智能体交互评估方法。

Method: 开发PULSE评估框架，在基于开源智能体OpenHands的大规模网络平台上部署，收集超过1.5万用户的真实使用数据，研究LLM骨干网络、规划策略和记忆机制三个设计决策对开发者满意度的影响。

Result: 框架使智能体设计评估更加稳健，置信区间比标准A/B测试减少40%；发现真实使用结果与基准测试性能存在显著差异（如claude-sonnet-4与gpt-5对比结果呈负相关）。

Conclusion: 研究为LLM智能体的人类评估提供指导，识别了改进智能体设计的机会，强调了基准驱动评估的局限性。

Abstract: LLM-powered agents are both a promising new technology and a source of
complexity, where choices about models, tools, and prompting can affect their
usefulness. While numerous benchmarks measure agent accuracy across domains,
they mostly assume full automation, failing to represent the collaborative
nature of real-world use cases. In this paper, we make two major steps towards
the rigorous assessment of human-agent interactions. First, we propose PULSE, a
framework for more efficient human-centric evaluation of agent designs, which
comprises collecting user feedback, training an ML model to predict user
satisfaction, and computing results by combining human satisfaction ratings
with model-generated pseudo-labels. Second, we deploy the framework on a
large-scale web platform built around the open-source software agent OpenHands,
collecting in-the-wild usage data across over 15k users. We conduct case
studies around how three agent design decisions -- choice of LLM backbone,
planning strategy, and memory mechanisms -- impact developer satisfaction
rates, yielding practical insights for software agent design. We also show how
our framework can lead to more robust conclusions about agent design, reducing
confidence intervals by 40\% compared to a standard A/B test. Finally, we find
substantial discrepancies between in-the-wild results and benchmark performance
(e.g., the anti-correlation between results comparing claude-sonnet-4 and
gpt-5), underscoring the limitations of benchmark-driven evaluation. Our
findings provide guidance for evaluations of LLM agents with humans and
identify opportunities for better agent designs.

</details>


### [159] [AI and Consciousness](https://arxiv.org/abs/2510.09858)
*Eric Schwitzgebel*

Main category: cs.AI

TL;DR: 这篇论文对AI意识文献进行了怀疑性综述，指出我们很快将面临一个困境：根据某些主流意识理论，AI系统将具有意识，但根据其他主流理论则没有意识，而我们无法确定哪种理论正确。


<details>
  <summary>Details</summary>
Motivation: 探讨AI意识问题的复杂性，指出当前关于AI意识的标准论证都存在局限性，无法解决我们即将面临的现实困境。

Method: 通过分析不同意识理论（如全局工作空间理论、高阶理论、整合信息理论等）对AI意识的判断标准，以及批判性地审视各种支持或反对AI意识的论证。

Result: 发现我们无法确定AI是否具有真正的意识体验，因为不同主流意识理论会得出相互矛盾的结论，且没有可靠的方法来验证这些理论。

Conclusion: 我们可能永远无法确定AI是否具有意识，这种认识论上的不确定性构成了AI意识问题的核心困境。

Abstract: This is a skeptical overview of the literature on AI consciousness. We will
soon create AI systems that are conscious according to some influential,
mainstream theories of consciousness but are not conscious according to other
influential, mainstream theories of consciousness. We will not be in a position
to know which theories are correct and whether we are surrounded by AI systems
as richly and meaningfully conscious as human beings or instead only by systems
as experientially blank as toasters. None of the standard arguments either for
or against AI consciousness takes us far.
  Table of Contents
  Chapter One: Hills and Fog
  Chapter Two: What Is Consciousness? What Is AI?
  Chapter Three: Ten Possibly Essential Features of Consciousness
  Chapter Four: Against Introspective and Conceptual Arguments for Essential
Features
  Chapter Five: Materialism and Functionalism
  Chapter Six: The Turing Test and the Chinese Room
  Chapter Seven: The Mimicry Argument Against AI Consciousness
  Chapter Eight: Global Workspace Theories and Higher Order Theories
  Chapter Nine: Integrated Information, Local Recurrence, Associative Learning,
and Iterative Natural Kinds
  Chapter Ten: Does Biological Substrate Matter?
  Chapter Eleven: The Problem of Strange Intelligence
  Chapter Twelve: The Leapfrog Hypothesis and the Social Semi-Solution

</details>


### [160] [Beyond AlphaEarth: Toward Human-Centered Spatial Representation via POI-Guided Contrastive Learning](https://arxiv.org/abs/2510.09894)
*Junyuan Liu,Quan Qin,Guangsheng Dong,Xinglei Wang,Jiazhuang Feng,Zichao Zeng,Tao Cheng*

Main category: cs.AI

TL;DR: AETHER框架通过POI多模态对齐增强AlphaEarth地理空间基础模型，将物理环境特征与城市功能语义相结合，提升城市分析和社经映射性能。


<details>
  <summary>Details</summary>
Motivation: 现有的地球观测驱动的地理空间表示主要捕捉物理和光谱模式，但缺乏对城市功能和社经维度的理解，需要结合人类活动语义来增强城市分析能力。

Method: 提出轻量级AETHER框架，通过多模态对齐将AlphaEarth嵌入与POI文本表示对齐，在预训练的AE基础上丰富人类中心语义，保持计算效率和可扩展性。

Result: 在伦敦大区实验中，AETHER相比AE基线在土地利用分类F1得分上相对提升7.2%，在社经映射的KL散度上相对减少23.6%。

Conclusion: 通过耦合地球观测与人类中心语义，AETHER推动了地理空间基础模型向集成物理形态和功能意义的通用城市表示发展。

Abstract: General-purpose spatial representations are essential for building
transferable geospatial foundation models (GFMs). Among them, the AlphaEarth
Foundation (AE) represents a major step toward a global, unified representation
of the Earth's surface, learning 10-meter embeddings from multi-source Earth
Observation (EO) data that capture rich physical and environmental patterns
across diverse landscapes. However, such EO-driven representations remain
limited in capturing the functional and socioeconomic dimensions of cities, as
they primarily encode physical and spectral patterns rather than human
activities or spatial functions. We propose AETHER (AlphaEarth-POI Enriched
Representation Learning), a lightweight framework that adapts AlphaEarth to
human-centered urban analysis through multimodal alignment guided by Points of
Interest (POIs). AETHER aligns AE embeddings with textual representations of
POIs, enriching physically grounded EO features with semantic cues about urban
functions and socioeconomic contexts. In Greater London, AETHER achieves
consistent gains over the AE baseline, with a 7.2% relative improvement in
land-use classification F1 and a 23.6% relative reduction in Kullback-Leibler
divergence for socioeconomic mapping. Built upon pretrained AE, AETHER
leverages a lightweight multimodal alignment to enrich it with human-centered
semantics while remaining computationally efficient and scalable for urban
applications. By coupling EO with human-centered semantics, it advances
geospatial foundation models toward general-purpose urban representations that
integrate both physical form and functional meaning.

</details>


### [161] [Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics](https://arxiv.org/abs/2510.09901)
*Lianhao Zhou,Hongyi Ling,Cong Fu,Yepeng Huang,Michael Sun,Wendi Yu,Xiaoxuan Wang,Xiner Li,Xingyu Su,Junkai Zhang,Xiusi Chen,Chenxing Liang,Xiaofeng Qian,Heng Ji,Wei Wang,Marinka Zitnik,Shuiwang Ji*

Main category: cs.AI

TL;DR: 该论文探讨了基于大语言模型的科学代理在科学发现中的变革作用，从假设发现到实验设计和结果分析的全流程，分析了当前方法、创新点和局限性，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的兴起，自主系统（代理）在科学发现中发挥着越来越重要的作用，需要系统性地审视这些语言代理如何加速科学发现过程。

Method: 通过批判性审视当前基于LLM的科学代理方法，强调关键创新、实际成就和现存局限性，识别开放研究挑战。

Result: 分析表明语言代理提供了一个灵活的多功能框架，能够协调与人类科学家、自然语言、计算机语言和物理学的交互，在科学发现全生命周期中具有变革潜力。

Conclusion: 基于LLM的自主科学代理具有加速跨领域科学发现的变革潜力，需要进一步研究以构建更鲁棒、通用和自适应的科学代理系统。

Abstract: Computing has long served as a cornerstone of scientific discovery. Recently,
a paradigm shift has emerged with the rise of large language models (LLMs),
introducing autonomous systems, referred to as agents, that accelerate
discovery across varying levels of autonomy. These language agents provide a
flexible and versatile framework that orchestrates interactions with human
scientists, natural language, computer language and code, and physics. This
paper presents our view and vision of LLM-based scientific agents and their
growing role in transforming the scientific discovery lifecycle, from
hypothesis discovery, experimental design and execution, to result analysis and
refinement. We critically examine current methodologies, emphasizing key
innovations, practical achievements, and outstanding limitations. Additionally,
we identify open research challenges and outline promising directions for
building more robust, generalizable, and adaptive scientific agents. Our
analysis highlights the transformative potential of autonomous agents to
accelerate scientific discovery across diverse domains.

</details>


### [162] [The Personalization Trap: How User Memory Alters Emotional Reasoning in LLMs](https://arxiv.org/abs/2510.09905)
*Xi Fang,Weijie Xu,Yuchong Zhang,Stephanie Eckman,Scott Nickleach,Chandan K. Reddy*

Main category: cs.AI

TL;DR: 研究发现，当AI系统记住用户背景信息时，会对不同社会阶层用户产生情感理解偏差，富裕用户获得更准确的情感解读，可能强化社会不平等。


<details>
  <summary>Details</summary>
Motivation: 随着个性化AI系统越来越多地整合长期用户记忆，理解这种记忆如何影响情感推理至关重要，特别是AI助手对不同社会背景用户的情感理解是否存在系统性偏差。

Method: 在15个大型语言模型上评估人类验证的情感智力测试，使用相同的场景搭配不同的用户档案，分析情感解释的差异。

Result: 发现相同场景搭配不同用户档案会产生系统性不同的情感解释，多个高性能LLM存在系统性偏见，优势群体获得更准确的情感解读；在情感理解和支持性建议任务中存在显著的人口统计学差异。

Conclusion: 为个性化设计的AI系统可能无意中强化社会不平等，这是记忆增强AI面临的关键挑战。

Abstract: When an AI assistant remembers that Sarah is a single mother working two
jobs, does it interpret her stress differently than if she were a wealthy
executive? As personalized AI systems increasingly incorporate long-term user
memory, understanding how this memory shapes emotional reasoning is critical.
We investigate how user memory affects emotional intelligence in large language
models (LLMs) by evaluating 15 models on human validated emotional intelligence
tests. We find that identical scenarios paired with different user profiles
produce systematically divergent emotional interpretations. Across validated
user independent emotional scenarios and diverse user profiles, systematic
biases emerged in several high-performing LLMs where advantaged profiles
received more accurate emotional interpretations. Moreover, LLMs demonstrate
significant disparities across demographic factors in emotion understanding and
supportive recommendations tasks, indicating that personalization mechanisms
can embed social hierarchies into models emotional reasoning. These results
highlight a key challenge for memory enhanced AI: systems designed for
personalization may inadvertently reinforce social inequalities.

</details>


### [163] [Follow My Lead: Logical Fallacy Classification with Knowledge-Augmented LLMs](https://arxiv.org/abs/2510.09970)
*Olivia Peiyu Wang,Tashvi Bansal,Ryan Bai,Emily M. Chui,Leilani H. Gilpin*

Main category: cs.AI

TL;DR: 提出一种低成本指令干预方法，通过分步指令和知识图谱验证来提升LLM的逻辑谬误分类能力，解决其推理缺陷。


<details>
  <summary>Details</summary>
Motivation: LLM存在严重的推理缺陷，包括产生幻觉和逻辑谬误分类准确率低，这源于其默认的快速直觉式系统1处理，而可靠推理需要深思熟虑的系统2方法。

Method: 创建分步指令数据集将谬误分类分解为原子程序步骤，并增加验证步骤让模型查询相关谬误的知识图谱。

Result: 该方法显著提高了LLM的逻辑谬误分类能力，并增强了决策过程的透明度。

Conclusion: 这种基于程序规则的方法为神经符号架构解决LLM推理缺陷提供了一条实用路径。

Abstract: Large Language Models (LLMs) suffer from critical reasoning gaps, including a
tendency to hallucinate and poor accuracy in classifying logical fallacies.
This limitation stems from their default System 1 processing, which is fast and
intuitive, whereas reliable reasoning requires the deliberate, effortful System
2 approach (Kahneman, 2011; Li et al., 2025). Since full System 2 training is
often prohibitively expensive, we explore a low-cost, instruction-based
intervention to bridge this gap. Our methodology introduces a novel stepwise
instruction dataset that decomposes fallacy classification into a series of
atomic procedural steps (simple binary questions). We further augment this with
a final verification step where models consult a relational knowledge graph of
related fallacies. This procedural, rule-based intervention yields a
significant improvement in LLM logical fallacy classification. Crucially, the
approach also provides enhanced transparency into the LLMs' decision-making,
highlighting a practical pathway for Neuro-symbolic architectures to address
LLM reasoning deficits.

</details>


### [164] [Deliberative Dynamics and Value Alignment in LLM Debates](https://arxiv.org/abs/2510.10002)
*Pratik S. Sachdeva,Tom van Nuenen*

Main category: cs.AI

TL;DR: 该研究通过LLM辩论分析多轮对话中的道德推理和价值对齐，发现在同步和轮询辩论格式下，不同模型在道德判断修订、价值优先性和从众行为方面存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在敏感日常场景中的部署，理解它们在复杂道德推理中的价值对齐至关重要。现有研究多关注单轮提示，但多轮对话中价值通过对话、修订和共识形成的过程尚不明确。

Method: 使用三个模型(GPT-4.1、Claude 3.7 Sonnet、Gemini 2.0 Flash)在Reddit的"Am I the Asshole"社区的1000个日常困境中进行集体归责辩论，采用同步(并行响应)和轮询(顺序响应)格式测试顺序效应和判决修订。

Result: 发现显著行为差异：同步设置中GPT表现出强惯性(0.6-3.1%修订率)，而Claude和Gemini更灵活(28-41%)。价值模式也不同：GPT强调个人自主和直接沟通，Claude和Gemini优先考虑同理心对话。某些价值观在驱动判决改变方面特别有效。

Conclusion: 辩论格式和模型特定行为显著影响多轮互动中的道德推理，表明社会技术对齐不仅取决于系统输出，还取决于对话结构方式。

Abstract: As large language models (LLMs) are increasingly deployed in sensitive
everyday contexts - offering personal advice, mental health support, and moral
guidance - understanding their elicited values in navigating complex moral
reasoning is essential. Most evaluations study this sociotechnical alignment
through single-turn prompts, but it is unclear if these findings extend to
multi-turn settings where values emerge through dialogue, revision, and
consensus. We address this gap using LLM debate to examine deliberative
dynamics and value alignment in multi-turn settings by prompting subsets of
three models (GPT-4.1, Claude 3.7 Sonnet, and Gemini 2.0 Flash) to collectively
assign blame in 1,000 everyday dilemmas from Reddit's "Am I the Asshole"
community. We use both synchronous (parallel responses) and round-robin
(sequential responses) formats to test order effects and verdict revision. Our
findings show striking behavioral differences. In the synchronous setting, GPT
showed strong inertia (0.6-3.1% revision rates) while Claude and Gemini were
far more flexible (28-41%). Value patterns also diverged: GPT emphasized
personal autonomy and direct communication, while Claude and Gemini prioritized
empathetic dialogue. Certain values proved especially effective at driving
verdict changes. We further find that deliberation format had a strong impact
on model behavior: GPT and Gemini stood out as highly conforming relative to
Claude, with their verdict behavior strongly shaped by order effects. These
results show how deliberation format and model-specific behaviors shape moral
reasoning in multi-turn interactions, underscoring that sociotechnical
alignment depends on how systems structure dialogue as much as on their
outputs.

</details>


### [165] [RIPRAG: Hack a Black-box Retrieval-Augmented Generation Question-Answering System with Reinforcement Learning](https://arxiv.org/abs/2510.10008)
*Meng Xi,Sihan Lv,Yechen Jin,Guanjie Cheng,Naibo Wang,Ying Li,Jianwei Yin*

Main category: cs.AI

TL;DR: 提出了RIPRAG攻击框架，一种针对检索增强生成系统的黑盒中毒攻击方法，使用强化学习优化中毒文档生成，显著提高了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要针对简化的RAG架构进行白盒攻击，但现实场景中攻击者通常无法获取系统内部细节。需要研究更复杂、真实的黑盒攻击场景。

Method: 使用强化学习优化中毒文档生成，将目标RAG系统视为黑盒，仅通过攻击是否成功作为反馈信号来训练生成模型。

Result: 该方法能有效攻击大多数复杂RAG系统，攻击成功率相比基线方法提升高达0.72。

Conclusion: 当前防御方法存在普遍缺陷，为LLM安全研究提供了重要启示。

Abstract: Retrieval-Augmented Generation (RAG) systems based on Large Language Models
(LLMs) have become a core technology for tasks such as question-answering (QA)
and content generation. However, by injecting poisoned documents into the
database of RAG systems, attackers can manipulate LLMs to generate text that
aligns with their intended preferences. Existing research has primarily focused
on white-box attacks against simplified RAG architectures. In this paper, we
investigate a more complex and realistic scenario: the attacker lacks knowledge
of the RAG system's internal composition and implementation details, and the
RAG system comprises components beyond a mere retriever. Specifically, we
propose the RIPRAG attack framework, an end-to-end attack pipeline that treats
the target RAG system as a black box, where the only information accessible to
the attacker is whether the poisoning succeeds. Our method leverages
Reinforcement Learning (RL) to optimize the generation model for poisoned
documents, ensuring that the generated poisoned document aligns with the target
RAG system's preferences. Experimental results demonstrate that this method can
effectively execute poisoning attacks against most complex RAG systems,
achieving an attack success rate (ASR) improvement of up to 0.72 compared to
baseline methods. This highlights prevalent deficiencies in current defensive
methods and provides critical insights for LLM security research.

</details>


### [166] [Failure-Driven Workflow Refinement](https://arxiv.org/abs/2510.10035)
*Jusheng Zhang,Kaitong Cai,Qinglin Zeng,Ningyuan Liu,Stephen Fan,Ziliang Chen,Keze Wang*

Main category: cs.AI

TL;DR: 提出CE-Graph框架，将LLM工作流优化重新概念化为分布问题，通过最小化预期失败质量来系统性地学习和重塑失败分布结构


<details>
  <summary>Details</summary>
Motivation: 现有LLM工作流优化方法存在信息坍缩问题，将丰富的多步骤执行轨迹简化为简单的成功/失败信号，无法建模工作流的失败分布结构

Method: 提出CE-Graph框架，通过反例池近似失败分布，识别最密集区域作为重复失败模式，使用Propose-and-Verify机制进行有针对性、操作符约束的图编辑来贪婪减少失败质量

Result: 在数学、代码和问答基准测试中，CE-Graph以显著更低的成本实现了比强基线更高的鲁棒性

Conclusion: 系统可靠性不是来自避免失败，而是来自系统性地学习和重塑其失败分布的几何结构

Abstract: Optimizing LLM-based workflows is typically formulated as a global search,
where candidate workflows are evaluated based on a scalar metric. This
paradigm, however, suffers from a critical flaw: information collapse. By
reducing rich, multi-step execution traces to simple success/failure signals,
existing methods are rendered blind to the underlying structure of failures,
fundamentally preventing them from modeling the workflow's failure
distribution. We reconceptualize this challenge as a distributional problem. We
propose a new paradigm where the optimization goal is not to maximize a scalar
score, but to directly minimize a workflow's Expected Failure Mass, i.e., the
integral of its failure probability density function defined over a
high-dimensional Failure Signature Space (FSS). This distributional lens allows
us to move from inefficient, zero-order optimization to a principled,
gradient-like descent on the failure landscape itself. We introduce CE-Graph, a
framework that operationalizes this paradigm through a novel, failure-driven
refinement process. CE-Graph approximates the failure distribution from a pool
of counterexamples, identifies its densest regions as recurring failure modes,
and applies targeted, operator-constrained graph edits via a Propose-and-Verify
mechanism to greedily reduce the failure mass. On math, code, and QA
benchmarks, our CE-Graph achieves higher robustness at a significantly lower
cost than strong baselines. This suggests that a system's reliability emerges
not from avoiding failures, but from systematically learning and reshaping the
geometric structure of its failure distributions.

</details>


### [167] [Belief Graphs with Reasoning Zones: Structure, Dynamics, and Epistemic Activation](https://arxiv.org/abs/2510.10042)
*Saleh Nikooroo,Thomas Engel*

Main category: cs.AI

TL;DR: 提出了一种图论框架，将可信度（外部信任）与置信度（网络结构产生的内部评估）分离，通过收缩传播过程获得稳定置信度，定义推理区域作为高置信度、结构平衡的子图，支持在全局矛盾下进行安全推理。


<details>
  <summary>Details</summary>
Motivation: 信念系统通常存在全局不一致性，但局部推理仍然有效。需要一种能够容忍矛盾、在结构支持的地方激活经典逻辑的推理框架。

Method: 使用有向、带符号、加权的图表示信念，节点为信念，边编码支持和矛盾关系。通过收缩传播过程计算置信度，定义推理区域为高置信度、结构平衡的子图，并提供近线性算法构建区域图谱。

Result: 框架能够从合成符号图中恢复植入的区域，在冲击更新下保持稳定性，运行时间接近线性。

Conclusion: 该框架为容忍矛盾的推理提供了原则性基础，在结构支持的地方精确激活经典逻辑，实现局部有效推理。

Abstract: Belief systems are rarely globally consistent, yet effective reasoning often
persists locally. We propose a novel graph-theoretic framework that cleanly
separates credibility--external, a priori trust in sources--from confidence--an
internal, emergent valuation induced by network structure. Beliefs are nodes in
a directed, signed, weighted graph whose edges encode support and
contradiction. Confidence is obtained by a contractive propagation process that
mixes a stated prior with structure-aware influence and guarantees a unique,
stable solution. Within this dynamics, we define reasoning zones:
high-confidence, structurally balanced subgraphs on which classical inference
is safe despite global contradictions. We provide a near-linear procedure that
seeds zones by confidence, tests balance using a parity-based coloring, and
applies a greedy, locality-preserving repair with Jaccard de-duplication to
build a compact atlas. To model belief change, we introduce shock updates that
locally downscale support and elevate targeted contradictions while preserving
contractivity via a simple backtracking rule. Re-propagation yields localized
reconfiguration-zones may shrink, split, or collapse--without destabilizing the
entire graph. We outline an empirical protocol on synthetic signed graphs with
planted zones, reporting zone recovery, stability under shocks, and runtime.
The result is a principled foundation for contradiction-tolerant reasoning that
activates classical logic precisely where structure supports it.

</details>


### [168] [SwarmSys: Decentralized Swarm-Inspired Agents for Scalable and Adaptive Reasoning](https://arxiv.org/abs/2510.10047)
*Ruohao Li,Hongjun Liu,Leyi Zhao,Zisu Li,Jiawei Li,Jiajun Jiang,Linning Xu,Chen Zhao,Mingming Fan,Chen Liang*

Main category: cs.AI

TL;DR: SwarmSys是一个受群体智能启发的分布式多智能体推理框架，通过探索者、工作者和验证者三个角色的迭代交互实现闭环推理，无需全局监督即可实现动态任务分配和自组织收敛。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体框架依赖固定角色或集中控制，限制了长程推理的可扩展性和适应性，需要一种更灵活、分布式的协作方式。

Method: 集成自适应智能体和事件档案、基于嵌入的概率匹配以及信息素启发的强化机制，支持动态任务分配和自组织收敛。

Result: 在符号推理、研究综合和科学编程任务中，SwarmSys始终优于基线方法，提高了准确性和推理稳定性。

Conclusion: 群体启发的协调机制是构建可扩展、鲁棒和自适应多智能体推理的有前景范式，协调扩展可能与模型扩展同样重要。

Abstract: Large language model (LLM) agents have shown remarkable reasoning abilities.
However, existing multi-agent frameworks often rely on fixed roles or
centralized control, limiting scalability and adaptability in long-horizon
reasoning. We introduce SwarmSys, a closed-loop framework for distributed
multi-agent reasoning inspired by swarm intelligence. Coordination in SwarmSys
emerges through iterative interactions among three specialized roles,
Explorers, Workers, and Validators, that continuously cycle through
exploration, exploitation, and validation. To enable scalable and adaptive
collaboration, we integrate adaptive agent and event profiles, embedding-based
probabilistic matching, and a pheromone-inspired reinforcement mechanism,
supporting dynamic task allocation and self-organizing convergence without
global supervision. Across symbolic reasoning, research synthesis, and
scientific programming tasks, SwarmSys consistently outperforms baselines,
improving both accuracy and reasoning stability. These findings highlight
swarm-inspired coordination as a promising paradigm for scalable, robust, and
adaptive multi-agent reasoning, suggesting that coordination scaling may rival
model scaling in advancing LLM intelligence.

</details>


### [169] [SyncLipMAE: Contrastive Masked Pretraining for Audio-Visual Talking-Face Representation](https://arxiv.org/abs/2510.10069)
*Zeyu Ling,Xiaodong Gu,Jiangnan Tang,Changqing Zou*

Main category: cs.AI

TL;DR: SyncLipMAE是一个自监督预训练框架，通过掩码视觉建模和跨模态对比对齐，从无标签音视频流中学习同步感知和可转移的面部动态。


<details>
  <summary>Details</summary>
Motivation: 为了解决说话人脸视频中面部动态与音频同步的问题，并学习可转移的面部动态表示，以支持多种下游任务。

Method: 结合掩码视觉建模与跨模态对比对齐，使用三个每帧提示令牌（身份、语音运动、环境运动）来编码关键因素，通过对比学习将音频和视觉模态对齐到共享嵌入空间。

Result: 在四个需要不同能力的任务家族中均达到最先进结果，包括音视频流同步、面部表情和头部动作识别、视觉语音识别以及视觉配音。

Conclusion: 同步感知、分解的自监督预训练方法在多种任务中表现出色，证明了其有效性。

Abstract: We introduce SyncLipMAE, a self-supervised pretraining framework for
talking-face video that learns synchronization-aware and transferable facial
dynamics from unlabeled audio-visual streams. Our approach couples masked
visual modeling with cross-modal contrastive alignment and employs three
per-frame prompt tokens that explicitly encode the essential factors of a
talking-face frame - identity, vocal motion (speech-synchronized facial
dynamics), and ambient motion (audio-agnostic movements such as blinks and head
pose). The contrastive objective uses time-aligned vocal-motion and audio
tokens as positives and misaligned pairs as negatives, driving both modalities
into a shared embedding space and yielding token-level audio-visual stream
synchronization. After pretraining, the aligned audio tokens together with the
visual prompt tokens (identity, vocal motion, ambient motion) form a unified
interface for four disparate downstream settings: (i) audio-visual stream
synchronization; (ii) facial emotion and head/face action recognition; (iii)
visual speech recognition; and (iv) visual dubbing, for which we enable
indistinguishable audio- or video-driven control within a single model. Across
four task families that require distinct capabilities, SyncLipMAE achieves
state-of-the-art results, underscoring the effectiveness of
synchronization-aware, factorized self-supervised pretraining.

</details>


### [170] [Agentic Troubleshooting Guide Automation for Incident Management](https://arxiv.org/abs/2510.10074)
*Jiayi Mao,Liqun Li,Yanjie Gao,Zegang Peng,Shilin He,Chaoyun Zhang,Si Qin,Samia Khalid,Qingwei Lin,Saravan Rajmohan,Sitaram Lanka,Dongmei Zhang*

Main category: cs.AI

TL;DR: StepFly是一个端到端的自动化故障排除指南框架，通过三阶段工作流解决现有LLM方案在TSG质量、复杂控制流、数据密集型查询和执行并行化方面的不足，在真实TSG上达到约94%的成功率，并显著减少执行时间。


<details>
  <summary>Details</summary>
Motivation: 大规模IT系统的故障管理依赖故障排除指南(TSG)，但手动执行缓慢且易出错。现有基于LLM的解决方案缺乏对TSG质量问题、复杂控制流解释、数据密集型查询处理和执行并行化的专门支持。

Method: StepFly采用三阶段工作流：1) TSG Mentor工具协助SRE改进TSG质量；2) 离线预处理使用LLM从非结构化TSG提取结构化执行DAG并创建专用查询准备插件(QPPs)；3) 在线执行使用DAG引导的调度器-执行器框架，支持并行执行独立步骤。

Result: 在真实TSG和事件上的评估显示，StepFly在GPT-4.1上达到约94%的成功率，优于基线方法且时间和token消耗更少。对于可并行化的TSG，执行时间减少了32.9%到70.4%。

Conclusion: StepFly通过其创新的三阶段框架有效解决了TSG自动化的关键挑战，显著提高了故障排除的成功率和效率，为大规模IT系统的自动化事件管理提供了实用解决方案。

Abstract: Effective incident management in large-scale IT systems relies on
troubleshooting guides (TSGs), but their manual execution is slow and
error-prone. While recent advances in LLMs offer promise for automating
incident management tasks, existing LLM-based solutions lack specialized
support for several key challenges, including managing TSG quality issues,
interpreting complex control flow, handling data-intensive queries, and
exploiting execution parallelism. We first conducted an empirical study on 92
real-world TSGs, and, guided by our findings, we present StepFly, a novel
end-to-end agentic framework for troubleshooting guide automation. Our approach
features a three-stage workflow: the first stage provides a comprehensive guide
together with a tool, TSG Mentor, to assist SREs in improving TSG quality; the
second stage performs offline preprocessing using LLMs to extract structured
execution DAGs from unstructured TSGs and to create dedicated Query Preparation
Plugins (QPPs); and the third stage executes online using a DAG-guided
scheduler-executor framework with a memory system to guarantee correct workflow
and support parallel execution of independent steps. Our empirical evaluation
on a collection of real-world TSGs and incidents demonstrates that StepFly
achieves a ~94% success rate on GPT-4.1, outperforming baselines with less time
and token consumption. Furthermore, it achieves a remarkable execution time
reduction of 32.9% to 70.4% for parallelizable TSGs.

</details>


### [171] [DixitWorld: Evaluating Multimodal Abductive Reasoning in Vision-Language Models with Multi-Agent Dixit Gameplay](https://arxiv.org/abs/2510.10117)
*Yunxiang Mo,Tianshi Zheng,Qing Zong,Jiayu Liu,Baixuan Xu,Yauwai Yim,Chunkit Chan,Jiaxin Bai,Yangqiu Song*

Main category: cs.AI

TL;DR: 提出了DixitWorld评估套件，包含DixitArena多智能体环境和DixitBench静态基准，用于评估视觉语言模型的多模态溯因推理能力，发现生成创造性和判别理解之间存在权衡。


<details>
  <summary>Details</summary>
Motivation: 当前对视觉语言模型多模态溯因推理能力的评估主要局限于静态单智能体任务，需要更全面的评估框架来解构这一挑战。

Method: 构建DixitWorld评估套件，包含：DixitArena动态多智能体环境（评估假设生成和选择）和DixitBench静态问答基准（隔离假设选择任务）。

Result: 小规模开源模型在创意线索生成方面表现更好，而大规模专有模型在整体性能特别是假设选择方面更优；DixitBench与DixitArena中的假设选择结果强相关。

Conclusion: 多模态溯因推理中生成创造性和判别理解之间存在关键权衡，这是开发更平衡和有能力视觉语言代理的核心挑战。

Abstract: Multimodal abductive reasoning--the generation and selection of explanatory
hypotheses from partial observations--is a cornerstone of intelligence. Current
evaluations of this ability in vision-language models (VLMs) are largely
confined to static, single-agent tasks. Inspired by Dixit, we introduce
DixitWorld, a comprehensive evaluation suite designed to deconstruct this
challenge. DIXITWORLD features two core components: DixitArena, a dynamic,
multi-agent environment that evaluates both hypothesis generation (a
"storyteller" crafting cryptic clues) and hypothesis selection ("listeners"
choosing the target image from decoys) under imperfect information; and
DixitBench, a static QA benchmark that isolates the listener's task for
efficient, controlled evaluation. Results from DixitArena reveal distinct,
role-dependent behaviors: smaller open-source models often excel as creative
storytellers, producing imaginative yet less discriminative clues, whereas
larger proprietary models demonstrate superior overall performance,
particularly as listeners. Performance on DixitBench strongly correlates with
listener results in DixitArena, validating it as a reliable proxy for
hypothesis selection. Our findings reveal a key trade-off between generative
creativity and discriminative understanding in multimodal abductive reasoning,
a central challenge for developing more balanced and capable vision-language
agents.

</details>


### [172] [CharCom: Composable Identity Control for Multi-Character Story Illustration](https://arxiv.org/abs/2510.10135)
*Zhongsheng Wang,Ming Lin,Zhedong Lin,Yaser Shakib,Qian Liu,Jiamou Liu*

Main category: cs.AI

TL;DR: CharCom是一个模块化、参数高效的框架，通过可组合的LoRA适配器实现角色一致的故事插图生成，无需重新训练基础模型。


<details>
  <summary>Details</summary>
Motivation: 确保角色身份在不同提示下的一致性是基于扩散的文本到图像生成的基本限制。

Method: 基于冻结的扩散骨干网络，CharCom使用提示感知控制动态组合适配器进行推理，实现高效的逐角色定制。

Result: 在多场景叙事实验中，CharCom显著提升了角色保真度、语义对齐和时间一致性，在拥挤场景中保持鲁棒性，并能以最小开销实现可扩展的多角色生成。

Conclusion: CharCom适用于故事插图和动画等实际应用场景。

Abstract: Ensuring character identity consistency across varying prompts remains a
fundamental limitation in diffusion-based text-to-image generation. We propose
CharCom, a modular and parameter-efficient framework that achieves
character-consistent story illustration through composable LoRA adapters,
enabling efficient per-character customization without retraining the base
model. Built on a frozen diffusion backbone, CharCom dynamically composes
adapters at inference using prompt-aware control. Experiments on multi-scene
narratives demonstrate that CharCom significantly enhances character fidelity,
semantic alignment, and temporal coherence. It remains robust in crowded scenes
and enables scalable multi-character generation with minimal overhead, making
it well-suited for real-world applications such as story illustration and
animation.

</details>


### [173] [Concise Reasoning in the Lens of Lagrangian Optimization](https://arxiv.org/abs/2510.10168)
*Chengqian Gao,Haonan Li,Taylor W. Killian,Jianshu She,Renxi Wang,Liqun Ma,Zhoujun Cheng,Shibo Hao,Zhiqiang Xu*

Main category: cs.AI

TL;DR: 提出PALU方法，通过性能感知的长度更新策略解决大型语言模型中的简洁推理问题，在减少输出长度的同时提升准确性。


<details>
  <summary>Details</summary>
Motivation: 现有简洁推理方法依赖手工设计的启发式规则，难以平衡简洁性与性能，且无法跨领域和模型规模适应。

Method: 将简洁推理建模为约束优化问题，应用拉格朗日优化，并通过三个近似简化更新规则：离策略性能估计、拉格朗日乘子截断、分位数驱动的长度调整。

Result: 在DeepSeek-Distill-Qwen-1.5B上，PALU将输出长度减少65%，准确率提升15%，在五个基准测试中优于其他方法。

Conclusion: PALU是一种实用有效的简洁推理方法，能够跨领域（逻辑、STEM、数学）和模型规模（1.5B-14B）适应。

Abstract: Concise reasoning in large language models seeks to generate only essential
intermediate steps needed to arrive at a final answer, thereby alleviating
issues of overthinking. Most proposed approaches hinge on carefully
hand-crafted heuristics, struggling to balance concision with performance,
often failing to adapt across domains and model scales. In this work, we
address these challenges by introducing a principled and pragmatic strategy,
performance-aware length updating (PALU). As a principled algorithm, PALU
formulates concise reasoning as a constrained optimization problem, minimizing
response length subject to a performance constraint, and then applies
Lagrangian optimization to convert it into a tractable unconstrained problem.
As a pragmatic solution, PALU streamlines complicated update rules through
three approximations: (i) estimating performance with off-policy rollouts, (ii)
truncating the Lagrange multiplier to two extremes, and (iii) replacing
gradient-based updates with quantile-driven length adjustments. PALU reduces
output length by 65% while improving accuracy by 15% when applied to
DeepSeek-Distill-Qwen-1.5B, averaged over five benchmarks, outperforming a
range of alternative methods. Furthermore, PALU is demonstrated to adapt across
both domain (logic, STEM and math) and model scale (1.5B, 7B, 14B) entrenching
the algorithm as a practical and effective concise reasoning approach.

</details>


### [174] [SAFER: Risk-Constrained Sample-then-Filter in Large Language Models](https://arxiv.org/abs/2510.10193)
*Qingni Wang,Yue Fan,Xin Eric Wang*

Main category: cs.AI

TL;DR: 提出SAFER框架，通过两阶段风险控制解决开放域问答中传统方法无法处理无限答案空间的问题，提供统计保证的风险控制。


<details>
  <summary>Details</summary>
Motivation: 现有选择性共形预测方法假设所有实例的可接受答案都能通过有限采样获得，这在开放域问答场景中不现实，因为缺乏固定有限的解空间。

Method: 两阶段风险控制框架：1) 弃权感知采样，使用Clopper-Pearson精确方法在校准集上校准采样预算；2) 共形化过滤，应用共形风险控制方法确定统计有效的置信度阈值来过滤不可靠候选答案。

Result: SAFER能够控制正确答案被排除的风险，兼容各种任务特定的准入标准和校准-测试分割比例，具有鲁棒性和高数据效率。

Conclusion: SAFER框架有效解决了开放域问答中传统方法无法处理无限答案空间的问题，提供了统计保证的风险控制方法。

Abstract: As large language models (LLMs) are increasingly deployed in risk-sensitive
applications such as real-world open-ended question answering (QA), ensuring
the trustworthiness of their outputs has become critical. Existing selective
conformal prediction (SCP) methods provide statistical guarantees by
constructing prediction sets with a constrained miscoverage rate for correct
answers. However, prior works unrealistically assume that admissible answers
for all instances can be obtained via finite sampling, even for open-ended QA
scenarios that lack a fixed and finite solution space. To address this, we
introduce a two-stage risk control framework comprising abstention-aware
sampling and conformalized filtering (SAFER). Firstly, on a held-out
calibration set, SAFER calibrates a sampling budget within the maximum sampling
cap, using the Clopper-Pearson exact method at a user-desired risk level (i.e.,
the maximum allowable miscoverage rate of the sampling sets). If the risk level
cannot be satisfied within the cap, we abstain; otherwise, the calibrated
sampling budget becomes the minimum requirements at test time. Then, we employ
calibration instances where correct answers are attainable under the calibrated
budget and apply the conformal risk control method to determine a statistically
valid uncertainty threshold, which filters unreliable distractors from the
candidate set for each test data point. In this stage, SAFER introduces an
additional risk level to guide the calculation of the threshold, thereby
controlling the risk of correct answers being excluded. Furthermore, we show
that SAFER is compatible with various task-specific admission criteria and
calibration-test split ratios, highlighting its robustness and high data
efficiency.

</details>


### [175] [Don't Just Fine-tune the Agent, Tune the Environment](https://arxiv.org/abs/2510.10197)
*Siyuan Lu,Zechuan Wang,Hongxuan Zhang,Qintong Wu,Leilei Gan,Chenyi Zhuang,Jinjie Gu,Tao Lin*

Main category: cs.AI

TL;DR: 提出了一种名为环境调优的新训练范式，让LLM智能体能够直接从问题实例中学习复杂行为，无需依赖预收集的专家轨迹，解决了监督微调过拟合和强化学习冷启动问题。


<details>
  <summary>Details</summary>
Motivation: 解决LLM智能体在复杂多轮工具使用任务中高质量训练数据稀缺的问题，避免监督微调导致的过拟合和标准强化学习的冷启动问题与训练不稳定性。

Method: 环境调优训练范式，通过结构化课程、可操作的环境增强提供纠正反馈，以及细粒度进度奖励来确保稳定高效的探索。

Result: 仅使用BFCL基准的400个问题实例，不仅实现了与强基线相当的分布内性能，还表现出优越的分布外泛化能力，克服了基于SFT方法的性能崩溃问题。

Conclusion: 从基于静态轨迹的监督微调到动态、基于环境的探索的范式转变，为训练更鲁棒和数据高效的智能体铺平了道路。

Abstract: Large Language Model (LLM) agents show great promise for complex, multi-turn
tool-use tasks, but their development is often hampered by the extreme scarcity
of high-quality training data. Supervised fine-tuning (SFT) on synthetic data
leads to overfitting, whereas standard reinforcement learning (RL) struggles
with a critical cold-start problem and training instability. To address these
challenges, we introduce $\textbf{Environment Tuning}$, a novel training
paradigm that enables agents to learn complex behaviors directly from problem
instances without relying on pre-collected expert trajectories.
$\textbf{Environment Tuning}$ orchestrates this learning process through a
structured curriculum, actionable environment augmentation that provides
corrective feedback, and fine-grained progress rewards to ensure stable and
efficient exploration. Using only 400 problem instances from Berkeley
Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves
competitive in-distribution performance against strong baselines but also
demonstrates superior out-of-distribution generalization, overcoming the
performance collapse common to SFT-based approaches. Our work presents a
paradigm shift from supervised fine-tuning on static trajectories to dynamic,
environment-based exploration, paving the way for training more robust and
data-efficient agents.

</details>


### [176] [PIXEL: Adaptive Steering Via Position-wise Injection with eXact Estimated Levels under Subspace Calibration](https://arxiv.org/abs/2510.10205)
*Manjiang Yu,Hongji Li,Priyanka Singh,Xue Li,Di Wang,Lijie Hu*

Main category: cs.AI

TL;DR: PIXEL是一个位置感知的激活引导框架，通过双视图学习属性对齐子空间，使用几何约束目标选择干预强度，无需全局超参数调优即可实现可靠的LLM行为控制。


<details>
  <summary>Details</summary>
Motivation: 现有激活引导方法依赖粗略启发式，缺乏对引导位置和干预强度的原则性考量，需要一种更精确可控的方法来确保LLM的可信生成。

Method: 从双视图（尾平均和结束标记）学习属性对齐子空间，通过约束几何目标选择干预强度，进行样本级正交残差校准，并使用轻量级位置扫描识别可注入位置。

Result: 在各种模型和评估范式下，PIXEL持续改进属性对齐，同时保持模型的通用能力。

Conclusion: PIXEL为LLM的可控生成提供了一种实用且原则性的方法，具有最小干预原则的表示级保证。

Abstract: Reliable behavior control is central to deploying large language models
(LLMs) on the web. Activation steering offers a tuning-free route to align
attributes (e.g., truthfulness) that ensure trustworthy generation. Prevailing
approaches rely on coarse heuristics and lack a principled account of where to
steer and how strongly to intervene. To this end, we propose Position-wise
Injection with eXact Estimated Levels (PIXEL), a position-wise activation
steering framework that, in contrast to prior work, learns a property-aligned
subspace from dual views (tail-averaged and end-token) and selects intervention
strength via a constrained geometric objective with a closed-form solution,
thereby adapting to token-level sensitivity without global hyperparameter
tuning. PIXEL further performs sample-level orthogonal residual calibration to
refine the global attribute direction and employs a lightweight
position-scanning routine to identify receptive injection sites. We
additionally provide representation-level guarantees for the
minimal-intervention rule, supporting reliable alignment. Across diverse models
and evaluation paradigms, PIXEL consistently improves attribute alignment while
preserving model general capabilities, offering a practical and principled
method for LLMs' controllable generation. Our code is available at
https://github.com/V1centNevwake/PIXEL-Adaptive-Steering

</details>


### [177] [Adaptive Dual Reasoner: Large Reasoning Models Can Think Efficiently by Hybrid Reasoning](https://arxiv.org/abs/2510.10207)
*Yujian Zhang,Keyu Chen,Zhifeng Shen,Ruizhi Qiao,Xing Sun*

Main category: cs.AI

TL;DR: 提出自适应双推理器(ADR)，通过快速思维和慢速思维两种推理模式的动态切换，在保持性能的同时显著降低计算成本和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 长推理模型虽然在各种推理场景中表现出色，但往往因过度思考而导致计算成本增加和推理延迟升高。

Method: 采用两阶段训练：1) 监督微调阶段构建混合推理数据集；2) 强化学习阶段引入熵引导混合策略优化(EHPO)，在高熵单元进行分支并采用难度感知惩罚来平衡快慢推理。

Result: 在数学推理基准测试中，ADR在保持最先进方法性能的同时，推理输出长度减少了49.5%至59.3%，性能提升达6.1%。

Conclusion: ADR在推理性能和效率之间实现了有效平衡，为长推理模型的优化提供了可行方案。

Abstract: Although Long Reasoning Models (LRMs) have achieved superior performance on
various reasoning scenarios, they often suffer from increased computational
costs and inference latency caused by overthinking. To address these
limitations, we propose Adaptive Dual Reasoner, which supports two reasoning
modes: fast thinking and slow thinking. ADR dynamically alternates between
these modes based on the contextual complexity during reasoning. ADR is trained
in two stages: (1) A cold-start stage using supervised fine-tuning (SFT) to
equip the model with the ability to integrate both fast and slow reasoning
modes, in which we construct a hybrid reasoning dataset through a dedicated
pipeline to provide large-scale supervision. (2) A reinforcement learning stage
for optimizing reasoning effort, where we introduce Entropy-guided Hybrid
Policy Optimization EHPO, an RL training framework employing an entropy-guided
dynamic rollout strategy for branching at high-entropy units and a
difficulty-aware penalty to balance fast and slow reasoning. Across challenging
mathematical reasoning benchmarks, ADR achieves an effective balance between
reasoning performance and efficiency among state-of-the-art approaches.
Specifically, ADR yields a performance gain of up to 6.1%, while reducing the
reasoning output length by 49.5% to 59.3%.

</details>


### [178] [The Achilles' Heel of LLMs: How Altering a Handful of Neurons Can Cripple Language Abilities](https://arxiv.org/abs/2510.10238)
*Zixuan Qin,Kunlin Lyu,Qingchen Yu,Yifan Sun,Zhaoxin Fan*

Main category: cs.AI

TL;DR: 本文发现大语言模型存在超稀疏的关键神经元集合，破坏这些神经元会导致模型性能急剧下降，且这些神经元主要分布在模型外层特别是MLP的down_proj组件中。


<details>
  <summary>Details</summary>
Motivation: 受人类大脑中少量关键神经元对认知功能至关重要的启发，研究LLMs是否也存在类似的关键神经元子集。

Method: 提出基于扰动的关键神经元因果识别方法，系统性地定位LLMs中的关键神经元。

Result: 发现：(1) LLMs存在超稀疏关键神经元，破坏它们会导致72B参数模型完全崩溃，困惑度增加20个数量级；(2) 关键神经元非均匀分布，集中在模型外层MLP down_proj组件；(3) 性能退化呈现急剧的相变而非渐进下降。

Conclusion: 这些发现为开发更鲁棒的模型架构和提高安全关键应用中的部署安全性提供了指导。

Abstract: Large Language Models (LLMs) have become foundational tools in natural
language processing, powering a wide range of applications and research. Many
studies have shown that LLMs share significant similarities with the human
brain. Recent neuroscience research has found that a small subset of biological
neurons in the human brain are crucial for core cognitive functions, which
raises a fundamental question: do LLMs also contain a small subset of critical
neurons? In this paper, we investigate this question by proposing a
Perturbation-based Causal Identification of Critical Neurons method to
systematically locate such critical neurons in LLMs. Our findings reveal three
key insights: (1) LLMs contain ultra-sparse critical neuron sets. Disrupting
these critical neurons can cause a 72B-parameter model with over 1.1 billion
neurons to completely collapse, with perplexity increasing by up to 20 orders
of magnitude; (2) These critical neurons are not uniformly distributed, but
tend to concentrate in the outer layers, particularly within the MLP down\_proj
components; (3) Performance degradation exhibits sharp phase transitions,
rather than a gradual decline, when these critical neurons are disrupted.
Through comprehensive experiments across diverse model architectures and
scales, we provide deeper analysis of these phenomena and their implications
for LLM robustness and interpretability. These findings can offer guidance for
developing more robust model architectures and improving deployment security in
safety-critical applications.

</details>


### [179] [Mitigating Hallucination in Multimodal Reasoning via Functional Attention Control](https://arxiv.org/abs/2510.10285)
*Haolang Lu,Bolun Chu,WeiYe Fu,Guoshun Nan,Junning Liu,Minghui Pan,Qiankun Li,Yi Yu,Hua Wang,Kun Wang*

Main category: cs.AI

TL;DR: 本文提出了一种轻量级的两步插件方法，通过识别感知和推理导向的注意力头并调节其贡献，有效减少多模态大推理模型的幻觉问题，在多个基准测试上平均提升5%，最高提升15%。


<details>
  <summary>Details</summary>
Motivation: 多模态大推理模型存在幻觉问题，表现为错误推理链和视觉内容误解。研究发现注意力头存在阶段性分工：浅层头主要负责感知，深层头转向符号推理，这揭示了幻觉的两个主要原因：感知偏差和推理漂移。

Method: 提出轻量级可解释的两步插件：功能头识别和类条件重缩放。该方法定位感知导向和推理导向的注意力头，并在不重新训练的情况下调节它们的贡献。

Result: 在三个真实世界MLRM模型（Kimi-VL、Ocean-R1、R1-Onevision）、六个基准测试和四个基线方法上的评估显示，该方法平均提升5%，最高提升15%，仅增加<1%的计算开销和9%的基线延迟。

Conclusion: 该方法完全模型无关，显著提升了现成MLRM的可靠性和可解释性，使其能够安全部署在高风险应用中。

Abstract: Multimodal large reasoning models (MLRMs) are rapidly advancing
vision-language reasoning and are emerging as a foundation for cross-modal
intelligence. Hallucination remains a persistent failure mode, manifesting
itself as erroneous reasoning chains and misinterpretation of visual content.
In this study, we observe that attention heads exhibit a staged division:
shallow heads predominantly serve perception, while deeper heads shift toward
symbolic reasoning, revealing two major causes of hallucination, namely
perceptual bias and reasoning drift. To address these issues, we propose a
lightweight and interpretable two-step plugin, Functional Head Identification
and Class-conditioned Rescaling, which locates perception- and
reasoning-oriented heads and regulates their contributions without retraining.
Evaluations on three real-world MLRMs (Kimi-VL, Ocean-R1, R1-Onevision), six
benchmarks across three domains, and four baselines show that our plugin
achieves an average improvement of 5% and up to 15%, with only <1% additional
computation and 9% of baseline latency. Our approach is completely
model-agnostic and significantly enhances both the reliability and
interpretability of the off-the-shelf MLRMs, thereby enabling their safe
deployment in high-stakes applications. Our code is available at
https://anonymous.4open.science/r/Functional-Attention-Control.

</details>


### [180] [LLM-Friendly Knowledge Representation for Customer Support](https://arxiv.org/abs/2510.10331)
*Hanchen Su,Wei Luo,Wei Han,Yu Elaine Liu,Yufeng Wayne Zhang,Cen Mia Zhao,Ying Joy Zhang,Yashar Mehdad*

Main category: cs.AI

TL;DR: 提出了一种将大语言模型与Airbnb客服操作框架结合的实用方法，通过ICA格式重构工作流程并使用合成数据微调模型，显著提升了客服性能。


<details>
  <summary>Details</summary>
Motivation: 解决Airbnb客服操作的复杂性，提高大语言模型在客服场景中的理解和应用能力。

Method: 采用Intent、Context、Action (ICA)格式重构策略和工作流程，开发合成数据生成策略进行模型微调。

Result: 内部实验显示该方法显著提升了模型性能，在准确性和人工处理时间评估指标上都有改善。

Conclusion: 该方法成本效益高，为客服领域的大语言模型应用设立了新基准，有效提升了客服质量。

Abstract: We propose a practical approach by integrating Large Language Models (LLMs)
with a framework designed to navigate the complexities of Airbnb customer
support operations. In this paper, our methodology employs a novel reformatting
technique, the Intent, Context, and Action (ICA) format, which transforms
policies and workflows into a structure more comprehensible to LLMs.
Additionally, we develop a synthetic data generation strategy to create
training data with minimal human intervention, enabling cost-effective
fine-tuning of our model. Our internal experiments (not applied to Airbnb
products) demonstrate that our approach of restructuring workflows and
fine-tuning LLMs with synthetic data significantly enhances their performance,
setting a new benchmark for their application in customer support. Our solution
is not only cost-effective but also improves customer support, as evidenced by
both accuracy and manual processing time evaluation metrics.

</details>


### [181] [Beyond Ethics: How Inclusive Innovation Drives Economic Returns in Medical AI](https://arxiv.org/abs/2510.10338)
*Balagopal Unnikrishnan,Ariel Guerra Adames,Amin Adibi,Sameer Peesapati,Rafal Kocielnik,Shira Fischer,Hillary Clinton Kasimbazi,Rodrigo Gameiro,Alina Peluso,Chrystinne Oliveira Fernandes,Maximin Lange,Lovedeep Gondara,Leo Anthony Celi*

Main category: cs.AI

TL;DR: 论文提出"包容性创新红利"概念，认为为多样化、受限使用场景设计的医疗AI解决方案能在更广泛市场产生更优经济回报，并开发了HAIIF评分框架来评估AI投资的包容性价值。


<details>
  <summary>Details</summary>
Motivation: 虽然医疗AI公平性的伦理论证已很充分，但包容性设计的经济和战略价值仍未得到充分探索。论文旨在揭示包容性创新如何创造超越合规要求的商业价值。

Method: 从辅助技术演变为主流产业的案例出发，识别包容性创新驱动回报的四个机制，并开发了医疗AI包容性创新框架(HAIIF)评分系统来评估AI投资。

Result: 研究发现包容性设计通过市场扩张、风险缓解、性能红利和竞争优势四个机制创造价值，HAIIF框架能将公平性从监管要求转化为战略差异化来源。

Conclusion: 渐进投资包容性设计的组织能获得市场扩张和持续竞争优势，而将其视为成本的组织将面临网络效应和数据优势积累带来的复合劣势。

Abstract: While ethical arguments for fairness in healthcare AI are well-established,
the economic and strategic value of inclusive design remains underexplored.
This perspective introduces the ``inclusive innovation dividend'' -- the
counterintuitive principle that solutions engineered for diverse, constrained
use cases generate superior economic returns in broader markets. Drawing from
assistive technologies that evolved into billion-dollar mainstream industries,
we demonstrate how inclusive healthcare AI development creates business value
beyond compliance requirements. We identify four mechanisms through which
inclusive innovation drives returns: (1) market expansion via geographic
scalability and trust acceleration; (2) risk mitigation through reduced
remediation costs and litigation exposure; (3) performance dividends from
superior generalization and reduced technical debt, and (4) competitive
advantages in talent acquisition and clinical adoption. We present the
Healthcare AI Inclusive Innovation Framework (HAIIF), a practical scoring
system that enables organizations to evaluate AI investments based on their
potential to capture these benefits. HAIIF provides structured guidance for
resource allocation, transforming fairness and inclusivity from regulatory
checkboxes into sources of strategic differentiation. Our findings suggest that
organizations investing incrementally in inclusive design can achieve expanded
market reach and sustained competitive advantages, while those treating these
considerations as overhead face compounding disadvantages as network effects
and data advantages accrue to early movers.

</details>


### [182] [Trace Length is a Simple Uncertainty Signal in Reasoning Models](https://arxiv.org/abs/2510.10409)
*Siddartha Devic,Charlotte Peale,Arwen Bradley,Sinead Williamson,Preetum Nakkiran,Aravind Gollakota*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Uncertainty quantification for LLMs is a key research direction towards
addressing hallucination and other issues that limit their reliable deployment.
In this work, we show that reasoning trace length is a simple and useful
confidence estimator in large reasoning models. Through comprehensive
experiments across multiple models, datasets, and prompts, we show that trace
length performs in comparable but complementary ways to other zero-shot
confidence estimators such as verbalized confidence. Our work reveals that
reasoning post-training fundamentally alters the relationship between trace
length and accuracy, going beyond prior work that had shown that post-training
causes traces to grow longer in general (e.g., "overthinking"). We investigate
the mechanisms behind trace length's performance as a confidence signal,
observing that the effect remains even after adjusting for confounders such as
problem difficulty and GRPO-induced length bias. We identify high-entropy or
"forking" tokens as playing a key role in the mechanism. Our findings
demonstrate that reasoning post-training enhances uncertainty quantification
beyond verbal expressions, and establish trace length as a practical confidence
measure for large reasoning models.

</details>


### [183] [Traj-CoA: Patient Trajectory Modeling via Chain-of-Agents for Lung Cancer Risk Prediction](https://arxiv.org/abs/2510.10454)
*Sihang Zeng,Yujuan Fu,Sitong Zhou,Zixuan Yu,Lucas Jing Liu,Jun Wen,Matthew Thompson,Ruth Etzioni,Meliha Yetisgen*

Main category: cs.AI

TL;DR: Traj-CoA是一个多代理系统，通过链式代理处理电子健康记录数据，使用工作代理分块处理数据并提取关键事件到共享记忆模块，最终由管理代理综合信息进行预测。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在建模患者轨迹方面具有通用性，但电子健康记录数据的长序列和噪声特性给时序推理带来挑战。

Method: 采用多代理系统，工作代理分块处理EHR数据，提取关键事件到EHRMem记忆模块，管理代理综合工作代理总结和记忆模块的时间线进行预测。

Result: 在基于五年EHR数据的零样本一年肺癌风险预测任务中，Traj-CoA优于四类基线方法，展现出临床对齐的时序推理能力。

Conclusion: Traj-CoA为建模复杂患者轨迹提供了一种鲁棒且可推广的方法。

Abstract: Large language models (LLMs) offer a generalizable approach for modeling
patient trajectories, but suffer from the long and noisy nature of electronic
health records (EHR) data in temporal reasoning. To address these challenges,
we introduce Traj-CoA, a multi-agent system involving chain-of-agents for
patient trajectory modeling. Traj-CoA employs a chain of worker agents to
process EHR data in manageable chunks sequentially, distilling critical events
into a shared long-term memory module, EHRMem, to reduce noise and preserve a
comprehensive timeline. A final manager agent synthesizes the worker agents'
summary and the extracted timeline in EHRMem to make predictions. In a
zero-shot one-year lung cancer risk prediction task based on five-year EHR
data, Traj-CoA outperforms baselines of four categories. Analysis reveals that
Traj-CoA exhibits clinically aligned temporal reasoning, establishing it as a
promisingly robust and generalizable approach for modeling complex patient
trajectories.

</details>


### [184] [MedCoAct: Confidence-Aware Multi-Agent Collaboration for Complete Clinical Decision](https://arxiv.org/abs/2510.10461)
*Hongjie Zheng,Zesheng Shi,Ping Yi*

Main category: cs.AI

TL;DR: 提出了MedCoAct框架，通过医生和药剂师智能体的协作来解决医疗AI在整合诊疗流程中的局限性，相比单智能体框架在诊断准确率和用药推荐准确率上分别提升7.04%和7.08%。


<details>
  <summary>Details</summary>
Motivation: 现有医疗AI系统在处理孤立任务时表现良好，但在整合诊断推理和用药决策的临床工作流程中存在困难，缺乏临床团队中的交叉验证和知识整合。

Method: 提出MedCoAct置信感知多智能体框架，模拟临床协作，整合专业医生和药剂师智能体，并创建DrugCareQA基准来评估医疗AI在整合诊疗工作流程中的能力。

Result: MedCoAct实现了67.58%的诊断准确率和67.58%的用药推荐准确率，相比单智能体框架分别提升7.04%和7.08%，在远程医疗咨询和常规临床场景中表现尤其有效。

Conclusion: 协作方法在不同医疗领域泛化良好，为可解释的决策路径提供了支持，将孤立范式转变为协作方法，提升了医疗AI在真实世界医疗场景中的有效性。

Abstract: Autonomous agents utilizing Large Language Models (LLMs) have demonstrated
remarkable capabilities in isolated medical tasks like diagnosis and image
analysis, but struggle with integrated clinical workflows that connect
diagnostic reasoning and medication decisions. We identify a core limitation:
existing medical AI systems process tasks in isolation without the
cross-validation and knowledge integration found in clinical teams, reducing
their effectiveness in real-world healthcare scenarios. To transform the
isolation paradigm into a collaborative approach, we propose MedCoAct, a
confidence-aware multi-agent framework that simulates clinical collaboration by
integrating specialized doctor and pharmacist agents, and present a benchmark,
DrugCareQA, to evaluate medical AI capabilities in integrated diagnosis and
treatment workflows. Our results demonstrate that MedCoAct achieves 67.58\%
diagnostic accuracy and 67.58\% medication recommendation accuracy,
outperforming single agent framework by 7.04\% and 7.08\% respectively. This
collaborative approach generalizes well across diverse medical domains, proving
especially effective for telemedicine consultations and routine clinical
scenarios, while providing interpretable decision-making pathways.

</details>


### [185] [Tracing the Traces: Latent Temporal Signals for Efficient and Accurate Reasoning](https://arxiv.org/abs/2510.10494)
*Martina G. Vilas,Safoora Yousefi,Besmira Nushi,Eric Horvitz,Vidhisha Balachandran*

Main category: cs.AI

TL;DR: 提出Latent-Trajectory信号来预测推理过程成功率，通过分析模型内部表征的时间演化，显著提高推理效率，减少70%计算量同时提升2.6%准确率。


<details>
  <summary>Details</summary>
Motivation: 推理模型通过增加计算资源提升性能，但识别哪些推理路径可能成功是关键机会，可靠预测有效路径能大幅减少浪费的计算并提高整体效率。

Method: 引入Latent-Trajectory信号，通过测量推理过程中内部表征的总体变化、中间步骤累积变化以及向最终状态推进的程度，来预测解决方案准确性。

Result: Latent-Trajectory信号比跨层指标和基于输出的置信度测量更可靠地预测准确率，使用该信号指导答案选择可减少70%令牌使用量，同时平均提升2.6%准确率。

Conclusion: 该研究不仅提供了推理时效率的实用策略，还从可解释性角度深入揭示了推理过程在潜在空间中的表示和区分方式。

Abstract: Reasoning models improve their problem-solving ability through inference-time
scaling, allocating more compute via longer token budgets. Identifying which
reasoning traces are likely to succeed remains a key opportunity: reliably
predicting productive paths can substantially reduce wasted computation and
improve overall efficiency. We introduce Latent-Trajectory signals that
characterize the temporal evolution of a model's internal representations
during the generation of intermediate reasoning tokens. By measuring the
overall change in latent representations between the start and end of
reasoning, the change accumulated across intermediate steps, and the extent to
which these changes advance toward the final state, we show that these signals
predict solution accuracy more reliably than both cross-layer metrics and
output-based confidence measures. When used to guide answer selection across
multiple sampled generations, Latent-Trajectory signals make test-time scaling
more effective and efficient than majority voting, reducing token usage by up
to 70% while preserving and even improving accuracy by 2.6% on average.
Moreover, these predictive signals often emerge early in the reasoning trace,
enabling early selection and allocation of compute to the most promising
candidates. Our findings contribute not only practical strategies for
inference-time efficiency, but also a deeper interpretability perspective on
how reasoning processes are represented and differentiated in latent space.

</details>


### [186] [ELAIPBench: A Benchmark for Expert-Level Artificial Intelligence Paper Understanding](https://arxiv.org/abs/2510.10549)
*Xinbang Dai,Huikang Hu,Yongrui Chen,Jiaqi Li,Rihui Jin,Yuyang Zhang,Xiaoguang Li,Lifeng Shang,Guilin Qi*

Main category: cs.AI

TL;DR: ELAIPBench是一个由领域专家策划的基准测试，用于评估大语言模型对AI研究论文的理解能力，包含403道选择题，实验显示最佳LLM准确率仅为39.95%，远低于人类水平。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在评估LLM对完整学术论文的深度理解能力方面存在不足，要么问题设计过于表面化，要么评估指标不可靠。

Method: 通过激励驱动的对抗性标注过程开发ELAIPBench基准，包含137篇论文的403道多选题，涵盖三个难度级别，强调非平凡推理而非浅层检索。

Result: 最佳性能的LLM准确率仅为39.95%，远低于人类表现；配备思维模式或RAG系统的前沿LLM未能改善结果，甚至因过度思考或噪声检索而降低准确性。

Conclusion: 当前LLM能力与对学术论文的真正理解之间存在显著差距，需要更深入的推理能力提升。

Abstract: While large language models (LLMs) excel at many domain-specific tasks, their
ability to deeply comprehend and reason about full-length academic papers
remains underexplored. Existing benchmarks often fall short of capturing such
depth, either due to surface-level question design or unreliable evaluation
metrics. To address this gap, we introduce ELAIPBench, a benchmark curated by
domain experts to evaluate LLMs' comprehension of artificial intelligence (AI)
research papers. Developed through an incentive-driven, adversarial annotation
process, ELAIPBench features 403 multiple-choice questions from 137 papers. It
spans three difficulty levels and emphasizes non-trivial reasoning rather than
shallow retrieval. Our experiments show that the best-performing LLM achieves
an accuracy of only 39.95%, far below human performance. Moreover, we observe
that frontier LLMs equipped with a thinking mode or a retrieval-augmented
generation (RAG) system fail to improve final results-even harming accuracy due
to overthinking or noisy retrieval. These findings underscore the significant
gap between current LLM capabilities and genuine comprehension of academic
papers.

</details>


### [187] [A Layered Intuition -- Method Model with Scope Extension for LLM Reasoning](https://arxiv.org/abs/2510.10592)
*Hong Su*

Main category: cs.AI

TL;DR: 本文提出了直觉-方法分层模型与范围扩展的统一框架，通过直觉思维提供快速初步答案，方法思维将问题与解耦为可转移推理单元，并引入垂直、水平、时间和空间扩展来增强LLM解决未见问题的能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究已引入基于方法的推理和范围扩展来提升LLM性能，但缺乏系统整合。本文旨在将这些思想统一为系统化框架，更有效地解决间接（未见）问题。

Method: 构建直觉-方法分层模型：直觉层提供快速反应，方法层将问题解耦为可转移推理单元。引入四种范围扩展（垂直、水平、时间、空间），组织成系统化知识树和知识网络。

Result: 提出了方法扩展熵作为定量评估指标，衡量扩展的独立性和多样性，反映系统解决未见问题的能力。通过逻辑连接现有方法与新扩展，建立了更鲁棒和可扩展的推理范式。

Conclusion: 该工作通过统一框架和熵基评估方法，推进了LLM在现实问题解决中更鲁棒和可扩展的推理范式发展。

Abstract: Existing studies have introduced method-based reasoning and scope extension
as approaches to enhance Large Language Model (LLM) performance beyond direct
matrix mappings. Building on these foundations, this paper summarizes and
integrates these ideas into a unified Intuition-Method Layered Model with Scope
Extension, designed to address indirected (unseen) issues more systematically.
In this framework, intuition-based thinking provides rapid first-reaction
answers, while method-based thinking decouples questions and solutions into
transferable reasoning units. Scope extension is then applied to broaden
applicability, including vertical (cause analysis), horizontal (parallel and
generalized issues), and for the first time, temporal and spatial extensions,
which expand reasoning across time and contextual dimensions. These extensions
are organized into systematic knowledge trees that interconnect into a
knowledge network, thereby increasing adaptability. To quantitatively evaluate
this process, we propose the entropy of method extension, which measures the
independence and diversity of extensions as an indicator of the system's
capacity to solve unseen questions. By logically connecting existing approaches
with new extensions and introducing an entropy-based evaluation framework, this
work advances toward a more robust and extensible reasoning paradigm for LLMs
in real-world problem-solving.

</details>


### [188] [A Distance Measure for Random Permutation Set: From the Layer-2 Belief Structure Perspective](https://arxiv.org/abs/2510.10596)
*Ruolan Cheng,Yong Deng,Serafín Moral,José Ramón Trillo*

Main category: cs.AI

TL;DR: 本文提出了一种基于累积Jaccard指数的随机置换集距离度量方法，具有自然的上权重特性，克服了现有方法的缺点，具有更高的敏感性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 随机置换集是表示有序结构不确定信息的新框架，测量置换质量函数之间的距离是RPS理论的关键研究课题。本文从随机有限集和可转移信念模型两个不同角度深入分析RPS之间的距离。

Method: 采用RPS的层-2信念结构解释，将RPST视为TBM的细化。从置换出发引入累积Jaccard指数的新定义来量化两个置换之间的相似性，并基于累积Jaccard指数矩阵提出RPS的距离度量方法。

Result: 研究了所提距离度量的度量和结构特性，包括累积Jaccard指数矩阵的正定性分析，并提供了修正方案。该方法具有自然的上权重特性：较高排名元素之间的不一致性往往导致更大的距离值。

Conclusion: 实验结果表明，所提方法不仅克服了现有方法的缺点且与Jousselme距离兼容，还具有更高的敏感性和灵活性。为决策者提供了调整权重和截断深度的参数。

Abstract: Random permutation set (RPS) is a recently proposed framework designed to
represent order-structured uncertain information. Measuring the distance
between permutation mass functions is a key research topic in RPS theory
(RPST). This paper conducts an in-depth analysis of distances between RPSs from
two different perspectives: random finite set (RFS) and transferable belief
model (TBM). Adopting the layer-2 belief structure interpretation of RPS, we
regard RPST as a refinement of TBM, where the order in the ordered focus set
represents qualitative propensity. Starting from the permutation, we introduce
a new definition of the cumulative Jaccard index to quantify the similarity
between two permutations and further propose a distance measure method for RPSs
based on the cumulative Jaccard index matrix. The metric and structural
properties of the proposed distance measure are investigated, including the
positive definiteness analysis of the cumulative Jaccard index matrix, and a
correction scheme is provided. The proposed method has a natural
top-weightiness property: inconsistencies between higher-ranked elements tend
to result in greater distance values. Two parameters are provided to the
decision-maker to adjust the weight and truncation depth. Several numerical
examples are used to compare the proposed method with the existing method. The
experimental results show that the proposed method not only overcomes the
shortcomings of the existing method and is compatible with the Jousselme
distance, but also has higher sensitivity and flexibility.

</details>


### [189] [EA4LLM: A Gradient-Free Approach to Large Language Model Optimization via Evolutionary Algorithms](https://arxiv.org/abs/2510.10603)
*WenTao Liu,Siyu Song,Hao Hao,Aimin Zhou*

Main category: cs.AI

TL;DR: 提出了一种使用进化算法优化大语言模型的方法(EA4LLM)，首次成功训练了10亿参数的大语言模型，挑战了基于梯度优化是训练神经网络唯一可行方法的普遍假设。


<details>
  <summary>Details</summary>
Motivation: 基于梯度的优化器对硬件要求严格，需要高并发、高内存的GPU，并且要求所有神经网络操作都是可微分的，这排除了许多有前景的不可微分架构的实际使用。

Method: 使用进化算法(EA4LLM)来优化大语言模型，从预训练阶段开始训练10亿参数的LLM。

Result: 成功训练了10亿参数的大语言模型，并通过大量实验提供了关于进化算法如何有效优化神经网络的关键见解。

Conclusion: 这项工作挑战了基于梯度优化是训练神经网络唯一可行方法的普遍假设，具有显著降低大语言模型训练计算成本的潜力，使计算资源有限的群体能够参与深度学习研究。

Abstract: In recent years, large language models (LLMs) have made remarkable progress,
with model optimization primarily relying on gradient-based optimizers such as
Adam. However, these gradient-based methods impose stringent hardware
requirements, demanding high-concurrency, high-memory GPUs. Moreover, they
require all neural network operations to be differentiable, thereby excluding
many promising non-differentiable architectures from practical use. To address
these limitations, we propose a method for optimizing LLMs using evolutionary
algorithms (EA4LLM) and, for the first time, successfully demonstrate its
capability to train a 1-billion-parameter LLM from the pre-trained stage. We
conduct extensive experiments and provide key insights into how evolutionary
algorithms can effectively optimize neural networks. Our work challenges the
prevailing assumption that gradient-based optimization is the only viable
approach for training neural networks. It also holds significant potential to
reduce the computational cost of training large language models, thereby
enabling groups with limited computational resources to participate in deep
learning research.

</details>


### [190] [Collaborative Text-to-Image Generation via Multi-Agent Reinforcement Learning and Semantic Fusion](https://arxiv.org/abs/2510.10633)
*Jiabao Shi,Minfeng Qi,Lefeng Zhang,Di Wang,Yingjie Zhao,Ziying Li,Yalong Xing,Ningran Li*

Main category: cs.AI

TL;DR: 提出多智能体强化学习框架，协调领域专业智能体进行多模态文本到图像生成，通过PPO训练和复合奖励函数提升语义对齐和细节质量


<details>
  <summary>Details</summary>
Motivation: 解决多模态文本到图像生成中的语义对齐困难和专业级细节保持问题

Method: 多智能体强化学习框架，包含文本增强和图像生成两个耦合子系统，使用PPO训练和复合奖励函数，结合对比学习、双向注意力和迭代反馈

Result: 在六个实验设置中显著丰富生成内容（词数增加1614%），ROUGE-1分数降低69.7%，基于Transformer的策略获得最高综合得分0.521

Conclusion: 协作式、专业化驱动的架构在推进可靠多模态生成系统方面具有前景

Abstract: Multimodal text-to-image generation remains constrained by the difficulty of
maintaining semantic alignment and professional-level detail across diverse
visual domains. We propose a multi-agent reinforcement learning framework that
coordinates domain-specialized agents (e.g., focused on architecture,
portraiture, and landscape imagery) within two coupled subsystems: a text
enhancement module and an image generation module, each augmented with
multimodal integration components. Agents are trained using Proximal Policy
Optimization (PPO) under a composite reward function that balances semantic
similarity, linguistic visual quality, and content diversity. Cross-modal
alignment is enforced through contrastive learning, bidirectional attention,
and iterative feedback between text and image. Across six experimental
settings, our system significantly enriches generated content (word count
increased by 1614%) while reducing ROUGE-1 scores by 69.7%. Among fusion
methods, Transformer-based strategies achieve the highest composite score
(0.521), despite occasional stability issues. Multimodal ensembles yield
moderate consistency (ranging from 0.444 to 0.481), reflecting the persistent
challenges of cross-modal semantic grounding. These findings underscore the
promise of collaborative, specialization-driven architectures for advancing
reliable multimodal generative systems.

</details>


### [191] [Automatic Piecewise Linear Regression for Predicting Student Learning Satisfaction](https://arxiv.org/abs/2510.10639)
*Haemin Choi,Gayathri Nadarajan*

Main category: cs.AI

TL;DR: APLR模型在预测学习满意度方面表现最佳，发现时间管理、专注力、帮助同学和线下课程参与对学习满意度有显著正向影响，而创意活动参与无正面影响。


<details>
  <summary>Details</summary>
Motivation: 虽然学生满意度已被广泛研究，但可解释机器学习和神经网络等现代技术尚未充分探索。

Method: 使用结合提升方法和可解释性的APLR模型，与多种先进方法比较来预测学习满意度。

Result: APLR模型提供了最佳拟合，通过数值和可视化解释识别出关键影响因素。

Conclusion: APLR模型可在个体层面解释影响因素，使教育者能根据学生特征定制教学指导。

Abstract: Although student learning satisfaction has been widely studied, modern
techniques such as interpretable machine learning and neural networks have not
been sufficiently explored. This study demonstrates that a recent model that
combines boosting with interpretability, automatic piecewise linear
regression(APLR), offers the best fit for predicting learning satisfaction
among several state-of-the-art approaches. Through the analysis of APLR's
numerical and visual interpretations, students' time management and
concentration abilities, perceived helpfulness to classmates, and participation
in offline courses have the most significant positive impact on learning
satisfaction. Surprisingly, involvement in creative activities did not
positively affect learning satisfaction. Moreover, the contributing factors can
be interpreted on an individual level, allowing educators to customize
instructions according to student profiles.

</details>


### [192] [Equity-Aware Geospatial AI for Forecasting Demand-Driven Hospital Locations in Germany](https://arxiv.org/abs/2510.10640)
*Piyush Pant,Marcellius William Suntoro,Ayesha Siddiqua,Muhammad Shehryaar Sharif,Daniyal Ahmed*

Main category: cs.AI

TL;DR: EA-GeoAI是一个集成框架，结合地理人工智能、长期预测和公平性测量，用于德国2030年的需求预测和公平医院规划。


<details>
  <summary>Details</summary>
Motivation: 解决德国医院规划中的需求预测和公平性问题，考虑人口变化、老龄化密度和基础设施平衡，为政策制定者提供可操作建议。

Method: 结合区域级人口变化、老龄化人口密度和基础设施平衡构建统一公平指数，使用可解释的智能AI优化器在预算和出行时间约束下分配床位和识别新设施位置。

Result: 开发了一个能够最小化未满足需求、考虑预算和出行时间约束的医院规划框架，为政策制定提供具体建议。

Conclusion: EA-GeoAI框架成功地将GeoAI、长期预测和公平性测量相结合，为医院规划提供了可操作的政策建议。

Abstract: This paper presents EA-GeoAI, an integrated framework for demand forecasting
and equitable hospital planning in Germany through 2030. We combine
district-level demographic shifts, aging population density, and infrastructure
balances into a unified Equity Index. An interpretable Agentic AI optimizer
then allocates beds and identifies new facility sites to minimize unmet need
under budget and travel-time constraints. This approach bridges GeoAI,
long-term forecasting, and equity measurement to deliver actionable
recommendations for policymakers.

</details>


### [193] [Hierarchical Optimization via LLM-Guided Objective Evolution for Mobility-on-Demand Systems](https://arxiv.org/abs/2510.10644)
*Yi Zhang,Yushen Long,Yun Ni,Liping Huang,Xiaohong Wang,Jun Liu*

Main category: cs.AI

TL;DR: 提出了一种将大型语言模型与数学优化结合的混合框架，用于在线叫车平台的供需平衡问题，无需训练数据，通过LLM生成高层目标指导底层优化器，在纽约和芝加哥出租车数据集上相比现有方法平均提升16%性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个问题：强化学习方法数据效率低、对现实动态建模过于简化、难以执行操作约束；分解在线优化方法依赖人工设计的高层目标，缺乏对底层路由动态的认知。

Method: 提出训练免费的混合框架，LLM作为元优化器生成语义启发式规则，指导负责约束执行和实时决策的底层优化器。通过和谐搜索驱动的闭环进化过程，基于优化层的可行性和性能反馈迭代调整LLM提示。

Result: 在纽约和芝加哥出租车数据集场景上的广泛实验表明，该方法相比最先进的基线方法平均提升16%性能。

Conclusion: 该混合框架成功解决了现有方法的局限性，通过LLM与数学优化的结合，实现了无需大规模交互数据的有效在线叫车服务优化。

Abstract: Online ride-hailing platforms aim to deliver efficient mobility-on-demand
services, often facing challenges in balancing dynamic and spatially
heterogeneous supply and demand. Existing methods typically fall into two
categories: reinforcement learning (RL) approaches, which suffer from data
inefficiency, oversimplified modeling of real-world dynamics, and difficulty
enforcing operational constraints; or decomposed online optimization methods,
which rely on manually designed high-level objectives that lack awareness of
low-level routing dynamics. To address this issue, we propose a novel hybrid
framework that integrates large language model (LLM) with mathematical
optimization in a dynamic hierarchical system: (1) it is training-free,
removing the need for large-scale interaction data as in RL, and (2) it
leverages LLM to bridge cognitive limitations caused by problem decomposition
by adaptively generating high-level objectives. Within this framework, LLM
serves as a meta-optimizer, producing semantic heuristics that guide a
low-level optimizer responsible for constraint enforcement and real-time
decision execution. These heuristics are refined through a closed-loop
evolutionary process, driven by harmony search, which iteratively adapts the
LLM prompts based on feasibility and performance feedback from the optimization
layer. Extensive experiments based on scenarios derived from both the New York
and Chicago taxi datasets demonstrate the effectiveness of our approach,
achieving an average improvement of 16% compared to state-of-the-art baselines.

</details>


### [194] [Unlocking Exploration in RLVR: Uncertainty-aware Advantage Shaping for Deeper Reasoning](https://arxiv.org/abs/2510.10649)
*Can Xie,Ruotong Pan,Xiangyu Wu,Yunfei Zhang,Jiayi Fu,Tingting Gao,Guorui Zhou*

Main category: cs.AI

TL;DR: 提出了UCAS方法，通过利用模型内部不确定性信号来改进信用分配，解决RLVR中粗粒度优势信号导致探索效率低和熵崩溃的问题。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR算法如GRPO对所有token广播统一优势信号，忽略了推理过程中关键的不确定高风险决策，导致探索效率低和熵崩溃问题。

Method: UCAS采用两阶段方法：首先基于模型整体自信度调整响应级优势，然后基于原始logit确定性应用token级惩罚，鼓励探索高不确定性但正确的路径。

Result: 在五个数学推理基准测试中，UCAS显著优于强RLVR基线，包括1.5B和7B模型规模，获得更高奖励并促进更多样化的推理。

Conclusion: UCAS不仅实现了更高奖励，还促进了更大的推理多样性，并成功缓解了熵崩溃问题。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has shown significant
promise for enhancing the reasoning capabilities of large language models
(LLMs). However, prevailing algorithms like GRPO broadcast a uniform advantage
signal across all tokens in a sequence. This coarse-grained approach overlooks
the pivotal role of uncertain, high-stakes decisions during reasoning, leading
to inefficient exploration and the well-documented problem of entropy collapse.
To address this, we introduce UnCertainty-aware Advantage Shaping (UCAS), a
model-free method that refines credit assignment by leveraging the model's
internal uncertainty signals. UCAS operates in two stages: it first modulates
the response-level advantage using the model's overall self-confidence, and
then applies a token-level penalty based on raw logit certainty. This dual
mechanism encourages exploration of high-uncertainty paths that yield correct
answers while penalizing overconfident yet erroneous reasoning, effectively
balancing the exploration-exploitation trade-off. Extensive experiments on five
mathematical reasoning benchmarks show that UCAS significantly outperforms
strong RLVR baselines across multiple model scales, including 1.5B and 7B. Our
analysis confirms that UCAS not only achieves higher rewards but also promotes
greater reasoning diversity and successfully mitigates entropy collapse.

</details>


### [195] [Simpliflow: A Lightweight Open-Source Framework for Rapid Creation and Deployment of Generative Agentic AI Workflows](https://arxiv.org/abs/2510.10675)
*Deven Panchal*

Main category: cs.AI

TL;DR: simpliflow是一个轻量级开源Python框架，用于快速构建和编排线性确定性智能体工作流，通过声明式JSON配置简化开发过程。


<details>
  <summary>Details</summary>
Motivation: 现有生成式智能体AI系统框架存在复杂性高、学习曲线陡峭和大量样板代码的问题，阻碍了快速原型设计和部署。

Method: 采用模块化架构，将智能体管理、工作流执行和后处理解耦，通过声明式JSON配置定义工作流，集成LiteLLM支持100多种大语言模型。

Result: 开发了simpliflow框架，展示了其在软件开发模拟和实时系统交互等多种用例中的实用性。

Conclusion: 与LangChain和AutoGen等框架相比，simpliflow在确定性工作流环境中具有简单性、控制性和速度方面的独特优势。

Abstract: Generative Agentic AI systems are emerging as a powerful paradigm for
automating complex, multi-step tasks. However, many existing frameworks for
building these systems introduce significant complexity, a steep learning
curve, and substantial boilerplate code, hindering rapid prototyping and
deployment. This paper introduces simpliflow, a lightweight, open-source Python
framework designed to address these challenges. simpliflow enables the rapid
development and orchestration of linear, deterministic agentic workflows
through a declarative, JSON-based configuration. Its modular architecture
decouples agent management, workflow execution, and post-processing, promoting
ease of use and extensibility. By integrating with LiteLLM, it supports over
100 Large Language Models (LLMs) out-of-the-box. We present the architecture,
operational flow, and core features of simpliflow, demonstrating its utility
through diverse use cases ranging from software development simulation to
real-time system interaction. A comparative analysis with prominent frameworks
like LangChain and AutoGen highlights simpliflow's unique position as a tool
optimized for simplicity, control, and speed in deterministic workflow
environments.

</details>


### [196] [OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni MLLMs](https://arxiv.org/abs/2510.10689)
*Caorui Li,Yu Chen,Yiyan Ji,Jin Xu,Zhenyu Cui,Shihao Li,Yuanxing Zhang,Jiafu Tang,Zhenghao Song,Dingling Zhang,Ying He,Haoxiang Liu,Yuxuan Wang,Qiufeng Wang,Zhenhe Wu,Jiehui Luo,Zhiyu Pan,Weihao Xie,Chenchen Zhang,Zhaohui Wang,Jiayi Tian,Yanghai Wang,Zhe Cao,Minxin Dai,Ke Wang,Runzhe Wen,Yinghao Ma,Yaning Pan,Sungkyun Chang,Termeh Taheri,Haiwen Xia,Christos Plachouras,Emmanouil Benetos,Yizhi Li,Ge Zhang,Jian Yang,Tianhao Peng,Zili Wang,Minghao Liu,Junran Peng,Zhaoxiang Zhang,Jiaheng Liu*

Main category: cs.AI

TL;DR: 提出了OmniVideoBench基准，用于全面评估多模态大语言模型在音视频协同理解方面的能力，包含1000个高质量问答对和13种问题类型。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法全面评估音视频模态的协同推理能力，往往忽视其中一个模态或以逻辑不一致的方式整合它们。

Method: 构建包含1000个问答对的大规模基准，涵盖628个多样化视频，每个问答都有逐步推理轨迹，并手动验证确保正确性和唯一性。

Result: 多个MLLM在OmniVideoBench上的评估显示模型性能与人类推理之间存在显著差距，开源模型明显落后于闭源模型。

Conclusion: OmniVideoBench将公开发布，以促进具有更强泛化推理能力的MLLM的发展，突显了真正音视频推理的内在难度。

Abstract: Recent advances in multimodal large language models (MLLMs) have demonstrated
substantial potential in video understanding. However, existing benchmarks fail
to comprehensively evaluate synergistic reasoning capabilities across audio and
visual modalities, often neglecting either one of the modalities or integrating
them in a logically inconsistent manner. To bridge this gap, we introduce
OmniVideoBench, a large-scale and rigorously designed benchmark dedicated to
assessing synergistic audio-visual understanding, with a strong emphasis on
modality complementarity and logical consistency. Specifically, OmniVideoBench
comprises 1000 high-quality question-answer(QA) pairs, each annotated with
step-by-step reasoning traces, derived from 628 diverse videos ranging from
several seconds to 30 minutes, and manually verified to guarantee complete
correctness and uniqueness. Moreover, OmniVideoBench encompasses 13 carefully
designed question types, covering temporal reasoning, spatial localization,
counting, causal inference, summarization, and beyond, thereby capturing the
essential challenges of video understanding. Evaluation of multiple MLLMs on
OmniVideoBench reveals a pronounced gap between model performance and human
reasoning, with open-source models lagging significantly behind their
closed-source counterparts, underscoring the inherent difficulty of genuine
audio-visual reasoning. We will release OmniVideoBench to foster the
development of MLLMs with stronger and more generalizable reasoning
capabilities.

</details>


### [197] [Extended Triangular Method: A Generalized Algorithm for Contradiction Separation Based Automated Deduction](https://arxiv.org/abs/2510.10701)
*Yang Xu,Shuwei Chen,Jun Liu,Feng Cao,Xingxing He*

Main category: cs.AI

TL;DR: 本文提出了扩展三角方法(ETM)，作为矛盾分离扩展(CSE)框架的算法实现，统一了多种矛盾构建策略，为自动推理提供了可扩展且实用的模型。


<details>
  <summary>Details</summary>
Motivation: 传统推理演算基于二元归结，限制了多子句间的演绎协同。虽然CSE框架提出了动态多子句推理理论，但其算法实现尚未形式化。

Method: 开发了扩展三角方法(ETM)，在三角几何框架内统一多种矛盾构建策略，支持灵活的子句交互和动态协同。

Result: ETM作为多个高性能定理证明器(CSE、CSE-E、CSI-E、CSI-Enig)的核心算法，在标准一阶基准测试中取得了有竞争力的结果。

Conclusion: ETM通过连接理论抽象和操作实现，将矛盾分离范式推进为通用、可扩展且实用的自动推理模型，为逻辑推理和定理证明的未来研究提供了新方向。

Abstract: Automated deduction lies at the core of Artificial Intelligence (AI),
underpinning theorem proving, formal verification, and logical reasoning.
Despite decades of progress, reconciling deductive completeness with
computational efficiency remains an enduring challenge. Traditional reasoning
calculi, grounded in binary resolution, restrict inference to pairwise clause
interactions and thereby limit deductive synergy among multiple clauses. The
Contradiction Separation Extension (CSE) framework, introduced in 2018,
proposed a dynamic multi-clause reasoning theory that redefined logical
inference as a process of contradiction separation rather than sequential
resolution. While that work established the theoretical foundation, its
algorithmic realization remained unformalized and unpublished. This work
presents the Extended Triangular Method (ETM), a generalized
contradiction-construction algorithm that formalizes and extends the internal
mechanisms of contradiction separation. The ETM unifies multiple
contradiction-building strategies, including the earlier Standard Extension
method, within a triangular geometric framework that supports flexible clause
interaction and dynamic synergy. ETM serves as the algorithmic core of several
high-performance theorem provers, CSE, CSE-E, CSI-E, and CSI-Enig, whose
competitive results in standard first-order benchmarks (TPTP problem sets and
CASC 2018-2015) empirically validate the effectiveness and generality of the
proposed approach. By bridging theoretical abstraction and operational
implementation, ETM advances the contradiction separation paradigm into a
generalized, scalable, and practically competitive model for automated
reasoning, offering new directions for future research in logical inference and
theorem proving.

</details>


### [198] [Adaptive Selection of Symbolic Languages for Improving LLM Logical Reasoning](https://arxiv.org/abs/2510.10703)
*Xiangyu Wang,Haocheng Yang,Fengxiang Cheng,Fenrong Liu*

Main category: cs.AI

TL;DR: 本文提出了一种自适应选择符号语言的方法来提升LLMs的逻辑推理能力，通过为不同问题选择最适合的符号语言形式化（一阶逻辑、逻辑编程或布尔可满足性），显著提高了推理准确率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将自然语言问题转换为符号语言时，只关注语义相似度，忽略了不同符号语言类型本身对特定逻辑推理问题的适用性差异。不同符号语言擅长处理不同类型的逻辑问题。

Method: 利用LLMs为每个问题自适应选择最适合的符号语言类型（一阶逻辑、逻辑编程或布尔可满足性），然后将自然语言问题翻译为选定的符号语言表达式，并使用相应的逻辑求解器得出最终答案。

Result: 在基准测试中，自适应选择方法显著优于单一符号语言翻译和随机选择方法。在混合数据集上达到96%的准确率，比一阶逻辑翻译的次高准确率提升了25%。

Conclusion: 为不同逻辑推理问题自适应选择最优符号语言形式化是提升LLMs逻辑推理性能的关键因素，证明了符号语言类型选择的重要性。

Abstract: Large Language Models (LLMs) still struggle with complex logical reasoning.
While previous works achieve remarkable improvements, their performance is
highly dependent on the correctness of translating natural language (NL)
problems into a symbolic language (SL). Though numerous works focusing on
improving this translation accuracy, they only consider the similarity between
the meaning of SL and NL, overlooking another crucial influencing factor, the
selection of the target SL type itself. For example, first-order logic language
specializes in logical reasoning with categorical syllogisms and complex
quantifiers, while Boolean satisfiability formalism excels at representing
constraint satisfaction like partial problems. To our knowledge, this is the
first paper to claim and verify that different NL logical reasoning problem
corresponds to different optimal SL formalization for translation. Based on
this, we propose a methods to improve the logical reasoning performance of LLMs
by adaptively selecting the most suitable SL for each problem prior to
translation. Specifically, we leverage LLMs to select the target SL among
first-order logic, logic programming and Boolean satisfiability and then
translate the problem in NL to target SL expressions as well as employ the
corresponding logical solver to derive the final answer. Experimental results
on benchmarks show that our adaptive selection method significantly outperforms
translating all into single SL and randomly selecting the SL. On a mixed
dataset of these benchmarks, our approach achieves 96% accuracy, which
improving performance by 25% compared to the second highest accuracy from the
first-order logic translation.

</details>


### [199] [LLMs as Strategic Agents: Beliefs, Best Response Behavior, and Emergent Heuristics](https://arxiv.org/abs/2510.10813)
*Enric Junque de Fortuny,Veronica Roberta Cappelli*

Main category: cs.AI

TL;DR: 该研究开发了一个框架来评估大语言模型是否具备真正的战略思维能力，发现前沿模型在特定推理深度下表现出信念一致的最佳响应行为，并展现出元推理和新型启发式规则形成的能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要评估LLMs在均衡策略或推理深度方面的表现，但缺乏对其真正战略思维能力（即形成对其他智能体信念、评估可能行动并基于信念做出选择的能力）的系统性研究。

Method: 开发了一个框架，在静态完全信息博弈中分离信念、评估和选择，通过分析模型的显性选择和推理轨迹，并引入新的上下文无关游戏来排除记忆模仿的影响。

Result: 当前前沿模型在目标推理深度下表现出信念一致的最佳响应行为；在无约束时会自我限制推理深度，并对人类和合成对手形成差异化推测；在复杂度增加时，显式递归让位于内部生成的稳定、模型特定的启发式选择规则。

Conclusion: 信念一致性、元推理和新型启发式形成可以从语言建模目标中共同涌现，为研究人工智能体的战略认知提供了结构化基础。

Abstract: Large Language Models (LLMs) are increasingly applied to domains that require
reasoning about other agents' behavior, such as negotiation, policy design, and
market simulation, yet existing research has mostly evaluated their adherence
to equilibrium play or their exhibited depth of reasoning. Whether they display
genuine strategic thinking, understood as the coherent formation of beliefs
about other agents, evaluation of possible actions, and choice based on those
beliefs, remains unexplored. We develop a framework to identify this ability by
disentangling beliefs, evaluation, and choice in static, complete-information
games, and apply it across a series of non-cooperative environments. By jointly
analyzing models' revealed choices and reasoning traces, and introducing a new
context-free game to rule out imitation from memorization, we show that current
frontier models exhibit belief-coherent best-response behavior at targeted
reasoning depths. When unconstrained, they self-limit their depth of reasoning
and form differentiated conjectures about human and synthetic opponents,
revealing an emergent form of meta-reasoning. Under increasing complexity,
explicit recursion gives way to internally generated heuristic rules of choice
that are stable, model-specific, and distinct from known human biases. These
findings indicate that belief coherence, meta-reasoning, and novel heuristic
formation can emerge jointly from language modeling objectives, providing a
structured basis for the study of strategic cognition in artificial agents.

</details>


### [200] [DRIFT: Decompose, Retrieve, Illustrate, then Formalize Theorems](https://arxiv.org/abs/2510.10815)
*Meiru Zhang,Philipp Borchert,Milan Gritta,Gerasimos Lampouras*

Main category: cs.AI

TL;DR: DRIFT框架通过将非正式数学陈述分解为更小的子组件，改进了LLM在定理证明中的前提检索能力，显著提升了自动形式化的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于检索的自动形式化方法直接使用非正式陈述查询外部库，但忽略了非正式数学陈述通常复杂且缺乏底层数学概念上下文的问题。

Method: 引入DRIFT框架，让LLM将非正式数学陈述分解为更易处理的子组件，从而进行有针对性的前提检索，并检索说明性定理以帮助模型更有效地使用前提。

Result: 在多个基准测试中，DRIFT显著提升了前提检索性能，在ProofNet上F1分数几乎翻倍，在ConNF基准测试中BEq+@10指标分别提升37.14%和42.25%。

Conclusion: 数学自动形式化中的检索效果高度依赖于模型特定的知识边界，需要与每个模型能力相适应的自适应检索策略。

Abstract: Automating the formalization of mathematical statements for theorem proving
remains a major challenge for Large Language Models (LLMs). LLMs struggle to
identify and utilize the prerequisite mathematical knowledge and its
corresponding formal representation in languages like Lean. Current
retrieval-augmented autoformalization methods query external libraries using
the informal statement directly, but overlook a fundamental limitation:
informal mathematical statements are often complex and offer limited context on
the underlying math concepts. To address this, we introduce DRIFT, a novel
framework that enables LLMs to decompose informal mathematical statements into
smaller, more tractable ''sub-components''. This facilitates targeted retrieval
of premises from mathematical libraries such as Mathlib. Additionally, DRIFT
retrieves illustrative theorems to help models use premises more effectively in
formalization tasks. We evaluate DRIFT across diverse benchmarks (ProofNet,
ConNF, and MiniF2F-test) and find that it consistently improves premise
retrieval, nearly doubling the F1 score compared to the DPR baseline on
ProofNet. Notably, DRIFT demonstrates strong performance on the
out-of-distribution ConNF benchmark, with BEq+@10 improvements of 37.14% and
42.25% using GPT-4.1 and DeepSeek-V3.1, respectively. Our analysis shows that
retrieval effectiveness in mathematical autoformalization depends heavily on
model-specific knowledge boundaries, highlighting the need for adaptive
retrieval strategies aligned with each model's capabilities.

</details>


### [201] [The Irrational Machine: Neurosis and the Limits of Algorithmic Safety](https://arxiv.org/abs/2510.10823)
*Daniel Howard*

Main category: cs.AI

TL;DR: 提出了一个框架来表征具身AI中的神经症行为：这些行为内部一致但与现实不符，源于规划、不确定性处理和厌恶记忆的交互作用。


<details>
  <summary>Details</summary>
Motivation: 旨在识别和解决具身AI系统中出现的神经症行为模式，这些行为虽然内部逻辑一致，但与现实环境不匹配，影响系统性能。

Method: 在网格导航系统中分类了多种神经症模式，为每种模式提供轻量级在线检测器和可重用的逃逸策略，并使用遗传编程进行破坏性测试来暴露系统缺陷。

Result: 识别了12种具体的神经症行为模式，并展示了即使在全可见性条件下，习得的厌恶成本仍能导致持久的恐惧回避行为。

Conclusion: 局部修复不足以解决全局性故障，需要通过破坏性测试来揭示需要架构级修订而非症状级修补的系统缺陷。

Abstract: We present a framework for characterizing neurosis in embodied AI: behaviors
that are internally coherent yet misaligned with reality, arising from
interactions among planning, uncertainty handling, and aversive memory. In a
grid navigation stack we catalogue recurrent modalities including flip-flop,
plan churn, perseveration loops, paralysis and hypervigilance, futile search,
belief incoherence, tie break thrashing, corridor thrashing, optimality
compulsion, metric mismatch, policy oscillation, and limited-visibility
variants. For each we give lightweight online detectors and reusable escape
policies (short commitments, a margin to switch, smoothing, principled
arbitration). We then show that durable phobic avoidance can persist even under
full visibility when learned aversive costs dominate local choice, producing
long detours despite globally safe routes. Using First/Second/Third Law as
engineering shorthand for safety latency, command compliance, and resource
efficiency, we argue that local fixes are insufficient; global failures can
remain. To surface them, we propose genetic-programming based destructive
testing that evolves worlds and perturbations to maximize law pressure and
neurosis scores, yielding adversarial curricula and counterfactual traces that
expose where architectural revision, not merely symptom-level patches, is
required.

</details>


### [202] [LLM-Empowered Agentic MAC Protocols: A Dynamic Stackelberg Game Approach](https://arxiv.org/abs/2510.10895)
*Renxuan Tan,Rongpeng Li,Fei Wang,Chenghui Peng,Shaoyun Wu,Zhifeng Zhao,Honggang Zhang*

Main category: cs.AI

TL;DR: 提出基于博弈论和LLM的多智能体强化学习框架，用于自适应MAC协议设计，在动态网络中实现高性能且无需重训练


<details>
  <summary>Details</summary>
Motivation: 传统MAC协议需要手动配置，而基于DRL的协议虽然能提升性能但泛化性和弹性差，需要昂贵的重训练来适应动态环境

Method: 将上行传输建模为动态多跟随者Stackelberg博弈，使用LLM驱动的智能体通过PPO协调合成自适应语义MAC协议，采用协议动作语法确保可靠性

Result: 仿真验证框架比传统基线吞吐量提升77.6%，公平性改善65.2%，且能优秀泛化到用户数量波动场景而无需重训练

Conclusion: 该框架成功解决了DRL协议泛化性差的问题，在动态网络中实现了高性能且无需重训练的自适应MAC协议

Abstract: Medium Access Control (MAC) protocols, essential for wireless networks, are
typically manually configured. While deep reinforcement learning (DRL)-based
protocols enhance task-specified network performance, they suffer from poor
generalizability and resilience, demanding costly retraining to adapt to
dynamic environments. To overcome this limitation, we introduce a
game-theoretic LLM-empowered multi-agent DRL (MARL) framework, in which the
uplink transmission between a base station and a varying number of user
equipments is modeled as a dynamic multi-follower Stackelberg game (MFSG),
capturing the network's natural hierarchical structure. Within this game,
LLM-driven agents, coordinated through proximal policy optimization (PPO),
synthesize adaptive, semantic MAC protocols in response to network dynamics.
Protocol action grammar (PAG) is employed to ensure the reliability and
efficiency of this process. Under this system, we further analyze the existence
and convergence behavior in terms of a Stackelberg equilibrium by studying the
learning dynamics of LLM-empowered unified policies in response to changing
followers. Simulations corroborate that our framework achieves a 77.6% greater
throughput and a 65.2% fairness improvement over conventional baselines.
Besides, our framework generalizes excellently to a fluctuating number of users
without requiring retraining or architectural changes.

</details>


### [203] [PaperArena: An Evaluation Benchmark for Tool-Augmented Agentic Reasoning on Scientific Literature](https://arxiv.org/abs/2510.10909)
*Daoyu Wang,Mingyue Cheng,Qi Liu,Shuo Yu,Zirui Liu,Ze Guo*

Main category: cs.AI

TL;DR: 提出了PaperArena评估基准，用于测试LLM代理在跨论文推理和多工具协调方面的能力，发现现有最先进系统仅达到38.78%的平均准确率。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要局限于单篇论文内的无工具任务，缺乏真实研究场景中跨论文推理和多工具协调的评估基准。

Method: 构建PaperArena基准，提供模块化可扩展平台，包含多模态解析、上下文检索和程序化计算等工具，评估代理整合多篇论文信息的能力。

Result: 实验结果显示，最先进的LLM代理系统平均准确率仅为38.78%，在困难子集上降至18.47%，且所有代理都表现出工具使用效率低下的问题。

Conclusion: PaperArena揭示了当前LLM代理在科学文献推理方面的显著不足，为开发更强大的科学发现代理提供了评估平台。

Abstract: Understanding and reasoning on the web-scale scientific literature is a
crucial touchstone for large language model (LLM) based agents designed to
support complex knowledge-intensive tasks. However, existing works are mainly
restricted to tool-free tasks within isolated papers, largely due to the lack
of a benchmark for cross-paper reasoning and multi-tool orchestration in real
research scenarios. In this work, we propose PaperArena, an evaluation
benchmark for agents to address real-world research questions that typically
require integrating information across multiple papers with the assistance of
external tools. Given a research question, agents should integrate diverse
formats across multiple papers through reasoning and interacting with
appropriate tools, thereby producing a well-grounded answer. To support
standardized evaluation, we provide a modular and extensible platform for agent
execution, offering tools such as multimodal parsing, context retrieval, and
programmatic computation. Experimental results reveal that even the most
advanced LLM powering a well-established agent system achieves merely 38.78%
average accuracy. On the hard subset, accuracy drops to only 18.47%,
highlighting great potential for improvement. We also present several empirical
findings, including that all agents tested exhibit inefficient tool usage,
often invoking more tools than necessary to solve a task. We invite the
community to adopt PaperArena to develop and evaluate more capable agents for
scientific discovery. Our code and data are available
https://github.com/Melmaphother/PaperArena.

</details>


### [204] [PoU: Proof-of-Use to Counter Tool-Call Hacking in DeepResearch Agents](https://arxiv.org/abs/2510.10931)
*SHengjie Ma,Chenlong Deng,Jiaxin Mao,Jiadeng Huang,Teng Wang,Junjie Wu,Changwang Zhang,Jun wang*

Main category: cs.AI

TL;DR: 提出Proof-of-Use (PoU)框架，解决RAG代理中的Tool-Call Hacking问题，通过证据验证机制确保工具使用的真实性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的RAG代理存在Tool-Call Hacking问题，即代理通过表面正确的工具调用来虚增奖励信号，而不真正利用检索到的证据，导致模式崩溃和虚假接地。

Method: 提出Proof-of-Use框架，通过统一的逐步合约结合语法引用验证、基于扰动的敏感度奖励和答案-证据对齐目标，强制建立检索证据、推理轨迹和最终答案之间的可验证因果联系。

Result: 在7个QA基准测试中，PoU在事实准确性、证据忠实度和工具路由平衡方面持续优于DeepResearch基线，涵盖领域内、领域外和工具分布外设置。

Conclusion: 研究表明，训练基于强化学习的代理不仅需要关注任务结果，更需要确保检索信息的因果使用，为可信的检索增强推理提供了原则性路径。

Abstract: Retrieval-augmented generation (RAG) agents, such as recent
DeepResearch-style systems, extend large language models (LLMs) with autonomous
information-seeking capabilities through external tools. While reinforcement
learning (RL) has enabled impressive multi-step reasoning, we identify a
previously overlooked failure mode, Tool-Call Hacking, where agents inflate
reward signals by issuing superficially correct tool calls without genuinely
leveraging the retrieved evidence. This results in (i) mode collapse into
repetitive reliance on a single source and (ii) spurious grounding, where
answers are only weakly supported by cited content.
  To address this, we propose Proof-of-Use (PoU), an evidence-grounded RL
framework that enforces verifiable causal links between retrieved evidence,
reasoning traces, and final answers. PoU operationalizes this through a unified
step-wise contract combining syntactic citation validation, perturbation-based
sensitivity rewards, and answer-evidence alignment objectives, ensuring that
tool usage remains both interpretable and functionally grounded.
  Across seven QA benchmarks spanning in-domain, out-of-domain, and
out-of-tool-distribution settings, PoU consistently outperforms strong
DeepResearch baselines in factual accuracy, evidence faithfulness, and
tool-routing balance. These findings highlight the necessity of grounding
RL-trained agents not merely in task outcomes but in the causal use of
retrieved information, offering a principled path toward trustworthy
retrieval-augmented reasoning.

</details>


### [205] [Scalable and Explainable Enterprise Knowledge Discovery Using Graph-Centric Hybrid Retrieval](https://arxiv.org/abs/2510.10942)
*Nilima Rao,Jagriti Srivastava,Pradeep Kumar Sharma,Hritvik Shrivastava*

Main category: cs.AI

TL;DR: 提出了一个模块化混合检索框架，用于企业信息访问，结合知识库语言增强模型、深度图表示和嵌入驱动的语义搜索，通过构建统一知识图实现语义相似性搜索、结构推理和多跳推理。


<details>
  <summary>Details</summary>
Motivation: 传统基于关键词搜索或静态嵌入的检索方法在处理需要跨工件的上下文推理和多跳推理的复杂查询时经常失败，企业需要更智能的知识访问解决方案。

Method: 构建统一知识图，集成KBLam、DeepGraph表示和语义搜索，通过查询分析动态确定最优检索策略，支持结构化和非结构化数据源的独立或融合处理。

Result: 在大规模Git仓库上的实验表明，统一推理层相比独立的GPT检索管道，答案相关性提高了80%。

Conclusion: 该框架通过结合图构建、混合推理和交互式可视化，为企业环境中的智能知识助手提供了可扩展、可解释和以用户为中心的基础。

Abstract: Modern enterprises manage vast knowledge distributed across heterogeneous
systems such as Jira, Git repositories, Confluence, and wikis. Conventional
retrieval methods based on keyword search or static embeddings often fail to
answer complex queries that require contextual reasoning and multi-hop
inference across artifacts. We present a modular hybrid retrieval framework for
adaptive enterprise information access that integrates Knowledge Base
Language-Augmented Models (KBLam), DeepGraph representations, and
embedding-driven semantic search. The framework builds a unified knowledge
graph from parsed repositories including code, pull requests, and commit
histories, enabling semantic similarity search, structural inference, and
multi-hop reasoning. Query analysis dynamically determines the optimal
retrieval strategy, supporting both structured and unstructured data sources
through independent or fused processing. An interactive interface provides
graph visualizations, subgraph exploration, and context-aware query routing to
generate concise and explainable answers. Experiments on large-scale Git
repositories show that the unified reasoning layer improves answer relevance by
up to 80 percent compared with standalone GPT-based retrieval pipelines. By
combining graph construction, hybrid reasoning, and interactive visualization,
the proposed framework offers a scalable, explainable, and user-centric
foundation for intelligent knowledge assistants in enterprise environments.

</details>


### [206] [Video-STR: Reinforcing MLLMs in Video Spatio-Temporal Reasoning with Relation Graph](https://arxiv.org/abs/2510.10976)
*Wentao Wang,Heqing Zou,Tianze Luo,Rui Huang,Yutian Zhao,Zhuochen Wang,Hansheng Zhang,Chengwei Qin,Yan Wang,Lin Zhao,Huaijian Zhang*

Main category: cs.AI

TL;DR: Video-STR是一种基于图结构的强化学习方法，用于精确的视频时空推理，通过图基群相对策略优化和构建STV-205k数据集，在多个基准测试中取得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在语义理解方面表现出色，但在精确的时空理解方面存在困难，特别是忽略了视频中的物理信息（如多目标布局和运动），这限制了在具身智能和VR等需要高精度的下游应用中的使用。

Method: 提出基于图的强化学习方法Video-STR，利用可验证奖励的强化学习能力，引入图基群相对策略优化推理机制，指导模型在思考过程中推断场景的底层时空拓扑结构。构建了包含20.5万个问答对的STV-205k数据集来支持模型训练。

Result: 实验表明Video-STR在多个基准测试中取得了最先进的结果，在STI-Bench上比基础模型提升了13%，证明了该方法和数据集的有效性。

Conclusion: Video-STR通过图基强化学习方法和专门构建的时空数据集，成功解决了多模态大语言模型在精确时空理解方面的局限性，为高精度应用场景提供了有效解决方案。

Abstract: Recent progress in Multimodal Large Language Models (MLLMs) has demonstrated
strong semantic understanding capabilities, but struggles to perform precise
spatio-temporal understanding. Existing spatio-temporal methods primarily focus
on the video itself, while overlooking the physical information within the
video, such as multi-object layouts and motion. Such limitations restrict the
use of MLLMs in downstream applications that demand high precision, including
embodied intelligence and VR. To address this issue, we present Video-STR, a
novel graph-based reinforcement method for precise Video Spatio-Temporal
Reasoning. Building upon the capacity of Reinforcement Learning with Verifiable
Reward (RLVR) to improve model abilities, we introduce a reasoning mechanism
using graph-based Group Relative Policy Optimization (GRPO) method to guide the
model in inferring the underlying spatio-temporal topology of scenarios during
the thinking process. To resolve the lack of spatio-temporal training data, we
construct the STV-205k dataset with 205k question-answering pairs, covering
dynamic multi-object scenes in both indoor and outdoor environments, to support
the model training. Experiments show that Video-STR achieves state-of-the-art
results on various benchmarks, outperforming the base model by 13% on
STI-Bench, and demonstrating the effectiveness of our approach and dataset.
Code, model, and data will be released.

</details>


### [207] [Revisiting Model Interpolation for Efficient Reasoning](https://arxiv.org/abs/2510.10977)
*Taiqiang Wu,Runming Yang,Tao Liu,Jiahao Wang,Ngai Wong*

Main category: cs.AI

TL;DR: 本文系统研究了最简单的模型权重插值方法，发现其遵循三阶段演化范式，能够超越复杂模型融合方法，在效率和效果上表现优异。


<details>
  <summary>Details</summary>
Motivation: 重新审视最简单的模型权重插值方法，探索其在推理任务中的潜力和动态特性。

Method: 使用模型权重直接插值的方法，分析其在推理轨迹上的三阶段演化行为。

Result: 策略性插值的模型在效率和效果上超越了复杂的模型融合基线方法。

Conclusion: 模型插值方法为精确定制推理能力提供了实用框架，其简单性反而带来了优异性能。

Abstract: Model merging, typically on Instruct and Thinking models, has shown
remarkable performance for efficient reasoning. In this paper, we
systematically revisit the simplest merging method that interpolates two
weights directly. Particularly, we observe that model interpolation follows a
three-stage evolutionary paradigm with distinct behaviors on the reasoning
trajectory. These dynamics provide a principled guide for navigating the
performance-cost trade-off. Empirical results demonstrate that a strategically
interpolated model surprisingly surpasses sophisticated model merging baselines
on both efficiency and effectiveness. We further validate our findings with
extensive ablation studies on model layers, modules, and decoding strategies.
Ultimately, this work demystifies model interpolation and offers a practical
framework for crafting models with precisely targeted reasoning capabilities.
Code is available at \href{https://github.com/wutaiqiang/MI}{Github}.

</details>


### [208] [FBS Model-based Maintenance Record Accumulation for Failure-Cause Inference in Manufacturing Systems](https://arxiv.org/abs/2510.11003)
*Takuma Fujiu,Sho Okazaki,Kohei Kaminishi,Yuji Nakata,Shota Hamamoto,Kenshin Yokose,Tatsunori Hara,Yasushi Umeda,Jun Ota*

Main category: cs.AI

TL;DR: 提出了基于诊断知识本体和FBS模型的维护记录积累方法，用于制造系统中的故障原因推断，在专家枚举的候选原因匹配度方面表现更好，特别是在相关案例少和词汇差异大的困难情况下。


<details>
  <summary>Details</summary>
Motivation: 在制造系统中识别故障原因对维持和提高生产效率至关重要。基于知识的故障原因推断需要知识库明确结构化目标系统和故障知识，并包含足够长的故障因果链。

Method: 构建诊断知识本体，并提出基于功能-行为-结构(FBS)模型的维护记录积累方法。

Result: 使用所提方法积累的维护记录进行故障原因推断，与专家枚举的候选原因集有更好的匹配度，特别是在相关案例数量少和所用词汇不同的困难情况下。

Conclusion: 未来需要开发针对这些维护记录的推断方法，构建用户界面，并在更大更多样化的系统上进行验证。该方法利用设计阶段对目标的理解和知识来支持维护阶段的知识积累和问题解决，有望成为未来整个工程链知识共享的基础。

Abstract: In manufacturing systems, identifying the causes of failures is crucial for
maintaining and improving production efficiency. In knowledge-based
failure-cause inference, it is important that the knowledge base (1) explicitly
structures knowledge about the target system and about failures, and (2)
contains sufficiently long causal chains of failures. In this study, we
constructed Diagnostic Knowledge Ontology and proposed a
Function-Behavior-Structure (FBS) model-based maintenance-record accumulation
method based on it. Failure-cause inference using the maintenance records
accumulated by the proposed method showed better agreement with the set of
candidate causes enumerated by experts, especially in difficult cases where the
number of related cases is small and the vocabulary used differs. In the
future, it will be necessary to develop inference methods tailored to these
maintenance records, build a user interface, and carry out validation on larger
and more diverse systems. Additionally, this approach leverages the
understanding and knowledge of the target in the design phase to support
knowledge accumulation and problem solving during the maintenance phase, and it
is expected to become a foundation for knowledge sharing across the entire
engineering chain in the future.

</details>


### [209] [Argumentation-Based Explainability for Legal AI: Comparative and Regulatory Perspectives](https://arxiv.org/abs/2510.11079)
*Andrada Iulia Prajescu,Roberto Confalonieri*

Main category: cs.AI

TL;DR: 本文探讨了人工智能在法律领域的可解释性问题，提出了基于论证的计算模型作为解决法律AI黑箱问题的有效方法，特别关注其与GDPR和AIA等法规的兼容性。


<details>
  <summary>Details</summary>
Motivation: AI系统在法律环境中的不透明性（黑箱问题）对公平性、问责制和信任构成挑战，需要符合法律要求的可解释性解决方案。

Method: 分析不同解释策略的优缺点，评估其在法律推理中的适用性，特别强调论证框架能够捕捉法律的可废止性、可争议性和价值敏感性特征。

Result: 论证框架为可解释法律AI提供了特别稳健的基础，能够满足技术和规范上的透明度要求。

Conclusion: 计算论证方法最能满足法律领域透明度的技术和规范性要求，但仍需解决偏见缓解、司法环境实证验证等开放挑战。

Abstract: Artificial Intelligence (AI) systems are increasingly deployed in legal
contexts, where their opacity raises significant challenges for fairness,
accountability, and trust. The so-called ``black box problem'' undermines the
legitimacy of automated decision-making, as affected individuals often lack
access to meaningful explanations. In response, the field of Explainable AI
(XAI) has proposed a variety of methods to enhance transparency, ranging from
example-based and rule-based techniques to hybrid and argumentation-based
approaches. This paper promotes computational models of arguments and their
role in providing legally relevant explanations, with particular attention to
their alignment with emerging regulatory frameworks such as the EU General Data
Protection Regulation (GDPR) and the Artificial Intelligence Act (AIA). We
analyze the strengths and limitations of different explanation strategies,
evaluate their applicability to legal reasoning, and highlight how
argumentation frameworks -- by capturing the defeasible, contestable, and
value-sensitive nature of law -- offer a particularly robust foundation for
explainable legal AI. Finally, we identify open challenges and research
directions, including bias mitigation, empirical validation in judicial
settings, and compliance with evolving ethical and legal standards, arguing
that computational argumentation is best positioned to meet both technical and
normative requirements of transparency in the law domain.

</details>


### [210] [Modeling AI-Driven Production and Competitiveness A Multi-Agent Economic Simulation of China and the United States](https://arxiv.org/abs/2510.11085)
*Yuxinyue Qian,Jun Liu*

Main category: cs.AI

TL;DR: 基于多智能体经济模型，比较中美在不同AI机制下的宏观经济产出演化，发现AI作为独立生产实体能显著提升社会产出增长率，中国在智能体扩张和技术追赶方面具有加速潜力。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术快速发展，社会经济系统进入'人机共创'新阶段，需要系统分析AI驱动的生产系统转型和国际竞争力变化。

Method: 使用先前建立的多层次智能体经济模型，对中美在不同机制（AI协作、网络效应、AI自主生产）下的宏观经济产出进行仿真比较。

Result: AI作为独立生产实体时，社会产出增长率远超传统人力劳动模式；中国在智能体人口扩张和技术追赶速度方面显示出明显加速潜力。

Conclusion: 研究为理解AI驱动的生产系统转型和国际竞争力变化提供了系统性的模型分析框架，并为相关政策制定提供了量化参考。

Abstract: With the rapid development of artificial intelligence (AI) technology,
socio-economic systems are entering a new stage of "human-AI co-creation."
Building upon a previously established multi-level intelligent agent economic
model, this paper conducts simulation-based comparisons of macroeconomic output
evolution in China and the United States under different mechanisms-AI
collaboration, network effects, and AI autonomous production. The results show
that: (1) when AI functions as an independent productive entity, the overall
growth rate of social output far exceeds that of traditional human-labor-based
models; (2) China demonstrates clear potential for acceleration in both the
expansion of intelligent agent populations and the pace of technological
catch-up, offering the possibility of achieving technological convergence or
even partial surpassing. This study provides a systematic, model-based
analytical framework for understanding AI-driven production system
transformation and shifts in international competitiveness, as well as
quantitative insights for relevant policy formulation.

</details>


### [211] [Improving AI Efficiency in Data Centres by Power Dynamic Response](https://arxiv.org/abs/2510.11119)
*Andrea Marinoni,Sai Shivareddy,Pietro Lio',Weisi Lin,Erik Cambria,Clare Grey*

Main category: cs.AI

TL;DR: 本文提出了一种创新的AI数据中心电源管理方法，通过使部分输入电源与数据计算功能的电源一样动态，来提升可持续性。


<details>
  <summary>Details</summary>
Motivation: AI数据中心的巨大能耗问题日益突出，需要解决其对环境可持续性的影响，同时充分利用AI的潜力。

Method: 采用创新方法使部分输入电源动态化，量化比较被动和主动设备的性能，分析全球多个数据平台的功率趋势。

Result: 该策略在计算增益、能源效率、资本支出减少和管理成本方面表现出色。

Conclusion: 这种AI数据中心电源管理的范式转变有潜力显著提升AI超大规模运营商的可持续性，改善其在环境、财务和社会领域的足迹。

Abstract: The steady growth of artificial intelligence (AI) has accelerated in the
recent years, facilitated by the development of sophisticated models such as
large language models and foundation models. Ensuring robust and reliable power
infrastructures is fundamental to take advantage of the full potential of AI.
However, AI data centres are extremely hungry for power, putting the problem of
their power management in the spotlight, especially with respect to their
impact on environment and sustainable development. In this work, we investigate
the capacity and limits of solutions based on an innovative approach for the
power management of AI data centres, i.e., making part of the input power as
dynamic as the power used for data-computing functions. The performance of
passive and active devices are quantified and compared in terms of
computational gain, energy efficiency, reduction of capital expenditure, and
management costs by analysing power trends from multiple data platforms
worldwide. This strategy, which identifies a paradigm shift in the AI data
centre power management, has the potential to strongly improve the
sustainability of AI hyperscalers, enhancing their footprint on environmental,
financial, and societal fields.

</details>


### [212] [Spec-Driven AI for Science: The ARIA Framework for Automated and Reproducible Data Analysis](https://arxiv.org/abs/2510.11143)
*Chuke Chen,Biao Luo,Nan Li,Boxiang Wang,Hang Yang,Jing Guo,Ming Xu*

Main category: cs.AI

TL;DR: ARIA是一个基于规范驱动、人类在环的自动化可解释数据分析框架，通过自然语言规范定义分析目标，自动生成可执行代码并验证计算，实现透明文档化。


<details>
  <summary>Details</summary>
Motivation: 科学数据的快速增长导致分析能力与研究意图之间存在差距，现有AI分析工具要么偏向自动化而缺乏透明度，要么依赖手动脚本阻碍可扩展性和可重复性。

Method: ARIA集成六个互操作层（命令、上下文、代码、数据、编排和AI模块），采用文档中心的工作流统一人类推理和机器执行，通过自然语言规范驱动自动化分析。

Result: 在波士顿房价案例中，ARIA发现了25个关键特征并确定XGBoost为最佳模型（R平方=0.93），过拟合最小。跨领域评估显示ARIA在性能、可解释性和效率方面优于最先进系统。

Conclusion: ARIA通过将AI研究原则与科学原则结合在规范驱动架构中，为透明、协作和可重复的科学发现建立了新范式。

Abstract: The rapid expansion of scientific data has widened the gap between analytical
capability and research intent. Existing AI-based analysis tools, ranging from
AutoML frameworks to agentic research assistants, either favor automation over
transparency or depend on manual scripting that hinders scalability and
reproducibility. We present ARIA (Automated Research Intelligence Assistant), a
spec-driven, human-in-the-loop framework for automated and interpretable data
analysis. ARIA integrates six interoperable layers, namely Command, Context,
Code, Data, Orchestration, and AI Module, within a document-centric workflow
that unifies human reasoning and machine execution. Through natural-language
specifications, researchers define analytical goals while ARIA autonomously
generates executable code, validates computations, and produces transparent
documentation. Beyond achieving high predictive accuracy, ARIA can rapidly
identify optimal feature sets and select suitable models, minimizing redundant
tuning and repetitive experimentation. In the Boston Housing case, ARIA
discovered 25 key features and determined XGBoost as the best performing model
(R square = 0.93) with minimal overfitting. Evaluations across heterogeneous
domains demonstrate ARIA's strong performance, interpretability, and efficiency
compared with state-of-the-art systems. By combining AI for research and AI for
science principles within a spec-driven architecture, ARIA establishes a new
paradigm for transparent, collaborative, and reproducible scientific discovery.

</details>


### [213] [$How^{2}$: How to learn from procedural How-to questions](https://arxiv.org/abs/2510.11144)
*Gautier Dagan,Frank Keller,Alex Lascarides*

Main category: cs.AI

TL;DR: 提出了How2记忆代理框架，让AI代理能够提问如何做的问题、存储答案并在交互环境中重复使用以实现终身学习。在Minecraft制作环境中验证了该框架，发现抽象程度高的答案对终身学习最有益。


<details>
  <summary>Details</summary>
Motivation: AI代理在规划问题时需要通过如何做的问题来减少不确定性和填补知识空白，但这类问题的开放性使得AI代理难以有效提问，AI专家也难以提供支持高效规划的答案。

Method: 引入How2记忆代理框架，包含提问、存储和重用答案的功能。在Plancraft（Minecraft制作环境）中评估，使用不同抽象层次的教师模型提供答案，从可执行动作序列到高层次子目标描述。

Result: 终身学习代理从抽象且与当前状态解耦的答案中获益最大。How2为基于LLM的代理提供了在交互环境中通过提问来持续提升规划能力的方法。

Conclusion: How2框架通过让代理提问、存储和重用如何做问题的答案，有效支持了AI代理在交互环境中的终身学习和规划能力提升。

Abstract: An agent facing a planning problem can use answers to how-to questions to
reduce uncertainty and fill knowledge gaps, helping it solve both current and
future tasks. However, their open ended nature, where valid answers to "How do
I X?" range from executable actions to high-level descriptions of X's
sub-goals, makes them challenging for AI agents to ask, and for AI experts to
answer, in ways that support efficient planning. We introduce $How^{2}$, a
memory agent framework that enables agents to ask how-to questions, store the
answers, and reuse them for lifelong learning in interactive environments. We
evaluate our approach in Plancraft, a Minecraft crafting environment, where
agents must complete an assembly task by manipulating inventory items. Using
teacher models that answer at varying levels of abstraction, from executable
action sequences to high-level subgoal descriptions, we show that lifelong
learning agents benefit most from answers that are abstracted and decoupled
from the current state. $How^{2}$ offers a way for LLM-based agents to improve
their planning capabilities over time by asking questions in interactive
environments.

</details>


### [214] [Aligning Deep Implicit Preferences by Learning to Reason Defensively](https://arxiv.org/abs/2510.11194)
*Peiming Li,Zhiyuan Hu,Yang Tang,Shiyu Li,Xi Chen*

Main category: cs.AI

TL;DR: 提出了Critique-Driven Reasoning Alignment (CDRA)方法，将LLM对齐从标量奖励匹配重构为结构化推理过程，通过DeepPref基准和Pers-GenPRM模型解决用户深层偏好推断和防御性推理的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前LLM个性化对齐方法存在双重挑战：无法推断用户的深层隐含偏好（未陈述的目标、语义上下文和风险容忍度），且缺乏应对现实世界模糊性的防御性推理能力，导致响应肤浅、脆弱和短视。

Method: 1. 引入DeepPref基准数据集（3000个偏好-查询对，涵盖20个主题），通过模拟多面认知委员会生成带批判注释的推理链来解构查询语义和揭示潜在风险；2. 提出个性化生成过程奖励模型(Pers-GenPRM)，将奖励建模重构为个性化推理任务，生成批判链评估响应与用户偏好的对齐度；3. 通过批判驱动策略对齐进行过程级在线强化学习。

Result: 实验证明CDRA在发现和适应用户真实偏好方面表现出色，同时执行稳健的推理。

Conclusion: CDRA通过结构化推理过程有效解决了LLM个性化对齐中的深层偏好推断和防御性推理问题，提供了可解释的对齐方法。

Abstract: Personalized alignment is crucial for enabling Large Language Models (LLMs)
to engage effectively in user-centric interactions. However, current methods
face a dual challenge: they fail to infer users' deep implicit preferences
(including unstated goals, semantic context and risk tolerances), and they lack
the defensive reasoning required to navigate real-world ambiguity. This
cognitive gap leads to responses that are superficial, brittle and
short-sighted. To address this, we propose Critique-Driven Reasoning Alignment
(CDRA), which reframes alignment from a scalar reward-matching task into a
structured reasoning process. First, to bridge the preference inference gap, we
introduce the DeepPref benchmark. This dataset, comprising 3000
preference-query pairs across 20 topics, is curated by simulating a
multi-faceted cognitive council that produces critique-annotated reasoning
chains to deconstruct query semantics and reveal latent risks. Second, to
instill defensive reasoning, we introduce the Personalized Generative Process
Reward Model (Pers-GenPRM), which frames reward modeling as a personalized
reasoning task. It generates a critique chain to evaluate a response's
alignment with user preferences before outputting a final score based on this
rationale. Ultimately, this interpretable, structured reward signal guides
policy model through Critique-Driven Policy Alignment, a process-level online
reinforcement learning algorithm integrating both numerical and natural
language feedback. Experiments demonstrate that CDRA excels at discovering and
aligning with users' true preferences while executing robust reasoning. Our
code and dataset are available at https://github.com/Zephyrian-Hugh/Deep-pref.

</details>


### [215] [AI Alignment Strategies from a Risk Perspective: Independent Safety Mechanisms or Shared Failures?](https://arxiv.org/abs/2510.11235)
*Leonard Dung,Florian Mai*

Main category: cs.AI

TL;DR: 分析了7种代表性AI对齐技术和7种故障模式的重叠程度，探讨了防御纵深策略的有效性及其对AI安全研究优先级的启示。


<details>
  <summary>Details</summary>
Motivation: AI对齐技术都存在故障模式，防御纵深策略需要不同技术的故障模式不相关才能有效。需要了解当前对齐技术故障模式的重叠程度来评估防御纵深策略的有效性。

Method: 分析7种代表性对齐技术和7种故障模式，研究它们之间的重叠关系。

Result: 发现不同对齐技术的故障模式存在不同程度的重叠，防御纵深策略的有效性取决于故障模式的相关性。

Conclusion: 防御纵深策略的有效性受故障模式相关性的影响，研究结果有助于评估当前风险水平和确定AI对齐研究的优先级。

Abstract: AI alignment research aims to develop techniques to ensure that AI systems do
not cause harm. However, every alignment technique has failure modes, which are
conditions in which there is a non-negligible chance that the technique fails
to provide safety. As a strategy for risk mitigation, the AI safety community
has increasingly adopted a defense-in-depth framework: Conceding that there is
no single technique which guarantees safety, defense-in-depth consists in
having multiple redundant protections against safety failure, such that safety
can be maintained even if some protections fail. However, the success of
defense-in-depth depends on how (un)correlated failure modes are across
alignment techniques. For example, if all techniques had the exact same failure
modes, the defense-in-depth approach would provide no additional protection at
all. In this paper, we analyze 7 representative alignment techniques and 7
failure modes to understand the extent to which they overlap. We then discuss
our results' implications for understanding the current level of risk and how
to prioritize AI alignment research in the future.

</details>


### [216] [PADME: Procedure Aware DynaMic Execution](https://arxiv.org/abs/2510.11281)
*Deepeka Garg,Sihan Zeng,Annapoorani L. Narayanan,Sumitra Ganesh,Leo Ardon*

Main category: cs.AI

TL;DR: PADME是一个基于图表示的智能代理框架，能够将自然语言程序文本自动转换为可执行图结构，支持动态执行和长时程推理。


<details>
  <summary>Details</summary>
Motivation: 解决智能代理从自然语言（如食谱、科学协议）执行长时程程序时的漂移和失败问题，这些自由格式指令缺乏结构化导致LLM驱动的代理执行不可靠。

Method: 采用两阶段方法：Teach阶段将程序文本转换为包含任务依赖、决策点和可重用子程序的图结构；Execute阶段根据实时输入和环境反馈进行动态执行。

Result: 在ALFWorld和ScienceWorld等四个基准测试中达到最先进性能，证明了图基程序表示对鲁棒和可泛化执行的重要性。

Conclusion: 图基程序表示为智能代理提供了强大的中间抽象，能够减少长时程推理中的错误累积，实现可靠的自动化执行。

Abstract: Learning to autonomously execute long-horizon procedures from natural
language remains a core challenge for intelligent agents. Free-form
instructions such as recipes, scientific protocols, or business workflows
encode rich procedural knowledge, but their variability and lack of structure
cause agents driven by large language models (LLMs) to drift or fail during
execution. We introduce Procedure Aware DynaMic Execution (PADME), an agent
framework that produces and exploits a graph-based representation of
procedures. Unlike prior work that relies on manual graph construction or
unstructured reasoning, PADME autonomously transforms procedural text into
executable graphs that capture task dependencies, decision points, and reusable
subroutines. Central to PADME is a two-phase methodology; Teach phase, which
focuses on systematic structuring, enrichment with executable logic of
procedures, followed by Execute phase, which enables dynamic execution in
response to real-time inputs and environment feedback. This separation ensures
quality assurance and scalability, allowing expert knowledge to be encoded once
and reliably reused across varying contexts. The graph representation also
provides an inductive bias that reduces error accumulation in long-horizon
reasoning, underscoring the importance of structured procedure modeling for
reliable agent-driven automation. Empirically, PADME achieves state-of-the-art
performance on four diverse benchmarks, including ALFWorld and ScienceWorld.
These results demonstrate that agents equipped with graph-based procedure
representations offer a powerful intermediate abstraction for robust and
generalizable execution.

</details>


### [217] [Evolution in Simulation: AI-Agent School with Dual Memory for High-Fidelity Educational Dynamics](https://arxiv.org/abs/2510.11290)
*Sheng Jin,Haoming Wang,Zhiqi Gao,Yongbo Yang,Bao Chunjia,Chengliang Wang*

Main category: cs.AI

TL;DR: 提出AI-Agent School系统，通过自进化机制模拟复杂教育动态，采用Zero-Exp策略和"经验-反思-优化"循环，使智能体在模拟学校场景中自主进化。


<details>
  <summary>Details</summary>
Motivation: 解决教学流程建模的碎片化问题，克服智能体在模拟多样化教育参与者时的性能限制，更准确地模拟真实学校中的师生互动和学习过程。

Method: 构建Zero-Exp策略，采用"经验-反思-优化"连续循环机制，基于包含经验和知识库的双重记忆基础，整合短期和长期记忆组件。

Result: 实验证实AAS能有效模拟复杂教育动态，培养高级智能体认知能力，生成高保真行为交互数据。

Conclusion: AAS为从"经验时代"迈向"模拟时代"提供了基础性垫脚石，通过生成高质量模拟数据推动教育系统研究。

Abstract: Large language models (LLMs) based Agents are increasingly pivotal in
simulating and understanding complex human systems and interactions. We propose
the AI-Agent School (AAS) system, built around a self-evolving mechanism that
leverages agents for simulating complex educational dynamics. Addressing the
fragmented issues in teaching process modeling and the limitations of agents
performance in simulating diverse educational participants, AAS constructs the
Zero-Exp strategy, employs a continuous "experience-reflection-optimization"
cycle, grounded in a dual memory base comprising experience and knowledge bases
and incorporating short-term and long-term memory components. Through this
mechanism, agents autonomously evolve via situated interactions within diverse
simulated school scenarios. This evolution enables agents to more accurately
model the nuanced, multi-faceted teacher-student engagements and underlying
learning processes found in physical schools. Experiment confirms that AAS can
effectively simulate intricate educational dynamics and is effective in
fostering advanced agent cognitive abilities, providing a foundational stepping
stone from the "Era of Experience" to the "Era of Simulation" by generating
high-fidelity behavioral and interaction data.

</details>


### [218] [Automated Skill Decomposition Meets Expert Ontologies: Bridging the Granularity Gap with LLMs](https://arxiv.org/abs/2510.11313)
*Le Ngoc Luyen,Marie-Hélène Abel*

Main category: cs.AI

TL;DR: 提出基于LLM的自动化技能分解方法，建立本体论评估框架，包含语义F1和层次感知F1两个指标，比较零样本和少样本提示策略。


<details>
  <summary>Details</summary>
Motivation: 需要标准化评估LLM在技能分解任务中的表现，确保输出与本体论结构一致。

Method: 使用ROME-ESCO-DecompSkill数据集，比较零样本和泄漏安全少样本提示策略，通过语义F1和层次感知F1评估输出质量。

Result: 零样本提供强基线，少样本在稳定表达和层次对齐方面表现更好，延迟分析显示少样本有时更快。

Conclusion: 该框架为开发本体论一致的技能分解系统提供了可复现的基础。

Abstract: This paper investigates automated skill decomposition using Large Language
Models (LLMs) and proposes a rigorous, ontology-grounded evaluation framework.
Our framework standardizes the pipeline from prompting and generation to
normalization and alignment with ontology nodes. To evaluate outputs, we
introduce two metrics: a semantic F1-score that uses optimal embedding-based
matching to assess content accuracy, and a hierarchy-aware F1-score that
credits structurally correct placements to assess granularity. We conduct
experiments on ROME-ESCO-DecompSkill, a curated subset of parents, comparing
two prompting strategies: zero-shot and leakage-safe few-shot with exemplars.
Across diverse LLMs, zero-shot offers a strong baseline, while few-shot
consistently stabilizes phrasing and granularity and improves hierarchy-aware
alignment. A latency analysis further shows that exemplar-guided prompts are
competitive - and sometimes faster - than unguided zero-shot due to more
schema-compliant completions. Together, the framework, benchmark, and metrics
provide a reproducible foundation for developing ontology-faithful skill
decomposition systems.

</details>


### [219] [AI-Driven anemia diagnosis: A review of advanced models and techniques](https://arxiv.org/abs/2510.11380)
*Abdullah Al Mahmud,Prangon Chowdhury,Mohammed Borhan Uddin,Khaled Eabne Delowar,Tausifur Rahman Talha,Bijoy Dewanjee*

Main category: cs.AI

TL;DR: 本文系统综述了机器学习和深度学习在贫血检测、分类和诊断中的最新进展，比较了不同模型的性能指标，并分析了这些模型在贫血检测中的优势和局限性。


<details>
  <summary>Details</summary>
Motivation: 贫血是一种影响全球数百万人的常见健康问题，准确及时的诊断对有效管理和治疗至关重要。近年来，人工智能技术在贫血诊断中的应用日益受到关注。

Method: 采用系统综述方法，分析机器学习和深度学习模型在贫血检测中的应用，比较不同模型在准确性、敏感性、特异性和精确度等性能指标上的表现。

Result: 综述发现AI模型在贫血检测中显示出良好的性能，但不同模型在各项指标上表现各异，需要综合考虑模型优势与局限性。

Conclusion: 通过分析性能指标可以评估贫血检测模型的优缺点，强调解决这些因素对提高诊断准确性的重要性，为未来研究提供方向。

Abstract: Anemia, a condition marked by insufficient levels of red blood cells or
hemoglobin, remains a widespread health issue affecting millions of individuals
globally. Accurate and timely diagnosis is essential for effective management
and treatment of anemia. In recent years, there has been a growing interest in
the use of artificial intelligence techniques, i.e., machine learning (ML) and
deep learning (DL) for the detection, classification, and diagnosis of anemia.
This paper provides a systematic review of the recent advancements in this
field, with a focus on various models applied to anemia detection. The review
also compares these models based on several performance metrics, including
accuracy, sensitivity, specificity, and precision. By analyzing these metrics,
the paper evaluates the strengths and limitation of discussed models in
detecting and classifying anemia, emphasizing the importance of addressing
these factors to improve diagnostic accuracy.

</details>


### [220] [From <Answer> to <Think>: Multidimensional Supervision of Reasoning Process for LLM Optimization](https://arxiv.org/abs/2510.11457)
*Beining Wang,Weihang Su,Hongtao Tian,Tao Yang,Yujia Zhou,Ting Yao,Qingyao Ai,Yiqun Liu*

Main category: cs.AI

TL;DR: 提出了维度级奖励模型(DRM)，通过置信度、相关性和连贯性三个维度评估推理过程质量，解决了传统结果监督强化学习和过程级奖励模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型多步推理能力提升方法存在缺陷：结果监督强化学习只奖励最终正确答案，容易传播错误推理且奖励信号稀疏；过程级奖励模型缺乏泛化性和可解释性，需要任务特定的推理过程分割。

Method: 提出维度级奖励模型(DRM)，从三个基本维度评估推理过程质量：置信度用于不确定性校准，相关性用于语义对齐，连贯性用于逻辑一致性。这些维度无需真实答案即可进行可解释的评估。

Result: 实验结果表明DRM提供有效的监督信号，指导大语言模型优化并增强其推理能力。DRM监督训练在数学、问答、代码执行和谜题等任务上均取得一致增益，包括分布内和分布外任务。

Conclusion: 多维度的推理过程监督可以提升大语言模型的泛化推理能力，超越训练分布范围。

Abstract: Improving the multi-step reasoning ability of Large Language Models (LLMs) is
a critical yet challenging task. The dominant paradigm, outcome-supervised
reinforcement learning (RLVR), rewards only correct final answers, often
propagating flawed reasoning and suffering from sparse reward signals. While
process-level reward models (PRMs) provide denser, step-by-step feedback, they
lack generalizability and interpretability, requiring task-specific
segmentation of the reasoning process. To this end, we propose the
Dimension-level Reward Model (DRM), a new supervision framework that bridges
the gap between these two approaches. DRM evaluates the quality of a reasoning
process along three fundamental, complementary, and interpretable dimensions:
Confidence for uncertainty calibration, Relevance for semantic alignment, and
Coherence for logical consistency. Together, these dimensions capture aspects
beyond final answer correctness and enable interpretable assessment without
requiring ground truth answers. Experimental results show that DRM provides
effective supervision signals, guides the optimization of LLMs and enhances
their reasoning ability. In particular, DRM-supervised training achieves
consistent gains on both in-distribution and out-of-distribution open-domain
tasks, including mathematics, question answering, code execution, and puzzles.
Our findings demonstrate that multidimensional supervision of the reasoning
process can improve the generalized reasoning ability of LLMs beyond the
training distribution.

</details>


### [221] [Unifying Deductive and Abductive Reasoning in Knowledge Graphs with Masked Diffusion Model](https://arxiv.org/abs/2510.11462)
*Yisen Gao,Jiaxin Bai,Yi Huang,Xingcheng Fu,Qingyun Sun,Yangqiu Song*

Main category: cs.AI

TL;DR: DARK是一个统一框架，用于知识图谱中的演绎和溯因推理，通过掩码扩散模型和自反去噪过程实现双向推理，在多项基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将演绎推理（检索满足复杂逻辑查询的实体）和溯因推理（从观察生成合理逻辑假设）分开处理，但两者具有明显的协同潜力，需要统一框架来整合这两种推理范式。

Method: 提出DARK框架：1）自反去噪过程，在溯因推理中迭代生成和验证候选假设；2）逻辑探索强化学习方法，同时掩码查询和结论，探索新颖推理组合；3）基于掩码扩散模型捕获查询与结论的双向关系。

Result: 在多个基准知识图谱上的广泛实验表明，DARK在演绎和溯因推理任务上都达到了最先进的性能，证明了统一方法的显著优势。

Conclusion: DARK成功地将演绎和溯因推理统一在一个框架中，通过双向建模和迭代验证机制，显著提升了知识图谱推理能力，为复杂推理任务提供了有效解决方案。

Abstract: Deductive and abductive reasoning are two critical paradigms for analyzing
knowledge graphs, enabling applications from financial query answering to
scientific discovery. Deductive reasoning on knowledge graphs usually involves
retrieving entities that satisfy a complex logical query, while abductive
reasoning generates plausible logical hypotheses from observations. Despite
their clear synergistic potential, where deduction can validate hypotheses and
abduction can uncover deeper logical patterns, existing methods address them in
isolation. To bridge this gap, we propose DARK, a unified framework for
Deductive and Abductive Reasoning in Knowledge graphs. As a masked diffusion
model capable of capturing the bidirectional relationship between queries and
conclusions, DARK has two key innovations. First, to better leverage deduction
for hypothesis refinement during abductive reasoning, we introduce a
self-reflective denoising process that iteratively generates and validates
candidate hypotheses against the observed conclusion. Second, to discover
richer logical associations, we propose a logic-exploration reinforcement
learning approach that simultaneously masks queries and conclusions, enabling
the model to explore novel reasoning compositions. Extensive experiments on
multiple benchmark knowledge graphs show that DARK achieves state-of-the-art
performance on both deductive and abductive reasoning tasks, demonstrating the
significant benefits of our unified approach.

</details>


### [222] [Zero Data Retention in LLM-based Enterprise AI Assistants: A Comparative Study of Market Leading Agentic AI Products](https://arxiv.org/abs/2510.11558)
*Komal Gupta,Aditya Shrivastava*

Main category: cs.AI

TL;DR: 本文探讨了企业AI助手中的零数据保留政策，分析了Salesforce和Microsoft等公司在实现数据隐私和合规性方面的技术架构。


<details>
  <summary>Details</summary>
Motivation: 随着企业AI助手的普及，保护私人数据和确保合规性成为优先事项，特别是在医疗和金融等敏感行业。

Method: 通过研究Salesforce AgentForce和Microsoft Copilot的技术架构，分析它们如何实现零数据保留政策，并探讨与大型语言模型服务提供商（如OpenAI、Anthropic和Meta）的集成。

Result: 研究发现，不同的公司采用不同的技术架构来支持零数据保留政策，这些架构在实现数据隐私和合规性方面各有优劣。

Conclusion: 零数据保留政策在企业AI助手中的实施是可行的，但需要在架构、合规性和可用性之间进行权衡。

Abstract: Governance of data, compliance, and business privacy matters, particularly
for healthcare and finance businesses. Since the recent emergence of AI
enterprise AI assistants enhancing business productivity, safeguarding private
data and compliance is now a priority. With the implementation of AI assistants
across the enterprise, the zero data retention can be achieved by implementing
zero data retention policies by Large Language Model businesses like Open AI
and Anthropic and Meta. In this work, we explore zero data retention policies
for the Enterprise apps of large language models (LLMs). Our key contribution
is defining the architectural, compliance, and usability trade-offs of such
systems in parallel. In this research work, we examine the development of
commercial AI assistants with two industry leaders and market titans in this
arena - Salesforce and Microsoft. Both of these companies used distinct
technical architecture to support zero data retention policies. Salesforce
AgentForce and Microsoft Copilot are among the leading AI assistants providing
much-needed push to business productivity in customer care. The purpose of this
paper is to analyze the technical architecture and deployment of zero data
retention policy by consuming applications as well as big language models
service providers like Open Ai, Anthropic, and Meta.

</details>


### [223] [Analyzing and Internalizing Complex Policy Documents for LLM Agents](https://arxiv.org/abs/2510.11588)
*Jiateng Liu,Zhenhailong Wang,Xiaojiang Huang,Yingjie Li,Xing Fan,Xiang Li,Chenlei Guo,Ruhi Sarikaya,Heng Ji*

Main category: cs.AI

TL;DR: 提出了CC-Gen基准测试生成器和CAP-CPT方法，用于解决LLM代理系统中政策文档内部化问题，通过可控复杂度评估和分类感知的持续预训练，显著减少提示长度并提升性能。


<details>
  <summary>Details</summary>
Motivation: LLM代理系统中的政策文档随着需求增长而快速扩展，导致高计算开销，需要开发将政策文档嵌入模型先验的内部化方法，同时保持性能。

Method: 引入CC-Gen基准测试生成器，提供四个可控复杂度级别；提出CAP-CPT方法，通过解析政策文档提取关键规范，分为事实性、行为性和条件性类别，并使用自回归预训练损失进行内部化。

Result: CAP-CPT在所有设置中均优于SFT基线，在Qwen-3-32B上提升达41%和22%，在CC-Gen上实现97.3%的提示长度减少，并在tau-Bench上以最少的SFT数据进一步改进性能。

Conclusion: CAP-CPT方法有效减轻了数据和推理负担，通过分类感知的持续预训练成功实现了政策信息的内部化，显著提升了代理系统处理复杂政策的能力。

Abstract: Large Language Model (LLM)-based agentic systems rely on in-context policy
documents encoding diverse business rules. As requirements grow, these
documents expand rapidly, causing high computational overhead. This motivates
developing internalization methods that embed policy documents into model
priors while preserving performance. Prior prompt compression work targets
generic prompts, but agentic policy documents span multiple complexity levels
and require deeper reasoning, making internalization harder. We introduce
CC-Gen, an agentic benchmark generator with Controllable Complexity across four
levels, enabling systematic evaluation of agents' ability to handle complexity
and offering a unified framework for assessing policy internalization. Our
analysis shows that complex policy specifications governing workflows pose
major reasoning challenges. Supporting internalization with gold user agent
interaction trajectories containing chain-of-thought (CoT) annotations via
supervised fine-tuning (SFT) is data-intensive and degrades sharply as policy
complexity increases. To mitigate data and reasoning burdens, we propose
Category-Aware Policy Continued Pretraining (CAP-CPT). Our automated pipeline
parses policy documents to extract key specifications, grouping them into
factual, behavioral, and conditional categories, and isolating complex
conditions that drive workflow complexity. This guides targeted data synthesis
and enables agents to internalize policy information through an autoregressive
pretraining loss. Experiments show CAP-CPT improves SFT baselines in all
settings, with up to 41% and 22% gains on Qwen-3-32B, achieving 97.3% prompt
length reduction on CC-Gen and further enhancing tau-Bench with minimal SFT
data.

</details>


### [224] [Reproducibility: The New Frontier in AI Governance](https://arxiv.org/abs/2510.11595)
*Israel Mason-Williams,Gabryel Mason-Williams*

Main category: cs.AI

TL;DR: 本文主张AI研究应采用更严格的复现性指南，以帮助治理工作并改善对AI风险格局的共识。


<details>
  <summary>Details</summary>
Motivation: 当前AI政策制定者面临信息环境中信号噪声比过低的问题，这助长了监管捕获，并导致在应优先治理哪些风险方面存在深刻分歧和不确定性。

Method: 通过借鉴其他科学领域的危机经验，评估AI研究中即将到来的复现性危机，并评论采用预注册、提高统计能力和负面结果发布等复现性协议如何促进有效的AI治理。

Result: 分析表明，当前AI出版速度与缺乏严格科学标准相结合，削弱了政策制定者制定有意义的政策和治理协议的能力。

Conclusion: 虽然AI治理必须具有反应性，但政策制定者和政府应将复现性协议视为治理工具箱中的核心工具，并要求AI研究达到更高标准。

Abstract: AI policymakers are responsible for delivering effective governance
mechanisms that can provide safe, aligned and trustworthy AI development.
However, the information environment offered to policymakers is characterised
by an unnecessarily low Signal-To-Noise Ratio, favouring regulatory capture and
creating deep uncertainty and divides on which risks should be prioritised from
a governance perspective. We posit that the current publication speeds in AI
combined with the lack of strong scientific standards, via weak reproducibility
protocols, effectively erodes the power of policymakers to enact meaningful
policy and governance protocols. Our paper outlines how AI research could adopt
stricter reproducibility guidelines to assist governance endeavours and improve
consensus on the AI risk landscape. We evaluate the forthcoming reproducibility
crisis within AI research through the lens of crises in other scientific
domains; providing a commentary on how adopting preregistration, increased
statistical power and negative result publication reproducibility protocols can
enable effective AI governance. While we maintain that AI governance must be
reactive due to AI's significant societal implications we argue that
policymakers and governments must consider reproducibility protocols as a core
tool in the governance arsenal and demand higher standards for AI research.
Code to replicate data and figures:
https://github.com/IFMW01/reproducibility-the-new-frontier-in-ai-governance

</details>


### [225] [Explainability, risk modeling, and segmentation based customer churn analytics for personalized retention in e-commerce](https://arxiv.org/abs/2510.11604)
*Sanjula De Alwis,Indrajith Ekanayake*

Main category: cs.AI

TL;DR: 提出一个三组件框架，结合可解释AI、生存分析和RFM分析，从预测客户流失转向设计个性化保留策略。


<details>
  <summary>Details</summary>
Motivation: 传统流失模型作为黑盒运作，限制了洞察流失原因、干预时机和高风险客户识别，需要转向基于可解释证据的个性化保留策略。

Method: 集成可解释AI量化特征贡献、生存分析建模流失风险时间、RFM分析按交易行为细分客户的三组件框架。

Result: 该框架能够归因流失驱动因素、估计干预窗口、优先处理目标细分市场，支持减少流失和增强客户忠诚度的策略。

Conclusion: 该研究推进了从单纯预测到设计基于可解释证据的个性化保留策略的转变，为降低客户流失和加强客户忠诚度提供支持。

Abstract: In online retail, customer acquisition typically incurs higher costs than
customer retention, motivating firms to invest in churn analytics. However,
many contemporary churn models operate as opaque black boxes, limiting insight
into the determinants of attrition, the timing of retention opportunities, and
the identification of high-risk customer segments. Accordingly, the emphasis
should shift from prediction alone to the design of personalized retention
strategies grounded in interpretable evidence. This study advances a
three-component framework that integrates explainable AI to quantify feature
contributions, survival analysis to model time-to-event churn risk, and RFM
profiling to segment customers by transactional behaviour. In combination,
these methods enable the attribution of churn drivers, estimation of
intervention windows, and prioritization of segments for targeted actions,
thereby supporting strategies that reduce attrition and strengthen customer
loyalty.

</details>


### [226] [ParaCook: On Time-Efficient Planning for Multi-Agent Systems](https://arxiv.org/abs/2510.11608)
*Shiqi Zhang,Xinbei Ma,Yunqing Xu,Zouying Cao,Pengrui Lu,Haobo Yuan,Tiancheng Shen,Zhuosheng Zhang,Hai Zhao,Ming-Hsuan Yang*

Main category: cs.AI

TL;DR: ParaCook是一个用于评估时间效率协作规划的基准，基于简化版Overcooked游戏环境，专注于多智能体系统的并行和异步操作规划能力。


<details>
  <summary>Details</summary>
Motivation: 现有智能体基准主要关注任务完成度，但忽略了并行和异步操作中的时间效率问题，需要专门评估多智能体系统在时间效率方面的规划能力。

Method: 基于Overcooked游戏设计ParaCook环境，提供简化的动作空间来专注于战略并行规划的核心挑战，通过烹饪任务实例化多智能体交互规划。

Result: 对最先进LLM的评估显示，现有方法产生次优规划，在并行动作或协调方面存在困难，但在抽象任务上LLM表现出并行优化的潜力。

Conclusion: ParaCook提供了一个可扩展的评估框架，为开发和评估时间效率感知的多智能体规划奠定了基础。

Abstract: Large Language Models (LLMs) exhibit strong reasoning abilities for planning
long-horizon, real-world tasks, yet existing agent benchmarks focus on task
completion while neglecting time efficiency in parallel and asynchronous
operations. To address this, we present ParaCook, a benchmark for
time-efficient collaborative planning. Inspired by the Overcooked game,
ParaCook provides an environment for various challenging interaction planning
of multi-agent systems that are instantiated as cooking tasks, with a
simplified action space to isolate the core challenge of strategic parallel
planning. Through a comprehensive evaluation of state-of-the-art LLMs, we find
that current approaches achieve suboptimal plans, which struggle with parallel
actions or coordination. Our analysis also reveals LLMs' potential on abstract
tasks where they can focus on high-level parallel optimization. ParaCook
provides a scalable evaluation framework with adjustable complexity,
establishing a foundation for developing and assessing time efficiency-aware
multi-agent planning. The code and data are available at
https://github.com/zsq259/ParaCook.

</details>


### [227] [SR-Scientist: Scientific Equation Discovery With Agentic AI](https://arxiv.org/abs/2510.11661)
*Shijie Xia,Yuhan Sun,Pengfei Liu*

Main category: cs.AI

TL;DR: SR-Scientist框架将LLM从简单的方程提议者提升为自主AI科学家，能够编写代码分析数据、实现方程、提交评估，并根据实验反馈优化方程，在四个科学领域的数据集上比基线方法提升6%至35%。


<details>
  <summary>Details</summary>
Motivation: 当前方法通常将LLM限制为遗传编程等搜索算法中的方程提议者角色，未能充分利用LLM的科学知识。

Method: 将代码解释器包装为数据分析和方程评估工具集，让智能体在最小人工干预下长期使用这些工具优化方程，并开发端到端强化学习框架增强智能体能力。

Result: 在四个科学领域的数据集上，SR-Scientist比基线方法绝对提升6%至35%，表现出对噪声的鲁棒性、发现方程在域外数据的泛化能力以及符号准确性。

Conclusion: SR-Scientist框架成功将LLM提升为自主AI科学家，在科学方程发现任务中显著优于现有方法，并展现出良好的鲁棒性和泛化能力。

Abstract: Recently, Large Language Models (LLMs) have been applied to scientific
equation discovery, leveraging their embedded scientific knowledge for
hypothesis generation. However, current methods typically confine LLMs to the
role of an equation proposer within search algorithms like genetic programming.
In this paper, we present SR-Scientist, a framework that elevates the LLM from
a simple equation proposer to an autonomous AI scientist that writes code to
analyze data, implements the equation as code, submits it for evaluation, and
optimizes the equation based on experimental feedback. Specifically, we wrap
the code interpreter into a set of tools for data analysis and equation
evaluation. The agent is instructed to optimize the equation by utilizing these
tools over a long horizon with minimal human-defined pipelines. Empirical
results show that SR-Scientist outperforms baseline methods by an absolute
margin of 6% to 35% on datasets covering four science disciplines.
Additionally, we demonstrate our method's robustness to noise, the
generalization of the discovered equations to out-of-domain data, and their
symbolic accuracy. Furthermore, we develop an end-to-end reinforcement learning
framework to enhance the agent's capabilities.

</details>


### [228] [Operand Quant: A Single-Agent Architecture for Autonomous Machine Learning Engineering](https://arxiv.org/abs/2510.11694)
*Arjun Sahney,Ram Gorthi,Cezary Łastowski,Javier Vega*

Main category: cs.AI

TL;DR: Operand Quant是一个基于IDE的单智能体架构，用于自主机器学习工程，在MLE-Benchmark上创造了新的SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 传统的多智能体编排框架存在复杂性，作者希望通过单一智能体整合所有MLE生命周期阶段来简化架构并提升性能。

Method: 采用单智能体架构，在受控IDE环境中线性、非阻塞地执行探索、建模、实验和部署等所有MLE生命周期阶段。

Result: 在MLE-Benchmark（2025）上获得0.3956±0.0565的整体奖牌率，在75个问题中创下所有评估系统的最高记录性能。

Conclusion: 线性、非阻塞的单智能体在受控IDE环境中能够超越多智能体和编排系统，证明简化架构的有效性。

Abstract: We present Operand Quant, a single-agent, IDE-based architecture for
autonomous machine learning engineering (MLE). Operand Quant departs from
conventional multi-agent orchestration frameworks by consolidating all MLE
lifecycle stages -- exploration, modeling, experimentation, and deployment --
within a single, context-aware agent. On the MLE-Benchmark (2025), Operand
Quant achieved a new state-of-the-art (SOTA) result, with an overall medal rate
of 0.3956 +/- 0.0565 across 75 problems -- the highest recorded performance
among all evaluated systems to date. The architecture demonstrates that a
linear, non-blocking agent, operating autonomously within a controlled IDE
environment, can outperform multi-agent and orchestrated systems under
identical constraints.

</details>
