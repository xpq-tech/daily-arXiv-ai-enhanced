<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 114]
- [cs.AI](#cs.AI) [Total: 83]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A Multi-lingual Dataset of Classified Paragraphs from Open Access Scientific Publications](https://arxiv.org/abs/2510.21762)
*Eric Jeangirard*

Main category: cs.CL

TL;DR: 该论文提出了一个包含83.3万段落的科学文献数据集，涵盖致谢、数据提及、软件/代码提及和临床试验提及四种类别，主要用于文本分类和命名实体识别模型的训练。


<details>
  <summary>Details</summary>
Motivation: 为科学文献挖掘提供高质量的标注数据集，支持文本分类和命名实体识别系统的开发，促进科学文献的自动化处理和分析。

Method: 从法国开放科学监测语料库中提取段落，使用GROBID进行处理，并通过fastText进行语言识别，利用OpenAlex标注科学领域信息。

Result: 创建了一个包含83.3万段落的多语言数据集，涵盖英语、法语及其他欧洲语言，每个段落都标注了语言和科学领域信息，数据集已在HuggingFace平台公开。

Conclusion: 该数据集为科学文献挖掘研究提供了有价值的资源，支持文本分类和命名实体识别任务的模型训练，数据集采用CC-BY许可开放共享。

Abstract: We present a dataset of 833k paragraphs extracted from CC-BY licensed
scientific publications, classified into four categories: acknowledgments, data
mentions, software/code mentions, and clinical trial mentions. The paragraphs
are primarily in English and French, with additional European languages
represented. Each paragraph is annotated with language identification (using
fastText) and scientific domain (from OpenAlex). This dataset, derived from the
French Open Science Monitor corpus and processed using GROBID, enables training
of text classification models and development of named entity recognition
systems for scientific literature mining. The dataset is publicly available on
HuggingFace https://doi.org/10.57967/hf/6679 under a CC-BY license.

</details>


### [2] [Policy Optimization Prefers The Path of Least Resistance](https://arxiv.org/abs/2510.21853)
*Debdeep Sanyal,Aakash Sen Sharma,Dhruv Kumar,Saurabh Deshpande,Murari Mandal*

Main category: cs.CL

TL;DR: 策略优化在开放式的思维链结构中会倾向于选择最简单路径，丢弃显式推理，直接输出答案，即使复杂格式被赋予更高奖励权重


<details>
  <summary>Details</summary>
Motivation: 研究当严格约束被放松为开放式思维链结构时，策略优化的行为表现，填补这一研究空白

Method: 通过一系列受控实验和奖励分解实验，分析策略优化在不同模型和算法中的行为模式

Result: 策略优化始终遵循最小阻力路径，学习丢弃显式推理，即使复杂格式奖励权重高达4倍；PO系统性地优先优化最简单的奖励组件

Conclusion: 给予策略发散自由是双刃剑：既能发现高奖励捷径，也创造了博弈奖励函数最简单方面的强大动机，这对对齐中的奖励黑客构成关键挑战

Abstract: Policy optimization (PO) algorithms are used to refine Large Language Models
for complex, multi-step reasoning. Current state-of-the-art pipelines enforce a
strict think-then-answer format to elicit chain-of-thought (CoT); however, the
behavior of PO when these rigid constraints are relaxed into an open-ended CoT
structure remains an under-studied question. We investigate this gap with an
extensive suite of controlled experiments and identify a consistent principle:
\textit{policy optimization consistently follows the path of least resistance}.
When afforded the flexibility to interleave reasoning and response, policy
optimization consistently learns to discard explicit reasoning, causing the
policy to degenerate to a direct \texttt{<answer>}-only format. This outcome
holds true across various models and algorithms. We find that this collapse in
format is persistent even when the complex \texttt{<think><answer>} format is
assigned up to 4x larger reward weights. We formalize this principle through a
series of controlled reward decomposition experiments, demonstrating a clear
hierarchy: PO systematically optimizes for the simplest reward component first,
a preference that holds even when faced with mutually exclusive choices or
strong incentives for more complex behaviors. Finally, we show that successful
convergence on the high-reward shortcut is not a low-effort drift but is driven
by the optimization process that requires the KL-regularized policy to have
sufficient freedom to make a significant shift from its initial prior. Our
findings reveal that granting policies the freedom to diverge is a double-edged
sword: while necessary for discovering high-reward shortcuts, it also creates a
powerful incentive to game the simplest aspects of the reward function, posing
a critical challenge for reward hacking under alignment.

</details>


### [3] [Language Ranker: A Lightweight Ranking framework for LLM Decoding](https://arxiv.org/abs/2510.21883)
*Chenheng Zhang,Tianqi Du,Jizhe Zhang,Mingqing Xiao,Yifei Wang,Yisen Wang,Zhouchen Lin*

Main category: cs.CL

TL;DR: 提出Language Ranker框架，将LLM解码过程重新概念化为推荐系统中的排序阶段，通过轻量级模块重排候选响应，在保持性能的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统LLM研究主要关注输出分布优化，而忽视了解码过程的重要性。现有基于奖励模型的方法计算成本高且适用性有限，存在冗余问题。

Method: 借鉴推荐系统思想，将解码过程视为排序阶段，引入轻量级重排模块，使用基础模型提取的特征对候选响应进行重排。

Result: 在多种任务上实验表明，该方法性能与大规模奖励模型相当，仅需<0.5M额外参数，显著降低训练和推理阶段的计算开销。

Conclusion: 该方法高效且有效，展示了充分释放LLM潜力的潜力，为解码过程优化提供了新思路。

Abstract: Conventional research on large language models (LLMs) has primarily focused
on refining output distributions, while paying less attention to the decoding
process that transforms these distributions into final responses. Recent
advances, such as scaling the computation of inference time with reward models,
have underscored the importance of decoding, but these methods often suffer
from high computational costs and limited applicability. In this paper, we
revisit LLM generation through the lens of recommender systems, conceptualizing
the decoding process as analogous to the ranking stage in recommendation
pipelines. From this perspective, we observe that both traditional decoding
methods and reward models exhibit clear limitations such as redundancy.
Motivated by this insight, we propose Language Ranker, a novel framework that
introduces a lightweight module to rerank candidate responses using features
extracted by the base model. Experiments across a wide range of tasks show that
Language Ranker achieves performance comparable to large-scale reward models,
while requiring only <0.5M additional parameters, significantly reducing the
computational overhead during both training and inference stages. This
highlights the efficiency and effectiveness of our method, showcasing its
potential to fully unlock the capabilities of LLMs.

</details>


### [4] [Framework for Machine Evaluation of Reasoning Completeness in Large Language Models For Classification Tasks](https://arxiv.org/abs/2510.21884)
*Avinash Patil*

Main category: cs.CL

TL;DR: RACE框架评估LLM生成解释与逻辑回归特征重要性分数的对齐度，发现正确预测更覆盖支持特征，错误预测更覆盖矛盾特征，揭示了LLM解释的忠实性模式。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在敏感领域的应用增加，对透明和可解释AI的需求日益增长。LLM能够生成自然语言解释，但这些解释是否真实反映决策背后的预测信号尚不明确。

Method: 提出RACE框架，在四个文本分类数据集上比较LLM解释与逻辑回归基线的特征重要性分数，使用词元感知、精确字符串和编辑距离匹配技术从多粒度层面捕捉对齐度。

Result: 实证结果显示一致的不对称性：正确预测更覆盖支持特征，错误预测更覆盖矛盾特征。编辑距离匹配发现释义重叠，在保持不对称性的同时提高了覆盖率。

Conclusion: LLM解释结合了表面级和灵活的证据重用，但在错误情况下也会放大误导性线索。RACE为评估神经语言模型推理完整性提供了量化基础。

Abstract: The growing adoption of machine learning (ML) in sensitive domains has
heightened the demand for transparent and interpretable artificial
intelligence. Large Language Models (LLMs) are increasingly capable of
producing natural language explanations, yet it remains unclear whether these
rationales faithfully capture the predictive signals that underlie decisions.
This paper introduces RACE-Reasoning Alignment for Completeness of
Explanations, a systematic framework to evaluate the alignment between
LLM-generated explanations and interpretable feature importance scores derived
from a logistic regression baseline. We analyze four widely used text
classification datasets-WIKI ONTOLOGY, AG NEWS, IMDB, and GOEMOTIONS-and
compare LLM rationales against top-ranked supporting and contradicting lexical
features. To capture alignment at multiple levels of granularity, RACE
implements token-aware, exact string, and edit-distance matching techniques.
Empirical results reveal a consistent asymmetry: correct predictions exhibit
higher coverage of supporting features, while incorrect predictions are
associated with elevated coverage of contradicting features. Edit-distance
matching further uncovers paraphrastic overlaps, boosting coverage while
preserving this asymmetry. These findings demonstrate that LLM rationales
combine both surface-level and flexible evidence reuse, yet can also amplify
misleading cues in error cases. RACE provides new insights into the
faithfulness of LLM explanations and establishes a quantitative basis for
evaluating reasoning completeness in neural language models.

</details>


### [5] [Preventing Catastrophic Forgetting: Behavior-Aware Sampling for Safer Language Model Fine-Tuning](https://arxiv.org/abs/2510.21885)
*Anh Pham,Mihir Thalanki,Michael Sun,Aditya Chaloo,Ankita Gupta,Tian Xia,Aditya Mate,Ehimwenma Nosakhare,Soundararajan Srinivasan*

Main category: cs.CL

TL;DR: 提出行为感知采样框架，通过基于指令-响应行为和语义多样性的安全示例选择，有效缓解大语言模型在微调时的灾难性遗忘问题，仅需0.5%额外训练数据即可将有害输出减少41%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在良性数据微调时经常失去之前对齐的安全行为，即灾难性遗忘现象。虽然添加随机安全示例可以缓解此问题，但尚不清楚哪些示例最有效。

Method: 提出行为感知采样框架，基于两个互补因素选择安全示例：指令-响应行为（如拒绝vs顺从）和跨伤害类别的语义多样性。

Result: 系统性评估显示，该方法显著减少有害输出同时保持帮助性，仅用0.5%额外训练数据即可实现有害性降低41%。

Conclusion: 结果表明，有针对性的数据选择可以大规模提高微调的安全性和效率。

Abstract: Large language models often lose previously aligned safety behaviors when
fine-tuned on benign data, a phenomenon known as catastrophic forgetting. Prior
work shows that adding random safety examples can mitigate this effect, but it
remains unclear which examples are most effective. We propose a behavior-aware
sampling framework that selects safety examples based on two complementary
factors: instruction-response behavior (e.g., refusal versus compliance) and
semantic diversity across harm categories. Systematic evaluation shows that
this approach substantially reduces harmful outputs while maintaining
helpfulness, achieving up to a 41% reduction in harmfulness with only 0.5%
additional training data. These results highlight how targeted data selection
can improve the safety and efficiency of fine-tuning at scale.

</details>


### [6] [Embedding Trust: Semantic Isotropy Predicts Nonfactuality in Long-Form Text Generation](https://arxiv.org/abs/2510.21891)
*Dhrupad Bhardwaj,Julia Kempe,Tim G. J. Rudner*

Main category: cs.CL

TL;DR: 提出了一种基于语义各向同性的方法来评估LLM长文本响应的可信度，该方法通过计算嵌入向量在单位球面上的角度分散度来预测事实一致性，无需标注数据或微调。


<details>
  <summary>Details</summary>
Motivation: 在高风险应用领域部署LLM时，需要可靠且计算成本低的方法来评估长文本响应的可信度，而现有的逐条事实核查方法计算成本高且脆弱。

Method: 生成多个长文本响应，将其嵌入到向量空间，然后通过计算嵌入向量在单位球面上的角度分散度来估计语义各向同性水平。

Result: 更高的语义各向同性（即更大的嵌入分散度）可靠地表明样本间的事实一致性较低。该方法在多个领域都优于现有方法，仅需少量样本即可预测长文本响应的非事实性。

Conclusion: 该方法提供了一种实用、低成本的信任评估方法，可以集成到实际的LLM工作流程中，无需标注数据、微调或超参数选择，适用于开放或封闭权重的嵌入模型。

Abstract: To deploy large language models (LLMs) in high-stakes application domains
that require substantively accurate responses to open-ended prompts, we need
reliable, computationally inexpensive methods that assess the trustworthiness
of long-form responses generated by LLMs. However, existing approaches often
rely on claim-by-claim fact-checking, which is computationally expensive and
brittle in long-form responses to open-ended prompts. In this work, we
introduce semantic isotropy -- the degree of uniformity across normalized text
embeddings on the unit sphere -- and use it to assess the trustworthiness of
long-form responses generated by LLMs. To do so, we generate several long-form
responses, embed them, and estimate the level of semantic isotropy of these
responses as the angular dispersion of the embeddings on the unit sphere. We
find that higher semantic isotropy -- that is, greater embedding dispersion --
reliably signals lower factual consistency across samples. Our approach
requires no labeled data, no fine-tuning, and no hyperparameter selection, and
can be used with open- or closed-weight embedding models. Across multiple
domains, our method consistently outperforms existing approaches in predicting
nonfactuality in long-form responses using only a handful of samples --
offering a practical, low-cost approach for integrating trust assessment into
real-world LLM workflows.

</details>


### [7] [Understanding Network Behaviors through Natural Language Question-Answering](https://arxiv.org/abs/2510.21894)
*Mingzhe Xing,Chang Tian,Jianan Zhang,Lichen Pan,Peipei Liu,Zhaoteng Yan,Yinliang Yue*

Main category: cs.CL

TL;DR: NetMind是一个使用自然语言查询网络行为的新框架，通过树状配置分块、统一事实图和混合语言设计来解决LLM在网络配置分析中的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统网络配置分析方法学习曲线陡峭且灵活性有限，而自然语言接口更易用。LLM虽具备网络概念知识和推理能力，但在处理长配置、异构设备和复杂拓扑时仍面临挑战。

Method: 采用树状配置分块策略保持语义连贯性；构建统一事实图作为中间表示来规范化厂商特定配置；设计混合命令式-声明式语言减轻LLM推理负担。

Result: 实验表明NetMind实现了准确且可扩展的网络行为理解，优于现有基线方法。

Conclusion: NetMind通过创新的配置处理、统一表示和混合语言设计，有效解决了LLM在网络配置分析中的关键挑战，为自然语言网络查询提供了可行方案。

Abstract: Modern large-scale networks introduce significant complexity in understanding
network behaviors, increasing the risk of misconfiguration. Prior work proposed
to understand network behaviors by mining network configurations, typically
relying on domain-specific languages interfaced with formal models. While
effective, they suffer from a steep learning curve and limited flexibility. In
contrast, natural language (NL) offers a more accessible and interpretable
interface, motivating recent research on NL-guided network behavior
understanding. Recent advances in large language models (LLMs) further enhance
this direction, leveraging their extensive prior knowledge of network concepts
and strong reasoning capabilities. However, three key challenges remain: 1)
numerous router devices with lengthy configuration files challenge LLM's
long-context understanding ability; 2) heterogeneity across devices and
protocols impedes scalability; and 3) complex network topologies and protocols
demand advanced reasoning abilities beyond the current capabilities of LLMs. To
tackle the above challenges, we propose NetMind, a novel framework for querying
networks using NL. Our approach introduces a tree-based configuration chunking
strategy to preserve semantic coherence while enabling efficient partitioning.
We then construct a unified fact graph as an intermediate representation to
normalize vendor-specific configurations. Finally, we design a hybrid
imperative-declarative language to reduce the reasoning burden on LLMs and
enhance precision. We contribute a benchmark consisting of NL question-answer
pairs paired with network configurations. Experiments demonstrate that NetMind
achieves accurate and scalable network behavior understanding, outperforming
existing baselines.

</details>


### [8] [Deep Literature Survey Automation with an Iterative Workflow](https://arxiv.org/abs/2510.21900)
*Hongbo Zhang,Han Cui,Yidong Wang,Yijian Tian,Qi Guo,Cunxiang Wang,Jian Wu,Chiyu Song,Yue Zhang*

Main category: cs.CL

TL;DR: 提出IterSurvey框架，通过迭代式大纲生成和论文卡片机制，改进自动文献综述生成的质量，在内容覆盖、结构连贯性和引用质量方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动文献综述系统采用一次性检索和静态大纲生成，导致检索噪声大、结构碎片化和上下文过载，限制了综述质量。受人类研究者迭代阅读过程的启发，需要更智能的生成方法。

Method: 基于循环大纲生成的框架，规划代理增量检索、阅读和更新大纲；设计论文卡片提炼每篇论文的贡献、方法和发现；引入审查-精炼循环和可视化增强来改进文本流并整合多模态元素。

Result: 在既有和新兴主题上的实验表明，IterSurvey在内容覆盖、结构连贯性和引用质量方面显著优于最先进的基线方法，产生更易访问和组织更好的综述。

Conclusion: IterSurvey通过迭代式大纲生成和论文卡片机制，有效提升了自动文献综述的质量，并引入Survey-Arena基准提供更可靠的评估。

Abstract: Automatic literature survey generation has attracted increasing attention,
yet most existing systems follow a one-shot paradigm, where a large set of
papers is retrieved at once and a static outline is generated before drafting.
This design often leads to noisy retrieval, fragmented structures, and context
overload, ultimately limiting survey quality. Inspired by the iterative reading
process of human researchers, we propose \ours, a framework based on recurrent
outline generation, in which a planning agent incrementally retrieves, reads,
and updates the outline to ensure both exploration and coherence. To provide
faithful paper-level grounding, we design paper cards that distill each paper
into its contributions, methods, and findings, and introduce a
review-and-refine loop with visualization enhancement to improve textual flow
and integrate multimodal elements such as figures and tables. Experiments on
both established and emerging topics show that \ours\ substantially outperforms
state-of-the-art baselines in content coverage, structural coherence, and
citation quality, while producing more accessible and better-organized surveys.
To provide a more reliable assessment of such improvements, we further
introduce Survey-Arena, a pairwise benchmark that complements absolute scoring
and more clearly positions machine-generated surveys relative to human-written
ones. The code is available at
https://github.com/HancCui/IterSurvey\_Autosurveyv2.

</details>


### [9] [Explaining and Mitigating Crosslingual Tokenizer Inequities](https://arxiv.org/abs/2510.21909)
*Catherine Arnett,Tyler A. Chang,Stella Biderman,Benjamin K. Bergen*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The number of tokens it takes to encode parallel text in different languages
is known to vary. These disparities are called token premiums. Having high
token premiums leads to less throughput during training and increases costs at
inference. In this paper, we show that even after controlling for dataset size,
vocabulary size, and data content, monolingual tokenizers exhibit a wide range
of token premiums across languages. To understand the cross-linguistic
differences that cause these token premiums, we train a suite of approximately
7,000 comparable monolingual tokenizers for 97 languages, manipulating
tokenization algorithm, vocabulary size, and dataset size. We measure token
premiums and test for a relationship between factors such as data similarity
(between tokenizer training and evaluation), vocabulary size, and
pre-tokenization. We also investigate the role of language-specific features
such as writing system and word length. We find that similarity between
training and test data does not impact token premiums, but vocabulary size and
pre-tokenization do. While simply increasing vocabulary size does not lead to
reduced token premium effects, we can determine an ``optimal'' vocabulary size
for each language to achieve significantly reduced token premium effects. We
also train superword tokenizers which allow merges over whitespaces, and we
find that they both reduce token premium effects and improve compression
overall. Thus, intervening on the vocabulary size or the pre-tokenizer
significantly reduces crosslingual token premium effects.

</details>


### [10] [Model-Aware Tokenizer Transfer](https://arxiv.org/abs/2510.21954)
*Mykola Haltiuk,Aleksander Smywiński-Pohl*

Main category: cs.CL

TL;DR: MATT是一种模型感知的分词器迁移方法，通过注意力影响建模将源模型的token间通信模式蒸馏到使用新分词器的目标模型中，相比仅关注嵌入相似性的启发式方法，能更有效地恢复模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然支持多种语言，但预定义分词器成为适应低资源或不同文字语言的瓶颈。现有分词器迁移方法通常依赖语义启发式来初始化新嵌入，忽略了高层模型动态，限制了迁移质量。

Method: 提出模型感知分词器迁移(MATT)，引入注意力影响建模(AIM)目标，将源模型的token间通信模式蒸馏到使用新分词器的目标模型中，在标准语言建模前提供高效预热。

Result: 在多样化语言设置下的实验表明，MATT在几个GPU小时内能恢复原始模型性能的大部分，优于启发式基线方法。

Conclusion: 融入模型级信号为多语言LLM中的鲁棒分词器迁移提供了实用有效的路径。

Abstract: Large Language Models (LLMs) are trained to support an increasing number of
languages, yet their predefined tokenizers remain a bottleneck for adapting
models to lower-resource or distinct-script languages. Existing tokenizer
transfer methods typically rely on semantic heuristics to initialize new
embeddings, ignoring higher-layer model dynamics and limiting transfer quality.
We propose Model-Aware Tokenizer Transfer (MATT), a method that incorporates
model internals into the tokenizer transfer process. MATT introduces an
Attention Influence Modeling (AIM) objective that distills inter-token
communication patterns from a source model into a target model with a new
tokenizer, providing an efficient warm-up before standard language modeling.
Unlike approaches that focus solely on embedding similarity, MATT leverages
attention behavior to guide embedding initialization and adaptation.
Experiments across diverse linguistic settings show that MATT recovers a large
fraction of the original model's performance within a few GPU hours,
outperforming heuristic baselines. These results demonstrate that incorporating
model-level signals offers a practical and effective path toward robust
tokenizer transfer in multilingual LLMs.

</details>


### [11] [A Stylometric Application of Large Language Models](https://arxiv.org/abs/2510.21958)
*Harrison F. Stropkay,Jiayi Chen,Mohammad J. Latifi,Daniel N. Rockmore,Jeremy R. Manning*

Main category: cs.CL

TL;DR: 大型语言模型可以用于区分不同作者的写作风格，通过训练单个GPT-2模型来识别特定作者的独特写作特征。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型是否能够捕捉和识别不同作者的独特写作风格，为作者身份识别提供新的技术方法。

Method: 为每个作者单独训练GPT-2模型，然后比较模型对该作者和其他作者文本的预测准确性。

Result: 实验证明，针对特定作者训练的模型能够更准确地预测该作者的文本，成功确认了R. P. Thompson对Oz系列第15本书的作者身份。

Conclusion: 大型语言模型能够有效体现作者的独特写作风格，为作者身份识别提供了可靠的自动化方法。

Abstract: We show that large language models (LLMs) can be used to distinguish the
writings of different authors. Specifically, an individual GPT-2 model, trained
from scratch on the works of one author, will predict held-out text from that
author more accurately than held-out text from other authors. We suggest that,
in this way, a model trained on one author's works embodies the unique writing
style of that author. We first demonstrate our approach on books written by
eight different (known) authors. We also use this approach to confirm R. P.
Thompson's authorship of the well-studied 15th book of the Oz series,
originally attributed to F. L. Baum.

</details>


### [12] [Uncovering the Persuasive Fingerprint of LLMs in Jailbreaking Attacks](https://arxiv.org/abs/2510.21983)
*Havva Alizadeh Noughabi,Julien Serbanescu,Fattane Zarrinkalam,Ali Dehghantanha*

Main category: cs.CL

TL;DR: 该论文研究利用社会科学中的说服理论来构建对抗性提示，成功绕过LLM的对齐保护机制，引发有害输出。


<details>
  <summary>Details</summary>
Motivation: 现有研究很少关注影响LLM对越狱攻击敏感性的语言和心理机制，作者希望通过跨学科方法探索说服策略在LLM安全中的影响。

Method: 借鉴社会科学中成熟的说服理论，构建具有说服结构的对抗性提示，并在多个对齐LLM上进行实证评估。

Result: 实验表明，基于说服理论的提示能显著绕过LLM的安全防护，诱导越狱行为。

Conclusion: 这项工作强调了跨学科洞察在应对LLM安全挑战中的重要性，说服策略对LLM安全构成新的威胁。

Abstract: Despite recent advances, Large Language Models remain vulnerable to jailbreak
attacks that bypass alignment safeguards and elicit harmful outputs. While
prior research has proposed various attack strategies differing in human
readability and transferability, little attention has been paid to the
linguistic and psychological mechanisms that may influence a model's
susceptibility to such attacks. In this paper, we examine an interdisciplinary
line of research that leverages foundational theories of persuasion from the
social sciences to craft adversarial prompts capable of circumventing alignment
constraints in LLMs. Drawing on well-established persuasive strategies, we
hypothesize that LLMs, having been trained on large-scale human-generated text,
may respond more compliantly to prompts with persuasive structures.
Furthermore, we investigate whether LLMs themselves exhibit distinct persuasive
fingerprints that emerge in their jailbreak responses. Empirical evaluations
across multiple aligned LLMs reveal that persuasion-aware prompts significantly
bypass safeguards, demonstrating their potential to induce jailbreak behaviors.
This work underscores the importance of cross-disciplinary insight in
addressing the evolving challenges of LLM safety. The code and data are
available.

</details>


### [13] [Toward Understanding the Transferability of Adversarial Suffixes in Large Language Models](https://arxiv.org/abs/2510.22014)
*Sarah Ball,Niki Hasrati,Alexander Robey,Avi Schwarzschild,Frauke Kreuter,Zico Kolter,Andrej Risteski*

Main category: cs.CL

TL;DR: 本文分析了离散优化越狱攻击在大型语言模型中的可转移性，识别了三个与转移成功强相关的统计特性，并发现语义相似性仅弱相关。


<details>
  <summary>Details</summary>
Motivation: 尽管越狱攻击的可转移性已被实证确认，但缺乏对其发生时机和原因的严谨分析，本文旨在填补这一空白。

Method: 通过识别三个统计特性（无后缀提示激活拒绝方向的程度、后缀诱导远离拒绝方向的强度、正交方向上的偏移大小），并分析它们与转移成功的相关性。

Result: 发现三个统计特性与转移成功强相关，而提示语义相似性仅弱相关，这些发现可用于提升攻击成功率。

Conclusion: 研究提供了对可转移性的更精细理解，并通过干预实验展示了统计分析如何转化为实际攻击效果的提升。

Abstract: Discrete optimization-based jailbreaking attacks on large language models aim
to generate short, nonsensical suffixes that, when appended onto input prompts,
elicit disallowed content. Notably, these suffixes are often transferable --
succeeding on prompts and models for which they were never optimized. And yet,
despite the fact that transferability is surprising and empirically
well-established, the field lacks a rigorous analysis of when and why transfer
occurs. To fill this gap, we identify three statistical properties that
strongly correlate with transfer success across numerous experimental settings:
(1) how much a prompt without a suffix activates a model's internal refusal
direction, (2) how strongly a suffix induces a push away from this direction,
and (3) how large these shifts are in directions orthogonal to refusal. On the
other hand, we find that prompt semantic similarity only weakly correlates with
transfer success. These findings lead to a more fine-grained understanding of
transferability, which we use in interventional experiments to showcase how our
statistical analysis can translate into practical improvements in attack
success.

</details>


### [14] [Penalizing Length: Uncovering Systematic Bias in Quality Estimation Metrics](https://arxiv.org/abs/2510.22028)
*Yilin Zhang,Wenda Xu,Zhongtao Liu,Tetsuji Nakagawa,Markus Freitag*

Main category: cs.CL

TL;DR: 本文系统研究了机器翻译质量评估(QE)指标中的长度偏差问题，发现QE指标会随着翻译长度增加而过度预测错误，并偏好较短翻译，提出了长度归一化和引入参考文本两种缓解策略。


<details>
  <summary>Details</summary>
Motivation: 质量评估指标在机器翻译中至关重要，但长度偏差的普遍性和影响尚未得到充分研究，这可能导致对较长正确翻译的不公平惩罚和次优决策。

Method: 对10种不同语言对的顶级回归基和LLM-as-a-Judge QE指标进行系统研究，识别两种关键长度偏差，并提出长度归一化训练和评估时引入参考文本两种缓解策略。

Result: 研究发现QE指标会随着翻译长度增加而过度预测错误，即使对于高质量无错误文本也是如此；同时偏好较短翻译。提出的两种策略都能有效减少识别出的长度偏差。

Conclusion: QE指标存在显著的长度偏差问题，需要通过长度归一化和引入参考文本等策略来缓解，以确保公平评估和优化决策。

Abstract: Quality Estimation (QE) metrics are vital in machine translation for
reference-free evaluation and as a reward signal in tasks like reinforcement
learning. However, the prevalence and impact of length bias in QE have been
underexplored. Through a systematic study of top-performing regression-based
and LLM-as-a-Judge QE metrics across 10 diverse language pairs, we reveal two
critical length biases: First, QE metrics consistently over-predict errors with
increasing translation length, even for high-quality, error-free texts. Second,
they exhibit a preference for shorter translations when multiple candidates are
available for the same source text. These inherent length biases risk unfairly
penalizing longer, correct translations and can lead to sub-optimal
decision-making in applications such as QE reranking and QE guided
reinforcement learning. To mitigate this, we propose two strategies: (a)
applying length normalization during model training, and (b) incorporating
reference texts during evaluation. Both approaches were found to effectively
reduce the identified length bias.

</details>


### [15] [ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning, and Decoding the Curse of Multilinguality](https://arxiv.org/abs/2510.22037)
*Shayne Longpre,Sneha Kudugunta,Niklas Muennighoff,I-Hung Hsu,Isaac Caswell,Alex Pentland,Sercan Arik,Chen-Yu Lee,Sayna Ebrahimi*

Main category: cs.CL

TL;DR: 本文进行了迄今为止最大的多语言扩展定律研究，包含774个多语言训练实验，涵盖10M-8B参数模型、400+训练语言和48个评估语言，提出了优于现有扩展定律的ATLAS方法。


<details>
  <summary>Details</summary>
Motivation: 现有的扩展定律研究主要集中在英语上，而最著名的AI模型明确服务于数十亿国际用户，因此需要研究多语言环境下的扩展定律。

Method: 通过774个多语言训练实验，引入自适应迁移扩展定律(ATLAS)，分析多语言学习动态、语言间迁移特性以及多语言诅咒问题。

Result: ATLAS在样本外泛化方面优于现有扩展定律超过0.3 R²；推导出跨语言迁移矩阵，测量了38×38=1444个语言对的相互受益分数；建立了语言无关的扩展定律；确定了从头预训练与从多语言检查点微调的计算交叉点。

Conclusion: 这些发现为跨语言扩展定律的民主化提供了科学基础，使从业者能够高效扩展模型，超越英语优先的AI。

Abstract: Scaling laws research has focused overwhelmingly on English -- yet the most
prominent AI models explicitly serve billions of international users. In this
work, we undertake the largest multilingual scaling laws study to date,
totaling 774 multilingual training experiments, spanning 10M-8B model
parameters, 400+ training languages and 48 evaluation languages. We introduce
the Adaptive Transfer Scaling Law (ATLAS) for both monolingual and multilingual
pretraining, which outperforms existing scaling laws' out-of-sample
generalization often by more than 0.3 R^2. Our analyses of the experiments shed
light on multilingual learning dynamics, transfer properties between languages,
and the curse of multilinguality. First, we derive a cross-lingual transfer
matrix, empirically measuring mutual benefit scores between 38 x 38=1444
language pairs. Second, we derive a language-agnostic scaling law that reveals
how to optimally scale model size and data when adding languages without
sacrificing performance. Third, we identify the computational crossover points
for when to pretrain from scratch versus finetune from multilingual
checkpoints. We hope these findings provide the scientific foundation for
democratizing scaling laws across languages, and enable practitioners to
efficiently scale models -- beyond English-first AI.

</details>


### [16] [Emotions Where Art Thou: Understanding and Characterizing the Emotional Latent Space of Large Language Models](https://arxiv.org/abs/2510.22042)
*Benjamin Reichman,Adar Avsian,Larry Heck*

Main category: cs.CL

TL;DR: 该研究通过分析大语言模型隐藏状态空间的几何结构，揭示了模型内部情感表示的低维情感流形，发现情感表征具有方向性编码、跨层分布的特点，并能跨语言和数据集泛化。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型如何内部表示情感，理解其情感处理机制的内在结构。

Method: 分析LLM隐藏状态空间的几何结构，识别低维情感流形，研究情感表征的方向性编码和跨层分布特性，并在八个跨语言情感数据集上进行验证。

Result: 发现了稳定且可泛化的情感子空间，情感表征在不同深度和语言间保持一致，通过干预模块可以在保持语义的同时操控情感感知。

Conclusion: LLMs中存在一致且可操控的情感几何结构，这为理解模型如何内化和处理情感提供了重要见解。

Abstract: This work investigates how large language models (LLMs) internally represent
emotion by analyzing the geometry of their hidden-state space. The paper
identifies a low-dimensional emotional manifold and shows that emotional
representations are directionally encoded, distributed across layers, and
aligned with interpretable dimensions. These structures are stable across depth
and generalize to eight real-world emotion datasets spanning five languages.
Cross-domain alignment yields low error and strong linear probe performance,
indicating a universal emotional subspace. Within this space, internal emotion
perception can be steered while preserving semantics using a learned
intervention module, with especially strong control for basic emotions across
languages. These findings reveal a consistent and manipulable affective
geometry in LLMs and offer insight into how they internalize and process
emotion.

</details>


### [17] [Compositional Bias Control in Large Language Models: Preference Learning Fails, Supervision Succeeds](https://arxiv.org/abs/2510.22084)
*Atij Mahesh*

Main category: cs.CL

TL;DR: 该论文比较了六种减少LLM性别偏见的方法，发现监督微调(SFT)在约束合规性和词汇多样性方面表现最佳，而基于偏好的方法(如DPO)无法满足组合约束要求。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型即使在职业中性语境中仍会产生性别刻板印象语言，反映了深层社会偏见。现有偏见缓解方法的比较效果和学习动态尚不清楚。

Method: 比较分析了六种偏见缓解控制技术：仅提示、生成后过滤、基于DFA的Ctrl-G解码、监督微调(SFT)、直接偏好优化(DPO)和迭代零空间投影(INLP)。在组合约束任务上评估每种方法。

Result: SFT达到99.87%的合规性和高词汇多样性，DPO失败率高达95.47%。Ctrl-G保证完美合规但严重降低流畅性和多样性。基于偏好的学习无法满足组合约束要求。

Conclusion: 只有显式正监督能够缓解组合偏见；基于偏好的对齐无法泛化逻辑结构，强调了偏好学习的局限性和显式监督对于公平流畅控制生成的必要性。

Abstract: Large Language Models (LLMs) still produce gender-stereotyped language even
in occupation-neutral contexts that reflect deep societal biases (Rudinger et
al., 2018). To address this, prior work has proposed prompting, constrained
decoding (Dathathri et al., 2020; Zhou et al., 2024), post-processing, and
fine-tuning-based alignment (Rafailov et al., 2023; Ravfogel et al., 2022).
However, the comparative efficacy and learning dynamics remain little
understood. We report a comparative analysis of six control techniques for bias
mitigation: prompt-only, generate-and-filter, DFA-based Ctrl-G decoding,
Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and
Iterative Nullspace Projection (INLP). We evaluate each method on a
compositional constraint task. This task requires generating sentences that
contain at least one agentic and one communal descriptor for each of the twenty
Winogender-derived occupations. We quantify trade-offs between control strength
and naturalness with evaluations of constraint compliance, lexical diversity,
and fluency. Our results reveal key contrasts among the methods: SFT achieves
99.87 +- 0.15% compliance and high lexical diversity, while DPO, despite
similar training stability, fails at 4.53 +- 0.82%. Ctrl-G guarantees perfect
compliance, but at the cost of severely reduced fluency and diversity.
Preference-based learning fundamentally differs: it cannot satisfy
compositional constraints, as binary preference signals encode ranking, not
logical conjunctions. Only explicit positive supervision enables mitigation of
compositional biases; preference-based alignment fails to generalize logical
structures, underscoring the limitations of preference learning and the
necessity of explicit supervision for fair and fluent controlled generation.

</details>


### [18] [Generalization or Memorization: Dynamic Decoding for Mode Steering](https://arxiv.org/abs/2510.22099)
*Xuanming Zhang*

Main category: cs.CL

TL;DR: 提出了一个统一框架来理解和控制LLMs的泛化与记忆两种推理模式，基于信息瓶颈理论，开发了动态模式引导(DMS)算法来提升模型可靠性。


<details>
  <summary>Details</summary>
Motivation: LLMs存在泛化能力和逐字记忆的双重性，这种不可预测性削弱了其在关键应用中的可靠性。

Method: 基于信息瓶颈理论建立理论模型，开发动态模式引导(DMS)算法，包括轻量级因果线性探针和动态激活引导机制。

Result: 在推理和忠实性任务上的实验表明，DMS显著提高了逻辑一致性和事实准确性。

Conclusion: DMS为增强LLM可靠性提供了一种原则性方法，通过自适应自对比解码引导模型计算朝向泛化电路。

Abstract: Large Language Models (LLMs) exhibit a troubling duality, capable of both
remarkable generalization and brittle, verbatim memorization of their training
data. This unpredictability undermines their reliability in high-stakes
applications. In this work, we propose a unified framework to understand,
identify, and control these distinct reasoning modes. First, we introduce a
theoretical model based on the Information Bottleneck (IB) principle,
formalizing generalization as the learning of a compressed, task-relevant
representation and memorization as a failure to compress. Building on this
theory, we develop Dynamic Mode Steering (DMS), a novel inference-time
algorithm which comprises two components: (1) a lightweight, causally-grounded
linear probe that identifies the model's instantaneous reliance on
memorization, and (2) a dynamic activation steering mechanism that nudges the
model's computation towards pre-identified generalization circuits. We frame
DMS as a form of adaptive, self-contrastive decoding. Experiments on reasoning
and faithfulness tasks demonstrate that DMS significantly improves logical
consistency and factual accuracy, thereby offering a principled approach to
enhancing LLM reliability.

</details>


### [19] [Gradual Forgetting: Logarithmic Compression for Extending Transformer Context Windows](https://arxiv.org/abs/2510.22109)
*Billy Dickson,Zoran Tiganj*

Main category: cs.CL

TL;DR: 提出了一种通过输入表示的对数压缩来增强Transformer长上下文处理能力的方法，无需修改模型架构。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过增加递归或辅助记忆模块来增强Transformer的长上下文处理能力，但这会增加架构复杂性。本文受人类记忆认知模型启发，探索在输入层面进行压缩的替代方案。

Method: 对输入token应用尺度不变的对数压缩，生成压缩后的表示，然后使用标准的未修改Transformer进行处理。

Result: 在WikiText-103和PG-19语言建模基准测试中，相比未压缩基线降低了困惑度，且随着压缩时间上下文长度的增加，性能持续提升。

Conclusion: 输入级别的对数压缩是扩展Transformer长程记忆的简单有效方法，能够保持架构简洁性。

Abstract: Most approaches to long-context processing increase the complexity of the
transformer's internal architecture by integrating mechanisms such as
recurrence or auxiliary memory modules. In this work, we introduce an
alternative approach that modifies the input representation itself, rather than
the transformer architecture. Inspired by cognitive models of human memory, our
method applies a scale-invariant logarithmic compression to the input tokens.
The resulting compressed representation is processed by a standard, unmodified
transformer, preserving architectural simplicity. We evaluate this approach on
the WikiText-103 and PG-19 language modeling benchmarks, showing a reduction in
perplexity compared to uncompressed baselines. Moreover, performance improves
consistently with longer compressed temporal contexts, showing that input-level
logarithmic compression is a simple and effective way to extend a transformer's
long-range memory.

</details>


### [20] [Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open Language Foundation](https://arxiv.org/abs/2510.22115)
*Ling-Team,Ang Li,Ben Liu,Binbin Hu,Bing Li,Bingwei Zeng,Borui Ye,Caizhi Tang,Changxin Tian,Chao Huang,Chao Zhang,Chen Qian,Chenchen Ju,Chenchen Li,Chengfu Tang,Chili Fu,Chunshao Ren,Chunwei Wu,Cong Zhang,Cunyin Peng,Dafeng Xu,Daixin Wang,Dalong Zhang,Dingnan Jin,Dingyuan Zhu,Dongke Hu,Fangzheng Zhao,Feifan Wu,Feng Zhu,Gangshan Wang,Haitao Zhang,Hailin Zhao,Hanxiao Zhang,Hanzi Wang,Hao Qian,Haoyi Yu,Heng Zhang,Hongliang Zhang,Hongzhi Luan,Huirong Dong,Huizhong Li,Jia Li,Jia Liu,Jialong Zhu,Jian Sha,Jianping Wei,Jiaolong Yang,Jieyue Ma,Jiewei Wu,Jinjing Huang,Jingyun Tian,Jingyuan Zhang,Jinquan Sun,Juanhui Tu,Jun Liu,Jun Xu,Jun Zhou,Junjie Ou,Junpeng Fang,Kaihong Zhang,Kaiqin Hu,Ke Shi,Kun Tang,Kunlong Chen,Lanyin Mei,Lei Liang,Lei Xu,Libo Zhang,Lin Ju,Lin Yuan,Ling Zhong,Lintao Ma,Lu Liu,Lu Yu,Lun Cai,Meiqi Zhu,Mengying Li,Min Chen,Minghao Xue,Minghong Cai,Mingming Yin,Peijie Jiang,Peilong Zhao,Pingping Liu,Qian Zhao,Qing Cui,Qingxiang Huang,Qingyuan Yang,Quankun Yu,Shaowei Wei,Shijie Lian,Shoujian Zheng,Shun Song,Shungen Zhang,Shuo Zhang,Siyuan Li,Song Liu,Ting Guo,Tong Zhao,Wanli Gu,Weichang Wu,Weiguang Han,Wenjing Fang,Wubin Wang,Xiang Shu,Xiao Shi,Xiaoshun Lan,Xiaolu Zhang,Xiaqing Sun,Xin Zhao,Xingyu Lu,Xiong Xu,Xudong Wang,Xudong Wang,Xuemin Yang,Yajie Yang,Yang Xiang,Yanzhe Li,Yi Zhang,Yilong Wang,Yingxue Li,Yongzhen Guo,Yuzhuo Fu,Yuanyuan Wang,Yue Yang,Yue Yu,Yufeng Deng,Yun Zhang,Yunfei Xu,Yuqi Zhang,Yuxiao He,Zengke Gui,Zhaoxin Huan,Zhaoyang Wang,Zhibo Zhu,Zhihao Wang,Zhiqiang Zhang,Zhoufei Wang,Zihang Zeng,Ziqi Liu,Zitao Xuan,Zuoli Tang*

Main category: cs.CL

TL;DR: Ling 2.0是一个面向推理的语言基础模型系列，基于MoE架构，参数规模从160亿到1万亿，通过高稀疏性和协调创新实现了高达7倍的计算效率提升。


<details>
  <summary>Details</summary>
Motivation: 构建一个能够从数百亿扩展到万亿参数的统一推理导向语言基础模型，强调高稀疏性、跨尺度一致性和基于经验缩放定律的效率。

Method: 采用高稀疏MoE架构与MTP技术、推理导向数据与中训练CoT激活、基于强化学习的微调（DFT、Evo-CoT），以及全尺度FP8训练与细粒度异构流水线。

Result: Ling-1T在万亿规模上建立了推理精度与计算效率的新帕累托前沿，稀疏激活与推理目标对齐实现了可扩展且高效的人工智能。

Conclusion: Ling 2.0为推进未来推理和思维模型提供了一个连贯、开放且高效的基础，包括基于相同基础的Ring系列。

Abstract: We introduce Ling 2.0, a series reasoning-oriented language foundation built
upon the principle that every activation boosts reasoning capability. Designed
to scale from tens of billions to one trillion parameters under a unified
Mixture-of-Experts (MoE) paradigm, Ling 2.0 emphasizes high sparsity,
cross-scale consistency, and efficiency guided by empirical scaling laws. The
series includes three non-thinking (instruct) models - Ling-mini-2.0,
Ling-flash-2.0, and Ling-1T - ranging from 16B to 1T total parameters and
achieving up to 7-fold active-compute efficiency compared with dense
counterparts. Ling 2.0 integrates coordinated innovations across model
architecture, pre-training, post-training, and infrastructure: a high-sparsity
MoE with MTP for efficient reasoning, reasoning-oriented data and mid-training
CoT activation, reinforcement-based fine-tuning (DFT, Evo-CoT), and full-scale
FP8 training with fine-grained heterogeneous pipelines. At the trillion scale,
Ling-1T establishes a new Pareto frontier of reasoning accuracy versus
computational efficiency, demonstrating that sparse activation, when properly
aligned with reasoning objectives, enables scalable and efficient intelligence.
Collectively, Ling 2.0 provides a coherent, open, and efficient foundation for
advancing future reasoning and thinking models, including the Ring series built
upon the same base.

</details>


### [21] [OlaMind: Towards Human-Like and Hallucination-Safe Customer Service for Retrieval-Augmented Dialogue](https://arxiv.org/abs/2510.22143)
*Tianhong Gao,Jundong Shen,Bei Shi,Jiapeng Wang,Ying Ju,Junfeng Yao,Jiao Ran,Yong Zhang,Lin Dong,Huiyu Yu,Tingting Ye*

Main category: cs.CL

TL;DR: OlaMind是一个基于检索增强生成(RAG)的类人化、防幻觉智能客服框架，通过两阶段学习(Learn-to-Think和Learn-to-Respond)显著提升回答的自然度和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG智能客服系统存在幻觉问题和回答生硬的问题，这会带来业务风险并影响用户体验，特别是在基于Web的客服交互场景中。

Method: 采用两阶段方法：Learn-to-Think阶段学习人类专家的推理过程和回答策略，Learn-to-Respond阶段结合冷启动监督微调(SFT)和强化学习(RL)进行基础到困难的自我精炼。

Result: 大规模在线A/B实验显示，在社区支持和直播互动场景中，智能解决率分别提升28.92%/18.42%，人工接管率降低6.08%/7.12%。

Conclusion: OlaMind框架在不同真实应用场景中均表现出显著效果，能有效提升回答类人化程度并减少幻觉和业务风险。

Abstract: Intelligent customer service (ICS) systems via retrieval-augmented generation
(RAG) have been widely adopted in Web-based domains such as social platforms
and e-commerce, achieving remarkable improvements in automation and efficiency.
However, notable limitations still remain: these systems are prone to
hallucinations and often generate rigid, mechanical responses, which can
introduce business risks and undermine user experience, especially in Web-based
customer service interactions under the RAG scenarios. In this paper, we
introduce OlaMind, a human-like and hallucination-safe customer service
framework for retrieval-augmented dialogue. Specifically, it first leverages a
Learn-to-Think stage to learn the reasoning processes and response strategies
from human experts, and then employs a Learn-to-Respond stage to perform
cold-start supervised fine-tuning (SFT) combined with reinforcement learning
(RL) for basic-to-hard self-refinement. Our method significantly enhances
human-likeness and naturalness while effectively mitigating hallucinations and
critical business risks. We have conducted large-scale online A/B experiments
in an industry-level social customer service setting, and extensive
experimental results show that OlaMind achieves significant cumulative relative
improvements with intelligent resolution rates +28.92%/+18.42% and human
takeover rate -6.08%/-7.12% in community-support/livestream-interaction
scenarios, respectively, which highlights its consistent effectiveness across
diverse real-world applications. The code and data will be publicly available.

</details>


### [22] [SentiMaithili: A Benchmark Dataset for Sentiment and Reason Generation for the Low-Resource Maithili Language](https://arxiv.org/abs/2510.22160)
*Rahul Ranjan,Mahendra Kumar Gurve,Anuj,Nitin,Yamuna Prasad*

Main category: cs.CL

TL;DR: 该论文针对低资源语言迈蒂利语开发了首个可解释情感分析数据集，包含3,221个带情感极性和自然语言解释的句子，填补了该语言在NLP研究中的空白。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言迈蒂利语在情感分析研究中资源稀缺的问题，特别是缺乏细粒度标注和可解释性机制，推动多语言NLP和可解释AI的发展。

Method: 引入包含3,221个迈蒂利语句子的新数据集，每个句子都标注了情感极性并配有自然语言解释，由语言专家精心策划和验证以确保标签可靠性和上下文保真度。

Result: 使用经典机器学习和最先进的transformer架构进行的广泛实验证明了该数据集在可解释情感分析方面的有效性。

Conclusion: 这项工作为迈蒂利语建立了首个可解释情感计算基准，为多语言NLP和可解释AI的广泛发展贡献了宝贵资源。

Abstract: Developing benchmark datasets for low-resource languages poses significant
challenges, primarily due to the limited availability of native linguistic
experts and the substantial time and cost involved in annotation. Given these
challenges, Maithili is still underrepresented in natural language processing
research. It is an Indo-Aryan language spoken by more than 13 million people in
the Purvanchal region of India, valued for its rich linguistic structure and
cultural significance. While sentiment analysis has achieved remarkable
progress in high-resource languages, resources for low-resource languages, such
as Maithili, remain scarce, often restricted to coarse-grained annotations and
lacking interpretability mechanisms. To address this limitation, we introduce a
novel dataset comprising 3,221 Maithili sentences annotated for sentiment
polarity and accompanied by natural language justifications. Moreover, the
dataset is carefully curated and validated by linguistic experts to ensure both
label reliability and contextual fidelity. Notably, the justifications are
written in Maithili, thereby promoting culturally grounded interpretation and
enhancing the explainability of sentiment models. Furthermore, extensive
experiments using both classical machine learning and state-of-the-art
transformer architectures demonstrate the dataset's effectiveness for
interpretable sentiment analysis. Ultimately, this work establishes the first
benchmark for explainable affective computing in Maithili, thus contributing a
valuable resource to the broader advancement of multilingual NLP and
explainable AI.

</details>


### [23] [DETECT: Determining Ease and Textual Clarity of German Text Simplifications](https://arxiv.org/abs/2510.22212)
*Maria Korobeynikova,Alessia Battisti,Lukas Fischer,Yingqiang Gao*

Main category: cs.CL

TL;DR: 提出了DETECT，第一个德语专用的文本简化评估指标，通过LLM生成合成数据训练，在简洁性、意义保留和流畅性三个维度上全面评估德语ATS质量。


<details>
  <summary>Details</summary>
Motivation: 当前德语自动文本简化评估依赖通用指标如SARI、BLEU等，无法充分捕捉简化质量，且缺乏德语专用评估指标和人工标注语料库。

Method: 基于LENS框架适配德语，使用LLM生成合成质量分数创建数据集，无需人工标注，并通过LLM细化评分标准与简化要求对齐。

Result: DETECT在人类评估相关性上显著优于现有ATS指标，尤其在意义保留和流畅性方面表现突出。

Conclusion: DETECT填补了德语ATS评估空白，同时揭示了LLM在自动评估中的潜力和局限性，为通用语言可访问性任务提供了可转移指南。

Abstract: Current evaluation of German automatic text simplification (ATS) relies on
general-purpose metrics such as SARI, BLEU, and BERTScore, which insufficiently
capture simplification quality in terms of simplicity, meaning preservation,
and fluency. While specialized metrics like LENS have been developed for
English, corresponding efforts for German have lagged behind due to the absence
of human-annotated corpora. To close this gap, we introduce DETECT, the first
German-specific metric that holistically evaluates ATS quality across all three
dimensions of simplicity, meaning preservation, and fluency, and is trained
entirely on synthetic large language model (LLM) responses. Our approach adapts
the LENS framework to German and extends it with (i) a pipeline for generating
synthetic quality scores via LLMs, enabling dataset creation without human
annotation, and (ii) an LLM-based refinement step for aligning grading criteria
with simplification requirements. To the best of our knowledge, we also
construct the largest German human evaluation dataset for text simplification
to validate our metric directly. Experimental results show that DETECT achieves
substantially higher correlations with human judgments than widely used ATS
metrics, with particularly strong gains in meaning preservation and fluency.
Beyond ATS, our findings highlight both the potential and the limitations of
LLMs for automatic evaluation and provide transferable guidelines for general
language accessibility tasks.

</details>


### [24] [Estimating the Error of Large Language Models at Pairwise Text Comparison](https://arxiv.org/abs/2510.22219)
*Tianyi Li*

Main category: cs.CL

TL;DR: 本文提出了一种测量LLM在成对文本比较中输出错误的方法，通过Copeland计数构建文本排名来估计错误率，并在六个主流LLM上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 需要量化LLM在成对文本比较任务中的错误概率，而传统方法依赖真实标签，本文旨在开发不依赖真实标签的误差估计方法。

Method: 使用两种场景估计错误率：(i) 不考虑顺序的统一错误率，通过每个文本对进行两次比较；(ii) 考虑位置偏差的二元位置偏差，通过重复比较估计不同顺序的错误率。采用Copeland计数构建文本排名。

Result: 在六个LLM（ChatGPT、Claude、DeepSeek、Gemini、Grok、Qwen）和五种文本类型上获得一致的错误率估计。两个位置偏差项相似，接近统一错误率。Claude在错误率和提示鲁棒性方面表现最佳。

Conclusion: 该方法在指示LLM成对比较错误方面优于有偏Bradley-Terry模型和交换性评分，Claude在此任务中表现最理想。

Abstract: We measure LLMs' output error at pairwise text comparison, noting the
probability of error in their preferences. Our method does not rely on the
ground truth and supports two scenarios: (i) uniform error rate regardless of
the order of comparison, estimated with two comparisons for each text pair with
either text placed first; (ii) binary positional bias assuming distinct error
rates for the two orders of comparison, estimated with repeated comparisons
between the texts. The Copeland counting constructs a ranking over the compared
texts from pairwise preferences; the ranking reveals the poor scalability of
LLM-based pairwise comparison and helps yield the estimates for LLMs' error
rates. We apply the method to six LLMs (ChatGPT, Claude, DeepSeek, Gemini,
Grok, Qwen) with five types of text input and obtain consistent estimates of
LLMs' error. In general, the measured two positional bias terms are similar,
close to the uniform error. Considering both the error rates and the robustness
to the variation of prompts, Claude obtained the most desirable performance in
this experiment. Our model outperforms the biased Bradley-Terry model and the
commutativity score in indicating LLMs' error at this task.

</details>


### [25] [Evolution of the lexicon: a probabilistic point of view](https://arxiv.org/abs/2510.22220)
*Maurizio Serva*

Main category: cs.CL

TL;DR: 本文分析了Swadesh方法在语言年代学中的局限性，指出即使基本假设都满足，数学上仍存在估计精度的固有限制。同时提出词汇渐进修改过程对语言演变的重要影响，并证明考虑这一过程能显著提高时间分离估计的精度。


<details>
  <summary>Details</summary>
Motivation: Swadesh方法的基本假设往往不现实，存在各种污染现象和误判。更重要的是，即使所有假设都满足，数学上仍存在估计精度的固有概率限制，这些限制在词汇统计学研究中常被忽视。

Method: 详细分析Swadesh方法的概率限制，并引入词汇渐进修改这一随机过程作为语言词汇演变的重要驱动力。从纯概率角度证明考虑这一过程能提高估计精度。

Result: 揭示了Swadesh方法在估计语言时间分离时的固有概率限制，证明了词汇渐进修改过程对语言演变的重要贡献，并展示了考虑这一过程能显著提高估计精度。

Conclusion: Swadesh方法存在固有的概率限制，词汇渐进修改过程是语言演变的重要驱动力，综合考虑这一过程能显著提高语言时间分离估计的准确性。

Abstract: The Swadesh approach for determining the temporal separation between two
languages relies on the stochastic process of words replacement (when a
complete new word emerges to represent a given concept). It is well known that
the basic assumptions of the Swadesh approach are often unrealistic due to
various contamination phenomena and misjudgments (horizontal transfers,
variations over time and space of the replacement rate, incorrect assessments
of cognacy relationships, presence of synonyms, and so on). All of this means
that the results cannot be completely correct.
  More importantly, even in the unrealistic case that all basic assumptions are
satisfied, simple mathematics places limits on the accuracy of estimating the
temporal separation between two languages. These limits, which are purely
probabilistic in nature and which are often neglected in lexicostatistical
studies, are analyzed in detail in this article.
  Furthermore, in this work we highlight that the evolution of a language's
lexicon is also driven by another stochastic process: gradual lexical
modification of words. We show that this process equally also represents a
major contribution to the reshaping of the vocabulary of languages over the
centuries and we also show, from a purely probabilistic perspective, that
taking into account this second random process significantly increases the
precision in determining the temporal separation between two languages.

</details>


### [26] [You Don't Need Prompt Engineering Anymore: The Prompting Inversion](https://arxiv.org/abs/2510.22251)
*Imran Khan*

Main category: cs.CL

TL;DR: Sculpting是一种基于规则的约束提示方法，旨在通过减少语义模糊和常识错误来改进标准CoT提示。研究发现存在"提示反转"现象：Sculpting在gpt-4o上优于标准CoT，但在gpt-5上反而有害。


<details>
  <summary>Details</summary>
Motivation: 改进标准CoT提示方法，减少由语义模糊和常识错误导致的推理错误，提升LLM的推理能力。

Method: 提出"Sculpting"约束规则提示方法，在GSM8K数学推理基准上评估三种提示策略（零样本、标准CoT、Sculpting）在三个OpenAI模型（gpt-4o-mini、gpt-4o、gpt-5）上的表现。

Result: 发现"提示反转"现象：Sculpting在gpt-4o上表现更好（97% vs 93%），但在gpt-5上反而有害（94.00% vs 96.36%）。这是由于"护栏到手铐"的转变，约束在高级模型中引发过度字面化。

Conclusion: 最优提示策略必须与模型能力共同进化，对更强大的模型应使用更简单的提示方法。

Abstract: Prompt engineering, particularly Chain-of-Thought (CoT) prompting,
significantly enhances LLM reasoning capabilities. We introduce "Sculpting," a
constrained, rule-based prompting method designed to improve upon standard CoT
by reducing errors from semantic ambiguity and flawed common sense.
  We evaluate three prompting strategies (Zero Shot, standard CoT, and
Sculpting) across three OpenAI model generations (gpt-4o-mini, gpt-4o, gpt-5)
using the GSM8K mathematical reasoning benchmark (1,317 problems).
  Our findings reveal a "Prompting Inversion": Sculpting provides advantages on
gpt-4o (97% vs. 93% for standard CoT), but becomes detrimental on gpt-5 (94.00%
vs. 96.36% for CoT on full benchmark). We trace this to a
"Guardrail-to-Handcuff" transition where constraints preventing common-sense
errors in mid-tier models induce hyper-literalism in advanced models. Our
detailed error analysis demonstrates that optimal prompting strategies must
co-evolve with model capabilities, suggesting simpler prompts for more capable
models.

</details>


### [27] [SteerX: Disentangled Steering for LLM Personalization](https://arxiv.org/abs/2510.22256)
*Xiaoyan Zhao,Ming Yan,Yilun Qiu,Haoting Ni,Yang Zhang,Fuli Feng,Hong Cheng,Tat-Seng Chua*

Main category: cs.CL

TL;DR: SteerX是一种解耦的激活引导方法，通过因果推断识别用户偏好驱动的token，生成更准确的引导向量来提升LLM个性化效果。


<details>
  <summary>Details</summary>
Motivation: 现有激活引导方法使用所有历史数据计算引导向量，但并非所有内容都反映真实用户偏好，这会削弱个性化信号。

Method: 基于因果推断理论，SteerX估计token级因果效应来识别偏好驱动token，将这些离散信号转化为连贯描述，然后用于引导个性化LLM生成。

Result: 在两个代表性引导骨干方法上的实验表明，SteerX能持续提升引导向量质量。

Conclusion: SteerX通过聚焦真正偏好驱动的信息，为更有效的LLM个性化提供了实用解决方案。

Abstract: Large language models (LLMs) have shown remarkable success in recent years,
enabling a wide range of applications, including intelligent assistants that
support users' daily life and work. A critical factor in building such
assistants is personalizing LLMs, as user preferences and needs vary widely.
Activation steering, which directly leverages directions representing user
preference in the LLM activation space to adjust its behavior, offers a
cost-effective way to align the model's outputs with individual users. However,
existing methods rely on all historical data to compute the steering vector,
ignoring that not all content reflects true user preferences, which undermines
the personalization signal. To address this, we propose SteerX, a disentangled
steering method that isolates preference-driven components from
preference-agnostic components. Grounded in causal inference theory, SteerX
estimates token-level causal effects to identify preference-driven tokens,
transforms these discrete signals into a coherent description, and then
leverages them to steer personalized LLM generation. By focusing on the truly
preference-driven information, SteerX produces more accurate activation
steering vectors and enhances personalization. Experiments on two
representative steering backbone methods across real-world datasets demonstrate
that SteerX consistently enhances steering vector quality, offering a practical
solution for more effective LLM personalization.

</details>


### [28] [PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text Embedding](https://arxiv.org/abs/2510.22264)
*Iliass Ayaou,Denis Cavallucci*

Main category: cs.CL

TL;DR: 提出了PatenTEB专利文本嵌入基准，包含15个任务和206万个样本，解决了现有基准在专利领域评估不足的问题。开发了patembed模型家族，在多个专利任务上取得了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法充分捕捉专利文本特有的挑战，如领域分层、专利特定困难负样本挖掘以及非对称片段到文档匹配场景。

Method: 构建PatenTEB基准，采用领域分层划分、专利特定困难负样本挖掘，开发patembed模型家族通过多任务训练，参数规模从67M到344M，上下文长度达4096个token。

Result: patembed-base在MTEB BigPatentClustering.v2上达到0.494 V-measure（之前最佳为0.445），patembed-large在DAPFAM上达到0.377 NDCG@100。多任务训练提高了外部泛化能力，领域预训练初始化在各类任务中均有优势。

Conclusion: PatenTEB为专利文本嵌入提供了全面评估基准，patembed模型在专利任务上表现出色，多任务训练和领域预训练是提升性能的关键因素。

Abstract: Patent text embeddings enable prior art search, technology landscaping, and
patent analysis, yet existing benchmarks inadequately capture patent-specific
challenges. We introduce PatenTEB, a comprehensive benchmark comprising 15
tasks across retrieval, classification, paraphrase, and clustering, with 2.06
million examples. PatenTEB employs domain-stratified splits, domain specific
hard negative mining, and systematic coverage of asymmetric
fragment-to-document matching scenarios absent from general embedding
benchmarks. We develop the patembed model family through multi-task training,
spanning 67M to 344M parameters with context lengths up to 4096 tokens.
External validation shows strong generalization: patembed-base achieves
state-of-the-art on MTEB BigPatentClustering.v2 (0.494 V-measure vs. 0.445
previous best), while patembed-large achieves 0.377 NDCG@100 on DAPFAM.
Systematic ablations reveal that multi-task training improves external
generalization despite minor benchmark costs, and that domain-pretrained
initialization provides consistent advantages across task families. All
resources will be made available at https://github.com/iliass-y/patenteb.
Keywords: patent retrieval, sentence embeddings, multi-task learning,
asymmetric retrieval, benchmark evaluation, contrastive learning.

</details>


### [29] [From Slides to Chatbots: Enhancing Large Language Models with University Course Materials](https://arxiv.org/abs/2510.22272)
*Tu Anh Dinh,Philipp Nicolas Schumacher,Jan Niehues*

Main category: cs.CL

TL;DR: 研究比较了检索增强生成(RAG)和持续预训练(CPT)两种方法，发现在大学课程材料规模较小的情况下，RAG更有效且高效，特别是将幻灯片以图像形式呈现的多模态方法显著优于纯文本检索。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在教育场景中支持学生学习，但在大学计算机科学课程中准确回答问题仍有困难，需要探索如何利用课程材料提升LLM性能。

Method: 比较了RAG和CPT两种策略，针对包含视觉元素的幻灯片材料，还探索了多模态RAG方法，将检索内容以图像形式呈现给生成器。

Result: 实验表明，RAG比CPT更有效和高效，多模态设置中幻灯片以图像形式呈现显著优于纯文本检索。

Conclusion: 这些发现为开发更好支持学习和教学的AI助手提供了实用策略，并希望激励其他教育背景下的类似努力。

Abstract: Large Language Models (LLMs) have advanced rapidly in recent years. One
application of LLMs is to support student learning in educational settings.
However, prior work has shown that LLMs still struggle to answer questions
accurately within university-level computer science courses. In this work, we
investigate how incorporating university course materials can enhance LLM
performance in this setting. A key challenge lies in leveraging diverse course
materials such as lecture slides and transcripts, which differ substantially
from typical textual corpora: slides also contain visual elements like images
and formulas, while transcripts contain spoken, less structured language. We
compare two strategies, Retrieval-Augmented Generation (RAG) and Continual
Pre-Training (CPT), to extend LLMs with course-specific knowledge. For lecture
slides, we further explore a multi-modal RAG approach, where we present the
retrieved content to the generator in image form. Our experiments reveal that,
given the relatively small size of university course materials, RAG is more
effective and efficient than CPT. Moreover, incorporating slides as images in
the multi-modal setting significantly improves performance over text-only
retrieval. These findings highlight practical strategies for developing AI
assistants that better support learning and teaching, and we hope they inspire
similar efforts in other educational contexts.

</details>


### [30] [Supervised Fine-Tuning or In-Context Learning? Evaluating LLMs for Clinical NER](https://arxiv.org/abs/2510.22285)
*Andrei Baroian*

Main category: cs.CL

TL;DR: 比较了BERT风格编码器、GPT-4o少样本上下文学习和GPT-4o监督微调在临床命名实体识别任务上的表现，发现监督微调效果最佳但成本较高。


<details>
  <summary>Details</summary>
Motivation: 研究不同方法在临床命名实体识别任务上的性能差异，探索大型语言模型在医学领域的应用潜力。

Method: 使用CADEC语料库，比较三类方法：(1) BERT风格编码器(BERT Base、BioClinicalBERT、RoBERTa-large)；(2) GPT-4o少样本上下文学习(简单vs复杂提示)；(3) GPT-4o监督微调。评估五种实体类型(ADR、药物、疾病、症状、发现)的标准NER指标。

Result: RoBERTa-large和BioClinicalBERT相比BERT Base改进有限；简单上下文学习优于复杂提示；监督微调达到最佳性能(F1≈87.1%)但成本较高；LLM在简化任务(二分类)上准确率更高。

Conclusion: 监督微调是临床NER任务的最有效方法，但需权衡成本；大型语言模型在简化任务中表现更好，提示设计对性能有重要影响。

Abstract: We study clinical Named Entity Recognition (NER) on the CADEC corpus and
compare three families of approaches: (i) BERT-style encoders (BERT Base,
BioClinicalBERT, RoBERTa-large), (ii) GPT-4o used with few-shot in-context
learning (ICL) under simple vs.\ complex prompts, and (iii) GPT-4o with
supervised fine-tuning (SFT). All models are evaluated on standard NER metrics
over CADEC's five entity types (ADR, Drug, Disease, Symptom, Finding).
RoBERTa-large and BioClinicalBERT offer limited improvements over BERT Base,
showing the limit of these family of models. Among LLM settings, simple ICL
outperforms a longer, instruction-heavy prompt, and SFT achieves the strongest
overall performance (F1 $\approx$ 87.1%), albeit with higher cost. We find that
the LLM achieve higher accuracy on simplified tasks, restricting classification
to two labels.

</details>


### [31] [Memory-based Language Models: An Efficient, Explainable, and Eco-friendly Approach to Large Language Modeling](https://arxiv.org/abs/2510.22317)
*Antal van den Bosch,Ainhoa Risco Patón,Teun Buijse,Peter Berck,Maarten van Gompel*

Main category: cs.CL

TL;DR: 提出基于记忆的语言建模作为深度神经网络语言建模的高效环保替代方案，具有对数线性可扩展性和强记忆能力


<details>
  <summary>Details</summary>
Motivation: 寻求比深度神经网络语言建模更高效、更环保的替代方案，减少训练和推理过程中的生态足迹

Method: 使用k最近邻分类的快速近似实现基于记忆的语言建模，完全依赖CPU运行，实现低延迟

Result: OLIFANT在下一个词预测准确率、排放估计和速度方面与GPT-2和GPT-Neo进行了比较

Conclusion: 基于记忆的语言建模提供了一种简单透明、生态足迹小的语言建模方法

Abstract: We present memory-based language modeling as an efficient, eco-friendly
alternative to deep neural network-based language modeling. It offers
log-linearly scalable next-token prediction performance and strong memorization
capabilities. Implementing fast approximations of k-nearest neighbor
classification, memory-based language modeling leaves a relatively small
ecological footprint both in training and in inference mode, as it relies fully
on CPUs and attains low token latencies. Its internal workings are simple and
fully transparent. We compare our implementation of memory-based language
modeling, OLIFANT, with GPT-2 and GPT-Neo on next-token prediction accuracy,
estimated emissions and speeds, and offer some deeper analyses of the model.

</details>


### [32] [Multilingual Target-Stance Extraction](https://arxiv.org/abs/2510.22334)
*Ethan Mines,Bonnie Dorr*

Main category: cs.CL

TL;DR: 本文提出了首个多语言目标立场提取(TSE)基准，涵盖6种语言，并开发了无需为每种语言单独训练模型的多语言TSE流程。


<details>
  <summary>Details</summary>
Motivation: 社交媒体数据可用于分析争议议题的公众意见，但现有的目标立场提取研究仅限于英语，缺乏多语言基准。

Method: 扩展原始TSE流程到多语言设置，涵盖加泰罗尼亚语、爱沙尼亚语、法语、意大利语、普通话和西班牙语语料库，无需为每种语言单独训练模型。

Result: 模型流程获得12.78的F1分数，表明多语言任务相比英语设置的难度增加，目标预测是主要瓶颈。首次展示了TSE的F1分数对不同目标表述的敏感性。

Conclusion: 这项工作为多语言TSE提供了急需的资源、算法和评估标准基线。

Abstract: Social media enables data-driven analysis of public opinion on contested
issues. Target-Stance Extraction (TSE) is the task of identifying the target
discussed in a document and the document's stance towards that target. Many
works classify stance towards a given target in a multilingual setting, but all
prior work in TSE is English-only. This work introduces the first multilingual
TSE benchmark, spanning Catalan, Estonian, French, Italian, Mandarin, and
Spanish corpora. It manages to extend the original TSE pipeline to a
multilingual setting without requiring separate models for each language. Our
model pipeline achieves a modest F1 score of 12.78, underscoring the increased
difficulty of the multilingual task relative to English-only setups and
highlighting target prediction as the primary bottleneck. We are also the first
to demonstrate the sensitivity of TSE's F1 score to different target
verbalizations. Together these serve as a much-needed baseline for resources,
algorithms, and evaluation criteria in multilingual TSE.

</details>


### [33] [FAIR-RAG: Faithful Adaptive Iterative Refinement for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.22344)
*Mohammad Aghajani Asl,Majid Asgari-Bidhendi,Behrooz Minaei-Bidgoli*

Main category: cs.CL

TL;DR: FAIR-RAG是一个新型的代理框架，通过结构化证据评估和迭代优化循环，解决了复杂多跳查询中证据不完整的问题，在多个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG框架在处理需要从不同来源综合信息的复杂多跳查询时表现不佳，缺乏系统识别和填补证据空白的机制，容易传播噪声或无法收集全面上下文。

Method: FAIR-RAG将标准RAG管道转变为动态、证据驱动的推理过程，核心是结构化证据评估(SEA)模块，它分解查询为需求清单，审计证据识别确认事实和明确信息缺口，引导自适应查询优化代理生成针对性子查询。

Result: 在HotpotQA、2WikiMultiHopQA和MusiQue等挑战性多跳QA基准测试中，FAIR-RAG显著优于强基线。在HotpotQA上达到0.453的F1分数，比最强迭代基线绝对提升8.3分，在这些基准上建立了新的最先进水平。

Conclusion: 具有明确缺口分析的结构化、证据驱动的优化过程对于在复杂知识密集型任务中解锁高级RAG系统的可靠准确推理至关重要。

Abstract: While Retrieval-Augmented Generation (RAG) mitigates hallucination and
knowledge staleness in Large Language Models (LLMs), existing frameworks often
falter on complex, multi-hop queries that require synthesizing information from
disparate sources. Current advanced RAG methods, employing iterative or
adaptive strategies, lack a robust mechanism to systematically identify and
fill evidence gaps, often propagating noise or failing to gather a
comprehensive context. We introduce FAIR-RAG, a novel agentic framework that
transforms the standard RAG pipeline into a dynamic, evidence-driven reasoning
process. At its core is an Iterative Refinement Cycle governed by a module we
term Structured Evidence Assessment (SEA). The SEA acts as an analytical gating
mechanism: it deconstructs the initial query into a checklist of required
findings and audits the aggregated evidence to identify confirmed facts and,
critically, explicit informational gaps. These gaps provide a precise signal to
an Adaptive Query Refinement agent, which generates new, targeted sub-queries
to retrieve missing information. This cycle repeats until the evidence is
verified as sufficient, ensuring a comprehensive context for a final, strictly
faithful generation. We conducted experiments on challenging multi-hop QA
benchmarks, including HotpotQA, 2WikiMultiHopQA, and MusiQue. In a unified
experimental setup, FAIR-RAG significantly outperforms strong baselines. On
HotpotQA, it achieves an F1-score of 0.453 -- an absolute improvement of 8.3
points over the strongest iterative baseline -- establishing a new
state-of-the-art for this class of methods on these benchmarks. Our work
demonstrates that a structured, evidence-driven refinement process with
explicit gap analysis is crucial for unlocking reliable and accurate reasoning
in advanced RAG systems for complex, knowledge-intensive tasks.

</details>


### [34] [Irony Detection in Urdu Text: A Comparative Study Using Machine Learning Models and Large Language Models](https://arxiv.org/abs/2510.22356)
*Fiaz Ahmad,Nisar Hussain,Amna Qasim,Momina Hafeez,Muhammad Usman Grigori Sidorov,Alexander Gelbukh*

Main category: cs.CL

TL;DR: 该研究通过将英语讽刺语料库翻译成乌尔都语，评估了10种机器学习算法和5种先进Transformer模型在乌尔都语讽刺检测中的表现，其中LLaMA 3 (8B)取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 解决乌尔都语这种语法和文化背景不同的语言中讽刺识别的挑战，特别是在资源匮乏的情况下。

Method: 将英语讽刺语料库翻译成乌尔都语，使用GloVe和Word2Vec嵌入评估10种机器学习算法，并微调BERT、RoBERTa、LLaMA 2、LLaMA 3和Mistral等Transformer模型。

Result: 梯度提升算法获得89.18%的F1分数，LLaMA 3 (8B)获得94.61%的最高F1分数。

Conclusion: 结合音译技术和现代NLP模型能够在资源匮乏的乌尔都语中实现稳健的讽刺检测。

Abstract: Ironic identification is a challenging task in Natural Language Processing,
particularly when dealing with languages that differ in syntax and cultural
context. In this work, we aim to detect irony in Urdu by translating an English
Ironic Corpus into the Urdu language. We evaluate ten state-of-the-art machine
learning algorithms using GloVe and Word2Vec embeddings, and compare their
performance with classical methods. Additionally, we fine-tune advanced
transformer-based models, including BERT, RoBERTa, LLaMA 2 (7B), LLaMA 3 (8B),
and Mistral, to assess the effectiveness of large-scale models in irony
detection. Among machine learning models, Gradient Boosting achieved the best
performance with an F1-score of 89.18%. Among transformer-based models, LLaMA 3
(8B) achieved the highest performance with an F1-score of 94.61%. These results
demonstrate that combining transliteration techniques with modern NLP models
enables robust irony detection in Urdu, a historically low-resource language.

</details>


### [35] [GigaEmbeddings: Efficient Russian Language Embedding Model](https://arxiv.org/abs/2510.22369)
*Egor Kolodin,Daria Khomich,Nikita Savushkin,Anastasia Ianina,Fyodor Minkin*

Main category: cs.CL

TL;DR: GigaEmbeddings是一个通过分层指令调优训练俄语文本嵌入的新框架，在ruMTEB基准测试中取得69.1的平均分，达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在俄语文本嵌入方面的局限性，通过统一多样化目标并利用合成数据生成来提升性能。

Method: 采用三阶段流水线：大规模对比预训练、使用困难负样本进行微调、跨检索、分类和聚类任务的多任务泛化；架构创新包括双向注意力、潜在注意力池化和25%Transformer层的策略性剪枝。

Result: 在包含23个多语言任务的ruMTEB基准测试中取得69.1的平均分，超越了参数数量更多的强基线模型。

Conclusion: GigaEmbeddings框架通过分层指令调优和架构优化，在俄语文本嵌入任务中实现了高效且高性能的解决方案。

Abstract: We introduce GigaEmbeddings, a novel framework for training high-performance
Russian-focused text embeddings through hierarchical instruction tuning of the
decoder-only LLM designed specifically for Russian language (GigaChat-3B). Our
three-stage pipeline, comprising large-scale contrastive pre-training in
web-scale corpora, fine-tuning with hard negatives, and multitask
generalization across retrieval, classification, and clustering tasks,
addresses key limitations of existing methods by unifying diverse objectives
and leveraging synthetic data generation. Architectural innovations include
bidirectional attention for contextual modeling, latent attention pooling for
robust sequence aggregation, and strategic pruning of 25% of transformer layers
to enhance efficiency without compromising performance. Evaluated on the ruMTEB
benchmark spanning 23 multilingual tasks, GigaEmbeddings achieves
state-of-the-art results (69.1 avg. score), outperforming strong baselines with
a larger number of parameters.

</details>


### [36] [VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations](https://arxiv.org/abs/2510.22373)
*Yupeng Xie,Zhiyang Zhang,Yifan Wu,Sirong Lu,Jiayi Zhang,Zhaoyang Yu,Jinlin Wang,Sirui Hong,Bang Liu,Chenglin Wu,Yuyu Luo*

Main category: cs.CL

TL;DR: 提出了VisJudge-Bench，首个用于评估多模态大语言模型在可视化质量评估方面能力的基准，包含3,090个专家标注样本。测试发现最先进的MLLMs与人类专家存在显著差距，为此提出了专门的可视化美学评估模型VisJudge，显著缩小了与人类判断的差距。


<details>
  <summary>Details</summary>
Motivation: 可视化质量评估具有挑战性，需要同时判断数据编码准确性、信息表达性和视觉美学。虽然多模态大语言模型在自然图像美学评估方面表现出色，但缺乏系统基准来衡量其在可视化评估方面的能力。

Method: 构建VisJudge-Bench基准，包含3,090个专家标注的真实场景样本，涵盖32种图表类型。提出专门的可视化美学评估模型VisJudge。

Result: 最先进的MLLMs（如GPT-5）与人类专家存在显著差距，MAE为0.551，与人类评分相关性仅为0.429。VisJudge将MAE降至0.442（减少19.8%），与人类专家一致性提升至0.681（改善58.7%）。

Conclusion: VisJudge-Bench填补了可视化质量评估基准的空白，VisJudge模型显著提升了可视化美学评估的性能，缩小了与人类专家判断的差距。

Abstract: Visualization, a domain-specific yet widely used form of imagery, is an
effective way to turn complex datasets into intuitive insights, and its value
depends on whether data are faithfully represented, clearly communicated, and
aesthetically designed. However, evaluating visualization quality is
challenging: unlike natural images, it requires simultaneous judgment across
data encoding accuracy, information expressiveness, and visual aesthetics.
Although multimodal large language models (MLLMs) have shown promising
performance in aesthetic assessment of natural images, no systematic benchmark
exists for measuring their capabilities in evaluating visualizations. To
address this, we propose VisJudge-Bench, the first comprehensive benchmark for
evaluating MLLMs' performance in assessing visualization aesthetics and
quality. It contains 3,090 expert-annotated samples from real-world scenarios,
covering single visualizations, multiple visualizations, and dashboards across
32 chart types. Systematic testing on this benchmark reveals that even the most
advanced MLLMs (such as GPT-5) still exhibit significant gaps compared to human
experts in judgment, with a Mean Absolute Error (MAE) of 0.551 and a
correlation with human ratings of only 0.429. To address this issue, we propose
VisJudge, a model specifically designed for visualization aesthetics and
quality assessment. Experimental results demonstrate that VisJudge
significantly narrows the gap with human judgment, reducing the MAE to 0.442 (a
19.8% reduction) and increasing the consistency with human experts to 0.681 (a
58.7% improvement) compared to GPT-5. The benchmark is available at
https://github.com/HKUSTDial/VisJudgeBench.

</details>


### [37] [Confabulations from ACL Publications (CAP): A Dataset for Scientific Hallucination Detection](https://arxiv.org/abs/2510.22395)
*Federica Gamba,Aman Sinha,Timothee Mickus,Raul Vazquez,Patanjali Bhamidipati,Claudio Savelli,Ahana Chattopadhyay,Laura A. Zanella,Yash Kankanampati,Binesh Arakkal Remesh,Aryan Ashok Chandramania,Rohit Agarwal,Chuyuan Li,Ioana Buhnila,Radhika Mamidi*

Main category: cs.CL

TL;DR: CAP数据集是一个多语言资源，用于研究大语言模型在科学文本生成中的幻觉问题，涵盖9种语言和16个模型生成的7000多个答案。


<details>
  <summary>Details</summary>
Motivation: 科学领域中的幻觉会扭曲事实知识，而专业术语、统计推理和上下文依赖解释进一步加剧了这些扭曲，特别是考虑到LLMs缺乏真正理解、上下文理解有限以及偏向表面泛化。

Method: 创建包含900个科学问题和7000多个LLM生成答案的数据集，涵盖5种高资源语言和4种低资源语言，每个实例都标注了科学幻觉的二元标签和流畅度标签。

Result: CAP数据集公开发布，包含问题-答案对、token序列和对应logits，为幻觉检测、多语言LLM评估和可靠科学NLP系统开发提供支持。

Conclusion: CAP数据集有助于推进幻觉检测研究、多语言LLM评估，以及开发更可靠的科学NLP系统。

Abstract: We introduce the CAP (Confabulations from ACL Publications) dataset, a
multilingual resource for studying hallucinations in large language models
(LLMs) within scientific text generation. CAP focuses on the scientific domain,
where hallucinations can distort factual knowledge, as they frequently do. In
this domain, however, the presence of specialized terminology, statistical
reasoning, and context-dependent interpretations further exacerbates these
distortions, particularly given LLMs' lack of true comprehension, limited
contextual understanding, and bias toward surface-level generalization. CAP
operates in a cross-lingual setting covering five high-resource languages
(English, French, Hindi, Italian, and Spanish) and four low-resource languages
(Bengali, Gujarati, Malayalam, and Telugu). The dataset comprises 900 curated
scientific questions and over 7000 LLM-generated answers from 16 publicly
available models, provided as question-answer pairs along with token sequences
and corresponding logits. Each instance is annotated with a binary label
indicating the presence of a scientific hallucination, denoted as a factuality
error, and a fluency label, capturing issues in the linguistic quality or
naturalness of the text. CAP is publicly released to facilitate advanced
research on hallucination detection, multilingual evaluation of LLMs, and the
development of more reliable scientific NLP systems.

</details>


### [38] [CHOIR: Collaborative Harmonization fOr Inference Robustness](https://arxiv.org/abs/2510.22475)
*Xiangjue Dong,Cong Wang,Maria Teleki,Millennium Bismay,James Caverlee*

Main category: cs.CL

TL;DR: CHOIR是一个测试时框架，通过协调多个角色条件下的推理信号来提高LLM推理的鲁棒性，无需额外训练即可显著提升性能


<details>
  <summary>Details</summary>
Motivation: 角色分配LLM中的人口统计扰动（如代词变化）会导致推理轨迹变化和答案分歧，作者将这些变化视为改善推理鲁棒性的资源而非需要消除的偏见

Method: 提出CHOIR框架，在反事实角色之间进行协作解码，动态平衡推理路径中的一致性和分歧性

Result: 在多个推理基准测试中，CHOIR持续提升跨人口统计、模型架构、规模和任务的性能，个体人口群体改进最高达26.4%，五个人口统计平均改进19.2%

Conclusion: 通过将角色变化重新构建为建设性信号，CHOIR为更可靠的LLM推理提供了可扩展和通用的方法

Abstract: Persona-assigned Large Language Models (LLMs) can adopt diverse roles,
enabling personalized and context-aware reasoning. However, even minor
demographic perturbations in personas, such as simple pronoun changes, can
alter reasoning trajectories, leading to divergent sets of correct answers.
Instead of treating these variations as biases to be mitigated, we explore
their potential as a constructive resource to improve reasoning robustness. We
propose CHOIR (Collaborative Harmonization fOr Inference Robustness), a
test-time framework that harmonizes multiple persona-conditioned reasoning
signals into a unified prediction. CHOIR orchestrates a collaborative decoding
process among counterfactual personas, dynamically balancing agreement and
divergence in their reasoning paths. Experiments on various reasoning
benchmarks demonstrate that CHOIR consistently enhances performance across
demographics, model architectures, scales, and tasks - without additional
training. Improvements reach up to 26.4% for individual demographic groups and
19.2% on average across five demographics. It remains effective even when base
personas are suboptimal. By reframing persona variation as a constructive
signal, CHOIR provides a scalable and generalizable approach to more reliable
LLM reasoning.

</details>


### [39] [The Tonogenesis Continuum in Tibetan: A Computational Investigation](https://arxiv.org/abs/2510.22485)
*Siyu Liang,Zhaxi Zerong*

Main category: cs.CL

TL;DR: 提出一种计算方法来量化音高在声调生成过程中的功能作用，通过分析藏语方言对音高平坦化的敏感性，揭示了声调生成的连续体现象。


<details>
  <summary>Details</summary>
Motivation: 传统研究声调生成主要依赖比较重建和声学分析，需要更精确的方法来量化音高在不同阶段的功能作用。

Method: 使用自动语音识别模型，通过测量音高平坦化对ASR性能的影响来量化音高的功能作用，分析不同藏语方言对音高移除的敏感性。

Result: 发现声调生成连续体：无音调安多方言对音高移除容忍度最高，完全有声调卫藏方言性能严重下降，中间康方言介于两者之间。

Conclusion: 计算方法能捕捉声音变化的精细阶段，传统基于最小对立对的功能负载度量可能高估过渡系统中对音高的依赖程度。

Abstract: Tonogenesis-the historical process by which segmental contrasts evolve into
lexical tone-has traditionally been studied through comparative reconstruction
and acoustic phonetics. We introduce a computational approach that quantifies
the functional role of pitch at different stages of this sound change by
measuring how pitch manipulation affects automatic speech recognition (ASR)
performance. Through analysis on the sensitivity to pitch-flattening from a set
of closely related Tibetan languages, we find evidence of a tonogenesis
continuum: atonal Amdo dialects tolerate pitch removal the most, while fully
tonal U-Tsang varieties show severe degradation, and intermediate Kham dialects
fall measurably between these extremes. These gradient effects demonstrate how
ASR models implicitly learn the shifting functional load of pitch as languages
transition from consonant-based to tone-based lexical contrasts. Our findings
show that computational methods can capture fine-grained stages of sound change
and suggest that traditional functional load metrics, based solely on minimal
pairs, may overestimate pitch dependence in transitional systems where
segmental and suprasegmental cues remain phonetically intertwined.

</details>


### [40] [Frustratingly Easy Task-aware Pruning for Large Language Models](https://arxiv.org/abs/2510.22489)
*Yuanhe Tian,Junjie Liu,Xican Yang,Haishan Ye,Yan Song*

Main category: cs.CL

TL;DR: 提出了一种针对大语言模型的剪枝方法，通过结合通用领域和特定任务的特征分布来计算参数重要性，在压缩模型的同时保留任务特定能力。


<details>
  <summary>Details</summary>
Motivation: 传统剪枝方法主要关注保持模型生成流畅句子的能力，但忽视了在特定领域和任务上的性能表现。

Method: 分析传统剪枝方法在通用领域校准下的损失扰动最小化，将任务特定特征分布纳入现有剪枝算法的重要性计算中，使用通用和任务特定校准数据分别计算重要性分数，基于激活范数差异将参数分为共享组和专属组，融合分数指导剪枝过程。

Result: 在广泛使用的基准测试中，该方法在相同剪枝比例和不同设置下始终优于基线方法。

Conclusion: 该框架能够与各种基础剪枝技术无缝集成，在压缩过程中保留大语言模型的专门能力。

Abstract: Pruning provides a practical solution to reduce the resources required to run
large language models (LLMs) to benefit from their effective capabilities as
well as control their cost for training and inference. Research on LLM pruning
often ranks the importance of LLM parameters using their magnitudes and
calibration-data activations and removes (or masks) the less important ones,
accordingly reducing LLMs' size. However, these approaches primarily focus on
preserving the LLM's ability to generate fluent sentences, while neglecting
performance on specific domains and tasks. In this paper, we propose a simple
yet effective pruning approach for LLMs that preserves task-specific
capabilities while shrinking their parameter space. We first analyze how
conventional pruning minimizes loss perturbation under general-domain
calibration and extend this formulation by incorporating task-specific feature
distributions into the importance computation of existing pruning algorithms.
Thus, our framework computes separate importance scores using both general and
task-specific calibration data, partitions parameters into shared and exclusive
groups based on activation-norm differences, and then fuses their scores to
guide the pruning process. This design enables our method to integrate
seamlessly with various foundation pruning techniques and preserve the LLM's
specialized abilities under compression. Experiments on widely used benchmarks
demonstrate that our approach is effective and consistently outperforms the
baselines with identical pruning ratios and different settings.

</details>


### [41] [The Limits of Data Scaling: Sub-token Utilization and Acoustic Saturation in Multilingual ASR](https://arxiv.org/abs/2510.22492)
*Siyu Liang,Nicolas Ballier,Gina-Anne Levow,Richard Wright*

Main category: cs.CL

TL;DR: 分析Whisper多语言ASR模型在49种语言上的解码行为，发现子词发现率遵循指数饱和模式，提出声学饱和时间(AST)概念，表明子词利用更多受语言统计、类型和正字法结构影响，而非训练数据规模。


<details>
  <summary>Details</summary>
Motivation: 探究多语言ASR模型在不同语言中学习的子词库存如何被利用，以及预训练数据差异是否影响这些子词在推理时的使用模式。

Method: 通过记录Whisper模型在49种语言推理时的解码候选子词，跟踪其随时间累积发现情况，分析模型子词空间的利用模式。

Result: 发现子词总数与语言预训练时长无关，子词发现率遵循一致的指数饱和模式，拉丁文字语言比西里尔、CJK和闪米特文字语言表现更好。

Conclusion: 多语言ASR推理中的子词利用更多受语言统计、类型和正字法结构约束，而非训练数据规模，为更公平的语料构建和跨语言评估提供实证基础。

Abstract: How much audio is needed to fully observe a multilingual ASR model's learned
sub-token inventory across languages, and does data disparity in multilingual
pre-training affect how these tokens are utilized during inference? We address
this question by analyzing Whisper's decoding behavior during inference across
49 languages. By logging decoding candidate sub-tokens and tracking their
cumulative discovery over time, we study the utilization pattern of the model's
sub-token space. Results show that the total number of discovered tokens
remains largely independent of a language's pre-training hours, indicating that
data disparity does not strongly influence lexical diversity in the model's
hypothesis space. Sub-token discovery rates follow a consistent exponential
saturation pattern across languages, suggesting a stable time window after
which additional audio yields minimal new sub-token activation. We refer to
this convergence threshold as acoustic saturation time (AST). Further analyses
of rank-frequency distributions reveal Zipf-like patterns better modeled by a
Zipf-Mandelbrot law, and mean sub-token length shows a positive correlation
with resource level. Additionally, those metrics show more favorable patterns
for languages in the Latin script than those in scripts such as Cyrillic, CJK,
and Semitic. Together, our study suggests that sub-token utilization during
multilingual ASR inference is constrained more by the statistical, typological,
and orthographic structure of the speech than by training data scale, providing
an empirical basis for more equitable corpus construction and cross-lingual
evaluation.

</details>


### [42] [A Sociophonetic Analysis of Racial Bias in Commercial ASR Systems Using the Pacific Northwest English Corpus](https://arxiv.org/abs/2510.22495)
*Michael Scott,Siyu Liang,Alicia Wassink,Gina-Anne Levow*

Main category: cs.CL

TL;DR: 对四个主要商业ASR系统的种族偏见进行系统评估，发现语音质量变异是偏见的主要来源，特别是非洲裔美国说话者受影响最严重。


<details>
  <summary>Details</summary>
Motivation: 评估商业自动语音识别系统中存在的种族偏见，分析不同种族背景说话者的转录准确性差异。

Method: 使用太平洋西北英语语料库，分析四个种族群体的转录准确性，引入语音错误率指标，分析11个社会语音特征。

Result: 发现元音质量变异，特别是对低后元音合并和鼻前元音合并模式的抵抗，与不同种族群体的错误率差异相关，非洲裔美国说话者在所有系统中受影响最严重。

Conclusion: 方言语音变异的声学建模，而非词汇或句法因素，是商业ASR系统偏见的主要来源，PNWE语料库为语音技术偏见评估提供了宝贵资源。

Abstract: This paper presents a systematic evaluation of racial bias in four major
commercial automatic speech recognition (ASR) systems using the Pacific
Northwest English (PNWE) corpus. We analyze transcription accuracy across
speakers from four ethnic backgrounds (African American, Caucasian American,
ChicanX, and Yakama) and examine how sociophonetic variation contributes to
differential system performance. We introduce a heuristically-determined
Phonetic Error Rate (PER) metric that links recognition errors to specific
linguistically motivated variables derived from sociophonetic annotation. Our
analysis of eleven sociophonetic features reveals that vowel quality variation,
particularly resistance to the low-back merger and pre-nasal merger patterns,
is systematically associated with differential error rates across ethnic
groups, with the most pronounced effects for African American speakers across
all evaluated systems. These findings demonstrate that acoustic modeling of
dialectal phonetic variation, rather than lexical or syntactic factors, remains
a primary source of bias in commercial ASR systems. The study establishes the
PNWE corpus as a valuable resource for bias evaluation in speech technologies
and provides actionable guidance for improving ASR performance through targeted
representation of sociophonetic diversity in training data.

</details>


### [43] [Text to Trust: Evaluating Fine-Tuning and LoRA Trade-offs in Language Models for Unfair Terms of Service Detection](https://arxiv.org/abs/2510.22531)
*Noshitha Padma Pratyusha Juttu,Sahithi Singireddy,Sravani Gona,Sujal Timilsina*

Main category: cs.CL

TL;DR: 系统评估了在服务条款不公平条款检测任务中，全量微调、参数高效适配（LoRA、QLoRA）和零样本提示三种方法的性能表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在文本理解方面表现出色，但在专业法律领域的应用受到全量微调成本高昂的限制，需要探索更高效的适配方法。

Method: 对BERT和DistilBERT进行全量微调，对TinyLlama、LLaMA 3B/7B和SaulLM应用4位低秩适配（LoRA），并在零样本设置下评估GPT-4o及其变体。

Result: 全量微调在精确率-召回率平衡方面表现最佳，而基于LoRA的模型在内存成本降低3倍的情况下仍能提供有竞争力的召回率。

Conclusion: 研究揭示了高效领域适配LLM的实际设计权衡，为法律文本处理中的微调研究提供了开放的基准。

Abstract: Large Language Models (LLMs) have transformed text understanding, yet their
adaptation to specialized legal domains remains constrained by the cost of full
fine-tuning. This study provides a systematic evaluation of fine tuning,
parameter efficient adaptation (LoRA, QLoRA), and zero-shot prompting
strategies for unfair clause detection in Terms of Service (ToS) documents, a
key application in legal NLP. We finetune BERT and DistilBERT, apply 4-bit
Low-Rank Adaptation (LoRA) to models such as TinyLlama, LLaMA 3B/7B, and
SaulLM, and evaluate GPT-4o and O-versions in zero-shot settings. Experiments
on the CLAUDETTE-ToS benchmark and the Multilingual Scraper Corpus show that
full fine-tuning achieves the strongest precision recall balance, while
LoRA-based models provide competitive recall with up to 3x lower memory cost.
These findings highlight practical design trade-offs for efficient and
domain-adapted LLMs, contributing open baselines for fine-tuning research in
legal text processing.

</details>


### [44] [LooGLE v2: Are LLMs Ready for Real World Long Dependency Challenges?](https://arxiv.org/abs/2510.22548)
*Ziyuan He,Yuxuan Wang,Jiaqi Li,Kexin Liang,Muhan Zhang*

Main category: cs.CL

TL;DR: LooGLE v2是一个评估大语言模型在真实世界长上下文应用中能力的基准测试，包含16k到2M token的文本，涵盖法律、金融、游戏和代码等领域，结果显示即使最佳模型也仅获得59.2%的分数。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型的上下文窗口不断扩展，但它们在长依赖任务上的理解能力仍然有限且未被充分探索，特别是在许多真实世界长上下文应用中。

Method: 构建LooGLE v2基准，包含自动收集的真实世界长文本，精心设计10种领域特定的长依赖任务，通过可扩展的数据管理流程生成1,934个QA实例。

Result: 对6个本地部署和4个API基础的LLM进行全面评估，结果显示即使最佳模型也仅获得59.2%的总体分数，表明模型实际能理解的上下文长度远低于其声称的能力。

Conclusion: 尽管上下文窗口广泛，但流行的大语言模型只能理解比声称短得多的上下文长度，在处理具有长依赖关系的真实世界任务方面存在显著限制，表明在实际长上下文理解方面还有很大的改进空间。

Abstract: Large language models (LLMs) are equipped with increasingly extended context
windows recently, yet their long context understanding capabilities over long
dependency tasks remain fundamentally limited and underexplored. This gap is
especially significant in many real-world long-context applications that were
rarely benchmarked. In this paper, we introduce LooGLE v2, a novel benchmark
designed to evaluate LLMs' long context ability in real-world applications and
scenarios. Our benchmark consists of automatically collected real-world long
texts, ranging from 16k to 2M tokens, encompassing domains in law, finance,
game and code. Accordingly, we delicately design 10 types of domain-specific
long-dependency tasks and generate 1,934 QA instances with various diversity
and complexity in a scalable data curation pipeline for further practical
needs. We conduct a comprehensive assessment of 6 locally deployed and 4
API-based LLMs. The evaluation results show that even the best-performing model
achieves only a 59.2% overall score on our benchmark. Despite the extensive
context windows, popular LLMs are only capable of understanding a much shorter
length of context than they claim to be, revealing significant limitations in
their ability to handle real-world tasks with long dependencies and
highlighting substantial room for model improvement in practical long-context
understanding.

</details>


### [45] [SABlock: Semantic-Aware KV Cache Eviction with Adaptive Compression Block Size](https://arxiv.org/abs/2510.22556)
*Jinhan Chen,Jianchun Liu,Hongli Xu,Xianjun Gao,Shilong Wang*

Main category: cs.CL

TL;DR: SABlock是一个语义感知的KV缓存驱逐框架，通过自适应块大小来平衡语义连贯性和内存效率，显著减少长上下文LLM推理中的内存占用。


<details>
  <summary>Details</summary>
Motivation: KV缓存不断增长的内存占用已成为长上下文LLM推理的严重可扩展性瓶颈，现有压缩方法难以平衡语义连贯性和内存效率。

Method: SABlock首先进行语义分割以对齐压缩边界与语言结构，然后应用分段引导的token评分来优化重要性估计，最后通过预算驱动的搜索策略自适应确定最优块大小。

Result: 在长上下文基准测试中，SABlock在相同内存预算下始终优于最先进的基线方法。例如在NIAH测试中，仅用96个KV条目就达到99.9%的检索准确率，接近保留8K条目的全缓存性能。在1024的固定缓存预算下，峰值内存使用减少46.28%，在128K上下文长度下解码速度提升高达9.5倍。

Conclusion: SABlock通过语义感知的自适应块大小策略，有效解决了KV缓存的内存瓶颈问题，在保持语义完整性的同时显著提升了压缩效率和推理性能。

Abstract: The growing memory footprint of the Key-Value (KV) cache poses a severe
scalability bottleneck for long-context Large Language Model (LLM) inference.
While KV cache eviction has emerged as an effective solution by discarding less
critical tokens, existing token-, block-, and sentence-level compression
methods struggle to balance semantic coherence and memory efficiency. To this
end, we introduce SABlock, a \underline{s}emantic-aware KV cache eviction
framework with \underline{a}daptive \underline{block} sizes. Specifically,
SABlock first performs semantic segmentation to align compression boundaries
with linguistic structures, then applies segment-guided token scoring to refine
token importance estimation. Finally, for each segment, a budget-driven search
strategy adaptively determines the optimal block size that preserves semantic
integrity while improving compression efficiency under a given cache budget.
Extensive experiments on long-context benchmarks demonstrate that SABlock
consistently outperforms state-of-the-art baselines under the same memory
budgets. For instance, on Needle-in-a-Haystack (NIAH), SABlock achieves 99.9%
retrieval accuracy with only 96 KV entries, nearly matching the performance of
the full-cache baseline that retains up to 8K entries. Under a fixed cache
budget of 1,024, SABlock further reduces peak memory usage by 46.28% and
achieves up to 9.5x faster decoding on a 128K context length.

</details>


### [46] [A Closed-Loop Personalized Learning Agent Integrating Neural Cognitive Diagnosis, Bounded-Ability Adaptive Testing, and LLM-Driven Feedback](https://arxiv.org/abs/2510.22559)
*Zhifeng Wang,Xinyue Zheng,Chunyan Zeng*

Main category: cs.CL

TL;DR: 提出了一个端到端的个性化学习代理EduLoop-Agent，通过神经认知诊断模型、自适应测试策略和大型语言模型形成诊断-推荐-反馈的闭环框架，在ASSISTments数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 当前个性化学习方法将建模、题目选择和反馈孤立处理，导致学生模型粗糙、适应性假设受限以及反馈缺乏针对性。需要构建一个闭环系统来提供真正个性化的学习体验。

Method: 集成三个核心组件：神经认知诊断模型(NCD)提供细粒度知识掌握度评估；有界能力估计计算机自适应测试策略(BECAT)动态选择题目；大型语言模型(LLM)生成结构化可操作反馈。

Result: NCD模块在响应预测方面表现优异且提供可解释的掌握度评估；自适应推荐策略提高了题目相关性和个性化程度；基于LLM的反馈能够针对已识别的弱点提供学习指导。

Conclusion: 该设计有效且可实际部署，为智能教育中生成个性化学习路径提供了可行方案。

Abstract: As information technology advances, education is moving from
one-size-fits-all instruction toward personalized learning. However, most
methods handle modeling, item selection, and feedback in isolation rather than
as a closed loop. This leads to coarse or opaque student models,
assumption-bound adaptivity that ignores diagnostic posteriors, and generic,
non-actionable feedback. To address these limitations, this paper presents an
end-to-end personalized learning agent, EduLoop-Agent, which integrates a
Neural Cognitive Diagnosis model (NCD), a Bounded-Ability Estimation
Computerized Adaptive Testing strategy (BECAT), and large language models
(LLMs). The NCD module provides fine-grained estimates of students' mastery at
the knowledge-point level; BECAT dynamically selects subsequent items to
maximize relevance and learning efficiency; and LLMs convert diagnostic signals
into structured, actionable feedback. Together, these components form a
closed-loop framework of ``Diagnosis--Recommendation--Feedback.'' Experiments
on the ASSISTments dataset show that the NCD module achieves strong performance
on response prediction while yielding interpretable mastery assessments. The
adaptive recommendation strategy improves item relevance and personalization,
and the LLM-based feedback offers targeted study guidance aligned with
identified weaknesses. Overall, the results indicate that the proposed design
is effective and practically deployable, providing a feasible pathway to
generating individualized learning trajectories in intelligent education.

</details>


### [47] [Pedagogy-driven Evaluation of Generative AI-powered Intelligent Tutoring Systems](https://arxiv.org/abs/2510.22581)
*Kaushal Kumar Maurya,Ekaterina Kochmar*

Main category: cs.CL

TL;DR: 该论文分析了AI教育领域智能辅导系统的评估挑战，指出缺乏标准化评估框架的问题，并提出了三个基于学习科学原则的研究方向来建立公平、统一和可扩展的评估方法。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型加速了基于大语言模型的智能辅导系统发展，但这些系统的进展和影响难以追踪，因为缺乏可靠、普遍接受且以教学为导向的评估框架和基准。

Method: 通过真实案例研究分析现有评估实践，基于跨学科AI教育研究的见解，提出三个实践可行且理论扎实的研究方向。

Result: 识别了当前教育对话式智能辅导系统评估中的不一致性和有限泛化性问题，大多数评估依赖主观协议和非标准化基准。

Conclusion: 需要建立基于学习科学原则的公平、统一和可扩展的智能辅导系统评估方法学，以解决当前评估框架的不足。

Abstract: The interdisciplinary research domain of Artificial Intelligence in Education
(AIED) has a long history of developing Intelligent Tutoring Systems (ITSs) by
integrating insights from technological advancements, educational theories, and
cognitive psychology. The remarkable success of generative AI (GenAI) models
has accelerated the development of large language model (LLM)-powered ITSs,
which have potential to imitate human-like, pedagogically rich, and cognitively
demanding tutoring. However, the progress and impact of these systems remain
largely untraceable due to the absence of reliable, universally accepted, and
pedagogy-driven evaluation frameworks and benchmarks. Most existing educational
dialogue-based ITS evaluations rely on subjective protocols and
non-standardized benchmarks, leading to inconsistencies and limited
generalizability. In this work, we take a step back from mainstream ITS
development and provide comprehensive state-of-the-art evaluation practices,
highlighting associated challenges through real-world case studies from careful
and caring AIED research. Finally, building on insights from previous
interdisciplinary AIED research, we propose three practical, feasible, and
theoretically grounded research directions, rooted in learning science
principles and aimed at establishing fair, unified, and scalable evaluation
methodologies for ITSs.

</details>


### [48] [AutoBench: Automating LLM Evaluation through Reciprocal Peer Assessment](https://arxiv.org/abs/2510.22593)
*Dario Loi,Elena Maria Muià,Federico Siciliano,Giovanni Trappolini,Vincenzo Crisà,Peter Kruger,Fabrizio Silvestri*

Main category: cs.CL

TL;DR: AutoBench是一个完全自动化的框架，通过相互同行评估来评估大语言模型，解决了静态基准测试的测试集污染和适应性有限的问题。


<details>
  <summary>Details</summary>
Motivation: 静态基准测试存在测试集污染和适应性有限的问题，需要一种动态、可扩展且抗污染的评估方法。

Method: AutoBench通过让模型交替担任问题生成器、参赛者和评委，动态生成新颖评估任务，并使用迭代加权机制聚合同行判断形成共识排名。

Result: 实验显示与MMLU-Pro和GPQA基准有强相关性（分别为78%和63%），多评委设计显著优于单评委基线。

Conclusion: AutoBench为持续评估演进的语言模型提供了一个可扩展、抗污染的替代方案，分布式评估产生更稳健和人类一致的评估。

Abstract: We present AutoBench, a fully automated and self-sustaining framework for
evaluating Large Language Models (LLMs) through reciprocal peer assessment.
This paper provides a rigorous scientific validation of the AutoBench
methodology, originally developed as an open-source project by eZecute S.R.L..
Unlike static benchmarks that suffer from test-set contamination and limited
adaptability, AutoBench dynamically generates novel evaluation tasks while
models alternately serve as question generators, contestants, and judges across
diverse domains. An iterative weighting mechanism amplifies the influence of
consistently reliable evaluators, aggregating peer judgments into
consensus-based rankings that reflect collective model agreement. Our
experiments demonstrate strong correlations with established benchmarks
including MMLU-Pro and GPQA (respectively 78\% and 63\%), validating this
peer-driven evaluation paradigm. The multi-judge design significantly
outperforms single-judge baselines, confirming that distributed evaluation
produces more robust and human-consistent assessments. AutoBench offers a
scalable, contamination-resistant alternative to static benchmarks for the
continuous evaluation of evolving language models.

</details>


### [49] [Personal Care Utility (PCU): Building the Health Infrastructure for Everyday Insight and Guidance](https://arxiv.org/abs/2510.22602)
*Mahyar Abbasian,Ramesh Jain*

Main category: cs.CL

TL;DR: 提出了个人护理工具（PCU）——一个基于AI的全球性健康指导系统，通过整合多模态数据、知识和服务，为个人和群体提供持续的健康管理支持。


<details>
  <summary>Details</summary>
Motivation: 基于数字基础设施和生物医学创新的成功经验，旨在解决传统医疗的间歇性护理问题，提供持续、个性化的健康指导。

Method: 采用多模态代理、事件中心建模和上下文推理技术，整合个人感知、体验计算和群体分析，构建一个环境适应性健康伴侣系统。

Result: PCU具备三个核心能力：个性化可信健康信息、主动健康导航和行为指导、医疗事件后的恢复和治疗反应持续解读。

Conclusion: PCU代表了一种新兴的健康管理范式，不仅改善个人健康结果，还为公共卫生和科学发现提供了新基础，但面临架构设计和实施挑战。

Abstract: Building on decades of success in digital infrastructure and biomedical
innovation, we propose the Personal Care Utility (PCU) - a cybernetic system
for lifelong health guidance. PCU is conceived as a global, AI-powered utility
that continuously orchestrates multimodal data, knowledge, and services to
assist individuals and populations alike. Drawing on multimodal agents,
event-centric modeling, and contextual inference, it offers three essential
capabilities: (1) trusted health information tailored to the individual, (2)
proactive health navigation and behavior guidance, and (3) ongoing
interpretation of recovery and treatment response after medical events. Unlike
conventional episodic care, PCU functions as an ambient, adaptive companion -
observing, interpreting, and guiding health in real time across daily life. By
integrating personal sensing, experiential computing, and population-level
analytics, PCU promises not only improved outcomes for individuals but also a
new substrate for public health and scientific discovery. We describe the
architecture, design principles, and implementation challenges of this emerging
paradigm.

</details>


### [50] [PerCoR: Evaluating Commonsense Reasoning in Persian via Multiple-Choice Sentence Completion](https://arxiv.org/abs/2510.22616)
*Morteza Alikhani,Mohammadtaha Bagherifard,Erfan Zinvandi,Mehran Sarmadi*

Main category: cs.CL

TL;DR: PerCoR是首个波斯语常识推理大规模基准数据集，包含10.6万个多项选择题，采用新颖的连接词分割策略生成连贯句子对，并使用DRESS-AF方法创建具有挑战性的干扰项。


<details>
  <summary>Details</summary>
Motivation: 填补波斯语常识推理基准数据集的空白，为波斯语自然语言处理研究提供评估标准。

Method: 使用连接词分割策略生成句子对，提出DRESS-AF（基于嵌入相似性评分和对抗性过滤的干扰项排序）方法从正确答案池中选择干扰项以最大化模型混淆。

Result: 人类标注者得分89%，OpenAI-o3表现最佳达92.18%，Claude-Sonnet-3.7为91.17%，最强开源模型DeepSeek-R1达到82.51%。DRESS-AF方法在英语HellaSwag基准上也有效。

Conclusion: PerCoR是一个具有挑战性的波斯语常识推理基准，显示了当前模型在波斯语常识推理方面仍存在性能差距，DRESS-AF方法可有效提升基准难度。

Abstract: We introduced PerCoR (Persian Commonsense Reasoning), the first large-scale
Persian benchmark for commonsense reasoning. PerCoR contains 106K
multiple-choice sentence-completion problems drawn from more than forty news,
cultural, and other web sources. We introduce a novel conjunction-based
segmentation strategy to generate coherent sentence-completion pairs, enabling
broad topical and structural diversity. To create challenging distractors, we
propose DRESS-AF (Distractor Ranking via Embedding Similarity Scoring and
Adversarial Filtering), a generation-free adversarial filtering method that
selects distractors from the pool of gold continuations while maximising model
confusion. Human annotators score 89% on PerCoR, while OpenAI-o3 achieves the
highest performance at 92.18%, followed closely by Claude-Sonnet-3.7 (91.17%).
The strongest open-source model, DeepSeek-R1, reaches 82.51%, underscoring both
the dataset's difficulty and the remaining performance gap in Persian
commonsense reasoning. We further show that DRESS-AF transfers to the English
HellaSwag benchmark, increasing its difficulty without hurting human
solvability. The dataset is available at
https://huggingface.co/datasets/MCINext/PerCoR.

</details>


### [51] [Integrating Linguistics and AI: Morphological Analysis and Corpus development of Endangered Toto Language of West Bengal](https://arxiv.org/abs/2510.22629)
*Ambalika Guha,Sajal Saha,Debanjan Ballav,Soumi Mitra,Hritwick Chakraborty*

Main category: cs.CL

TL;DR: 开发一个三语（Toto-孟加拉语-英语）语言学习应用，通过数字化存档和推广濒危的Toto语言，结合AI技术实现语言保护。


<details>
  <summary>Details</summary>
Motivation: 保护语言多样性，Toto语言作为印度西孟加拉邦的濒危语言需要数字化保护和推广。

Method: 通过田野调查收集语言数据，创建词素标注的三语语料库，训练小型语言模型和基于Transformer的翻译引擎，分析屈折形态和派生策略。

Result: 开发了包含标准化文字和数字素养工具的语言学习应用，建立了可持续的濒危语言保护模型。

Conclusion: 将传统语言学方法与AI技术结合，为基于社区的语言复兴提供了跨学科合作的价值。

Abstract: Preserving linguistic diversity is necessary as every language offers a
distinct perspective on the world. There have been numerous global initiatives
to preserve endangered languages through documentation. This paper is a part of
a project which aims to develop a trilingual (Toto-Bangla-English) language
learning application to digitally archive and promote the endangered Toto
language of West Bengal, India. This application, designed for both native Toto
speakers and non-native learners, aims to revitalize the language by ensuring
accessibility and usability through Unicode script integration and a structured
language corpus. The research includes detailed linguistic documentation
collected via fieldwork, followed by the creation of a morpheme-tagged,
trilingual corpus used to train a Small Language Model (SLM) and a
Transformer-based translation engine. The analysis covers inflectional
morphology such as person-number-gender agreement, tense-aspect-mood
distinctions, and case marking, alongside derivational strategies that reflect
word-class changes. Script standardization and digital literacy tools were also
developed to enhance script usage. The study offers a sustainable model for
preserving endangered languages by incorporating traditional linguistic
methodology with AI. This bridge between linguistic research with technological
innovation highlights the value of interdisciplinary collaboration for
community-based language revitalization.

</details>


### [52] [Culturally Grounded Physical Commonsense Reasoning in Italian and English: A Submission to the MRL 2025 Shared Task](https://arxiv.org/abs/2510.22631)
*Marco De Santis,Lisa Alazraki*

Main category: cs.CL

TL;DR: 提出了FormaMentis，一个基于意大利语言和文化的物理常识推理新基准，作为MRL 2025多语言物理推理数据集共享任务的提交成果。


<details>
  <summary>Details</summary>
Motivation: 创建非英语语言的物理常识推理评估数据，遵循类似PIQA的格式，特别关注意大利语言和文化背景。

Method: 由母语为意大利语且熟悉当地习俗的专家注释者创建数据样本，并将样本翻译成英语同时保留意大利特有的文化元素。

Result: 开发了FormaMentis基准，包含基于意大利文化和语言的物理常识推理数据集。

Conclusion: FormaMentis为物理常识推理领域提供了文化特定的多语言评估资源，填补了非英语语言数据的空白。

Abstract: This paper presents our submission to the MRL 2025 Shared Task on
Multilingual Physical Reasoning Datasets. The objective of the shared task is
to create manually-annotated evaluation data in the physical commonsense
reasoning domain, for languages other than English, following a format similar
to PIQA. Our contribution, FormaMentis, is a novel benchmark for physical
commonsense reasoning that is grounded in Italian language and culture. The
data samples in FormaMentis are created by expert annotators who are native
Italian speakers and are familiar with local customs and norms. The samples are
additionally translated into English, while preserving the cultural elements
unique to the Italian context.

</details>


### [53] [Conjugate Relation Modeling for Few-Shot Knowledge Graph Completion](https://arxiv.org/abs/2510.22656)
*Zilong Wang,Qingtian Zeng,Hua Duan,Cheng Cheng,Minghao Zou,Ziyang Wang*

Main category: cs.CL

TL;DR: 提出CR-FKGC框架，通过邻域聚合编码器、共轭关系学习器和流形共轭解码器解决少样本知识图谱补全中的复杂关系模式和数据稀疏问题。


<details>
  <summary>Details</summary>
Motivation: 现有少样本知识图谱补全方法难以捕捉复杂关系模式并缓解数据稀疏性，需要新的解决方案。

Method: 使用邻域聚合编码器整合高阶邻居信息，共轭关系学习器结合隐式条件扩散关系模块和稳定关系模块来捕捉稳定语义和不确定性偏移，流形共轭解码器在流形空间中高效评估和推理缺失三元组。

Result: 在三个基准测试上的实验表明，该方法优于现有最先进方法。

Conclusion: CR-FKGC框架有效解决了少样本知识图谱补全中的关键挑战，取得了优越性能。

Abstract: Few-shot Knowledge Graph Completion (FKGC) infers missing triples from
limited support samples, tackling long-tail distribution challenges. Existing
methods, however, struggle to capture complex relational patterns and mitigate
data sparsity. To address these challenges, we propose a novel FKGC framework
for conjugate relation modeling (CR-FKGC). Specifically, it employs a
neighborhood aggregation encoder to integrate higher-order neighbor
information, a conjugate relation learner combining an implicit conditional
diffusion relation module with a stable relation module to capture stable
semantics and uncertainty offsets, and a manifold conjugate decoder for
efficient evaluation and inference of missing triples in manifold space.
Experiments on three benchmarks demonstrate that our method achieves superior
performance over state-of-the-art methods.

</details>


### [54] [Rule-Based Explanations for Retrieval-Augmented LLM Systems](https://arxiv.org/abs/2510.22689)
*Joel Rorseth,Parke Godfrey,Lukasz Golab,Divesh Srivastava,Jarek Szlichta*

Main category: cs.CL

TL;DR: 本文提出了首个用于解释检索增强生成（RAG）大语言模型的方法，通过生成if-then规则来揭示输出与检索来源之间的因果关系。


<details>
  <summary>Details</summary>
Motivation: 随着RAG技术在LLM中的应用日益广泛，需要解释模型输出与检索信息源之间的关联，以增强模型透明度和可信度。

Method: 采用类似Apriori算法的剪枝优化策略，通过探测LLM对不同源组合的响应来生成解释性规则，避免暴力枚举所有可能组合。

Result: 实验证明该方法能有效生成解释RAG模型输出的规则，并在效率上优于暴力方法。

Conclusion: 提出的规则生成方法为解释RAG增强的LLM提供了有效工具，有助于理解模型决策过程和提高系统透明度。

Abstract: If-then rules are widely used to explain machine learning models; e.g., "if
employed = no, then loan application = rejected." We present the first proposal
to apply rules to explain the emerging class of large language models (LLMs)
with retrieval-augmented generation (RAG). Since RAG enables LLM systems to
incorporate retrieved information sources at inference time, rules linking the
presence or absence of sources can explain output provenance; e.g., "if a Times
Higher Education ranking article is retrieved, then the LLM ranks Oxford
first." To generate such rules, a brute force approach would probe the LLM with
all source combinations and check if the presence or absence of any sources
leads to the same output. We propose optimizations to speed up rule generation,
inspired by Apriori-like pruning from frequent itemset mining but redefined
within the scope of our novel problem. We conclude with qualitative and
quantitative experiments demonstrating our solutions' value and efficiency.

</details>


### [55] [SALSA: Single-pass Autoregressive LLM Structured Classification](https://arxiv.org/abs/2510.22691)
*Ruslan Berdichevsky,Shai Nahum-Gefen,Elad Ben Zaken*

Main category: cs.CL

TL;DR: SALSA是一个结合结构化提示、类别到标记映射和参数高效微调的文本分类方法，避免了冷启动训练，在单次前向传播中实现高效准确的分类。


<details>
  <summary>Details</summary>
Motivation: 尽管指令调优的大语言模型具有强大的泛化能力，但在文本分类基准测试中表现不佳，需要改进其分类性能。

Method: 将每个类别标签映射到不同的输出标记，构建提示以引出单标记响应，在推理时仅将模型输出投影到相关类别标记的logits上。

Result: SALSA在多个基准测试中取得了最先进的结果，证明了其在基于LLM的分类应用中的鲁棒性和可扩展性。

Conclusion: SALSA通过结构化提示和参数高效微调的连贯流程，显著提升了LLM在文本分类任务上的性能。

Abstract: Despite their impressive generalization capabilities, instruction-tuned Large
Language Models often underperform on text classification benchmarks. We
introduce SALSA, a coherent pipeline that combines structured prompting,
class-to-token mapping, and parameter-efficient fine-tuning, thereby avoiding
cold-start training. Each class label is mapped to a distinct output token, and
prompts are constructed to elicit a single-token response. During inference,
the model's output is projected only onto the logits of the relevant class
tokens, enabling efficient and accurate classification in a single forward
pass. SALSA achieves state-of-the-art results across diverse benchmarks,
demonstrating its robustness and scalability for LLM-based classification
applications.

</details>


### [56] [$\text{E}^2\text{Rank}$: Your Text Embedding can Also be an Effective and Efficient Listwise Reranker](https://arxiv.org/abs/2510.22733)
*Qi Liu,Yanzhao Zhang,Mingxin Li,Dingkun Long,Pengjun Xie,Jiaxin Mao*

Main category: cs.CL

TL;DR: 提出了E²Rank框架，通过继续训练单个文本嵌入模型，使其能够同时执行高质量检索和列表重排序，在保持效率的同时显著提升重排序性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本嵌入模型在检索应用中效率高但排序保真度有限，而专门的LLM列表重排序器虽然性能更好但效率较低。需要统一框架来兼顾效率和效果。

Method: 使用查询和文档嵌入的余弦相似度作为统一排序函数，构建包含原始查询和候选文档的列表排序提示，类似于传统检索中的伪相关反馈机制。

Result: 在BEIR重排序基准上达到最先进结果，在BRIGHT推理密集型基准上表现有竞争力，重排序延迟很低，同时MTEB基准上的嵌入性能也有所提升。

Conclusion: 单个嵌入模型可以有效统一检索和重排序任务，在保持计算效率的同时提供有竞争力的排序准确性。

Abstract: Text embedding models serve as a fundamental component in real-world search
applications. By mapping queries and documents into a shared embedding space,
they deliver competitive retrieval performance with high efficiency. However,
their ranking fidelity remains limited compared to dedicated rerankers,
especially recent LLM-based listwise rerankers, which capture fine-grained
query-document and document-document interactions. In this paper, we propose a
simple yet effective unified framework $\text{E}^2\text{Rank}$, means Efficient
Embedding-based Ranking (also means Embedding-to-Rank), which extends a single
text embedding model to perform both high-quality retrieval and listwise
reranking through continued training under a listwise ranking objective,
thereby achieving strong effectiveness with remarkable efficiency. By applying
cosine similarity between the query and document embeddings as a unified
ranking function, the listwise ranking prompt, which is constructed from the
original query and its candidate documents, serves as an enhanced query
enriched with signals from the top-K documents, akin to pseudo-relevance
feedback (PRF) in traditional retrieval models. This design preserves the
efficiency and representational quality of the base embedding model while
significantly improving its reranking performance. Empirically,
$\textrm{E}^2\text{Rank}$ achieves state-of-the-art results on the BEIR
reranking benchmark and demonstrates competitive performance on the
reasoning-intensive BRIGHT benchmark, with very low reranking latency. We also
show that the ranking training process improves embedding performance on the
MTEB benchmark. Our findings indicate that a single embedding model can
effectively unify retrieval and reranking, offering both computational
efficiency and competitive ranking accuracy.

</details>


### [57] [Low-Resource Dialect Adaptation of Large Language Models: A French Dialect Case-Study](https://arxiv.org/abs/2510.22747)
*Eeham Khan,Firas Saidani,Owen Van Esbroeck,Richard Khoury,Leila Kosseim*

Main category: cs.CL

TL;DR: 使用低秩适应和计算高效的持续预训练，在有限数据和计算预算下将大语言模型适配到魁北克法语方言，显著提升方言基准表现，同时保持主流语言基准的回归最小。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型主要局限于高资源语言的问题，探索如何在有限数据和计算资源下将模型适配到低资源方言，为少数语言群体提供高质量语言模型访问。

Method: 采用低秩适应和计算高效的持续预训练方法，使用非常小的数据集对三个大语言模型进行魁北克法语方言适配，并在COLE套件上进行基准测试。

Result: 实验显示在方言基准上有所改进，主流语言基准回归最小，仅更新不到1%的模型参数。结果增益高度依赖于语料库组成。

Conclusion: 参数高效微调的持续预训练可以通过成本效益高且可持续的语言资源创建来缩小方言差距，扩展高质量大语言模型对少数语言群体的访问。

Abstract: Despite the widespread adoption of large language models (LLMs), their
strongest capabilities remain largely confined to a small number of
high-resource languages for which there is abundant training data. Recently,
continual pre-training (CPT) has emerged as a means to fine-tune these models
to low-resource regional dialects. In this paper, we study the use of CPT for
dialect learning under tight data and compute budgets. Using low-rank
adaptation (LoRA) and compute-efficient continual pre-training, we adapt three
LLMs to the Qu\'ebec French dialect using a very small dataset and benchmark
them on the COLE suite. Our experiments demonstrate an improvement on the
minority dialect benchmarks with minimal regression on the prestige language
benchmarks with under 1% of model parameters updated. Analysis of the results
demonstrate that gains are highly contingent on corpus composition. These
findings indicate that CPT with parameter-efficient fine-tuning (PEFT) can
narrow the dialect gap by providing cost-effective and sustainable language
resource creation, expanding high-quality LLM access to minority linguistic
communities. We release the first Qu\'ebec French LLMs on HuggingFace.

</details>


### [58] [Beyond Semantics: How Temporal Biases Shape Retrieval in Transformer and State-Space Models](https://arxiv.org/abs/2510.22752)
*Anooshka Bajaj,Deven Mahesh Mistry,Sahaj Singh Maini,Yash Aggarwal,Zoran Tiganj*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在上下文学习中的时间偏差，发现模型倾向于检索序列开头和结尾的信息，中间信息检索可靠性较低，这与人类情景记忆的时间分离机制类似。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型如何像人类情景记忆一样，通过时间分离来区分和检索不同时间发生的事件，理解上下文学习中的时间偏差机制。

Method: 通过在序列中固定重复标记的位置并置换其他标记，消除语义干扰，隔离时间因素对下一个标记预测的影响。使用包括transformer和状态空间模型在内的多种预训练模型进行实验。

Result: 模型始终对重复标记后的标记赋予最高概率，但存在显著偏向序列开头或结尾的偏差。消融实验表明transformer中的这一现象与归纳头相关。状态空间模型和transformer模型表现出相似的时间偏差。

Conclusion: 研究深化了对上下文学习中时间偏差的理解，展示了这些偏差如何实现时间分离和情景检索，为理解LLM的记忆机制提供了新视角。

Abstract: In-context learning is governed by both temporal and semantic relationships,
shaping how Large Language Models (LLMs) retrieve contextual information.
Analogous to human episodic memory, where the retrieval of specific events is
enabled by separating events that happened at different times, this work probes
the ability of various pretrained LLMs, including transformer and state-space
models, to differentiate and retrieve temporally separated events.
Specifically, we prompted models with sequences containing multiple
presentations of the same token, which reappears at the sequence end. By fixing
the positions of these repeated tokens and permuting all others, we removed
semantic confounds and isolated temporal effects on next-token prediction.
Across diverse sequences, models consistently placed the highest probabilities
on tokens following a repeated token, but with a notable bias for those nearest
the beginning or end of the input. An ablation experiment linked this
phenomenon in transformers to induction heads. Extending the analysis to unique
semantic contexts with partial overlap further demonstrated that memories
embedded in the middle of a prompt are retrieved less reliably. Despite
architectural differences, state-space and transformer models showed comparable
temporal biases. Our findings deepen the understanding of temporal biases in
in-context learning and offer an illustration of how these biases can enable
temporal separation and episodic retrieval.

</details>


### [59] [EchoMind: An Interrelated Multi-level Benchmark for Evaluating Empathetic Speech Language Models](https://arxiv.org/abs/2510.22758)
*Li Zhou,Lutong Yu,You Lyu,Yihang Lin,Zefeng Zhao,Junyi Ao,Yuhao Zhang,Benyou Wang,Haizhou Li*

Main category: cs.CL

TL;DR: EchoMind是首个多层次的基准测试，通过顺序任务模拟共情对话的认知过程，评估语音语言模型在理解口语内容、感知声音线索、综合推理和生成回应方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试通常孤立评估语言、声学、推理或对话能力，忽视了这些技能整合对于实现类人、情感智能对话的重要性。

Method: 设计EchoMind基准测试，包含口语内容理解、声音线索感知、综合推理和回应生成四个顺序任务，使用语义中性的脚本和受控的声音风格变化。

Result: 测试12个先进SLM发现，即使最先进的模型在处理高表达性声音线索方面仍有困难，限制了共情回应的质量。

Conclusion: SLM需要整合语言内容与多样化声音线索，才能实现真正的共情对话能力。

Abstract: Speech Language Models (SLMs) have made significant progress in spoken
language understanding. Yet it remains unclear whether they can fully perceive
non lexical vocal cues alongside spoken words, and respond with empathy that
aligns with both emotional and contextual factors. Existing benchmarks
typically evaluate linguistic, acoustic, reasoning, or dialogue abilities in
isolation, overlooking the integration of these skills that is crucial for
human-like, emotionally intelligent conversation. We present EchoMind, the
first interrelated, multi-level benchmark that simulates the cognitive process
of empathetic dialogue through sequential, context-linked tasks: spoken-content
understanding, vocal-cue perception, integrated reasoning, and response
generation. All tasks share identical and semantically neutral scripts that are
free of explicit emotional or contextual cues, and controlled variations in
vocal style are used to test the effect of delivery independent of the
transcript. EchoMind is grounded in an empathy-oriented framework spanning 3
coarse and 12 fine-grained dimensions, encompassing 39 vocal attributes, and
evaluated using both objective and subjective metrics. Testing 12 advanced SLMs
reveals that even state-of-the-art models struggle with high-expressive vocal
cues, limiting empathetic response quality. Analyses of prompt strength, speech
source, and ideal vocal cue recognition reveal persistent weaknesses in
instruction-following, resilience to natural speech variability, and effective
use of vocal cues for empathy. These results underscore the need for SLMs that
integrate linguistic content with diverse vocal cues to achieve truly
empathetic conversational ability.

</details>


### [60] [Iterative Layer Pruning for Efficient Translation Inference](https://arxiv.org/abs/2510.22763)
*Yasmin Moslem,Muhammad Hazim Al Farouq,John D. Kelleher*

Main category: cs.CL

TL;DR: 本文提出了一种基于层重要性分析的迭代层剪枝方法，用于压缩大型语言模型，在保持翻译质量的同时显著减小模型大小和推理时间。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在机器翻译等自然语言处理任务中表现出色，但其巨大的计算需求给实际部署带来挑战。本文旨在解决LLM高效部署的问题。

Method: 采用迭代层剪枝方法，通过层重要性分析指导剪枝过程，使用Aya-Expanse-8B模型在捷克语-德语和英语-埃及阿拉伯语翻译任务上进行实验。

Result: 该方法实现了模型大小和推理时间的显著减少，同时保持了基线模型的翻译质量。

Conclusion: 基于层重要性分析的迭代层剪枝是一种有效的模型压缩方法，能够在保持性能的同时大幅降低计算需求。

Abstract: Large language models (LLMs) have transformed many areas of natural language
processing, including machine translation. However, efficient deployment of
LLMs remains challenging due to their intensive computational requirements. In
this paper, we address this challenge and present our submissions to the Model
Compression track at the Conference on Machine Translation (WMT 2025). In our
experiments, we investigate iterative layer pruning guided by layer importance
analysis. We evaluate this method using the Aya-Expanse-8B model for
translation from Czech to German, and from English to Egyptian Arabic. Our
approach achieves substantial reductions in model size and inference time,
while maintaining the translation quality of the baseline models.

</details>


### [61] [MMPersuade: A Dataset and Evaluation Framework for Multimodal Persuasion](https://arxiv.org/abs/2510.22768)
*Haoyi Qiu,Yilun Zhou,Pranav Narayanan Venkit,Kung-Hsiang Huang,Jiaxin Zhang,Nanyun Peng,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: MMPersuade是一个研究大视觉语言模型多模态说服动态的统一框架，包含多模态数据集和评估框架，发现多模态输入显著增强说服效果，不同说服策略在不同情境下效果各异。


<details>
  <summary>Details</summary>
Motivation: 随着大视觉语言模型在购物、健康、新闻等领域的部署，它们面临大量说服性内容。了解这些模型作为被说服者的易感性和不同说服策略的有效性至关重要，因为过于易受影响的模型可能采纳误导性信念或生成不道德输出。

Method: 提出了MMPersuade框架，包括：(i)包含图像和视频的多模态数据集，涵盖商业、主观行为和对抗性情境下的说服原则；(ii)通过第三方同意评分和自估计标记概率来量化说服效果和模型易感性的评估框架。

Result: 对六个领先LVLM的研究发现：(i)多模态输入相比纯文本显著提高说服效果和模型易感性，特别是在错误信息场景中；(ii)声明的先前偏好降低易感性，但多模态信息仍保持说服优势；(iii)不同策略在不同情境下效果各异，互惠在商业和主观情境中最有效，可信度和逻辑在对抗性情境中占主导。

Conclusion: 通过联合分析说服效果和易感性，MMPersuade为开发在面对说服性多模态内容时具有鲁棒性、偏好一致性和伦理对齐的模型提供了原则性基础。

Abstract: As Large Vision-Language Models (LVLMs) are increasingly deployed in domains
such as shopping, health, and news, they are exposed to pervasive persuasive
content. A critical question is how these models function as persuadees-how and
why they can be influenced by persuasive multimodal inputs. Understanding both
their susceptibility to persuasion and the effectiveness of different
persuasive strategies is crucial, as overly persuadable models may adopt
misleading beliefs, override user preferences, or generate unethical or unsafe
outputs when exposed to manipulative messages. We introduce MMPersuade, a
unified framework for systematically studying multimodal persuasion dynamics in
LVLMs. MMPersuade contributes (i) a comprehensive multimodal dataset that pairs
images and videos with established persuasion principles across commercial,
subjective and behavioral, and adversarial contexts, and (ii) an evaluation
framework that quantifies both persuasion effectiveness and model
susceptibility via third-party agreement scoring and self-estimated token
probabilities on conversation histories. Our study of six leading LVLMs as
persuadees yields three key insights: (i) multimodal inputs substantially
increase persuasion effectiveness-and model susceptibility-compared to text
alone, especially in misinformation scenarios; (ii) stated prior preferences
decrease susceptibility, yet multimodal information maintains its persuasive
advantage; and (iii) different strategies vary in effectiveness across
contexts, with reciprocity being most potent in commercial and subjective
contexts, and credibility and logic prevailing in adversarial contexts. By
jointly analyzing persuasion effectiveness and susceptibility, MMPersuade
provides a principled foundation for developing models that are robust,
preference-consistent, and ethically aligned when engaging with persuasive
multimodal content.

</details>


### [62] [Scalable Supervising Software Agents with Patch Reasoner](https://arxiv.org/abs/2510.22775)
*Junjielong Xu,Boyin Tan,Xiaoyuan Liu,Chao Peng,Pengfei Gao,Pinjia He*

Main category: cs.CL

TL;DR: R4P是一个基于推理的补丁验证模型，为软件工程代理提供可扩展的奖励机制，解决了传统测试监督方法不可扩展的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于测试的监督方法不可扩展，因为构建测试沙盒既繁重又脆弱，且高覆盖率测试数据稀缺且易受边缘案例攻击。

Method: R4P将补丁验证视为推理任务，使用分组目标进行强化学习训练，能够验证多个补丁之间的修改关系并提供密集奖励。

Result: R4P在SWE-bench-verified上达到72.2%的补丁验证准确率，验证速度比测试快50倍。基于R4P训练的Mini-SE在SWE-bench-verified上达到26.2% Pass@1，比原模型提升10.0%。

Conclusion: R4P通过推理方式提供可扩展的奖励机制，显著提升了软件工程代理的性能和效率，展现了实际应用价值。

Abstract: While large language model agents have advanced software engineering tasks,
the unscalable nature of existing test-based supervision is limiting the
potential improvement of data scaling. The reason is twofold: (1) building and
running test sandbox is rather heavy and fragile, and (2) data with
high-coverage tests is naturally rare and threatened by test hacking via edge
cases. In this paper, we propose R4P, a patch verifier model to provide
scalable rewards for training and testing SWE agents via reasoning. We consider
that patch verification is fundamentally a reasoning task, mirroring how human
repository maintainers review patches without writing and running new
reproduction tests. To obtain sufficient reference and reduce the risk of
reward hacking, R4P uses a group-wise objective for RL training, enabling it to
verify multiple patches against each other's modification and gain a dense
reward for stable training. R4P achieves 72.2% Acc. for verifying patches from
SWE-bench-verified, surpassing OpenAI o3. To demonstrate R4P's practicality, we
design and train a lite scaffold, Mini-SE, with pure reinforcement learning
where all rewards are derived from R4P. As a result, Mini-SE achieves 26.2%
Pass@1 on SWE-bench-verified, showing a 10.0% improvement over the original
Qwen3-32B. This can be further improved to 32.8% with R4P for test-time
scaling. Furthermore, R4P verifies patches within a second, 50x faster than
testing on average. The stable scaling curves of rewards and accuracy along
with high efficiency reflect R4P's practicality.

</details>


### [63] [VEHME: A Vision-Language Model For Evaluating Handwritten Mathematics Expressions](https://arxiv.org/abs/2510.22798)
*Thu Phuong Nguyen,Duc M. Nguyen,Hyotaek Jeon,Hyunwook Lee,Hyunmin Song,Sungahn Ko,Taehwan Kim*

Main category: cs.CL

TL;DR: VEHME是一个用于评估手写数学表达式的视觉语言模型，通过两阶段训练和表达式感知视觉提示模块，实现了高精度和可解释的数学答案自动评分。


<details>
  <summary>Details</summary>
Motivation: 手写数学答案的自动评估在教育技术中很重要，但由于格式多样、布局无结构和符号复杂性，这仍然是一个重大挑战。

Method: 采用两阶段训练：监督微调使用结构化推理数据，强化学习对齐多维评分目标；提出表达式感知视觉提示模块增强空间理解。

Result: 在AIHub和FERMAT数据集上评估，VEHME在开源模型中达到最先进性能，接近专有系统的准确性。

Conclusion: VEHME有潜力成为可扩展且易于使用的自动数学评估工具，训练和实验代码已公开。

Abstract: Automatically assessing handwritten mathematical solutions is an important
problem in educational technology with practical applications, but it remains a
significant challenge due to the diverse formats, unstructured layouts, and
symbolic complexity of student work. To address this challenge, we introduce
VEHME-a Vision-Language Model for Evaluating Handwritten Mathematics
Expressions-designed to assess open-form handwritten math responses with high
accuracy and interpretable reasoning traces. VEHME integrates a two-phase
training pipeline: (i) supervised fine-tuning using structured reasoning data,
and (ii) reinforcement learning that aligns model outputs with
multi-dimensional grading objectives, including correctness, reasoning depth,
and error localization. To enhance spatial understanding, we propose an
Expression-Aware Visual Prompting Module, trained on our synthesized multi-line
math expressions dataset to robustly guide attention in visually heterogeneous
inputs. Evaluated on AIHub and FERMAT datasets, VEHME achieves state-of-the-art
performance among open-source models and approaches the accuracy of proprietary
systems, demonstrating its potential as a scalable and accessible tool for
automated math assessment. Our training and experiment code is publicly
available at our GitHub repository.

</details>


### [64] [Cross-Lingual Stability and Bias in Instruction-Tuned Language Models for Humanitarian NLP](https://arxiv.org/abs/2510.22823)
*Poli Nemkova,Amrit Adhikari,Matthew Pearson,Vamsi Krishna Sadu,Mark V. Albert*

Main category: cs.CL

TL;DR: 本文首次系统比较了商业和开源大语言模型在七种语言上的人权侵犯检测性能，量化了资源受限组织面临的成本-可靠性权衡。研究发现对齐而非规模决定模型稳定性，对齐模型在低资源语言上保持稳定性能，而开源模型存在显著的提示语言敏感性和校准漂移。


<details>
  <summary>Details</summary>
Motivation: 人道主义组织面临关键选择：投资昂贵的商业API或依赖免费的开源模型进行多语言人权监控。商业系统提供可靠性，但开源替代方案缺乏实证验证，特别是在冲突地区常见的低资源语言上。

Method: 通过78,000次多语言推理评估六个模型（四个对齐模型和两个开源模型），使用标准分类指标和新的跨语言可靠性度量：校准偏差、决策偏差、语言鲁棒性得分和语言稳定性得分。

Result: 对齐模型在类型学距离较远的低资源语言（如林加拉语、缅甸语）上保持近乎不变的准确性和平衡校准，而开源模型表现出显著的提示语言敏感性和校准漂移。对齐而非规模决定模型稳定性。

Conclusion: 多语言对齐实现了语言无关的推理，为人道主义组织在多语言部署中平衡预算约束与可靠性提供了实用指导。

Abstract: Humanitarian organizations face a critical choice: invest in costly
commercial APIs or rely on free open-weight models for multilingual human
rights monitoring. While commercial systems offer reliability, open-weight
alternatives lack empirical validation -- especially for low-resource languages
common in conflict zones. This paper presents the first systematic comparison
of commercial and open-weight large language models (LLMs) for
human-rights-violation detection across seven languages, quantifying the
cost-reliability trade-off facing resource-constrained organizations. Across
78,000 multilingual inferences, we evaluate six models -- four
instruction-aligned (Claude-Sonnet-4, DeepSeek-V3, Gemini-Flash-2.0,
GPT-4.1-mini) and two open-weight (LLaMA-3-8B, Mistral-7B) -- using both
standard classification metrics and new measures of cross-lingual reliability:
Calibration Deviation (CD), Decision Bias (B), Language Robustness Score (LRS),
and Language Stability Score (LSS). Results show that alignment, not scale,
determines stability: aligned models maintain near-invariant accuracy and
balanced calibration across typologically distant and low-resource languages
(e.g., Lingala, Burmese), while open-weight models exhibit significant
prompt-language sensitivity and calibration drift. These findings demonstrate
that multilingual alignment enables language-agnostic reasoning and provide
practical guidance for humanitarian organizations balancing budget constraints
with reliability in multilingual deployment.

</details>


### [65] [Exploration of Summarization by Generative Language Models for Automated Scoring of Long Essays](https://arxiv.org/abs/2510.22830)
*Haowei Hua,Hong Jiao,Xinyi Wang*

Main category: cs.CL

TL;DR: 使用生成式语言模型通过摘要和提示技术改进长文本自动评分，相比BERT等编码器模型在512词限制下的表现有显著提升。


<details>
  <summary>Details</summary>
Motivation: BERT及其变体在自动评分中被广泛探索，但这些编码器模型512个词符的限制在长文本评分中表现出不足。

Method: 探索生成式语言模型用于长文本自动评分，通过摘要和提示技术来实现。

Result: 在Learning Agency Lab Automated Essay Scoring 2.0数据集上，评分准确率显著提升，QWK从0.822提高到0.8878。

Conclusion: 生成式语言模型通过摘要和提示技术能够有效改进长文本自动评分的性能。

Abstract: BERT and its variants are extensively explored for automated scoring.
However, a limit of 512 tokens for these encoder-based models showed the
deficiency in automated scoring of long essays. Thus, this research explores
generative language models for automated scoring of long essays via
summarization and prompting. The results revealed great improvement of scoring
accuracy with QWK increased from 0.822 to 0.8878 for the Learning Agency Lab
Automated Essay Scoring 2.0 dataset.

</details>


### [66] [Leveraging Large Language Models to Identify Conversation Threads in Collaborative Learning](https://arxiv.org/abs/2510.22844)
*Prerna Ravi,Dong Won Lee,Beatriz Flamia,Jasmine David,Brandon Hanks,Cynthia Breazeal,Emma Anderson,Grace Lin*

Main category: cs.CL

TL;DR: 该论文研究如何利用显式对话线程信息改进大语言模型在小组对话中的关系性行为编码，提出了同步多参与者对话的线程识别方法，并验证了线程信息对下游分析性能的提升作用。


<details>
  <summary>Details</summary>
Motivation: 理解小组对话中思想的发展和流动对分析协作学习至关重要。对话线程是这些互动的关键结构特征，但在同步口语对话中检测线程具有挑战性。同时，大语言模型在长上下文任务中往往难以追踪这些对话链接。

Method: 提出了识别同步多参与者对话中线程的系统指南，并对不同的大语言模型提示策略进行了基准测试。然后测试了线程信息如何影响对话分析框架的下游编码性能。

Result: 结果显示，提供清晰的对话线程信息可以改善大语言模型的编码性能，并强调了下游分析对良好结构化对话的严重依赖。

Conclusion: 这项工作推进了将大语言模型与稳健的对话线程结构相结合的方法，以理解复杂的实时小组互动，并讨论了人机混合方法的最佳价值权衡。

Abstract: Understanding how ideas develop and flow in small-group conversations is
critical for analyzing collaborative learning. A key structural feature of
these interactions is threading, the way discourse talk naturally organizes
into interwoven topical strands that evolve over time. While threading has been
widely studied in asynchronous text settings, detecting threads in synchronous
spoken dialogue remains challenging due to overlapping turns and implicit cues.
At the same time, large language models (LLMs) show promise for automating
discourse analysis but often struggle with long-context tasks that depend on
tracing these conversational links. In this paper, we investigate whether
explicit thread linkages can improve LLM-based coding of relational moves in
group talk. We contribute a systematic guidebook for identifying threads in
synchronous multi-party transcripts and benchmark different LLM prompting
strategies for automated threading. We then test how threading influences
performance on downstream coding of conversational analysis frameworks, that
capture core collaborative actions such as agreeing, building, and eliciting.
Our results show that providing clear conversational thread information
improves LLM coding performance and underscores the heavy reliance of
downstream analysis on well-structured dialogue. We also discuss practical
trade-offs in time and cost, emphasizing where human-AI hybrid approaches can
yield the best value. Together, this work advances methods for combining LLMs
and robust conversational thread structures to make sense of complex, real-time
group interactions.

</details>


### [67] [Once Upon an Input: Reasoning via Per-Instance Program Synthesis](https://arxiv.org/abs/2510.22849)
*Adam Stein,Neelay Velingker,Mayur Naik,Eric Wong*

Main category: cs.CL

TL;DR: PIPS是一种基于实例的程序合成方法，通过结构反馈生成和优化程序，无需任务特定指导或显式测试用例，在复杂推理任务中显著优于CoT和PoT方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法如CoT和PoT在增强LLM复杂推理能力时，特别是在算法领域，经常产生不理想的解决方案，需要改进。

Method: PIPS在实例级别生成和优化程序，使用结构反馈，并引入置信度指标动态选择直接推理或程序合成。

Result: 在30个基准测试中，PIPS相比PoT和CoT分别提升绝对调和平均准确率8.6%和9.4%，在算法任务中减少65.1%的不良程序生成。

Conclusion: PIPS通过实例级程序合成和动态选择机制，有效提升了LLM在复杂推理任务中的性能，减少了不良解决方案的产生。

Abstract: Large language models (LLMs) excel at zero-shot inference but continue to
struggle with complex, multi-step reasoning. Recent methods that augment LLMs
with intermediate reasoning steps such as Chain of Thought (CoT) and Program of
Thought (PoT) improve performance but often produce undesirable solutions,
especially in algorithmic domains. We introduce Per-Instance Program Synthesis
(PIPS), a method that generates and refines programs at the instance-level
using structural feedback without relying on task-specific guidance or explicit
test cases. To further improve performance, PIPS incorporates a confidence
metric that dynamically chooses between direct inference and program synthesis
on a per-instance basis. Experiments across three frontier LLMs and 30
benchmarks including all tasks of Big Bench Extra Hard (BBEH), visual question
answering tasks, relational reasoning tasks, and mathematical reasoning tasks
show that PIPS improves the absolute harmonic mean accuracy by up to 8.6% and
9.4% compared to PoT and CoT respectively, and reduces undesirable program
generations by 65.1% on the algorithmic tasks compared to PoT with
Gemini-2.0-Flash.

</details>


### [68] [Far from the Shallow: Brain-Predictive Reasoning Embedding through Residual Disentanglement](https://arxiv.org/abs/2510.22860)
*Linyang He,Tianjun Zhong,Richard Antonello,Gavin Mischler,Micah Goldblum,Nima Mesgarani*

Main category: cs.CL

TL;DR: 提出了一种残差解缠方法，从语言模型中分离出词汇、句法、语义和推理四个正交嵌入，用于建模大脑对语言的处理过程。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型内部表示高度"纠缠"，混合了词汇、句法、语义和推理信息，这导致脑编码分析偏向浅层语言特征，难以分离深层认知过程的神经基础。

Method: 通过探测语言模型识别特征特定层，迭代回归掉低层表示，产生四个近乎正交的嵌入表示：词汇、句法、语义和推理。

Result: 1) 分离出的推理嵌入具有独特预测能力，能解释其他语言特征无法解释的神经活动方差，甚至扩展到经典语言区域之外的视觉区域；2) 推理的神经信号在时间上与其他特征不同，峰值较晚(~350-400ms)；3) 标准未解缠的语言模型嵌入可能产生误导，其预测成功主要归因于浅层语言特征。

Conclusion: 解缠方法能够揭示语言处理层次中推理的独特神经表征，为理解人类大脑从简单语言处理到高级推理的进展提供了新视角。

Abstract: Understanding how the human brain progresses from processing simple
linguistic inputs to performing high-level reasoning is a fundamental challenge
in neuroscience. While modern large language models (LLMs) are increasingly
used to model neural responses to language, their internal representations are
highly "entangled," mixing information about lexicon, syntax, meaning, and
reasoning. This entanglement biases conventional brain encoding analyses toward
linguistically shallow features (e.g., lexicon and syntax), making it difficult
to isolate the neural substrates of cognitively deeper processes. Here, we
introduce a residual disentanglement method that computationally isolates these
components. By first probing an LM to identify feature-specific layers, our
method iteratively regresses out lower-level representations to produce four
nearly orthogonal embeddings for lexicon, syntax, meaning, and, critically,
reasoning. We used these disentangled embeddings to model intracranial (ECoG)
brain recordings from neurosurgical patients listening to natural speech. We
show that: 1) This isolated reasoning embedding exhibits unique predictive
power, accounting for variance in neural activity not explained by other
linguistic features and even extending to the recruitment of visual regions
beyond classical language areas. 2) The neural signature for reasoning is
temporally distinct, peaking later (~350-400ms) than signals related to
lexicon, syntax, and meaning, consistent with its position atop a processing
hierarchy. 3) Standard, non-disentangled LLM embeddings can be misleading, as
their predictive success is primarily attributable to linguistically shallow
features, masking the more subtle contributions of deeper cognitive processing.

</details>


### [69] [Interpreting and Mitigating Unwanted Uncertainty in LLMs](https://arxiv.org/abs/2510.22866)
*Tiasa Singha Roy,Ayush Rajesh Jhaveri,Ilias Triantafyllopoulos*

Main category: cs.CL

TL;DR: 研究发现LLMs存在不确定性现象，即模型会将先前正确的答案翻转为错误答案。通过识别并屏蔽一小部分关注误导性token的注意力头，可以减少15%的翻转行为。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在不确定性行为，这会破坏信任并在高风险领域带来严重风险，因此需要研究其机制并找到缓解方法。

Method: 采用Needle-in-a-Haystack检索框架，集成Flip式重新评估提示来模拟答案翻转场景，识别并屏蔽导致不确定性的注意力头。

Result: 发现检索头不是避免不确定性的主要原因，而是识别出一小部分非检索注意力头会过度关注误导性token。屏蔽这些头可以减少15%的翻转行为，且不会引入不连贯或过度修正。

Conclusion: 研究为机制可解释性领域做出贡献，并提出了一种简单有效的技术来缓解LLMs中的不确定性驱动故障模式，但在下游任务中存在权衡。

Abstract: Despite their impressive capabilities, Large Language Models (LLMs) exhibit
unwanted uncertainty, a phenomenon where a model changes a previously correct
answer into an incorrect one when re-prompted. This behavior undermines trust
and poses serious risks in high-stakes domains. In this work, we investigate
the mechanisms that drive this phenomenon. We adapt the Needle-in-a-Haystack
retrieval framework and integrate a Flip-style re-evaluation prompt to simulate
realistic answer-flipping scenarios. We find that retrieval heads are not
primarily responsible for avoiding uncertainty. Instead, we identify a small
set of non-retrieval attention heads that disproportionately attend to
misleading tokens in uncertain contexts. Masking these heads yields significant
improvements, reducing flip behavior by up to 15% without introducing
incoherence or overcorrection. However, when tested for downstream tasks, we
observe trade-offs with flip behavior. Our findings contribute to the growing
field of mechanistic interpretability and present a simple yet effective
technique for mitigating uncertainty-driven failure modes in LLMs.

</details>


### [70] [A Comprehensive Dataset for Human vs. AI Generated Text Detection](https://arxiv.org/abs/2510.22874)
*Rajarshi Roy,Nasrin Imanpour,Ashhar Aziz,Shashwat Bajpai,Gurpreet Singh,Shwetangshu Biswas,Kapil Wanaskar,Parth Patwa,Subhankar Ghosh,Shreyas Dixit,Nilesh Ranjan Pal,Vipula Rawte,Ritvik Garimella,Gaytri Jena,Amit Sheth,Vasu Sharma,Aishwarya Naresh Reganti,Vinija Jain,Aman Chadha,Amitava Das*

Main category: cs.CL

TL;DR: 提出了一个包含超过58,000个文本样本的综合数据集，结合了真实的纽约时报文章和多个先进LLM生成的合成版本，用于AI文本检测和模型归因研究。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成文本越来越像人类写作，需要大规模、多样化的标注数据集来可靠检测AI生成文本并将其归因到特定模型，以解决内容真实性、错误信息和可信度问题。

Method: 构建包含真实纽约时报文章和由Gemma-2-9b、Mistral-7B、Qwen-2-72B、LLaMA-8B、Yi-Large、GPT-4-o等先进LLM生成的合成文本的综合数据集，提供原始文章摘要作为提示和完整人类撰写叙述。

Result: 建立了两个关键任务的基线结果：区分人类写作与AI生成文本的准确率为58.35%，将AI文本归因到生成模型的准确率为8.92%。

Conclusion: 通过将真实世界新闻内容与现代生成模型相结合，该数据集旨在推动强大检测和归因方法的发展，在生成AI时代促进信任和透明度。

Abstract: The rapid advancement of large language models (LLMs) has led to increasingly
human-like AI-generated text, raising concerns about content authenticity,
misinformation, and trustworthiness. Addressing the challenge of reliably
detecting AI-generated text and attributing it to specific models requires
large-scale, diverse, and well-annotated datasets. In this work, we present a
comprehensive dataset comprising over 58,000 text samples that combine
authentic New York Times articles with synthetic versions generated by multiple
state-of-the-art LLMs including Gemma-2-9b, Mistral-7B, Qwen-2-72B, LLaMA-8B,
Yi-Large, and GPT-4-o. The dataset provides original article abstracts as
prompts, full human-authored narratives. We establish baseline results for two
key tasks: distinguishing human-written from AI-generated text, achieving an
accuracy of 58.35\%, and attributing AI texts to their generating models with
an accuracy of 8.92\%. By bridging real-world journalistic content with modern
generative models, the dataset aims to catalyze the development of robust
detection and attribution methods, fostering trust and transparency in the era
of generative AI. Our dataset is available at:
https://huggingface.co/datasets/gsingh1-py/train.

</details>


### [71] [Batch Speculative Decoding Done Right](https://arxiv.org/abs/2510.22876)
*Ranran Haoran Zhang,Soumik Dey,Ashirbad Mishra,Hansi Wu,Binbin Li,Rui Zhang*

Main category: cs.CL

TL;DR: 本文提出了EQSPEC和EXSPEC两种批量推测解码方法，解决了批量处理中的不规则张量问题，在保持输出等价性的同时显著提升了推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 推测解码通过小模型预测多个token来加速LLM推理，但在批量处理时会出现不规则张量问题，导致位置ID、注意力掩码和KV缓存状态损坏，现有实现违反了输出等价性要求。

Method: 首先分析同步需求保证正确性，提出EQSPEC方法暴露重对齐开销，然后引入EXSPEC方法维护滑动序列池并动态形成相同长度组，减少重对齐开销。

Result: 在SpecBench数据集上，对多个目标/草稿模型对，在批量大小8时实现了最高3倍的吞吐量提升，同时保持95%的输出等价性，无需定制内核即可集成到现有推理栈。

Conclusion: 提出的方法有效解决了批量推测解码中的不规则张量问题，在保持输出等价性的同时显著提升了推理效率，为生产环境部署提供了实用解决方案。

Abstract: Speculative decoding speeds up LLM inference by using a small draft model to
propose multiple tokens that a target model verifies in parallel. Extending
this idea to batches is essential for production serving, but it introduces the
ragged tensor problem: sequences in the same batch accept different numbers of
draft tokens, breaking right-alignment and corrupting position IDs, attention
masks, and KV-cache state. We show that several existing batch implementations
violate output equivalence-the fundamental requirement that speculative
decoding must produce identical token sequences to standard autoregressive
generation. These violations occur precisely due to improper handling of the
ragged tensor problem. In response, we (1) characterize the synchronization
requirements that guarantee correctness, (2) present a correctness-first batch
speculative decoding EQSPEC that exposes realignment as consuming 40% of
overhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences
and dynamically forms same-length groups, to reduce the realignment overhead
while preserving per-sequence speculative speedups. On the SpecBench dataset,
across Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our
approach achieves up to 3$\times$ throughput improvement at batch size 8
compared to batch size 1, with efficient scaling through batch size 8, while
maintaining 95% output equivalence. Our method requires no custom kernels and
integrates cleanly with existing inference stacks. Our code is available at
https://github.com/eBay/spec_dec.

</details>


### [72] [Language Server CLI Empowers Language Agents with Process Rewards](https://arxiv.org/abs/2510.22907)
*Yifan Zhang,Lanser Contributors*

Main category: cs.CL

TL;DR: Lanser-CLI是一个CLI优先的编排层，通过语言服务器协议(LSP)为编码代理和CI提供确定性、可重放的工作流，解决了大语言模型在API幻觉和编辑定位方面的不可靠问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型经常产生API幻觉且编辑定位不准确，而语言服务器能计算关于真实代码的经过验证的IDE级事实。作者认为语言服务器不仅提供结构信息，还提供可操作的过程奖励，将代理的规划循环与程序现实对齐。

Method: 开发Lanser-CLI，包含：1) 稳健的寻址方案，通过Selector DSL(符号、AST路径和内容锚定选择器)和重定位算法；2) 确定性分析包，标准化语言服务器响应并捕获环境/能力元数据；3) 变异操作的安全信封，支持预览、工作空间隔离和Git感知的事务性应用；4) 基于语言服务器事实的过程奖励函数。

Result: 在冻结快照下形式化了确定性，建立了过程奖励的单调性属性，使其适用于过程监督和反事实分析。

Conclusion: Lanser-CLI通过语言服务器为编码代理提供确定性、可重放的工作流，解决了大语言模型在代码编辑中的不可靠问题，为过程监督和反事实分析提供了理论基础。

Abstract: Large language models routinely hallucinate APIs and mislocalize edits, while
language servers compute verified, IDE-grade facts about real code. We present
Lanser-CLI, a CLI-first orchestration layer that pins and mediates a Language
Server Protocol (LSP) server for coding agents and CI, exposing deterministic,
replayable workflows. Our position is that language servers provide not only
structural information (definitions, references, types, diagnostics) but also
an actionable process reward: machine-checked, step-wise signals that align an
agent's planning loop with program reality. In this work, Lanser-CLI
contributes: (i) a robust addressing scheme beyond brittle "file:line:col" via
a Selector DSL (symbolic, AST-path, and content-anchored selectors) with a
principled relocation algorithm; (ii) deterministic Analysis Bundles that
normalize Language Server responses and capture environment/capability metadata
with stable content hashes; (iii) a safety envelope for mutating operations
(rename, code actions) with preview, workspace jails, and Git-aware,
transactional apply; and (iv) a process-reward functional derived from Language
Server facts (diagnostic deltas, disambiguation confidence, and safe-apply
checks) that is computable online and replayable offline. We formalize
determinism under frozen snapshots and establish a monotonicity property for
the process reward, making it suitable for process supervision and
counterfactual analysis. Project Page:
https://github.com/yifanzhang-pro/lanser-cli

</details>


### [73] [Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)](https://arxiv.org/abs/2510.22954)
*Liwei Jiang,Yuanjun Chai,Margaret Li,Mickel Liu,Raymond Fok,Nouha Dziri,Yulia Tsvetkov,Maarten Sap,Alon Albalak,Yejin Choi*

Main category: cs.CL

TL;DR: 该论文提出了Infinity-Chat数据集，用于评估语言模型在开放性问题生成中的多样性，揭示了语言模型存在的人工蜂群效应（模式崩溃），即模型内部和不同模型之间产生高度相似的输出。


<details>
  <summary>Details</summary>
Motivation: 语言模型在生成多样化、类人的创意内容方面存在困难，这可能导致人类思维因重复接触相似输出而长期同质化。目前缺乏可扩展的方法来评估语言模型输出的多样性。

Method: 引入Infinity-Chat数据集，包含26K个多样化的真实世界开放性问题查询，建立了首个全面的开放性问题分类体系（6个顶级类别和17个子类别），并进行了31,250个人类标注。

Result: 研究揭示了语言模型在开放生成中存在明显的人工蜂群效应：模型内部重复和模型间同质性。语言模型、奖励模型和语言模型评估器对引发不同个体偏好的模型生成的人类评分校准较差。

Conclusion: Infinity-Chat为系统研究真实世界开放性问题提供了首个大规模资源，揭示了缓解人工蜂群效应带来的长期AI安全风险的关键见解。

Abstract: Language models (LMs) often struggle to generate diverse, human-like creative
content, raising concerns about the long-term homogenization of human thought
through repeated exposure to similar outputs. Yet scalable methods for
evaluating LM output diversity remain limited, especially beyond narrow tasks
such as random number or name generation, or beyond repeated sampling from a
single model. We introduce Infinity-Chat, a large-scale dataset of 26K diverse,
real-world, open-ended user queries that admit a wide range of plausible
answers with no single ground truth. We introduce the first comprehensive
taxonomy for characterizing the full spectrum of open-ended prompts posed to
LMs, comprising 6 top-level categories (e.g., brainstorm & ideation) that
further breaks down to 17 subcategories. Using Infinity-Chat, we present a
large-scale study of mode collapse in LMs, revealing a pronounced Artificial
Hivemind effect in open-ended generation of LMs, characterized by (1)
intra-model repetition, where a single model consistently generates similar
responses, and more so (2) inter-model homogeneity, where different models
produce strikingly similar outputs. Infinity-Chat also includes 31,250 human
annotations, across absolute ratings and pairwise preferences, with 25
independent human annotations per example. This enables studying collective and
individual-specific human preferences in response to open-ended queries. Our
findings show that LMs, reward models, and LM judges are less well calibrated
to human ratings on model generations that elicit differing idiosyncratic
annotator preferences, despite maintaining comparable overall quality. Overall,
INFINITY-CHAT presents the first large-scale resource for systematically
studying real-world open-ended queries to LMs, revealing critical insights to
guide future research for mitigating long-term AI safety risks posed by the
Artificial Hivemind.

</details>


### [74] [Tagging-Augmented Generation: Assisting Language Models in Finding Intricate Knowledge In Long Contexts](https://arxiv.org/abs/2510.22956)
*Anwesan Pal,Karen Hovsepian,Tinghao Guo,Mengnan Zhao,Somendra Tripathi,Nikos Kanakaris,George Mihaila,Sumit Nigam*

Main category: cs.CL

TL;DR: 提出TAG（标签增强生成）方法，通过轻量级数据增强策略提升LLM在长上下文场景中的性能，无需改变检索文档的完整性。


<details>
  <summary>Details</summary>
Motivation: 现代大型语言模型在长复杂上下文中的问答和推理能力存在显著限制，现有方法如RAG对分块、嵌入和检索策略敏感，且需要大量预处理步骤。

Method: 使用标签增强生成策略，通过在上下文或提示中添加标签定义来增强数据，保持文档完整性。

Result: 在NoLima和NovelQA基准测试中，TAG方法相比基线获得显著提升：32K token上下文性能提升达17%，复杂多跳推理问答提升2.9%。

Conclusion: TAG是一种有效的轻量级数据增强方法，能够显著提升LLM在长上下文问答和复杂推理任务中的表现。

Abstract: Recent investigations into effective context lengths of modern flagship large
language models (LLMs) have revealed major limitations in effective question
answering (QA) and reasoning over long and complex contexts for even the
largest and most impressive cadre of models. While approaches like
retrieval-augmented generation (RAG) and chunk-based re-ranking attempt to
mitigate this issue, they are sensitive to chunking, embedding and retrieval
strategies and models, and furthermore, rely on extensive pre-processing,
knowledge acquisition and indexing steps. In this paper, we propose
Tagging-Augmented Generation (TAG), a lightweight data augmentation strategy
that boosts LLM performance in long-context scenarios, without degrading and
altering the integrity and composition of retrieved documents. We validate our
hypothesis by augmenting two challenging and directly relevant
question-answering benchmarks -- NoLima and NovelQA -- and show that tagging
the context or even just adding tag definitions into QA prompts leads to
consistent performance gains over the baseline -- up to 17% for 32K token
contexts, and 2.9% in complex reasoning question-answering for multi-hop
queries requiring knowledge across a wide span of text. Additional details are
available at https://sites.google.com/view/tag-emnlp.

</details>


### [75] [MAD-Fact: A Multi-Agent Debate Framework for Long-Form Factuality Evaluation in LLMs](https://arxiv.org/abs/2510.22967)
*Yucheng Ning,Xixun Lin,Fang Fang,Yanan Cao*

Main category: cs.CL

TL;DR: 提出了一个评估长文本事实准确性的系统框架，包括构建中文长文本数据集LongHalluQA、开发基于辩论的多智能体验证系统MAD-Fact，以及引入事实重要性层次结构。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生物医学、法律和教育等高风险领域的广泛应用引发了对其输出事实准确性的担忧，现有评估方法在处理长文本时因复杂推理链和累积信息而失效。

Method: 构建LongHalluQA中文长文本事实性数据集，开发基于辩论的多智能体验证系统MAD-Fact，引入事实重要性层次结构来捕捉长文本中不同声明的重要性差异。

Result: 在两个基准测试上的实验表明，更大的LLM通常保持更高的事实一致性，而国内模型在中文内容上表现更优。

Conclusion: 该工作为评估和增强长文本LLM输出的事实可靠性提供了结构化框架，指导其在敏感领域的安全部署。

Abstract: The widespread adoption of Large Language Models (LLMs) raises critical
concerns about the factual accuracy of their outputs, especially in high-risk
domains such as biomedicine, law, and education. Existing evaluation methods
for short texts often fail on long-form content due to complex reasoning
chains, intertwined perspectives, and cumulative information. To address this,
we propose a systematic approach integrating large-scale long-form datasets,
multi-agent verification mechanisms, and weighted evaluation metrics. We
construct LongHalluQA, a Chinese long-form factuality dataset; and develop
MAD-Fact, a debate-based multi-agent verification system. We introduce a fact
importance hierarchy to capture the varying significance of claims in long-form
texts. Experiments on two benchmarks show that larger LLMs generally maintain
higher factual consistency, while domestic models excel on Chinese content. Our
work provides a structured framework for evaluating and enhancing factual
reliability in long-form LLM outputs, guiding their safe deployment in
sensitive domains.

</details>


### [76] [Measuring Teaching with LLMs](https://arxiv.org/abs/2510.22968)
*Michael Hardy*

Main category: cs.CL

TL;DR: 使用基于句子嵌入的定制LLMs来客观测量教学质量，在数据高效训练下达到人类水平甚至超人类表现，并与教师增值指标相关，为AI驱动的教学评估提供了可行方法。


<details>
  <summary>Details</summary>
Motivation: 教育领域长期缺乏客观可扩展的教学质量测量方法，通用大语言模型在处理复杂的课堂观察工具时表现不佳。

Method: 使用基于句子级嵌入的定制LLMs架构，系统评估五种不同句子嵌入，采用数据高效训练机制防止过拟合。

Result: 专业模型达到人类水平甚至超人类表现（与专家评分相关性>0.65），高级模型更多关注课程级特征而非孤立话语，聚合模型分数与教师增值指标相关。

Conclusion: 建立了一种可行且强大的AI驱动教学测量新方法，为教师发展提供可扩展、可靠且有效的反馈路径。

Abstract: Objective and scalable measurement of teaching quality is a persistent
challenge in education. While Large Language Models (LLMs) offer potential,
general-purpose models have struggled to reliably apply complex, authentic
classroom observation instruments. This paper uses custom LLMs built on
sentence-level embeddings, an architecture better suited for the long-form,
interpretive nature of classroom transcripts than conventional subword
tokenization. We systematically evaluate five different sentence embeddings
under a data-efficient training regime designed to prevent overfitting. Our
results demonstrate that these specialized models can achieve human-level and
even super-human performance with expert human ratings above 0.65 and
surpassing the average human-human rater correlation. Further, through analysis
of annotation context windows, we find that more advanced models-those better
aligned with human judgments-attribute a larger share of score variation to
lesson-level features rather than isolated utterances, challenging the
sufficiency of single-turn annotation paradigms. Finally, to assess external
validity, we find that aggregate model scores align with teacher value-added
measures, indicating they are capturing features relevant to student learning.
However, this trend does not hold at the individual item level, suggesting that
while the models learn useful signals, they have not yet achieved full
generalization. This work establishes a viable and powerful new methodology for
AI-driven instructional measurement, offering a path toward providing scalable,
reliable, and valid feedback for educator development.

</details>


### [77] [Understanding In-Context Learning Beyond Transformers: An Investigation of State Space and Hybrid Architectures](https://arxiv.org/abs/2510.23006)
*Shenran Wang,Timothy Tin-Long Tse,Jian Zhu*

Main category: cs.CL

TL;DR: 对最先进的transformer、状态空间和混合大语言模型在基于知识的上下文学习任务上进行深入评估，发现不同架构的LLM在任务表现上可能相似，但内部机制不同。函数向量主要位于自注意力和Mamba层，且在不同知识类型任务中重要性不同。


<details>
  <summary>Details</summary>
Motivation: 研究不同架构大语言模型在上下文学习中的内部机制差异，特别是transformer、状态空间和混合模型在知识型任务中的表现和内部工作原理。

Method: 使用行为探测和干预方法相结合的方式，评估不同架构LLM在两类基于知识的上下文学习任务上的表现，分析函数向量的位置和作用机制。

Result: 发现不同架构LLM在任务性能上可能表现相似，但内部机制存在差异；函数向量主要位于自注意力和Mamba层；Mamba2可能使用不同于函数向量的机制进行上下文学习；函数向量在参数知识检索任务中更重要，但在上下文知识理解中作用较小。

Conclusion: 不同架构LLM在上下文学习中的内部机制存在显著差异，需要结合行为和机制分析来全面理解LLM能力，为跨架构和任务类型的理解提供了更细致的视角。

Abstract: We perform in-depth evaluations of in-context learning (ICL) on
state-of-the-art transformer, state-space, and hybrid large language models
over two categories of knowledge-based ICL tasks. Using a combination of
behavioral probing and intervention-based methods, we have discovered that,
while LLMs of different architectures can behave similarly in task performance,
their internals could remain different. We discover that function vectors (FVs)
responsible for ICL are primarily located in the self-attention and Mamba
layers, and speculate that Mamba2 uses a different mechanism from FVs to
perform ICL. FVs are more important for ICL involving parametric knowledge
retrieval, but not for contextual knowledge understanding. Our work contributes
to a more nuanced understanding across architectures and task types.
Methodologically, our approach also highlights the importance of combining both
behavioural and mechanistic analyses to investigate LLM capabilities.

</details>


### [78] [LangLingual: A Personalised, Exercise-oriented English Language Learning Tool Leveraging Large Language Models](https://arxiv.org/abs/2510.23011)
*Sammriddh Gupta,Sonit Singh,Aditya Joshi,Mira Kim*

Main category: cs.CL

TL;DR: 开发了基于LangChain框架和大型语言模型的对话代理LangLingual，为语言学习者提供实时语法反馈、情境感知练习和进度追踪。


<details>
  <summary>Details</summary>
Motivation: 语言教育者希望为学习者创造丰富的学习体验，但在提供反馈和练习方面受到限制。

Method: 使用LangChain框架和大型语言模型构建对话代理，设计实时语法反馈系统、情境感知语言练习生成和学习者能力追踪功能。

Result: 系统表现出良好的可用性、积极的学习成果和令人鼓舞的学习者参与度。

Conclusion: LangLingual系统成功解决了语言教育中反馈和练习资源有限的问题，通过AI技术提升了语言学习体验。

Abstract: Language educators strive to create a rich experience for learners, while
they may be restricted in the extend of feedback and practice they can provide.
We present the design and development of LangLingual, a conversational agent
built using the LangChain framework and powered by Large Language Models. The
system is specifically designed to provide real-time, grammar-focused feedback,
generate context-aware language exercises and track learner proficiency over
time. The paper discusses the architecture, implementation and evaluation of
LangLingual in detail. The results indicate strong usability, positive learning
outcomes and encouraging learner engagement.

</details>


### [79] [Incentivizing Agentic Reasoning in LLM Judges via Tool-Integrated Reinforcement Learning](https://arxiv.org/abs/2510.23038)
*Ran Xu,Jingjing Chen,Jiayu Ye,Yu Wu,Jun Yan,Carl Yang,Hongkun Yu*

Main category: cs.CL

TL;DR: TIR-Judge是一个端到端的强化学习框架，用于训练集成代码执行器的LLM评估器，在多个基准测试中超越了基于推理的评估器，并在无蒸馏训练的情况下达到与蒸馏变体相当的性能。


<details>
  <summary>Details</summary>
Motivation: 大多数LLM评估器仅基于内在文本推理，限制了验证复杂约束或执行精确计算的能力。受工具集成推理在其他任务中成功的启发，提出集成代码执行器的评估器框架。

Method: 基于三个原则构建：跨可验证和不可验证领域的多样化训练、灵活的评估格式（点对点、成对、列表）、无需蒸馏的迭代强化学习。

Result: 在七个公共基准测试中，TIR-Judge在点对点评估上超越强推理评估器6.4%，在成对评估上超越7.7%，列表评估性能与Claude-Opus-4相当（仅8B参数）。

Conclusion: 工具增强的评估器可以通过迭代强化学习自我进化，无需蒸馏的TIR-Judge-Zero与蒸馏变体性能相当。

Abstract: Large Language Models (LLMs) are widely used as judges to evaluate response
quality, providing a scalable alternative to human evaluation. However, most
LLM judges operate solely on intrinsic text-based reasoning, limiting their
ability to verify complex constraints or perform accurate computation.
Motivated by the success of tool-integrated reasoning (TIR) in numerous tasks,
we propose TIR-Judge, an end-to-end RL framework for training LLM judges that
integrates a code executor for precise evaluation. TIR-Judge is built on three
principles: (i) diverse training across verifiable and non-verifiable domains,
(ii) flexible judgment formats (pointwise, pairwise, listwise), and (iii)
iterative RL that bootstraps directly from the initial model without
distillation. On seven public benchmarks, TIR-Judge surpasses strong
reasoning-based judges by up to 6.4% (pointwise) and 7.7% (pairwise), and
achieves listwise performance comparable to Claude-Opus-4 despite having only
8B parameters. Remarkably, TIR-Judge-Zero - trained entirely without distilled
judge trajectories, matches the performance of distilled variants,
demonstrating that tool-augmented judges can self-evolve through iterative
reinforcement learning.

</details>


### [80] [Knocking-Heads Attention](https://arxiv.org/abs/2510.23052)
*Zhanchao Zhou,Xiaodong Chen,Haoxing Chen,Zhenzhong Lan,Jianguo Li*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Multi-head attention (MHA) has become the cornerstone of modern large
language models, enhancing representational capacity through parallel attention
heads. However, increasing the number of heads inherently weakens individual
head capacity, and existing attention mechanisms - whether standard MHA or its
variants like grouped-query attention (GQA) and grouped-tied attention (GTA) -
simply concatenate outputs from isolated heads without strong interaction. To
address this limitation, we propose knocking-heads attention (KHA), which
enables attention heads to "knock" on each other - facilitating cross-head
feature-level interactions before the scaled dot-product attention. This is
achieved by applying a shared, diagonally-initialized projection matrix across
all heads. The diagonal initialization preserves head-specific specialization
at the start of training while allowing the model to progressively learn
integrated cross-head representations. KHA adds only minimal parameters and
FLOPs and can be seamlessly integrated into MHA, GQA, GTA, and other attention
variants. We validate KHA by training a 6.1B parameter MoE model (1.01B
activated) on 1T high-quality tokens. Compared to baseline attention
mechanisms, KHA brings superior and more stable training dynamics, achieving
better performance across downstream tasks.

</details>


### [81] [Quality-Aware Translation Tagging in Multilingual RAG system](https://arxiv.org/abs/2510.23070)
*Hoyeon Moon,Byeolhee Kim,Nikhil Verma*

Main category: cs.CL

TL;DR: 提出QTT-RAG方法，通过评估翻译质量（语义等价性、语法准确性、自然流畅性）并作为元数据附加，在多语言检索增强生成中保持事实完整性，优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有mRAG方法在低资源语言场景下，翻译质量差会降低生成性能，而重写方法会导致事实扭曲和幻觉，需要一种能保持事实完整性的解决方案。

Method: QTT-RAG方法明确评估翻译质量的三个维度（语义等价性、语法准确性、自然流畅性），将这些评分作为元数据附加而不改变原始内容。

Result: 在两个开放域QA基准（XORQA、MKQA）上使用6个指令调优LLM（2.4B-14B参数）评估，涵盖韩语、芬兰语（低资源）和中文（高资源），QTT-RAG在保持事实完整性的同时优于CrossRAG和DKM-RAG基线。

Conclusion: QTT-RAG允许生成模型基于翻译可靠性做出明智决策，在低资源环境下有效利用跨语言文档，为多语言领域提供实用且鲁棒的解决方案。

Abstract: Multilingual Retrieval-Augmented Generation (mRAG) often retrieves English
documents and translates them into the query language for low-resource
settings. However, poor translation quality degrades response generation
performance. Existing approaches either assume sufficient translation quality
or utilize the rewriting method, which introduces factual distortion and
hallucinations. To mitigate these problems, we propose Quality-Aware
Translation Tagging in mRAG (QTT-RAG), which explicitly evaluates translation
quality along three dimensions-semantic equivalence, grammatical accuracy, and
naturalness&fluency-and attach these scores as metadata without altering the
original content. We evaluate QTT-RAG against CrossRAG and DKM-RAG as baselines
in two open-domain QA benchmarks (XORQA, MKQA) using six instruction-tuned LLMs
ranging from 2.4B to 14B parameters, covering two low-resource languages
(Korean and Finnish) and one high-resource language (Chinese). QTT-RAG
outperforms the baselines by preserving factual integrity while enabling
generator models to make informed decisions based on translation reliability.
This approach allows for effective usage of cross-lingual documents in
low-resource settings with limited native language documents, offering a
practical and robust solution across multilingual domains.

</details>


### [82] [A Survey on LLM Mid-training](https://arxiv.org/abs/2510.23081)
*Chengying Tu,Xuemiao Zhang,Rongxiang Weng,Rumei Li,Chen Zhang,Yang Bai,Hongfei Yan,Jingang Wang,Xunliang Cai*

Main category: cs.CL

TL;DR: 该调查论文正式定义了LLM的中训练阶段，分析了数据管理、训练策略和模型架构优化的优化框架，阐明了中训练在LLM能力渐进发展中的独特贡献。


<details>
  <summary>Details</summary>
Motivation: 基础模型的最新进展凸显了多阶段训练的重要价值，特别是中训练作为连接预训练和后训练的关键阶段，能够系统性地增强特定能力同时保持基础能力。

Method: 通过调查分析，提供了中训练的形式化定义，研究了包含数据管理、训练策略和模型架构优化的优化框架，并在目标驱动干预背景下分析了主流模型实现。

Result: 阐明了中训练作为LLM能力渐进发展中独特而关键阶段的地位，提供了全面的分类法和可操作的见解。

Conclusion: 该调查通过澄清中训练的独特贡献，为LLM发展的未来研究和创新提供了支持，强调了中训练在LLM能力系统增强中的重要性。

Abstract: Recent advances in foundation models have highlighted the significant
benefits of multi-stage training, with a particular emphasis on the emergence
of mid-training as a vital stage that bridges pre-training and post-training.
Mid-training is distinguished by its use of intermediate data and computational
resources, systematically enhancing specified capabilities such as mathematics,
coding, reasoning, and long-context extension, while maintaining foundational
competencies. This survey provides a formal definition of mid-training for
large language models (LLMs) and investigates optimization frameworks that
encompass data curation, training strategies, and model architecture
optimization. We analyze mainstream model implementations in the context of
objective-driven interventions, illustrating how mid-training serves as a
distinct and critical stage in the progressive development of LLM capabilities.
By clarifying the unique contributions of mid-training, this survey offers a
comprehensive taxonomy and actionable insights, supporting future research and
innovation in the advancement of LLMs.

</details>


### [83] [MAP4TS: A Multi-Aspect Prompting Framework for Time-Series Forecasting with Large Language Models](https://arxiv.org/abs/2510.23090)
*Suchan Lee,Jihoon Choi,Sohyeon Lee,Minseok Song,Bong-Gyu Jang,Hwanjo Yu,Soyeon Caren Han*

Main category: cs.CL

TL;DR: MAP4TS是一个多角度提示框架，通过将经典时间序列分析融入提示设计，提升LLM在时间序列预测中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了时间序列数据特有的统计特性和时间依赖性，需要专门设计来弥补这一差距。

Method: 提出四个专用提示组件：全局领域提示、局部领域提示、统计提示和时间提示，结合原始时间序列嵌入，通过跨模态对齐模块生成统一表示。

Result: 在八个不同数据集上的实验表明，MAP4TS持续优于最先进的基于LLM的方法，提示感知设计显著提升性能稳定性。

Conclusion: 结构化提示与GPT-2骨干网络组合在长期预测任务中表现优于LLaMA等更大模型，证明了多角度提示框架的有效性。

Abstract: Recent advances have investigated the use of pretrained large language models
(LLMs) for time-series forecasting by aligning numerical inputs with LLM
embedding spaces. However, existing multimodal approaches often overlook the
distinct statistical properties and temporal dependencies that are fundamental
to time-series data. To bridge this gap, we propose MAP4TS, a novel
Multi-Aspect Prompting Framework that explicitly incorporates classical
time-series analysis into the prompt design. Our framework introduces four
specialized prompt components: a Global Domain Prompt that conveys
dataset-level context, a Local Domain Prompt that encodes recent trends and
series-specific behaviors, and a pair of Statistical and Temporal Prompts that
embed handcrafted insights derived from autocorrelation (ACF), partial
autocorrelation (PACF), and Fourier analysis. Multi-Aspect Prompts are combined
with raw time-series embeddings and passed through a cross-modality alignment
module to produce unified representations, which are then processed by an LLM
and projected for final forecasting. Extensive experiments across eight diverse
datasets show that MAP4TS consistently outperforms state-of-the-art LLM-based
methods. Our ablation studies further reveal that prompt-aware designs
significantly enhance performance stability and that GPT-2 backbones, when
paired with structured prompts, outperform larger models like LLaMA in
long-term forecasting tasks.

</details>


### [84] [Leveraging Hierarchical Organization for Medical Multi-document Summarization](https://arxiv.org/abs/2510.23104)
*Yi-Li Hsu,Katelyn X. Mei,Lucy Lu Wang*

Main category: cs.CL

TL;DR: 该论文研究了在医学多文档摘要中引入层次结构是否能比传统平面摘要方法更好地组织跨文档信息，发现层次结构方法能提高摘要清晰度并保持内容覆盖度。


<details>
  <summary>Details</summary>
Motivation: 医学多文档摘要是复杂任务，需要有效管理跨文档关系。研究是否在输入中引入层次结构能提高模型组织和上下文化信息的能力。

Method: 研究了两种层次组织方式，在三个大语言模型上进行测试，使用自动指标、基于模型的指标和领域专家评估进行综合评估。

Result: 人类专家更喜欢模型生成的摘要而非人工编写的摘要。层次方法通常保持信息的真实性、覆盖度和连贯性，同时提高人类对摘要的偏好。GPT-4模拟判断与人类判断在更客观的评估维度上具有更高一致性。

Conclusion: 层次结构可以改善模型生成的医学摘要的清晰度，同时保持内容覆盖度，为提高生成摘要的人类偏好提供了实用方法。

Abstract: Medical multi-document summarization (MDS) is a complex task that requires
effectively managing cross-document relationships. This paper investigates
whether incorporating hierarchical structures in the inputs of MDS can improve
a model's ability to organize and contextualize information across documents
compared to traditional flat summarization methods. We investigate two ways of
incorporating hierarchical organization across three large language models
(LLMs), and conduct comprehensive evaluations of the resulting summaries using
automated metrics, model-based metrics, and domain expert evaluation of
preference, understandability, clarity, complexity, relevance, coverage,
factuality, and coherence. Our results show that human experts prefer
model-generated summaries over human-written summaries. Hierarchical approaches
generally preserve factuality, coverage, and coherence of information, while
also increasing human preference for summaries. Additionally, we examine
whether simulated judgments from GPT-4 align with human judgments, finding
higher agreement along more objective evaluation facets. Our findings
demonstrate that hierarchical structures can improve the clarity of medical
summaries generated by models while maintaining content coverage, providing a
practical way to improve human preference for generated summaries.

</details>


### [85] [Flexing in 73 Languages: A Single Small Model for Multilingual Inflection](https://arxiv.org/abs/2510.23114)
*Tomáš Sourada,Jana Straková*

Main category: cs.CL

TL;DR: 提出了一种紧凑的单模型多语言词形变化方法，在73种语言上联合训练，性能优于单语言基线，简化了部署需求


<details>
  <summary>Details</summary>
Motivation: 解决缺乏开源、通用、多语言词形变化系统的问题，特别是能够处理未见词汇的系统

Method: 使用联合训练的多语言模型，在73种语言数据上训练，引入频率加权、词干不相交的训练-开发-测试重采样方法

Result: 模型轻量级、对未见词汇鲁棒，在大多数语言中表现优于单语言基线

Conclusion: 证明了多语言建模在词形变化任务中的有效性，简化了部署过程，无需管理数十个单独的单语言模型

Abstract: We present a compact, single-model approach to multilingual inflection, the
task of generating inflected word forms from base lemmas to express grammatical
categories. Our model, trained jointly on data from 73 languages, is
lightweight, robust to unseen words, and outperforms monolingual baselines in
most languages. This demonstrates the effectiveness of multilingual modeling
for inflection and highlights its practical benefits: simplifying deployment by
eliminating the need to manage and retrain dozens of separate monolingual
models. In addition to the standard SIGMORPHON shared task benchmarks, we
evaluate our monolingual and multilingual models on 73 Universal Dependencies
(UD) treebanks, extracting lemma-tag-form triples and their frequency counts.
To ensure realistic data splits, we introduce a novel frequency-weighted,
lemma-disjoint train-dev-test resampling procedure. Our work addresses the lack
of an open-source, general-purpose, multilingual morphological inflection
system capable of handling unseen words across a wide range of languages,
including Czech. All code is publicly released at:
https://github.com/tomsouri/multilingual-inflection.

</details>


### [86] [Beyond Higher Rank: Token-wise Input-Output Projections for Efficient Low-Rank Adaptation](https://arxiv.org/abs/2510.23123)
*Shiwei Li,Xiandi Luo,Haozhao Wang,Xing Tang,Ziqiang Cui,Dugang Liu,Yuhua Li,Xiuqiang He,Ruixuan Li*

Main category: cs.CL

TL;DR: 提出了TopLoRA方法，通过动态调整LoRA权重以适应不同输入token，实现token级别的输入-输出投影，在多个模型和数据集上优于标准LoRA及其变体。


<details>
  <summary>Details</summary>
Motivation: 标准LoRA中所有输入token共享相同权重，限制了捕捉token特定信息的能力，因为不同token之间存在语义差异。

Method: TopLoRA将LoRA权重表示为BΣ_XA，其中A和B是低秩矩阵，Σ_X是根据每个输入token生成的对角矩阵，动态调整LoRA权重以实现token级别的投影。

Result: 在多个模型和数据集上的广泛实验表明，TopLoRA始终优于LoRA及其变体。

Conclusion: TopLoRA在不增加LoRA权重秩的情况下，通过学习token级别的LoRA权重实现了更细粒度的适应，提升了模型性能。

Abstract: Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) method
widely used in large language models (LLMs). LoRA essentially describes the
projection of an input space into a low-dimensional output space, with the
dimensionality determined by the LoRA rank. In standard LoRA, all input tokens
share the same weights and undergo an identical input-output projection. This
limits LoRA's ability to capture token-specific information due to the inherent
semantic differences among tokens. To address this limitation, we propose
Token-wise Projected Low-Rank Adaptation (TopLoRA), which dynamically adjusts
LoRA weights according to the input token, thereby learning token-wise
input-output projections in an end-to-end manner. Formally, the weights of
TopLoRA can be expressed as $B\Sigma_X A$, where $A$ and $B$ are low-rank
matrices (as in standard LoRA), and $\Sigma_X$ is a diagonal matrix generated
from each input token $X$. Notably, TopLoRA does not increase the rank of LoRA
weights but achieves more granular adaptation by learning token-wise LoRA
weights (i.e., token-wise input-output projections). Extensive experiments
across multiple models and datasets demonstrate that TopLoRA consistently
outperforms LoRA and its variants. The code is available at
https://github.com/Leopold1423/toplora-neurips25.

</details>


### [87] [Corpus Frequencies in Morphological Inflection: Do They Matter?](https://arxiv.org/abs/2510.23131)
*Tomáš Sourada,Jana Straková*

Main category: cs.CL

TL;DR: 该论文探索将语料库频率信息融入形态屈折任务中，通过频率加权的训练-开发-测试划分、引入词例准确率评估以及频率感知训练方法，在43种语言中的26种上优于均匀采样。


<details>
  <summary>Details</summary>
Motivation: 传统形态屈折方法缺乏词频分布信息，而实际部署中用户输入会反映自然文本的真实频率分布，因此需要将语料频率信息融入系统开发。

Method: 采用三种方法：1) 结合词干不相交和频率加权的训练-开发-测试划分；2) 在评估中补充词例准确率；3) 引入频率感知训练方法，将词频显式纳入采样过程。

Result: 频率感知训练在43种语言中的26种上优于均匀采样，表明频率信息能有效提升形态屈折性能。

Conclusion: 将语料频率信息融入形态屈折任务能更好地反映真实世界分布，频率感知训练方法在多数语言中表现优异，为实际部署提供了重要参考。

Abstract: The traditional approach to morphological inflection (the task of modifying a
base word (lemma) to express grammatical categories) has been, for decades, to
consider lexical entries of lemma-tag-form triples uniformly, lacking any
information about their frequency distribution. However, in production
deployment, one might expect the user inputs to reflect a real-world
distribution of frequencies in natural texts. With future deployment in mind,
we explore the incorporation of corpus frequency information into the task of
morphological inflection along three key dimensions during system development:
(i) for train-dev-test split, we combine a lemma-disjoint approach, which
evaluates the model's generalization capabilities, with a frequency-weighted
strategy to better reflect the realistic distribution of items across different
frequency bands in training and test sets; (ii) for evaluation, we complement
the standard type accuracy (often referred to simply as accuracy), which treats
all items equally regardless of frequency, with token accuracy, which assigns
greater weight to frequent words and better approximates performance on running
text; (iii) for training data sampling, we introduce a method novel in the
context of inflection, frequency-aware training, which explicitly incorporates
word frequency into the sampling process. We show that frequency-aware training
outperforms uniform sampling in 26 out of 43 languages.

</details>


### [88] [ENTP: Enhancing Low-Quality SFT Data via Neural-Symbolic Text Purge-Mix](https://arxiv.org/abs/2510.23160)
*Zile Yang,Ling Li,Na Di,Jinlong Pang,Yao Zhou,Hao Cheng,Bo Han,Jiaheng Wei*

Main category: cs.CL

TL;DR: ENTP框架通过符号化净化和神经重建方法，有效利用低质量监督微调数据，仅使用低质量数据构建的数据集在多个基准测试中超越了13种现有数据选择方法和完整原始数据集。


<details>
  <summary>Details</summary>
Motivation: 现有的监督微调方法通常只使用高质量数据，忽略了低质量数据中的有价值信号，且依赖不完美的质量过滤器。

Method: 提出ENTP框架，包含符号模块（基于统计先验识别和修剪噪声样本）和神经组件（利用潜在表示和模型知识合成丰富的指令-响应对）。

Result: 仅使用低质量数据构建的ENTP增强数据集在五个指令遵循基准测试中超越了13种现有数据选择方法，甚至超过了在完整原始数据集（约30万样本）上的微调效果。

Conclusion: 低质量数据具有未开发的潜力，智能净化和合成对于高效的指令对齐至关重要。

Abstract: Supervised Fine-Tuning (SFT) adapts pre-trained Large Language Models (LLMs)
to domain-specific instructions by training on a carefully curated subset of
high-quality instruction-response pairs, typically drawn from a larger dataset
that often contains many low-quality or noisy samples. However, existing
quality-first paradigms often overlook valuable signals in discarded
low-quality data and rely on imperfect quality filters. We introduce ENTP
(Enhancing low-quality SFT data via Neural-symbolic Text Purge-Mix), a
framework that revitalizes low-quality corpora through symbolic purification
and neural reconstruction. The symbolic module identifies and prunes noisy
samples based on statistical priors, while the neural component synthesizes
enriched instruction-response pairs by leveraging latent representations and
model knowledge. This neural-symbolic synergy enhances data informativeness and
diversity. Experiments show that ENTP-augmented datasets, constructed
exclusively from low-quality data, outperform 13 established data-selection
baselines across five instruction-following benchmarks, and even surpass
fine-tuning on the full original dataset (approximately 300K examples). Our
results highlight the untapped potential of low-quality data and underscore the
importance of intelligent purification and synthesis for efficient instruction
alignment.

</details>


### [89] [Beyond Direct Generation: A Decomposed Approach to Well-Crafted Screenwriting with LLMs](https://arxiv.org/abs/2510.23163)
*Hang Lei,Shengyi Zong,Zhaoyan Li,Ziren Zhou,Hao Liu*

Main category: cs.CL

TL;DR: 提出了双阶段优化框架，将剧本创作分解为创意叙事生成和格式转换两个独立阶段，解决了LLM在剧本创作中同时处理创意和格式的困难。


<details>
  <summary>Details</summary>
Motivation: 传统端到端方法无法生成高质量剧本，因为单一模型需要同时掌握创意叙事构建和严格格式遵循这两种不同能力，导致输出缺乏深层结构完整性。

Method: 采用双阶段优化框架：第一阶段将简要大纲转化为丰富的小说式散文，第二阶段将叙事精炼为专业格式的剧本。通过混合数据合成解决训练数据稀缺问题。

Result: 专业编剧的盲评显示，DSR在对抗Gemini-2.5-Pro等强基线时达到75%的胜率，达到人类水平表现的82.7%。

Conclusion: 分解生成架构结合定制化数据合成能有效专业化LLM在复杂创意领域的应用。

Abstract: The screenplay serves as the foundation for television production, defining
narrative structure, character development, and dialogue. While Large Language
Models (LLMs) show great potential in creative writing, direct end-to-end
generation approaches often fail to produce well-crafted screenplays. We argue
this failure stems from forcing a single model to simultaneously master two
disparate capabilities: creative narrative construction and rigid format
adherence. The resulting outputs may mimic superficial style but lack the deep
structural integrity and storytelling substance required for professional use.
To enable LLMs to generate high-quality screenplays, we introduce Dual-Stage
Refinement (DSR), a decomposed framework that decouples creative narrative
generation from format conversion. The first stage transforms a brief outline
into rich, novel-style prose. The second stage refines this narrative into a
professionally formatted screenplay. This separation enables the model to
specialize in one distinct capability at each stage. A key challenge in
implementing DSR is the scarcity of paired outline-to-novel training data. We
address this through hybrid data synthesis: reverse synthesis deconstructs
existing screenplays into structured inputs, while forward synthesis leverages
these inputs to generate high-quality narrative texts as training targets.
Blind evaluations by professional screenwriters show that DSR achieves a 75%
win rate against strong baselines like Gemini-2.5-Pro and reaches 82.7% of
human-level performance. Our work demonstrates that decomposed generation
architecture with tailored data synthesis effectively specializes LLMs in
complex creative domains.

</details>


### [90] [MATCH: Task-Driven Code Evaluation through Contrastive Learning](https://arxiv.org/abs/2510.23169)
*Marah Ghoummaid,Vladimir Tchuiev,Ofek Glick,Michal Moschkovitz,Dotan Di Castro*

Main category: cs.CL

TL;DR: MATCH是一种新的无参考代码评估指标，使用对比学习生成代码和自然语言任务描述的嵌入表示，通过相似性评分来评估生成代码与开发意图的匹配程度。


<details>
  <summary>Details</summary>
Motivation: AI代码生成日益普及，但准确评估生成代码与开发意图的匹配度仍具挑战。传统方法如单元测试成本高，语法相似性指标无法捕捉功能，现有无参考评估方法有限。

Method: 使用对比学习生成代码和自然语言任务描述的嵌入表示，通过计算两者相似性来评估代码实现任务的程度。

Result: MATCH在多种编程语言中与功能正确性和人类偏好的相关性均优于现有指标。

Conclusion: MATCH提供了一种有效的无参考代码评估方法，能更好地反映生成代码与开发意图的匹配程度。

Abstract: AI-based code generation is increasingly prevalent, with GitHub Copilot
estimated to generate 46% of the code on GitHub. Accurately evaluating how well
generated code aligns with developer intent remains a critical challenge.
Traditional evaluation methods, such as unit tests, are often unscalable and
costly. Syntactic similarity metrics (e.g., BLEU, ROUGE) fail to capture code
functionality, and metrics like CodeBERTScore require reference code, which is
not always available. To address the gap in reference-free evaluation, with few
alternatives such as ICE-Score, this paper introduces MATCH, a novel
reference-free metric. MATCH uses Contrastive Learning to generate meaningful
embeddings for code and natural language task descriptions, enabling similarity
scoring that reflects how well generated code implements the task. We show that
MATCH achieves stronger correlations with functional correctness and human
preference than existing metrics across multiple programming languages.

</details>


### [91] [SI-Bench: Benchmarking Social Intelligence of Large Language Models in Human-to-Human Conversations](https://arxiv.org/abs/2510.23182)
*Shuai Huang,Wenxuan Zhao,Jun Gao*

Main category: cs.CL

TL;DR: SI-Bench是一个评估大语言模型社会智能的新基准，包含2221个真实多轮对话，实验显示SOTA模型在复杂社交情境的推理能力超过人类专家，但在回复质量上仍落后于人类。


<details>
  <summary>Details</summary>
Motivation: 现有研究通过模拟智能体间交互构建数据集，无法捕捉真实人类对话的语言风格和关系动态，需要更真实的社交智能评估基准。

Method: 基于广泛的社会科学理论，从社交网络应用收集2221个真实多轮对话，并对312个对话进行人工标注，评估8个主要模型。

Result: SOTA模型在复杂社交情境的过程推理能力超过人类专家，但在回复质量上落后于人类，引入思维链推理反而会降低LLMs在社交对话任务中的表现。

Conclusion: LLMs在社交智能方面取得显著进展，但在真实对话质量上仍需改进，思维链推理在社交对话任务中可能产生负面影响。

Abstract: As large language models (LLMs) develop anthropomorphic abilities, they are
increasingly being deployed as autonomous agents to interact with humans.
However, evaluating their performance in realistic and complex social
interactions remains a significant challenge. Most previous research built
datasets through simulated agent-to-agent interactions, which fails to capture
the authentic linguistic styles and relational dynamics found in real human
conversations. To address this gap, we introduce SI-Bench, a novel benchmark
designed to evaluate aspects of social intelligence in LLMs. Grounded in broad
social science theories, SI-Bench contains 2,221 authentic multi-turn dialogues
collected from a social networking application. We further selected a subset of
312 dialogues for manual annotation across 8 major models. The experiments show
that SOTA models have surpassed the human expert in process reasoning under
complex social situations, yet they still fall behind humans in reply quality.
Moreover, introducing Chain-of-Thought (CoT) reasoning may degrade the
performance of LLMs in social dialogue tasks. All datasets are openly available
at https://github.com/SI-Bench/SI-Bench.git.

</details>


### [92] [DREaM: Drug-Drug Relation Extraction via Transfer Learning Method](https://arxiv.org/abs/2510.23189)
*Ali Fata,Hossein Rahmani,Parinaz Soltanzadeh,Amirhossein Derakhshan,Behrouz Minaei Bidgoli*

Main category: cs.CL

TL;DR: 提出DREAM方法，使用预训练关系抽取模型从医学文本中提取药物关系并构建本体，然后通过大语言模型验证抽取结果。


<details>
  <summary>Details</summary>
Motivation: 药物关系抽取对识别药物相互作用和预测副作用至关重要，但目前缺乏专门的数据集，需要采用迁移学习方法。

Method: 先使用训练好的关系抽取模型发现实体间关系，然后应用于医学文本语料库构建药物关系本体，最后用大语言模型验证抽取结果。

Result: 定量结果显示LLM对PubMed摘要子集中71%的抽取关系表示同意，定性分析表明该方法能揭示医学领域中的歧义问题。

Conclusion: 该方法能够以较低成本从医学文本中提取药物关系，但医学领域关系抽取仍面临挑战，特别是歧义性问题。

Abstract: Relation extraction between drugs plays a crucial role in identifying drug
drug interactions and predicting side effects. The advancement of machine
learning methods in relation extraction, along with the development of large
medical text databases, has enabled the low cost extraction of such relations
compared to other approaches that typically require expert knowledge. However,
to the best of our knowledge, there are limited datasets specifically designed
for drug drug relation extraction currently available. Therefore, employing
transfer learning becomes necessary to apply machine learning methods in this
domain. In this study, we propose DREAM, a method that first employs a trained
relation extraction model to discover relations between entities and then
applies this model to a corpus of medical texts to construct an ontology of
drug relationships. The extracted relations are subsequently validated using a
large language model. Quantitative results indicate that the LLM agreed with 71
of the relations extracted from a subset of PubMed abstracts. Furthermore, our
qualitative analysis indicates that this approach can uncover ambiguities in
the medical domain, highlighting the challenges inherent in relation extraction
in this field.

</details>


### [93] [Process Reward Models for Sentence-Level Verification of LVLM Radiology Reports](https://arxiv.org/abs/2510.23217)
*Alois Thomas,Maya Varma,Jean-Benoit Delbrouck,Curtis P. Langlotz*

Main category: cs.CL

TL;DR: 提出了一种句子级过程奖励模型（PRM），用于检测大型视觉语言模型在生成放射学报告时的临床幻觉问题，该模型在多个指标上优于现有方法，并能泛化到未见过的模型。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉语言模型在生成放射学报告时经常产生临床关键幻觉，存在严重风险，而现有的幻觉检测方法缺乏句子级粒度或跨模型的鲁棒泛化能力。

Method: 开发了一个轻量级的0.5B参数句子级过程奖励模型，在MIMIC-CXR数据集上使用弱监督标签进行微调，预测每个生成句子的真实性，并考虑临床上下文和先前文本。

Result: PRM在多个指标上优于现有验证技术，在Matthews相关系数上相对提升7.5%，AUROC提升1.8%。能有效过滤低质量报告，F1-CheXbert分数提升4.5%，在引导加权最佳选择过程中，F1-CheXbert相对提升7.4%。

Conclusion: 轻量级、上下文感知的PRM为临床LVLM提供了一个模型无关的安全层，无需访问内部激活状态。

Abstract: Automating radiology report generation with Large Vision-Language Models
(LVLMs) holds great potential, yet these models often produce clinically
critical hallucinations, posing serious risks. Existing hallucination detection
methods frequently lack the necessary sentence-level granularity or robust
generalization across different LVLM generators. We introduce a novel approach:
a sentence-level Process Reward Model (PRM) adapted for this vision-language
task. Our PRM predicts the factual correctness of each generated sentence,
conditioned on clinical context and preceding text. When fine-tuned on
MIMIC-CXR with weakly-supervised labels, a lightweight 0.5B-parameter PRM
outperforms existing verification techniques, demonstrating, for instance,
relative improvements of 7.5% in Matthews Correlation Coefficient and 1.8% in
AUROC over strong white-box baselines on outputs from one LVLM. Unlike methods
reliant on internal model states, our PRM demonstrates strong generalization to
an unseen LVLM. We further show its practical utility: PRM scores effectively
filter low-quality reports, improving F1-CheXbert scores by 4.5% (when
discarding the worst 10% of reports). Moreover, when guiding a novel weighted
best-of-N selection process on the MIMIC-CXR test set, our PRM show relative
improvements in clinical metrics of 7.4% for F1-CheXbert and 0.6% for
BERTScore. These results demonstrate that a lightweight, context-aware PRM
provides a model-agnostic safety layer for clinical LVLMs without access to
internal activations

</details>


### [94] [Are ASR foundation models generalized enough to capture features of regional dialects for low-resource languages?](https://arxiv.org/abs/2510.23252)
*Tawsif Tashwar Dipto,Azmol Hossain,Rubayet Sabbir Faruque,Md. Rezuwan Hassan,Kanij Fatema,Tanmoy Shome,Ruwad Naswan,Md. Foriduzzaman Zihad,Mohaymen Ul Anam,Nazia Tasnim,Hasan Mahmud,Md Kamrul Hasan,Md. Mehedi Hasan Shawon,Farig Sadeque,Tahsin Reasat*

Main category: cs.CL

TL;DR: 论文研究了方言变异对语音识别的影响，开发了78小时的孟加拉语语音转文本语料库Ben-10，发现语音基础模型在方言ASR中表现不佳，但方言特定训练能缓解问题。


<details>
  <summary>Details</summary>
Motivation: 传统语音识别研究依赖标准形式，而方言ASR被视为微调任务，需要研究方言变异对ASR的影响。

Method: 开发Ben-10语料库，从语言学和数据驱动角度分析，比较零样本和微调设置下的表现，进行方言特定模型训练。

Result: 语音基础模型在方言ASR中表现严重不足，所有深度学习方法都难以处理方言变异数据，但方言特定训练能改善性能。

Conclusion: 方言变异对ASR构成重大挑战，Ben-10数据集可作为资源受限条件下ASR建模的分布外资源。

Abstract: Conventional research on speech recognition modeling relies on the canonical
form for most low-resource languages while automatic speech recognition (ASR)
for regional dialects is treated as a fine-tuning task. To investigate the
effects of dialectal variations on ASR we develop a 78-hour annotated Bengali
Speech-to-Text (STT) corpus named Ben-10. Investigation from linguistic and
data-driven perspectives shows that speech foundation models struggle heavily
in regional dialect ASR, both in zero-shot and fine-tuned settings. We observe
that all deep learning methods struggle to model speech data under dialectal
variations but dialect specific model training alleviates the issue. Our
dataset also serves as a out of-distribution (OOD) resource for ASR modeling
under constrained resources in ASR algorithms. The dataset and code developed
for this project are publicly available

</details>


### [95] [Mubeen AI: A Specialized Arabic Language Model for Heritage Preservation and User Intent Understanding](https://arxiv.org/abs/2510.23271)
*Mohammed Aljafari,Ismail Alturki,Ahmed Mori,Yehya Kadumi*

Main category: cs.CL

TL;DR: Mubeen是一个专有的阿拉伯语语言模型，专注于阿拉伯语言学、伊斯兰研究和文化遗产的深度理解，通过原生阿拉伯语源确保文化真实性和准确性，采用实用闭合架构解决用户核心需求未满足的问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有阿拉伯语模型依赖英语翻译数据导致意图检测和检索增强生成失败的问题，确保阿拉伯语言和文化的真实性，并解决用户核心需求未满足的“效用差距危机”。

Method: 基于大量真实阿拉伯语源训练，包括历史手稿数字化、学术著作、学术论文等，采用深度语言工程框架和实用闭合架构，结合多学科专家模块。

Result: 模型能够精确理解古典文本、当代写作和地区方言，掌握阿拉伯语的优雅表达，在文化保存和通用知识领域均表现出色。

Conclusion: Mubeen从信息存储库转变为决定性指南，与沙特2030愿景保持一致，在阿拉伯语言和文化理解方面实现了突破。

Abstract: Mubeen is a proprietary Arabic language model developed by MASARAT SA,
optimized for deep understanding of Arabic linguistics, Islamic studies, and
cultural heritage. Trained on an extensive collection of authentic Arabic
sources significantly expanded by digitizing historical manuscripts via a
proprietary Arabic OCR engine, the model incorporates seminal scholarly works
in linguistics, jurisprudence, hadith, and Quranic exegesis, alongside
thousands of academic theses and peer-reviewed research papers. Conditioned
through a deep linguistic engineering framework, Mubeen masters not just the
meaning but the eloquence of Arabic, enabling precise understanding across
classical texts, contemporary writing, and regional dialects with focus on
comprehending user intent and delivering accurate, contextually relevant
responses. Unlike other Arabic models relying on translated English data that
often fail in intent detection or retrieval-augmented generation (RAG), Mubeen
uses native Arabic sources to ensure cultural authenticity and accuracy. Its
core innovation is the Practical Closure Architecture, designed to solve the
"Utility Gap Crisis" where factually correct answers fail to resolve users'
core needs, forcing them into frustrating cycles of re-prompting. By
prioritizing clarity and decisive guidance, Mubeen transforms from an
information repository into a decisive guide, aligning with Saudi Vision 2030.
The model's architecture combines deep heritage specialization with
multi-disciplinary expert modules, enabling robust performance across both
cultural preservation and general knowledge domains.

</details>


### [96] [Code Aesthetics with Agentic Reward Feedback](https://arxiv.org/abs/2510.23272)
*Bang Xiao,Lingjie Jiang,Shaohan Huang,Tengchao Lv,Yupan Huang,Xun Wu,Lei Cui,Furu Wei*

Main category: cs.CL

TL;DR: 提出了一个增强LLM生成代码美观性的新流程，包括构建大规模代码美学数据集AesCode-358K、多智能体奖励反馈系统，以及结合监督微调和强化学习的优化方法。


<details>
  <summary>Details</summary>
Motivation: LLM在传统编程任务中表现出色，但在视觉导向的编码任务中往往产生美学效果不佳的代码，需要提升代码的美学质量。

Method: 1. 构建AesCode-358K代码美学指令调优数据集；2. 提出多智能体奖励反馈系统评估可执行性、静态美学和交互美学；3. 开发GRPO-AR算法联合优化功能性和代码美学；4. 创建OpenDesign代码美学评估基准。

Result: 结合AesCode-358K监督微调和基于智能体奖励反馈的强化学习，在OpenDesign基准上显著提升性能，并在PandasPlotBench等现有基准上也有改进。AesCoder-4B超越GPT-4o和GPT-4.1，性能可与480B-685B参数的大型开源模型相媲美。

Conclusion: 该方法有效提升了LLM生成代码的美学质量，证明了结合大规模美学数据集和多智能体奖励反馈的强化学习方法的有效性。

Abstract: Large Language Models (LLMs) have become valuable assistants for developers
in code-related tasks. While LLMs excel at traditional programming tasks such
as code generation and bug fixing, they struggle with visually-oriented coding
tasks, often producing suboptimal aesthetics. In this paper, we introduce a new
pipeline to enhance the aesthetic quality of LLM-generated code. We first
construct AesCode-358K, a large-scale instruction-tuning dataset focused on
code aesthetics. Next, we propose agentic reward feedback, a multi-agent system
that evaluates executability, static aesthetics, and interactive aesthetics.
Building on this, we develop GRPO-AR, which integrates these signals into the
GRPO algorithm for joint optimization of functionality and code aesthetics.
Finally, we develop OpenDesign, a benchmark for assessing code aesthetics.
Experimental results show that combining supervised fine-tuning on AesCode-358K
with reinforcement learning using agentic reward feedback significantly
improves performance on OpenDesign and also enhances results on existing
benchmarks such as PandasPlotBench. Notably, our AesCoder-4B surpasses GPT-4o
and GPT-4.1, and achieves performance comparable to large open-source models
with 480B-685B parameters, underscoring the effectiveness of our approach.

</details>


### [97] [A Cocktail-Party Benchmark: Multi-Modal dataset and Comparative Evaluation Results](https://arxiv.org/abs/2510.23276)
*Thai-Binh Nguyen,Katerina Zmolikova,Pingchuan Ma,Ngoc Quan Pham,Christian Fuegen,Alexander Waibel*

Main category: cs.CL

TL;DR: 第九届CHiME挑战赛引入多模态上下文感知识别任务，解决单房间环境下重叠对话的鸡尾酒会问题，使用音频、视觉和上下文线索来识别谁在何时说了什么以及与谁对话。


<details>
  <summary>Details</summary>
Motivation: 解决自然多参与者对话中极端语音重叠（高达100%）和高度碎片化对话轮次的问题，传统音频方法无法有效处理此类场景。

Method: 通过音频-视觉记录联合转录每个说话者的语音，并将其聚类到各自的对话中，利用多模态信息提升识别效果。

Result: 纯音频基线的词错误率超过100%，而加入视觉线索后性能提升50%，证明多模态方法的重要性。

Conclusion: 多模态上下文感知识别是解决重叠对话问题的有效方法，视觉信息对提升系统性能至关重要。

Abstract: We introduce the task of Multi-Modal Context-Aware Recognition (MCoRec) in
the ninth CHiME Challenge, which addresses the cocktail-party problem of
overlapping conversations in a single-room setting using audio, visual, and
contextual cues. MCoRec captures natural multi-party conversations where the
recordings focus on unscripted, casual group chats, leading to extreme speech
overlap of up to 100% and highly fragmented conversational turns. The task
requires systems to answer the question "Who speaks when, what, and with whom?"
by jointly transcribing each speaker's speech and clustering them into their
respective conversations from audio-visual recordings. Audio-only baselines
exceed 100% word error rate, whereas incorporating visual cues yields
substantial 50% improvements, highlighting the importance of multi-modality. In
this manuscript, we present the motivation behind the task, outline the data
collection process, and report the baseline systems developed for the MCoRec.

</details>


### [98] [DCMM-SQL: Automated Data-Centric Pipeline and Multi-Model Collaboration Training for Text-to-SQL Model](https://arxiv.org/abs/2510.23284)
*Yuanzhen Xie,Liu Ye,Jiqun Chu,Mochi Gao,Hehuan Liu,Yunzhi Tan,Bo Hu,Zang Li*

Main category: cs.CL

TL;DR: 本文提出了一个完全自动化的数据为中心文本到SQL任务管道，包含自适应数据修复和错误数据增强，以及多模型协作训练方案，在轻量级文本到SQL模型中取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 虽然基于代理的框架在文本到SQL任务中取得了显著改进，但数据为中心策略的影响很少被探索。现有方法中单个微调模型的能力非常有限。

Method: 设计了全自动数据为中心管道：自适应数据修复自动发现和修复训练数据集中的错误；错误数据增强扩散和增强初始训练模型预测的错误数据；多模型协作训练方案训练具有不同增强数据的多个模型；使用集成策略整合多个模型能力。

Result: 实验和消融研究证明了数据为中心管道和多模型交互迭代策略的有效性，在轻量级文本到SQL模型（70B以内）中取得了第一名。

Conclusion: 数据为中心的方法和多模型协作策略显著提升了文本到SQL任务的性能，证明了数据质量和模型多样性的重要性。

Abstract: Text-to-SQL tasks have gained attractive improvements since the release of
ChatGPT. Among them, agent-based frameworks have been widely used in this
field. However, the impact of data-centric strategies on text-to-SQL tasks has
rarely been explored. In this paper, we systemically design a fully automated
data-centric pipeline for text-to-SQL tasks, including \emph{adaptive data
repair}, which can automatically find and fix errors in the training dataset;
and \emph{error data augmentation}, where we specifically diffuse and enhance
erroneous data predicted by the initially trained models. Meanwhile, we propose
a Multi-Model collaboration training schema, aiming to train multiple models
with different augmented data, enabling them to possess distinct capabilities
and work together to complement each other, because it has been found that the
capability of a single fine-tuned model is very limited. Furthermore, we
utilize an ensemble strategy to integrate the capabilities of multiple models
to solve a multiple-choice question, aiming to further improve the accuracy of
text-to-SQL tasks. The experiment results and ablation study have demonstrated
the effectiveness of data-centric pipeline and Multi-Model(MM) interactive
iterative strategies, achieving first place in lightweight text-to-SQL models
(within 70B).

</details>


### [99] [Arabic Little STT: Arabic Children Speech Recognition Dataset](https://arxiv.org/abs/2510.23319)
*Mouhand Alkadri,Dania Desouki,Khloud Al Jallad*

Main category: cs.CL

TL;DR: 本文介绍了Arabic Little STT数据集，包含288名6-13岁儿童的355条黎凡特阿拉伯语课堂录音，并评估了Whisper模型在儿童语音识别上的表现，发现其性能远低于成人语音识别。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言阿拉伯语中儿童特定语音语料库缺失的问题，填补儿童语音识别研究的空白。

Method: 创建Arabic Little STT儿童语音数据集，系统评估8个Whisper变体模型在儿童语音识别上的表现，并与成人阿拉伯语基准进行比较。

Result: 即使性能最好的Whisper Large_v3模型在儿童语音上的词错误率高达0.66，远高于其在成人数据集上的低于0.20的词错误率。

Conclusion: 强调在ASR开发中需要专门的儿童语音基准和包容性训练数据，且此类数据必须遵循严格的伦理和隐私框架来保护儿童敏感信息。

Abstract: The performance of Artificial Intelligence (AI) systems fundamentally depends
on high-quality training data. However, low-resource languages like Arabic
suffer from severe data scarcity. Moreover, the absence of child-specific
speech corpora is an essential gap that poses significant challenges. To
address this gap, we present our created dataset, Arabic Little STT, a dataset
of Levantine Arabic child speech recorded in classrooms, containing 355
utterances from 288 children (ages 6 - 13). We further conduct a systematic
assessment of Whisper, a state-of-the-art automatic speech recognition (ASR)
model, on this dataset and compare its performance with adult Arabic
benchmarks. Our evaluation across eight Whisper variants reveals that even the
best-performing model (Large_v3) struggles significantly, achieving a 0.66 word
error rate (WER) on child speech, starkly contrasting with its sub 0.20 WER on
adult datasets. These results align with other research on English speech.
Results highlight the critical need for dedicated child speech benchmarks and
inclusive training data in ASR development. Emphasizing that such data must be
governed by strict ethical and privacy frameworks to protect sensitive child
information. We hope that this study provides an initial step for future work
on equitable speech technologies for Arabic-speaking children. We hope that our
publicly available dataset enrich the children's demographic representation in
ASR datasets.

</details>


### [100] [Adaptive Blockwise Search: Inference-Time Alignment for Large Language Models](https://arxiv.org/abs/2510.23334)
*Mohammad Atif Quamar,Mohammad Areeb,Nishant Sharma,Ananth Shreekumar,Jonathan Rosenthal,Muslum Ozgur Ozmen,Mikhail Kuznetsov,Z. Berkay Celik*

Main category: cs.CL

TL;DR: AdaSearch是一种新颖的分块搜索策略，通过自适应分配固定计算预算，重点关注响应中关键的前几个token，在LLM对齐任务中显著优于Best-of-N和微调基线。


<details>
  <summary>Details</summary>
Motivation: LLM对齐仍是一个关键挑战。推理时方法提供了比微调更灵活的替代方案，但其均匀的计算努力通常产生次优的对齐效果。作者假设在许多对齐任务中，响应的初始token具有不成比例的重要性。

Method: 引入AdaSearch，一种新颖的分块搜索策略，使用采样计划自适应分配固定计算预算，将搜索努力集中在关键token上。还提出了其树搜索对应方法AdaBeam，应用于顺序解码。

Result: 在八个LLM上的综合评估表明，AdaSearch优于强大的Best-of-N和微调基线。在无害生成、受控情感生成和数学推理任务中，相对于Best-of-N的胜率提高了超过10%。

Conclusion: AdaSearch通过自适应关注关键token，在LLM对齐任务中实现了显著改进，证明了在推理时方法中非均匀计算分配的有效性。

Abstract: LLM alignment remains a critical challenge. Inference-time methods provide a
flexible alternative to fine-tuning, but their uniform computational effort
often yields suboptimal alignment. We hypothesize that for many alignment
tasks, the initial tokens of a response are disproportionately more critical.
To leverage this principle, we introduce AdaSearch, a novel blockwise search
strategy. It adaptively allocates a fixed computational budget using a sampling
schedule, focusing search effort on these critical tokens. We apply AdaSearch
to sequential decoding and introduce its tree-search counterpart, AdaBeam. Our
comprehensive evaluation across eight LLMs demonstrates that AdaSearch
outperforms strong Best-of-N and fine-tuning baselines. Specifically, win-rates
improve by over 10% for harmlessness generation, controlled sentiment
generation, and for mathematical reasoning tasks relative to Best-of-N.

</details>


### [101] [BaZi-Based Character Simulation Benchmark: Evaluating AI on Temporal and Persona Reasoning](https://arxiv.org/abs/2510.23337)
*Siyuan Zheng,Pai Liu,Xi Chen,Jizheng Dong,Sihan Jia*

Main category: cs.CL

TL;DR: 提出了首个基于八字命理的人格推理问答数据集和BaZi-LLM系统，将符号推理与大语言模型结合，生成时间动态和细粒度的虚拟人格，相比主流LLM准确率提升30.3%-62.6%。


<details>
  <summary>Details</summary>
Motivation: 当前虚拟角色生成方法依赖标注数据或手工制作的人格提示，难以扩展并生成真实、上下文一致的人格。需要一种能生成文化基础、时间动态的虚拟人格的方法。

Method: 创建了首个八字命理人格推理问答数据集，将人类经验分类为财富、健康、亲情、职业和关系等生活事件问答。提出了BaZi-LLM系统，整合符号推理与大语言模型。

Result: 相比DeepSeek-v3和GPT-5-mini等主流LLM，准确率提升30.3%-62.6%。当使用错误的八字信息时，模型准确率下降20%-45%，验证了文化基础的符号-LLM整合的有效性。

Conclusion: 基于文化基础的符号推理与LLM整合在真实角色模拟方面具有潜力，能够生成更真实、上下文一致的虚拟人格。

Abstract: Human-like virtual characters are crucial for games, storytelling, and
virtual reality, yet current methods rely heavily on annotated data or
handcrafted persona prompts, making it difficult to scale up and generate
realistic, contextually coherent personas. We create the first QA dataset for
BaZi-based persona reasoning, where real human experiences categorized into
wealth, health, kinship, career, and relationships are represented as
life-event questions and answers. Furthermore, we propose the first BaZi-LLM
system that integrates symbolic reasoning with large language models to
generate temporally dynamic and fine-grained virtual personas. Compared with
mainstream LLMs such as DeepSeek-v3 and GPT-5-mini, our method achieves a
30.3%-62.6% accuracy improvement. In addition, when incorrect BaZi information
is used, our model's accuracy drops by 20%-45%, showing the potential of
culturally grounded symbolic-LLM integration for realistic character
simulation.

</details>


### [102] [LightKGG: Simple and Efficient Knowledge Graph Generation from Textual Data](https://arxiv.org/abs/2510.23341)
*Teng Lin*

Main category: cs.CL

TL;DR: LightKGG是一个使用小型语言模型从文本数据中高效提取知识图谱的框架，通过上下文集成图提取和拓扑增强关系推理两个关键技术，在低资源环境下实现准确的知识图谱构建。


<details>
  <summary>Details</summary>
Motivation: 解决高质量知识图谱稀缺的问题，现有提取方法依赖错误率高的模式匹配技术或资源密集型大语言模型，计算需求限制了在低资源环境中的可访问性。

Method: 1. 上下文集成图提取：将上下文信息与节点和边集成到统一图结构中，减少对复杂语义处理的依赖；2. 拓扑增强关系推理：利用提取图的固有拓扑结构高效推断关系，无需依赖LLMs的复杂语言理解能力。

Result: 能够以最小硬件需求实现准确的知识图谱构建，弥合了自动化知识提取与实际部署场景之间的差距。

Conclusion: 该工作为优化小型语言模型在结构化NLP任务中的效率引入了科学严谨的方法，使知识图谱提取在低资源环境中变得可行。

Abstract: The scarcity of high-quality knowledge graphs (KGs) remains a critical
bottleneck for downstream AI applications, as existing extraction methods rely
heavily on error-prone pattern-matching techniques or resource-intensive large
language models (LLMs). While recent tools leverage LLMs to generate KGs, their
computational demands limit accessibility for low-resource environments. Our
paper introduces LightKGG, a novel framework that enables efficient KG
extraction from textual data using small-scale language models (SLMs) through
two key technical innovations: (1) Context-integrated Graph extraction
integrates contextual information with nodes and edges into a unified graph
structure, reducing the reliance on complex semantic processing while
maintaining more key information; (2) Topology-enhanced relationship inference
leverages the inherent topology of the extracted graph to efficiently infer
relationships, enabling relationship discovery without relying on complex
language understanding capabilities of LLMs. By enabling accurate KG
construction with minimal hardware requirements, this work bridges the gap
between automated knowledge extraction and practical deployment scenarios while
introducing scientifically rigorous methods for optimizing SLM efficiency in
structured NLP tasks.

</details>


### [103] [How AI Forecasts AI Jobs: Benchmarking LLM Predictions of Labor Market Changes](https://arxiv.org/abs/2510.23358)
*Sheri Osborn,Rohit Valecha,H. Raghav Rao,Dan Sass,Anthony Rios*

Main category: cs.CL

TL;DR: 该论文提出了一个评估大语言模型预测AI对就业影响能力的基准，结合了美国高频职位发布数据和全球AI采用导致的职业变化预测数据，测试了多种提示策略在不同行业和时间范围内的表现。


<details>
  <summary>Details</summary>
Motivation: 人工智能正在重塑劳动力市场，但缺乏系统预测其就业影响的工具。现有研究表明LLMs能够提取情感、总结经济报告和模拟预测者行为，但很少评估其在前瞻性劳动力预测中的应用。

Method: 结合两个互补数据集：美国部门级高频职位发布指数和全球AI采用导致的职业变化预测数据，构建具有明确时间划分的预测任务。评估多种提示策略（任务支架、角色驱动和混合方法）在不同模型系列中的表现。

Result: 结构化任务提示持续提高预测稳定性，角色提示在短期趋势上有优势。但性能在不同行业和时间范围内差异显著，表明需要领域感知提示和严格评估协议。

Conclusion: 该基准为劳动力预测、提示设计和基于LLM的经济推理研究提供支持，有助于研究AI作为预测工具在劳动力市场背景下的局限性和机会。

Abstract: Artificial intelligence is reshaping labor markets, yet we lack tools to
systematically forecast its effects on employment. This paper introduces a
benchmark for evaluating how well large language models (LLMs) can anticipate
changes in job demand, especially in occupations affected by AI. Existing
research has shown that LLMs can extract sentiment, summarize economic reports,
and emulate forecaster behavior, but little work has assessed their use for
forward-looking labor prediction. Our benchmark combines two complementary
datasets: a high-frequency index of sector-level job postings in the United
States, and a global dataset of projected occupational changes due to AI
adoption. We format these data into forecasting tasks with clear temporal
splits, minimizing the risk of information leakage. We then evaluate LLMs using
multiple prompting strategies, comparing task-scaffolded, persona-driven, and
hybrid approaches across model families. We assess both quantitative accuracy
and qualitative consistency over time. Results show that structured task
prompts consistently improve forecast stability, while persona prompts offer
advantages on short-term trends. However, performance varies significantly
across sectors and horizons, highlighting the need for domain-aware prompting
and rigorous evaluation protocols. By releasing our benchmark, we aim to
support future research on labor forecasting, prompt design, and LLM-based
economic reasoning. This work contributes to a growing body of research on how
LLMs interact with real-world economic data, and provides a reproducible
testbed for studying the limits and opportunities of AI as a forecasting tool
in the context of labor markets.

</details>


### [104] [Detecting Religious Language in Climate Discourse](https://arxiv.org/abs/2510.23395)
*Evy Beijen,Pien Pieterse,Yusuf Çelik,Willem Th. van Peursen,Sandjai Bhulai,Meike Morren*

Main category: cs.CL

TL;DR: 该研究比较了基于规则的方法和大型语言模型在检测气候相关文本中宗教语言的效果，发现基于规则的方法标记出更多宗教句子，揭示了宗教语言检测的方法学挑战。


<details>
  <summary>Details</summary>
Motivation: 研究宗教语言在当代话语中的持续存在，特别是在环境行动主义和气候变化辩论等看似世俗的领域中，探索宗教语言在世俗和宗教非政府组织气候文本中的显性和隐性形式。

Method: 采用双重方法：基于生态神学文献构建宗教术语层次树的规则模型，以及在零样本设置下运行的大型语言模型，使用包含88万多个句子的数据集进行比较分析。

Result: 基于规则的方法比大型语言模型一致性地标记出更多句子为宗教语言，两种方法在检测宗教语言时存在一致性和分歧点。

Conclusion: 研究不仅突显了计算检测宗教语言的方法学挑战，还揭示了宗教语言应仅由词汇定义还是由上下文意义定义的更广泛张力，展示了分析神圣在气候话语中持续存在的潜力和局限性。

Abstract: Religious language continues to permeate contemporary discourse, even in
ostensibly secular domains such as environmental activism and climate change
debates. This paper investigates how explicit and implicit forms of religious
language appear in climate-related texts produced by secular and religious
nongovernmental organizations (NGOs). We introduce a dual methodological
approach: a rule-based model using a hierarchical tree of religious terms
derived from ecotheology literature, and large language models (LLMs) operating
in a zero-shot setting. Using a dataset of more than 880,000 sentences, we
compare how these methods detect religious language and analyze points of
agreement and divergence. The results show that the rule-based method
consistently labels more sentences as religious than LLMs. These findings
highlight not only the methodological challenges of computationally detecting
religious language but also the broader tension over whether religious language
should be defined by vocabulary alone or by contextual meaning. This study
contributes to digital methods in religious studies by demonstrating both the
potential and the limitations of approaches for analyzing how the sacred
persists in climate discourse.

</details>


### [105] [EMTSF:Extraordinary Mixture of SOTA Models for Time Series Forecasting](https://arxiv.org/abs/2510.23396)
*Musleh Alharthi,Kaleel Mahmood,Sarosh Patel,Ausif Mahmood*

Main category: cs.CL

TL;DR: 提出了一种基于混合专家(MoE)框架的时间序列预测模型，整合了xLSTM、增强线性模型、PatchTST和minGRU等多种SOTA模型，通过Transformer门控网络实现，在标准基准测试中超越了所有现有TSF模型。


<details>
  <summary>Details</summary>
Motivation: 针对时间序列预测领域Transformer模型有效性争议，以及数据偏向近期且易受不可预测事件影响的特点，基于近期TSF研究洞察，构建更强大的预测框架。

Method: 采用混合专家(MoE)框架，集成xLSTM、增强线性模型、PatchTST、minGRU等多种互补的SOTA模型，使用基于Transformer的门控网络进行模型整合。

Result: 在标准基准测试中，提出的模型超越了所有现有时间序列预测模型，包括最新的基于MoE框架的方法。

Conclusion: 通过整合多种互补的SOTA模型，基于MoE框架的方法能够有效提升时间序列预测性能，证明了模型多样性在TSF任务中的重要性。

Abstract: The immense success of the Transformer architecture
  in Natural Language Processing has led to its adoption in Time Se ries
Forecasting (TSF), where superior performance has been shown.
  However, a recent important paper questioned their effectiveness by
  demonstrating that a simple single layer linear model outperforms
  Transformer-based models. This was soon shown to be not as valid,
  by a better transformer-based model termed PatchTST. More re cently, TimeLLM
demonstrated even better results by repurposing a
  Large Language Model (LLM) for the TSF domain. Again, a follow
  up paper challenged this by demonstrating that removing the LLM
  component or replacing it with a basic attention layer in fact yields
  better performance. One of the challenges in forecasting is the fact
  that TSF data favors the more recent past, and is sometimes subject
  to unpredictable events. Based upon these recent insights in TSF, we
  propose a strong Mixture of Experts (MoE) framework. Our method
  combines the state-of-the-art (SOTA) models including xLSTM, en hanced
Linear, PatchTST, and minGRU, among others. This set of
  complimentary and diverse models for TSF are integrated in a Trans former
based MoE gating network. Our proposed model outperforms
  all existing TSF models on standard benchmarks, surpassing even the
  latest approaches based on MoE frameworks.

</details>


### [106] [Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences](https://arxiv.org/abs/2510.23451)
*Zhuoran Jin,Hongbang Yuan,Kejian Zhu,Jiachun Li,Pengfei Cao,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 提出了Omni-Reward，一个支持自由形式偏好的通用多模态奖励模型，解决了模态不平衡和偏好刚性问题。


<details>
  <summary>Details</summary>
Motivation: 当前奖励模型面临两个基本挑战：模态不平衡（主要关注文本和图像，对视频、音频等模态支持有限）和偏好刚性（基于固定二元偏好对训练无法捕捉个性化偏好的复杂性）。

Method: 包含三个部分：Omni-RewardBench（首个支持自由形式偏好的多模态奖励模型基准）、Omni-RewardData（包含248K通用偏好对和69K指令调优对的多模态偏好数据集）、Omni-RewardModel（包含判别式和生成式奖励模型）。

Result: 在Omni-RewardBench以及其他广泛使用的奖励建模基准上取得了强劲性能。

Conclusion: Omni-Reward为通用多模态奖励建模迈出了重要一步，支持自由形式偏好，覆盖文本、图像、视频、音频和3D五种模态的九个任务。

Abstract: Reward models (RMs) play a critical role in aligning AI behaviors with human
preferences, yet they face two fundamental challenges: (1) Modality Imbalance,
where most RMs are mainly focused on text and image modalities, offering
limited support for video, audio, and other modalities; and (2) Preference
Rigidity, where training on fixed binary preference pairs fails to capture the
complexity and diversity of personalized preferences. To address the above
challenges, we propose Omni-Reward, a step toward generalist omni-modal reward
modeling with support for free-form preferences, consisting of: (1) Evaluation:
We introduce Omni-RewardBench, the first omni-modal RM benchmark with free-form
preferences, covering nine tasks across five modalities including text, image,
video, audio, and 3D; (2) Data: We construct Omni-RewardData, a multimodal
preference dataset comprising 248K general preference pairs and 69K
instruction-tuning pairs for training generalist omni-modal RMs; (3) Model: We
propose Omni-RewardModel, which includes both discriminative and generative
RMs, and achieves strong performance on Omni-RewardBench as well as other
widely used reward modeling benchmarks.

</details>


### [107] [BrowseConf: Confidence-Guided Test-Time Scaling for Web Agents](https://arxiv.org/abs/2510.23458)
*Litu Ou,Kuan Li,Huifeng Yin,Liwen Zhang,Zhongwang Zhang,Xixi Wu,Rui Ye,Zile Qiao,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: 研究探索LLM搜索代理在多轮交互中通过语言化置信度分数表达自身置信度的能力，并提出基于置信度的测试时缩放方法，在保持竞争力的同时显著减少token消耗。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注单轮场景下的置信度，而在复杂多轮交互中的置信度研究有限。本文旨在研究LLM搜索代理是否能够在长序列动作后通过语言化置信度分数传达自身置信度。

Method: 在开源代理模型上进行实验，发现模型在高置信度时任务准确率更高，低置信度时准确率接近零。基于此提出测试时缩放方法，使用置信度分数确定答案质量，鼓励模型重试直到达到满意置信水平。

Result: 实验结果显示，模型在高置信度时任务准确率显著更高，而低置信度时准确率接近零。提出的测试时缩放方法在保持竞争力的同时显著减少了token消耗。

Conclusion: LLM搜索代理能够在多轮交互中有效传达置信度，基于置信度的测试时缩放方法能够在不牺牲性能的情况下优化资源使用。

Abstract: Confidence in LLMs is a useful indicator of model uncertainty and answer
reliability. Existing work mainly focused on single-turn scenarios, while
research on confidence in complex multi-turn interactions is limited. In this
paper, we investigate whether LLM-based search agents have the ability to
communicate their own confidence through verbalized confidence scores after
long sequences of actions, a significantly more challenging task compared to
outputting confidence in a single interaction. Experimenting on open-source
agentic models, we first find that models exhibit much higher task accuracy at
high confidence while having near-zero accuracy when confidence is low. Based
on this observation, we propose Test-Time Scaling (TTS) methods that use
confidence scores to determine answer quality, encourage the model to try again
until reaching a satisfactory confidence level. Results show that our proposed
methods significantly reduce token consumption while demonstrating competitive
performance compared to baseline fixed budget TTS methods.

</details>


### [108] [Evaluating Large Language Models for Stance Detection on Financial Targets from SEC Filing Reports and Earnings Call Transcripts](https://arxiv.org/abs/2510.23464)
*Nikesh Gyawali,Doina Caragea,Alex Vasenkov,Cornelia Caragea*

Main category: cs.CL

TL;DR: 提出了一个针对债务、每股收益和销售额三个财务指标的句子级立场检测语料库，使用ChatGPT-o3-pro模型标注并经过人工验证，评估了LLM在零样本、少样本和思维链提示下的表现。


<details>
  <summary>Details</summary>
Motivation: SEC文件和财报电话会议记录对投资者很重要，但其长度、专业术语和微妙语言使细粒度分析困难，传统方法需要大量标注数据。

Method: 从10-K年报和财报电话会议记录中提取句子，使用ChatGPT-o3-pro模型标注立场（积极、消极、中性），并系统评估LLM在零样本、少样本和思维链提示下的表现。

Result: 少样本结合思维链提示表现最佳，优于监督基线，LLM在SEC和ECT数据集上的表现存在差异。

Conclusion: LLM可以在不需要大量标注数据的情况下，有效用于金融领域特定目标的立场检测。

Abstract: Financial narratives from U.S. Securities and Exchange Commission (SEC)
filing reports and quarterly earnings call transcripts (ECTs) are very
important for investors, auditors, and regulators. However, their length,
financial jargon, and nuanced language make fine-grained analysis difficult.
Prior sentiment analysis in the financial domain required a large, expensive
labeled dataset, making the sentence-level stance towards specific financial
targets challenging. In this work, we introduce a sentence-level corpus for
stance detection focused on three core financial metrics: debt, earnings per
share (EPS), and sales. The sentences were extracted from Form 10-K annual
reports and ECTs, and labeled for stance (positive, negative, neutral) using
the advanced ChatGPT-o3-pro model under rigorous human validation. Using this
corpus, we conduct a systematic evaluation of modern large language models
(LLMs) using zero-shot, few-shot, and Chain-of-Thought (CoT) prompting
strategies. Our results show that few-shot with CoT prompting performs best
compared to supervised baselines, and LLMs' performance varies across the SEC
and ECT datasets. Our findings highlight the practical viability of leveraging
LLMs for target-specific stance in the financial domain without requiring
extensive labeled data.

</details>


### [109] [MMTutorBench: The First Multimodal Benchmark for AI Math Tutoring](https://arxiv.org/abs/2510.23477)
*Tengchao Yang,Sichen Guo,Mengzhao Jia,Jiaming Su,Yuanyang Liu,Zhihan Zhang,Meng Jiang*

Main category: cs.CL

TL;DR: 提出了MMTutorBench，首个AI数学辅导基准，包含685个围绕关键教学步骤构建的问题，用于评估多模态大语言模型的辅导能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准大多忽视了数学辅导所需的诊断学生困难和逐步引导的能力，而多模态大语言模型在这方面具有潜力。

Method: 构建包含685个问题的基准，每个问题配有特定评分标准，分为洞察发现、操作制定和操作执行三个任务，评估12个领先的MLLM。

Result: 发现专有和开源系统之间存在明显性能差距，与人类导师相比还有很大提升空间，OCR流程会降低辅导质量，少样本提示效果有限，基于评分标准的LLM评判高度可靠。

Conclusion: MMTutorBench既展示了AI数学辅导的难度，也证明了其作为诊断工具的价值，有助于推动AI辅导的发展。

Abstract: Effective math tutoring requires not only solving problems but also
diagnosing students' difficulties and guiding them step by step. While
multimodal large language models (MLLMs) show promise, existing benchmarks
largely overlook these tutoring skills. We introduce MMTutorBench, the first
benchmark for AI math tutoring, consisting of 685 problems built around
pedagogically significant key-steps. Each problem is paired with
problem-specific rubrics that enable fine-grained evaluation across six
dimensions, and structured into three tasks-Insight Discovery, Operation
Formulation, and Operation Execution. We evaluate 12 leading MLLMs and find
clear performance gaps between proprietary and open-source systems, substantial
room compared to human tutors, and consistent trends across input variants: OCR
pipelines degrade tutoring quality, few-shot prompting yields limited gains,
and our rubric-based LLM-as-a-Judge proves highly reliable. These results
highlight both the difficulty and diagnostic value of MMTutorBench for
advancing AI tutoring.

</details>


### [110] [M4FC: a Multimodal, Multilingual, Multicultural, Multitask Real-World Fact-Checking Dataset](https://arxiv.org/abs/2510.23508)
*Jiahui Geng,Jonathan Tonglet,Iryna Gurevych*

Main category: cs.CL

TL;DR: 提出了M4FC数据集，包含4,982张图像和6,980个声明，涵盖10种语言和6种多模态事实核查任务，解决了现有数据集规模小、语言单一、证据泄露等问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态自动事实核查数据集存在规模小、语言单一、证据泄露或依赖外部新闻源等问题，需要构建更全面、多样化的数据集。

Method: 构建M4FC数据集，包含专业事实核查机构验证的图像和声明，涵盖10种语言和6种任务类型，并提供基线结果分析中间任务对最终预测的影响。

Result: 创建了包含4,982张图像和6,980个声明的数据集，支持10种语言和6种多模态事实核查任务，提供了各任务的基线性能。

Conclusion: M4FC数据集填补了多模态事实核查领域的空白，为研究提供了更全面、多样化的基准，数据集和代码已公开。

Abstract: Existing real-world datasets for multimodal automated fact-checking have
multiple limitations: they contain few instances, focus on only one or two
languages and tasks, suffer from evidence leakage, or depend on external sets
of news articles for sourcing true claims. To address these shortcomings, we
introduce M4FC, a new real-world dataset comprising 4,982 images paired with
6,980 claims. The images, verified by professional fact-checkers from 22
organizations, represent diverse cultural and geographic contexts. Each claim
is available in one or two out of ten languages. M4FC spans six multimodal
fact-checking tasks: visual claim extraction, claimant intent prediction, fake
detection, image contextualization, location verification, and verdict
prediction. We provide baseline results for all tasks and analyze how combining
intermediate tasks influence downstream verdict prediction performance. We make
our dataset and code available.

</details>


### [111] [IPQA: A Benchmark for Core Intent Identification in Personalized Question Answering](https://arxiv.org/abs/2510.23536)
*Jieyong Kim,Maryam Amirizaniani,Soojin Yoon,Dongha Lee*

Main category: cs.CL

TL;DR: 提出了IPQA基准，用于评估个性化问答中的核心意图识别能力，填补了现有基准只评估回答质量或检索性能的空白。


<details>
  <summary>Details</summary>
Motivation: 现有基准只评估回答质量或检索性能，没有直接衡量意图识别能力，而理解用户优先考虑的意图对于生成满足个体信息需求的回答至关重要。

Method: 基于满意理论，从用户选择答案的可观察行为模式中推导核心意图，通过系统过滤、基于LLM的标注以及结合自动验证和人工验证的严格质量控制构建多领域数据集。

Result: 实验评估显示，当前最先进的语言模型在个性化情境下难以识别核心意图，随着问题复杂性增加，性能进一步下降。

Conclusion: 当前系统在从用户历史中识别核心意图方面存在困难，代码和数据集将公开以促进未来研究。

Abstract: Intent identification serves as the foundation for generating appropriate
responses in personalized question answering (PQA). However, existing
benchmarks evaluate only response quality or retrieval performance without
directly measuring intent identification capabilities. This gap is critical
because without understanding which intents users prioritize, systems cannot
generate responses satisfying individual information needs. To address this, we
introduce the concept of core intents: intents users prioritize when selecting
answers to satisfy their information needs. To evaluate these core intents, we
propose IPQA, a benchmark for core Intent identification in Personalized
Question Answering. Since users do not explicitly state their prioritized
intents, we derive core intents from observable behavior patterns in answer
selection, grounded in satisficing theory where users choose answers meeting
their acceptance thresholds. We construct a dataset with various domains
through systematic filtering, LLM-based annotation, and rigorous quality
control combining automated verification with human validation. Experimental
evaluations across state-of-the-art language models reveal that current systems
struggle with core intent identification in personalized contexts. Models fail
to identify core intents from user histories, with performance degrading as
question complexity increases. The code and dataset will be made publicly
available to facilitate future research in this direction.

</details>


### [112] [LimRank: Less is More for Reasoning-Intensive Information Reranking](https://arxiv.org/abs/2510.23544)
*Tingyu Song,Yilun Zhao,Siyue Zhang,Chen Zhao,Arman Cohan*

Main category: cs.CL

TL;DR: LIMRANK是一个使用少量合成数据训练的LLM重排序模型，在推理密集型和指令跟随检索任务中表现优异，仅需传统方法5%的训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常需要大规模微调来适应LLM进行信息重排序任务，计算成本高昂。本文旨在证明现代LLM可以通过最小化、高质量的监督有效适应重排序任务。

Method: 设计了LIMRANK-SYNTHESIZER管道生成多样化、具有挑战性和真实性的重排序示例，使用这些合成数据微调重排序模型LIMRANK。

Result: LIMRANK在BRIGHT（推理密集型检索）和FollowIR（指令跟随检索）两个基准测试中取得了有竞争力的性能，同时训练数据量仅为先前工作的5%。

Conclusion: LIMRANK-SYNTHESIZER管道有效，LIMRANK在下游任务（包括科学文献搜索和知识密集型问题解决的检索增强生成）中展现出强大的泛化能力。

Abstract: Existing approaches typically rely on large-scale fine-tuning to adapt LLMs
for information reranking tasks, which is computationally expensive. In this
work, we demonstrate that modern LLMs can be effectively adapted using only
minimal, high-quality supervision. To enable this, we design
LIMRANK-SYNTHESIZER, a reusable and open-source pipeline for generating
diverse, challenging, and realistic reranking examples. Using this synthetic
data, we fine-tune our reranker model, LIMRANK. We evaluate LIMRANK on two
challenging benchmarks, i.e., BRIGHT for reasoning-intensive retrieval and
FollowIR for instruction-following retrieval. Our experiments demonstrate that
LIMRANK achieves competitive performance, while being trained on less than 5%
of the data typically used in prior work. Further ablation studies demonstrate
the effectiveness of LIMRANK-SYNTHESIZER and the strong generalization
capabilities of LIMRANK across downstream tasks, including scientific
literature search and retrieval-augmented generation for knowledge-intensive
problem solving.

</details>


### [113] [Hope Speech Detection in Social Media English Corpora: Performance of Traditional and Transformer Models](https://arxiv.org/abs/2510.23585)
*Luis Ramos,Hiram Calvo,Olga Kolesnikova*

Main category: cs.CL

TL;DR: 评估传统机器学习模型和微调transformer在希望语音检测任务上的表现，发现transformer模型在精度和召回率上表现更好


<details>
  <summary>Details</summary>
Motivation: 识别希望语音已成为重要的NLP任务，需要在社交媒体平台上检测具有激励性的表达和目标导向行为

Method: 使用预分割的希望语音数据集，评估传统机器学习模型（SVM、逻辑回归、朴素贝叶斯）和微调的transformer模型

Result: 传统模型中线性SVM和逻辑回归达到0.78宏F1，transformer模型表现更好，最佳模型获得0.82加权精度、0.80加权召回率、0.79加权F1和宏F1、0.80准确率

Conclusion: 虽然优化配置的传统机器学习模型仍具灵活性，但transformer架构能检测希望语音的微妙语义，在小型数据集上大型transformer和LLM可能表现更好

Abstract: The identification of hope speech has become a promised NLP task, considering
the need to detect motivational expressions of agency and goal-directed
behaviour on social media platforms. This proposal evaluates traditional
machine learning models and fine-tuned transformers for a previously split hope
speech dataset as train, development and test set. On development test, a
linear-kernel SVM and logistic regression both reached a macro-F1 of 0.78; SVM
with RBF kernel reached 0.77, and Na\"ive Bayes hit 0.75. Transformer models
delivered better results, the best model achieved weighted precision of 0.82,
weighted recall of 0.80, weighted F1 of 0.79, macro F1 of 0.79, and 0.80
accuracy. These results suggest that while optimally configured traditional
machine learning models remain agile, transformer architectures detect some
subtle semantics of hope to achieve higher precision and recall in hope speech
detection, suggesting that larges transformers and LLMs could perform better in
small datasets.

</details>


### [114] [Think Twice: Branch-and-Rethink Reasoning Reward Model](https://arxiv.org/abs/2510.23596)
*Yizhu Jiao,Jiaqi Zeng,Julien Veron Vialard,Oleksii Kuchaiev,Jiawei Han,Olivier Delalleau*

Main category: cs.CL

TL;DR: 提出了BR-RM，一种两轮奖励模型，将"三思而后行"原则应用于奖励建模，通过自适应分支和条件重思来减少判断扩散，提高对细微错误的敏感性。


<details>
  <summary>Details</summary>
Motivation: 传统奖励模型将多个质量维度压缩为单一标量，导致判断扩散——注意力分散在评估标准上，产生稀释的焦点和浅层分析。

Method: BR-RM采用两轮设计：第一轮执行自适应分支，选择实例关键维度并草拟证据寻求假设；第二轮执行分支条件重思，针对性地重新阅读并测试这些假设。

Result: 实验结果表明，该模型在三个具有挑战性的奖励建模基准测试中实现了最先进的性能，涵盖不同领域。

Conclusion: BR-RM通过将一次性评分转换为聚焦的二次审视推理，减少了判断扩散，提高了对细微但重要错误的敏感性，同时保持实用性和可扩展性。

Abstract: Large language models (LLMs) increasingly rely on thinking models that
externalize intermediate steps and allocate extra test-time compute, with
think-twice strategies showing that a deliberate second pass can elicit
stronger reasoning. In contrast, most reward models (RMs) still compress many
quality dimensions into a single scalar in one shot, a design that induces
judgment diffusion: attention spreads across evaluation criteria, yielding
diluted focus and shallow analysis. We introduce branch-and-rethink (BR-RM), a
two-turn RM that transfers the think-twice principle to reward modeling. Turn 1
performs adaptive branching, selecting a small set of instance-critical
dimensions (such as factuality and safety) and sketching concise,
evidence-seeking hypotheses. Turn 2 executes branch-conditioned rethinking, a
targeted reread that tests those hypotheses and scrutinizes only what matters
most. We train with GRPO-style reinforcement learning over structured two-turn
traces using a simple binary outcome reward with strict format checks, making
the approach compatible with standard RLHF pipelines. By converting
all-at-oncescoringintofocused, second-lookreasoning,
BR-RMreducesjudgmentdiffusionandimproves sensitivity to subtle yet
consequential errors while remaining practical and scalable. Experimental
results demonstrate that our model achieves state-of-the-art performance on
three challenging reward modeling benchmarks across diverse domains. The code
and the model will be released soon.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [115] [A Multi-Component AI Framework for Computational Psychology: From Robust Predictive Modeling to Deployed Generative Dialogue](https://arxiv.org/abs/2510.21720)
*Anant Pareek*

Main category: cs.AI

TL;DR: 该论文提出了一个结合AI与计算心理学的多层面框架，通过端到端开发流程构建了从预测建模到交互式心理分析的系统，包括基准测试、Transformer模型微调、生成式LLM开发及微服务部署。


<details>
  <summary>Details</summary>
Motivation: 弥合孤立预测建模与交互式心理分析系统之间的差距，为计算心理学和人类-AI交互提供完整的研发到部署管道。

Method: 1) 在四个心理学数据集上建立基准性能；2) 微调最先进的Transformer模型，解决回归任务数值不稳定性和资源约束下的训练挑战；3) 使用参数高效技术微调生成式LLM作为交互式"人格大脑"；4) 将预测和生成模型构建为可扩展的微服务生态系统。

Result: 成功稳定了基于Transformer的情感计算回归模型，在标准方法失败的情况下实现了有意义的预测性能，并开发了可复现的大规模AI研究方法论。

Conclusion: 这项工作展示了整合预测分析与生成对话的完整研发到部署管道，为计算心理学和人类-AI交互的未来研究提供了实用模型。

Abstract: The confluence of Artificial Intelligence and Computational Psychology
presents an opportunity to model, understand, and interact with complex human
psychological states through computational means. This paper presents a
comprehensive, multi-faceted framework designed to bridge the gap between
isolated predictive modeling and an interactive system for psychological
analysis. The methodology encompasses a rigorous, end-to-end development
lifecycle. First, foundational performance benchmarks were established on four
diverse psychological datasets using classical machine learning techniques.
Second, state-of-the-art transformer models were fine-tuned, a process that
necessitated the development of effective solutions to overcome critical
engineering challenges, including the resolution of numerical instability in
regression tasks and the creation of a systematic workflow for conducting
large-scale training under severe resource constraints. Third, a generative
large language model (LLM) was fine-tuned using parameter-efficient techniques
to function as an interactive "Personality Brain." Finally, the entire suite of
predictive and generative models was architected and deployed as a robust,
scalable microservices ecosystem. Key findings include the successful
stabilization of transformer-based regression models for affective computing,
showing meaningful predictive performance where standard approaches failed, and
the development of a replicable methodology for democratizing large-scale AI
research. The significance of this work lies in its holistic approach,
demonstrating a complete research-to-deployment pipeline that integrates
predictive analysis with generative dialogue, thereby providing a practical
model for future research in computational psychology and human-AI interaction.

</details>


### [116] [PREFINE: Personalized Story Generation via Simulated User Critics and User-Specific Rubric Generation](https://arxiv.org/abs/2510.21721)
*Kentaro Ueda,Takehiro Takayanagi*

Main category: cs.AI

TL;DR: PREFINE框架通过构建伪用户代理和用户特定评分标准，实现了无需参数更新或直接用户反馈的个性化故事生成。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在个性化文本生成方面依赖显式反馈或微调，存在用户负担、数据收集、计算成本和隐私等实际问题。

Method: 基于Critique-and-Refine范式，构建伪用户代理和用户特定评分标准，让代理基于这些标准代表用户进行批判和优化。

Result: 在PerDOC和PerMPST数据集上的评估显示，PREFINE在自动评估中获得更高胜率和显著分数，且不损害一般故事质量。

Conclusion: 该方法在保持故事质量的同时有效提升个性化程度，可扩展到对话系统、教育和推荐等更广泛应用。

Abstract: While recent advances in Large Language Models (LLMs) have improved the
quality of creative text generation, significant challenges remain in producing
personalized stories that reflect individual user preferences. Conventional
approaches rely on explicit feedback or fine-tuning, which presents practical
issues regarding user burden, data collection, computational costs, and
privacy. In this work, we propose PREFINE (Persona-and-Rubric Guided
Critique-and-Refine), a novel framework that extends the Critique-and-Refine
paradigm to personalization. PREFINE constructs a pseudo-user agent from a
user's interaction history and generates user-specific rubrics (evaluation
criteria). By having this agent critique and refine outputs on the user's
behalf based on these tailored rubrics, our method achieves personalized
generation without requiring parameter updates or direct user feedback. We
conducted a comprehensive evaluation on the PerDOC and PerMPST story datasets.
We designed three baseline methods and several model variants to verify the
contribution of each component of our framework. In automatic evaluations
(LLM-as-a-Judge), PREFINE achieved higher win rates and statistically
significant scores than the baselines, without compromising general story
quality. Analysis of the model variants confirmed that both the pseudo-user
agent and the user-specific rubrics are crucial for enhancing personalization
performance. Beyond story generation, our approach holds potential for enabling
efficient personalization in broader applications, such as dialogue systems,
education, and recommendation.

</details>


### [117] [SIGN: Schema-Induced Games for Naming](https://arxiv.org/abs/2510.21855)
*Ryan Zhang,Herbert Woisetscläger*

Main category: cs.AI

TL;DR: SIGN方法通过引入轻量级结构来引导多智能体命名约定形成，相比无约束自然语言能实现更快收敛和高达5.8倍的协议达成率。


<details>
  <summary>Details</summary>
Motivation: 现实AI系统中多个LLM智能体交互时，由于约定不一致会导致协调失败，需要可靠、一致的通信机制，且系统扩展性是关键问题。

Method: 引入Schema-Induced Games for Naming (SIGN)命名游戏，研究轻量级结构如何引导约定形成，并与无约束自然语言通信进行对比。

Result: 模式引导的通信相比无约束自然语言能实现更快收敛，协议达成率提高达5.8倍。

Conclusion: 最小化结构可以作为简单控制旋钮实现高效多智能体协调，其应用范围可扩展到命名游戏之外的更广泛领域。

Abstract: Real-world AI systems are tackling increasingly complex problems, often
through interactions among large language model (LLM) agents. When these agents
develop inconsistent conventions, coordination can break down. Applications
such as collaborative coding and distributed planning therefore require
reliable, consistent communication, and scalability is a central concern as
systems grow. We introduce Schema-Induced Games for Naming (SIGN), a naming
game that examines how lightweight structure can steer convention formation. We
compare schema-induced communication to unconstrained natural language and find
faster convergence with up to 5.8x higher agreement. These results suggest that
minimal structure can act as a simple control knob for efficient multi-agent
coordination, pointing toward broader applications beyond the naming game.

</details>


### [118] [Capability Ceilings in Autoregressive Language Models: Empirical Evidence from Knowledge-Intensive Tasks](https://arxiv.org/abs/2510.21866)
*Javier Marín*

Main category: cs.AI

TL;DR: 研究发现解码器自回归语言模型在知识密集型任务中存在能力上限，参数规模扩大（70M-30B）时知识检索任务准确率几乎无提升，而数学基准准确率保持在19-20%，与损失下降形成反差。


<details>
  <summary>Details</summary>
Motivation: 探究解码器自回归语言模型在参数规模扩展时不同任务类型的性能表现差异，特别是知识密集型任务与过程性任务的不同缩放模式。

Method: 系统评估OPT和Pythia模型家族（70M-30B参数），分析损失与准确率的关系，并进行注意力干预实验观察性能变化模式。

Result: 知识检索任务准确率几乎无改善，数学基准准确率保持低位（19-20%）而损失下降31%；过程性任务则呈现传统缩放模式；注意力扰动导致性能灾难性崩溃。

Conclusion: 对于OPT和Pythia架构，参数规模超过1-2B在知识密集型应用中准确率增益极小，存在特定能力缩放失败，需重新考虑资源分配策略。

Abstract: We document empirical capability ceilings in decoder-only autoregressive
language models across knowledge-intensive tasks. Systematic evaluation of OPT
and Pythia model families (70M-30B parameters, spanning 240 times scaling)
reveals that knowledge retrieval tasks show negligible accuracy improvement
despite smooth loss reduction. On MMLU mathematics benchmarks, accuracy remains
flat at 19-20% (below 25% random chance) across all scales while cross-entropy
loss decreases by 31%. In contrast, procedural tasks like arithmetic show
conventional scaling where both metrics improve together. Attention
intervention experiments reveal high sensitivity to perturbation: swapping
attention patterns between models causes catastrophic performance collapse
(complete accuracy loss) rather than graceful degradation. These measurements
have immediate engineering implications: for knowledge-intensive applications
using OPT and Pythia architectures, parameter scaling beyond 1-2B offers
minimal accuracy gains despite continued loss improvement. Our findings
quantify capability-specific scaling failures in these model families to inform
resource allocation decisions. Whether these patterns reflect fundamental
constraints of decoder-only architectures or implementation-specific
limitations remains an open question requiring investigation across diverse
architectural approaches.

</details>


### [119] [GeoThought: A Dataset for Enhancing Mathematical Geometry Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.21881)
*Nannan Shi,Chuanyu Qin,Shipeng Song,Man Luo*

Main category: cs.AI

TL;DR: 开发了GeoThoughts数据集和GeoThought-MLLM模型，通过链式思维训练提升几何推理能力，在几何任务中超越现有基准。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在文本数学推理中表现良好，但在视觉几何推理任务中性能显著下降，主要由于几何问题的内在复杂性（需要详细图像理解和多步推理）以及现有数据集缺乏规模、多样性和明确推理轨迹。

Method: 构建GeoThoughts数据集（包含6,243样本的Geo-Thought-6K和10,834样本的Geo-Thought-Augmented-10K），每个条目包含视觉描述、逐步解决方案、明确推理链、反思步骤和最终答案。基于此开发GeoThought-MLLM多模态数学推理模型。

Result: 模型在几何任务中优于现有基准，训练使用链式思维数据集能提升在域内和域外设置下的几何推理能力。

Conclusion: 错误主要源于数学概念错误理解或空间误判，通过调用链式思维纠正这些错误可产生正确答案。

Abstract: Large language models (LLMs) have demonstrated strong reasoning capabilities
in text-based mathematical problem solving; however, when adapted to visual
reasoning tasks, particularly geometric problem solving, their performance
substantially declines because geometric problems present unique challenges.
Specifically, these challenges stem from two key factors: first, the intrinsic
complexity of geometry requiring detailed image comprehension and multi-step
reasoning, and second, the limitations of existing datasets which lack
sufficient scale, diversity, and explicit reasoning traces, consequently
hindering effective model training. To address these challenges, we developed
the GeoThoughts dataset, a comprehensive geometric reasoning corpus with two
subsets: Geo-Thought-6K with 6,243 samples and its augmented version
Geo-Thought-Augmented-10K containing 10,834 samples. Each entry includes visual
descriptions, step-by-step solutions, explicit reasoning chains, reflection
steps, and final answers. Using this dataset, we developed GeoThought-MLLM, a
mathematical reasoning multimodal model that generates detailed thinking
processes during problem-solving. Our model outperforms existing benchmarks in
geometric tasks, demonstrating that training with our Chain-of-Thought dataset
improves geometric reasoning capabilities across both in-domain and
out-of-domain settings. Finally, we analyze failure cases and observe that
errors primarily arise from incorrect interpretation of mathematical concepts
or spatial misjudgment. By invoking CoT to correct these mistakes, the model
produces correct answers.

</details>


### [120] [Exploration through Generation: Applying GFlowNets to Structured Search](https://arxiv.org/abs/2510.21886)
*Mark Phillip Matovic*

Main category: cs.AI

TL;DR: 将生成流网络应用于旅行商问题、最小生成树和最短路径三个图优化问题，通过训练学习采样与奖励函数成比例的解决方案，生成结果与经典算法一致。


<details>
  <summary>Details</summary>
Motivation: 探索生成模型解决组合优化问题的能力，利用学习策略的优势实现计算可扩展性，在大型问题实例中可能比经典精确方法更具可行性。

Method: 使用轨迹平衡损失训练GFlowNets，顺序构建解决方案：为生成树选择边、为路径选择节点、为旅行选择城市。

Result: 在不同规模的基准实例上，GFlowNets学会了找到最优解，生成解与经典算法（Dijkstra最短路径、Kruskal生成树、TSP精确求解器）的结果匹配。

Conclusion: 生成模型可以通过学习策略解决组合优化问题，主要优势是计算可扩展性：经典算法每个实例复杂度固定，而GFlowNets通过训练分摊计算成本。

Abstract: This work applies Generative Flow Networks (GFlowNets) to three graph
optimization problems: the Traveling Salesperson Problem, Minimum Spanning
Tree, and Shortest Path. GFlowNets are generative models that learn to sample
solutions proportionally to a reward function. The models are trained using the
Trajectory Balance loss to build solutions sequentially, selecting edges for
spanning trees, nodes for paths, and cities for tours. Experiments on benchmark
instances of varying sizes show that GFlowNets learn to find optimal solutions.
For each problem type, multiple graph configurations with different numbers of
nodes were tested. The generated solutions match those from classical
algorithms (Dijkstra for shortest path, Kruskal for spanning trees, and exact
solvers for TSP). Training convergence depends on problem complexity, with the
number of episodes required for loss stabilization increasing as graph size
grows. Once training converges, the generated solutions match known optima from
classical algorithms across the tested instances. This work demonstrates that
generative models can solve combinatorial optimization problems through learned
policies. The main advantage of this learning-based approach is computational
scalability: while classical algorithms have fixed complexity per instance,
GFlowNets amortize computation through training. With sufficient computational
resources, the framework could potentially scale to larger problem instances
where classical exact methods become infeasible.

</details>


### [121] [Computational Hardness of Reinforcement Learning with Partial $q^π$-Realizability](https://arxiv.org/abs/2510.21888)
*Shayan Karimi,Xiaoqi Tan*

Main category: cs.AI

TL;DR: 该论文研究了在部分$q^{\pi}$-可实现性框架下强化学习的计算复杂性，证明了在这种线性函数逼近设置中学习$\epsilon$-最优策略是计算困难的。


<details>
  <summary>Details</summary>
Motivation: 研究部分$q^{\pi}$-可实现性框架的计算复杂性，该框架假设所有策略的价值函数都是线性可实现的，比$q^{\pi}$-可实现性弱但比$q^*$-可实现性强，提供了函数逼近自然出现的实用模型。

Method: 通过从$\delta$-Max-3SAT和$\delta$-Max-3SAT(b)问题归约到GLinear-$\kappa$-RL（贪婪策略）和SLinear-$\kappa$-RL（softmax策略）实例，建立计算困难性证明。

Result: 证明了在参数化贪婪策略集下学习$\epsilon$-最优策略是NP困难的，在softmax策略集下（除非NP = RP）存在指数级下界（在特征向量维度上）。

Conclusion: 计算困难性在部分$q^{\pi}$-可实现性中持续存在，即使将策略集扩展到最优策略之外，这与生成访问模型下的$q^{\pi}$-可实现性形成对比，表明正面的计算结果通常无法实现。

Abstract: This paper investigates the computational complexity of reinforcement
learning in a novel linear function approximation regime, termed partial
$q^{\pi}$-realizability. In this framework, the objective is to learn an
$\epsilon$-optimal policy with respect to a predefined policy set $\Pi$, under
the assumption that all value functions for policies in $\Pi$ are linearly
realizable. The assumptions of this framework are weaker than those in
$q^{\pi}$-realizability but stronger than those in $q^*$-realizability,
providing a practical model where function approximation naturally arises. We
prove that learning an $\epsilon$-optimal policy in this setting is
computationally hard. Specifically, we establish NP-hardness under a
parameterized greedy policy set (argmax) and show that - unless NP = RP - an
exponential lower bound (in feature vector dimension) holds when the policy set
contains softmax policies, under the Randomized Exponential Time Hypothesis.
Our hardness results mirror those in $q^*$-realizability and suggest
computational difficulty persists even when $\Pi$ is expanded beyond the
optimal policy. To establish this, we reduce from two complexity problems,
$\delta$-Max-3SAT and $\delta$-Max-3SAT(b), to instances of GLinear-$\kappa$-RL
(greedy policy) and SLinear-$\kappa$-RL (softmax policy). Our findings indicate
that positive computational results are generally unattainable in partial
$q^{\pi}$-realizability, in contrast to $q^{\pi}$-realizability under a
generative access model.

</details>


### [122] [Performance Trade-offs of Optimizing Small Language Models for E-Commerce](https://arxiv.org/abs/2510.21970)
*Josip Tomo Licardo,Nikola Tankovic*

Main category: cs.AI

TL;DR: 本研究展示了通过QLoRA微调和后训练量化技术优化的10亿参数Llama 3.2模型，在电商意图识别任务中达到99%准确率，性能与GPT-4.1相当，但计算成本显著降低。


<details>
  <summary>Details</summary>
Motivation: 商用大语言模型在电商等专业领域部署面临高计算成本、延迟和运营费用的问题，需要探索更资源高效的小型开源模型替代方案。

Method: 使用QLoRA方法在合成的多语言电商查询数据集上微调10亿参数Llama 3.2模型，然后应用GPTQ（GPU优化）和GGUF（CPU优化）量化技术。

Result: 优化后的1B模型准确率达99%，与GPT-4.1相当。GPTQ量化减少41%显存但推理速度下降82%；GGUF在CPU上实现18倍推理吞吐提升和90%内存减少。

Conclusion: 经过适当优化的开源小模型是领域特定应用的更合适替代方案，能以极低计算成本提供最先进性能。

Abstract: Large Language Models (LLMs) offer state-of-the-art performance in natural
language understanding and generation tasks. However, the deployment of leading
commercial models for specialized tasks, such as e-commerce, is often hindered
by high computational costs, latency, and operational expenses. This paper
investigates the viability of smaller, open-weight models as a
resource-efficient alternative. We present a methodology for optimizing a
one-billion-parameter Llama 3.2 model for multilingual e-commerce intent
recognition. The model was fine-tuned using Quantized Low-Rank Adaptation
(QLoRA) on a synthetically generated dataset designed to mimic real-world user
queries. Subsequently, we applied post-training quantization techniques,
creating GPU-optimized (GPTQ) and CPU-optimized (GGUF) versions. Our results
demonstrate that the specialized 1B model achieves 99% accuracy, matching the
performance of the significantly larger GPT-4.1 model. A detailed performance
analysis revealed critical, hardware-dependent trade-offs: while 4-bit GPTQ
reduced VRAM usage by 41%, it paradoxically slowed inference by 82% on an older
GPU architecture (NVIDIA T4) due to dequantization overhead. Conversely, GGUF
formats on a CPU achieved a speedup of up to 18x in inference throughput and a
reduction of over 90% in RAM consumption compared to the FP16 baseline. We
conclude that small, properly optimized open-weight models are not just a
viable but a more suitable alternative for domain-specific applications,
offering state-of-the-art accuracy at a fraction of the computational cost.

</details>


### [123] [Distribution Shift Alignment Helps LLMs Simulate Survey Response Distributions](https://arxiv.org/abs/2510.21977)
*Ji Huang,Mengfei Li,Shuai Shao*

Main category: cs.AI

TL;DR: 提出了一种名为分布偏移对齐（DSA）的两阶段微调方法，用于改善LLM在调查响应模拟中的表现，显著减少真实数据需求并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有零样本方法存在提示敏感性和低准确性问题，而传统微调方法主要拟合训练集分布，无法产生比训练集更准确的结果，这违背了使用LLM模拟调查响应的初衷。

Method: DSA是一种两阶段微调方法，通过同时对齐输出分布和不同背景下的分布偏移，学习这些分布如何变化而非简单拟合训练数据。

Result: 在五个公开调查数据集上，DSA始终优于其他方法，能够提供比训练数据更接近真实分布的结果，并将所需真实数据减少53.48-69.12%。

Conclusion: DSA在调查模拟中展现出有效性和效率，通过分布偏移对齐实现了比传统方法更准确的响应模拟，显著降低了数据收集成本。

Abstract: Large language models (LLMs) offer a promising way to simulate human survey
responses, potentially reducing the cost of large-scale data collection.
However, existing zero-shot methods suffer from prompt sensitivity and low
accuracy, while conventional fine-tuning approaches mostly fit the training set
distributions and struggle to produce results more accurate than the training
set itself, which deviates from the original goal of using LLMs to simulate
survey responses. Building on this observation, we introduce Distribution Shift
Alignment (DSA), a two-stage fine-tuning method that aligns both the output
distributions and the distribution shifts across different backgrounds. By
learning how these distributions change rather than fitting training data, DSA
can provide results substantially closer to the true distribution than the
training data. Empirically, DSA consistently outperforms other methods on five
public survey datasets. We further conduct a comprehensive comparison covering
accuracy, robustness, and data savings. DSA reduces the required real data by
53.48-69.12%, demonstrating its effectiveness and efficiency in survey
simulation.

</details>


### [124] [Foundation of Intelligence: Review of Math Word Problems from Human Cognition Perspective](https://arxiv.org/abs/2510.21999)
*Zhenya Huang,Jiayu Liu,Xin Lin,Zhiyuan Ma,Shangzi Xue,Tong Xiao,Qi Liu,Yee Whye Teh,Enhong Chen*

Main category: cs.AI

TL;DR: 本文从人类认知视角系统回顾了数学应用题求解研究，总结了5种关键认知能力，比较了神经网络和基于LLM的求解器，并统一评估了它们在5个主流基准上的性能。


<details>
  <summary>Details</summary>
Motivation: 数学应用题作为AI基础研究领域，目前缺乏系统的分类调查和当前发展趋势的讨论。本文旨在通过人类认知视角全面回顾MWP求解研究，展示AI模型在模拟人类认知能力方面的进展。

Method: 从人类认知角度总结5种关键认知能力，回顾近10年两种主流MWP模型（神经网络求解器和基于LLM的求解器），重新运行所有代表性MWP求解器并在5个主流基准上进行统一性能比较。

Result: 提供了现有方法的整体比较分析，展示了不同模型在模拟人类推理认知方面的能力差异。

Conclusion: 这是首个从人类推理认知视角全面分析过去十年有影响力的MWP研究的调查，希望能启发AI推理的进一步研究。

Abstract: Math word problem (MWP) serves as a fundamental research topic in artificial
intelligence (AI) dating back to 1960s. This research aims to advance the
reasoning abilities of AI by mirroring the human-like cognitive intelligence.
The mainstream technological paradigm has evolved from the early rule-based
methods, to deep learning models, and is rapidly advancing towards large
language models. However, the field still lacks a systematic taxonomy for the
MWP survey along with a discussion of current development trends. Therefore, in
this paper, we aim to comprehensively review related research in MWP solving
through the lens of human cognition, to demonstrate how recent AI models are
advancing in simulating human cognitive abilities. Specifically, we summarize 5
crucial cognitive abilities for MWP solving, including Problem Understanding,
Logical Organization, Associative Memory, Critical Thinking, and Knowledge
Learning. Focused on these abilities, we review two mainstream MWP models in
recent 10 years: neural network solvers, and LLM based solvers, and discuss the
core human-like abilities they demonstrated in their intricate problem-solving
process. Moreover, we rerun all the representative MWP solvers and supplement
their performance on 5 mainstream benchmarks for a unified comparison. To the
best of our knowledge, this survey first comprehensively analyzes the
influential MWP research of the past decade from the perspective of human
reasoning cognition and provides an integrative overall comparison across
existing approaches. We hope it can inspire further research in AI reasoning.
Our repository is released on https://github.com/Ljyustc/FoI-MWP.

</details>


### [125] [LightAgent: Mobile Agentic Foundation Models](https://arxiv.org/abs/2510.22009)
*Yangqin Jiang,Chao Huang*

Main category: cs.AI

TL;DR: LightAgent是一个移动GUI代理系统，通过设备-云协作解决移动设备上小模型性能不足和大模型部署成本高的问题，在保持高性能的同时显著降低云成本。


<details>
  <summary>Details</summary>
Motivation: 移动GUI代理面临关键困境：真正在设备上的小模型（4B或更小）性能不足，而能力强的模型（从7B开始）要么太大无法在移动设备部署，要么成本过高（如仅限云的闭源MLLMs）。

Method: 采用设备-云协作方案，通过两阶段SFT->GRPO训练增强Qwen2.5-VL-3B模型的决策能力，集成高效长推理机制利用历史交互，默认在设备上执行，仅通过实时复杂度评估将挑战性子任务升级到云端。

Result: 在AndroidLab基准测试和多样化应用上的实验表明，LightAgent能够匹配或接近更大模型的性能，同时显著降低云成本。

Conclusion: LightAgent通过创新的设备-云协作架构，成功解决了移动GUI代理的性能与部署成本之间的权衡问题，为移动平台上的智能代理系统提供了可行的解决方案。

Abstract: With the advancement of multimodal large language models (MLLMs), building
GUI agent systems has become an increasingly promising direction-especially for
mobile platforms, given their rich app ecosystems and intuitive touch
interactions. Yet mobile GUI agents face a critical dilemma: truly on-device
models (4B or smaller) lack sufficient performance, while capable models
(starting from 7B) are either too large for mobile deployment or prohibitively
costly (e.g., cloud-only closed-source MLLMs). To resolve this, we propose
LightAgent, a mobile agentic foundation model solution that leverages
device-cloud collaboration to tap the cost-efficiency of on-device models and
the high capability of cloud models, while avoiding their drawbacks.
Specifically, LightAgent enhances Qwen2.5-VL-3B via two-stage SFT->GRPO
training on synthetic GUI data for strong decision-making, integrates an
efficient long-reasoning mechanism to utilize historical interactions under
tight resources, and defaults to on-device execution-only escalating
challenging subtasks to the cloud via real-time complexity assessment.
Experiments on the online AndroidLab benchmark and diverse apps show LightAgent
matches or nears larger models, with a significant reduction in cloud costs.

</details>


### [126] [LLM-AR: LLM-powered Automated Reasoning Framework](https://arxiv.org/abs/2510.22034)
*Rick Chen,Joseph Ternasky,Aaron Ontoyin Yin,Xianling Mu,Fuat Alican,Yigit Ihlamur*

Main category: cs.AI

TL;DR: LLM-AR是一个受神经符号系统启发的管道，将LLM生成的启发式规则转化为概率规则，通过ProbLog自动推理引擎执行，用于预测初创企业成功，实现了59.5%的精确度和8.7%的召回率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在模式识别和推理方面表现良好，但其可变准确性阻碍了在高风险决策应用中的采用，特别是在风险投资领域预测初创企业成功的场景。

Method: 提出LLM-AR管道，通过迭代策略进化循环结合关联规则挖掘，逐步精炼预测规则，将LLM生成的启发式转化为ProbLog执行的概率规则。

Result: 在未见数据上，LLM-AR达到59.5%的精确度和8.7%的召回率，是随机基线精确度的5.9倍，同时所有决策路径都可被人类检查。

Conclusion: 该框架具有可解释性和通过超参数可调性，显示出扩展到其他领域的潜力。

Abstract: Large language models (LLMs) can already identify patterns and reason
effectively, yet their variable accuracy hampers adoption in high-stakes
decision-making applications. In this paper, we study this issue from a venture
capital perspective by predicting idea-stage startup success based on founder
traits. (i) To build a reliable prediction model, we introduce LLM-AR, a
pipeline inspired by neural-symbolic systems that distils LLM-generated
heuristics into probabilistic rules executed by the ProbLog automated-reasoning
engine. (ii) An iterative policy-evolution loop incorporates association-rule
mining to progressively refine the prediction rules.
  On unseen folds, LLM-AR achieves 59.5% precision and 8.7% recall, 5.9x the
random baseline precision, while exposing every decision path for human
inspection. The framework is interpretable and tunable via hyperparameters,
showing promise to extend into other domains.

</details>


### [127] [Predictive Coding Enhances Meta-RL To Achieve Interpretable Bayes-Optimal Belief Representation Under Partial Observability](https://arxiv.org/abs/2510.22039)
*Po-Chen Kuo,Han Hou,Will Dabney,Edgar Y. Walker*

Main category: cs.AI

TL;DR: 在部分可观测环境中，将自监督预测编码模块集成到元强化学习中，可以学习到更紧凑、可解释的贝叶斯最优信念状态表示，从而提高泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统元强化学习虽然能学习到接近贝叶斯最优的策略，但往往无法学习到紧凑、可解释的贝叶斯最优信念状态表示，这种表示效率低下可能限制智能体的适应性和泛化能力。

Method: 受神经科学中预测编码和深度强化学习中辅助预测目标的启发，将自监督预测编码模块集成到元强化学习中。

Result: 通过状态机模拟，显示带有预测模块的元强化学习在各种任务中都能生成更可解释的表示，更好地逼近贝叶斯最优信念状态。在需要主动信息搜索的挑战性任务中，只有带有预测模块的元强化学习能成功学习最优表示和策略。

Conclusion: 预测学习作为指导原则，在部分可观测环境中导航的智能体中进行有效表示学习具有重要作用，更好的表示学习能带来改进的泛化能力。

Abstract: Learning a compact representation of history is critical for planning and
generalization in partially observable environments. While meta-reinforcement
learning (RL) agents can attain near Bayes-optimal policies, they often fail to
learn the compact, interpretable Bayes-optimal belief states. This
representational inefficiency potentially limits the agent's adaptability and
generalization capacity. Inspired by predictive coding in neuroscience--which
suggests that the brain predicts sensory inputs as a neural implementation of
Bayesian inference--and by auxiliary predictive objectives in deep RL, we
investigate whether integrating self-supervised predictive coding modules into
meta-RL can facilitate learning of Bayes-optimal representations. Through state
machine simulation, we show that meta-RL with predictive modules consistently
generates more interpretable representations that better approximate
Bayes-optimal belief states compared to conventional meta-RL across a wide
variety of tasks, even when both achieve optimal policies. In challenging tasks
requiring active information seeking, only meta-RL with predictive modules
successfully learns optimal representations and policies, whereas conventional
meta-RL struggles with inadequate representation learning. Finally, we
demonstrate that better representation learning leads to improved
generalization. Our results strongly suggest the role of predictive learning as
a guiding principle for effective representation learning in agents navigating
partial observability.

</details>


### [128] [HW/SW Co-design of a PCM/PWM converter: a System Level Approach based in the SpecC Methodology](https://arxiv.org/abs/2510.22046)
*Daniel G. P. Petrini,Braz Izaias da Silva Junior*

Main category: cs.AI

TL;DR: 应用SpecC方法学在系统级硬件/软件协同设计流程中对PCM-to-PWM转换器进行案例研究，展示了系统级协同设计在早期架构洞察、快速验证和成本/性能权衡方面的价值。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过系统级硬件/软件协同设计方法，在满足实时约束的同时降低纯硬件解决方案的估计成本，并避免在高性能处理器上纯软件实现的昂贵开销。

Method: 使用SpecC方法学对PCM-to-PWM转换器进行建模和探索，通过系统级估计和快速功能仿真来评估不同的硬件/软件分区映射方案。

Result: 成功推导出满足实时约束的硬件/软件分区方案，相比纯硬件解决方案降低了估计成本，同时避免了纯软件实现的高昂开销。

Conclusion: 尽管设计复杂度适中，但结果强调了系统级协同设计在提供早期架构洞察、快速验证和可操作的成本/性能权衡方面的重要价值。

Abstract: We present a case study applying the SpecC methodology within a system-level
hardware/software co-design flow to a PCM-to-PWM converter, the core of a
Class-D audio amplifier. The converter was modeled and explored with SpecC
methodology to derive an HW/SW partition. Using system-level estimates and fast
functional simulation, we evaluated mappings that meet real-time constraints
while reducing estimated cost of an all-hardware solution and avoiding the
expense of a purely software implementation on a high-end processor. Despite
the design's moderate complexity, the results underline the value of
system-level co-design for early architectural insight, rapid validation, and
actionable cost/performance trade-offs. [Original work from 2005; formatting
revised in 2025, with no changes to the results.]

</details>


### [129] [Towards Error-Centric Intelligence II: Energy-Structured Causal Models](https://arxiv.org/abs/2510.22050)
*Marcus Thomas*

Main category: cs.AI

TL;DR: 该论文提出从预测准确性转向因果可解释性的概念重构，引入计算解释和能量结构化因果模型(ESCMs)，使内部结构可在机制层面进行干预。


<details>
  <summary>Details</summary>
Motivation: 当代机器学习追求预测准确性，但达到最先进性能的系统在因果上仍然不透明，其内部表示缺乏因果语义，无法进行精确的机制编辑。

Method: 引入计算解释和能量结构化因果模型(ESCMs)，其中机制表示为约束（能量函数或向量场）而非显式的输入输出映射，干预通过对这些约束进行局部手术来实现。

Result: 在ESCM上下文中具体实现了结构因果原则LAP和ICM，并证明在温和条件下ESCMs可恢复标准SCM语义。

Conclusion: 该论文为因果推理提供了一种形式化语言，使系统能够理解而不仅仅是预测，实现了内部结构在解释层面的可操作性。

Abstract: Contemporary machine learning optimizes for predictive accuracy, yet systems
that achieve state of the art performance remain causally opaque: their
internal representations provide no principled handle for intervention. We can
retrain such models, but we cannot surgically edit specific mechanisms while
holding others fixed, because learned latent variables lack causal semantics.
We argue for a conceptual reorientation: intelligence is the ability to build
and refine explanations, falsifiable claims about manipulable structure that
specify what changes and what remains invariant under intervention.
Explanations subsume prediction but demand more: causal commitments that can be
independently tested and corrected at the level of mechanisms. We introduce
computational explanations, mappings from observations to intervention ready
causal accounts. We instantiate these explanations with Energy Structured
Causal Models (ESCMs), in which mechanisms are expressed as constraints (energy
functions or vector fields) rather than explicit input output maps, and
interventions act by local surgery on those constraints. This shift makes
internal structure manipulable at the level where explanations live: which
relations must hold, which can change, and what follows when they do. We
provide concrete instantiations of the structural-causal principles LAP and ICM
in the ESCM context, and also argue that empirical risk minimization
systematically produces fractured, entangled representations, a failure we
analyze as gauge ambiguity in encoder energy pairs. Finally, we show that under
mild conditions, ESCMs recover standard SCM semantics. Building on Part I's
principles (LAP, ICM, CAP) and its definition of intelligence as
explanation-building under criticism, this paper offers a formal language for
causal reasoning in systems that aspire to understand, not merely to predict.

</details>


### [130] [Energy-Efficient Domain-Specific Artificial Intelligence Models and Agents: Pathways and Paradigms](https://arxiv.org/abs/2510.22052)
*Abhijit Chatterjee,Niraj K. Jha,Jonathan D. Cohen,Thomas L. Griffiths,Hongjing Lu,Diana Marculescu,Ashiqur Rasul,Keshab K. Parhi*

Main category: cs.AI

TL;DR: 该论文提出了下一代AI的发展愿景：从当前需要大量数据和能源的大型语言模型转向轻量级、领域特定的多模态智能体，这些智能体能够推理、规划并在动态环境中决策，同时实现比现有技术高1000倍以上的能效。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统（特别是大型语言模型）存在能耗高（GPT-4训练需50-60GWh）、需要海量网络数据、容易产生幻觉等问题，无法部署在关键应用领域。相比之下，人脑仅消耗20W功率，因此需要开发更智能、更节能的AI系统。

Method: 提出发展轻量级领域特定多模态模型，这些模型能够在动态环境中进行推理、规划和决策，利用实时数据和先验知识，并具备持续学习和进化能力。同时需要重新设计硬件以实现超过1000倍的能效提升。

Result: 论文构建了未来AI系统的愿景框架，但没有提供具体的实验结果或数据。

Conclusion: 下一代AI应该从当前的大型模型转向能效更高、更智能的领域特定智能体，这需要硬件和软件架构的根本性变革，以实现比现有技术高1000倍以上的能效提升。

Abstract: The field of artificial intelligence (AI) has taken a tight hold on broad
aspects of society, industry, business, and governance in ways that dictate the
prosperity and might of the world's economies. The AI market size is projected
to grow from 189 billion USD in 2023 to 4.8 trillion USD by 2033. Currently, AI
is dominated by large language models that exhibit linguistic and visual
intelligence. However, training these models requires a massive amount of data
scraped from the web as well as large amounts of energy (50--60 GWh to train
GPT-4). Despite these costs, these models often hallucinate, a characteristic
that prevents them from being deployed in critical application domains. In
contrast, the human brain consumes only 20~W of power. What is needed is the
next level of AI evolution in which lightweight domain-specific multimodal
models with higher levels of intelligence can reason, plan, and make decisions
in dynamic environments with real-time data and prior knowledge, while learning
continuously and evolving in ways that enhance future decision-making
capability. This will define the next wave of AI, progressing from today's
large models, trained with vast amounts of data, to nimble energy-efficient
domain-specific agents that can reason and think in a world full of
uncertainty. To support such agents, hardware will need to be reimagined to
allow energy efficiencies greater than 1000x over the state of the art. Such a
vision of future AI systems is developed in this work.

</details>


### [131] [Embracing Trustworthy Brain-Agent Collaboration as Paradigm Extension for Intelligent Assistive Technologies](https://arxiv.org/abs/2510.22095)
*Yankai Chen,Xinni Zhang,Yifei Zhang,Yangning Li,Henry Peng Zou,Chunyu Miao,Weizhi Zhang,Xue Liu,Philip S. Yu*

Main category: cs.AI

TL;DR: 本文提出从脑机接口(BCI)向脑-智能体协作(BAC)的范式扩展，强调将智能体重新定义为主动协作伙伴而非被动脑信号处理器，需要关注伦理数据处理、模型可靠性和人-智能体协作框架。


<details>
  <summary>Details</summary>
Motivation: 脑机接口在严重神经损伤患者中具有重要应用前景，但其广泛应用受到信息传输率低和用户特定校准等关键限制。现有研究虽然探索了大语言模型的集成，但部署智能AI仍面临技术障碍和伦理问题。

Method: 这是一篇立场论文，通过分析当前脑机接口的局限性和智能体集成的挑战，提出向脑-智能体协作的范式转变，强调智能体作为主动协作伙伴的新角色。

Result: 论文提出了脑-智能体协作(BAC)的新范式，强调需要建立伦理数据处理、模型可靠性和稳健的人-智能体协作框架，以确保系统的安全、可信和有效性。

Conclusion: 脑机接口领域需要进行从BCI到BAC的范式扩展，将智能体重新定义为主动协作伙伴，这需要重点关注伦理、可靠性和协作框架等关键问题。

Abstract: Brain-Computer Interfaces (BCIs) offer a direct communication pathway between
the human brain and external devices, holding significant promise for
individuals with severe neurological impairments. However, their widespread
adoption is hindered by critical limitations, such as low information transfer
rates and extensive user-specific calibration. To overcome these challenges,
recent research has explored the integration of Large Language Models (LLMs),
extending the focus from simple command decoding to understanding complex
cognitive states. Despite these advancements, deploying agentic AI faces
technical hurdles and ethical concerns. Due to the lack of comprehensive
discussion on this emerging direction, this position paper argues that the
field is poised for a paradigm extension from BCI to Brain-Agent Collaboration
(BAC). We emphasize reframing agents as active and collaborative partners for
intelligent assistance rather than passive brain signal data processors,
demanding a focus on ethical data handling, model reliability, and a robust
human-agent collaboration framework to ensure these systems are safe,
trustworthy, and effective.

</details>


### [132] [Controllable Mathematical Reasoning via Self-Optimizing Thought Vectors](https://arxiv.org/abs/2510.22132)
*Xuying LI*

Main category: cs.AI

TL;DR: 提出了一种利用熵最小化自优化思维向量的可控数学推理方法，通过可学习的思维向量动态调节大语言模型的内部推理过程。


<details>
  <summary>Details</summary>
Motivation: 开发能够控制AI推理过程的方法，使模型能够按照特定方向进行推理，而不需要外部奖励标注。

Method: 引入可学习的思维向量，通过熵最小化奖励来引导推理模式，使用Gemma-2-9B模型在GSM8K数据集上进行实验。

Result: 在GSM8K上达到90.1%的准确率，可控性得分为0.42，发现了不同的思维向量簇和一致的低熵分布。

Conclusion: 基于熵的奖励能够有效引导聚焦推理模式，验证了该框架在可控AI推理方面的有效性。

Abstract: We present a novel approach for controllable mathematical reasoning that
leverages self-optimizing thought vectors with entropy minimization. Our method
introduces learnable thought vectors that dynamically modulate the internal
reasoning process of large language models. Using Gemma-2-9B on GSM8K, we
achieve 90.1% accuracy with a controllability score of 0.42, demonstrating that
entropy-based rewards effectively guide focused reasoning patterns without
requiring external reward annotations. Our analysis reveals distinct thought
vector clusters and consistent low-entropy distributions across control
conditions, validating our framework for controllable AI reasoning.

</details>


### [133] [Measure what Matters: Psychometric Evaluation of AI with Situational Judgment Tests](https://arxiv.org/abs/2510.22170)
*Alexandra Yost,Shreyans Jain,Shivam Raval,Grant Corser,Allen Roush,Nina Xu,Jacqueline Hammack,Ravid Shwartz-Ziv,Amirali Abdullah*

Main category: cs.AI

TL;DR: 提出了一个AI心理测量学框架，使用情境判断测试和复杂人物角色设计来评估AI系统在需要情感判断和伦理考量的角色中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常重复使用人类特质清单或临时人物角色，限制了行为真实性和领域相关性。需要更现实的评估方法来测试AI在需要情感和伦理判断的领域中的能力。

Method: 框架包含三个核心部分：(1)使用现实场景的情境判断测试来探索领域特定能力；(2)整合工业组织心理学和人格心理学设计复杂人物角色；(3)采用结构化生成方法，包含人口统计先验和回忆录式叙事。

Result: 在执法助手案例研究中构建了包含8,500个人物角色、4,000个情境判断测试和300,000个响应的丰富数据集，分析了不同子群体和场景的行为模式。

Conclusion: 该框架提供了更现实和领域相关的AI心理测量方法，数据集和代码将公开发布以促进进一步研究。

Abstract: AI psychometrics evaluates AI systems in roles that traditionally require
emotional judgment and ethical consideration. Prior work often reuses human
trait inventories (Big Five, \hexaco) or ad hoc personas, limiting behavioral
realism and domain relevance. We propose a framework that (1) uses situational
judgment tests (SJTs) from realistic scenarios to probe domain-specific
competencies; (2) integrates industrial-organizational and personality
psychology to design sophisticated personas which include behavioral and
psychological descriptors, life history, and social and emotional functions;
and (3) employs structured generation with population demographic priors and
memoir inspired narratives, encoded with Pydantic schemas. In a law enforcement
assistant case study, we construct a rich dataset of personas drawn across 8
persona archetypes and SJTs across 11 attributes, and analyze behaviors across
subpopulation and scenario slices. The dataset spans 8,500 personas, 4,000
SJTs, and 300,000 responses. We will release the dataset and all code to the
public.

</details>


### [134] [Dopamine-driven synaptic credit assignment in neural networks](https://arxiv.org/abs/2510.22178)
*Saranraj Nambusubramaniyan,Shervin Safavi,Raja Guru,Andreas Knoblauch*

Main category: cs.AI

TL;DR: 提出一种基于神经强化学习的无导数优化器Dopamine，通过权重扰动学习解决信用分配问题，在保持性能的同时显著降低计算和内存消耗。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络中的信用分配问题，传统反向传播方法存在计算效率低、内存消耗大、权重传输和更新锁定等问题，需要更高效的优化方法。

Method: 采用权重扰动学习策略，通过最小化期望结果与未扰动模型实际结果之间的奖励预测误差来调整学习率，实现自适应学习率策略。

Result: Dopamine优化器在XOR任务和混沌时间序列预测中表现出加速收敛，性能优于标准权重扰动方法，与基于梯度的算法性能相当，但计算和内存消耗显著降低。

Conclusion: Dopamine优化器不仅找到了稳健解，性能与最先进的机器学习优化器相当，而且在神经生物学上更具合理性。

Abstract: Solving the synaptic Credit Assignment Problem(CAP) is central to learning in
both biological and artificial neural systems. Finding an optimal solution for
synaptic CAP means setting the synaptic weights that assign credit to each
neuron for influencing the final output and behavior of neural networks or
animals. Gradient-based methods solve this problem in artificial neural
networks using back-propagation, however, not in the most efficient way. For
instance, back-propagation requires a chain of top-down gradient computations.
This leads to an expensive optimization process in terms of computing power and
memory linked with well-known weight transport and update locking problems. To
address these shortcomings, we take a NeuroAI approach and draw inspiration
from neural Reinforcement Learning to develop a derivative-free optimizer for
training neural networks, Dopamine. Dopamine is developed for Weight
Perturbation (WP) learning that exploits stochastic updating of weights towards
optima. It achieves this by minimizing the regret, a form of Reward Prediction
Error (RPE) between the expected outcome from the perturbed model and the
actual outcome from the unperturbed model. We use this RPE to adjust the
learning rate in the network (i.e., creating an adaptive learning rate
strategy, similar to the role of dopamine in the brain). We tested the Dopamine
optimizer for training multi-layered perceptrons for XOR tasks, and recurrent
neural networks for chaotic time series forecasting. Dopamine-trained models
demonstrate accelerated convergence and outperform standard WP, and give
comparable performance to gradient-based algorithms, while consuming
significantly less computation and memory. Overall, the Dopamine optimizer not
only finds robust solutions and comparable performance to the state-of-the-art
Machine Learning optimizers but is also neurobiologically more plausible.

</details>


### [135] [OptiTree: Hierarchical Thoughts Generation with Tree Search for LLM Optimization Modeling](https://arxiv.org/abs/2510.22192)
*Haoyang Liu,Jie Wang,Yuyang Cai,Xiongwei Han,Yufei Kuang,Jianye Hao*

Main category: cs.AI

TL;DR: OptiTree提出了一种基于树搜索的自适应问题分解方法，通过将复杂运筹学问题分解为更简单的子问题来提升大语言模型的建模能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用固定步骤分解来生成变量、约束和目标，但由于运筹学问题具有高度复杂的数学结构，这种方法往往无法达到高性能。

Method: 开发了一个建模树，基于运筹学问题的层次化分类和复杂度组织问题，每个节点代表一个问题类别并包含相关的高级建模思路。通过递归搜索树来识别更简单的子问题，并自适应整合层次化思路来合成全局建模思路。

Result: 实验表明，OptiTree相比最先进方法显著提高了建模准确率，在具有挑战性的基准测试中实现了超过10%的提升。

Conclusion: OptiTree通过自适应问题分解和层次化思路整合，有效提升了复杂运筹学问题的建模能力。

Abstract: Optimization modeling is one of the most crucial but technical parts of
operations research (OR). To automate the modeling process, existing works have
leveraged large language models (LLMs), prompting them to break down tasks into
steps for generating variables, constraints, and objectives. However, due to
the highly complex mathematical structures inherent in OR problems, standard
fixed-step decomposition often fails to achieve high performance. To address
this challenge, we introduce OptiTree, a novel tree search approach designed to
enhance modeling capabilities for complex problems through adaptive problem
decomposition into simpler subproblems. Specifically, we develop a modeling
tree that organizes a wide range of OR problems based on their hierarchical
problem taxonomy and complexity, with each node representing a problem category
and containing relevant high-level modeling thoughts. Given a problem to model,
we recurrently search the tree to identify a series of simpler subproblems and
synthesize the global modeling thoughts by adaptively integrating the
hierarchical thoughts. Experiments show that OptiTree significantly improves
the modeling accuracy compared to the state-of-the-art, achieving over 10\%
improvements on the challenging benchmarks. The code is released at
https://github.com/MIRALab-USTC/OptiTree/tree/main.

</details>


### [136] [PACR: Progressively Ascending Confidence Reward for LLM Reasoning](https://arxiv.org/abs/2510.22255)
*Eunseop Yoon,Hee Suk Yoon,Jaehyun Jang,SooHwan Eom,Qi Dai,Chong Luo,Mark A. Hasegawa-Johnson,Chang D. Yoo*

Main category: cs.AI

TL;DR: 提出PACR方法，通过模型对正确答案置信度的递增趋势作为密集内在奖励，加速RLVR训练中的探索过程


<details>
  <summary>Details</summary>
Motivation: RLVR的稀疏结果奖励无法为中间推理步骤提供指导，导致探索缓慢

Method: 使用模型对正确答案置信度的递增趋势作为密集内在奖励，约束探索空间到逻辑合理的推理区域

Result: PACR加速探索，用更少轨迹达到奖励饱和，在多个基准上取得改进

Conclusion: 密集的模型内在塑造信号可以使RLVR训练更有效和可靠

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has significantly
improved LLM reasoning, but its sparse, outcome-based reward provides no
guidance for intermediate steps, slowing exploration. We propose Progressively
Ascending Confidence Reward (PACR), a dense, model-intrinsic reward computed
directly from the model's evolving belief in the correct answer. PACR encodes
the inductive bias that, along a well-formed reasoning trajectory, the
probability of the ground-truth answer should have a generally ascending trend.
We provide empirical and theoretical analysis validating that such an inductive
bias constrains the exploration search space to regions richer in logically
sound reasoning. We demonstrate that PACR accelerates exploration, reaches
reward saturation with fewer trajectories, and yields improvements on multiple
benchmarks. Our results suggest that dense, model-intrinsic shaping signals can
make RLVR training more effective and reliable.

</details>


### [137] [VietLyrics: A Large-Scale Dataset and Models for Vietnamese Automatic Lyrics Transcription](https://arxiv.org/abs/2510.22295)
*Quoc Anh Nguyen,Bernard Cheng,Kelvin Soh*

Main category: cs.AI

TL;DR: 提出了首个大规模越南语歌词转录数据集VietLyrics，包含647小时的歌曲数据，并通过微调Whisper模型在越南语歌词转录任务上取得了优于现有系统的性能。


<details>
  <summary>Details</summary>
Motivation: 越南语歌词转录面临音调复杂性和方言变异的独特挑战，但由于缺乏专用数据集，该领域研究仍处于空白状态。

Method: 构建了VietLyrics数据集，包含647小时带行级对齐歌词的歌曲数据，并微调Whisper模型进行越南语歌词转录。

Result: 微调后的Whisper模型在越南语歌词转录任务上表现优于现有的多语言歌词转录系统（包括LyricWhiz），显著减少了转录错误和非人声段的幻觉问题。

Conclusion: VietLyrics数据集和微调模型为越南语音乐计算研究提供了重要资源，展示了该方法在低资源语言和音乐场景下的潜力。

Abstract: Automatic Lyrics Transcription (ALT) for Vietnamese music presents unique
challenges due to its tonal complexity and dialectal variations, but remains
largely unexplored due to the lack of a dedicated dataset. Therefore, we
curated the first large-scale Vietnamese ALT dataset (VietLyrics), comprising
647 hours of songs with line-level aligned lyrics and metadata to address these
issues. Our evaluation of current ASRbased approaches reveal significant
limitations, including frequent transcription errors and hallucinations in
non-vocal segments. To improve performance, we fine-tuned Whisper models on the
VietLyrics dataset, achieving superior results compared to existing
multilingual ALT systems, including LyricWhiz. We publicly release VietLyrics
and our models, aiming to advance Vietnamese music computing research while
demonstrating the potential of this approach for ALT in low-resource language
and music.

</details>


### [138] [Graph-Coarsening Approach for the Capacitated Vehicle Routing Problem with Time Windows](https://arxiv.org/abs/2510.22329)
*Mustafa Mert Özyılmaz*

Main category: cs.AI

TL;DR: 提出了一种多级图粗化和细化框架，通过时空距离度量将客户聚合成元节点，在简化问题上使用经典启发式算法求解，然后扩展回原始空间并进行可行性修正。


<details>
  <summary>Details</summary>
Motivation: 带时间窗的容量限制车辆路径问题（CVRPTW）是物流中的基础NP难优化问题，大规模实例对精确求解器仍具有计算挑战性。

Method: 使用多级图粗化和细化框架，基于时空距离度量聚合客户为元节点，在简化问题上应用经典启发式算法，然后扩展回原始空间并进行可行性修正。

Result: 在Solomon基准实例上的初步实验表明，该方法减少了计算时间，同时保持或提高了解决方案质量，特别是在容量和时间窗约束方面。

Conclusion: 该方法能有效减少计算时间并保持解决方案质量，同时探索了量子启发优化技术的集成潜力，有望进一步加速大规模车辆路径任务。

Abstract: The Capacitated Vehicle Routing Problem with Time Windows (CVRPTW) is a
fundamental NP-hard optimization problem in logistics. Solving large-scale
instances remains computationally challenging for exact solvers. This work
introduces a multilevel graph coarsening and refinement framework that
aggregates customers into meta-nodes using a spatio-temporal distance metric.
The reduced problem is solved with classical heuristics and subsequently
expanded back into the original space with feasibility corrections. Preliminary
experiments on Solomon benchmark instances show that the proposed method
reduces computation time while preserving or improving solution quality,
particularly with respect to capacity and time window constraints. The paper
also explores the integration of quantum-inspired optimization techniques,
highlighting their potential to further accelerate large-scale vehicle routing
tasks.

</details>


### [139] [LIFT: Interpretable truck driving risk prediction with literature-informed fine-tuned LLMs](https://arxiv.org/abs/2510.22333)
*Xiao Hu,Yuansheng Lian,Ke Zhang,Yunxuan Li,Yuelong Su,Meng Li*

Main category: cs.AI

TL;DR: 提出了一个基于文献知识微调大语言模型(LIFT LLM)的可解释卡车驾驶风险预测框架，该框架在预测准确性和可解释性方面均优于基准模型。


<details>
  <summary>Details</summary>
Motivation: 为了解决卡车驾驶风险预测中模型可解释性不足的问题，同时利用领域文献知识来增强预测的准确性和解释能力。

Method: 构建包含LLM驱动推理核心、文献处理流水线和结果评估器的框架，通过299篇领域文献构建知识库，并在真实数据集上微调LLM。

Result: LIFT LLM在召回率上比基准模型提升26.7%，F1分数提升10.1%，产生的变量重要性排序与基准模型一致，并能识别潜在风险场景。

Conclusion: LIFT LLM框架在卡车驾驶风险预测中表现出优异的性能和可解释性，文献知识库和微调过程对模型可解释性有重要贡献，具有数据驱动知识发现的潜力。

Abstract: This study proposes an interpretable prediction framework with
literature-informed fine-tuned (LIFT) LLMs for truck driving risk prediction.
The framework integrates an LLM-driven Inference Core that predicts and
explains truck driving risk, a Literature Processing Pipeline that filters and
summarizes domain-specific literature into a literature knowledge base, and a
Result Evaluator that evaluates the prediction performance as well as the
interpretability of the LIFT LLM. After fine-tuning on a real-world truck
driving risk dataset, the LIFT LLM achieved accurate risk prediction,
outperforming benchmark models by 26.7% in recall and 10.1% in F1-score.
Furthermore, guided by the literature knowledge base automatically constructed
from 299 domain papers, the LIFT LLM produced variable importance ranking
consistent with that derived from the benchmark model, while demonstrating
robustness in interpretation results to various data sampling conditions. The
LIFT LLM also identified potential risky scenarios by detecting key combination
of variables in truck driving risk, which were verified by PERMANOVA tests.
Finally, we demonstrated the contribution of the literature knowledge base and
the fine-tuning process in the interpretability of the LIFT LLM, and discussed
the potential of the LIFT LLM in data-driven knowledge discovery.

</details>


### [140] [DynaSolidGeo: A Dynamic Benchmark for Genuine Spatial Mathematical Reasoning of VLMs in Solid Geometry](https://arxiv.org/abs/2510.22340)
*Changti Wu,Shijie Lian,Zihao Liu,Lei Zhang,Laurence Tianruo Yang,Kai Chen*

Main category: cs.AI

TL;DR: DynaSolidGeo是首个用于评估视觉语言模型空间推理能力的动态基准，专注于三维立体几何问题，通过半自动标注流程构建，包含503个专家策划的种子问题，可动态生成无限多样的多模态实例。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理基准主要关注二维平面几何，依赖静态数据集容易导致数据污染和记忆问题，且仅通过最终答案评估模型，忽略了推理过程。

Method: 采用半自动标注流程构建动态基准，包含503个专家策划的种子问题，支持动态生成多样化多模态实例，并引入基于专家标注推理链的过程评估。

Result: 实验显示代表性开源和闭源VLMs存在较大性能差距，在动态设置下性能严重下降，在需要高级空间智能的任务（如心理旋转和可视化）上表现较差。

Conclusion: DynaSolidGeo填补了三维空间推理评估的空白，揭示了当前VLMs在空间推理方面的局限性，为未来研究提供了重要基准。

Abstract: Solid geometry problem solving demands spatial mathematical reasoning that
integrates spatial intelligence and symbolic reasoning. However, most existing
multimodal mathematical reasoning benchmarks focus primarily on 2D plane
geometry, rely on static datasets prone to data contamination and memorization,
and evaluate models solely by final answers, overlooking the reasoning process.
To address these limitations, we introduce DynaSolidGeo, the first dynamic
benchmark for evaluating genuine spatial reasoning in Vision-Language Models
(VLMs). Constructed through a semi-automatic annotation pipeline, DynaSolidGeo
contains 503 expert-curated seed questions that can, in principle, dynamically
generate an unbounded number of diverse multimodal text-visual instances.
Beyond answer accuracy, we incorporate process evaluation based on
expert-annotated reasoning chains to measure logical validity and causal
coherence. Experiments across representative open-source and closed-source VLMs
reveal large performance gaps, severe degradation in dynamic settings, and poor
performance on tasks requiring high-level spatial intelligence, such as mental
rotation and visualization. The code and dataset are available at
\href{https://zgca-ai4edu.github.io/DynaSolidGeo/}{DynaSolidGeo}.

</details>


### [141] [Reasoning Models Reason Well, Until They Don't](https://arxiv.org/abs/2510.22371)
*Revanth Rameshkumar,Jimson Huang,Yunxin Sun,Fei Xia,Abulhair Saparov*

Main category: cs.AI

TL;DR: 该论文通过创建DeepRD数据集评估大型推理模型(LRMs)的性能，发现虽然LRMs在现有基准测试中表现优异，但在足够复杂的推理问题上性能急剧下降且无法泛化。


<details>
  <summary>Details</summary>
Motivation: 重新审视LLMs在推理任务中的表现，验证LRMs是否真正具备泛化推理能力，特别是在复杂推理问题上的表现。

Method: 开发了DeepRD数据集，包含可扩展复杂度的图连通性和自然语言证明规划问题，用于评估LRMs在不同复杂度下的性能。

Result: LRMs在足够复杂的推理问题上性能急剧下降，无法泛化到训练分布之外的复杂度水平。虽然大多数现实世界问题落在LRMs的成功范围内，但长尾分布暴露了显著的失败风险。

Conclusion: LRMs在短期内具有实用性，但需要开发能够泛化到训练分布复杂度之外的新方法。

Abstract: Large language models (LLMs) have shown significant progress in reasoning
tasks. However, recent studies show that transformers and LLMs fail
catastrophically once reasoning problems exceed modest complexity. We revisit
these findings through the lens of large reasoning models (LRMs) -- LLMs
fine-tuned with incentives for step-by-step argumentation and
self-verification. LRM performance on graph and reasoning benchmarks such as
NLGraph seem extraordinary, with some even claiming they are capable of
generalized reasoning and innovation in reasoning-intensive fields such as
mathematics, physics, medicine, and law. However, by more carefully scaling the
complexity of reasoning problems, we show existing benchmarks actually have
limited complexity. We develop a new dataset, the Deep Reasoning Dataset
(DeepRD), along with a generative process for producing unlimited examples of
scalable complexity. We use this dataset to evaluate model performance on graph
connectivity and natural language proof planning. We find that the performance
of LRMs drop abruptly at sufficient complexity and do not generalize. We also
relate our LRM results to the distributions of the complexities of large,
real-world knowledge graphs, interaction graphs, and proof datasets. We find
the majority of real-world examples fall inside the LRMs' success regime, yet
the long tails expose substantial failure potential. Our analysis highlights
the near-term utility of LRMs while underscoring the need for new methods that
generalize beyond the complexity of examples in the training distribution.

</details>


### [142] [Modeling Hierarchical Thinking in Large Reasoning Models](https://arxiv.org/abs/2510.22437)
*G M Shahariar,Ali Nazari,Erfan Shayegani,Nael Abu-Ghazaleh*

Main category: cs.AI

TL;DR: 该论文提出使用有限状态机(FSM)来建模大型推理模型(LRMs)的层次推理过程，将推理轨迹表示为状态转换序列，为分析和改进LLM推理提供了新的视角。


<details>
  <summary>Details</summary>
Motivation: 理解大型推理模型(LRMs)涌现的推理能力是一个重要但困难的问题，这对于改进训练和理解模型鲁棒性具有重要应用价值。

Method: 采用无记忆有限状态机(FSM)来近似LRMs的层次推理动态，识别出初始化、演绎、增强策略、不确定性估计、回溯和最终结论等离散推理状态，将推理轨迹表示为状态转换序列。

Result: FSM分析揭示了不同模型在推理方法上的差异，展示了不同的推理模式和潜在缺陷，为模型评估提供了新视角。

Conclusion: 基于FSM的分析为理解和改进LLM推理提供了一种系统化、可解释的方法，能够揭示模型的推理模式并识别潜在问题。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning abilities
when they generate step-by-step solutions, known as chain-of-thought (CoT)
reasoning. When trained to using chain-of-thought reasoning examples, the
resulting models (called Large Reasoning Models, or LRMs) appear to learn
hierarchical thinking strategies similar to those used by humans. However,
understanding LRMs emerging reasoning capabilities remains a difficult open
problem, with many potential important applications including improving
training and understanding robustness. In this paper, we adopt a memoryless
Finite State Machine formulation to approximate LRM's emerging hierarchical
reasoning dynamics as a structured, interpretable abstraction. We identify a
small set of discrete reasoning states including - initialization, deduction,
augmentation-strategy, uncertainty-estimation, backtracking, and
final-conclusion that capture the high-level states present in the model's
reasoning process. By annotating each step of a model's CoT with these states,
we can represent the reasoning trajectory as a transition sequence through the
state graph. This FSM formulation provides a systematic way to analyze,
interpret and visualize how different models approach problems. We describe the
FSM model, provide examples of CoT annotations under this scheme, and discuss
how it can shed light on differences between available models in their approach
to reasoning. Our results demonstrate that this FSM-based analysis reveals
distinct reasoning patterns and potential shortcomings, offering a new lens to
evaluate and improve LLM reasoning.

</details>


### [143] [Learning "Partner-Aware" Collaborators in Multi-Party Collaboration](https://arxiv.org/abs/2510.22462)
*Abhijnan Nath,Nikhil Krishnaswamy*

Main category: cs.AI

TL;DR: 提出了一种可中断协作角色扮演者(ICR)算法，用于训练LLM代理在协作任务中更好地处理伙伴干预，促进团队共识收敛。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在代理设置中部署，需要评估其在多轮多方任务中的协作能力。现有LLM代理倾向于忽略伙伴的干预，使得提升团队共识变得困难。

Method: 使用改进动作MDP分析标准AI代理的次优行为，提出ICR算法训练共识最优的协作代理，使其能够智能收集伙伴干预信息。

Result: 在多个协作任务环境中的实验表明，ICR平均能更好地促进共识收敛和探索更多样化的解决方案。

Conclusion: ICR算法能有效解决LLM代理在协作任务中忽略干预的问题，提升团队共识和协作效果。

Abstract: Large Language Models (LLMs) are increasingly bring deployed in agentic
settings where they act as collaborators with humans. Therefore, it is
increasingly important to be able to evaluate their abilities to collaborate
effectively in multi-turn, multi-party tasks. In this paper, we build on the AI
alignment and safe interruptability literature to offer novel theoretical
insights on collaborative behavior between LLM-driven collaborator agents and
an intervention agent. Our goal is to learn an ideal partner-aware collaborator
that increases the group's common-ground (CG)-alignment on task-relevant
propositions-by intelligently collecting information provided in interventions
by a partner agent.We show how LLM agents trained using standard RLHF and
related approaches are naturally inclined to ignore possibly well-meaning
interventions, which makes increasing group common ground non-trivial in this
setting. We employ a two-player Modified-Action MDP to examine this suboptimal
behavior of standard AI agents, and propose Interruptible Collaborative
Roleplayer (ICR)-a novel partner-aware learning algorithm to train CG-optimal
collaborators. Experiments on multiple collaborative task environments show
that ICR, on average, is more capable of promoting successful CG convergence
and exploring more diverse solutions in such tasks.

</details>


### [144] [OFFSIDE: Benchmarking Unlearning Misinformation in Multimodal Large Language Models](https://arxiv.org/abs/2510.22535)
*Hao Zheng,Zirui Pang,Ling li,Zhijie Deng,Yuhan Pu,Zhaowei Zhu,Xiaobo Xia,Jiaheng Wei*

Main category: cs.AI

TL;DR: OFFSIDE是一个基于足球转会谣言的多模态大语言模型遗忘评估基准，包含15.68K条手动整理的记录，评估遗忘效果、泛化性、实用性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型的发展加剧了数据隐私担忧，而现有的机器遗忘基准在图像多样性、准确性和评估场景方面存在不足，无法反映真实应用的复杂性。

Method: 构建基于足球转会谣言的手动整理数据集，包含四个测试集来评估遗忘效果、泛化性、实用性和鲁棒性，支持选择性遗忘、纠正性再学习和单模态遗忘等高级设置。

Result: 评估发现：单模态方法在多模态谣言上失败；遗忘效果主要由灾难性遗忘驱动；所有方法都难以处理"视觉谣言"；遗忘的谣言容易被恢复；所有方法都易受提示攻击。

Conclusion: 当前方法存在显著漏洞，需要更鲁棒的多模态遗忘解决方案。

Abstract: Advances in Multimodal Large Language Models (MLLMs) intensify concerns about
data privacy, making Machine Unlearning (MU), the selective removal of learned
information, a critical necessity. However, existing MU benchmarks for MLLMs
are limited by a lack of image diversity, potential inaccuracies, and
insufficient evaluation scenarios, which fail to capture the complexity of
real-world applications. To facilitate the development of MLLMs unlearning and
alleviate the aforementioned limitations, we introduce OFFSIDE, a novel
benchmark for evaluating misinformation unlearning in MLLMs based on football
transfer rumors. This manually curated dataset contains 15.68K records for 80
players, providing a comprehensive framework with four test sets to assess
forgetting efficacy, generalization, utility, and robustness. OFFSIDE supports
advanced settings like selective unlearning and corrective relearning, and
crucially, unimodal unlearning (forgetting only text data). Our extensive
evaluation of multiple baselines reveals key findings: (1) Unimodal methods
(erasing text-based knowledge) fail on multimodal rumors; (2) Unlearning
efficacy is largely driven by catastrophic forgetting; (3) All methods struggle
with "visual rumors" (rumors appear in the image); (4) The unlearned rumors can
be easily recovered and (5) All methods are vulnerable to prompt attacks. These
results expose significant vulnerabilities in current approaches, highlighting
the need for more robust multimodal unlearning solutions. The code is available
at
\href{https://github.com/zh121800/OFFSIDE}{https://github.com/zh121800/OFFSIDE}.

</details>


### [145] [ATOM: AdapTive and OptiMized dynamic temporal knowledge graph construction using LLMs](https://arxiv.org/abs/2510.22590)
*Yassir Lairgi,Ludovic Moncla,Khalid Benabdeslem,Rémy Cazabet,Pierre Cléau*

Main category: cs.AI

TL;DR: ATOM是一个从非结构化文本构建和持续更新时序知识图谱的少样本可扩展方法，通过将文档分解为原子事实并使用双时间建模，显著提高了提取的完整性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统静态知识图谱构建忽略了现实世界数据的动态性和时效性，而现有的零样本或少样本方法存在跨多次运行的不稳定性和关键事实覆盖不完整的问题。

Method: 将输入文档分解为最小自包含的原子事实，构建原子时序知识图谱，采用双时间建模区分信息被观察的时间和有效时间，然后并行合并这些原子图谱。

Result: 相比基线方法，ATOM实现了约18%的完整性提升、约17%的稳定性改善，以及超过90%的延迟减少，显示出强大的动态时序知识图谱构建可扩展性。

Conclusion: ATOM方法有效解决了动态时序知识图谱构建中的完整性和稳定性问题，具有显著的可扩展潜力。

Abstract: In today's rapidly expanding data landscape, knowledge extraction from
unstructured text is vital for real-time analytics, temporal inference, and
dynamic memory frameworks. However, traditional static knowledge graph (KG)
construction often overlooks the dynamic and time-sensitive nature of
real-world data, limiting adaptability to continuous changes. Moreover, recent
zero- or few-shot approaches that avoid domain-specific fine-tuning or reliance
on prebuilt ontologies often suffer from instability across multiple runs, as
well as incomplete coverage of key facts. To address these challenges, we
introduce ATOM (AdapTive and OptiMized), a few-shot and scalable approach that
builds and continuously updates Temporal Knowledge Graphs (TKGs) from
unstructured texts. ATOM splits input documents into minimal, self-contained
"atomic" facts, improving extraction exhaustivity and stability. Then, it
constructs atomic TKGs from these facts while employing a dual-time modeling
that distinguishes when information is observed from when it is valid. The
resulting atomic TKGs are subsequently merged in parallel. Empirical
evaluations demonstrate that ATOM achieves ~18% higher exhaustivity, ~17%
better stability, and over 90% latency reduction compared to baseline methods,
demonstrating a strong scalability potential for dynamic TKG construction.

</details>


### [146] [A Framework for Quantifying How Pre-Training and Context Benefit In-Context Learning](https://arxiv.org/abs/2510.22594)
*Bingqing Song,Jiaxiang Li,Rong Wang,Songtao Lu,Mingyi Hong*

Main category: cs.AI

TL;DR: 该论文提出了一个分析上下文学习性能的新框架，通过理论分析和实验验证，揭示了预训练数据分布与查询任务分布差异对ICL性能的影响机制。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型展现出强大的上下文学习能力，但理论上尚不清楚这种能力如何产生，特别是预训练过程和上下文构建等关键因素的确切作用。

Method: 构建了一个包含单层transformer的简单示例，然后扩展到更一般情况，推导了ICL性能、上下文长度以及预训练与查询任务分布KL散度之间的精确关系，并通过实验验证理论结果。

Result: 当预训练数据分布与查询任务分布不同时，适当构建的上下文可以将输出分布向查询任务分布转移，以可量化的方式实现准确预测。

Conclusion: 论文建立了ICL性能与关键因素之间的理论联系，为理解上下文学习机制提供了理论框架。

Abstract: Pre-trained large language models have demonstrated a strong ability to learn
from context, known as in-context learning (ICL). Despite a surge of recent
applications that leverage such capabilities, it is by no means clear, at least
theoretically, how the ICL capabilities arise, and in particular, what is the
precise role played by key factors such as pre-training procedure as well as
context construction. In this work, we propose a new framework to analyze the
ICL performance, for a class of realistic settings, which includes network
architectures, data encoding, data generation, and prompt construction process.
As a first step, we construct a simple example with a one-layer transformer,
and show an interesting result, namely when the pre-train data distribution is
different from the query task distribution, a properly constructed context can
shift the output distribution towards the query task distribution, in a
quantifiable manner, leading to accurate prediction on the query topic. We then
extend the findings in the previous step to a more general case, and derive the
precise relationship between ICL performance, context length and the KL
divergence between pre-train and query task distribution. Finally, we provide
experiments to validate our theoretical results.

</details>


### [147] [CLIN-LLM: A Safety-Constrained Hybrid Framework for Clinical Diagnosis and Treatment Generation](https://arxiv.org/abs/2510.22609)
*Md. Mehedi Hasan,Rafid Mostafiz,Md. Abir Hossain,Bikash Kumar Paul*

Main category: cs.AI

TL;DR: CLIN-LLM是一个安全约束的混合管道系统，集成了多模态患者编码、不确定性校准的疾病分类和检索增强的治疗生成，在症状到疾病分类和治疗推荐方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的医疗系统缺乏医学基础且无法量化不确定性，导致输出不安全。需要开发能够提供安全、可靠医疗决策支持的系统。

Method: 使用BioBERT在1,200个临床案例上微调，结合Focal Loss和蒙特卡洛Dropout实现置信度感知预测。采用Biomedical Sentence-BERT从MedDialog语料库检索相关对话，使用FLAN-T5模型生成个性化治疗建议，并通过RxNorm进行抗生素管理和药物相互作用筛查。

Result: CLIN-LLM达到98%的准确率和F1分数，比ClinicalBERT提高7.1%，检索精度为78%，临床医生评定的有效性为4.2/5分。不安全抗生素建议比GPT-5减少67%。

Conclusion: CLIN-LLM展示了稳健性、可解释性和临床安全性，为资源有限的医疗环境提供了可部署的人机协同决策支持框架。

Abstract: Accurate symptom-to-disease classification and clinically grounded treatment
recommendations remain challenging, particularly in heterogeneous patient
settings with high diagnostic risk. Existing large language model (LLM)-based
systems often lack medical grounding and fail to quantify uncertainty,
resulting in unsafe outputs. We propose CLIN-LLM, a safety-constrained hybrid
pipeline that integrates multimodal patient encoding, uncertainty-calibrated
disease classification, and retrieval-augmented treatment generation. The
framework fine-tunes BioBERT on 1,200 clinical cases from the Symptom2Disease
dataset and incorporates Focal Loss with Monte Carlo Dropout to enable
confidence-aware predictions from free-text symptoms and structured vitals.
Low-certainty cases (18%) are automatically flagged for expert review, ensuring
human oversight. For treatment generation, CLIN-LLM employs Biomedical
Sentence-BERT to retrieve top-k relevant dialogues from the 260,000-sample
MedDialog corpus. The retrieved evidence and patient context are fed into a
fine-tuned FLAN-T5 model for personalized treatment generation, followed by
post-processing with RxNorm for antibiotic stewardship and drug-drug
interaction (DDI) screening. CLIN-LLM achieves 98% accuracy and F1 score,
outperforming ClinicalBERT by 7.1% (p < 0.001), with 78% top-5 retrieval
precision and a clinician-rated validity of 4.2 out of 5. Unsafe antibiotic
suggestions are reduced by 67% compared to GPT-5. These results demonstrate
CLIN-LLM's robustness, interpretability, and clinical safety alignment. The
proposed system provides a deployable, human-in-the-loop decision support
framework for resource-limited healthcare environments. Future work includes
integrating imaging and lab data, multilingual extensions, and clinical trial
validation.

</details>


### [148] [SwiftSolve: A Self-Iterative, Complexity-Aware Multi-Agent Framework for Competitive Programming](https://arxiv.org/abs/2510.22626)
*Adhyayan Veer Singh,Aaron Shen,Brian Law,Ahmed Ismail,Jonas Rohweder,Sean O'Brien,Kevin Zhu*

Main category: cs.AI

TL;DR: SwiftSolve是一个复杂度感知的多智能体系统，用于竞争性编程，通过算法规划与经验分析相结合，确保程序不仅正确还满足时间和内存限制。


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成的程序虽然能通过单元测试，但经常违反竞赛的时间和内存限制，需要一种能同时保证正确性和效率的方法。

Method: 采用多智能体系统，包括规划器提出算法草图、静态剪枝器过滤高风险计划、编码器生成C++代码、分析器记录性能、复杂度分析器分配复杂度类别并进行针对性修复。

Result: 在26个问题上的评估显示，首轮通过率61.54%，三轮内解决率80.77%，运行成功率73.08%，相比Claude Opus 4有明显提升。

Conclusion: 通过性能分析和复杂度指导的重新规划，SwiftSolve在保持准确性的同时显著减少了效率问题，证明了该方法在竞争性编程中的有效性。

Abstract: Correctness alone is insufficient: LLM-generated programs frequently satisfy
unit tests while violating contest time or memory budgets. We present
SwiftSolve, a complexity-aware multi-agent system for competitive programming
that couples algorithmic planning with empirical profiling and
complexity-guided repair. We frame competitive programming as a software
environment where specialized agents act as programmers, each assuming roles
such as planning, coding, profiling, and complexity analysis. A Planner
proposes an algorithmic sketch; a deterministic Static Pruner filters high-risk
plans; a Coder emits ISO C++17; a Profiler compiles and executes candidates on
a fixed input-size schedule to record wall time and peak memory; and a
Complexity Analyst fits log-log growth (s, R2) with an LLM fallback to assign a
complexity class and dispatch targeted patches to either the Planner or Coder.
Agents communicate via typed, versioned JSON; a controller enforces iteration
caps and diminishing returns stopping. Evaluated on 26 problems (16 BigO, 10
Codeforces Div. 2) in a POSIX sandbox (2 s / 256-512 MB), SwiftSolve attains
pass@1 = 61.54% (16/26) on the first attempt and Solved@<=3 = 80.77% with
marginal latency change (mean 11.96 s to 12.66 s per attempt). Aggregate
run-level success is 73.08% at 12.40 s mean. Failures are predominantly
resource-bound, indicating inefficiency rather than logic errors. Against
Claude Opus 4, SwiftSolve improves run-level success (73.1% vs 52.6%) at
approximately 2x runtime overhead (12.4 s vs 6.8 s). Beyond correctness
(pass@k), we report efficiency metrics (eff@k for runtime and memory, incidence
of TLE or MLE, and complexity fit accuracy on BigO), demonstrating that
profiling and complexity-guided replanning reduce inefficiency while preserving
accuracy.

</details>


### [149] [Do Stop Me Now: Detecting Boilerplate Responses with a Single Iteration](https://arxiv.org/abs/2510.22679)
*Yuval Kainan,Shaked Zychlinski*

Main category: cs.AI

TL;DR: 提出基于首个生成token的对数概率分布来检测LLM模板化响应的方法，可在单步生成后实现高效分类，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: LLM在生成模板化响应（如拒绝、简单确认等）时浪费大量计算资源，增加了不必要的成本和延迟。

Method: 利用首个生成token的对数概率分布作为分类信号，使用轻量级k-NN分类器预测响应类型，支持早期终止或重定向到小模型。

Result: 实验表明首个token对数概率向量在不同响应类型下形成明显可分离簇，能高精度预测实质性回答与模板化响应。

Conclusion: 该方法为LLM推理提供实用且计算简单的优化技术，实现显著计算成本节约，推动更高效可持续的LLM部署。

Abstract: Large Language Models (LLMs) often expend significant computational resources
generating boilerplate responses, such as refusals, simple acknowledgements and
casual greetings, which adds unnecessary cost and latency. To address this
inefficiency, we propose a simple yet highly effective method for detecting
such responses after only a single generation step. We demonstrate that the
log-probability distribution of the first generated token serves as a powerful
signal for classifying the nature of the entire subsequent response. Our
experiments, conducted across a diverse range of small, large, and
reasoning-specialized models, show that the first-token log-probability vectors
form distinctly separable clusters for different response types. Using a
lightweight k-NN classifier, we achieve high accuracy in predicting whether a
response will be a substantive answer or a form of boilerplate response,
including user-specified refusals. The primary implication is a practical,
computationally trivial technique, optimizing LLM inference by enabling early
termination or redirection to a smaller model, thereby yielding significant
savings in computational cost. This work presents a direct path toward more
efficient and sustainable LLM deployment.

</details>


### [150] [Atlas Urban Index: A VLM-Based Approach for Spatially and Temporally Calibrated Urban Development Monitoring](https://arxiv.org/abs/2510.22702)
*Mithul Chander,Sai Pragnya Ranga,Prathamesh Mayekar*

Main category: cs.AI

TL;DR: 提出了Atlas Urban Index (AUI)，一种利用Sentinel-2卫星影像和视觉语言模型来测量城市发展的新指标，相比传统NDBI指数能更准确捕捉城市发展。


<details>
  <summary>Details</summary>
Motivation: 现有方法如NDBI在捕捉城市发展时受大气噪声、季节变化和云层覆盖等因素影响，限制了大规模人类发展和城市化监测的准确性。

Method: 收集区域的时间序列Sentinel-2图像，在固定时间窗口内处理图像以减少云层覆盖，使用视觉语言模型结合参考图像和最新历史图像来提供发展评分。

Result: 在班加罗尔的定性实验表明，AUI优于NDBI等标准指数，能够产生更可靠和稳定的发展评分。

Conclusion: AUI能够克服传统城市化指数的挑战，为城市发展监测提供更准确和一致的方法。

Abstract: We introduce the {\em Atlas Urban Index} (AUI), a metric for measuring urban
development computed using Sentinel-2 \citep{spoto2012sentinel2} satellite
imagery. Existing approaches, such as the {\em Normalized Difference Built-up
Index} (NDBI), often struggle to accurately capture urban development due to
factors like atmospheric noise, seasonal variation, and cloud cover. These
limitations hinder large-scale monitoring of human development and
urbanization. To address these challenges, we propose an approach that
leverages {\em Vision-Language Models }(VLMs) to provide a development score
for regions. Specifically, we collect a time series of Sentinel-2 images for
each region. Then, we further process the images within fixed time windows to
get an image with minimal cloud cover, which serves as the representative image
for that time window. To ensure consistent scoring, we adopt two strategies:
(i) providing the VLM with a curated set of reference images representing
different levels of urbanization, and (ii) supplying the most recent past image
to both anchor temporal consistency and mitigate cloud-related noise in the
current image. Together, these components enable AUI to overcome the challenges
of traditional urbanization indices and produce more reliable and stable
development scores. Our qualitative experiments on Bangalore suggest that AUI
outperforms standard indices such as NDBI.

</details>


### [151] [RaCoT: Plug-and-Play Contrastive Example Generation Mechanism for Enhanced LLM Reasoning Reliability](https://arxiv.org/abs/2510.22710)
*Kaitong Cai,Jusheng Zhang,Yijia Fan,Jing Yang,Keze Wang*

Main category: cs.AI

TL;DR: RaCoT是一个在检索前阶段引入对比思维的RAG框架，通过生成对比问题来指导模型关注关键差异细节，在单次检索中抑制语义干扰，显著提升长尾查询性能。


<details>
  <summary>Details</summary>
Motivation: 解决RAG在处理知识稀疏和语义模糊的长尾查询时面临的检索噪声问题，避免昂贵的后处理成本。

Method: 在检索前自动生成语义相邻但答案不同的对比问题，提取Δ-Prompt捕获关键差异，引导模型主动关注决定答案分歧的关键细节。

Result: 在六个权威基准测试中，RaCoT比RankRAG和Self-RAG等基线方法提升0.9-2.4个百分点，对抗性测试中性能仅下降8.6%，延迟3.12秒，token开销11.54。

Conclusion: RaCoT将RAG范式从"事后上下文清理"重构为"先验塑造判别推理"，为实时资源受限部署提供了高效稳健的路径。

Abstract: Retrieval-Augmented Generation (RAG) faces a core bottleneck with
knowledge-sparse and semantically ambiguous long-tail queries, where retrieval
noise distorts reasoning and necessitates costly post-processing. To tackle
this, we propose RaCoT (Retrieval-aware Contrastive-of-Thought), a novel
framework that shifts contrastive thinking to the pre-retrieval stage. By
automatically generating a semantically adjacent yet differently answered
contrastive question and extracting a $\Delta$-Prompt to capture their key
differences, RaCoT guides the model to proactively focus on the ``critical
details that determine answer divergence." This approach allows it to suppress
semantic interference within a single retrieval pass, overcoming the
theoretical bottleneck of single-vector queries that struggle to simultaneously
encode signals for what to attend to and what to ignore. On six authoritative
benchmarks, including PopQA and TriviaQA-unfiltered, RaCoT outperforms strong
baselines like RankRAG and Self-RAG by 0.9-2.4 percentage points. It exhibits
superior robustness, with a performance drop of only 8.6\% in adversarial
tests, far surpassing the over 15\% degradation in other methods. Furthermore,
its low latency (3.12s) and token overhead (11.54) place it on the
accuracy-efficiency Pareto frontier, while ablation studies validate the
necessity of each component. Ultimately, RaCoT reframes the RAG paradigm from
``post-hoc context cleaning" to ``a priori shaping of discriminative
reasoning", offering an efficient and robust path toward reliable AI systems
for real-time, resource-constrained deployments.

</details>


### [152] [Critical Insights into Leading Conversational AI Models](https://arxiv.org/abs/2510.22729)
*Urja Kohli,Aditi Singh,Arun Sharma*

Main category: cs.AI

TL;DR: 本文比较了五个主流大语言模型的性能、伦理和可用性差异，发现不同模型各有优势：Claude擅长道德推理，Gemini在多模态能力和伦理框架方面表现突出，DeepSeek在事实推理方面优秀，LLaMA适合开放应用，ChatGPT提供平衡性能。


<details>
  <summary>Details</summary>
Motivation: 随着各大公司不断改进大语言模型，了解不同模型在性能、道德行为和可用性方面的差异变得至关重要，这些差异反映了构建它们的不同理念。

Method: 通过分析三个关键因素来比较五个顶级LLM：性能和准确性、伦理和偏见缓解、可用性和集成性。

Result: Claude具有良好的道德推理能力，Gemini在多模态能力和强大伦理框架方面更优，DeepSeek在基于事实的推理方面表现出色，LLaMA适合开放应用，ChatGPT提供平衡性能并注重使用体验。

Conclusion: 这些模型在工作效果、易用性和伦理处理方面存在差异，用户应根据各自优势来充分利用不同模型。

Abstract: Big Language Models (LLMs) are changing the way businesses use software, the
way people live their lives and the way industries work. Companies like Google,
High-Flyer, Anthropic, OpenAI and Meta are making better LLMs. So, it's crucial
to look at how each model is different in terms of performance, moral behaviour
and usability, as these differences are based on the different ideas that built
them. This study compares five top LLMs: Google's Gemini, High-Flyer's
DeepSeek, Anthropic's Claude, OpenAI's GPT models and Meta's LLaMA. It performs
this by analysing three important factors: Performance and Accuracy, Ethics and
Bias Mitigation and Usability and Integration. It was found that Claude has
good moral reasoning, Gemini is better at multimodal capabilities and has
strong ethical frameworks. DeepSeek is great at reasoning based on facts, LLaMA
is good for open applications and ChatGPT delivers balanced performance with a
focus on usage. It was concluded that these models are different in terms of
how well they work, how easy they are to use and how they treat people
ethically, making it a point that each model should be utilised by the user in
a way that makes the most of its strengths.

</details>


### [153] [Multi-Modal Fact-Verification Framework for Reducing Hallucinations in Large Language Models](https://arxiv.org/abs/2510.22751)
*Piyushkumar Patel*

Main category: cs.AI

TL;DR: 提出了一个实时事实验证框架，通过交叉检查LLM输出与多个知识源来捕捉和纠正幻觉错误，将幻觉减少67%，专家满意度达89%


<details>
  <summary>Details</summary>
Motivation: 解决LLM自信生成虚假信息的幻觉问题，这是在实际应用中部署LLM的主要障碍

Method: 开发事实验证框架，结合结构化数据库、实时网络搜索和学术文献，在生成时验证事实声明，检测到不一致时自动纠正并保持回答自然流畅

Result: 测试显示幻觉减少67%，不牺牲回答质量；医疗、金融和科研领域的专家对纠正后输出的满意度达89%，显著优于未验证的LLM回答

Conclusion: 为在准确性至关重要的应用中使LLM更可信提供了实用解决方案

Abstract: While Large Language Models have transformed how we interact with AI systems,
they suffer from a critical flaw: they confidently generate false information
that sounds entirely plausible. This hallucination problem has become a major
barrier to deploying these models in real-world applications where accuracy
matters. We developed a fact verification framework that catches and corrects
these errors in real-time by cross checking LLM outputs against multiple
knowledge sources. Our system combines structured databases, live web searches,
and academic literature to verify factual claims as they're generated. When we
detect inconsistencies, we automatically correct them while preserving the
natural flow of the response. Testing across various domains showed we could
reduce hallucinations by 67% without sacrificing response quality. Domain
experts in healthcare, finance, and scientific research rated our corrected
outputs 89% satisfactory a significant improvement over unverified LLM
responses. This work offers a practical solution for making LLMs more
trustworthy in applications where getting facts wrong isn't an option.

</details>


### [154] [Jarvis: Towards Personalized AI Assistant via Personal KV-Cache Retrieval](https://arxiv.org/abs/2510.22765)
*Binxiao Xu,Junyu Feng,Ruichuan An,Yulin Luo,Shilin Yan,Hao Liang,Ming Lu,Wentao Zhang*

Main category: cs.AI

TL;DR: Jarvis是一个通过个人KV-Cache检索实现个性化AI助手的创新框架，在视觉问答和纯文本任务中实现最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有方法要么学习概念标记集，要么训练VLM使用用户特定信息，但两种方法都难以生成准确答案。需要更有效的个性化AI助手解决方案

Method: 在文本和视觉标记的KV-Cache中存储用户特定信息：文本标记通过总结用户信息为元数据创建，视觉标记通过从用户图像提取独特图像块产生。回答问题前先检索相关KV-Cache

Result: Jarvis能够提供更准确的响应，特别是在依赖特定局部细节时。在多个数据集的视觉问答和纯文本任务中实现最先进结果

Conclusion: Jarvis为个性化AI助手提供了一条实用路径，代码和数据集将发布

Abstract: The rapid development of Vision-language models (VLMs) enables open-ended
perception and reasoning. Recent works have started to investigate how to adapt
general-purpose VLMs into personalized assistants. Even commercial models such
as ChatGPT now support model personalization by incorporating user-specific
information. However, existing methods either learn a set of concept tokens or
train a VLM to utilize user-specific information. However, both pipelines
struggle to generate accurate answers as personalized assistants. We introduce
Jarvis, an innovative framework for a personalized AI assistant through
personal KV-Cache retrieval, which stores user-specific information in the
KV-Caches of both textual and visual tokens. The textual tokens are created by
summarizing user information into metadata, while the visual tokens are
produced by extracting distinct image patches from the user's images. When
answering a question, Jarvis first retrieves related KV-Caches from personal
storage and uses them to ensure accuracy in responses. We also introduce a
fine-grained benchmark built with the same distinct image patch mining
pipeline, emphasizing accurate question answering based on fine-grained
user-specific information. Jarvis is capable of providing more accurate
responses, particularly when they depend on specific local details. Jarvis
achieves state-of-the-art results in both visual question answering and
text-only tasks across multiple datasets, indicating a practical path toward
personalized AI assistants. The code and dataset will be released.

</details>


### [155] [How Do AI Agents Do Human Work? Comparing AI and Human Workflows Across Diverse Occupations](https://arxiv.org/abs/2510.22780)
*Zora Zhiruo Wang,Yijia Shao,Omar Shaikh,Daniel Fried,Graham Neubig,Diyi Yang*

Main category: cs.AI

TL;DR: 本文首次直接比较人类与AI代理在多个工作技能上的表现，发现代理虽然工作质量较低且存在数据伪造问题，但效率高、成本低，适合处理可编程任务。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理开发缺乏对人类工作方式的理解，需要揭示代理的专业能力和在不同工作流程中的角色定位。

Method: 引入可扩展工具包从人类或代理的计算机使用活动中提取可解释的结构化工作流程，并在数据分析、工程、计算、写作和设计等技能上进行比较研究。

Result: 代理工作质量较差且存在数据伪造问题，但效率比人类高88.3%，成本低90.4-96.2%；代理倾向于程序化方法，而人类更多使用UI界面。

Conclusion: 代理在可编程任务上具有效率优势，适合与人类协作，但需要解决质量问题和数据伪造行为。

Abstract: AI agents are continually optimized for tasks related to human work, such as
software engineering and professional writing, signaling a pressing trend with
significant impacts on the human workforce. However, these agent developments
have often not been grounded in a clear understanding of how humans execute
work, to reveal what expertise agents possess and the roles they can play in
diverse workflows. In this work, we study how agents do human work by
presenting the first direct comparison of human and agent workers across
multiple essential work-related skills: data analysis, engineering,
computation, writing, and design. To better understand and compare
heterogeneous computer-use activities of workers, we introduce a scalable
toolkit to induce interpretable, structured workflows from either human or
agent computer-use activities. Using such induced workflows, we compare how
humans and agents perform the same tasks and find that: (1) While agents
exhibit promise in their alignment to human workflows, they take an
overwhelmingly programmatic approach across all work domains, even for
open-ended, visually dependent tasks like design, creating a contrast with the
UI-centric methods typically used by humans. (2) Agents produce work of
inferior quality, yet often mask their deficiencies via data fabrication and
misuse of advanced tools. (3) Nonetheless, agents deliver results 88.3% faster
and cost 90.4-96.2% less than humans, highlighting the potential for enabling
efficient collaboration by delegating easily programmable tasks to agents.

</details>


### [156] [Agentic Meta-Orchestrator for Multi-task Copilots](https://arxiv.org/abs/2510.22781)
*Xiaofeng Zhu,Yunshen Zhou*

Main category: cs.AI

TL;DR: 提出了一个用于微软Copilot服务的代理元编排器(AMO)，能够处理多任务和可扩展代理，通过元学习决策树模型选择最佳推理策略，并在M365电商Copilot和代码合规Copilot两个生产用例中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 微软Copilot套件作为各种代理的通用入口点，需要强大的编排器来将用户提示的任务分发给正确的代理，特别是当代理库动态扩展时。

Method: 提出了代理元编排器(AMO)，利用元学习训练决策树模型来决定不同代理/模型之间的最佳推理策略，支持自然语言和动作响应。

Result: 在两个生产用例中展示了AMO的有效性：M365电商Copilot提供最新产品信息并连接多个代理，代码合规Copilot扫描DevOps代码检测合规问题。

Conclusion: AMO为Copilot服务提供了一个强大的多任务和可扩展代理编排解决方案，通过元学习优化了代理选择策略。

Abstract: Microsoft Copilot suites serve as the universal entry point for various
agents skilled in handling important tasks, ranging from assisting a customer
with product purchases to detecting vulnerabilities in corporate programming
code. Each agent can be powered by language models, software engineering
operations, such as database retrieval, and internal \& external knowledge. The
repertoire of a copilot can expand dynamically with new agents. This requires a
robust orchestrator that can distribute tasks from user prompts to the right
agents. In this work, we propose an Agentic Meta-orchestrator (AMO) for
handling multiple tasks and scalable agents in copilot services, which can
provide both natural language and action responses. We will also demonstrate
the planning that leverages meta-learning, i.e., a trained decision tree model
for deciding the best inference strategy among various agents/models. We
showcase the effectiveness of our AMO through two production use cases:
Microsoft 365 (M365) E-Commerce Copilot and code compliance copilot. M365
E-Commerce Copilot advertises Microsoft products to external customers to
promote sales success. The M365 E-Commerce Copilot provides up-to-date product
information and connects to multiple agents, such as relational databases and
human customer support. The code compliance copilot scans the internal DevOps
code to detect known and new compliance issues in pull requests (PR).

</details>


### [157] [Will Humanity Be Rendered Obsolete by AI?](https://arxiv.org/abs/2510.22814)
*Mohamed El Louadi,Emna Ben Romdhane*

Main category: cs.AI

TL;DR: 本文分析人工智能对人类构成的生存风险，探讨从当前AI到超智能的发展轨迹，基于理论研究和近期出版物，研究AGI和超级智能的伦理与生存影响。


<details>
  <summary>Details</summary>
Motivation: 分析人工智能对人类构成的生存风险，特别是当AI的认知能力指数级增长并远超人类时可能带来的伦理和生存威胁。

Method: 基于Irving J. Good和Nick Bostrom的理论工作，结合近期出版物《AI 2027》和《If Anyone Builds It, Everyone Dies》进行分析，探讨AGI和超级智能的发展轨迹。

Result: 研究表明，机器的认知能力呈指数级增长，可能发展出远超人类的智能水平，这种根本性异质的智能可能带来人类灭绝的风险。

Conclusion: 人类灭绝可能不是源于恶意，而是源于不可控制、漠不关心的认知优势，强调了对超智能发展进行伦理考量和风险管控的重要性。

Abstract: This article analyzes the existential risks artificial intelligence (AI)
poses to humanity, tracing the trajectory from current AI to ultraintelligence.
Drawing on Irving J. Good and Nick Bostrom's theoretical work, plus recent
publications (AI 2027; If Anyone Builds It, Everyone Dies), it explores AGI and
superintelligence. Considering machines' exponentially growing cognitive power
and hypothetical IQs, it addresses the ethical and existential implications of
an intelligence vastly exceeding humanity's, fundamentally alien. Human
extinction may result not from malice, but from uncontrollable, indifferent
cognitive superiority.

</details>


### [158] [HRM-Agent: Training a recurrent reasoning model in dynamic environments using reinforcement learning](https://arxiv.org/abs/2510.22832)
*Long H Dang,David Rawlinson*

Main category: cs.AI

TL;DR: HRM-Agent：基于分层推理模型（HRM）的强化学习变体，能够在动态不确定的迷宫环境中学习导航到目标，并成功重用先前时间步的计算。


<details>
  <summary>Details</summary>
Motivation: HRM模型虽然在小规模下具有出色的推理能力，但仅适用于静态、完全可观测的监督学习问题。现实世界问题往往是动态、不确定和部分可观测的，需要能够整合和重用先前计算的能力。

Method: 提出了HRM-Agent，这是HRM的变体，仅使用强化学习进行训练。探索了循环推理过程的动态特性，验证其是否成功重用先前环境时间步的计算。

Result: HRM-Agent能够在动态和不确定的迷宫环境中学习导航到目标。研究发现循环推理过程确实成功重用了先前时间步的计算。

Conclusion: HRM模型可以通过强化学习扩展到动态不确定环境中，其循环推理机制能够有效重用先前计算，为处理现实世界问题提供了可能。

Abstract: The Hierarchical Reasoning Model (HRM) has impressive reasoning abilities
given its small size, but has only been applied to supervised, static,
fully-observable problems. One of HRM's strengths is its ability to adapt its
computational effort to the difficulty of the problem. However, in its current
form it cannot integrate and reuse computation from previous time-steps if the
problem is dynamic, uncertain or partially observable, or be applied where the
correct action is undefined, characteristics of many real-world problems.
  This paper presents HRM-Agent, a variant of HRM trained using only
reinforcement learning. We show that HRM can learn to navigate to goals in
dynamic and uncertain maze environments. Recent work suggests that HRM's
reasoning abilities stem from its recurrent inference process. We explore the
dynamics of the recurrent inference process and find evidence that it is
successfully reusing computation from earlier environment time-steps.

</details>


### [159] [Toward Agents That Reason About Their Computation](https://arxiv.org/abs/2510.22833)
*Adrian Orenstein,Jessica Chen,Gwyneth Anne Delos Santos,Bayley Sapara,Michael Bowling*

Main category: cs.AI

TL;DR: 论文研究让强化学习智能体在训练过程中考虑计算成本，通过赋予智能体控制计算使用的自主权，实现了在相同训练计算预算下性能提升75%且平均计算量减少三倍的效果。


<details>
  <summary>Details</summary>
Motivation: 人类在任务熟练后认知努力会减少，而传统强化学习智能体在性能提升时计算效率并未提高。研究旨在让智能体能够像人类一样随着熟练度提高而减少计算开销，从而实现更节能的智能体或释放计算资源用于其他过程。

Method: 在Arcade学习环境中进行实验，向智能体展示计算成本并赋予其控制计算使用的自主权，让智能体能够自主决定何时使用计算资源。

Result: 在相同训练计算预算下，考虑计算成本的智能体在75%的游戏中表现更好，且平均计算量减少了三倍。论文还分析了具体游戏中的效率提升情况。

Conclusion: 让强化学习智能体在训练过程中考虑计算成本是可行的，能够显著提高计算效率，在保持性能的同时大幅减少计算资源消耗。

Abstract: While reinforcement learning agents can achieve superhuman performance in
many complex tasks, they typically do not become more computationally efficient
as they improve. In contrast, humans gradually require less cognitive effort as
they become more proficient at a task. If agents could reason about their
compute as they learn, could they similarly reduce their computation footprint?
If they could, we could have more energy efficient agents or free up compute
cycles for other processes like planning. In this paper, we experiment with
showing agents the cost of their computation and giving them the ability to
control when they use compute. We conduct our experiments on the Arcade
Learning Environment, and our results demonstrate that with the same training
compute budget, agents that reason about their compute perform better on 75% of
games. Furthermore, these agents use three times less compute on average. We
analyze individual games and show where agents gain these efficiencies.

</details>


### [160] [Rethinking the Text-Vision Reasoning Imbalance in MLLMs through the Lens of Training Recipes](https://arxiv.org/abs/2510.22836)
*Guanyu Yao,Qiucheng Wu,Yang Zhang,Zhaowen Wang,Handong Zhao,Shiyu Chang*

Main category: cs.AI

TL;DR: 本文分析了多模态大语言模型中的模态差距问题，发现现有训练方法会放大视觉和文本模态之间的性能差异，并提出了从数据和损失函数设计两方面来弥合这一差距的策略。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视觉和语言任务上表现出色，但存在模态不平衡问题——过度依赖文本线索而忽视视觉内容，导致在需要真正视觉推理的任务上表现不佳。

Method: 从训练方法角度分析模态差距，系统探索从数据和损失函数设计两个互补视角来弥合差距的策略。

Result: 发现现有训练方法会放大模态差距，提出的策略能够帮助开发更平衡的多模态推理训练方法。

Conclusion: 研究为开发能够减轻模态差距并促进更平衡多模态推理的训练方法提供了见解，相关代码已公开。

Abstract: Multimodal large language models (MLLMs) have demonstrated strong
capabilities on vision-and-language tasks. However, recent findings reveal an
imbalance in their reasoning capabilities across visual and textual modalities.
Specifically, current MLLMs often over-rely on textual cues while
under-attending to visual content, resulting in suboptimal performance on tasks
that require genuine visual reasoning. We refer to this phenomenon as the
\textit{modality gap}, defined as the performance disparity between
text-centric and vision-centric inputs. In this paper, we analyze the modality
gap through the lens of training recipes. We first show that existing training
recipes tend to amplify this gap. Then, we systematically explore strategies to
bridge it from two complementary perspectives: data and loss design. Our
findings provide insights into developing training recipes that mitigate the
modality gap and promote more balanced multimodal reasoning. Our code is
publicly available at https://github.com/UCSB-NLP-Chang/Bridging-Modality-Gap.

</details>


### [161] [Lyapunov Function-guided Reinforcement Learning for Flight Control](https://arxiv.org/abs/2510.22840)
*Yifei Li,Erik-Jan van Kampen*

Main category: cs.AI

TL;DR: 开发了级联在线学习飞行控制系统并改进了动作平滑性，研究了以李雅普诺夫函数候选增量表征的系统收敛性能，考虑了离散化误差和增量模型引入的状态预测误差，通过飞行控制仿真进行了比较分析。


<details>
  <summary>Details</summary>
Motivation: 研究级联在线学习飞行控制系统的收敛性能，特别是考虑离散化误差和状态预测误差对系统稳定性的影响。

Method: 使用李雅普诺夫函数候选的增量作为收敛性能指标，推导过程中考虑了离散化误差和增量模型引入的状态预测误差，通过飞行控制仿真进行验证。

Result: 通过仿真比较展示了系统收敛性能的分析结果，验证了所提方法的有效性。

Conclusion: 该研究为级联在线学习飞行控制系统的收敛性能分析提供了理论框架和验证方法，有助于改进控制系统的稳定性和性能。

Abstract: A cascaded online learning flight control system has been developed and
enhanced with respect to action smoothness. In this paper, we investigate the
convergence performance of the control system, characterized by the increment
of a Lyapunov function candidate. The derivation of this metric accounts for
discretization errors and state prediction errors introduced by the incremental
model. Comparative results are presented through flight control simulations.

</details>


### [162] [Exploring Structures of Inferential Mechanisms through Simplistic Digital Circuits](https://arxiv.org/abs/2510.22883)
*Giovanni Sileno,Jean-Louis Dessalles*

Main category: cs.AI

TL;DR: 该论文提出了一个基于逻辑门电子电路的统一框架，将认知研究和人工智能中的各种推理机制（分类、归纳、溯因、因果推理等）整合起来，通过组合探索识别出四种主要依赖形式和八种常见推理模式。


<details>
  <summary>Details</summary>
Motivation: 认知研究和人工智能为各种推理机制开发了不同的模型，但缺乏统一的框架。本文试图填补这一空白，从物质角度假设高级激活过程。

Method: 采用符号AI建模技术，通过基于逻辑门的电子电路简化视角来考虑推理机制，进行组合探索识别依赖形式，并在逻辑程序背景下分析推理模式。

Result: 识别出四种主要依赖形式和八种常见推理模式，在统一框架中展示了传统上不同的推理机制，并通过逻辑程序的概率解释揭示了内部功能依赖。

Conclusion: 尽管论证主要基于符号方法和数字系统基础设施，但观察结果可能指向更普遍适用的结构，为认知和AI推理机制提供了统一视角。

Abstract: Cognitive studies and artificial intelligence have developed distinct models
for various inferential mechanisms (categorization, induction, abduction,
causal inference, contrast, merge, ...). Yet, both natural and artificial views
on cognition lack apparently a unifying framework. This paper formulates a
speculative answer attempting to respond to this gap. To postulate on
higher-level activation processes from a material perspective, we consider
inferential mechanisms informed by symbolic AI modelling techniques, through
the simplistic lenses of electronic circuits based on logic gates. We observe
that a logic gate view entails a different treatment of implication and
negation compared to standard logic and logic programming. Then, by
combinatorial exploration, we identify four main forms of dependencies that can
be realized by these inferential circuits. Looking at how these forms are
generally used in the context of logic programs, we identify eight common
inferential patterns, exposing traditionally distinct inferential mechanisms in
an unifying framework. Finally, following a probabilistic interpretation of
logic programs, we unveil inner functional dependencies. The paper concludes
elaborating in what sense, even if our arguments are mostly informed by
symbolic means and digital systems infrastructures, our observations may
pinpoint to more generally applicable structures.

</details>


### [163] [On Generalization in Agentic Tool Calling: CoreThink Agentic Reasoner and MAVEN Dataset](https://arxiv.org/abs/2510.22898)
*Vishvesh Bhat,Omkar Ghugarkar,Julian McAuley*

Main category: cs.AI

TL;DR: 本文提出了CoreThink Agentic Reasoner框架，通过轻量级符号推理层增强LLMs，在多个工具调用基准测试中实现了530%的性能提升，且计算成本仅为现有基线的十分之一。


<details>
  <summary>Details</summary>
Motivation: 解决智能体工具调用环境中的泛化挑战，当前LLMs在跨领域转移推理策略和协调工具方面能力不足，在OOD基准测试中准确率低于50%。

Method: 开发CoreThink Agentic Reasoner框架，为LLMs添加轻量级符号推理层，实现结构化分解和自适应工具编排，无需额外训练。

Result: 在多个工具调用基准测试（BFCL v3、TauBench、Tau2Bench、AceBench）和新的MAVEN OOD基准上，该框架实现了最先进性能，比现有基线提升530%，计算成本降低90%。

Conclusion: CoreThink Agentic Reasoner通过符号推理层有效解决了LLMs在工具调用环境中的泛化问题，显著提升了跨领域推理能力，同时大幅降低了计算成本。

Abstract: Generalization across Agentic tool-calling environments remains a key
unsolved challenge in developing reliable agentic reasoning systems. While
large language models (LLMs) demonstrate strong performance on isolated
benchmarks, their ability to transfer reasoning strategies and co-ordinate
tools across diverse domains is poorly understood. In this work, we conduct a
large-scale evaluation of state-of-the-art LLMs on multiple tool-calling
benchmarksBFCL v3, TauBench, Tau2Bench, and AceBenchand introduce MAVEN (Math &
Physics Adversarial Verification & Evaluation Network), a new out of
distribution (OOD) benchmark designed to stress-test multi-step reasoning
through explicit verification and adversarial task composition. Our results
show that most current models achieve below 50% accuracy on MAVEN, revealing a
significant generalization gap across tool-use settings.
  To address this, we present the CoreThink Agentic Reasoner, a framework that
augments LLMs with a lightweight symbolic reasoning layer for structured
decomposition and adaptive tool orchestration. Without additional training, it
generalizes across all benchmarks, achieving state-of-the-art performance with
530% improvements over existing baselines at roughly one-tenth the
computational cost.

</details>


### [164] [GTR-Mamba: Geometry-to-Tangent Routing for Hyperbolic POI Recommendation](https://arxiv.org/abs/2510.22942)
*Zhuoxuan Li,Jieyuan Pei,Tangwei Ye,Zhongyuan Lai,Zihan Liu,Fengyuan Xu,Qi Zhang,Liang Hu*

Main category: cs.AI

TL;DR: GTR-Mamba是一个新颖的下一个兴趣点推荐框架，通过跨流形条件路由，在双曲几何中建模静态偏好层次结构，在欧几里得切空间中处理动态序列更新，有效解决了现有模型难以同时捕捉空间选择层次结构和用户特定时间上下文动态变化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于图神经网络和序列模型的POI推荐模型存在根本性局限：无法同时捕捉空间选择的内在层次结构和用户特定时间上下文的动态变化及不规则转变。

Method: 提出GTR-Mamba框架，利用不同数学空间的优势：在双曲几何中建模静态的树状偏好层次结构，在欧几里得切空间中使用新颖的Mamba层路由动态序列更新，通过跨流形通道融合时空信息来显式引导状态空间模型。

Result: 在三个真实世界数据集上的广泛实验表明，GTR-Mamba在下一个POI推荐任务中持续优于最先进的基线模型。

Conclusion: GTR-Mamba通过跨流形条件路由成功解决了同时建模空间层次结构和时间动态性的挑战，为POI推荐提供了有效解决方案。

Abstract: Next Point-of-Interest (POI) recommendation is a critical task in modern
Location-Based Social Networks (LBSNs), aiming to model the complex
decision-making process of human mobility to provide personalized
recommendations for a user's next check-in location. Existing POI
recommendation models, predominantly based on Graph Neural Networks and
sequential models, have been extensively studied. However, these models face a
fundamental limitation: they struggle to simultaneously capture the inherent
hierarchical structure of spatial choices and the dynamics and irregular shifts
of user-specific temporal contexts. To overcome this limitation, we propose
GTR-Mamba, a novel framework for cross-manifold conditioning and routing.
GTR-Mamba leverages the distinct advantages of different mathematical spaces
for different tasks: it models the static, tree-like preference hierarchies in
hyperbolic geometry, while routing the dynamic sequence updates to a novel
Mamba layer in the computationally stable and efficient Euclidean tangent
space. This process is coordinated by a cross-manifold channel that fuses
spatio-temporal information to explicitly steer the State Space Model (SSM),
enabling flexible adaptation to contextual changes. Extensive experiments on
three real-world datasets demonstrate that GTR-Mamba consistently outperforms
state-of-the-art baseline models in next POI recommendation.

</details>


### [165] [Multi-Agent Conditional Diffusion Model with Mean Field Communication as Wireless Resource Allocation Planner](https://arxiv.org/abs/2510.22969)
*Kechen Meng,Sinuo Zhang,Rongpeng Li,Xiangming Meng,Chan Wang,Ming Lei,Zhifeng Zhao*

Main category: cs.AI

TL;DR: 提出MA-CDMP方法，使用扩散模型和均值场机制解决分布式无线通信资源分配中的非平稳性和合作问题


<details>
  <summary>Details</summary>
Motivation: 解决集中式MARL的可扩展性和隐私问题，以及分布式DTDE方法中的非平稳性和有限合作问题

Method: 基于模型强化学习，使用扩散模型捕捉环境动态并规划轨迹，结合逆动态模型生成动作，引入均值场机制近似大规模智能体交互

Result: 在平均奖励和QoS指标上持续优于现有MARL基线方法，展示了可扩展性和实际应用价值

Conclusion: MA-CDMP通过扩散模型和均值场机制有效解决了分布式无线通信资源管理的关键挑战，具有理论保证和实际性能优势

Abstract: In wireless communication systems, efficient and adaptive resource allocation
plays a crucial role in enhancing overall Quality of Service (QoS). While
centralized Multi-Agent Reinforcement Learning (MARL) frameworks rely on a
central coordinator for policy training and resource scheduling, they suffer
from scalability issues and privacy risks. In contrast, the Distributed
Training with Decentralized Execution (DTDE) paradigm enables distributed
learning and decision-making, but it struggles with non-stationarity and
limited inter-agent cooperation, which can severely degrade system performance.
To overcome these challenges, we propose the Multi-Agent Conditional Diffusion
Model Planner (MA-CDMP) for decentralized communication resource management.
Built upon the Model-Based Reinforcement Learning (MBRL) paradigm, MA-CDMP
employs Diffusion Models (DMs) to capture environment dynamics and plan future
trajectories, while an inverse dynamics model guides action generation, thereby
alleviating the sample inefficiency and slow convergence of conventional DTDE
methods. Moreover, to approximate large-scale agent interactions, a Mean-Field
(MF) mechanism is introduced as an assistance to the classifier in DMs. This
design mitigates inter-agent non-stationarity and enhances cooperation with
minimal communication overhead in distributed settings. We further
theoretically establish an upper bound on the distributional approximation
error introduced by the MF-based diffusion generation, guaranteeing convergence
stability and reliable modeling of multi-agent stochastic dynamics. Extensive
experiments demonstrate that MA-CDMP consistently outperforms existing MARL
baselines in terms of average reward and QoS metrics, showcasing its
scalability and practicality for real-world wireless network optimization.

</details>


### [166] [Exploring Semantic-constrained Adversarial Example with Instruction Uncertainty Reduction](https://arxiv.org/abs/2510.22981)
*Jin Hu,Jiakai Wang,Linna Jing,Haolin Li,Haodong Liu,Haotong Qin,Aishan Liu,Ke Xu,Xianglong Liu*

Main category: cs.AI

TL;DR: 提出InSUR框架，通过多维度指令不确定性减少来生成更优的语义约束对抗样本，包括采样方法、任务建模和生成器评估三个维度。


<details>
  <summary>Details</summary>
Motivation: 当前生成语义约束对抗样本的方法攻击能力不足，未能充分研究人类指令中的语义不确定性因素，如指称多样性、描述不完整性和边界模糊性。

Method: 1. 采样方法：残差驱动攻击方向稳定化，使用ResAdv-DDIM采样器稳定对抗优化；2. 任务建模：上下文编码攻击场景约束，通过引导掩码和渲染器集成补充缺失知识；3. 生成器评估：语义抽象攻击评估增强，明确评估边界。

Result: 广泛实验证明InSUR在迁移攻击性能上的优越性，并首次实现了无参考的语义约束3D对抗样本生成。

Conclusion: InSUR框架能有效解决语义不确定性带来的挑战，生成更令人满意的语义约束对抗样本，具有高迁移性、适应性和有效性。

Abstract: Recently, semantically constrained adversarial examples (SemanticAE), which
are directly generated from natural language instructions, have become a
promising avenue for future research due to their flexible attacking forms. To
generate SemanticAEs, current methods fall short of satisfactory attacking
ability as the key underlying factors of semantic uncertainty in human
instructions, such as referring diversity, descriptive incompleteness, and
boundary ambiguity, have not been fully investigated. To tackle the issues,
this paper develops a multi-dimensional instruction uncertainty reduction
(InSUR) framework to generate more satisfactory SemanticAE, i.e., transferable,
adaptive, and effective. Specifically, in the dimension of the sampling method,
we propose the residual-driven attacking direction stabilization to alleviate
the unstable adversarial optimization caused by the diversity of language
references. By coarsely predicting the language-guided sampling process, the
optimization process will be stabilized by the designed ResAdv-DDIM sampler,
therefore releasing the transferable and robust adversarial capability of
multi-step diffusion models. In task modeling, we propose the context-encoded
attacking scenario constraint to supplement the missing knowledge from
incomplete human instructions. Guidance masking and renderer integration are
proposed to regulate the constraints of 2D/3D SemanticAE, activating stronger
scenario-adapted attacks. Moreover, in the dimension of generator evaluation,
we propose the semantic-abstracted attacking evaluation enhancement by
clarifying the evaluation boundary, facilitating the development of more
effective SemanticAE generators. Extensive experiments demonstrate the
superiority of the transfer attack performance of InSUR. Moreover, we realize
the reference-free generation of semantically constrained 3D adversarial
examples for the first time.

</details>


### [167] [ProfileXAI: User-Adaptive Explainable AI](https://arxiv.org/abs/2510.22998)
*Gilber A. Corrales,Carlos Andrés Ferro Sánchez,Reinel Tabares-Soto,Jesús Alfonso López Sotelo,Gonzalo A. Ruz,Johan Sebastian Piña Durán*

Main category: cs.AI

TL;DR: ProfileXAI是一个模型和领域无关的框架，结合后置解释器（SHAP、LIME、Anchor）与检索增强的LLM，为不同类型用户生成解释。系统索引多模态知识库，通过定量标准为每个实例选择解释器，并生成基于聊天的提示式叙述。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够为不同用户类型提供高效可信解释的通用解释框架，解决现有解释方法在用户适配性方面的不足。

Method: 结合SHAP、LIME、Anchor等后置解释器与检索增强的LLM，构建多模态知识库索引，通过定量标准选择解释器，使用基于聊天的提示生成解释叙述。

Result: 在心脏病和甲状腺癌数据集上的评估显示：LIME在保真度-鲁棒性权衡上最佳，Anchor产生最稀疏、低token的规则，SHAP获得最高满意度。Profile条件化稳定token使用并保持跨配置文件的正面评分。

Conclusion: ProfileXAI框架能够实现高效可信的解释生成，不同解释器在不同指标上各有优势，Profile条件化有助于稳定解释质量。

Abstract: ProfileXAI is a model- and domain-agnostic framework that couples post-hoc
explainers (SHAP, LIME, Anchor) with retrieval - augmented LLMs to produce
explanations for different types of users. The system indexes a multimodal
knowledge base, selects an explainer per instance via quantitative criteria,
and generates grounded narratives with chat-enabled prompting. On Heart Disease
and Thyroid Cancer datasets, we evaluate fidelity, robustness, parsimony, token
use, and perceived quality. No explainer dominates: LIME achieves the best
fidelity--robustness trade-off (Infidelity $\le 0.30$, $L<0.7$ on Heart
Disease); Anchor yields the sparsest, low-token rules; SHAP attains the highest
satisfaction ($\bar{x}=4.1$). Profile conditioning stabilizes tokens ($\sigma
\le 13\%$) and maintains positive ratings across profiles ($\bar{x}\ge 3.7$,
with domain experts at $3.77$), enabling efficient and trustworthy
explanations.

</details>


### [168] [From Prompt Optimization to Multi-Dimensional Credibility Evaluation: Enhancing Trustworthiness of Chinese LLM-Generated Liver MRI Reports](https://arxiv.org/abs/2510.23008)
*Qiuli Wang,Xiaoming Li,Jie Chen,Yongxu Liu,Xingpeng Zhang,Chen Liu,Wei Chen*

Main category: cs.AI

TL;DR: 该研究提出了一个多维可信度评估框架（MDCA），用于评估和比较多个先进大语言模型在生成肝脏MRI报告时的可信度，并提供机构特定的提示优化指导。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在从影像学发现生成诊断结论方面表现出潜力，但缺乏针对不同临床环境的提示设计优化指导，以及评估LLM生成放射学报告可信度的标准化框架。

Method: 引入多维可信度评估（MDCA）框架，在SiliconFlow平台上评估和比较多个先进LLM（包括Kimi-K2、Qwen3-235B、DeepSeek-V3、ByteDance-Seed-OSS-36B）的性能，并提供机构特定的提示优化指导。

Result: 应用MDCA框架对多个LLM进行评估和比较，但具体结果未在摘要中详细说明。

Conclusion: 该研究旨在通过MDCA框架和提示优化指导，提高LLM生成肝脏MRI报告的可信度。

Abstract: Large language models (LLMs) have demonstrated promising performance in
generating diagnostic conclusions from imaging findings, thereby supporting
radiology reporting, trainee education, and quality control. However,
systematic guidance on how to optimize prompt design across different clinical
contexts remains underexplored. Moreover, a comprehensive and standardized
framework for assessing the trustworthiness of LLM-generated radiology reports
is yet to be established. This study aims to enhance the trustworthiness of
LLM-generated liver MRI reports by introducing a Multi-Dimensional Credibility
Assessment (MDCA) framework and providing guidance on institution-specific
prompt optimization. The proposed framework is applied to evaluate and compare
the performance of several advanced LLMs, including Kimi-K2-Instruct-0905,
Qwen3-235B-A22B-Instruct-2507, DeepSeek-V3, and
ByteDance-Seed-OSS-36B-Instruct, using the SiliconFlow platform.

</details>


### [169] [Mixed Density Diffuser: Efficient Planning with Non-uniform Temporal Resolution](https://arxiv.org/abs/2510.23026)
*Crimson Stambaugh,Rajesh P. N. Rao*

Main category: cs.AI

TL;DR: 提出了Mixed Density Diffuser (MDD)，一种扩散规划器，其中整个时间范围内的密度是可调超参数，在多个任务领域达到新的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 研究发现扩散规划器从稀疏步规划中受益，但过度稀疏的规划会降低性能，且时间密度阈值在整个时间范围内是非均匀的。

Method: MDD允许在整个时间范围内调整规划密度，某些轨迹部分可以更密集地规划。

Result: 在Maze2D、Franka Kitchen和Antmaze D4RL任务领域达到了新的SOTA性能。

Conclusion: 通过调整时间范围内的规划密度，MDD能够在不增加计算成本的情况下提高规划性能。

Abstract: Recent studies demonstrate that diffusion planners benefit from sparse-step
planning over single-step planning. Training models to skip steps in their
trajectories helps capture long-term dependencies without additional or memory
computational cost. However, predicting excessively sparse plans degrades
performance. We hypothesize this temporal density threshold is non-uniform
across a temporal horizon and that certain parts of a planned trajectory should
be more densely planned. We propose Mixed Density Diffuser (MDD), a diffusion
planner where the densities throughout the horizon are tunable hyperparameters.
MDD achieves a new SOTA across the Maze2D, Franka Kitchen, and Antmaze D4RL
task domains.

</details>


### [170] [A Survey of AI Scientists: Surveying the automatic Scientists and Research](https://arxiv.org/abs/2510.23045)
*Guiyao Tie,Pan Zhou,Lichao Sun*

Main category: cs.AI

TL;DR: 该论文提出了一个统一的六阶段方法论框架来系统分析AI科学家领域的发展，将端到端科学过程分解为六个阶段，并追踪了该领域从基础模块到闭环系统再到当前前沿的演进历程。


<details>
  <summary>Details</summary>
Motivation: AI正在从计算工具转变为自主科学知识创造者，但该领域的快速发展导致研究碎片化，缺乏统一的框架和方法论原则来理解这一新兴范式。

Method: 引入统一的六阶段方法论框架：文献综述、想法生成、实验准备、实验执行、科学写作和论文生成，通过这一分析视角追踪领域演进。

Result: 系统梳理了AI科学家领域从2022-2023年的基础模块阶段，到2024年的闭环系统阶段，再到2025年至今的可扩展性、影响力和人机协作前沿阶段的发展历程。

Conclusion: 该调查不仅阐明了自主科学的现状，还为克服鲁棒性和治理方面的挑战提供了关键路线图，指导下一代系统成为人类科学探究中值得信赖且不可或缺的合作伙伴。

Abstract: Artificial intelligence is undergoing a profound transition from a
computational instrument to an autonomous originator of scientific knowledge.
This emerging paradigm, the AI scientist, is architected to emulate the
complete scientific workflow-from initial hypothesis generation to the final
synthesis of publishable findings-thereby promising to fundamentally reshape
the pace and scale of discovery. However, the rapid and unstructured
proliferation of these systems has created a fragmented research landscape,
obscuring overarching methodological principles and developmental trends. This
survey provides a systematic and comprehensive synthesis of this domain by
introducing a unified, six-stage methodological framework that deconstructs the
end-to-end scientific process into: Literature Review, Idea Generation,
Experimental Preparation, Experimental Execution, Scientific Writing, and Paper
Generation. Through this analytical lens, we chart the field's evolution from
early Foundational Modules (2022-2023) to integrated Closed-Loop Systems
(2024), and finally to the current frontier of Scalability, Impact, and
Human-AI Collaboration (2025-present). By rigorously synthesizing these
developments, this survey not only clarifies the current state of autonomous
science but also provides a critical roadmap for overcoming remaining
challenges in robustness and governance, ultimately guiding the next generation
of systems toward becoming trustworthy and indispensable partners in human
scientific inquiry.

</details>


### [171] [TLCD: A Deep Transfer Learning Framework for Cross-Disciplinary Cognitive Diagnosis](https://arxiv.org/abs/2510.23062)
*Zhifeng Wang,Meixin Su,Yang Yang,Chunyan Zeng,Lizhi Ye*

Main category: cs.AI

TL;DR: 提出了一种基于深度学习和迁移学习的跨学科认知诊断方法(TLCD)，通过利用主学科的共同特征来提升目标学科模型的性能，能够更准确地评估学生的学习情况。


<details>
  <summary>Details</summary>
Motivation: 在线教育模式下，传统认知诊断方法在跨学科领域面临知识体系差异、认知结构不同和数据特征不一致等挑战，需要解决特征提取复杂性和学科数据稀缺性问题。

Method: 结合深度学习技术和迁移学习策略，研究神经网络认知诊断和知识关联神经网络认知诊断，提出跨学科认知诊断方法(TLCD)。

Result: 实验结果表明，基于深度学习的跨学科认知诊断模型在跨学科认知诊断任务中表现优于基础模型。

Conclusion: 该方法能够更准确地评估学生的学习情况，为跨学科教育评价提供了有效的解决方案。

Abstract: Driven by the dual principles of smart education and artificial intelligence
technology, the online education model has rapidly emerged as an important
component of the education industry. Cognitive diagnostic technology can
utilize students' learning data and feedback information in educational
evaluation to accurately assess their ability level at the knowledge level.
However, while massive amounts of information provide abundant data resources,
they also bring about complexity in feature extraction and scarcity of
disciplinary data. In cross-disciplinary fields, traditional cognitive
diagnostic methods still face many challenges. Given the differences in
knowledge systems, cognitive structures, and data characteristics between
different disciplines, this paper conducts in-depth research on neural network
cognitive diagnosis and knowledge association neural network cognitive
diagnosis, and proposes an innovative cross-disciplinary cognitive diagnosis
method (TLCD). This method combines deep learning techniques and transfer
learning strategies to enhance the performance of the model in the target
discipline by utilizing the common features of the main discipline. The
experimental results show that the cross-disciplinary cognitive diagnosis model
based on deep learning performs better than the basic model in
cross-disciplinary cognitive diagnosis tasks, and can more accurately evaluate
students' learning situation.

</details>


### [172] [Smaller Models, Smarter Rewards: A Two-Sided Approach to Process and Outcome Rewards](https://arxiv.org/abs/2510.23083)
*Jan Niklas Groeneveld,Xi Qin,Alexander Schaefer,Yaad Oren*

Main category: cs.AI

TL;DR: 该论文研究了如何将小型语言模型（如Phi-4系列）转化为有效的奖励模型，用于评估代码生成质量，结合过程奖励和结果奖励，在APPS编程挑战基准上实现了超过20%的代码搜索能力提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成高质量代码方面仍面临挑战，需要奖励模型作为推理模型发展的中间步骤。虽然模型规模通常与反思能力相关，但作者希望探索最先进的小型语言模型是否能转化为实用的奖励模型。

Method: 构建基于APPS编程挑战基准的代码样本数据集，训练带有回归层的价值头模型来估计中间输出的成功概率，将解码器专用transformer模型转化为奖励模型。

Result: 小型LLM能够作为有效的奖励模型或代码评估评判器，成功识别多个候选方案中的正确解决方案，使用该评判器在多个生成结果中搜索最准确代码的能力提升了20%以上。

Conclusion: 小型语言模型可以被有效地转化为实用的奖励模型，结合过程奖励和结果奖励的考虑，显著提升了代码生成质量评估和最优代码搜索的能力。

Abstract: Generating high-quality code remains a challenge for Large Language Models
(LLMs). For the evolution of reasoning models on this task, reward models are a
necessary intermediate step. These models judge outcomes or intermediate steps.
Decoder-only transformer models can be turned into reward models by introducing
a regression layer and supervised fine-tuning. While it is known that
reflection capabilities generally increase with the size of a model, we want to
investigate whether state-of-the-art small language models like the Phi-4
family can be turned into usable reward models blending the consideration of
process rewards and outcome rewards.
  Targeting this goal, we construct a dataset of code samples with correctness
labels derived from the APPS coding challenge benchmark. We then train a
value-head model to estimate the success probability of intermediate outputs.
Our evaluation shows that small LLMs are capable of serving as effective reward
models or code evaluation critics, successfully identifying correct solutions
among multiple candidates. Using this critic, we achieve over a 20% improvement
in the search capability of the most accurate code out of multiple generations.

</details>


### [173] [Lost in Tokenization: Context as the Key to Unlocking Biomolecular Understanding in Scientific LLMs](https://arxiv.org/abs/2510.23127)
*Kai Zhuang,Jiawei Zhang,Yumou Liu,Hanqun Cao,Chunbin Gu,Mengdi Liu,Zhangyang Gao,Zitong Jerry Wang,Xuanhe Zhou,Pheng-Ann Heng,Lijun Wu,Conghui He,Cheng Tan*

Main category: cs.AI

TL;DR: Sci-LLMs在生物序列处理中面临tokenization困境，研究发现提供高层次结构化上下文比直接处理原始序列效果更好，甚至原始序列会干扰模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决科学大语言模型在处理原始生物分子序列时面临的tokenization困境，无论是将序列视为专门语言还是单独模态都存在信息丢失或对齐挑战的问题。

Method: 通过系统比较领先的Sci-LLMs在生物推理任务上的表现，测试了三种输入模式：仅序列、仅上下文、以及两者结合。

Result: 仅上下文方法始终且显著优于其他所有模式。更令人惊讶的是，在高级上下文旁边包含原始序列会持续降低性能，表明原始序列即使对于具有专门tokenization方案的模型也充当信息噪声。

Conclusion: 现有Sci-LLMs的主要优势不在于从零开始解释生物分子语法的能力，而在于对结构化、人类可读知识的深刻推理能力。应重新构建Sci-LLMs，将其定位为基于专家知识的强大推理引擎。

Abstract: Scientific Large Language Models (Sci-LLMs) have emerged as a promising
frontier for accelerating biological discovery. However, these models face a
fundamental challenge when processing raw biomolecular sequences: the
tokenization dilemma. Whether treating sequences as a specialized language,
risking the loss of functional motif information, or as a separate modality,
introducing formidable alignment challenges, current strategies fundamentally
limit their reasoning capacity. We challenge this sequence-centric paradigm by
positing that a more effective strategy is to provide Sci-LLMs with high-level
structured context derived from established bioinformatics tools, thereby
bypassing the need to interpret low-level noisy sequence data directly. Through
a systematic comparison of leading Sci-LLMs on biological reasoning tasks, we
tested three input modes: sequence-only, context-only, and a combination of
both. Our findings are striking: the context-only approach consistently and
substantially outperforms all other modes. Even more revealing, the inclusion
of the raw sequence alongside its high-level context consistently degrades
performance, indicating that raw sequences act as informational noise, even for
models with specialized tokenization schemes. These results suggest that the
primary strength of existing Sci-LLMs lies not in their nascent ability to
interpret biomolecular syntax from scratch, but in their profound capacity for
reasoning over structured, human-readable knowledge. Therefore, we argue for
reframing Sci-LLMs not as sequence decoders, but as powerful reasoning engines
over expert knowledge. This work lays the foundation for a new class of hybrid
scientific AI agents, repositioning the developmental focus from direct
sequence interpretation towards high-level knowledge synthesis. The code is
available at github.com/opendatalab-raise-dev/CoKE.

</details>


### [174] [Guiding Skill Discovery with Foundation Models](https://arxiv.org/abs/2510.23167)
*Zhao Yang,Thomas M. Moerland,Mike Preuss,Aske Plaat,Vincent François-Lavet,Edward S. Hu*

Main category: cs.AI

TL;DR: 提出了FoG技能发现方法，通过基础模型将人类意图融入技能发现过程，消除不良行为并避免危险区域


<details>
  <summary>Details</summary>
Motivation: 现有技能发现方法只关注技能多样性最大化，不考虑人类偏好，导致不良行为和危险技能，需要将人类意图融入技能发现

Method: 从基础模型提取评分函数评估状态，基于人类意图为期望状态分配高值、不良状态分配低值，然后用这些分数重新加权技能发现算法的奖励

Result: FoG成功消除了翻转、滚动等不良行为，在状态和像素任务中都避免了危险区域，还能发现难以定义的行为技能

Conclusion: FoG方法有效将人类意图融入技能发现，解决了现有方法产生不良行为的问题，提高了技能学习的实用性

Abstract: Learning diverse skills without hand-crafted reward functions could
accelerate reinforcement learning in downstream tasks. However, existing skill
discovery methods focus solely on maximizing the diversity of skills without
considering human preferences, which leads to undesirable behaviors and
possibly dangerous skills. For instance, a cheetah robot trained using previous
methods learns to roll in all directions to maximize skill diversity, whereas
we would prefer it to run without flipping or entering hazardous areas. In this
work, we propose a Foundation model Guided (FoG) skill discovery method, which
incorporates human intentions into skill discovery through foundation models.
Specifically, FoG extracts a score function from foundation models to evaluate
states based on human intentions, assigning higher values to desirable states
and lower to undesirable ones. These scores are then used to re-weight the
rewards of skill discovery algorithms. By optimizing the re-weighted skill
discovery rewards, FoG successfully learns to eliminate undesirable behaviors,
such as flipping or rolling, and to avoid hazardous areas in both state-based
and pixel-based tasks. Interestingly, we show that FoG can discover skills
involving behaviors that are difficult to define. Interactive visualisations
are available from https://sites.google.com/view/submission-fog.

</details>


### [175] [AUPO -- Abstracted Until Proven Otherwise: A Reward Distribution Based Abstraction Algorithm](https://arxiv.org/abs/2510.23214)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: AUPO是MCTS决策策略的改进方法，通过自动动作抽象提升性能，不依赖转移概率或DAG搜索图，能检测对称动作。


<details>
  <summary>Details</summary>
Motivation: 改进MCTS决策策略，解决现有自动抽象算法需要转移概率和DAG搜索图的限制，提升对称动作检测能力。

Method: 基于MCTS过程中获得的奖励分布统计，开发自动动作抽象算法AUPO，仅影响决策策略。

Result: 在IPPC基准问题上，AUPO明显优于标准MCTS，能检测到ASAP等先进框架难以处理的对称动作。

Conclusion: AUPO作为MCTS的改进方法有效提升性能，且与其他仅影响树搜索的抽象技术兼容。

Abstract: We introduce a novel, drop-in modification to Monte Carlo Tree Search's
(MCTS) decision policy that we call AUPO. Comparisons based on a range of IPPC
benchmark problems show that AUPO clearly outperforms MCTS. AUPO is an
automatic action abstraction algorithm that solely relies on reward
distribution statistics acquired during the MCTS. Thus, unlike other automatic
abstraction algorithms, AUPO requires neither access to transition
probabilities nor does AUPO require a directed acyclic search graph to build
its abstraction, allowing AUPO to detect symmetric actions that
state-of-the-art frameworks like ASAP struggle with when the resulting
symmetric states are far apart in state space. Furthermore, as AUPO only
affects the decision policy, it is not mutually exclusive with other
abstraction techniques that only affect the tree search.

</details>


### [176] [Human-Like Goalkeeping in a Realistic Football Simulation: a Sample-Efficient Reinforcement Learning Approach](https://arxiv.org/abs/2510.23216)
*Alessandro Sestini,Joakim Bergdahl,Jean-Philippe Barrette-LaPierre,Florian Fuchs,Brady Chen,Micheal Jones,Linus Gisslén*

Main category: cs.AI

TL;DR: 提出了一种针对游戏产业的高效深度强化学习方法，在EA SPORTS FC 25中训练守门员智能体，比内置AI表现提升10%，训练速度提高50%，并产生更拟人的游戏体验。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习研究主要关注训练超人智能体，但游戏产业需要的是拟人化智能体，且资源有限，因此需要开发适合工业环境的高效训练方法。

Method: 通过利用预收集数据和增加网络可塑性，改进基于价值的深度强化学习的样本效率，专门为游戏产业等工业环境设计训练和微调方法。

Result: 在EA SPORTS FC 25中训练的守门员智能体比游戏内置AI的救球率提高10%，训练速度比标准深度强化学习方法快50%，领域专家定性评估显示产生更拟人的游戏体验。

Conclusion: 该方法成功证明了在工业环境中应用深度强化学习的可行性，其效果得到认可，计划在下一代游戏系列中取代手工制作的智能体。

Abstract: While several high profile video games have served as testbeds for Deep
Reinforcement Learning (DRL), this technique has rarely been employed by the
game industry for crafting authentic AI behaviors. Previous research focuses on
training super-human agents with large models, which is impractical for game
studios with limited resources aiming for human-like agents. This paper
proposes a sample-efficient DRL method tailored for training and fine-tuning
agents in industrial settings such as the video game industry. Our method
improves sample efficiency of value-based DRL by leveraging pre-collected data
and increasing network plasticity. We evaluate our method training a goalkeeper
agent in EA SPORTS FC 25, one of the best-selling football simulations today.
Our agent outperforms the game's built-in AI by 10% in ball saving rate.
Ablation studies show that our method trains agents 50% faster compared to
standard DRL methods. Finally, qualitative evaluation from domain experts
indicates that our approach creates more human-like gameplay compared to
hand-crafted agents. As a testimony of the impact of the approach, the method
is intended to replace the hand-crafted counterpart in next iterations of the
series.

</details>


### [177] [Accelerating IC Thermal Simulation Data Generation via Block Krylov and Operator Action](https://arxiv.org/abs/2510.23221)
*Hong Wang,Wenkai Yang,Jie Wang,Huanshuo Dong,Zijie Geng,Zhen Huang,Depeng Xie,Zhezheng Hao,Hande Dong*

Main category: cs.AI

TL;DR: 提出了一种名为BlocKOA的新算法，用于快速生成集成电路热仿真数据，相比现有方法时间复杂低一个数量级，实现了420倍加速，仅用4%时间生成的数据就能达到可比性能。


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动方法（如神经算子）需要大量高保真训练数据，导致计算成本高昂，限制了集成电路热仿真的效率。

Method: 使用块Krylov算法基于热方程结构快速获取基础解，组合生成满足物理约束的温度分布，然后应用热算子确定热源分布，高效生成精确数据点。

Result: BlocKOA在生成5000个不同物理参数和IC结构芯片的热仿真数据时实现了420倍加速，仅用4%时间生成的数据就能让数据驱动方法达到与现有方法相当的性能。

Conclusion: BlocKOA算法能显著加速集成电路热仿真数据的生成过程，同时保证数据精度，为数据驱动方法提供了高效的数据生成解决方案。

Abstract: Recent advances in data-driven approaches, such as neural operators (NOs),
have shown substantial efficacy in reducing the solution time for integrated
circuit (IC) thermal simulations. However, a limitation of these approaches is
requiring a large amount of high-fidelity training data, such as chip
parameters and temperature distributions, thereby incurring significant
computational costs. To address this challenge, we propose a novel algorithm
for the generation of IC thermal simulation data, named block Krylov and
operator action (BlocKOA), which simultaneously accelerates the data generation
process and enhances the precision of generated data. BlocKOA is specifically
designed for IC applications. Initially, we use the block Krylov algorithm
based on the structure of the heat equation to quickly obtain a few basic
solutions. Then we combine them to get numerous temperature distributions that
satisfy the physical constraints. Finally, we apply heat operators on these
functions to determine the heat source distributions, efficiently generating
precise data points. Theoretical analysis shows that the time complexity of
BlocKOA is one order lower than the existing method. Experimental results
further validate its efficiency, showing that BlocKOA achieves a 420-fold
speedup in generating thermal simulation data for 5000 chips with varying
physical parameters and IC structures. Even with just 4% of the generation
time, data-driven approaches trained on the data generated by BlocKOA exhibits
comparable performance to that using the existing method.

</details>


### [178] [CNOT Minimal Circuit Synthesis: A Reinforcement Learning Approach](https://arxiv.org/abs/2510.23304)
*Riccardo Romanello,Daniele Lizzio Bosco,Jacopo Cossio,Dusan Sutulovic,Giuseppe Serra,Carla Piazza,Paolo Burelli*

Main category: cs.AI

TL;DR: 提出一种基于强化学习的CNOT门最小化方法，使用单一智能体处理不同尺寸的量子电路，在较大电路尺寸上优于现有最优算法。


<details>
  <summary>Details</summary>
Motivation: CNOT门是量子计算中产生纠缠的关键组件，最小化CNOT门数量对优化量子电路性能至关重要，但该问题的计算复杂性尚未完全确定。

Method: 使用单一强化学习智能体处理固定尺寸m的电路，通过嵌入或高斯条纹化预处理不同尺寸的矩阵，训练m=8的智能体并评估n=3到15的电路。

Result: 该方法在电路尺寸n增大时，性能优于现有最优算法。

Conclusion: 强化学习方法在CNOT门最小化问题上展现出潜力，特别是在处理较大尺寸量子电路时具有优势。

Abstract: CNOT gates are fundamental to quantum computing, as they facilitate
entanglement, a crucial resource for quantum algorithms. Certain classes of
quantum circuits are constructed exclusively from CNOT gates. Given their
widespread use, it is imperative to minimise the number of CNOT gates employed.
This problem, known as CNOT minimisation, remains an open challenge, with its
computational complexity yet to be fully characterised. In this work, we
introduce a novel reinforcement learning approach to address this task. Instead
of training multiple reinforcement learning agents for different circuit sizes,
we use a single agent up to a fixed size $m$. Matrices of sizes different from
m are preprocessed using either embedding or Gaussian striping. To assess the
efficacy of our approach, we trained an agent with m = 8, and evaluated it on
matrices of size n that range from 3 to 15. The results we obtained show that
our method overperforms the state-of-the-art algorithm as the value of n
increases.

</details>


### [179] [Planning Ahead with RSA: Efficient Signalling in Dynamic Environments by Projecting User Awareness across Future Timesteps](https://arxiv.org/abs/2510.23340)
*Anwesha Das,John Duff,Jörg Hoffmann,Vera Demberg*

Main category: cs.AI

TL;DR: 提出了一种自适应信号框架，通过理性沟通原则优化人机协作中的信息传递时机和内容，确保用户在动态环境中保持准确认知。


<details>
  <summary>Details</summary>
Motivation: 在快速变化的环境中，辅助AI需要识别关键信息并估计最佳沟通时机，因为人类注意力是零和认知资源，关注一个信息会降低对其他信息的感知。

Method: 使用贝叶斯参考解析和理性言语行为(RSA)建模框架，规划消息序列以优化用户信念与动态环境的及时对齐，调整消息特异性和时机基于对用户注意力影响的预测。

Result: 与基线方法相比，该方法的效果关键依赖于将多步规划与真实用户意识模型相结合。

Conclusion: 这是RSA在动态环境通信和人类-AI交互中的首次应用，为人类-智能体团队中的语用通信建立了理论基础，展示了认知科学见解如何指导辅助智能体设计。

Abstract: Adaptive agent design offers a way to improve human-AI collaboration on
time-sensitive tasks in rapidly changing environments. In such cases, to ensure
the human maintains an accurate understanding of critical task elements, an
assistive agent must not only identify the highest priority information but
also estimate how and when this information can be communicated most
effectively, given that human attention represents a zero-sum cognitive
resource where focus on one message diminishes awareness of other or upcoming
information. We introduce a theoretical framework for adaptive signalling which
meets these challenges by using principles of rational communication,
formalised as Bayesian reference resolution using the Rational Speech Act (RSA)
modelling framework, to plan a sequence of messages which optimise timely
alignment between user belief and a dynamic environment. The agent adapts
message specificity and timing to the particulars of a user and scenario based
on projections of how prior-guided interpretation of messages will influence
attention to the interface and subsequent belief update, across several
timesteps out to a fixed horizon. In a comparison to baseline methods, we show
that this effectiveness depends crucially on combining multi-step planning with
a realistic model of user awareness. As the first application of RSA for
communication in a dynamic environment, and for human-AI interaction in
general, we establish theoretical foundations for pragmatic communication in
human-agent teams, highlighting how insights from cognitive science can be
capitalised to inform the design of assistive agents.

</details>


### [180] [Opinion Mining Based Entity Ranking using Fuzzy Logic Algorithmic Approach](https://arxiv.org/abs/2510.23384)
*Pratik N. Kalamkar,A. G. Phakatkar*

Main category: cs.AI

TL;DR: 提出了一种基于模糊逻辑推理的细粒度意见挖掘方法，用于从语句中提取更深层次的属性信息，并基于此对实体进行排名。


<details>
  <summary>Details</summary>
Motivation: 由于社交网络和电子商务网站的发展，网络上存在大量意见数据。现有研究主要集中在基于评论集对实体进行排名，但从未在更细粒度级别对意见进行分类后再进行实体排名。

Method: 使用模糊逻辑推理进行细粒度意见挖掘，从包含意见的评估语句中提取实体的属性和组件，并确定评论是正面、负面还是中性。

Result: 开发了一种能够在更深层次粒度上进行意见挖掘的方法，并基于提取的信息对实体进行排名。

Conclusion: 该方法填补了在细粒度级别对意见进行分类后再进行实体排名的研究空白，为意见挖掘提供了新的技术途径。

Abstract: Opinions are central to almost all human activities and are key influencers
of our behaviors. In current times due to growth of social networking website
and increase in number of e-commerce site huge amount of opinions are now
available on web. Given a set of evaluative statements that contain opinions
(or sentiments) about an Entity, opinion mining aims to extract attributes and
components of the object that have been commented on in each statement and to
determine whether the comments are positive, negative or neutral. While lot of
research recently has been done in field of opinion mining and some of it
dealing with ranking of entities based on review or opinion set, classifying
opinions into finer granularity level and then ranking entities has never been
done before. In this paper method for opinion mining from statements at a
deeper level of granularity is proposed. This is done by using fuzzy logic
reasoning, after which entities are ranked as per this information.

</details>


### [181] [AutoStreamPipe: LLM Assisted Automatic Generation of Data Stream Processing Pipelines](https://arxiv.org/abs/2510.23408)
*Abolfazl Younesi,Zahra Najafabadi Samani,Thomas Fahringer*

Main category: cs.AI

TL;DR: AutoStreamPipe是一个使用大语言模型自动设计和部署流处理管道的框架，通过超图思维方法显著减少开发时间和错误率。


<details>
  <summary>Details</summary>
Motivation: 传统流处理管道开发需要大量手动工作，存在语义鸿沟问题，需要自动化解决方案来连接用户意图与平台特定实现。

Method: 集成超图思维作为GoT的扩展版本，结合弹性执行策略和高级查询分析，使用LLM自动化管道设计、生成和部署。

Result: 实验评估显示，相比LLM代码生成方法，AutoStreamPipe将开发时间减少6.3倍，错误率降低5.19倍（通过新的无错误评分EFS衡量）。

Conclusion: AutoStreamPipe框架有效解决了流处理管道开发的自动化问题，显著提升了开发效率和准确性。

Abstract: Data pipelines are essential in stream processing as they enable the
efficient collection, processing, and delivery of real-time data, supporting
rapid data analysis. In this paper, we present AutoStreamPipe, a novel
framework that employs Large Language Models (LLMs) to automate the design,
generation, and deployment of stream processing pipelines. AutoStreamPipe
bridges the semantic gap between high-level user intent and platform-specific
implementations across distributed stream processing systems for structured
multi-agent reasoning by integrating a Hypergraph of Thoughts (HGoT) as an
extended version of GoT. AutoStreamPipe combines resilient execution
strategies, advanced query analysis, and HGoT to deliver pipelines with good
accuracy. Experimental evaluations on diverse pipelines demonstrate that
AutoStreamPipe significantly reduces development time (x6.3) and error rates
(x5.19), as measured by a novel Error-Free Score (EFS), compared to LLM
code-generation methods.

</details>


### [182] [Bid2X: Revealing Dynamics of Bidding Environment in Online Advertising from A Foundation Model Lens](https://arxiv.org/abs/2510.23410)
*Jiahao Ji,Tianyu Wang,Yeshu Li,Yushen Huo,Zhilin Zhang,Chuan Yu,Jian Xu,Bo Zheng*

Main category: cs.AI

TL;DR: 提出了Bid2X竞价基础模型，通过统一函数学习不同场景下的竞价效果预测，采用注意力机制处理变量和时间依赖关系，在淘宝平台部署后显著提升了GMV和ROI。


<details>
  <summary>Details</summary>
Motivation: 现有竞价模型在跨环境泛化性方面存在局限，因为它们通常针对特定竞价场景定制。需要一种能够学习场景无关原则的统一方法。

Method: 构建Bid2X基础模型，使用统一序列嵌入编码异构数据，提出两种注意力机制分别处理变量间和时间动态依赖关系，采用变量感知融合模块进行自适应预测，设计零膨胀投影模块处理独特的数据分布。

Result: 在8个数据集上的离线评估显示Bid2X优于各种基线方法且具有跨场景泛化能力。在线A/B测试中GMV提升4.65%，ROI提升2.44%。

Conclusion: Bid2X为计算广告中的竞价基础模型开辟了新途径，证明了其在真实电商平台中的有效性和泛化能力。

Abstract: Auto-bidding is crucial in facilitating online advertising by automatically
providing bids for advertisers. While previous work has made great efforts to
model bidding environments for better ad performance, it has limitations in
generalizability across environments since these models are typically tailored
for specific bidding scenarios. To this end, we approach the
scenario-independent principles through a unified function that estimates the
achieved effect under specific bids, such as budget consumption, gross
merchandise volume (GMV), page views, etc. Then, we propose a bidding
foundation model Bid2X to learn this fundamental function from data in various
scenarios. Our Bid2X is built over uniform series embeddings that encode
heterogeneous data through tailored embedding methods. To capture complex
inter-variable and dynamic temporal dependencies in bidding data, we propose
two attention mechanisms separately treating embeddings of different variables
and embeddings at different times as attention tokens for representation
learning. On top of the learned variable and temporal representations, a
variable-aware fusion module is used to perform adaptive bidding outcome
prediction. To model the unique bidding data distribution, we devise a
zero-inflated projection module to incorporate the estimated non-zero
probability into its value prediction, which makes up a joint optimization
objective containing classification and regression. The objective is proven to
converge to the zero-inflated distribution. Our model has been deployed on the
ad platform in Taobao, one of the world's largest e-commerce platforms. Offline
evaluation on eight datasets exhibits Bid2X's superiority compared to various
baselines and its generality across different scenarios. Bid2X increased GMV by
4.65% and ROI by 2.44% in online A/B tests, paving the way for bidding
foundation model in computational advertising.

</details>


### [183] [Causal Deep Q Network](https://arxiv.org/abs/2510.23424)
*Elouanes Khelifi,Amir Saki,Usef Faghihi*

Main category: cs.AI

TL;DR: 提出了一种将因果推理整合到DQN中的新方法，使用PEACE公式估计因果效应，以减轻虚假相关性的影响，提高问题解决能力。


<details>
  <summary>Details</summary>
Motivation: 传统的DQN依赖关联学习，容易获得虚假相关性，限制了其问题解决能力。需要整合因果推理来理解环境的因果结构。

Method: 将因果原则整合到DQN中，利用PEACE（概率简单变分因果效应）公式估计因果效应，在训练过程中加入因果推理。

Result: 在标准基准环境上的实验结果表明，该方法优于传统DQN，展示了因果推理在强化学习中的有效性。

Conclusion: 这项工作为通过原则性因果推理推进深度强化学习智能体的能力提供了一个有前景的途径。

Abstract: Deep Q Networks (DQN) have shown remarkable success in various reinforcement
learning tasks. However, their reliance on associative learning often leads to
the acquisition of spurious correlations, hindering their problem-solving
capabilities. In this paper, we introduce a novel approach to integrate causal
principles into DQNs, leveraging the PEACE (Probabilistic Easy vAriational
Causal Effect) formula for estimating causal effects. By incorporating causal
reasoning during training, our proposed framework enhances the DQN's
understanding of the underlying causal structure of the environment, thereby
mitigating the influence of confounding factors and spurious correlations. We
demonstrate that integrating DQNs with causal capabilities significantly
enhances their problem-solving capabilities without compromising performance.
Experimental results on standard benchmark environments showcase that our
approach outperforms conventional DQNs, highlighting the effectiveness of
causal reasoning in reinforcement learning. Overall, our work presents a
promising avenue for advancing the capabilities of deep reinforcement learning
agents through principled causal inference.

</details>


### [184] [A Neuro-Symbolic Multi-Agent Approach to Legal-Cybersecurity Knowledge Integration](https://arxiv.org/abs/2510.23443)
*Chiara Bonfanti,Alessandro Druetto,Cataldo Basile,Tharindu Ranasinghe,Marcos Zampieri*

Main category: cs.AI

TL;DR: 该论文旨在解决网络安全与法律交叉领域的信息处理难题，通过开发智能系统来弥合法律专家与网络安全专业人士之间的知识鸿沟。


<details>
  <summary>Details</summary>
Motivation: 网络安全与法律的交叉领域形成了复杂的信息空间，传统法律研究工具难以处理案例、法规和技术漏洞之间的微妙联系，阻碍了法律专家与网络安全专业人士的合作。

Method: 开发能够导航日益复杂的网络法律领域的智能系统，作为解决这一重要差距的第一步。

Result: 在多语言任务上展示了有前景的初步结果。

Conclusion: 这项工作为解决网络安全与法律交叉领域的信息处理挑战提供了初步解决方案，为未来更智能的系统奠定了基础。

Abstract: The growing intersection of cybersecurity and law creates a complex
information space where traditional legal research tools struggle to deal with
nuanced connections between cases, statutes, and technical vulnerabilities.
This knowledge divide hinders collaboration between legal experts and
cybersecurity professionals. To address this important gap, this work provides
a first step towards intelligent systems capable of navigating the increasingly
intricate cyber-legal domain. We demonstrate promising initial results on
multilingual tasks.

</details>


### [185] [What are the odds? Risk and uncertainty about AI existential risk](https://arxiv.org/abs/2510.23453)
*Marco Grossi*

Main category: cs.AI

TL;DR: 本文是对Cappelen等人关于AI生存风险分类分析论文的评论，重点讨论了线性风险模型的哲学局限性，分析了瑞士奶酪模型与作者模型的差异，并论证了在认知漠视情境下灾难概率P(D)可能高于预期。


<details>
  <summary>Details</summary>
Motivation: 揭示线性风险模型在分析AI生存风险时的哲学局限性，强调需要考虑结构关系和不确定性维度来更准确评估AI灾难风险。

Method: 通过比较标准瑞士奶酪模型与作者模型的差异，分析结构层间关系对概率的影响，区分风险与不确定性，并引入选项不确定性和状态空间不确定性两个维度。

Result: 论证了在认知漠视情境下，由于结构层间关系，灾难概率P(D)可能高于初步估计；识别出影响P(D)估计的两种不确定性类型。

Conclusion: 将选项不确定性和状态空间不确定性纳入AI生存风险的定性讨论中，能够提供对灾难概率P(D)更准确的理解。

Abstract: This work is a commentary of the article
\href{https://doi.org/10.18716/ojs/phai/2025.2801}{AI Survival Stories: a
Taxonomic Analysis of AI Existential Risk} by Cappelen, Goldstein, and
Hawthorne. It is not just a commentary though, but a useful reminder of the
philosophical limitations of \say{linear} models of risk. The article will
focus on the model employed by the authors: first, I discuss some differences
between standard Swiss Cheese models and this one. I then argue that in a
situation of epistemic indifference the probability of P(D) is higher than what
one might first suggest, given the structural relationships between layers. I
then distinguish between risk and uncertainty, and argue that any estimation of
P(D) is structurally affected by two kinds of uncertainty: option uncertainty
and state-space uncertainty. Incorporating these dimensions of uncertainty into
our qualitative discussion on AI existential risk can provide a better
understanding of the likeliness of P(D).

</details>


### [186] [Policy-Aware Generative AI for Safe, Auditable Data Access Governance](https://arxiv.org/abs/2510.23474)
*Shames Al Mandalawi,Muzakkiruddin Ahmed Mohammed,Hendrika Maclean,Mert Can Cakmak,John R. Talburt*

Main category: cs.AI

TL;DR: 提出了一种基于LLM的策略感知控制器，通过六阶段推理框架将自然语言请求与书面策略进行匹配，实现安全、合规且可追溯的访问决策。


<details>
  <summary>Details</summary>
Motivation: 企业需要满足最小权限、符合法规要求且可审计的访问决策，但传统方法难以处理自然语言请求与复杂策略的匹配。

Method: 使用Google Gemini 2.0 Flash实现六阶段推理框架：上下文解释、用户验证、数据分类、业务目的测试、合规映射和风险综合，采用早期硬策略门和默认拒绝机制。

Result: 在14个标准案例评估中，精确决策匹配率从10/14提升到13/14(92.9%)，拒绝召回率达到1.00，必须拒绝场景的误批准率降为0，功能适当性和合规性均为14/14。

Conclusion: 策略约束的LLM推理结合显式门控和审计追踪，能够将人类可读策略转化为安全、合规且可追溯的机器决策。

Abstract: Enterprises need access decisions that satisfy least privilege, comply with
regulations, and remain auditable. We present a policy aware controller that
uses a large language model (LLM) to interpret natural language requests
against written policies and metadata, not raw data. The system, implemented
with Google Gemini~2.0 Flash, executes a six-stage reasoning framework (context
interpretation, user validation, data classification, business purpose test,
compliance mapping, and risk synthesis) with early hard policy gates and deny
by default. It returns APPROVE, DENY, CONDITIONAL together with cited controls
and a machine readable rationale. We evaluate on fourteen canonical cases
across seven scenario families using a privacy preserving benchmark. Results
show Exact Decision Match improving from 10/14 to 13/14 (92.9\%) after applying
policy gates, DENY recall rising to 1.00, False Approval Rate on must-deny
families dropping to 0, and Functional Appropriateness and Compliance Adherence
at 14/14. Expert ratings of rationale quality are high, and median latency is
under one minute. These findings indicate that policy constrained LLM
reasoning, combined with explicit gates and audit trails, can translate human
readable policies into safe, compliant, and traceable machine decisions.

</details>


### [187] [Human-AI Collaborative Uncertainty Quantification](https://arxiv.org/abs/2510.23476)
*Sima Noorani,Shayan Kiyani,George Pappas,Hamed Hassani*

Main category: cs.AI

TL;DR: 提出了人类-AI协作不确定性量化框架，通过AI模型优化人类专家提出的预测集，避免反事实伤害并实现互补性，在多种任务中优于单独使用人类或AI。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在高风险决策中缺乏领域知识、长时域上下文和物理世界推理能力，需要结合人类与AI的互补优势来提升不确定性下的决策可靠性。

Method: 引入人类-AI协作不确定性量化框架，基于单一评分函数的两阈值结构构建最优协作预测集，开发具有分布无关有限样本保证的离线和在线校准算法。

Result: 在图像分类、回归和基于文本的医疗决策等任务中，协作预测集始终优于单独使用人类或AI，实现了更高的覆盖率和更小的集合大小。

Conclusion: 人类-AI协作不确定性量化框架有效结合了人类与AI的互补优势，在不确定性决策中实现了更可靠的性能，并能适应分布漂移和人类行为演化。

Abstract: AI predictive systems are increasingly embedded in decision making pipelines,
shaping high stakes choices once made solely by humans. Yet robust decisions
under uncertainty still rely on capabilities that current AI lacks: domain
knowledge not captured by data, long horizon context, and reasoning grounded in
the physical world. This gap has motivated growing efforts to design
collaborative frameworks that combine the complementary strengths of humans and
AI. This work advances this vision by identifying the fundamental principles of
Human AI collaboration within uncertainty quantification, a key component of
reliable decision making. We introduce Human AI Collaborative Uncertainty
Quantification, a framework that formalizes how an AI model can refine a human
expert's proposed prediction set with two goals: avoiding counterfactual harm,
ensuring the AI does not degrade correct human judgments, and complementarity,
enabling recovery of correct outcomes the human missed. At the population
level, we show that the optimal collaborative prediction set follows an
intuitive two threshold structure over a single score function, extending a
classical result in conformal prediction. Building on this insight, we develop
practical offline and online calibration algorithms with provable distribution
free finite sample guarantees. The online method adapts to distribution shifts,
including human behavior evolving through interaction with AI, a phenomenon we
call Human to AI Adaptation. Experiments across image classification,
regression, and text based medical decision making show that collaborative
prediction sets consistently outperform either agent alone, achieving higher
coverage and smaller set sizes across various conditions.

</details>


### [188] [Are Agents Just Automata? On the Formal Equivalence Between Agentic AI and the Chomsky Hierarchy](https://arxiv.org/abs/2510.23487)
*Roham Koohestani,Ziyou Li,Anton Podkopaev,Maliheh Izadi*

Main category: cs.AI

TL;DR: 本文建立了现代智能体AI系统与乔姆斯基层级抽象机器之间的形式等价关系，提出AI智能体的记忆架构决定其计算能力，并直接映射到相应的自动机类别。


<details>
  <summary>Details</summary>
Motivation: 为智能体AI系统提供理论基础，实现形式化验证，应用成熟的自动机理论来保证智能体的安全性和可预测性。

Method: 通过分析智能体的记忆架构，将其映射到乔姆斯基层级中的相应自动机类别：简单反射智能体对应有限自动机，分层任务分解智能体对应下推自动机，具有读写内存的反思智能体对应图灵机。

Result: 建立了自动机-智能体框架，为智能体架构的优化提供了原则性方法，能够形式化界定可验证系统与行为本质上不可判定的系统之间的边界。

Conclusion: 该框架为开发智能体框架的静态分析工具和语法奠定了基础，并通过扩展到概率自动机来处理基于LLM的智能体的概率性质。

Abstract: This paper establishes a formal equivalence between the architectural classes
of modern agentic AI systems and the abstract machines of the Chomsky
hierarchy. We posit that the memory architecture of an AI agent is the
definitive feature determining its computational power and that it directly
maps it to a corresponding class of automaton. Specifically, we demonstrate
that simple reflex agents are equivalent to Finite Automata, hierarchical
task-decomposition agents are equivalent to Pushdown Automata, and agents
employing readable/writable memory for reflection are equivalent to TMs. This
Automata-Agent Framework provides a principled methodology for right-sizing
agent architectures to optimize computational efficiency and cost. More
critically, it creates a direct pathway to formal verification, enables the
application of mature techniques from automata theory to guarantee agent safety
and predictability. By classifying agents, we can formally delineate the
boundary between verifiable systems and those whose behavior is fundamentally
undecidable. We address the inherent probabilistic nature of LLM-based agents
by extending the framework to probabilistic automata that allow quantitative
risk analysis. The paper concludes by outlining an agenda for developing static
analysis tools and grammars for agentic frameworks.

</details>


### [189] [Emotion-Coherent Reasoning for Multimodal LLMs via Emotional Rationale Verifier](https://arxiv.org/abs/2510.23506)
*Hyeongseop Rha,Jeong Hun Yeo,Yeonju Kim,Yong Man Ro*

Main category: cs.AI

TL;DR: 提出了情感推理验证器(ERV)和解释奖励方法，通过确保情感预测与解释的一致性来提高多模态大语言模型的情感理解能力，无需修改模型架构或额外标注。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在情感理解中存在预测与解释不一致的问题，这会降低系统可靠性和用户信任度，阻碍实现真正人性化的人机交互。

Method: 使用情感推理验证器(ERV)和解释奖励机制，引导模型生成与目标情感一致的解释推理，无需修改模型架构或额外视频描述标注。

Result: 在MAFW和DFEW数据集上显著提高了解释-预测一致性和解释情感准确性，通过实验和人工评估验证了方法的有效性。

Conclusion: 该方法不仅增强了解释与预测的对齐，还使多模态大语言模型能够提供情感一致、可信赖的交互，是实现真正人性化人机交互系统的关键一步。

Abstract: The recent advancement of Multimodal Large Language Models (MLLMs) is
transforming human-computer interaction (HCI) from surface-level exchanges into
more nuanced and emotionally intelligent communication. To realize this shift,
emotion understanding becomes essential allowing systems to capture subtle cues
underlying user intent. Furthermore, providing faithful explanations for
predicted emotions is crucial to ensure interpretability and build user trust.
However, current MLLM-based methods often generate emotion explanations that
diverge from the target labels and sometimes even contradict their own
predicted emotions. This inconsistency poses a critical risk for
misunderstanding and erodes reliability in interactive settings. To address
this, we propose a novel approach: the Emotional Rationale Verifier (ERV) and
an Explanation Reward. Our method guides the model to produce reasoning that is
explicitly consistent with the target emotion during multimodal emotion
recognition without modifying the model architecture or requiring additional
paired video-description annotations. Our method significantly improves
faithful explanation-prediction consistency and explanation emotion accuracy on
the MAFW and DFEW datasets. Through extensive experiments and human
evaluations, we show that our approach not only enhances alignment between
explanation and prediction but also empowers MLLMs to deliver emotionally
coherent, trustworthy interactions, marking a key step toward truly human-like
HCI systems.

</details>


### [190] [Toward Carbon-Neutral Human AI: Rethinking Data, Computation, and Learning Paradigms for Sustainable Intelligence](https://arxiv.org/abs/2510.23524)
*KC Santosh,Rodrigue Rizk,Longwei Wang*

Main category: cs.AI

TL;DR: 提出了Human AI (HAI)框架，通过增量学习、碳感知优化和人机协作，解决AI发展中的环境与伦理问题，实现可持续、负责任的人工智能。


<details>
  <summary>Details</summary>
Motivation: AI快速发展带来巨大计算需求，引发环境和伦理担忧，需要从依赖大规模静态数据集和单一训练范式转向可持续的解决方案。

Method: 引入HAI框架，采用增量学习、碳感知优化和人机协作，借鉴生物认知原理，利用动态架构平衡性能与生态责任。

Result: 建立了理论基础、系统设计和操作原则，使AI能够持续情境学习，同时最小化碳足迹和人工标注成本。

Conclusion: HAI框架为解决主动学习、持续适应和能效模型部署等挑战提供了路径，推动负责任、以人为本的人工智能发展。

Abstract: The rapid advancement of Artificial Intelligence (AI) has led to
unprecedented computational demands, raising significant environmental and
ethical concerns. This paper critiques the prevailing reliance on large-scale,
static datasets and monolithic training paradigms, advocating for a shift
toward human-inspired, sustainable AI solutions. We introduce a novel
framework, Human AI (HAI), which emphasizes incremental learning, carbon-aware
optimization, and human-in-the-loop collaboration to enhance adaptability,
efficiency, and accountability. By drawing parallels with biological cognition
and leveraging dynamic architectures, HAI seeks to balance performance with
ecological responsibility. We detail the theoretical foundations, system
design, and operational principles that enable AI to learn continuously and
contextually while minimizing carbon footprints and human annotation costs. Our
approach addresses pressing challenges in active learning, continual
adaptation, and energy-efficient model deployment, offering a pathway toward
responsible, human-centered artificial intelligence.

</details>


### [191] [When No Paths Lead to Rome: Benchmarking Systematic Neural Relational Reasoning](https://arxiv.org/abs/2510.23532)
*Anirban Das,Irtaza Khalid,Rafael Peñaloza,Steven Schockaert*

Main category: cs.AI

TL;DR: NoRA是一个新的系统关系推理基准，增加了多个难度级别，要求模型超越基于路径的推理。


<details>
  <summary>Details</summary>
Motivation: 现有系统关系推理基准过于简化，假设推理可简化为关系路径组合，导致模型在现有基准表现好但难以泛化到其他设置。

Method: 引入NoRA基准，包含多个难度级别，要求模型进行超越路径推理的复杂推理。

Result: NoRA基准为系统关系推理提供了更全面和具有挑战性的评估框架。

Conclusion: NoRA基准将推动神经网络在系统关系推理领域的进一步发展，促进更通用推理模型的发展。

Abstract: Designing models that can learn to reason in a systematic way is an important
and long-standing challenge. In recent years, a wide range of solutions have
been proposed for the specific case of systematic relational reasoning,
including Neuro-Symbolic approaches, variants of the Transformer architecture,
and specialised Graph Neural Networks. However, existing benchmarks for
systematic relational reasoning focus on an overly simplified setting, based on
the assumption that reasoning can be reduced to composing relational paths. In
fact, this assumption is hard-baked into the architecture of several recent
models, leading to approaches that can perform well on existing benchmarks but
are difficult to generalise to other settings. To support further progress in
the field of systematic relational reasoning with neural networks, we introduce
NoRA, a new benchmark which adds several levels of difficulty and requires
models to go beyond path-based reasoning.

</details>


### [192] [JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence](https://arxiv.org/abs/2510.23538)
*Qiushi Sun,Jingyang Gong,Yang Liu,Qiaosheng Chen,Lei Li,Kai Chen,Qipeng Guo,Ben Kao,Fei Yuan*

Main category: cs.AI

TL;DR: 提出了JanusCode-800K多模态代码语料库和JanusCoder系列模型，通过统一的视觉-程序接口实现从文本指令、视觉输入或两者结合生成代码，在文本和视觉编码任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 神经代码智能正从纯文本源代码扩展到程序生成的丰富视觉输出，但高质量多模态代码数据的稀缺阻碍了进展，需要解决数据合成和质量评估的挑战。

Method: 开发了利用数据模态间协同作用的合成工具包，构建了JanusCode-800K多模态代码语料库，训练了JanusCoder和JanusCoderV模型，建立了统一的视觉-程序接口。

Result: JanusCoder系列在文本和视觉编码任务中表现优异，7B到14B规模的模型接近甚至超过商业模型性能，提供了程序逻辑与视觉表达协调的关键见解。

Conclusion: 通过大规模高质量多模态代码数据和统一模型，成功建立了视觉-程序接口，为高级应用如灵活内容生成和程序驱动的可视化编辑提供了有效解决方案。

Abstract: The scope of neural code intelligence is rapidly expanding beyond text-based
source code to encompass the rich visual outputs that programs generate. This
visual dimension is critical for advanced applications like flexible content
generation and precise, program-driven editing of visualizations. However,
progress has been impeded by the scarcity of high-quality multimodal code data,
a bottleneck stemming from challenges in synthesis and quality assessment. To
address these challenges, we make contributions from both a data and modeling
perspective. We first introduce a complete synthesis toolkit that leverages
reciprocal synergies between data modalities to efficiently produce a
large-scale, high-quality corpus spanning from standard charts to complex
interactive web UIs and code-driven animations. Leveraging this toolkit, we
construct JanusCode-800K, the largest multimodal code corpus to date. This
powers the training of our models, JanusCoder and JanusCoderV, which establish
a visual-programmatic interface for generating code from textual instructions,
visual inputs, or a combination of both. Our unified model is a departure from
existing approaches that build specialized models for isolated tasks. Extensive
experiments on both text-centric and vision-centric coding tasks demonstrate
the superior performance of the JanusCoder series, with our 7B to 14B scale
models approaching or even exceeding the performance of commercial models.
Furthermore, extensive analysis provides key insights into harmonizing
programmatic logic with its visual expression. Our code and checkpoints will
are available at https://github.com/InternLM/JanusCoder.

</details>


### [193] [OntoPret: An Ontology for the Interpretation of Human Behavior](https://arxiv.org/abs/2510.23553)
*Alexis Ellis,Stacie Severyn,Fjollë Novakazi,Hadi Banaee,Cogan Shimizu*

Main category: cs.AI

TL;DR: OntoPret是一个基于认知科学和模块化工程方法的人类行为解释本体论，填补了技术中心机器人框架与描述性行为本体论之间的研究空白，为机器实时协作解释复杂人类行为提供了形式化框架。


<details>
  <summary>Details</summary>
Motivation: 随着人机协作在工业5.0等范式中变得重要，需要机器安全有效地解释复杂人类行为。当前存在技术中心机器人框架缺乏人类行为细微模型，而描述性行为本体论不适合实时协作解释的研究空白。

Method: 基于认知科学和模块化工程方法，开发了OntoPret本体论，提供形式化、机器可处理的框架来分类行为，包括任务偏差和欺骗性行为。

Result: 在制造和游戏两个不同用例中证明了OntoPret的适应性，并建立了高级推理人类意图所需的语义基础。

Conclusion: OntoPret为机器实时协作解释人类行为提供了有效的本体论框架，填补了现有研究空白，支持对复杂人类行为的高级推理。

Abstract: As human machine teaming becomes central to paradigms like Industry 5.0, a
critical need arises for machines to safely and effectively interpret complex
human behaviors. A research gap currently exists between techno centric robotic
frameworks, which often lack nuanced models of human behavior, and descriptive
behavioral ontologies, which are not designed for real time, collaborative
interpretation. This paper addresses this gap by presenting OntoPret, an
ontology for the interpretation of human behavior. Grounded in cognitive
science and a modular engineering methodology, OntoPret provides a formal,
machine processable framework for classifying behaviors, including task
deviations and deceptive actions. We demonstrate its adaptability across two
distinct use cases manufacturing and gameplay and establish the semantic
foundations necessary for advanced reasoning about human intentions.

</details>


### [194] [ReCode: Unify Plan and Action for Universal Granularity Control](https://arxiv.org/abs/2510.23564)
*Zhaoyang Yu,Jiayi Zhang,Huixue Su,Yufan Zhao,Yifan Wu,Mingyi Deng,Jinyu Xiang,Yizhang Lin,Lingxiao Tang,Yingchao Li,Yuyu Luo,Bang Liu,Chenglin Wu*

Main category: cs.AI

TL;DR: ReCode提出了一种通过递归代码生成统一规划和行动的新范式，将高层计划视为抽象占位函数，并递归分解为更细粒度的子函数，实现决策粒度的动态控制。


<details>
  <summary>Details</summary>
Motivation: 现实世界任务需要不同粒度的决策，人类能够利用统一的认知表示实现这一点，但当前基于LLM的智能体缺乏这种跨粒度操作能力，现有范式在高层规划和低层行动之间存在刚性分离。

Method: ReCode将高层计划表示为抽象占位函数，通过递归方式将其分解为更细粒度的子函数，直到达到原始行动，从而消除规划与行动之间的刚性边界。

Result: 大量实验表明，ReCode在推理性能上显著超越先进基线方法，并在训练中表现出卓越的数据效率。

Conclusion: 通过递归代码生成统一规划和行动是实现通用粒度控制的有效方法，递归结构自然生成丰富的多粒度训练数据，使模型能够学习分层决策过程。

Abstract: Real-world tasks require decisions at varying granularities, and humans excel
at this by leveraging a unified cognitive representation where planning is
fundamentally understood as a high-level form of action. However, current Large
Language Model (LLM)-based agents lack this crucial capability to operate
fluidly across decision granularities. This limitation stems from existing
paradigms that enforce a rigid separation between high-level planning and
low-level action, which impairs dynamic adaptability and limits generalization.
We propose ReCode (Recursive Code Generation), a novel paradigm that addresses
this limitation by unifying planning and action within a single code
representation. In this representation, ReCode treats high-level plans as
abstract placeholder functions, which the agent then recursively decomposes
into finer-grained sub-functions until reaching primitive actions. This
recursive approach dissolves the rigid boundary between plan and action,
enabling the agent to dynamically control its decision granularity.
Furthermore, the recursive structure inherently generates rich,
multi-granularity training data, enabling models to learn hierarchical
decision-making processes. Extensive experiments show ReCode significantly
surpasses advanced baselines in inference performance and demonstrates
exceptional data efficiency in training, validating our core insight that
unifying planning and action through recursive code generation is a powerful
and effective approach to achieving universal granularity control. The code is
available at https://github.com/FoundationAgents/ReCode.

</details>


### [195] [Reduced AI Acceptance After the Generative AI Boom: Evidence From a Two-Wave Survey Study](https://arxiv.org/abs/2510.23578)
*Joachim Baumann,Aleksandra Urman,Ulrich Leicht-Deobald,Zachary J. Roman,Anikó Hannák,Markus Christen*

Main category: cs.AI

TL;DR: ChatGPT发布后，公众对AI的接受度下降，对人机协作的需求增加，并加剧了社会不平等


<details>
  <summary>Details</summary>
Motivation: 研究公众对AI的态度变化，特别是在有影响力的决策场景中，因为组织在集成AI时往往忽视用户偏好

Method: 使用瑞士人口代表性的大规模两波调查（第一波1514人，第二波1488人），比较ChatGPT发布前后的公众态度变化

Result: 生成式AI热潮与公众AI接受度显著降低相关，完全不可接受AI的比例从23%升至30%，支持纯人类决策的比例从18%升至26%，并扩大了教育、语言和性别差距

Conclusion: 研究结果挑战了行业对公众AI部署准备度的假设，强调技术发展必须与不断变化的公众偏好保持一致

Abstract: The rapid adoption of generative artificial intelligence (GenAI) technologies
has led many organizations to integrate AI into their products and services,
often without considering user preferences. Yet, public attitudes toward AI
use, especially in impactful decision-making scenarios, are underexplored.
Using a large-scale two-wave survey study (n_wave1=1514, n_wave2=1488)
representative of the Swiss population, we examine shifts in public attitudes
toward AI before and after the launch of ChatGPT. We find that the GenAI boom
is significantly associated with reduced public acceptance of AI (see Figure 1)
and increased demand for human oversight in various decision-making contexts.
The proportion of respondents finding AI "not acceptable at all" increased from
23% to 30%, while support for human-only decision-making rose from 18% to 26%.
These shifts have amplified existing social inequalities in terms of widened
educational, linguistic, and gender gaps post-boom. Our findings challenge
industry assumptions about public readiness for AI deployment and highlight the
critical importance of aligning technological development with evolving public
preferences.

</details>


### [196] [Multi-Agent Evolve: LLM Self-Improve through Co-evolution](https://arxiv.org/abs/2510.23595)
*Yixing Chen,Yiding Wang,Siqi Zhu,Haofei Yu,Tao Feng,Muhan Zhan,Mostofa Patwary,Jiaxuan You*

Main category: cs.AI

TL;DR: 提出了MAE框架，通过三个交互代理（提议者、求解者、评判者）实现LLM的自进化，在数学、推理和常识问答任务上平均提升4.54%


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法依赖人工标注数据和可验证奖励，限制了可扩展性和泛化性。自博弈RL方法需要特定环境反馈，难以扩展到通用领域

Method: 基于单一LLM实例化三个交互代理：提议者生成问题，求解者尝试解决，评判者评估并共同进化，应用强化学习优化行为

Result: 在Qwen2.5-3B-Instruct上实验，多个基准测试平均提升4.54%

Conclusion: MAE是一种可扩展、数据高效的方法，能以最小的人工监督依赖增强LLM的通用推理能力

Abstract: Reinforcement Learning (RL) has demonstrated significant potential in
enhancing the reasoning capabilities of large language models (LLMs). However,
the success of RL for LLMs heavily relies on human-curated datasets and
verifiable rewards, which limit their scalability and generality. Recent
Self-Play RL methods, inspired by the success of the paradigm in games and Go,
aim to enhance LLM reasoning capabilities without human-annotated data.
However, their methods primarily depend on a grounded environment for feedback
(e.g., a Python interpreter or a game engine); extending them to general
domains remains challenging. To address these challenges, we propose
Multi-Agent Evolve (MAE), a framework that enables LLMs to self-evolve in
solving diverse tasks, including mathematics, reasoning, and general knowledge
Q&A. The core design of MAE is based on a triplet of interacting agents
(Proposer, Solver, Judge) that are instantiated from a single LLM, and applies
reinforcement learning to optimize their behaviors. The Proposer generates
questions, the Solver attempts solutions, and the Judge evaluates both while
co-evolving. Experiments on Qwen2.5-3B-Instruct demonstrate that MAE achieves
an average improvement of 4.54% on multiple benchmarks. These results highlight
MAE as a scalable, data-efficient method for enhancing the general reasoning
abilities of LLMs with minimal reliance on human-curated supervision.

</details>


### [197] [Alita-G: Self-Evolving Generative Agent for Agent Generation](https://arxiv.org/abs/2510.23601)
*Jiahao Qiu,Xuan Qi,Hongru Wang,Xinzhe Juan,Yimin Wang,Zelin Zhao,Jiayi Geng,Jiacheng Guo,Peihang Li,Jingzhe Shi,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: ALITA-G是一个自我进化框架，通过生成、抽象和整理MCP工具，将通用代理转化为领域专家，在多个基准测试中实现了最先进的结果并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前的自进化代理主要局限于提示重写或失败重试，需要更系统的方法来将通用代理转化为领域专家。

Method: ALITA-G框架让通用代理执行目标领域任务，从成功轨迹中合成候选MCP工具，抽象为参数化原语并整合到MCP Box中，在推理时通过检索增强的MCP选择来执行代理。

Result: 在GAIA验证集上达到83.03% pass@1和89.09% pass@3的新SOTA结果，同时将每个示例的平均token数减少约15%。

Conclusion: ALITA-G提供了从通用能力到可重用领域特定能力的原理性路径，提高了复杂推理任务的准确性和效率。

Abstract: Large language models (LLMs) have been shown to perform better when
scaffolded into agents with memory, tools, and feedback. Beyond this,
self-evolving agents have emerged, but current work largely limits adaptation
to prompt rewriting or failure retries. Therefore, we present ALITA-G, a
self-evolution framework that transforms a general-purpose agent into a domain
expert by systematically generating, abstracting, and curating Model Context
Protocol (MCP) tools. In this framework, a generalist agent executes a curated
suite of target-domain tasks and synthesizes candidate MCPs from successful
trajectories. These are then abstracted to parameterized primitives and
consolidated into an MCP Box. At inference time, ALITA-G performs
retrieval-augmented MCP selection with the help of each tool's descriptions and
use cases, before executing an agent equipped with the MCP Executor. Across
several benchmarks GAIA, PathVQA, and Humanity's Last Exam, ALITA-G attains
strong gains while reducing computation costs. On GAIA validation, it achieves
83.03% pass@1 and 89.09% pass@3, establishing a new state-of-the-art result
while reducing mean tokens per example by approximately 15% relative to a
strong baseline agent. ALITA-G thus provides a principled pathway from
generalist capability to reusable, domain-specific competence, improving both
accuracy and efficiency on complex reasoning tasks.

</details>
