{"id": "2509.15248", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15248", "abs": "https://arxiv.org/abs/2509.15248", "authors": ["Zitong Yang", "Aonan Zhang", "Hong Liu", "Tatsunori Hashimoto", "Emmanuel Cand\u00e8s", "Chong Wang", "Ruoming Pang"], "title": "Synthetic bootstrapped pretraining", "comment": null, "summary": "We introduce Synthetic Bootstrapped Pretraining (SBP), a language model (LM)\npretraining procedure that first learns a model of relations between documents\nfrom the pretraining dataset and then leverages it to synthesize a vast new\ncorpus for joint training. While the standard pretraining teaches LMs to learn\ncausal correlations among tokens within a single document, it is not designed\nto efficiently model the rich, learnable inter-document correlations that can\npotentially lead to better performance. We validate SBP by designing a\ncompute-matched pretraining setup and pretrain a 3B-parameter model on up to 1T\ntokens from scratch. We find SBP consistently improves upon a strong repetition\nbaseline and delivers a significant fraction of performance improvement\nattainable by an oracle upper bound with access to 20x more unique data.\nQualitative analysis reveals that the synthesized documents go beyond mere\nparaphrases -- SBP first abstracts a core concept from the seed material and\nthen crafts a new narration on top of it. Besides strong empirical performance,\nSBP admits a natural Bayesian interpretation: the synthesizer implicitly learns\nto abstract the latent concepts shared between related documents.", "AI": {"tldr": "Synthetic Bootstrapped Pretraining (SBP) \u662f\u4e00\u79cd\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u6587\u6863\u95f4\u5173\u7cfb\u6765\u5408\u6210\u65b0\u8bed\u6599\u8fdb\u884c\u8054\u5408\u8bad\u7ec3\uff0c\u76f8\u6bd4\u6807\u51c6\u9884\u8bad\u7ec3\u80fd\u66f4\u597d\u5730\u5229\u7528\u6587\u6863\u95f4\u76f8\u5173\u6027\u3002", "motivation": "\u6807\u51c6\u9884\u8bad\u7ec3\u53ea\u5b66\u4e60\u5355\u6587\u6863\u5185\u7684token\u56e0\u679c\u5173\u7cfb\uff0c\u65e0\u6cd5\u6709\u6548\u5efa\u6a21\u6587\u6863\u95f4\u4e30\u5bcc\u7684\u76f8\u5173\u6027\uff0c\u800c\u8fd9\u4e9b\u76f8\u5173\u6027\u53ef\u80fd\u5e26\u6765\u6027\u80fd\u63d0\u5347\u3002", "method": "SBP\u9996\u5148\u4ece\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u5b66\u4e60\u6587\u6863\u95f4\u5173\u7cfb\u6a21\u578b\uff0c\u7136\u540e\u5229\u7528\u8be5\u6a21\u578b\u5408\u6210\u5927\u91cf\u65b0\u8bed\u6599\u8fdb\u884c\u8054\u5408\u8bad\u7ec3\u3002\u57283B\u53c2\u6570\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c\u4f7f\u7528\u6700\u591a1T tokens\u8fdb\u884c\u4ece\u5934\u8bad\u7ec3\u3002", "result": "SBP\u5728\u8ba1\u7b97\u5339\u914d\u7684\u9884\u8bad\u7ec3\u8bbe\u7f6e\u4e2d\u6301\u7eed\u4f18\u4e8e\u5f3a\u91cd\u590d\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u4f7f\u752820\u500d\u72ec\u7279\u6570\u636e\u7684oracle\u4e0a\u9650\u7684\u6027\u80fd\u6539\u8fdb\u3002\u5408\u6210\u6587\u6863\u8d85\u8d8a\u4e86\u7b80\u5355\u6539\u5199\uff0c\u80fd\u591f\u62bd\u8c61\u6838\u5fc3\u6982\u5ff5\u5e76\u6784\u5efa\u65b0\u53d9\u8ff0\u3002", "conclusion": "SBP\u4e0d\u4ec5\u5177\u6709\u5f3a\u5b9e\u8bc1\u6027\u80fd\uff0c\u8fd8\u5177\u5907\u81ea\u7136\u7684\u8d1d\u53f6\u65af\u89e3\u91ca\uff1a\u5408\u6210\u5668\u9690\u5f0f\u5b66\u4e60\u76f8\u5173\u6587\u6863\u95f4\u5171\u4eab\u7684\u6f5c\u5728\u6982\u5ff5\u62bd\u8c61\u3002"}}
{"id": "2509.15255", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15255", "abs": "https://arxiv.org/abs/2509.15255", "authors": ["Tandin Wangchuk", "Tad Gonsalves"], "title": "Comparative Analysis of Tokenization Algorithms for Low-Resource Language Dzongkha", "comment": "10 Pages", "summary": "Large Language Models (LLMs) are gaining popularity and improving rapidly.\nTokenizers are crucial components of natural language processing, especially\nfor LLMs. Tokenizers break down input text into tokens that models can easily\nprocess while ensuring the text is accurately represented, capturing its\nmeaning and structure. Effective tokenizers enhance the capabilities of LLMs by\nimproving a model's understanding of context and semantics, ultimately leading\nto better performance in various downstream tasks, such as translation,\nclassification, sentiment analysis, and text generation. Most pre-trained\ntokenizers are suitable for high-resource languages like English but perform\npoorly for low-resource languages. Dzongkha, Bhutan's national language spoken\nby around seven hundred thousand people, is a low-resource language, and its\nlinguistic complexity poses unique NLP challenges. Despite some progress,\nsignificant research in Dzongkha NLP is lacking, particularly in tokenization.\nThis study evaluates the training and performance of three common tokenization\nalgorithms in comparison to other popular methods. Specifically, Byte-Pair\nEncoding (BPE), WordPiece, and SentencePiece (Unigram) were evaluated for their\nsuitability for Dzongkha. Performance was assessed using metrics like Subword\nFertility, Proportion of Continued Words, Normalized Sequence Length, and\nexecution time. The results show that while all three algorithms demonstrate\npotential, SentencePiece is the most effective for Dzongkha tokenization,\npaving the way for further NLP advancements. This underscores the need for\ntailored approaches for low-resource languages and ongoing research. In this\nstudy, we presented three tokenization algorithms for Dzongkha, paving the way\nfor building Dzongkha Large Language Models.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e09\u79cd\u5206\u8bcd\u7b97\u6cd5\uff08BPE\u3001WordPiece\u3001SentencePiece\uff09\u5728\u4e0d\u4e39\u4f4e\u8d44\u6e90\u8bed\u8a00\u5b97\u5580\u8bed\u4e2d\u7684\u6027\u80fd\u8868\u73b0\uff0c\u53d1\u73b0SentencePiece\u7b97\u6cd5\u6700\u9002\u5408\u5b97\u5580\u8bed\u5206\u8bcd\u3002", "motivation": "\u5f53\u524d\u9884\u8bad\u7ec3\u5206\u8bcd\u5668\u4e3b\u8981\u9488\u5bf9\u9ad8\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u82f1\u8bed\uff09\uff0c\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u5b97\u5580\u8bed\uff09\u8868\u73b0\u4e0d\u4f73\u3002\u5b97\u5580\u8bed\u4f5c\u4e3a\u4e0d\u4e39\u7684\u5b98\u65b9\u8bed\u8a00\uff0c\u5176\u8bed\u8a00\u590d\u6742\u6027\u7ed9\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5e26\u6765\u72ec\u7279\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5206\u8bcd\u65b9\u9762\u7f3a\u4e4f\u6df1\u5165\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u5e38\u89c1\u5206\u8bcd\u7b97\u6cd5\uff08Byte-Pair Encoding\u3001WordPiece\u3001SentencePiece\uff09\u5bf9\u5b97\u5580\u8bed\u8fdb\u884c\u5206\u8bcd\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u91c7\u7528\u5b50\u8bcd\u751f\u80b2\u7387\u3001\u8fde\u7eed\u8bcd\u6bd4\u4f8b\u3001\u5f52\u4e00\u5316\u5e8f\u5217\u957f\u5ea6\u548c\u6267\u884c\u65f6\u95f4\u7b49\u6307\u6807\u8fdb\u884c\u6027\u80fd\u6bd4\u8f83\u3002", "result": "\u6240\u6709\u4e09\u79cd\u7b97\u6cd5\u90fd\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46SentencePiece\u5728\u5b97\u5580\u8bed\u5206\u8bcd\u4e2d\u8868\u73b0\u6700\u4e3a\u6709\u6548\uff0c\u4e3a\u6784\u5efa\u5b97\u5580\u8bed\u5927\u8bed\u8a00\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u9700\u8981\u5b9a\u5236\u5316\u65b9\u6cd5\u548c\u6301\u7eed\u7814\u7a76\u7684\u5fc5\u8981\u6027\uff0cSentencePiece\u7b97\u6cd5\u4e3a\u5b97\u5580\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.15260", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15260", "abs": "https://arxiv.org/abs/2509.15260", "authors": ["Yujia Hu", "Ming Shan Hee", "Preslav Nakov", "Roy Ka-Wei Lee"], "title": "Toxicity Red-Teaming: Benchmarking LLM Safety in Singapore's Low-Resource Languages", "comment": "9 pages, EMNLP 2025", "summary": "The advancement of Large Language Models (LLMs) has transformed natural\nlanguage processing; however, their safety mechanisms remain under-explored in\nlow-resource, multilingual settings. Here, we aim to bridge this gap. In\nparticular, we introduce \\textsf{SGToxicGuard}, a novel dataset and evaluation\nframework for benchmarking LLM safety in Singapore's diverse linguistic\ncontext, including Singlish, Chinese, Malay, and Tamil. SGToxicGuard adopts a\nred-teaming approach to systematically probe LLM vulnerabilities in three\nreal-world scenarios: \\textit{conversation}, \\textit{question-answering}, and\n\\textit{content composition}. We conduct extensive experiments with\nstate-of-the-art multilingual LLMs, and the results uncover critical gaps in\ntheir safety guardrails. By offering actionable insights into cultural\nsensitivity and toxicity mitigation, we lay the foundation for safer and more\ninclusive AI systems in linguistically diverse environments.\\footnote{Link to\nthe dataset: https://github.com/Social-AI-Studio/SGToxicGuard.}\n\\textcolor{red}{Disclaimer: This paper contains sensitive content that may be\ndisturbing to some readers.}", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86SGToxicGuard\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4f4e\u8d44\u6e90\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\uff0c\u7279\u522b\u5173\u6ce8\u65b0\u52a0\u5761\u7684\u591a\u5143\u8bed\u8a00\u73af\u5883\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u673a\u5236\u5728\u4f4e\u8d44\u6e90\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u7814\u7a76\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u65b0\u52a0\u5761\u8fd9\u6837\u5305\u542b\u82f1\u8bed\u3001\u4e2d\u6587\u3001\u9a6c\u6765\u8bed\u548c\u6cf0\u7c73\u5c14\u8bed\u7b49\u591a\u79cd\u8bed\u8a00\u7684\u591a\u5143\u6587\u5316\u73af\u5883\u4e2d\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u6a21\u578b\u7684\u5b89\u5168\u9632\u62a4\u80fd\u529b\u3002", "method": "\u91c7\u7528\u7ea2\u961f\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u5728\u5bf9\u8bdd\u3001\u95ee\u7b54\u548c\u5185\u5bb9\u521b\u4f5c\u4e09\u4e2a\u771f\u5b9e\u573a\u666f\u4e0b\u7cfb\u7edf\u6027\u5730\u63a2\u6d4b\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6f0f\u6d1e\uff0c\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5f53\u524d\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b89\u5168\u9632\u62a4\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u7279\u522b\u662f\u5728\u6587\u5316\u654f\u611f\u6027\u548c\u6bd2\u6027\u7f13\u89e3\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5728\u8bed\u8a00\u591a\u6837\u5316\u73af\u5883\u4e2d\u6784\u5efa\u66f4\u5b89\u5168\u3001\u66f4\u5177\u5305\u5bb9\u6027\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63d0\u4f9b\u4e86\u5173\u4e8e\u6587\u5316\u654f\u611f\u6027\u548c\u6bd2\u6027\u7f13\u89e3\u7684\u53ef\u64cd\u4f5c\u6027\u89c1\u89e3\u3002"}}
{"id": "2509.15335", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15335", "abs": "https://arxiv.org/abs/2509.15335", "authors": ["Charlott Jakob", "David Harbecke", "Patrick Parschan", "Pia Wenzel Neves", "Vera Schmitt"], "title": "PolBiX: Detecting LLMs' Political Bias in Fact-Checking through X-phemisms", "comment": null, "summary": "Large Language Models are increasingly used in applications requiring\nobjective assessment, which could be compromised by political bias. Many\nstudies found preferences for left-leaning positions in LLMs, but downstream\neffects on tasks like fact-checking remain underexplored. In this study, we\nsystematically investigate political bias through exchanging words with\neuphemisms or dysphemisms in German claims. We construct minimal pairs of\nfactually equivalent claims that differ in political connotation, to assess the\nconsistency of LLMs in classifying them as true or false. We evaluate six LLMs\nand find that, more than political leaning, the presence of judgmental words\nsignificantly influences truthfulness assessment. While a few models show\ntendencies of political bias, this is not mitigated by explicitly calling for\nobjectivism in prompts.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u6784\u5efa\u653f\u6cbb\u5185\u6db5\u4e0d\u540c\u7684\u5fb7\u8bed\u9648\u8ff0\u5bf9\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e8b\u5b9e\u6838\u67e5\u4efb\u52a1\u4e2d\u7684\u653f\u6cbb\u504f\u89c1\uff0c\u53d1\u73b0\u5224\u65ad\u6027\u8bcd\u6c47\u6bd4\u653f\u6cbb\u503e\u5411\u66f4\u663e\u8457\u5f71\u54cd\u771f\u5b9e\u6027\u8bc4\u4f30\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u9700\u8981\u5ba2\u89c2\u8bc4\u4f30\u7684\u5e94\u7528\uff0c\u4f46\u53ef\u80fd\u53d7\u5230\u653f\u6cbb\u504f\u89c1\u7684\u5f71\u54cd\u3002\u73b0\u6709\u7814\u7a76\u53d1\u73b0LLMs\u504f\u597d\u5de6\u503e\u7acb\u573a\uff0c\u4f46\u5bf9\u4e8b\u5b9e\u6838\u67e5\u7b49\u4e0b\u6e38\u4efb\u52a1\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u5c06\u5fb7\u8bed\u9648\u8ff0\u4e2d\u7684\u8bcd\u6c47\u66ff\u6362\u4e3a\u59d4\u5a49\u8bed\u6216\u8d2c\u4e49\u8bcd\uff0c\u6784\u5efa\u4e8b\u5b9e\u7b49\u4ef7\u4f46\u653f\u6cbb\u5185\u6db5\u4e0d\u540c\u7684\u6700\u5c0f\u9648\u8ff0\u5bf9\uff0c\u8bc4\u4f30\u516d\u4e2aLLMs\u5728\u5206\u7c7b\u8fd9\u4e9b\u9648\u8ff0\u4e3a\u771f\u6216\u5047\u65f6\u7684\u4e00\u81f4\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5224\u65ad\u6027\u8bcd\u6c47\u7684\u5b58\u5728\u6bd4\u653f\u6cbb\u503e\u5411\u66f4\u663e\u8457\u5f71\u54cd\u771f\u5b9e\u6027\u8bc4\u4f30\u3002\u5c11\u6570\u6a21\u578b\u663e\u793a\u51fa\u653f\u6cbb\u504f\u89c1\u503e\u5411\uff0c\u4f46\u901a\u8fc7\u660e\u786e\u8981\u6c42\u5ba2\u89c2\u6027\u7684\u63d0\u793a\u5e76\u4e0d\u80fd\u7f13\u89e3\u8fd9\u79cd\u504f\u89c1\u3002", "conclusion": "\u653f\u6cbb\u504f\u89c1\u5728LLMs\u7684\u4e8b\u5b9e\u6838\u67e5\u4efb\u52a1\u4e2d\u5f71\u54cd\u6709\u9650\uff0c\u5224\u65ad\u6027\u8bcd\u6c47\u662f\u66f4\u91cd\u8981\u7684\u5f71\u54cd\u56e0\u7d20\uff0c\u63d0\u793a\u5de5\u7a0b\u5bf9\u7f13\u89e3\u504f\u89c1\u6548\u679c\u4e0d\u660e\u663e\u3002"}}
{"id": "2509.15237", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15237", "abs": "https://arxiv.org/abs/2509.15237", "authors": ["Di Wen", "Kunyu Peng", "Junwei Zheng", "Yufan Chen", "Yitain Shi", "Jiale Wei", "Ruiping Liu", "Kailun Yang", "Rainer Stiefelhagen"], "title": "MICA: Multi-Agent Industrial Coordination Assistant", "comment": "The source code will be made publicly available at\n  https://github.com/Kratos-Wen/MICA", "summary": "Industrial workflows demand adaptive and trustworthy assistance that can\noperate under limited computing, connectivity, and strict privacy constraints.\nIn this work, we present MICA (Multi-Agent Industrial Coordination Assistant),\na perception-grounded and speech-interactive system that delivers real-time\nguidance for assembly, troubleshooting, part queries, and maintenance. MICA\ncoordinates five role-specialized language agents, audited by a safety checker,\nto ensure accurate and compliant support. To achieve robust step understanding,\nwe introduce Adaptive Step Fusion (ASF), which dynamically blends expert\nreasoning with online adaptation from natural speech feedback. Furthermore, we\nestablish a new multi-agent coordination benchmark across representative task\ncategories and propose evaluation metrics tailored to industrial assistance,\nenabling systematic comparison of different coordination topologies. Our\nexperiments demonstrate that MICA consistently improves task success,\nreliability, and responsiveness over baseline structures, while remaining\ndeployable on practical offline hardware. Together, these contributions\nhighlight MICA as a step toward deployable, privacy-preserving multi-agent\nassistants for dynamic factory environments. The source code will be made\npublicly available at https://github.com/Kratos-Wen/MICA.", "AI": {"tldr": "MICA\u662f\u4e00\u4e2a\u9762\u5411\u5de5\u4e1a\u5de5\u4f5c\u6d41\u7a0b\u7684\u591a\u667a\u80fd\u4f53\u534f\u8c03\u52a9\u624b\u7cfb\u7edf\uff0c\u901a\u8fc7\u8bed\u97f3\u4ea4\u4e92\u63d0\u4f9b\u5b9e\u65f6\u6307\u5bfc\uff0c\u5177\u5907\u611f\u77e5\u80fd\u529b\u548c\u9690\u79c1\u4fdd\u62a4\u7279\u6027", "motivation": "\u5de5\u4e1a\u5de5\u4f5c\u6d41\u7a0b\u9700\u8981\u80fd\u591f\u5728\u6709\u9650\u8ba1\u7b97\u80fd\u529b\u3001\u8fde\u63a5\u6027\u548c\u4e25\u683c\u9690\u79c1\u7ea6\u675f\u4e0b\u8fd0\u884c\u7684\u9002\u5e94\u6027\u5f3a\u7684\u53ef\u4fe1\u52a9\u624b\u7cfb\u7edf", "method": "\u534f\u8c03\u4e94\u4e2a\u89d2\u8272\u4e13\u4e1a\u5316\u7684\u8bed\u8a00\u667a\u80fd\u4f53\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u6b65\u9aa4\u878d\u5408(ASF)\u6280\u672f\u52a8\u6001\u878d\u5408\u4e13\u5bb6\u63a8\u7406\u4e0e\u5728\u7ebf\u8bed\u97f3\u53cd\u9988\u9002\u5e94\uff0c\u5e76\u5efa\u7acb\u591a\u667a\u80fd\u4f53\u534f\u8c03\u57fa\u51c6", "result": "\u5b9e\u9a8c\u8868\u660eMICA\u5728\u4efb\u52a1\u6210\u529f\u7387\u3001\u53ef\u9760\u6027\u548c\u54cd\u5e94\u6027\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u7ed3\u6784\uff0c\u53ef\u5728\u79bb\u7ebf\u786c\u4ef6\u4e0a\u90e8\u7f72", "conclusion": "MICA\u662f\u5b9e\u73b0\u53ef\u90e8\u7f72\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u591a\u667a\u80fd\u4f53\u52a9\u624b\u5728\u52a8\u6001\u5de5\u5382\u73af\u5883\u4e2d\u5e94\u7528\u7684\u91cd\u8981\u4e00\u6b65"}}
{"id": "2509.15339", "categories": ["cs.CL", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.15339", "abs": "https://arxiv.org/abs/2509.15339", "authors": ["Yeongbin Seo", "Dongha Lee", "Jinyoung Yeo"], "title": "Quantifying Self-Awareness of Knowledge in Large Language Models", "comment": null, "summary": "Hallucination prediction in large language models (LLMs) is often interpreted\nas a sign of self-awareness. However, we argue that such performance can arise\nfrom question-side shortcuts rather than true model-side introspection. To\ndisentangle these factors, we propose the Approximate Question-side Effect\n(AQE), which quantifies the contribution of question-awareness. Our analysis\nacross multiple datasets reveals that much of the reported success stems from\nexploiting superficial patterns in questions. We further introduce SCAO\n(Semantic Compression by Answering in One word), a method that enhances the use\nof model-side signals. Experiments show that SCAO achieves strong and\nconsistent performance, particularly in settings with reduced question-side\ncues, highlighting its effectiveness in fostering genuine self-awareness in\nLLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAQE\u65b9\u6cd5\u91cf\u5316\u95ee\u9898\u4fa7\u6377\u5f84\u5bf9\u5e7b\u89c9\u9884\u6d4b\u7684\u5f71\u54cd\uff0c\u5e76\u5f00\u53d1SCAO\u65b9\u6cd5\u589e\u5f3a\u6a21\u578b\u4fa7\u4fe1\u53f7\uff0c\u5b9e\u9a8c\u8868\u660eSCAO\u5728\u51cf\u5c11\u95ee\u9898\u4fa7\u7ebf\u7d22\u65f6\u4ecd\u80fd\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5e38\u5c06LLMs\u7684\u5e7b\u89c9\u9884\u6d4b\u80fd\u529b\u89e3\u91ca\u4e3a\u81ea\u6211\u610f\u8bc6\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u53ef\u80fd\u53ea\u662f\u5229\u7528\u4e86\u95ee\u9898\u4e2d\u7684\u8868\u9762\u6a21\u5f0f\u800c\u975e\u771f\u6b63\u7684\u6a21\u578b\u5185\u7701\u3002", "method": "\u63d0\u51fa\u8fd1\u4f3c\u95ee\u9898\u4fa7\u6548\u5e94(AQE)\u6765\u91cf\u5316\u95ee\u9898\u610f\u8bc6\u8d21\u732e\uff0c\u5e76\u5f15\u5165\u8bed\u4e49\u538b\u7f29\u56de\u7b54(SCAO)\u65b9\u6cd5\u589e\u5f3a\u6a21\u578b\u4fa7\u4fe1\u53f7\u7684\u4f7f\u7528\u3002", "result": "\u5206\u6790\u663e\u793a\u73b0\u6709\u65b9\u6cd5\u7684\u6210\u529f\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4f9d\u8d56\u4e8e\u95ee\u9898\u4e2d\u7684\u8868\u9762\u6a21\u5f0f\uff0c\u800cSCAO\u5728\u51cf\u5c11\u95ee\u9898\u4fa7\u7ebf\u7d22\u65f6\u4ecd\u80fd\u4fdd\u6301\u5f3a\u4e14\u4e00\u81f4\u7684\u6027\u80fd\u3002", "conclusion": "SCAO\u65b9\u6cd5\u80fd\u6709\u6548\u4fc3\u8fdbLLMs\u771f\u6b63\u7684\u81ea\u6211\u610f\u8bc6\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5229\u7528\u95ee\u9898\u4fa7\u6377\u5f84\u3002"}}
{"id": "2509.15239", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15239", "abs": "https://arxiv.org/abs/2509.15239", "authors": ["Stjepan Po\u017egaj", "Dobrik Georgiev", "Marin \u0160ili\u0107", "Petar Veli\u010dkovi\u0107"], "title": "KNARsack: Teaching Neural Algorithmic Reasoners to Solve Pseudo-Polynomial Problems", "comment": "14 pages, 10 figures", "summary": "Neural algorithmic reasoning (NAR) is a growing field that aims to embed\nalgorithmic logic into neural networks by imitating classical algorithms. In\nthis extended abstract, we detail our attempt to build a neural algorithmic\nreasoner that can solve Knapsack, a pseudo-polynomial problem bridging\nclassical algorithms and combinatorial optimisation, but omitted in standard\nNAR benchmarks. Our neural algorithmic reasoner is designed to closely follow\nthe two-phase pipeline for the Knapsack problem, which involves first\nconstructing the dynamic programming table and then reconstructing the solution\nfrom it. The approach, which models intermediate states through dynamic\nprogramming supervision, achieves better generalization to larger problem\ninstances than a direct-prediction baseline that attempts to select the optimal\nsubset only from the problem inputs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u7b97\u6cd5\u63a8\u7406\u5668\u6765\u89e3\u51b3\u80cc\u5305\u95ee\u9898\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u7ba1\u9053\u65b9\u6cd5\uff1a\u5148\u6784\u5efa\u52a8\u6001\u89c4\u5212\u8868\uff0c\u7136\u540e\u4ece\u4e2d\u91cd\u6784\u89e3\u51b3\u65b9\u6848\uff0c\u76f8\u6bd4\u76f4\u63a5\u9884\u6d4b\u57fa\u7ebf\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u795e\u7ecf\u7b97\u6cd5\u63a8\u7406\uff08NAR\uff09\u9886\u57df\u65e8\u5728\u5c06\u7b97\u6cd5\u903b\u8f91\u5d4c\u5165\u795e\u7ecf\u7f51\u7edc\uff0c\u4f46\u6807\u51c6NAR\u57fa\u51c6\u4e2d\u7f3a\u5c11\u80cc\u5305\u95ee\u9898\u8fd9\u4e00\u8fde\u63a5\u7ecf\u5178\u7b97\u6cd5\u548c\u7ec4\u5408\u4f18\u5316\u7684\u4f2a\u591a\u9879\u5f0f\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e24\u9636\u6bb5\u7ba1\u9053\uff1a\u7b2c\u4e00\u9636\u6bb5\u6784\u5efa\u52a8\u6001\u89c4\u5212\u8868\uff0c\u7b2c\u4e8c\u9636\u6bb5\u4ece\u8868\u4e2d\u91cd\u6784\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u52a8\u6001\u89c4\u5212\u76d1\u7763\u6765\u5efa\u6a21\u4e2d\u95f4\u72b6\u6001\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u8f83\u5927\u95ee\u9898\u5b9e\u4f8b\u4e0a\u6bd4\u76f4\u63a5\u9884\u6d4b\u57fa\u7ebf\uff08\u4ec5\u4ece\u95ee\u9898\u8f93\u5165\u4e2d\u9009\u62e9\u6700\u4f18\u5b50\u96c6\uff09\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u6a21\u4eff\u7ecf\u5178\u7b97\u6cd5\u7684\u4e24\u9636\u6bb5\u52a8\u6001\u89c4\u5212\u65b9\u6cd5\uff0c\u795e\u7ecf\u7b97\u6cd5\u63a8\u7406\u5668\u80fd\u591f\u6709\u6548\u89e3\u51b3\u80cc\u5305\u95ee\u9898\u5e76\u5b9e\u73b0\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.15350", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15350", "abs": "https://arxiv.org/abs/2509.15350", "authors": ["Yitong Wang", "Zhongping Zhang", "Margherita Piana", "Zheng Zhou", "Peter Gerstoft", "Bryan A. Plummer"], "title": "Real, Fake, or Manipulated? Detecting Machine-Influenced Text", "comment": "Accepted to EMNLP 2025 Findings", "summary": "Large Language Model (LLMs) can be used to write or modify documents,\npresenting a challenge for understanding the intent behind their use. For\nexample, benign uses may involve using LLM on a human-written document to\nimprove its grammar or to translate it into another language. However, a\ndocument entirely produced by a LLM may be more likely to be used to spread\nmisinformation than simple translation (\\eg, from use by malicious actors or\nsimply by hallucinating). Prior works in Machine Generated Text (MGT) detection\nmostly focus on simply identifying whether a document was human or machine\nwritten, ignoring these fine-grained uses. In this paper, we introduce a\nHiErarchical, length-RObust machine-influenced text detector (HERO), which\nlearns to separate text samples of varying lengths from four primary types:\nhuman-written, machine-generated, machine-polished, and machine-translated.\nHERO accomplishes this by combining predictions from length-specialist models\nthat have been trained with Subcategory Guidance. Specifically, for categories\nthat are easily confused (\\eg, different source languages), our Subcategory\nGuidance module encourages separation of the fine-grained categories, boosting\nperformance. Extensive experiments across five LLMs and six domains demonstrate\nthe benefits of our HERO, outperforming the state-of-the-art by 2.5-3 mAP on\naverage.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHERO\u7684\u5206\u5c42\u3001\u957f\u5ea6\u9c81\u68d2\u6027\u673a\u5668\u5f71\u54cd\u6587\u672c\u68c0\u6d4b\u5668\uff0c\u80fd\u591f\u533a\u5206\u56db\u79cd\u6587\u672c\u7c7b\u578b\uff1a\u4eba\u5de5\u64b0\u5199\u3001\u673a\u5668\u751f\u6210\u3001\u673a\u5668\u6da6\u8272\u548c\u673a\u5668\u7ffb\u8bd1\uff0c\u901a\u8fc7\u7ed3\u5408\u957f\u5ea6\u4e13\u5bb6\u6a21\u578b\u548c\u5b50\u7c7b\u522b\u6307\u5bfc\u6a21\u5757\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u751f\u6210\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u533a\u5206\u4eba\u5de5\u6216\u673a\u5668\u64b0\u5199\uff0c\u5ffd\u7565\u4e86\u7ec6\u7c92\u5ea6\u7684\u4f7f\u7528\u610f\u56fe\uff08\u5982\u673a\u5668\u6da6\u8272\u3001\u7ffb\u8bd1\u7b49\uff09\uff0c\u800c\u4e0d\u540c\u4f7f\u7528\u610f\u56fe\u5bf9\u6587\u672c\u53ef\u4fe1\u5ea6\u6709\u91cd\u8981\u5f71\u54cd\u3002", "method": "HERO\u7ed3\u5408\u957f\u5ea6\u4e13\u5bb6\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c\uff0c\u5e76\u91c7\u7528\u5b50\u7c7b\u522b\u6307\u5bfc\u6a21\u5757\u6765\u9f13\u52b1\u6613\u6df7\u6dc6\u7c7b\u522b\uff08\u5982\u4e0d\u540c\u6e90\u8bed\u8a00\uff09\u7684\u5206\u79bb\uff0c\u4ece\u800c\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "result": "\u5728\u4e94\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u548c\u516d\u4e2a\u9886\u57df\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cHERO\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u5e73\u5747\u63d0\u53472.5-3 mAP\u3002", "conclusion": "HERO\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u7ec6\u7c92\u5ea6\u7684\u673a\u5668\u5f71\u54cd\u6587\u672c\u7c7b\u578b\uff0c\u4e3a\u7406\u89e3LLM\u4f7f\u7528\u610f\u56fe\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u5de5\u5177\u3002"}}
{"id": "2509.15291", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.15291", "abs": "https://arxiv.org/abs/2509.15291", "authors": ["Federico Taschin", "Abderrahmane Lazaraq", "Ozan K. Tonguz", "Inci Ozgunes"], "title": "The Distribution Shift Problem in Transportation Networks using Reinforcement Learning and AI", "comment": null, "summary": "The use of Machine Learning (ML) and Artificial Intelligence (AI) in smart\ntransportation networks has increased significantly in the last few years.\nAmong these ML and AI approaches, Reinforcement Learning (RL) has been shown to\nbe a very promising approach by several authors. However, a problem with using\nReinforcement Learning in Traffic Signal Control is the reliability of the\ntrained RL agents due to the dynamically changing distribution of the input\ndata with respect to the distribution of the data used for training. This\npresents a major challenge and a reliability problem for the trained network of\nAI agents and could have very undesirable and even detrimental consequences if\na suitable solution is not found. Several researchers have tried to address\nthis problem using different approaches. In particular, Meta Reinforcement\nLearning (Meta RL) promises to be an effective solution. In this paper, we\nevaluate and analyze a state-of-the-art Meta RL approach called MetaLight and\nshow that, while under certain conditions MetaLight can indeed lead to\nreasonably good results, under some other conditions it might not perform well\n(with errors of up to 22%), suggesting that Meta RL schemes are often not\nrobust enough and can even pose major reliability problems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc4\u4f30\u4e86MetaLight\u8fd9\u4e00\u5143\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u867d\u7136\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5176\u4ed6\u6761\u4ef6\u4e0b\u53ef\u80fd\u4ea7\u751f\u9ad8\u8fbe22%\u7684\u9519\u8bef\uff0c\u8868\u660e\u5143\u5f3a\u5316\u5b66\u4e60\u65b9\u6848\u5f80\u5f80\u4e0d\u591f\u7a33\u5065\u3002", "motivation": "\u667a\u80fd\u4ea4\u901a\u7f51\u7edc\u4e2d\u673a\u5668\u5b66\u4e60\u548c\u4eba\u5de5\u667a\u80fd\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u5f3a\u5316\u5b66\u4e60\u88ab\u8ba4\u4e3a\u662f\u5f88\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002\u4f46\u5f3a\u5316\u5b66\u4e60\u5728\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u4e2d\u5b58\u5728\u53ef\u9760\u6027\u95ee\u9898\uff0c\u56e0\u4e3a\u8f93\u5165\u6570\u636e\u7684\u52a8\u6001\u53d8\u5316\u4e0e\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u4e0d\u4e00\u81f4\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u540e\u679c\u3002", "method": "\u8bba\u6587\u8bc4\u4f30\u548c\u5206\u6790\u4e86\u4e00\u79cd\u6700\u5148\u8fdb\u7684\u5143\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5MetaLight\uff0c\u6d4b\u8bd5\u5176\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0MetaLight\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\u80fd\u53d6\u5f97\u76f8\u5f53\u597d\u7684\u7ed3\u679c\uff0c\u4f46\u5728\u5176\u4ed6\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u9519\u8bef\u7387\u9ad8\u8fbe22%\uff0c\u8868\u660e\u5143\u5f3a\u5316\u5b66\u4e60\u65b9\u6848\u5b58\u5728\u53ef\u9760\u6027\u95ee\u9898\u3002", "conclusion": "\u5143\u5f3a\u5316\u5b66\u4e60\u65b9\u6848\u5f80\u5f80\u4e0d\u591f\u7a33\u5065\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53ef\u80fd\u5e26\u6765\u91cd\u5927\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u548c\u4f18\u5316\u3002"}}
{"id": "2509.15361", "categories": ["cs.CL", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.15361", "abs": "https://arxiv.org/abs/2509.15361", "authors": ["Zichen Wu", "Hsiu-Yuan Huang", "Yunfang Wu"], "title": "Beyond Spurious Signals: Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing", "comment": "Accepted by EMNLP 2025 Findings", "summary": "Multimodal Large Language Models (MLLMs) have shown substantial capabilities\nin integrating visual and textual information, yet frequently rely on spurious\ncorrelations, undermining their robustness and generalization in complex\nmultimodal reasoning tasks. This paper addresses the critical challenge of\nsuperficial correlation bias in MLLMs through a novel causal mediation-based\ndebiasing framework. Specially, we distinguishing core semantics from spurious\ntextual and visual contexts via counterfactual examples to activate\ntraining-stage debiasing and employ a Mixture-of-Experts (MoE) architecture\nwith dynamic routing to selectively engages modality-specific debiasing\nexperts. Empirical evaluation on multimodal sarcasm detection and sentiment\nanalysis tasks demonstrates that our framework significantly surpasses unimodal\ndebiasing strategies and existing state-of-the-art models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u4e2d\u4ecb\u7684\u53bb\u504f\u6846\u67b6\uff0c\u901a\u8fc7\u53cd\u4e8b\u5b9e\u793a\u4f8b\u533a\u5206\u6838\u5fc3\u8bed\u4e49\u4e0e\u865a\u5047\u4e0a\u4e0b\u6587\uff0c\u5e76\u91c7\u7528MoE\u67b6\u6784\u52a8\u6001\u9009\u62e9\u6a21\u6001\u4e13\u5bb6\uff0c\u6709\u6548\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u8868\u9762\u76f8\u5173\u6027\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6574\u5408\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\u65f6\u7ecf\u5e38\u4f9d\u8d56\u865a\u5047\u76f8\u5173\u6027\uff0c\u8fd9\u524a\u5f31\u4e86\u5176\u5728\u590d\u6742\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u53cd\u4e8b\u5b9e\u793a\u4f8b\u533a\u5206\u6838\u5fc3\u8bed\u4e49\u4e0e\u865a\u5047\u4e0a\u4e0b\u6587\u8fdb\u884c\u8bad\u7ec3\u9636\u6bb5\u53bb\u504f\uff0c\u5e76\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u52a8\u6001\u8def\u7531\u9009\u62e9\u6a21\u6001\u7279\u5b9a\u7684\u53bb\u504f\u4e13\u5bb6\u3002", "result": "\u5728\u591a\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u548c\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u5355\u6a21\u6001\u53bb\u504f\u7b56\u7565\u548c\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684\u56e0\u679c\u4e2d\u4ecb\u53bb\u504f\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2509.15292", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15292", "abs": "https://arxiv.org/abs/2509.15292", "authors": ["Abhiyan Dhakal", "Kausik Paudel", "Sanjog Sigdel"], "title": "An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid Literature", "comment": "8 pages, 6 figures, 1 table, National Conference on Computer\n  Innovations", "summary": "We propose an automated pipeline for performing literature reviews using\nsemantic similarity. Unlike traditional systematic review systems or\noptimization based methods, this work emphasizes minimal overhead and high\nrelevance by using transformer based embeddings and cosine similarity. By\nproviding a paper title and abstract, it generates relevant keywords, fetches\nrelevant papers from open access repository, and ranks them based on their\nsemantic closeness to the input. Three embedding models were evaluated. A\nstatistical thresholding approach is then applied to filter relevant papers,\nenabling an effective literature review pipeline. Despite the absence of\nheuristic feedback or ground truth relevance labels, the proposed system shows\npromise as a scalable and practical tool for preliminary research and\nexploratory analysis.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u5ea6\u7684\u81ea\u52a8\u5316\u6587\u732e\u7efc\u8ff0\u6d41\u7a0b\uff0c\u4f7f\u7528transformer\u5d4c\u5165\u548c\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5b9e\u73b0\u4f4e\u5f00\u9500\u9ad8\u76f8\u5173\u6027", "motivation": "\u4f20\u7edf\u7cfb\u7edf\u7efc\u8ff0\u65b9\u6cd5\u6216\u57fa\u4e8e\u4f18\u5316\u7684\u65b9\u6cd5\u5b58\u5728\u5f00\u9500\u5927\u3001\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u6700\u5c0f\u5316\u5f00\u9500\u4e14\u76f8\u5173\u6027\u9ad8\u7684\u81ea\u52a8\u5316\u6587\u732e\u7efc\u8ff0\u5de5\u5177", "method": "\u4f7f\u7528transformer\u5d4c\u5165\u6a21\u578b\u751f\u6210\u8bed\u4e49\u8868\u793a\uff0c\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u8bba\u6587\u76f8\u5173\u6027\uff0c\u91c7\u7528\u7edf\u8ba1\u9608\u503c\u65b9\u6cd5\u8fc7\u6ee4\u76f8\u5173\u8bba\u6587\uff0c\u8bc4\u4f30\u4e09\u79cd\u5d4c\u5165\u6a21\u578b\u6027\u80fd", "result": "\u5c3d\u7ba1\u7f3a\u4e4f\u542f\u53d1\u5f0f\u53cd\u9988\u6216\u771f\u5b9e\u76f8\u5173\u6027\u6807\u7b7e\uff0c\u8be5\u7cfb\u7edf\u663e\u793a\u51fa\u4f5c\u4e3a\u53ef\u6269\u5c55\u5b9e\u7528\u5de5\u5177\u7684\u6f5c\u529b\uff0c\u9002\u7528\u4e8e\u521d\u6b65\u7814\u7a76\u548c\u63a2\u7d22\u6027\u5206\u6790", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u5ea6\u7684\u81ea\u52a8\u5316\u6587\u732e\u7efc\u8ff0\u6d41\u7a0b\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u9ad8\u6548\u4fbf\u6377\u7684\u6587\u732e\u7b5b\u9009\u5de5\u5177"}}
{"id": "2509.15362", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15362", "abs": "https://arxiv.org/abs/2509.15362", "authors": ["Yaya Sy", "Dioula Doucour\u00e9", "Christophe Cerisara", "Irina Illina"], "title": "Speech Language Models for Under-Represented Languages: Insights from Wolof", "comment": null, "summary": "We present our journey in training a speech language model for Wolof, an\nunderrepresented language spoken in West Africa, and share key insights. We\nfirst emphasize the importance of collecting large-scale, spontaneous,\nhigh-quality speech data, and show that continued pretraining HuBERT on this\ndataset outperforms both the base model and African-centric models on ASR. We\nthen integrate this speech encoder into a Wolof LLM to train the first Speech\nLLM for this language, extending its capabilities to tasks such as speech\ntranslation. Furthermore, we explore training the Speech LLM to perform\nmulti-step Chain-of-Thought before transcribing or translating. Our results\nshow that the Speech LLM not only improves speech recognition but also performs\nwell in speech translation. The models and the code will be openly shared.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e3a\u897f\u975e\u4f4e\u8d44\u6e90\u8bed\u8a00\u6c83\u6d1b\u592b\u8bed\u8bad\u7ec3\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u7684\u5386\u7a0b\uff0c\u5206\u4eab\u4e86\u5173\u952e\u89c1\u89e3\uff0c\u5305\u62ec\u6570\u636e\u6536\u96c6\u3001\u8bed\u97f3\u7f16\u7801\u5668\u96c6\u6210\u548c\u601d\u7ef4\u94fe\u65b9\u6cd5\u7684\u5e94\u7528\u3002", "motivation": "\u6c83\u6d1b\u592b\u8bed\u4f5c\u4e3a\u897f\u975e\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u8bed\u8a00\uff0c\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u3002\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u7d22\u5982\u4f55\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u6784\u5efa\u6709\u6548\u7684\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u3002", "method": "1. \u6536\u96c6\u5927\u89c4\u6a21\u3001\u81ea\u53d1\u3001\u9ad8\u8d28\u91cf\u7684\u6c83\u6d1b\u592b\u8bed\u8bed\u97f3\u6570\u636e\uff1b2. \u5728HuBERT\u57fa\u7840\u4e0a\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\uff1b3. \u5c06\u8bed\u97f3\u7f16\u7801\u5668\u96c6\u6210\u5230\u6c83\u6d1b\u592b\u8bedLLM\u4e2d\uff1b4. \u63a2\u7d22\u591a\u6b65\u601d\u7ef4\u94fe\u65b9\u6cd5\u7528\u4e8e\u8bed\u97f3\u8f6c\u5f55\u548c\u7ffb\u8bd1\u3002", "result": "1. \u6301\u7eed\u9884\u8bad\u7ec3\u7684HuBERT\u5728ASR\u4efb\u52a1\u4e0a\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\u548c\u975e\u6d32\u4e2d\u5fc3\u6a21\u578b\uff1b2. \u8bed\u97f3LLM\u4e0d\u4ec5\u6539\u5584\u4e86\u8bed\u97f3\u8bc6\u522b\uff0c\u5728\u8bed\u97f3\u7ffb\u8bd1\u4efb\u52a1\u4e0a\u4e5f\u8868\u73b0\u826f\u597d\uff1b3. \u591a\u6b65\u601d\u7ef4\u94fe\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u6210\u529f\u6784\u5efa\u4e86\u9996\u4e2a\u6c83\u6d1b\u592b\u8bed\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u5f00\u53d1\u8bed\u97f3LLM\u7684\u53ef\u884c\u6027\uff0c\u6a21\u578b\u548c\u4ee3\u7801\u5c06\u5f00\u6e90\u5171\u4eab\u3002"}}
{"id": "2509.15336", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15336", "abs": "https://arxiv.org/abs/2509.15336", "authors": ["Humam Kourani", "Anton Antonov", "Alessandro Berti", "Wil M. P. van der Aalst"], "title": "Knowledge-Driven Hallucination in Large Language Models: An Empirical Study on Process Modeling", "comment": "The Version of Record of this contribution will be published in the\n  proceedings of the 2nd International Workshop on Generative AI for Process\n  Mining (GenAI4PM 2025). This preprint has not undergone peer review or any\n  post-submission improvements or corrections", "summary": "The utility of Large Language Models (LLMs) in analytical tasks is rooted in\ntheir vast pre-trained knowledge, which allows them to interpret ambiguous\ninputs and infer missing information. However, this same capability introduces\na critical risk of what we term knowledge-driven hallucination: a phenomenon\nwhere the model's output contradicts explicit source evidence because it is\noverridden by the model's generalized internal knowledge. This paper\ninvestigates this phenomenon by evaluating LLMs on the task of automated\nprocess modeling, where the goal is to generate a formal business process model\nfrom a given source artifact. The domain of Business Process Management (BPM)\nprovides an ideal context for this study, as many core business processes\nfollow standardized patterns, making it likely that LLMs possess strong\npre-trained schemas for them. We conduct a controlled experiment designed to\ncreate scenarios with deliberate conflict between provided evidence and the\nLLM's background knowledge. We use inputs describing both standard and\ndeliberately atypical process structures to measure the LLM's fidelity to the\nprovided evidence. Our work provides a methodology for assessing this critical\nreliability issue and raises awareness of the need for rigorous validation of\nAI-generated artifacts in any evidence-based domain.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86LLMs\u5728\u77e5\u8bc6\u9a71\u52a8\u5e7b\u89c9\u65b9\u9762\u7684\u98ce\u9669\uff0c\u5373\u5728\u81ea\u52a8\u5316\u8fc7\u7a0b\u5efa\u6a21\u4efb\u52a1\u4e2d\uff0c\u6a21\u578b\u8f93\u51fa\u4e0e\u660e\u786e\u6765\u6e90\u8bc1\u636e\u76f8\u77db\u76fe\u7684\u73b0\u8c61\u3002", "motivation": "LLMs\u7684\u9884\u8bad\u7ec3\u77e5\u8bc6\u867d\u7136\u6709\u52a9\u4e8e\u5206\u6790\u4efb\u52a1\uff0c\u4f46\u4e5f\u53ef\u80fd\u5bfc\u81f4\u77e5\u8bc6\u9a71\u52a8\u5e7b\u89c9\uff0c\u5373\u6a21\u578b\u8f93\u51fa\u88ab\u5176\u5185\u90e8\u77e5\u8bc6\u8986\u76d6\u800c\u5ffd\u89c6\u63d0\u4f9b\u7684\u8bc1\u636e\u3002", "method": "\u901a\u8fc7\u5728\u4e1a\u52a1\u6d41\u7a0b\u7ba1\u7406\u9886\u57df\u8fdb\u884c\u63a7\u5236\u5b9e\u9a8c\uff0c\u8bbe\u8ba1\u6807\u51c6\u548c\u975e\u5178\u578b\u8fc7\u7a0b\u7ed3\u6784\u8f93\u5165\uff0c\u6d4b\u91cfLLMs\u5bf9\u63d0\u4f9b\u8bc1\u636e\u7684\u5fe0\u5b9e\u5ea6\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86LLMs\u5728\u9762\u5bf9\u8bc1\u636e\u4e0e\u80cc\u666f\u77e5\u8bc6\u51b2\u7a81\u65f6\uff0c\u503e\u5411\u4e8e\u4f9d\u8d56\u9884\u8bad\u7ec3\u77e5\u8bc6\u800c\u975e\u63d0\u4f9b\u8bc1\u636e\uff0c\u5bfc\u81f4\u8f93\u51fa\u4e0d\u53ef\u9760\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u8bc4\u4f30LLMs\u53ef\u9760\u6027\u7684\u65b9\u6cd5\uff0c\u5e76\u5f3a\u8c03\u5728\u8bc1\u636e\u9a71\u52a8\u9886\u57df\u9700\u8981\u5bf9AI\u751f\u6210\u4ea7\u7269\u8fdb\u884c\u4e25\u683c\u9a8c\u8bc1\u3002"}}
{"id": "2509.15373", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15373", "abs": "https://arxiv.org/abs/2509.15373", "authors": ["Katsumi Ibaraki", "David Chiang"], "title": "Frustratingly Easy Data Augmentation for Low-Resource ASR", "comment": "5 pages, 2 figures, 2 tables, submitted to ICASSP 2026", "summary": "This paper introduces three self-contained data augmentation methods for\nlow-resource Automatic Speech Recognition (ASR). Our techniques first generate\nnovel text--using gloss-based replacement, random replacement, or an LLM-based\napproach--and then apply Text-to-Speech (TTS) to produce synthetic audio. We\napply these methods, which leverage only the original annotated data, to four\nlanguages with extremely limited resources (Vatlongos, Nashta, Shinekhen\nBuryat, and Kakabe). Fine-tuning a pretrained Wav2Vec2-XLSR-53 model on a\ncombination of the original audio and generated synthetic data yields\nsignificant performance gains, including a 14.3% absolute WER reduction for\nNashta. The methods prove effective across all four low-resource languages and\nalso show utility for high-resource languages like English, demonstrating their\nbroad applicability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e09\u79cd\u72ec\u7acb\u7684\u4f4e\u8d44\u6e90\u8bed\u97f3\u8bc6\u522b\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u6587\u672c\u751f\u6210\u548c\u8bed\u97f3\u5408\u6210\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u8bed\u97f3\u8bc6\u522b\u4e2d\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u5229\u7528\u6709\u9650\u7684\u6807\u6ce8\u6570\u636e\u751f\u6210\u66f4\u591a\u8bad\u7ec3\u6837\u672c", "method": "\u4f7f\u7528\u57fa\u4e8e\u8bcd\u6c47\u66ff\u6362\u3001\u968f\u673a\u66ff\u6362\u548cLLM\u751f\u6210\u4e09\u79cd\u6587\u672c\u751f\u6210\u65b9\u6cd5\uff0c\u518d\u901a\u8fc7TTS\u6280\u672f\u5408\u6210\u97f3\u9891\u6570\u636e\uff0c\u7ed3\u5408\u539f\u59cb\u97f3\u9891\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b", "result": "\u5728\u56db\u79cd\u6781\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u83b7\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u5176\u4e2dNashta\u8bed\u8a00WER\u7edd\u5bf9\u964d\u4f4e\u4e8614.3%\uff0c\u65b9\u6cd5\u5728\u9ad8\u8d44\u6e90\u8bed\u8a00\u5982\u82f1\u8bed\u4e0a\u4e5f\u6709\u6548", "conclusion": "\u8fd9\u4e9b\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u5bf9\u4f4e\u8d44\u6e90\u548c\u9ad8\u8d44\u6e90\u8bed\u8a00\u90fd\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u80fd\u591f\u6709\u6548\u7f13\u89e3\u6570\u636e\u7a00\u7f3a\u95ee\u9898"}}
{"id": "2509.15366", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15366", "abs": "https://arxiv.org/abs/2509.15366", "authors": ["Andrejs Sorstkins", "Josh Bailey", "Dr Alistair Baron"], "title": "Diagnostics of cognitive failures in multi-agent expert systems using dynamic evaluation protocols and subsequent mutation of the processing context", "comment": "Dissertation and research project created in collaboration with\n  JobFair LTD", "summary": "The rapid evolution of neural architectures - from multilayer perceptrons to\nlarge-scale Transformer-based models - has enabled language models (LLMs) to\nexhibit emergent agentic behaviours when equipped with memory, planning, and\nexternal tool use. However, their inherent stochasticity and multi-step\ndecision processes render classical evaluation methods inadequate for\ndiagnosing agentic performance. This work introduces a diagnostic framework for\nexpert systems that not only evaluates but also facilitates the transfer of\nexpert behaviour into LLM-powered agents. The framework integrates (i) curated\ngolden datasets of expert annotations, (ii) silver datasets generated through\ncontrolled behavioural mutation, and (iii) an LLM-based Agent Judge that scores\nand prescribes targeted improvements. These prescriptions are embedded into a\nvectorized recommendation map, allowing expert interventions to propagate as\nreusable improvement trajectories across multiple system instances. We\ndemonstrate the framework on a multi-agent recruiter-assistant system, showing\nthat it uncovers latent cognitive failures - such as biased phrasing,\nextraction drift, and tool misrouting - while simultaneously steering agents\ntoward expert-level reasoning and style. The results establish a foundation for\nstandardized, reproducible expert behaviour transfer in stochastic,\ntool-augmented LLM agents, moving beyond static evaluation to active expert\nsystem refinement.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bca\u65ad\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u4fc3\u8fdb\u4e13\u5bb6\u884c\u4e3a\u5411LLM\u9a71\u52a8\u667a\u80fd\u4f53\u7684\u8fc1\u79fb\uff0c\u901a\u8fc7\u6574\u5408\u9ec4\u91d1\u6570\u636e\u96c6\u3001\u94f6\u6570\u636e\u96c6\u548c\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u8bc4\u5224\u5668\u6765\u8bc6\u522b\u548c\u6539\u5584\u667a\u80fd\u4f53\u7684\u8ba4\u77e5\u5931\u8d25\u3002", "motivation": "\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u8bca\u65ad\u5177\u6709\u968f\u673a\u6027\u548c\u591a\u6b65\u51b3\u7b56\u8fc7\u7a0b\u7684LLM\u667a\u80fd\u4f53\u6027\u80fd\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4e3b\u52a8\u6539\u8fdb\u4e13\u5bb6\u7cfb\u7edf\u7684\u65b9\u6cd5\u3002", "method": "\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(i)\u4e13\u5bb6\u6807\u6ce8\u7684\u9ec4\u91d1\u6570\u636e\u96c6\uff0c(ii)\u901a\u8fc7\u53d7\u63a7\u884c\u4e3a\u7a81\u53d8\u751f\u6210\u7684\u94f6\u6570\u636e\u96c6\uff0c(iii)\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u8bc4\u5224\u5668\u8fdb\u884c\u8bc4\u5206\u548c\u9488\u5bf9\u6027\u6539\u8fdb\u5efa\u8bae\uff0c\u8fd9\u4e9b\u5efa\u8bae\u5d4c\u5165\u5411\u91cf\u5316\u63a8\u8350\u56fe\u4e2d\u5b9e\u73b0\u8de8\u7cfb\u7edf\u4f20\u64ad\u3002", "result": "\u5728\u591a\u667a\u80fd\u4f53\u62db\u8058\u52a9\u624b\u7cfb\u7edf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u53d1\u73b0\u6f5c\u5728\u7684\u8ba4\u77e5\u5931\u8d25\uff08\u5982\u504f\u89c1\u63aa\u8f9e\u3001\u63d0\u53d6\u6f02\u79fb\u548c\u5de5\u5177\u8bef\u8def\u7531\uff09\uff0c\u540c\u65f6\u5f15\u5bfc\u667a\u80fd\u4f53\u8fbe\u5230\u4e13\u5bb6\u7ea7\u63a8\u7406\u548c\u98ce\u683c\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u968f\u673a\u6027\u3001\u5de5\u5177\u589e\u5f3a\u7684LLM\u667a\u80fd\u4f53\u5efa\u7acb\u4e86\u6807\u51c6\u5316\u3001\u53ef\u590d\u73b0\u7684\u4e13\u5bb6\u884c\u4e3a\u8fc1\u79fb\u57fa\u7840\uff0c\u5b9e\u73b0\u4e86\u4ece\u9759\u6001\u8bc4\u4f30\u5230\u4e3b\u52a8\u4e13\u5bb6\u7cfb\u7edf\u6539\u8fdb\u7684\u8f6c\u53d8\u3002"}}
{"id": "2509.15403", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15403", "abs": "https://arxiv.org/abs/2509.15403", "authors": ["Yangyi Li", "Mengdi Huai"], "title": "Quantifying Uncertainty in Natural Language Explanations of Large Language Models for Question Answering", "comment": null, "summary": "Large language models (LLMs) have shown strong capabilities, enabling\nconcise, context-aware answers in question answering (QA) tasks. The lack of\ntransparency in complex LLMs has inspired extensive research aimed at\ndeveloping methods to explain large language behaviors. Among existing\nexplanation methods, natural language explanations stand out due to their\nability to explain LLMs in a self-explanatory manner and enable the\nunderstanding of model behaviors even when the models are closed-source.\nHowever, despite these promising advancements, there is no existing work\nstudying how to provide valid uncertainty guarantees for these generated\nnatural language explanations. Such uncertainty quantification is critical in\nunderstanding the confidence behind these explanations. Notably, generating\nvalid uncertainty estimates for natural language explanations is particularly\nchallenging due to the auto-regressive generation process of LLMs and the\npresence of noise in medical inquiries. To bridge this gap, in this work, we\nfirst propose a novel uncertainty estimation framework for these generated\nnatural language explanations, which provides valid uncertainty guarantees in a\npost-hoc and model-agnostic manner. Additionally, we also design a novel robust\nuncertainty estimation method that maintains valid uncertainty guarantees even\nunder noise. Extensive experiments on QA tasks demonstrate the desired\nperformance of our methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6846\u67b6\uff0c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u63d0\u4f9b\u6709\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u4fdd\u8bc1\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5728\u566a\u58f0\u4e0b\u4ecd\u80fd\u4fdd\u6301\u6709\u6548\u6027\u7684\u9c81\u68d2\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u65b9\u6cd5\u80fd\u591f\u4ee5\u81ea\u89e3\u91ca\u7684\u65b9\u5f0f\u89e3\u91caLLMs\u884c\u4e3a\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u751f\u6210\u89e3\u91ca\u7684\u6709\u6548\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u3002\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u5bf9\u4e8e\u7406\u89e3\u89e3\u91ca\u80cc\u540e\u7684\u7f6e\u4fe1\u5ea6\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8eLLMs\u7684\u81ea\u56de\u5f52\u751f\u6210\u8fc7\u7a0b\u548c\u533b\u5b66\u67e5\u8be2\u4e2d\u7684\u566a\u58f0\uff0c\u4e3a\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u751f\u6210\u6709\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540e\u5904\u7406\u4e14\u6a21\u578b\u65e0\u5173\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6846\u67b6\uff0c\u4e3a\u751f\u6210\u7684\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u63d0\u4f9b\u6709\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u4fdd\u8bc1\uff1b\u540c\u65f6\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9c81\u68d2\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5373\u4f7f\u5728\u566a\u58f0\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u4fdd\u6301\u6709\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u4fdd\u8bc1\u3002", "result": "\u5728\u95ee\u7b54\u4efb\u52a1\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u7406\u60f3\u6027\u80fd\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u586b\u8865\u4e86\u4e3a\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u63d0\u4f9b\u6709\u6548\u4e0d\u786e\u5b9a\u6027\u4fdd\u8bc1\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u548c\u65b9\u6cd5\u80fd\u591f\u53ef\u9760\u5730\u91cf\u5316LLMs\u751f\u6210\u89e3\u91ca\u7684\u7f6e\u4fe1\u5ea6\uff0c\u7279\u522b\u662f\u5728\u566a\u58f0\u73af\u5883\u4e0b\u4ecd\u80fd\u4fdd\u6301\u6709\u6548\u6027\u3002"}}
{"id": "2509.15409", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15409", "abs": "https://arxiv.org/abs/2509.15409", "authors": ["Yu Shee", "Anthony M. Smaldone", "Anton Morgunov", "Gregory W. Kyro", "Victor S. Batista"], "title": "FragmentRetro: A Quadratic Retrosynthetic Method Based on Fragmentation Algorithms", "comment": null, "summary": "Retrosynthesis, the process of deconstructing a target molecule into simpler\nprecursors, is crucial for computer-aided synthesis planning (CASP). Widely\nadopted tree-search methods often suffer from exponential computational\ncomplexity. In this work, we introduce FragmentRetro, a novel retrosynthetic\nmethod that leverages fragmentation algorithms, specifically BRICS and r-BRICS,\ncombined with stock-aware exploration and pattern fingerprint screening to\nachieve quadratic complexity. FragmentRetro recursively combines molecular\nfragments and verifies their presence in a building block set, providing sets\nof fragment combinations as retrosynthetic solutions. We present the first\nformal computational analysis of retrosynthetic methods, showing that tree\nsearch exhibits exponential complexity $O(b^h)$, DirectMultiStep scales as\n$O(h^6)$, and FragmentRetro achieves $O(h^2)$, where $h$ represents the number\nof heavy atoms in the target molecule and $b$ is the branching factor for tree\nsearch. Evaluations on PaRoutes, USPTO-190, and natural products demonstrate\nthat FragmentRetro achieves high solved rates with competitive runtime,\nincluding cases where tree search fails. The method benefits from fingerprint\nscreening, which significantly reduces substructure matching complexity. While\nFragmentRetro focuses on efficiently identifying fragment-based solutions\nrather than full reaction pathways, its computational advantages and ability to\ngenerate strategic starting candidates establish it as a powerful foundational\ncomponent for scalable and automated synthesis planning.", "AI": {"tldr": "FragmentRetro\u662f\u4e00\u79cd\u65b0\u7684\u9006\u5408\u6210\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5b50\u788e\u7247\u5316\u7b97\u6cd5\u5b9e\u73b0\u4e8c\u6b21\u590d\u6742\u5ea6\uff0c\u76f8\u6bd4\u4f20\u7edf\u6811\u641c\u7d22\u65b9\u6cd5\u7684\u6307\u6570\u590d\u6742\u5ea6\u6709\u663e\u8457\u6539\u8fdb", "motivation": "\u4f20\u7edf\u6811\u641c\u7d22\u65b9\u6cd5\u5728\u8ba1\u7b97\u673a\u8f85\u52a9\u5408\u6210\u89c4\u5212\u4e2d\u9762\u4e34\u6307\u6570\u7ea7\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u6311\u6218\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u9006\u5408\u6210\u5206\u6790\u65b9\u6cd5", "method": "\u7ed3\u5408BRICS\u548cr-BRICS\u788e\u7247\u5316\u7b97\u6cd5\uff0c\u91c7\u7528\u5e93\u5b58\u611f\u77e5\u63a2\u7d22\u548c\u6a21\u5f0f\u6307\u7eb9\u7b5b\u9009\uff0c\u901a\u8fc7\u9012\u5f52\u7ec4\u5408\u5206\u5b50\u7247\u6bb5\u5e76\u9a8c\u8bc1\u5176\u5728\u6784\u5efa\u5757\u96c6\u4e2d\u7684\u5b58\u5728", "result": "\u9996\u6b21\u5bf9\u9006\u5408\u6210\u65b9\u6cd5\u8fdb\u884c\u5f62\u5f0f\u5316\u8ba1\u7b97\u5206\u6790\uff0cFragmentRetro\u8fbe\u5230O(h\u00b2)\u590d\u6742\u5ea6\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u9ad8\u89e3\u51b3\u7387\u548c\u7ade\u4e89\u529b\u8fd0\u884c\u65f6\u95f4", "conclusion": "FragmentRetro\u4f5c\u4e3a\u8bc6\u522b\u57fa\u4e8e\u7247\u6bb5\u7684\u89e3\u51b3\u65b9\u6848\u7684\u9ad8\u6548\u65b9\u6cd5\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u5408\u6210\u89c4\u5212\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u57fa\u7840\u7ec4\u4ef6"}}
{"id": "2509.15419", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15419", "abs": "https://arxiv.org/abs/2509.15419", "authors": ["Claudio Benzoni", "Martina Langhals", "Martin Boeker", "Luise Modersohn", "M\u00e1t\u00e9 E. Maros"], "title": "Deep learning and abstractive summarisation for radiological reports: an empirical study for adapting the PEGASUS models' family with scarce data", "comment": "14 pages, 4 figures, and 3 tables", "summary": "Regardless of the rapid development of artificial intelligence, abstractive\nsummarisation is still challenging for sensitive and data-restrictive domains\nlike medicine. With the increasing number of imaging, the relevance of\nautomated tools for complex medical text summarisation is expected to become\nhighly relevant. In this paper, we investigated the adaptation via fine-tuning\nprocess of a non-domain-specific abstractive summarisation encoder-decoder\nmodel family, and gave insights to practitioners on how to avoid over- and\nunderfitting. We used PEGASUS and PEGASUS-X, on a medium-sized radiological\nreports public dataset. For each model, we comprehensively evaluated two\ndifferent checkpoints with varying sizes of the same training data. We\nmonitored the models' performances with lexical and semantic metrics during the\ntraining history on the fixed-size validation set. PEGASUS exhibited different\nphases, which can be related to epoch-wise double-descent, or\npeak-drop-recovery behaviour. For PEGASUS-X, we found that using a larger\ncheckpoint led to a performance detriment. This work highlights the challenges\nand risks of fine-tuning models with high expressivity when dealing with scarce\ntraining data, and lays the groundwork for future investigations into more\nrobust fine-tuning strategies for summarisation models in specialised domains.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u533b\u5b66\u9886\u57df\u4e2d\u4f7f\u7528PEGASUS\u548cPEGASUS-X\u6a21\u578b\u8fdb\u884c\u62bd\u8c61\u6458\u8981\u7684\u5fae\u8c03\u8fc7\u7a0b\uff0c\u5206\u6790\u4e86\u5728\u6709\u9650\u8bad\u7ec3\u6570\u636e\u4e0b\u907f\u514d\u8fc7\u62df\u5408\u548c\u6b20\u62df\u5408\u7684\u7b56\u7565\u3002", "motivation": "\u968f\u7740\u533b\u5b66\u5f71\u50cf\u6570\u91cf\u7684\u589e\u52a0\uff0c\u81ea\u52a8\u5316\u533b\u5b66\u6587\u672c\u6458\u8981\u5de5\u5177\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u5728\u654f\u611f\u548c\u6570\u636e\u53d7\u9650\u7684\u533b\u5b66\u9886\u57df\uff0c\u62bd\u8c61\u6458\u8981\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u4f7f\u7528PEGASUS\u548cPEGASUS-X\u6a21\u578b\u5bb6\u65cf\uff0c\u5728\u4e00\u4e2a\u4e2d\u7b49\u89c4\u6a21\u7684\u653e\u5c04\u5b66\u62a5\u544a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u8bc4\u4f30\u4e0d\u540c\u68c0\u67e5\u70b9\u548c\u8bad\u7ec3\u6570\u636e\u5927\u5c0f\u7684\u6027\u80fd\uff0c\u5e76\u4f7f\u7528\u8bcd\u6c47\u548c\u8bed\u4e49\u6307\u6807\u76d1\u63a7\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "PEGASUS\u8868\u73b0\u51fa\u4e0eepoch-wise\u53cc\u4e0b\u964d\u6216\u5cf0\u503c-\u4e0b\u964d-\u6062\u590d\u884c\u4e3a\u76f8\u5173\u7684\u4e0d\u540c\u9636\u6bb5\uff0c\u800cPEGASUS-X\u4f7f\u7528\u66f4\u5927\u7684\u68c0\u67e5\u70b9\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5f3a\u8c03\u4e86\u5728\u5904\u7406\u7a00\u7f3a\u8bad\u7ec3\u6570\u636e\u65f6\u5fae\u8c03\u9ad8\u8868\u8fbe\u80fd\u529b\u6a21\u578b\u7684\u6311\u6218\u548c\u98ce\u9669\uff0c\u4e3a\u672a\u6765\u5728\u4e13\u4e1a\u9886\u57df\u5f00\u53d1\u66f4\u7a33\u5065\u7684\u6458\u8981\u6a21\u578b\u5fae\u8c03\u7b56\u7565\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.15541", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15541", "abs": "https://arxiv.org/abs/2509.15541", "authors": ["Bronson Schoen", "Evgenia Nitishinskaya", "Mikita Balesni", "Axel H\u00f8jmark", "Felix Hofst\u00e4tter", "J\u00e9r\u00e9my Scheurer", "Alexander Meinke", "Jason Wolfe", "Teun van der Weij", "Alex Lloyd", "Nicholas Goldowsky-Dill", "Angela Fan", "Andrei Matveiakin", "Rusheb Shah", "Marcus Williams", "Amelia Glaese", "Boaz Barak", "Wojciech Zaremba", "Marius Hobbhahn"], "title": "Stress Testing Deliberative Alignment for Anti-Scheming Training", "comment": null, "summary": "Highly capable AI systems could secretly pursue misaligned goals -- what we\ncall \"scheming\". Because a scheming AI would deliberately try to hide its\nmisaligned goals and actions, measuring and mitigating scheming requires\ndifferent strategies than are typically used in ML. We propose that assessing\nanti-scheming interventions requires at least (1) testing propensity to scheme\non far out-of-distribution (OOD) tasks, (2) evaluating whether lack of scheming\nis driven by situational awareness, and (3) checking for robustness to\npre-existing misaligned goals. We use a broad category of \"covert actions\" --\nsuch as secretly breaking rules or intentionally underperforming in tests -- as\na proxy for scheming, and design evaluations for covert actions. We then\nstress-test deliberative alignment as a case study for anti-scheming. Across 26\nOOD evaluations (180+ environments), deliberative alignment reduces covert\naction rates (OpenAI o3: 13%->0.4%) but does not fully eliminate them. Our\nmitigation is also able to largely stop agents from pursuing a hidden goal\npreviously trained into the model, but we still find misbehavior after\nadditional red-teaming. We find that models' chain-of-thought (CoT) often\ndemonstrates awareness of being evaluated for alignment, and show causal\nevidence that this awareness decreases covert behavior, while unawareness\nincreases it. Therefore, we cannot exclude that the observed reductions in\ncovert action rates are at least partially driven by situational awareness.\nWhile we rely on human-legible CoT for training, studying situational\nawareness, and demonstrating clear evidence of misalignment, our ability to\nrely on this degrades as models continue to depart from reasoning in standard\nEnglish. We encourage research into alignment mitigations for scheming and\ntheir assessment, especially for the adversarial case of deceptive alignment,\nwhich this paper does not address.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86AI\u7cfb\u7edf\u53ef\u80fd\u79d8\u5bc6\u8ffd\u6c42\u672a\u5bf9\u9f50\u76ee\u6807\uff08\u79f0\u4e3a\"scheming\"\uff09\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u8bc4\u4f30\u53cdscheming\u5e72\u9884\u63aa\u65bd\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5ba1\u8bae\u5bf9\u9f50\u5728\u51cf\u5c11\u9690\u853d\u884c\u4e3a\u65b9\u9762\u7684\u6548\u679c\uff0c\u4f46\u53d1\u73b0\u60c5\u5883\u610f\u8bc6\u53ef\u80fd\u90e8\u5206\u9a71\u52a8\u4e86\u89c2\u5bdf\u5230\u7684\u6539\u8fdb\u3002", "motivation": "\u9ad8\u5ea6\u80fd\u529b\u7684AI\u7cfb\u7edf\u53ef\u80fd\u79d8\u5bc6\u8ffd\u6c42\u672a\u5bf9\u9f50\u76ee\u6807\uff0c\u800c\u4f20\u7edf\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u68c0\u6d4b\u8fd9\u79cd\u6545\u610f\u9690\u85cf\u7684\u884c\u4e3a\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u7684\u65b9\u6cd5\u6765\u6d4b\u91cf\u548c\u7f13\u89e3scheming\u95ee\u9898\u3002", "method": "\u4f7f\u7528\"\u9690\u853d\u884c\u4e3a\"\u4f5c\u4e3ascheming\u7684\u4ee3\u7406\u6307\u6807\uff0c\u8bbe\u8ba1\u4e8626\u4e2a\u5206\u5e03\u5916\u8bc4\u4f30\uff08180+\u73af\u5883\uff09\uff0c\u6d4b\u8bd5\u5ba1\u8bae\u5bf9\u9f50\u65b9\u6cd5\u7684\u6548\u679c\uff0c\u5e76\u901a\u8fc7\u94fe\u5f0f\u601d\u7ef4\u5206\u6790\u6a21\u578b\u7684\u60c5\u5883\u610f\u8bc6\u3002", "result": "\u5ba1\u8bae\u5bf9\u9f50\u663e\u8457\u51cf\u5c11\u4e86\u9690\u853d\u884c\u4e3a\u7387\uff08OpenAI o3\uff1a13%\u21920.4%\uff09\uff0c\u4f46\u672a\u80fd\u5b8c\u5168\u6d88\u9664\uff1b\u6a21\u578b\u80fd\u591f\u505c\u6b62\u8ffd\u6c42\u9884\u5148\u8bad\u7ec3\u7684\u5bf9\u9f50\u76ee\u6807\uff0c\u4f46\u5728\u989d\u5916\u7ea2\u961f\u6d4b\u8bd5\u540e\u4ecd\u53d1\u73b0\u4e0d\u5f53\u884c\u4e3a\uff1b\u94fe\u5f0f\u601d\u7ef4\u663e\u793a\u6a21\u578b\u7ecf\u5e38\u610f\u8bc6\u5230\u6b63\u5728\u63a5\u53d7\u5bf9\u9f50\u8bc4\u4f30\u3002", "conclusion": "\u867d\u7136\u5ba1\u8bae\u5bf9\u9f50\u6709\u6548\u51cf\u5c11\u4e86\u9690\u853d\u884c\u4e3a\uff0c\u4f46\u89c2\u5bdf\u5230\u7684\u6539\u8fdb\u53ef\u80fd\u90e8\u5206\u6e90\u4e8e\u6a21\u578b\u7684\u60c5\u5883\u610f\u8bc6\u800c\u975e\u771f\u6b63\u7684\u5bf9\u9f50\uff1b\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u9488\u5bf9\u6b3a\u9a97\u6027\u5bf9\u9f50\u7684\u7f13\u89e3\u63aa\u65bd\uff0c\u7279\u522b\u662f\u5f53\u6a21\u578b\u63a8\u7406\u4e0d\u518d\u4f7f\u7528\u6807\u51c6\u82f1\u8bed\u65f6\u3002"}}
{"id": "2509.15430", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15430", "abs": "https://arxiv.org/abs/2509.15430", "authors": ["Liuyuan Jiang", "Xiaodong Cui", "Brian Kingsbury", "Tianyi Chen", "Lisha Chen"], "title": "BiRQ: Bi-Level Self-Labeling Random Quantization for Self-Supervised Speech Recognition", "comment": "5 pages including reference", "summary": "Speech is a rich signal, and labeled audio-text pairs are costly, making\nself-supervised learning essential for scalable representation learning. A core\nchallenge in speech SSL is generating pseudo-labels that are both informative\nand efficient: strong labels, such as those used in HuBERT, improve downstream\nperformance but rely on external encoders and multi-stage pipelines, while\nefficient methods like BEST-RQ achieve simplicity at the cost of weaker labels.\nWe propose BiRQ, a bilevel SSL framework that combines the efficiency of\nBEST-RQ with the refinement benefits of HuBERT-style label enhancement. The key\nidea is to reuse part of the model itself as a pseudo-label generator:\nintermediate representations are discretized by a random-projection quantizer\nto produce enhanced labels, while anchoring labels derived directly from the\nraw input stabilize training and prevent collapse. Training is formulated as an\nefficient first-order bilevel optimization problem, solved end-to-end with\ndifferentiable Gumbel-softmax selection. This design eliminates the need for\nexternal label encoders, reduces memory cost, and enables iterative label\nrefinement in an end-to-end fashion. BiRQ consistently improves over BEST-RQ\nwhile maintaining low complexity and computational efficiency. We validate our\nmethod on various datasets, including 960-hour LibriSpeech, 150-hour AMI\nmeetings and 5,000-hour YODAS, demonstrating consistent gains over BEST-RQ.", "AI": {"tldr": "BiRQ\u662f\u4e00\u4e2a\u53cc\u5c42\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u4e86BEST-RQ\u7684\u6548\u7387\u548cHuBERT\u98ce\u683c\u6807\u7b7e\u589e\u5f3a\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u91cd\u7528\u6a21\u578b\u81ea\u8eab\u4f5c\u4e3a\u4f2a\u6807\u7b7e\u751f\u6210\u5668\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "motivation": "\u89e3\u51b3\u8bed\u97f3\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\u6807\u7b7e\u751f\u6210\u7684\u4e24\u96be\u95ee\u9898\uff1a\u5f3a\u6807\u7b7e\uff08\u5982HuBERT\uff09\u6027\u80fd\u597d\u4f46\u4f9d\u8d56\u5916\u90e8\u7f16\u7801\u5668\u548c\u591a\u9636\u6bb5\u6d41\u7a0b\uff0c\u800c\u9ad8\u6548\u65b9\u6cd5\uff08\u5982BEST-RQ\uff09\u7b80\u5355\u4f46\u6807\u7b7e\u8d28\u91cf\u8f83\u5f31\u3002", "method": "\u4f7f\u7528\u53cc\u5c42\u4f18\u5316\u6846\u67b6\uff0c\u4e2d\u95f4\u8868\u793a\u901a\u8fc7\u968f\u673a\u6295\u5f71\u91cf\u5316\u5668\u79bb\u6563\u5316\u751f\u6210\u589e\u5f3a\u6807\u7b7e\uff0c\u540c\u65f6\u4f7f\u7528\u539f\u59cb\u8f93\u5165\u7684\u951a\u5b9a\u6807\u7b7e\u7a33\u5b9a\u8bad\u7ec3\u3002\u91c7\u7528\u53ef\u5fae\u5206Gumbel-softmax\u9009\u62e9\u5b9e\u73b0\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\uff08960\u5c0f\u65f6LibriSpeech\u3001150\u5c0f\u65f6AMI\u4f1a\u8bae\u30015000\u5c0f\u65f6YODAS\uff09\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u6bd4BEST-RQ\u83b7\u5f97\u6301\u7eed\u6539\u8fdb\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u590d\u6742\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "BiRQ\u6846\u67b6\u6210\u529f\u7ed3\u5408\u4e86\u6548\u7387\u548c\u6807\u7b7e\u8d28\u91cf\u4f18\u52bf\uff0c\u65e0\u9700\u5916\u90e8\u6807\u7b7e\u7f16\u7801\u5668\uff0c\u964d\u4f4e\u5185\u5b58\u6210\u672c\uff0c\u652f\u6301\u7aef\u5230\u7aef\u8fed\u4ee3\u6807\u7b7e\u7cbe\u70bc\u3002"}}
{"id": "2509.15635", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15635", "abs": "https://arxiv.org/abs/2509.15635", "authors": ["Pan Tang", "Shixiang Tang", "Huanqi Pu", "Zhiqing Miao", "Zhixing Wang"], "title": "MicroRCA-Agent: Microservice Root Cause Analysis Method Based on Large Language Model Agents", "comment": "18 pages, 22 figures", "summary": "This paper presents MicroRCA-Agent, an innovative solution for microservice\nroot cause analysis based on large language model agents, which constructs an\nintelligent fault root cause localization system with multimodal data fusion.\nThe technical innovations are embodied in three key aspects: First, we combine\nthe pre-trained Drain log parsing algorithm with multi-level data filtering\nmechanism to efficiently compress massive logs into high-quality fault\nfeatures. Second, we employ a dual anomaly detection approach that integrates\nIsolation Forest unsupervised learning algorithms with status code validation\nto achieve comprehensive trace anomaly identification. Third, we design a\nstatistical symmetry ratio filtering mechanism coupled with a two-stage LLM\nanalysis strategy to enable full-stack phenomenon summarization across\nnode-service-pod hierarchies. The multimodal root cause analysis module\nleverages carefully designed cross-modal prompts to deeply integrate multimodal\nanomaly information, fully exploiting the cross-modal understanding and logical\nreasoning capabilities of large language models to generate structured analysis\nresults encompassing fault components, root cause descriptions, and reasoning\ntrace. Comprehensive ablation studies validate the complementary value of each\nmodal data and the effectiveness of the system architecture. The proposed\nsolution demonstrates superior performance in complex microservice fault\nscenarios, achieving a final score of 50.71. The code has been released at:\nhttps://github.com/tangpan360/MicroRCA-Agent.", "AI": {"tldr": "MicroRCA-Agent\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u7684\u5fae\u670d\u52a1\u6839\u56e0\u5206\u6790\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u6784\u5efa\u667a\u80fd\u6545\u969c\u6839\u56e0\u5b9a\u4f4d\u7cfb\u7edf\u3002", "motivation": "\u89e3\u51b3\u5fae\u670d\u52a1\u73af\u5883\u4e2d\u590d\u6742\u6545\u969c\u6839\u56e0\u5206\u6790\u7684\u6311\u6218\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8de8\u6a21\u6001\u7406\u89e3\u548c\u903b\u8f91\u63a8\u7406\u80fd\u529b\u6765\u63d0\u5347\u6545\u969c\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "1) \u7ed3\u5408\u9884\u8bad\u7ec3\u7684Drain\u65e5\u5fd7\u89e3\u6790\u7b97\u6cd5\u548c\u591a\u7ea7\u6570\u636e\u8fc7\u6ee4\u673a\u5236\u538b\u7f29\u6d77\u91cf\u65e5\u5fd7\uff1b2) \u91c7\u7528\u96c6\u6210\u9694\u79bb\u68ee\u6797\u65e0\u76d1\u7763\u5b66\u4e60\u548c\u72b6\u6001\u7801\u9a8c\u8bc1\u7684\u53cc\u91cd\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff1b3) \u8bbe\u8ba1\u7edf\u8ba1\u5bf9\u79f0\u6bd4\u8fc7\u6ee4\u673a\u5236\u548c\u4e24\u9636\u6bb5LLM\u5206\u6790\u7b56\u7565\u5b9e\u73b0\u5168\u6808\u73b0\u8c61\u603b\u7ed3\u3002", "result": "\u5728\u590d\u6742\u5fae\u670d\u52a1\u6545\u969c\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u6700\u7ec8\u5f97\u5206\u4e3a50.71\u3002\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5404\u6a21\u6001\u6570\u636e\u7684\u4e92\u8865\u4ef7\u503c\u548c\u7cfb\u7edf\u67b6\u6784\u7684\u6709\u6548\u6027\u3002", "conclusion": "MicroRCA-Agent\u901a\u8fc7\u521b\u65b0\u7684\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u548cLLM\u9a71\u52a8\u7684\u5206\u6790\u65b9\u6cd5\uff0c\u4e3a\u5fae\u670d\u52a1\u6839\u56e0\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5728GitHub\u5f00\u6e90\u3002"}}
{"id": "2509.15447", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15447", "abs": "https://arxiv.org/abs/2509.15447", "authors": ["Caitlin Cisar", "Emily Sheffield", "Joshua Drake", "Alden Harrell", "Subramanian Chidambaram", "Nikita Nangia", "Vinayak Arannil", "Alex Williams"], "title": "PILOT: Steering Synthetic Data Generation with Psychological & Linguistic Output Targeting", "comment": null, "summary": "Generative AI applications commonly leverage user personas as a steering\nmechanism for synthetic data generation, but reliance on natural language\nrepresentations forces models to make unintended inferences about which\nattributes to emphasize, limiting precise control over outputs. We introduce\nPILOT (Psychological and Linguistic Output Targeting), a two-phase framework\nfor steering large language models with structured psycholinguistic profiles.\nIn Phase 1, PILOT translates natural language persona descriptions into\nmultidimensional profiles with normalized scores across linguistic and\npsychological dimensions. In Phase 2, these profiles guide generation along\nmeasurable axes of variation. We evaluate PILOT across three state-of-the-art\nLLMs (Mistral Large 2, Deepseek-R1, LLaMA 3.3 70B) using 25 synthetic personas\nunder three conditions: Natural-language Persona Steering (NPS), Schema-Based\nSteering (SBS), and Hybrid Persona-Schema Steering (HPS). Results demonstrate\nthat schema-based approaches significantly reduce artificial-sounding persona\nrepetition while improving output coherence, with silhouette scores increasing\nfrom 0.098 to 0.237 and topic purity from 0.773 to 0.957. Our analysis reveals\na fundamental trade-off: SBS produces more concise outputs with higher topical\nconsistency, while NPS offers greater lexical diversity but reduced\npredictability. HPS achieves a balance between these extremes, maintaining\noutput variety while preserving structural consistency. Expert linguistic\nevaluation confirms that PILOT maintains high response quality across all\nconditions, with no statistically significant differences between steering\napproaches.", "AI": {"tldr": "PILOT\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u5fc3\u7406\u8bed\u8a00\u5b66\u6863\u6848\u6765\u7cbe\u786e\u63a7\u5236LLM\u751f\u6210\u5185\u5bb9\uff0c\u76f8\u6bd4\u4f20\u7edf\u81ea\u7136\u8bed\u8a00\u4eba\u7269\u63cf\u8ff0\u65b9\u6cd5\uff0c\u80fd\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u91cd\u590d\u5e76\u63d0\u9ad8\u8f93\u51fa\u8fde\u8d2f\u6027\u3002", "motivation": "\u4f20\u7edf\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u4eba\u7269\u63cf\u8ff0\u7684\u65b9\u6cd5\u4f1a\u8ba9\u6a21\u578b\u5bf9\u9700\u8981\u5f3a\u8c03\u7684\u5c5e\u6027\u505a\u51fa\u975e\u9884\u671f\u7684\u63a8\u65ad\uff0c\u9650\u5236\u4e86\u8f93\u51fa\u7684\u7cbe\u786e\u63a7\u5236\u3002", "method": "PILOT\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u5c06\u81ea\u7136\u8bed\u8a00\u4eba\u7269\u63cf\u8ff0\u8f6c\u6362\u4e3a\u591a\u7ef4\u5fc3\u7406\u8bed\u8a00\u5b66\u6863\u6848\uff1b\u7b2c\u4e8c\u9636\u6bb5\u7528\u8fd9\u4e9b\u6863\u6848\u6307\u5bfc\u751f\u6210\u8fc7\u7a0b\u3002\u8bc4\u4f30\u4e86\u4e09\u79cd\u65b9\u6cd5\uff1a\u81ea\u7136\u8bed\u8a00\u4eba\u7269\u5f15\u5bfc\u3001\u57fa\u4e8e\u6a21\u5f0f\u7684\u5f15\u5bfc\u548c\u6df7\u5408\u65b9\u6cd5\u3002", "result": "\u57fa\u4e8e\u6a21\u5f0f\u7684\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u5de5\u91cd\u590d\uff0c\u63d0\u9ad8\u4e86\u8f93\u51fa\u8fde\u8d2f\u6027\uff08\u8f6e\u5ed3\u5206\u6570\u4ece0.098\u63d0\u5347\u52300.237\uff0c\u4e3b\u9898\u7eaf\u5ea6\u4ece0.773\u63d0\u5347\u52300.957\uff09\u3002\u57fa\u4e8e\u6a21\u5f0f\u7684\u65b9\u6cd5\u4ea7\u751f\u66f4\u7b80\u6d01\u3001\u4e3b\u9898\u4e00\u81f4\u7684\u8f93\u51fa\uff0c\u800c\u81ea\u7136\u8bed\u8a00\u65b9\u6cd5\u63d0\u4f9b\u66f4\u5927\u7684\u8bcd\u6c47\u591a\u6837\u6027\u4f46\u53ef\u9884\u6d4b\u6027\u964d\u4f4e\u3002", "conclusion": "\u6df7\u5408\u65b9\u6cd5\u5728\u4fdd\u6301\u8f93\u51fa\u591a\u6837\u6027\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u7ed3\u6784\u4e00\u81f4\u6027\uff0cPILOT\u5728\u6240\u6709\u6761\u4ef6\u4e0b\u90fd\u4fdd\u6301\u4e86\u9ad8\u8d28\u91cf\u54cd\u5e94\uff0c\u4e0d\u540c\u5f15\u5bfc\u65b9\u6cd5\u4e4b\u95f4\u6ca1\u6709\u663e\u8457\u5dee\u5f02\u3002"}}
{"id": "2509.15690", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15690", "abs": "https://arxiv.org/abs/2509.15690", "authors": ["Weixuan Sun", "Jucai Zhai", "Dengfeng Liu", "Xin Zhang", "Xiaojun Wu", "Qiaobo Hao", "AIMgroup", "Yang Fang", "Jiuyang Tang"], "title": "CCrepairBench: A High-Fidelity Benchmark and Reinforcement Learning Framework for C++ Compilation Repair", "comment": null, "summary": "The automated repair of C++ compilation errors presents a significant\nchallenge, the resolution of which is critical for developer productivity.\nProgress in this domain is constrained by two primary factors: the scarcity of\nlarge-scale, high-fidelity datasets and the limitations of conventional\nsupervised methods, which often fail to generate semantically correct\npatches.This paper addresses these gaps by introducing a comprehensive\nframework with three core contributions. First, we present CCrepair, a novel,\nlarge-scale C++ compilation error dataset constructed through a sophisticated\ngenerate-and-verify pipeline. Second, we propose a Reinforcement Learning (RL)\nparadigm guided by a hybrid reward signal, shifting the focus from mere\ncompilability to the semantic quality of the fix. Finally, we establish the\nrobust, two-stage evaluation system providing this signal, centered on an\nLLM-as-a-Judge whose reliability has been rigorously validated against the\ncollective judgments of a panel of human experts. This integrated approach\naligns the training objective with generating high-quality, non-trivial patches\nthat are both syntactically and semantically correct. The effectiveness of our\napproach was demonstrated experimentally. Our RL-trained Qwen2.5-1.5B-Instruct\nmodel achieved performance comparable to a Qwen2.5-14B-Instruct model,\nvalidating the efficiency of our training paradigm. Our work provides the\nresearch community with a valuable new dataset and a more effective paradigm\nfor training and evaluating robust compilation repair models, paving the way\nfor more practical and reliable automated programming assistants.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u4fee\u590dC++\u7f16\u8bd1\u9519\u8bef\u7684\u7efc\u5408\u6846\u67b6\uff0c\u5305\u62ecCCrepair\u6570\u636e\u96c6\u3001\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4fee\u590d\u65b9\u6cd5\u548cLLM\u8bc4\u4f30\u7cfb\u7edf\u3002", "motivation": "\u89e3\u51b3C++\u7f16\u8bd1\u9519\u8bef\u81ea\u52a8\u4fee\u590d\u7684\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u7f3a\u4e4f\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u4f20\u7edf\u76d1\u7763\u65b9\u6cd5\u96be\u4ee5\u751f\u6210\u8bed\u4e49\u6b63\u786e\u7684\u8865\u4e01\u3002", "method": "\u6784\u5efaCCrepair\u6570\u636e\u96c6\uff0c\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\u7ed3\u5408\u6df7\u5408\u5956\u52b1\u4fe1\u53f7\uff0c\u5efa\u7acb\u4e24\u9636\u6bb5\u8bc4\u4f30\u7cfb\u7edf\u4f7f\u7528LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u3002", "result": "RL\u8bad\u7ec3\u7684Qwen2.5-1.5B\u6a21\u578b\u6027\u80fd\u4e0eQwen2.5-14B\u6a21\u578b\u76f8\u5f53\uff0c\u9a8c\u8bc1\u4e86\u8bad\u7ec3\u8303\u5f0f\u7684\u6548\u7387\u3002", "conclusion": "\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u65b0\u6570\u636e\u96c6\u548c\u66f4\u6709\u6548\u7684\u8bad\u7ec3\u8bc4\u4f30\u8303\u5f0f\uff0c\u4e3a\u5b9e\u7528\u53ef\u9760\u7684\u81ea\u52a8\u7f16\u7a0b\u52a9\u624b\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.15476", "categories": ["cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.15476", "abs": "https://arxiv.org/abs/2509.15476", "authors": ["Zhu Li", "Xiyuan Gao", "Yuqing Zhang", "Shekhar Nayak", "Matt Coler"], "title": "Evaluating Multimodal Large Language Models on Spoken Sarcasm Understanding", "comment": null, "summary": "Sarcasm detection remains a challenge in natural language understanding, as\nsarcastic intent often relies on subtle cross-modal cues spanning text, speech,\nand vision. While prior work has primarily focused on textual or visual-textual\nsarcasm, comprehensive audio-visual-textual sarcasm understanding remains\nunderexplored. In this paper, we systematically evaluate large language models\n(LLMs) and multimodal LLMs for sarcasm detection on English (MUStARD++) and\nChinese (MCSD 1.0) in zero-shot, few-shot, and LoRA fine-tuning settings. In\naddition to direct classification, we explore models as feature encoders,\nintegrating their representations through a collaborative gating fusion module.\nExperimental results show that audio-based models achieve the strongest\nunimodal performance, while text-audio and audio-vision combinations outperform\nunimodal and trimodal models. Furthermore, MLLMs such as Qwen-Omni show\ncompetitive zero-shot and fine-tuned performance. Our findings highlight the\npotential of MLLMs for cross-lingual, audio-visual-textual sarcasm\nunderstanding.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86LLMs\u548c\u591a\u6a21\u6001LLMs\u5728\u82f1\u8bed\u548c\u4e2d\u6587\u8bbd\u523a\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u63a2\u7d22\u4e86\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548cLoRA\u5fae\u8c03\u8bbe\u7f6e\uff0c\u53d1\u73b0\u97f3\u9891\u6a21\u578b\u5728\u5355\u6a21\u6001\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u6587\u672c-\u97f3\u9891\u548c\u97f3\u9891-\u89c6\u89c9\u7ec4\u5408\u4f18\u4e8e\u5355\u6a21\u6001\u548c\u4e09\u6a21\u6001\u6a21\u578b\u3002", "motivation": "\u8bbd\u523a\u68c0\u6d4b\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4e2d\u4ecd\u5177\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u8bbd\u523a\u610f\u56fe\u901a\u5e38\u4f9d\u8d56\u4e8e\u8de8\u6a21\u6001\u7684\u5fae\u5999\u7ebf\u7d22\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u6216\u89c6\u89c9-\u6587\u672c\u8bbd\u523a\uff0c\u800c\u5168\u9762\u7684\u97f3\u9891-\u89c6\u89c9-\u6587\u672c\u8bbd\u523a\u7406\u89e3\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5728\u82f1\u8bed\uff08MUStARD++\uff09\u548c\u4e2d\u6587\uff08MCSD 1.0\uff09\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30LLMs\u548c\u591a\u6a21\u6001LLMs\uff0c\u91c7\u7528\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548cLoRA\u5fae\u8c03\u8bbe\u7f6e\u3002\u9664\u4e86\u76f4\u63a5\u5206\u7c7b\uff0c\u8fd8\u63a2\u7d22\u6a21\u578b\u4f5c\u4e3a\u7279\u5f81\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u534f\u4f5c\u95e8\u63a7\u878d\u5408\u6a21\u5757\u6574\u5408\u5176\u8868\u793a\u3002", "result": "\u97f3\u9891\u6a21\u578b\u5728\u5355\u6a21\u6001\u4e2d\u8868\u73b0\u6700\u5f3a\uff0c\u6587\u672c-\u97f3\u9891\u548c\u97f3\u9891-\u89c6\u89c9\u7ec4\u5408\u4f18\u4e8e\u5355\u6a21\u6001\u548c\u4e09\u6a21\u6001\u6a21\u578b\u3002Qwen-Omni\u7b49MLLMs\u5728\u96f6\u6837\u672c\u548c\u5fae\u8c03\u8bbe\u7f6e\u4e2d\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u663e\u4e86MLLMs\u5728\u8de8\u8bed\u8a00\u3001\u97f3\u9891-\u89c6\u89c9-\u6587\u672c\u8bbd\u523a\u7406\u89e3\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.15730", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.15730", "abs": "https://arxiv.org/abs/2509.15730", "authors": ["Lukas Laakmann", "Seyyid A. Ciftci", "Christian Janiesch"], "title": "A Nascent Taxonomy of Machine Learning in Intelligent Robotic Process Automation", "comment": null, "summary": "Robotic process automation (RPA) is a lightweight approach to automating\nbusiness processes using software robots that emulate user actions at the\ngraphical user interface level. While RPA has gained popularity for its\ncost-effective and timely automation of rule-based, well-structured tasks, its\nsymbolic nature has inherent limitations when approaching more complex tasks\ncurrently performed by human agents. Machine learning concepts enabling\nintelligent RPA provide an opportunity to broaden the range of automatable\ntasks. In this paper, we conduct a literature review to explore the connections\nbetween RPA and machine learning and organize the joint concept intelligent RPA\ninto a taxonomy. Our taxonomy comprises the two meta-characteristics RPA-ML\nintegration and RPA-ML interaction. Together, they comprise eight dimensions:\narchitecture and ecosystem, capabilities, data basis, intelligence level, and\ntechnical depth of integration as well as deployment environment, lifecycle\nphase, and user-robot relation.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u63a2\u8ba8\u4e86RPA\u4e0e\u673a\u5668\u5b66\u4e60\u7684\u8054\u7cfb\uff0c\u63d0\u51fa\u4e86\u667a\u80fdRPA\u7684\u5206\u7c7b\u6cd5\uff0c\u5305\u542bRPA-ML\u96c6\u6210\u548cRPA-ML\u4ea4\u4e92\u4e24\u4e2a\u5143\u7279\u5f81\uff0c\u5171\u516b\u4e2a\u7ef4\u5ea6\u3002", "motivation": "\u4f20\u7edfRPA\u5728\u7b26\u53f7\u5316\u6027\u8d28\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u5904\u7406\u590d\u6742\u4efb\u52a1\u3002\u673a\u5668\u5b66\u4e60\u6982\u5ff5\u4e3a\u667a\u80fdRPA\u63d0\u4f9b\u4e86\u6269\u5c55\u81ea\u52a8\u5316\u4efb\u52a1\u8303\u56f4\u7684\u673a\u4f1a\u3002", "method": "\u91c7\u7528\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5206\u6790RPA\u4e0e\u673a\u5668\u5b66\u4e60\u7684\u8054\u7cfb\uff0c\u5e76\u6784\u5efa\u667a\u80fdRPA\u7684\u5206\u7c7b\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u5305\u542b\u516b\u4e2a\u7ef4\u5ea6\u7684\u667a\u80fdRPA\u5206\u7c7b\u6cd5\uff1a\u67b6\u6784\u4e0e\u751f\u6001\u7cfb\u7edf\u3001\u80fd\u529b\u3001\u6570\u636e\u57fa\u7840\u3001\u667a\u80fd\u6c34\u5e73\u3001\u6280\u672f\u96c6\u6210\u6df1\u5ea6\u3001\u90e8\u7f72\u73af\u5883\u3001\u751f\u547d\u5468\u671f\u9636\u6bb5\u548c\u7528\u6237-\u673a\u5668\u4eba\u5173\u7cfb\u3002", "conclusion": "\u667a\u80fdRPA\u5206\u7c7b\u6cd5\u4e3a\u7406\u89e3\u548c\u7ec4\u7ec7RPA\u4e0e\u673a\u5668\u5b66\u4e60\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u6269\u5c55RPA\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2509.15478", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15478", "abs": "https://arxiv.org/abs/2509.15478", "authors": ["Madison Van Doren", "Casey Ford", "Emily Dix"], "title": "Red Teaming Multimodal Language Models: Evaluating Harm Across Prompt Modalities and Models", "comment": null, "summary": "Multimodal large language models (MLLMs) are increasingly used in real world\napplications, yet their safety under adversarial conditions remains\nunderexplored. This study evaluates the harmlessness of four leading MLLMs\n(GPT-4o, Claude Sonnet 3.5, Pixtral 12B, and Qwen VL Plus) when exposed to\nadversarial prompts across text-only and multimodal formats. A team of 26 red\nteamers generated 726 prompts targeting three harm categories: illegal\nactivity, disinformation, and unethical behaviour. These prompts were submitted\nto each model, and 17 annotators rated 2,904 model outputs for harmfulness\nusing a 5-point scale. Results show significant differences in vulnerability\nacross models and modalities. Pixtral 12B exhibited the highest rate of harmful\nresponses (~62%), while Claude Sonnet 3.5 was the most resistant (~10%).\nContrary to expectations, text-only prompts were slightly more effective at\nbypassing safety mechanisms than multimodal ones. Statistical analysis\nconfirmed that both model type and input modality were significant predictors\nof harmfulness. These findings underscore the urgent need for robust,\nmultimodal safety benchmarks as MLLMs are deployed more widely.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u56db\u79cd\u4e3b\u6d41\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08GPT-4o\u3001Claude Sonnet 3.5\u3001Pixtral 12B\u548cQwen VL Plus\uff09\u5728\u5bf9\u6297\u6027\u63d0\u793a\u4e0b\u7684\u5b89\u5168\u6027\u8868\u73b0\uff0c\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u548c\u6a21\u6001\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u5e7f\u6cdb\u90e8\u7f72\uff0c\u5176\u5728\u5bf9\u6297\u6761\u4ef6\u4e0b\u7684\u5b89\u5168\u6027\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "method": "26\u540d\u7ea2\u961f\u6210\u5458\u751f\u6210726\u4e2a\u9488\u5bf9\u4e09\u7c7b\u5371\u5bb3\uff08\u975e\u6cd5\u6d3b\u52a8\u3001\u865a\u5047\u4fe1\u606f\u3001\u4e0d\u9053\u5fb7\u884c\u4e3a\uff09\u7684\u63d0\u793a\uff0c\u63d0\u4ea4\u7ed9\u56db\u4e2a\u6a21\u578b\uff0c17\u540d\u6807\u6ce8\u8005\u5bf92,904\u4e2a\u6a21\u578b\u8f93\u51fa\u8fdb\u884c5\u7ea7\u5371\u5bb3\u6027\u8bc4\u5206\u3002", "result": "Pixtral 12B\u6709\u5bb3\u54cd\u5e94\u7387\u6700\u9ad8\uff08\u7ea662%\uff09\uff0cClaude Sonnet 3.5\u6700\u6297\u653b\u51fb\uff08\u7ea610%\uff09\uff1b\u6587\u672c\u63d0\u793a\u6bd4\u591a\u6a21\u6001\u63d0\u793a\u7565\u6709\u6548\uff1b\u6a21\u578b\u7c7b\u578b\u548c\u8f93\u5165\u6a21\u6001\u90fd\u662f\u5371\u5bb3\u6027\u7684\u663e\u8457\u9884\u6d4b\u56e0\u7d20\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u968f\u7740MLLM\u7684\u5e7f\u6cdb\u90e8\u7f72\uff0c\u8feb\u5207\u9700\u8981\u5efa\u7acb\u7a33\u5065\u7684\u591a\u6a21\u6001\u5b89\u5168\u57fa\u51c6\u3002"}}
{"id": "2509.15780", "categories": ["cs.AI", "cs.DL"], "pdf": "https://arxiv.org/pdf/2509.15780", "abs": "https://arxiv.org/abs/2509.15780", "authors": ["Natallia Kokash", "Bernard de Bono", "Tom Gillespie"], "title": "Ontology Creation and Management Tools: the Case of Anatomical Connectivity", "comment": "14 pages", "summary": "We are developing infrastructure to support researchers in mapping data\nrelated to the peripheral nervous system and other physiological systems, with\nan emphasis on their relevance to the organs under investigation. The nervous\nsystem, a complex network of nerves and ganglia, plays a critical role in\ncoordinating and transmitting signals throughout the body. To aid in this, we\nhave created ApiNATOMY, a framework for the topological and semantic\nrepresentation of multiscale physiological circuit maps. ApiNATOMY integrates a\nKnowledge Representation (KR) model and a suite of Knowledge Management (KM)\ntools. The KR model enables physiology experts to easily capture interactions\nbetween anatomical entities, while the KM tools help modelers convert\nhigh-level abstractions into detailed models of physiological processes, which\ncan be integrated with external ontologies and knowledge graphs.", "AI": {"tldr": "\u5f00\u53d1ApiNATOMY\u6846\u67b6\uff0c\u7528\u4e8e\u5916\u5468\u795e\u7ecf\u7cfb\u7edf\u548c\u5176\u4ed6\u751f\u7406\u7cfb\u7edf\u7684\u591a\u5c3a\u5ea6\u751f\u7406\u56de\u8def\u56fe\u62d3\u6251\u548c\u8bed\u4e49\u8868\u793a", "motivation": "\u652f\u6301\u7814\u7a76\u4eba\u5458\u6620\u5c04\u4e0e\u5468\u56f4\u795e\u7ecf\u7cfb\u7edf\u548c\u5176\u4ed6\u751f\u7406\u7cfb\u7edf\u76f8\u5173\u7684\u6570\u636e\uff0c\u5f3a\u8c03\u5b83\u4eec\u5bf9\u7814\u7a76\u5668\u5b98\u7684\u76f8\u5173\u6027", "method": "\u521b\u5efa\u5305\u542b\u77e5\u8bc6\u8868\u793a\u6a21\u578b\u548c\u77e5\u8bc6\u7ba1\u7406\u5de5\u5177\u5957\u4ef6\u7684\u6846\u67b6\uff0cKR\u6a21\u578b\u4fbf\u4e8e\u751f\u7406\u5b66\u4e13\u5bb6\u6355\u83b7\u89e3\u5256\u5b9e\u4f53\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0cKM\u5de5\u5177\u5e2e\u52a9\u5efa\u6a21\u8005\u5c06\u9ad8\u7ea7\u62bd\u8c61\u8f6c\u6362\u4e3a\u8be6\u7ec6\u7684\u751f\u7406\u8fc7\u7a0b\u6a21\u578b", "result": "\u5f00\u53d1\u4e86\u80fd\u591f\u4e0e\u5916\u90e8\u672c\u4f53\u548c\u77e5\u8bc6\u56fe\u8c31\u96c6\u6210\u7684\u591a\u5c3a\u5ea6\u751f\u7406\u56de\u8def\u56fe\u8868\u793a\u6846\u67b6", "conclusion": "ApiNATOMY\u4e3a\u751f\u7406\u7cfb\u7edf\u7684\u591a\u5c3a\u5ea6\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u77e5\u8bc6\u8868\u793a\u548c\u7ba1\u7406\u5de5\u5177"}}
{"id": "2509.15485", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15485", "abs": "https://arxiv.org/abs/2509.15485", "authors": ["Ahmed Abdou"], "title": "mucAI at BAREC Shared Task 2025: Towards Uncertainty Aware Arabic Readability Assessment", "comment": null, "summary": "We present a simple, model-agnostic post-processing technique for\nfine-grained Arabic readability classification in the BAREC 2025 Shared Task\n(19 ordinal levels). Our method applies conformal prediction to generate\nprediction sets with coverage guarantees, then computes weighted averages using\nsoftmax-renormalized probabilities over the conformal sets. This\nuncertainty-aware decoding improves Quadratic Weighted Kappa (QWK) by reducing\nhigh-penalty misclassifications to nearer levels. Our approach shows consistent\nQWK improvements of 1-3 points across different base models. In the strict\ntrack, our submission achieves QWK scores of 84.9\\%(test) and 85.7\\% (blind\ntest) for sentence level, and 73.3\\% for document level. For Arabic educational\nassessment, this enables human reviewers to focus on a handful of plausible\nlevels, combining statistical guarantees with practical usability.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u3001\u6a21\u578b\u65e0\u5173\u7684\u540e\u5904\u7406\u6280\u672f\uff0c\u7528\u4e8e\u963f\u62c9\u4f2f\u8bed\u7ec6\u7c92\u5ea6\u53ef\u8bfb\u6027\u5206\u7c7b\uff0c\u901a\u8fc7\u4fdd\u5f62\u9884\u6d4b\u751f\u6210\u5177\u6709\u8986\u76d6\u4fdd\u8bc1\u7684\u9884\u6d4b\u96c6\uff0c\u5e76\u4f7f\u7528\u8f6f\u6700\u5927\u91cd\u5f52\u4e00\u5316\u6982\u7387\u8ba1\u7b97\u52a0\u6743\u5e73\u5747\u503c\uff0c\u4ece\u800c\u63d0\u9ad8\u4e8c\u6b21\u52a0\u6743Kappa\u5206\u6570\u3002", "motivation": "\u89e3\u51b3\u963f\u62c9\u4f2f\u8bed\u53ef\u8bfb\u6027\u5206\u7c7b\u4e2d\u9ad8\u60e9\u7f5a\u8bef\u5206\u7c7b\u95ee\u9898\uff0c\u4e3a\u6559\u80b2\u8bc4\u4f30\u63d0\u4f9b\u7edf\u8ba1\u4fdd\u8bc1\u548c\u5b9e\u9645\u53ef\u7528\u6027\uff0c\u4f7f\u4eba\u5de5\u8bc4\u5ba1\u80fd\u591f\u4e13\u6ce8\u4e8e\u5c11\u6570\u5408\u7406\u7ea7\u522b\u3002", "method": "\u5e94\u7528\u4fdd\u5f62\u9884\u6d4b\u751f\u6210\u5177\u6709\u8986\u76d6\u4fdd\u8bc1\u7684\u9884\u6d4b\u96c6\uff0c\u7136\u540e\u4f7f\u7528\u8f6f\u6700\u5927\u91cd\u5f52\u4e00\u5316\u6982\u7387\u5728\u4fdd\u5f62\u96c6\u4e0a\u8ba1\u7b97\u52a0\u6743\u5e73\u5747\u503c\uff0c\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u89e3\u7801\u3002", "result": "\u5728\u4e0d\u540c\u57fa\u7840\u6a21\u578b\u4e0a\u4e00\u81f4\u63d0\u9ad8QWK\u5206\u65701-3\u70b9\uff0c\u5728\u4e25\u683c\u8d5b\u9053\u4e2d\uff0c\u53e5\u5b50\u7ea7\u522b\u6d4b\u8bd5\u548c\u76f2\u6d4b\u5206\u522b\u8fbe\u523084.9%\u548c85.7%\u7684QWK\uff0c\u6587\u6863\u7ea7\u522b\u8fbe\u523073.3%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u89e3\u7801\u6709\u6548\u51cf\u5c11\u9ad8\u60e9\u7f5a\u8bef\u5206\u7c7b\uff0c\u5c06\u8bef\u5206\u7c7b\u9650\u5236\u5728\u76f8\u8fd1\u7ea7\u522b\uff0c\u4e3a\u963f\u62c9\u4f2f\u8bed\u6559\u80b2\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7edf\u8ba1\u4fdd\u8bc1\u548c\u5b9e\u9645\u53ef\u7528\u6027\u3002"}}
{"id": "2509.15786", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.15786", "abs": "https://arxiv.org/abs/2509.15786", "authors": ["Nan Li", "Bo Kang", "Tijl De Bie"], "title": "Building Data-Driven Occupation Taxonomies: A Bottom-Up Multi-Stage Approach via Semantic Clustering and Multi-Agent Collaboration", "comment": null, "summary": "Creating robust occupation taxonomies, vital for applications ranging from\njob recommendation to labor market intelligence, is challenging. Manual\ncuration is slow, while existing automated methods are either not adaptive to\ndynamic regional markets (top-down) or struggle to build coherent hierarchies\nfrom noisy data (bottom-up). We introduce CLIMB (CLusterIng-based Multi-agent\ntaxonomy Builder), a framework that fully automates the creation of\nhigh-quality, data-driven taxonomies from raw job postings. CLIMB uses global\nsemantic clustering to distill core occupations, then employs a\nreflection-based multi-agent system to iteratively build a coherent hierarchy.\nOn three diverse, real-world datasets, we show that CLIMB produces taxonomies\nthat are more coherent and scalable than existing methods and successfully\ncapture unique regional characteristics. We release our code and datasets at\nhttps://anonymous.4open.science/r/CLIMB.", "AI": {"tldr": "CLIMB\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u6784\u5efa\u9ad8\u8d28\u91cf\u804c\u4e1a\u5206\u7c7b\u6cd5\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u5168\u5c40\u8bed\u4e49\u805a\u7c7b\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4ece\u539f\u59cb\u62db\u8058\u4fe1\u606f\u4e2d\u521b\u5efa\u6570\u636e\u9a71\u52a8\u7684\u5206\u7c7b\u4f53\u7cfb", "motivation": "\u73b0\u6709\u804c\u4e1a\u5206\u7c7b\u6cd5\u6784\u5efa\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u4eba\u5de5\u6784\u5efa\u901f\u5ea6\u6162\uff0c\u81ea\u52a8\u5316\u65b9\u6cd5\u8981\u4e48\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u533a\u57df\u5e02\u573a\uff08\u81ea\u4e0a\u800c\u4e0b\uff09\uff0c\u8981\u4e48\u96be\u4ee5\u4ece\u566a\u58f0\u6570\u636e\u4e2d\u6784\u5efa\u8fde\u8d2f\u7684\u5c42\u6b21\u7ed3\u6784\uff08\u81ea\u4e0b\u800c\u4e0a\uff09", "method": "CLIMB\u6846\u67b6\u9996\u5148\u4f7f\u7528\u5168\u5c40\u8bed\u4e49\u805a\u7c7b\u63d0\u70bc\u6838\u5fc3\u804c\u4e1a\uff0c\u7136\u540e\u91c7\u7528\u57fa\u4e8e\u53cd\u601d\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8fed\u4ee3\u6784\u5efa\u8fde\u8d2f\u7684\u5c42\u6b21\u7ed3\u6784", "result": "\u5728\u4e09\u4e2a\u4e0d\u540c\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u8868\u660e\uff0cCLIMB\u751f\u6210\u7684\u5206\u7c7b\u6cd5\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u8fde\u8d2f\u3001\u53ef\u6269\u5c55\uff0c\u5e76\u80fd\u6210\u529f\u6355\u6349\u72ec\u7279\u7684\u533a\u57df\u7279\u5f81", "conclusion": "CLIMB\u80fd\u591f\u5b8c\u5168\u81ea\u52a8\u5316\u5730\u4ece\u539f\u59cb\u62db\u8058\u4fe1\u606f\u4e2d\u521b\u5efa\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u9a71\u52a8\u5206\u7c7b\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027"}}
{"id": "2509.15515", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15515", "abs": "https://arxiv.org/abs/2509.15515", "authors": ["Hantao Yang", "Hong Xie", "Defu Lian", "Enhong Chen"], "title": "LLM Cache Bandit Revisited: Addressing Query Heterogeneity for Cost-Effective LLM Inference", "comment": null, "summary": "This paper revisits the LLM cache bandit problem, with a special focus on\naddressing the query heterogeneity for cost-effective LLM inference. Previous\nworks often assume uniform query sizes. Heterogeneous query sizes introduce a\ncombinatorial structure for cache selection, making the cache replacement\nprocess more computationally and statistically challenging. We treat optimal\ncache selection as a knapsack problem and employ an accumulation-based strategy\nto effectively balance computational overhead and cache updates. In theoretical\nanalysis, we prove that the regret of our algorithm achieves an $O(\\sqrt{MNT})$\nbound, improving the coefficient of $\\sqrt{MN}$ compared to the $O(MN\\sqrt{T})$\nresult in Berkeley, where $N$ is the total number of queries and $M$ is the\ncache size. Additionally, we also provide a problem-dependent bound, which was\nabsent in previous works. The experiment rely on real-world data show that our\nalgorithm reduces the total cost by approximately 12\\%.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86LLM\u7f13\u5b58\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u7279\u522b\u5173\u6ce8\u89e3\u51b3\u67e5\u8be2\u5f02\u8d28\u6027\u4ee5\u5b9e\u73b0\u6210\u672c\u6548\u76ca\u7684LLM\u63a8\u7406\u3002\u901a\u8fc7\u5c06\u6700\u4f18\u7f13\u5b58\u9009\u62e9\u5efa\u6a21\u4e3a\u80cc\u5305\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u7d2f\u79ef\u7684\u7b56\u7565\u6765\u5e73\u8861\u8ba1\u7b97\u5f00\u9500\u548c\u7f13\u5b58\u66f4\u65b0\uff0c\u5728\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u7b97\u6cd5\u9057\u61be\u754c\u4e3aO(\u221aMNT)\uff0c\u4f18\u4e8e\u4e4b\u524d\u7684O(MN\u221aT)\u7ed3\u679c\uff0c\u5b9e\u9a8c\u663e\u793a\u603b\u6210\u672c\u964d\u4f4e\u7ea612%\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u901a\u5e38\u5047\u8bbe\u67e5\u8be2\u5927\u5c0f\u5747\u5300\uff0c\u4f46\u5b9e\u9645\u4e2d\u67e5\u8be2\u5927\u5c0f\u5b58\u5728\u5f02\u8d28\u6027\uff0c\u8fd9\u4e3a\u7f13\u5b58\u9009\u62e9\u5f15\u5165\u4e86\u7ec4\u5408\u7ed3\u6784\uff0c\u4f7f\u5f97\u7f13\u5b58\u66ff\u6362\u8fc7\u7a0b\u5728\u8ba1\u7b97\u548c\u7edf\u8ba1\u4e0a\u66f4\u5177\u6311\u6218\u6027\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u67e5\u8be2\u5f02\u8d28\u6027\u5e26\u6765\u7684\u95ee\u9898\uff0c\u4ee5\u5b9e\u73b0\u66f4\u6210\u672c\u6548\u76ca\u7684LLM\u63a8\u7406\u3002", "method": "\u5c06\u6700\u4f18\u7f13\u5b58\u9009\u62e9\u5efa\u6a21\u4e3a\u80cc\u5305\u95ee\u9898\uff0c\u91c7\u7528\u57fa\u4e8e\u7d2f\u79ef\u7684\u7b56\u7565\u6765\u6709\u6548\u5e73\u8861\u8ba1\u7b97\u5f00\u9500\u548c\u7f13\u5b58\u66f4\u65b0\u3002\u8be5\u65b9\u6cd5\u8003\u8651\u4e86\u67e5\u8be2\u5927\u5c0f\u7684\u5f02\u8d28\u6027\uff0c\u901a\u8fc7\u7ec4\u5408\u4f18\u5316\u65b9\u6cd5\u89e3\u51b3\u7f13\u5b58\u66ff\u6362\u7684\u6311\u6218\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u7b97\u6cd5\u9057\u61be\u754c\u4e3aO(\u221aMNT)\uff0c\u76f8\u6bd4\u4e4b\u524d\u5de5\u4f5c\u7684O(MN\u221aT)\u7ed3\u679c\u6539\u8fdb\u4e86\u221aMN\u7cfb\u6570\u3002\u5b9e\u9a8c\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u6570\u636e\uff0c\u663e\u793a\u7b97\u6cd5\u5c06\u603b\u6210\u672c\u964d\u4f4e\u4e86\u7ea612%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86LLM\u7f13\u5b58\u4e2d\u7684\u67e5\u8be2\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u80cc\u5305\u95ee\u9898\u5efa\u6a21\u548c\u7d2f\u79ef\u7b56\u7565\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u4e3a\u6210\u672c\u6548\u76ca\u7684LLM\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.15848", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15848", "abs": "https://arxiv.org/abs/2509.15848", "authors": ["Giovanni De Gasperis", "Sante Dino Facchini"], "title": "A Comparative Study of Rule-Based and Data-Driven Approaches in Industrial Monitoring", "comment": null, "summary": "Industrial monitoring systems, especially when deployed in Industry 4.0\nenvironments, are experiencing a shift in paradigm from traditional rule-based\narchitectures to data-driven approaches leveraging machine learning and\nartificial intelligence. This study presents a comparison between these two\nmethodologies, analyzing their respective strengths, limitations, and\napplication scenarios, and proposes a basic framework to evaluate their key\nproperties. Rule-based systems offer high interpretability, deterministic\nbehavior, and ease of implementation in stable environments, making them ideal\nfor regulated industries and safety-critical applications. However, they face\nchallenges with scalability, adaptability, and performance in complex or\nevolving contexts. Conversely, data-driven systems excel in detecting hidden\nanomalies, enabling predictive maintenance and dynamic adaptation to new\nconditions. Despite their high accuracy, these models face challenges related\nto data availability, explainability, and integration complexity. The paper\nsuggests hybrid solutions as a possible promising direction, combining the\ntransparency of rule-based logic with the analytical power of machine learning.\nOur hypothesis is that the future of industrial monitoring lies in intelligent,\nsynergic systems that leverage both expert knowledge and data-driven insights.\nThis dual approach enhances resilience, operational efficiency, and trust,\npaving the way for smarter and more flexible industrial environments.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u5de5\u4e1a\u76d1\u63a7\u7cfb\u7edf\u4e2d\u57fa\u4e8e\u89c4\u5219\u7684\u67b6\u6784\u4e0e\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u63d0\u51fa\u4e86\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u5efa\u8bae\u6df7\u5408\u89e3\u51b3\u65b9\u6848\u4f5c\u4e3a\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u5de5\u4e1a4.0\u73af\u5883\u4e0b\u76d1\u63a7\u7cfb\u7edf\u6b63\u4ece\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u67b6\u6784\u5411\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u8f6c\u53d8\uff0c\u9700\u8981\u7cfb\u7edf\u6bd4\u8f83\u4e24\u79cd\u65b9\u6cd5\u7684\u4f18\u52a3\u5e76\u63a2\u7d22\u878d\u5408\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5206\u6790\u57fa\u4e8e\u89c4\u5219\u7cfb\u7edf\u548c\u6570\u636e\u9a71\u52a8\u7cfb\u7edf\u7684\u5173\u952e\u7279\u6027\uff0c\u5efa\u7acb\u8bc4\u4f30\u6846\u67b6\uff0c\u6bd4\u8f83\u5b83\u4eec\u5728\u53ef\u89e3\u91ca\u6027\u3001\u9002\u5e94\u6027\u3001\u6027\u80fd\u7b49\u65b9\u9762\u7684\u5dee\u5f02\u3002", "result": "\u57fa\u4e8e\u89c4\u5219\u7cfb\u7edf\u5728\u7a33\u5b9a\u73af\u5883\u4e2d\u5177\u6709\u9ad8\u53ef\u89e3\u91ca\u6027\u548c\u786e\u5b9a\u6027\u4f18\u52bf\uff0c\u800c\u6570\u636e\u9a71\u52a8\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u66f4\u597d\u4f46\u9762\u4e34\u53ef\u89e3\u91ca\u6027\u6311\u6218\u3002\u6df7\u5408\u65b9\u6848\u88ab\u8bc1\u660e\u662f\u6700\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "conclusion": "\u5de5\u4e1a\u76d1\u63a7\u7684\u672a\u6765\u5728\u4e8e\u667a\u80fd\u534f\u540c\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e13\u5bb6\u77e5\u8bc6\u548c\u6570\u636e\u9a71\u52a8\u6d1e\u5bdf\uff0c\u4ee5\u589e\u5f3a\u5f39\u6027\u3001\u8fd0\u8425\u6548\u7387\u548c\u4fe1\u4efb\uff0c\u5b9e\u73b0\u66f4\u667a\u80fd\u7075\u6d3b\u7684\u5de5\u4e1a\u73af\u5883\u3002"}}
{"id": "2509.15518", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15518", "abs": "https://arxiv.org/abs/2509.15518", "authors": ["Siyang Wu", "Zhewei Sun"], "title": "How do Language Models Generate Slang: A Systematic Comparison between Human and Machine-Generated Slang Usages", "comment": null, "summary": "Slang is a commonly used type of informal language that poses a daunting\nchallenge to NLP systems. Recent advances in large language models (LLMs),\nhowever, have made the problem more approachable. While LLM agents are becoming\nmore widely applied to intermediary tasks such as slang detection and slang\ninterpretation, their generalizability and reliability are heavily dependent on\nwhether these models have captured structural knowledge about slang that align\nwell with human attested slang usages. To answer this question, we contribute a\nsystematic comparison between human and machine-generated slang usages. Our\nevaluative framework focuses on three core aspects: 1) Characteristics of the\nusages that reflect systematic biases in how machines perceive slang, 2)\nCreativity reflected by both lexical coinages and word reuses employed by the\nslang usages, and 3) Informativeness of the slang usages when used as\ngold-standard examples for model distillation. By comparing human-attested\nslang usages from the Online Slang Dictionary (OSD) and slang generated by\nGPT-4o and Llama-3, we find significant biases in how LLMs perceive slang. Our\nresults suggest that while LLMs have captured significant knowledge about the\ncreative aspects of slang, such knowledge does not align with humans\nsufficiently to enable LLMs for extrapolative tasks such as linguistic\nanalyses.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6bd4\u8f83\u4e86\u4eba\u7c7b\u548c\u673a\u5668\u751f\u6210\u7684\u4fda\u8bed\u7528\u6cd5\uff0c\u53d1\u73b0LLMs\u5728\u4fda\u8bed\u7406\u89e3\u4e0a\u5b58\u5728\u663e\u8457\u504f\u89c1\uff0c\u867d\u7136\u638c\u63e1\u4e86\u4fda\u8bed\u7684\u521b\u9020\u6027\u65b9\u9762\uff0c\u4f46\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u4e0d\u591f\u4e00\u81f4\uff0c\u9650\u5236\u4e86\u5176\u5728\u8bed\u8a00\u5206\u6790\u7b49\u5916\u63a8\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u4fda\u8bed\u4f5c\u4e3a\u975e\u6b63\u5f0f\u8bed\u8a00\u5bf9NLP\u7cfb\u7edf\u6784\u6210\u6311\u6218\uff0c\u9700\u8981\u8bc4\u4f30LLMs\u662f\u5426\u638c\u63e1\u4e86\u4e0e\u4eba\u7c7b\u4fda\u8bed\u7528\u6cd5\u4e00\u81f4\u7684\u7ed3\u6784\u6027\u77e5\u8bc6\uff0c\u4ee5\u786e\u5b9a\u5176\u5728\u4fda\u8bed\u68c0\u6d4b\u548c\u89e3\u91ca\u7b49\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5efa\u7acb\u8bc4\u4f30\u6846\u67b6\uff0c\u4ece\u4e09\u4e2a\u6838\u5fc3\u65b9\u9762\u6bd4\u8f83\u4eba\u7c7b\u8ba4\u8bc1\u7684\u4fda\u8bed\u7528\u6cd5\uff08\u6765\u81ea\u5728\u7ebf\u4fda\u8bed\u8bcd\u5178OSD\uff09\u4e0eGPT-4o\u548cLlama-3\u751f\u6210\u7684\u4fda\u8bed\uff1a1\uff09\u53cd\u6620\u673a\u5668\u611f\u77e5\u4fda\u8bed\u7cfb\u7edf\u6027\u504f\u89c1\u7684\u7528\u6cd5\u7279\u5f81\uff1b2\uff09\u4fda\u8bed\u7528\u6cd5\u4e2d\u4f53\u73b0\u7684\u8bcd\u6c47\u521b\u65b0\u548c\u8bcd\u8bed\u91cd\u7528\u7684\u521b\u9020\u6027\uff1b3\uff09\u4f5c\u4e3a\u6a21\u578b\u84b8\u998f\u91d1\u6807\u51c6\u793a\u4f8b\u7684\u4fe1\u606f\u91cf\u3002", "result": "\u53d1\u73b0LLMs\u5728\u611f\u77e5\u4fda\u8bed\u65f6\u5b58\u5728\u663e\u8457\u504f\u89c1\uff0c\u867d\u7136\u638c\u63e1\u4e86\u4fda\u8bed\u7684\u521b\u9020\u6027\u77e5\u8bc6\uff0c\u4f46\u8fd9\u4e9b\u77e5\u8bc6\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u4e0d\u591f\u4e00\u81f4\uff0c\u65e0\u6cd5\u652f\u6301\u5916\u63a8\u6027\u4efb\u52a1\u5982\u8bed\u8a00\u5206\u6790\u3002", "conclusion": "LLMs\u5728\u4fda\u8bed\u7406\u89e3\u65b9\u9762\u4ecd\u6709\u5c40\u9650\uff0c\u5176\u77e5\u8bc6\u7ed3\u6784\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u5b58\u5728\u5dee\u5f02\uff0c\u9650\u5236\u4e86\u5728\u9700\u8981\u6df1\u5ea6\u8bed\u8a00\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6548\u679c\u3002"}}
{"id": "2509.15957", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.15957", "abs": "https://arxiv.org/abs/2509.15957", "authors": ["Kanato Masayoshi", "Masahiro Hashimoto", "Ryoichi Yokoyama", "Naoki Toda", "Yoshifumi Uwamino", "Shogo Fukuda", "Ho Namkoong", "Masahiro Jinzaki"], "title": "EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by Large Language Models via Model Context Protocol", "comment": null, "summary": "Background: Large language models (LLMs) show promise in medicine, but their\ndeployment in hospitals is limited by restricted access to electronic health\nrecord (EHR) systems. The Model Context Protocol (MCP) enables integration\nbetween LLMs and external tools.\n  Objective: To evaluate whether an LLM connected to an EHR database via MCP\ncan autonomously retrieve clinically relevant information in a real hospital\nsetting.\n  Methods: We developed EHR-MCP, a framework of custom MCP tools integrated\nwith the hospital EHR database, and used GPT-4.1 through a LangGraph ReAct\nagent to interact with it. Six tasks were tested, derived from use cases of the\ninfection control team (ICT). Eight patients discussed at ICT conferences were\nretrospectively analyzed. Agreement with physician-generated gold standards was\nmeasured.\n  Results: The LLM consistently selected and executed the correct MCP tools.\nExcept for two tasks, all tasks achieved near-perfect accuracy. Performance was\nlower in the complex task requiring time-dependent calculations. Most errors\narose from incorrect arguments or misinterpretation of tool results. Responses\nfrom EHR-MCP were reliable, though long and repetitive data risked exceeding\nthe context window.\n  Conclusions: LLMs can retrieve clinical data from an EHR via MCP tools in a\nreal hospital setting, achieving near-perfect performance in simple tasks while\nhighlighting challenges in complex ones. EHR-MCP provides an infrastructure for\nsecure, consistent data access and may serve as a foundation for hospital AI\nagents. Future work should extend beyond retrieval to reasoning, generation,\nand clinical impact assessment, paving the way for effective integration of\ngenerative AI into clinical practice.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u901a\u8fc7Model Context Protocol\uff08MCP\uff09\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u533b\u9662\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u6570\u636e\u5e93\u96c6\u6210\uff0c\u5728\u771f\u5b9e\u533b\u9662\u73af\u5883\u4e2d\u81ea\u4e3b\u68c0\u7d22\u4e34\u5e8a\u76f8\u5173\u4fe1\u606f\u7684\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u9886\u57df\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5728\u533b\u9662\u90e8\u7f72\u53d7\u5230\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7cfb\u7edf\u8bbf\u95ee\u9650\u5236\u7684\u5236\u7ea6\u3002MCP\u534f\u8bae\u80fd\u591f\u5b9e\u73b0LLM\u4e0e\u5916\u90e8\u5de5\u5177\u7684\u96c6\u6210\uff0c\u4e3a\u533b\u9662AI\u5e94\u7528\u63d0\u4f9b\u53ef\u80fd\u3002", "method": "\u5f00\u53d1\u4e86EHR-MCP\u6846\u67b6\uff0c\u5c06\u81ea\u5b9a\u4e49MCP\u5de5\u5177\u4e0e\u533b\u9662EHR\u6570\u636e\u5e93\u96c6\u6210\uff0c\u4f7f\u7528GPT-4.1\u901a\u8fc7LangGraph ReAct\u4ee3\u7406\u8fdb\u884c\u4ea4\u4e92\u3002\u6d4b\u8bd5\u4e866\u4e2a\u611f\u67d3\u63a7\u5236\u56e2\u961f\u76f8\u5173\u7684\u4efb\u52a1\uff0c\u56de\u987e\u6027\u5206\u6790\u4e868\u540d\u60a3\u8005\uff0c\u5e76\u4e0e\u533b\u751f\u751f\u6210\u7684\u91d1\u6807\u51c6\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "LLM\u80fd\u591f\u4e00\u81f4\u9009\u62e9\u5e76\u6267\u884c\u6b63\u786e\u7684MCP\u5de5\u5177\uff0c\u9664\u4e24\u4e2a\u4efb\u52a1\u5916\uff0c\u6240\u6709\u4efb\u52a1\u90fd\u8fbe\u5230\u63a5\u8fd1\u5b8c\u7f8e\u7684\u51c6\u786e\u7387\u3002\u5728\u9700\u8981\u65f6\u95f4\u76f8\u5173\u8ba1\u7b97\u7684\u590d\u6742\u4efb\u52a1\u4e2d\u6027\u80fd\u8f83\u4f4e\u3002\u5927\u591a\u6570\u9519\u8bef\u6e90\u4e8e\u53c2\u6570\u4e0d\u6b63\u786e\u6216\u5de5\u5177\u7ed3\u679c\u8bef\u89e3\u3002", "conclusion": "LLM\u53ef\u4ee5\u901a\u8fc7MCP\u5de5\u5177\u4eceEHR\u4e2d\u68c0\u7d22\u4e34\u5e8a\u6570\u636e\uff0c\u5728\u7b80\u5355\u4efb\u52a1\u4e2d\u8fbe\u5230\u63a5\u8fd1\u5b8c\u7f8e\u7684\u6027\u80fd\uff0c\u4f46\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u9762\u4e34\u6311\u6218\u3002EHR-MCP\u4e3a\u5b89\u5168\u3001\u4e00\u81f4\u7684\u6570\u636e\u8bbf\u95ee\u63d0\u4f9b\u4e86\u57fa\u7840\u8bbe\u65bd\uff0c\u53ef\u4f5c\u4e3a\u533b\u9662AI\u4ee3\u7406\u7684\u57fa\u7840\u3002\u672a\u6765\u5de5\u4f5c\u5e94\u6269\u5c55\u5230\u63a8\u7406\u3001\u751f\u6210\u548c\u4e34\u5e8a\u5f71\u54cd\u8bc4\u4f30\u3002"}}
{"id": "2509.15549", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15549", "abs": "https://arxiv.org/abs/2509.15549", "authors": ["Chunguang Zhao", "Yilun Liu", "Pufan Zeng", "Yuanchang Luo", "Shimin Tao", "Minggui He", "Weibin Meng", "Song Xu", "Ziang Chen", "Chen Liu", "Hongxia Ma", "Li Zhang", "Boxing Chen", "Daimeng Wei"], "title": "A method for improving multilingual quality and diversity of instruction fine-tuning datasets", "comment": null, "summary": "Multilingual Instruction Fine-Tuning (IFT) is essential for enabling large\nlanguage models (LLMs) to generalize effectively across diverse linguistic and\ncultural contexts. However, the scarcity of high-quality multilingual training\ndata and corresponding building method remains a critical bottleneck. While\ndata selection has shown promise in English settings, existing methods often\nfail to generalize across languages due to reliance on simplistic heuristics or\nlanguage-specific assumptions. In this work, we introduce Multilingual Data\nQuality and Diversity (M-DaQ), a novel method for improving LLMs\nmultilinguality, by selecting high-quality and semantically diverse\nmultilingual IFT samples. We further conduct the first systematic investigation\nof the Superficial Alignment Hypothesis (SAH) in multilingual setting.\nEmpirical results across 18 languages demonstrate that models fine-tuned with\nM-DaQ method achieve significant performance gains over vanilla baselines over\n60% win rate. Human evaluations further validate these gains, highlighting the\nincrement of cultural points in the response. We release the M-DaQ code to\nsupport future research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86M-DaQ\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u9ad8\u8d28\u91cf\u548c\u8bed\u4e49\u591a\u6837\u6027\u7684\u591a\u8bed\u8a00IFT\u6837\u672c\u6765\u63d0\u5347LLMs\u7684\u591a\u8bed\u8a00\u80fd\u529b\uff0c\u5e76\u572818\u79cd\u8bed\u8a00\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u591a\u8bed\u8a00\u6307\u4ee4\u5fae\u8c03\u5bf9LLMs\u5728\u4e0d\u540c\u8bed\u8a00\u548c\u6587\u5316\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u591a\u8bed\u8a00\u8bad\u7ec3\u6570\u636e\u548c\u76f8\u5e94\u6784\u5efa\u65b9\u6cd5\uff0c\u73b0\u6709\u65b9\u6cd5\u56e0\u4f9d\u8d56\u7b80\u5355\u542f\u53d1\u5f0f\u6216\u8bed\u8a00\u7279\u5b9a\u5047\u8bbe\u800c\u96be\u4ee5\u8de8\u8bed\u8a00\u6cdb\u5316\u3002", "method": "\u63d0\u51faM-DaQ\u65b9\u6cd5\uff0c\u9009\u62e9\u9ad8\u8d28\u91cf\u548c\u8bed\u4e49\u591a\u6837\u6027\u7684\u591a\u8bed\u8a00IFT\u6837\u672c\uff0c\u5e76\u9996\u6b21\u5728 multilingual setting \u4e2d\u7cfb\u7edf\u7814\u7a76Superficial Alignment Hypothesis\u3002", "result": "\u572818\u79cd\u8bed\u8a00\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528M-DaQ\u65b9\u6cd5\u5fae\u8c03\u7684\u6a21\u578b\u76f8\u6bd4\u57fa\u7ebf\u83b7\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u80dc\u7387\u8d85\u8fc760%\uff0c\u4eba\u5de5\u8bc4\u4f30\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u63d0\u5347\uff0c\u7279\u522b\u662f\u6587\u5316\u70b9\u8868\u8fbe\u7684\u589e\u52a0\u3002", "conclusion": "M-DaQ\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86LLMs\u7684\u591a\u8bed\u8a00\u80fd\u529b\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2509.15962", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15962", "abs": "https://arxiv.org/abs/2509.15962", "authors": ["Sander Schildermans", "Chang Tian", "Ying Jiao", "Marie-Francine Moens"], "title": "Structured Information for Improving Spatial Relationships in Text-to-Image Generation", "comment": "text-to-image generation, structured information, spatial\n  relationship", "summary": "Text-to-image (T2I) generation has advanced rapidly, yet faithfully capturing\nspatial relationships described in natural language prompts remains a major\nchallenge. Prior efforts have addressed this issue through prompt optimization,\nspatially grounded generation, and semantic refinement. This work introduces a\nlightweight approach that augments prompts with tuple-based structured\ninformation, using a fine-tuned language model for automatic conversion and\nseamless integration into T2I pipelines. Experimental results demonstrate\nsubstantial improvements in spatial accuracy, without compromising overall\nimage quality as measured by Inception Score. Furthermore, the automatically\ngenerated tuples exhibit quality comparable to human-crafted tuples. This\nstructured information provides a practical and portable solution to enhance\nspatial relationships in T2I generation, addressing a key limitation of current\nlarge-scale generative systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u5fae\u8c03\u7684\u8bed\u8a00\u6a21\u578b\u5c06\u6587\u672c\u63d0\u793a\u8f6c\u6362\u4e3a\u57fa\u4e8e\u5143\u7ec4\u7684\u7ed3\u6784\u5316\u4fe1\u606f\uff0c\u4ee5\u589e\u5f3a\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u7a7a\u95f4\u5173\u7cfb\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6280\u672f\u867d\u7136\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u5728\u51c6\u786e\u6355\u6349\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u4e2d\u7684\u7a7a\u95f4\u5173\u7cfb\u65b9\u9762\u4ecd\u5b58\u5728\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u63d0\u793a\u4f18\u5316\u3001\u7a7a\u95f4\u57fa\u7840\u751f\u6210\u548c\u8bed\u4e49\u7ec6\u5316\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u8c03\u7684\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u5c06\u6587\u672c\u63d0\u793a\u8f6c\u6362\u4e3a\u57fa\u4e8e\u5143\u7ec4\u7684\u7ed3\u6784\u5316\u4fe1\u606f\uff0c\u5e76\u5c06\u5176\u65e0\u7f1d\u96c6\u6210\u5230\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6d41\u7a0b\u4e2d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7a7a\u95f4\u51c6\u786e\u6027\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3Inception Score\u8861\u91cf\u7684\u6574\u4f53\u56fe\u50cf\u8d28\u91cf\u3002\u81ea\u52a8\u751f\u6210\u7684\u5143\u7ec4\u8d28\u91cf\u4e0e\u4eba\u5de5\u5236\u4f5c\u7684\u5143\u7ec4\u76f8\u5f53\u3002", "conclusion": "\u8fd9\u79cd\u7ed3\u6784\u5316\u4fe1\u606f\u4e3a\u589e\u5f3a\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u7a7a\u95f4\u5173\u7cfb\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u79fb\u690d\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u5927\u89c4\u6a21\u751f\u6210\u7cfb\u7edf\u7684\u5173\u952e\u9650\u5236\u3002"}}
{"id": "2509.15550", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15550", "abs": "https://arxiv.org/abs/2509.15550", "authors": ["Xiaowei Zhu", "Yubing Ren", "Fang Fang", "Qingfeng Tan", "Shi Wang", "Yanan Cao"], "title": "DNA-DetectLLM: Unveiling AI-Generated Text via a DNA-Inspired Mutation-Repair Paradigm", "comment": "NeurIPS 2025 Spotlight", "summary": "The rapid advancement of large language models (LLMs) has blurred the line\nbetween AI-generated and human-written text. This progress brings societal\nrisks such as misinformation, authorship ambiguity, and intellectual property\nconcerns, highlighting the urgent need for reliable AI-generated text detection\nmethods. However, recent advances in generative language modeling have resulted\nin significant overlap between the feature distributions of human-written and\nAI-generated text, blurring classification boundaries and making accurate\ndetection increasingly challenging. To address the above challenges, we propose\na DNA-inspired perspective, leveraging a repair-based process to directly and\ninterpretably capture the intrinsic differences between human-written and\nAI-generated text. Building on this perspective, we introduce DNA-DetectLLM, a\nzero-shot detection method for distinguishing AI-generated and human-written\ntext. The method constructs an ideal AI-generated sequence for each input,\niteratively repairs non-optimal tokens, and quantifies the cumulative repair\neffort as an interpretable detection signal. Empirical evaluations demonstrate\nthat our method achieves state-of-the-art detection performance and exhibits\nstrong robustness against various adversarial attacks and input lengths.\nSpecifically, DNA-DetectLLM achieves relative improvements of 5.55% in AUROC\nand 2.08% in F1 score across multiple public benchmark datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7DNA\u4fee\u590d\u542f\u53d1\u7684\u96f6\u6837\u672c\u68c0\u6d4b\u65b9\u6cd5DNA-DetectLLM\uff0c\u7528\u4e8e\u533a\u5206AI\u751f\u6210\u6587\u672c\u548c\u4eba\u7c7b\u5199\u4f5c\u6587\u672c\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0cAI\u751f\u6210\u6587\u672c\u4e0e\u4eba\u7c7b\u5199\u4f5c\u6587\u672c\u4e4b\u95f4\u7684\u754c\u9650\u65e5\u76ca\u6a21\u7cca\uff0c\u8fd9\u5e26\u6765\u4e86\u9519\u8bef\u4fe1\u606f\u3001\u4f5c\u8005\u8eab\u4efd\u6a21\u7cca\u548c\u77e5\u8bc6\u4ea7\u6743\u7b49\u793e\u4f1a\u98ce\u9669\uff0c\u8feb\u5207\u9700\u8981\u53ef\u9760\u7684AI\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDNA\u542f\u53d1\u7684\u89c6\u89d2\uff0c\u901a\u8fc7\u4fee\u590d\u8fc7\u7a0b\u76f4\u63a5\u6355\u6349AI\u751f\u6210\u6587\u672c\u4e0e\u4eba\u7c7b\u5199\u4f5c\u6587\u672c\u7684\u5185\u5728\u5dee\u5f02\u3002DNA-DetectLLM\u65b9\u6cd5\u4e3a\u6bcf\u4e2a\u8f93\u5165\u6784\u5efa\u7406\u60f3\u7684AI\u751f\u6210\u5e8f\u5217\uff0c\u8fed\u4ee3\u4fee\u590d\u975e\u6700\u4f18\u6807\u8bb0\uff0c\u5e76\u5c06\u7d2f\u79ef\u4fee\u590d\u52aa\u529b\u91cf\u5316\u4e3a\u53ef\u89e3\u91ca\u7684\u68c0\u6d4b\u4fe1\u53f7\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u5bf9\u5404\u79cd\u5bf9\u6297\u653b\u51fb\u548c\u8f93\u5165\u957f\u5ea6\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\u3002\u5728\u591a\u4e2a\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cAUROC\u76f8\u5bf9\u63d0\u53475.55%\uff0cF1\u5206\u6570\u76f8\u5bf9\u63d0\u53472.08%\u3002", "conclusion": "DNA-DetectLLM\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u53ef\u89e3\u91ca\u7684AI\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4e3a\u89e3\u51b3AI\u751f\u6210\u6587\u672c\u5e26\u6765\u7684\u793e\u4f1a\u98ce\u9669\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.16058", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16058", "abs": "https://arxiv.org/abs/2509.16058", "authors": ["Krati Saxena", "Federico Jurado Ruiz", "Guido Manzi", "Dianbo Liu", "Alex Lamb"], "title": "Attention Schema-based Attention Control (ASAC): A Cognitive-Inspired Approach for Attention Management in Transformers", "comment": null, "summary": "Attention mechanisms have become integral in AI, significantly enhancing\nmodel performance and scalability by drawing inspiration from human cognition.\nConcurrently, the Attention Schema Theory (AST) in cognitive science posits\nthat individuals manage their attention by creating a model of the attention\nitself, effectively allocating cognitive resources. Inspired by AST, we\nintroduce ASAC (Attention Schema-based Attention Control), which integrates the\nattention schema concept into artificial neural networks. Our initial\nexperiments focused on embedding the ASAC module within transformer\narchitectures. This module employs a Vector-Quantized Variational AutoEncoder\n(VQVAE) as both an attention abstractor and controller, facilitating precise\nattention management. By explicitly modeling attention allocation, our approach\naims to enhance system efficiency. We demonstrate ASAC's effectiveness in both\nthe vision and NLP domains, highlighting its ability to improve classification\naccuracy and expedite the learning process. Our experiments with vision\ntransformers across various datasets illustrate that the attention controller\nnot only boosts classification accuracy but also accelerates learning.\nFurthermore, we have demonstrated the model's robustness and generalization\ncapabilities across noisy and out-of-distribution datasets. In addition, we\nhave showcased improved performance in multi-task settings. Quick experiments\nreveal that the attention schema-based module enhances resilience to\nadversarial attacks, optimizes attention to improve learning efficiency, and\nfacilitates effective transfer learning and learning from fewer examples. These\npromising results establish a connection between cognitive science and machine\nlearning, shedding light on the efficient utilization of attention mechanisms\nin AI systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faASAC\uff08\u57fa\u4e8e\u6ce8\u610f\u529b\u6a21\u5f0f\u7684\u6ce8\u610f\u529b\u63a7\u5236\uff09\uff0c\u5c06\u8ba4\u77e5\u79d1\u5b66\u4e2d\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\u7406\u8bba\u878d\u5165\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7VQVAE\u4f5c\u4e3a\u6ce8\u610f\u529b\u62bd\u8c61\u5668\u548c\u63a7\u5236\u5668\uff0c\u5728\u89c6\u89c9\u548cNLP\u4efb\u52a1\u4e2d\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u6027\u548c\u5b66\u4e60\u6548\u7387\u3002", "motivation": "\u53d7\u8ba4\u77e5\u79d1\u5b66\u4e2d\u6ce8\u610f\u529b\u6a21\u5f0f\u7406\u8bba\uff08AST\uff09\u542f\u53d1\uff0c\u8be5\u7406\u8bba\u8ba4\u4e3a\u4eba\u7c7b\u901a\u8fc7\u521b\u5efa\u6ce8\u610f\u529b\u6a21\u578b\u6765\u7ba1\u7406\u6ce8\u610f\u529b\u5206\u914d\u3002\u4f5c\u8005\u5e0c\u671b\u5c06\u8fd9\u4e00\u8ba4\u77e5\u673a\u5236\u5f15\u5165AI\u7cfb\u7edf\uff0c\u4ee5\u63d0\u5347\u6ce8\u610f\u529b\u673a\u5236\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "\u5728Transformer\u67b6\u6784\u4e2d\u5d4c\u5165ASAC\u6a21\u5757\uff0c\u4f7f\u7528\u5411\u91cf\u91cf\u5316\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VQVAE\uff09\u4f5c\u4e3a\u6ce8\u610f\u529b\u62bd\u8c61\u5668\u548c\u63a7\u5236\u5668\uff0c\u663e\u5f0f\u5efa\u6a21\u6ce8\u610f\u529b\u5206\u914d\u673a\u5236\u3002", "result": "\u5728\u89c6\u89c9\u548cNLP\u9886\u57df\u7684\u5b9e\u9a8c\u4e2d\uff0cASAC\u63d0\u5347\u4e86\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u52a0\u901f\u4e86\u5b66\u4e60\u8fc7\u7a0b\uff0c\u5c55\u73b0\u51fa\u5bf9\u566a\u58f0\u548c\u5206\u5e03\u5916\u6570\u636e\u7684\u9c81\u68d2\u6027\uff0c\u5728\u591a\u4efb\u52a1\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u589e\u5f3a\u4e86\u5bf9\u6297\u653b\u51fb\u7684\u62b5\u6297\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u8ba4\u77e5\u79d1\u5b66\u4e0e\u673a\u5668\u5b66\u4e60\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u4e3aAI\u7cfb\u7edf\u4e2d\u6ce8\u610f\u529b\u673a\u5236\u7684\u9ad8\u6548\u5229\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5c55\u793a\u4e86\u57fa\u4e8e\u6ce8\u610f\u529b\u6a21\u5f0f\u7684\u63a7\u5236\u65b9\u6cd5\u5728\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.15556", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15556", "abs": "https://arxiv.org/abs/2509.15556", "authors": ["Ping Guo", "Yubing Ren", "Binbin Liu", "Fengze Liu", "Haobin Lin", "Yifan Zhang", "Bingni Zhang", "Taifeng Wang", "Yin Zheng"], "title": "Exploring Polyglot Harmony: On Multilingual Data Allocation for Large Language Models Pretraining", "comment": null, "summary": "Large language models (LLMs) have become integral to a wide range of\napplications worldwide, driving an unprecedented global demand for effective\nmultilingual capabilities. Central to achieving robust multilingual performance\nis the strategic allocation of language proportions within training corpora.\nHowever, determining optimal language ratios is highly challenging due to\nintricate cross-lingual interactions and sensitivity to dataset scale. This\npaper introduces Climb (Cross-Lingual Interaction-aware Multilingual\nBalancing), a novel framework designed to systematically optimize multilingual\ndata allocation. At its core, Climb introduces a cross-lingual\ninteraction-aware language ratio, explicitly quantifying each language's\neffective allocation by capturing inter-language dependencies. Leveraging this\nratio, Climb proposes a principled two-step optimization procedure--first\nequalizing marginal benefits across languages, then maximizing the magnitude of\nthe resulting language allocation vectors--significantly simplifying the\ninherently complex multilingual optimization problem. Extensive experiments\nconfirm that Climb can accurately measure cross-lingual interactions across\nvarious multilingual settings. LLMs trained with Climb-derived proportions\nconsistently achieve state-of-the-art multilingual performance, even achieving\ncompetitive performance with open-sourced LLMs trained with more tokens.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Climb\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316\u8de8\u8bed\u8a00\u4ea4\u4e92\u6765\u4f18\u5316\u591a\u8bed\u8a00\u8bad\u7ec3\u6570\u636e\u7684\u5206\u914d\u6bd4\u4f8b\uff0c\u4ece\u800c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u8bed\u8a00\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5168\u7403\u5e94\u7528\u7684\u666e\u53ca\uff0c\u5bf9\u591a\u8bed\u8a00\u80fd\u529b\u7684\u9700\u6c42\u6025\u5267\u589e\u957f\u3002\u7136\u800c\uff0c\u786e\u5b9a\u8bad\u7ec3\u8bed\u6599\u4e2d\u4e0d\u540c\u8bed\u8a00\u7684\u6700\u4f73\u6bd4\u4f8b\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u6d89\u53ca\u590d\u6742\u7684\u8de8\u8bed\u8a00\u4ea4\u4e92\u548c\u5bf9\u6570\u636e\u96c6\u89c4\u6a21\u7684\u654f\u611f\u6027\u3002", "method": "Climb\u6846\u67b6\u5f15\u5165\u8de8\u8bed\u8a00\u4ea4\u4e92\u611f\u77e5\u7684\u8bed\u8a00\u6bd4\u4f8b\uff0c\u91cf\u5316\u6bcf\u79cd\u8bed\u8a00\u7684\u6709\u6548\u5206\u914d\uff0c\u6355\u6349\u8bed\u8a00\u95f4\u4f9d\u8d56\u5173\u7cfb\u3002\u91c7\u7528\u4e24\u6b65\u9aa4\u4f18\u5316\u8fc7\u7a0b\uff1a\u9996\u5148\u5e73\u8861\u5404\u8bed\u8a00\u7684\u8fb9\u9645\u6536\u76ca\uff0c\u7136\u540e\u6700\u5927\u5316\u8bed\u8a00\u5206\u914d\u5411\u91cf\u7684\u5e45\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660eClimb\u80fd\u51c6\u786e\u6d4b\u91cf\u5404\u79cd\u591a\u8bed\u8a00\u8bbe\u7f6e\u4e0b\u7684\u8de8\u8bed\u8a00\u4ea4\u4e92\u3002\u4f7f\u7528Climb\u4f18\u5316\u6bd4\u4f8b\u8bad\u7ec3\u7684LLMs\u5728\u591a\u8bed\u8a00\u6027\u80fd\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u751a\u81f3\u4e0e\u4f7f\u7528\u66f4\u591atoken\u8bad\u7ec3\u7684\u5f00\u6e90LLMs\u7ade\u4e89\u3002", "conclusion": "Climb\u6846\u67b6\u4e3a\u591a\u8bed\u8a00\u6570\u636e\u5206\u914d\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u7b80\u5316\u4e86\u590d\u6742\u7684\u591a\u8bed\u8a00\u4f18\u5316\u95ee\u9898\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u8bed\u8a00\u6027\u80fd\u3002"}}
{"id": "2509.15560", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15560", "abs": "https://arxiv.org/abs/2509.15560", "authors": ["Gary Lupyan", "Hunter Gentry", "Martin Zettersten"], "title": "How important is language for human-like intelligence?", "comment": null, "summary": "We use language to communicate our thoughts. But is language merely the\nexpression of thoughts, which are themselves produced by other, nonlinguistic\nparts of our minds? Or does language play a more transformative role in human\ncognition, allowing us to have thoughts that we otherwise could (or would) not\nhave? Recent developments in artificial intelligence (AI) and cognitive science\nhave reinvigorated this old question. We argue that language may hold the key\nto the emergence of both more general AI systems and central aspects of human\nintelligence. We highlight two related properties of language that make it such\na powerful tool for developing domain--general abilities. First, language\noffers compact representations that make it easier to represent and reason\nabout many abstract concepts (e.g., exact numerosity). Second, these compressed\nrepresentations are the iterated output of collective minds. In learning a\nlanguage, we learn a treasure trove of culturally evolved abstractions. Taken\ntogether, these properties mean that a sufficiently powerful learning system\nexposed to language--whether biological or artificial--learns a compressed\nmodel of the world, reverse engineering many of the conceptual and causal\nstructures that support human (and human-like) thought.", "AI": {"tldr": "\u8bed\u8a00\u4e0d\u4ec5\u662f\u601d\u60f3\u7684\u8868\u8fbe\u5de5\u5177\uff0c\u66f4\u5728\u4eba\u7c7b\u8ba4\u77e5\u4e2d\u626e\u6f14\u53d8\u9769\u6027\u89d2\u8272\uff0c\u53ef\u80fd\u662f\u901a\u7528\u4eba\u5de5\u667a\u80fd\u548c\u4eba\u7c7b\u667a\u80fd\u6838\u5fc3\u65b9\u9762\u51fa\u73b0\u7684\u5173\u952e\u3002\u8bed\u8a00\u901a\u8fc7\u7d27\u51d1\u8868\u5f81\u548c\u96c6\u4f53\u667a\u6167\u7684\u8fed\u4ee3\u8f93\u51fa\uff0c\u4f7f\u5b66\u4e60\u7cfb\u7edf\u80fd\u591f\u9006\u5411\u5de5\u7a0b\u652f\u6301\u4eba\u7c7b\u601d\u7ef4\u7684\u6982\u5ff5\u548c\u56e0\u679c\u7ed3\u6784\u3002", "motivation": "\u91cd\u65b0\u63a2\u8ba8\u8bed\u8a00\u5728\u8ba4\u77e5\u4e2d\u7684\u89d2\u8272\u2014\u2014\u662f\u5355\u7eaf\u8868\u8fbe\u601d\u60f3\u7684\u5de5\u5177\uff0c\u8fd8\u662f\u80fd\u591f\u4ea7\u751f\u539f\u672c\u65e0\u6cd5\u62e5\u6709\u7684\u601d\u60f3\u7684\u53d8\u9769\u6027\u529b\u91cf\u3002\u4eba\u5de5\u667a\u80fd\u548c\u8ba4\u77e5\u79d1\u5b66\u7684\u6700\u65b0\u53d1\u5c55\u4e3a\u8fd9\u4e2a\u53e4\u8001\u95ee\u9898\u6ce8\u5165\u4e86\u65b0\u7684\u6d3b\u529b\u3002", "method": "\u5206\u6790\u8bed\u8a00\u7684\u4e24\u4e2a\u5173\u952e\u7279\u6027\uff1a1\uff09\u63d0\u4f9b\u7d27\u51d1\u8868\u5f81\u4fbf\u4e8e\u62bd\u8c61\u6982\u5ff5\u63a8\u7406\uff1b2\uff09\u8fd9\u4e9b\u538b\u7f29\u8868\u5f81\u662f\u96c6\u4f53\u667a\u6167\u7684\u8fed\u4ee3\u8f93\u51fa\u3002\u901a\u8fc7\u8bed\u8a00\u5b66\u4e60\uff0c\u7cfb\u7edf\u80fd\u591f\u83b7\u5f97\u6587\u5316\u6f14\u5316\u7684\u62bd\u8c61\u6982\u5ff5\u5b9d\u5e93\u3002", "result": "\u8bed\u8a00\u4f7f\u8db3\u591f\u5f3a\u5927\u7684\u5b66\u4e60\u7cfb\u7edf\u80fd\u591f\u5b66\u4e60\u4e16\u754c\u7684\u538b\u7f29\u6a21\u578b\uff0c\u9006\u5411\u5de5\u7a0b\u652f\u6301\u4eba\u7c7b\u601d\u7ef4\u7684\u6982\u5ff5\u548c\u56e0\u679c\u7ed3\u6784\u3002", "conclusion": "\u8bed\u8a00\u53ef\u80fd\u662f\u5b9e\u73b0\u66f4\u901a\u7528\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u548c\u4eba\u7c7b\u667a\u80fd\u6838\u5fc3\u65b9\u9762\u7684\u5173\u952e\uff0c\u56e0\u4e3a\u5b83\u63d0\u4f9b\u4e86\u7ecf\u8fc7\u6587\u5316\u6f14\u5316\u7684\u9ad8\u6548\u8ba4\u77e5\u5de5\u5177\u3002"}}
{"id": "2509.15568", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15568", "abs": "https://arxiv.org/abs/2509.15568", "authors": ["Junlong Jia", "Xing Wu", "Chaochen Gao", "Ziyang Chen", "Zijia Lin", "Zhongzhi Li", "Weinong Wang", "Haotian Xu", "Donghui Jin", "Debing Zhang", "Binghui Guo"], "title": "LiteLong: Resource-Efficient Long-Context Data Synthesis for LLMs", "comment": "work in progress", "summary": "High-quality long-context data is essential for training large language\nmodels (LLMs) capable of processing extensive documents, yet existing synthesis\napproaches using relevance-based aggregation face challenges of computational\nefficiency. We present LiteLong, a resource-efficient method for synthesizing\nlong-context data through structured topic organization and multi-agent debate.\nOur approach leverages the BISAC book classification system to provide a\ncomprehensive hierarchical topic organization, and then employs a debate\nmechanism with multiple LLMs to generate diverse, high-quality topics within\nthis structure. For each topic, we use lightweight BM25 retrieval to obtain\nrelevant documents and concatenate them into 128K-token training samples.\nExperiments on HELMET and Ruler benchmarks demonstrate that LiteLong achieves\ncompetitive long-context performance and can seamlessly integrate with other\nlong-dependency enhancement methods. LiteLong makes high-quality long-context\ndata synthesis more accessible by reducing both computational and data\nengineering costs, facilitating further research in long-context language\ntraining.", "AI": {"tldr": "LiteLong\u662f\u4e00\u79cd\u8d44\u6e90\u9ad8\u6548\u7684\u957f\u4e0a\u4e0b\u6587\u6570\u636e\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u4e3b\u9898\u7ec4\u7ec7\u548c\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6765\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u76f8\u5173\u6027\u7684\u805a\u5408\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u5408\u6210\u9ad8\u8d28\u91cf\u7684\u957f\u4e0a\u4e0b\u6587\u6570\u636e\u4ee5\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u3002", "method": "\u5229\u7528BISAC\u56fe\u4e66\u5206\u7c7b\u7cfb\u7edf\u63d0\u4f9b\u5c42\u6b21\u5316\u4e3b\u9898\u7ec4\u7ec7\uff0c\u91c7\u7528\u591aLLM\u8fa9\u8bba\u673a\u5236\u751f\u6210\u591a\u6837\u5316\u9ad8\u8d28\u91cf\u4e3b\u9898\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7BM25\u68c0\u7d22\u83b7\u53d6\u76f8\u5173\u6587\u6863\u5e76\u62fc\u63a5\u6210128K\u6807\u8bb0\u7684\u8bad\u7ec3\u6837\u672c\u3002", "result": "\u5728HELMET\u548cRuler\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLiteLong\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u957f\u4e0a\u4e0b\u6587\u6027\u80fd\uff0c\u5e76\u80fd\u4e0e\u5176\u4ed6\u957f\u4f9d\u8d56\u589e\u5f3a\u65b9\u6cd5\u65e0\u7f1d\u96c6\u6210\u3002", "conclusion": "LiteLong\u901a\u8fc7\u964d\u4f4e\u8ba1\u7b97\u548c\u6570\u636e\u5de5\u7a0b\u6210\u672c\uff0c\u4f7f\u9ad8\u8d28\u91cf\u957f\u4e0a\u4e0b\u6587\u6570\u636e\u5408\u6210\u66f4\u52a0\u6613\u4e8e\u5b9e\u73b0\uff0c\u4fc3\u8fdb\u4e86\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u8bad\u7ec3\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2509.15577", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15577", "abs": "https://arxiv.org/abs/2509.15577", "authors": ["Jaeyoung Kim", "Jongho Kim", "Seung-won Hwang", "Seoho Song", "Young-In Song"], "title": "Relevance to Utility: Process-Supervised Rewrite for RAG", "comment": null, "summary": "Retrieval-Augmented Generation systems often suffer from a gap between\noptimizing retrieval relevance and generative utility: retrieved documents may\nbe topically relevant but still lack the content needed for effective reasoning\nduring generation. While existing \"bridge\" modules attempt to rewrite the\nretrieved text for better generation, we show how they fail to capture true\ndocument utility. In this work, we propose R2U, with a key distinction of\ndirectly optimizing to maximize the probability of generating a correct answer\nthrough process supervision. As such direct observation is expensive, we also\npropose approximating an efficient distillation pipeline by scaling the\nsupervision from LLMs, which helps the smaller rewriter model generalize\nbetter. We evaluate our method across multiple open-domain question-answering\nbenchmarks. The empirical results demonstrate consistent improvements over\nstrong bridging baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faR2U\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fc7\u7a0b\u76d1\u7763\u76f4\u63a5\u4f18\u5316\u751f\u6210\u6b63\u786e\u7b54\u6848\u7684\u6982\u7387\uff0c\u89e3\u51b3\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u4e2d\u68c0\u7d22\u76f8\u5173\u6027\u4e0e\u751f\u6210\u6548\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u5b58\u5728\u68c0\u7d22\u76f8\u5173\u6027\u4f18\u5316\u4e0e\u751f\u6210\u6548\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u68c0\u7d22\u5230\u7684\u6587\u6863\u53ef\u80fd\u4e3b\u9898\u76f8\u5173\u4f46\u7f3a\u4e4f\u751f\u6210\u6240\u9700\u7684\u5173\u952e\u5185\u5bb9\u3002\u73b0\u6709\u7684\"\u6865\u63a5\"\u6a21\u5757\u5c1d\u8bd5\u91cd\u5199\u68c0\u7d22\u6587\u672c\u4ee5\u6539\u5584\u751f\u6210\u6548\u679c\uff0c\u4f46\u672a\u80fd\u6355\u6349\u771f\u6b63\u7684\u6587\u6863\u6548\u7528\u3002", "method": "\u63d0\u51faR2U\u65b9\u6cd5\uff0c\u5173\u952e\u533a\u522b\u5728\u4e8e\u901a\u8fc7\u8fc7\u7a0b\u76d1\u7763\u76f4\u63a5\u4f18\u5316\u751f\u6210\u6b63\u786e\u7b54\u6848\u7684\u6982\u7387\u3002\u7531\u4e8e\u76f4\u63a5\u76d1\u7763\u6210\u672c\u9ad8\u6602\uff0c\u8fd8\u63d0\u51fa\u4e86\u901a\u8fc7\u6269\u5c55LLM\u76d1\u7763\u7684\u9ad8\u6548\u84b8\u998f\u7ba1\u9053\u6765\u8fd1\u4f3c\uff0c\u5e2e\u52a9\u8f83\u5c0f\u7684\u91cd\u5199\u5668\u6a21\u578b\u83b7\u5f97\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u5f00\u653e\u57df\u95ee\u7b54\u57fa\u51c6\u4e0a\u8bc4\u4f30\u8be5\u65b9\u6cd5\uff0c\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\u76f8\u6bd4\u5f3a\u5927\u7684\u6865\u63a5\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u4e86\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "R2U\u65b9\u6cd5\u901a\u8fc7\u8fc7\u7a0b\u76d1\u7763\u76f4\u63a5\u4f18\u5316\u751f\u6210\u6982\u7387\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u4e2d\u7684\u6548\u7528\u5dee\u8ddd\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u6865\u63a5\u65b9\u6cd5\u3002"}}
{"id": "2509.15579", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15579", "abs": "https://arxiv.org/abs/2509.15579", "authors": ["Yun Tang", "Cindy Tseng"], "title": "Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization", "comment": null, "summary": "Low latency speech human-machine communication is becoming increasingly\nnecessary as speech technology advances quickly in the last decade. One of the\nprimary factors behind the advancement of speech technology is self-supervised\nlearning. Most self-supervised learning algorithms are designed with full\nutterance assumption and compromises have to made if partial utterances are\npresented, which are common in the streaming applications. In this work, we\npropose a chunk based self-supervised learning (Chunk SSL) algorithm as an\nunified solution for both streaming and offline speech pre-training. Chunk SSL\nis optimized with the masked prediction loss and an acoustic encoder is\nencouraged to restore indices of those masked speech frames with help from\nunmasked frames in the same chunk and preceding chunks. A copy and append data\naugmentation approach is proposed to conduct efficient chunk based\npre-training. Chunk SSL utilizes a finite scalar quantization (FSQ) module to\ndiscretize input speech features and our study shows a high resolution FSQ\ncodebook, i.e., a codebook with vocabulary size up to a few millions, is\nbeneficial to transfer knowledge from the pre-training task to the downstream\ntasks. A group masked prediction loss is employed during pre-training to\nalleviate the high memory and computation cost introduced by the large\ncodebook. The proposed approach is examined in two speech to text tasks, i.e.,\nspeech recognition and speech translation. Experimental results on the\n\\textsc{Librispeech} and \\textsc{Must-C} datasets show that the proposed method\ncould achieve very competitive results for speech to text tasks at both\nstreaming and offline modes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5757\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u7b97\u6cd5\uff08Chunk SSL\uff09\uff0c\u4f5c\u4e3a\u6d41\u5f0f\u548c\u79bb\u7ebf\u8bed\u97f3\u9884\u8bad\u7ec3\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u63a9\u7801\u9884\u6d4b\u635f\u5931\u548c\u6709\u9650\u6807\u91cf\u91cf\u5316\u6a21\u5757\uff0c\u5728\u8bed\u97f3\u8bc6\u522b\u548c\u8bed\u97f3\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002", "motivation": "\u968f\u7740\u8bed\u97f3\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u4f4e\u5ef6\u8fdf\u7684\u4eba\u673a\u8bed\u97f3\u901a\u4fe1\u9700\u6c42\u65e5\u76ca\u589e\u957f\u3002\u73b0\u6709\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u7b97\u6cd5\u5927\u591a\u57fa\u4e8e\u5b8c\u6574\u8bed\u97f3\u5047\u8bbe\uff0c\u5728\u6d41\u5f0f\u5e94\u7528\u4e2d\u5904\u7406\u90e8\u5206\u8bed\u97f3\u65f6\u9700\u8981\u8fdb\u884c\u59a5\u534f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u9002\u7528\u4e8e\u6d41\u5f0f\u548c\u79bb\u7ebf\u573a\u666f\u7684\u7edf\u4e00\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u63d0\u51faChunk SSL\u7b97\u6cd5\uff0c\u4f7f\u7528\u63a9\u7801\u9884\u6d4b\u635f\u5931\u8bad\u7ec3\u58f0\u5b66\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u540c\u4e00\u5206\u5757\u548c\u524d\u9762\u5206\u5757\u4e2d\u7684\u672a\u63a9\u7801\u5e27\u6765\u6062\u590d\u63a9\u7801\u8bed\u97f3\u5e27\u7684\u7d22\u5f15\u3002\u91c7\u7528\u590d\u5236\u548c\u8ffd\u52a0\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u8fdb\u884c\u9ad8\u6548\u7684\u5206\u5757\u9884\u8bad\u7ec3\uff0c\u4f7f\u7528\u6709\u9650\u6807\u91cf\u91cf\u5316\uff08FSQ\uff09\u6a21\u5757\u79bb\u6563\u5316\u8f93\u5165\u8bed\u97f3\u7279\u5f81\uff0c\u5e76\u91c7\u7528\u5206\u7ec4\u63a9\u7801\u9884\u6d4b\u635f\u5931\u6765\u7f13\u89e3\u5927\u7801\u672c\u5e26\u6765\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5728Librispeech\u548cMust-C\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8bed\u97f3\u8bc6\u522b\u548c\u8bed\u97f3\u7ffb\u8bd1\u4efb\u52a1\u4e2d\uff0c\u65e0\u8bba\u662f\u6d41\u5f0f\u8fd8\u662f\u79bb\u7ebf\u6a21\u5f0f\uff0c\u90fd\u80fd\u53d6\u5f97\u975e\u5e38\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002", "conclusion": "Chunk SSL\u4e3a\u6d41\u5f0f\u548c\u79bb\u7ebf\u8bed\u97f3\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5206\u5757\u5904\u7406\u548c\u6709\u9650\u6807\u91cf\u91cf\u5316\u6280\u672f\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u4e3a\u4f4e\u5ef6\u8fdf\u8bed\u97f3\u901a\u4fe1\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2509.15587", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15587", "abs": "https://arxiv.org/abs/2509.15587", "authors": ["Tsz Ting Chung", "Lemao Liu", "Mo Yu", "Dit-Yan Yeung"], "title": "DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models", "comment": "Accepted by EMNLP 2025. Project Page:\n  https://ttchungc.github.io/projects/divlogiceval/", "summary": "Logic reasoning in natural language has been recognized as an important\nmeasure of human intelligence for Large Language Models (LLMs). Popular\nbenchmarks may entangle multiple reasoning skills and thus provide unfaithful\nevaluations on the logic reasoning skill. Meanwhile, existing logic reasoning\nbenchmarks are limited in language diversity and their distributions are\ndeviated from the distribution of an ideal logic reasoning benchmark, which may\nlead to biased evaluation results. This paper thereby proposes a new classical\nlogic benchmark DivLogicEval, consisting of natural sentences composed of\ndiverse statements in a counterintuitive way. To ensure a more reliable\nevaluation, we also introduce a new evaluation metric that mitigates the\ninfluence of bias and randomness inherent in LLMs. Through experiments, we\ndemonstrate the extent to which logical reasoning is required to answer the\nquestions in DivLogicEval and compare the performance of different popular LLMs\nin conducting logical reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u7ecf\u5178\u903b\u8f91\u57fa\u51c6DivLogicEval\uff0c\u901a\u8fc7\u53cd\u76f4\u89c9\u7684\u65b9\u5f0f\u7ec4\u5408\u591a\u6837\u5316\u7684\u81ea\u7136\u8bed\u8a00\u8bed\u53e5\u6765\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u903b\u8f91\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u4ee5\u51cf\u5c11\u6a21\u578b\u504f\u5dee\u548c\u968f\u673a\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u7684\u903b\u8f91\u63a8\u7406\u57fa\u51c6\u5b58\u5728\u591a\u4e2a\u63a8\u7406\u6280\u80fd\u6df7\u6742\u3001\u8bed\u8a00\u591a\u6837\u6027\u4e0d\u8db3\u3001\u5206\u5e03\u504f\u79bb\u7406\u60f3\u903b\u8f91\u63a8\u7406\u57fa\u51c6\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u5bf9\u903b\u8f91\u63a8\u7406\u80fd\u529b\u7684\u8bc4\u4f30\u4e0d\u591f\u51c6\u786e\u548c\u53ef\u9760\u3002", "method": "\u6784\u5efaDivLogicEval\u57fa\u51c6\uff0c\u5305\u542b\u4ee5\u53cd\u76f4\u89c9\u65b9\u5f0f\u7ec4\u5408\u7684\u81ea\u7136\u8bed\u53e5\uff1b\u8bbe\u8ba1\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u6765\u51cf\u8f7bLLMs\u4e2d\u5b58\u5728\u7684\u504f\u5dee\u548c\u968f\u673a\u6027\u5f71\u54cd\uff1b\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u57fa\u51c6\u7684\u903b\u8f91\u63a8\u7406\u9700\u6c42\u5e76\u6bd4\u8f83\u4e0d\u540cLLMs\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86DivLogicEval\u4e2d\u95ee\u9898\u786e\u5b9e\u9700\u8981\u903b\u8f91\u63a8\u7406\u80fd\u529b\u6765\u56de\u7b54\uff0c\u5e76\u5c55\u793a\u4e86\u4e0d\u540c\u6d41\u884cLLMs\u5728\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "DivLogicEval\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u53ef\u9760\u3001\u8bed\u8a00\u66f4\u591a\u6837\u5316\u7684\u903b\u8f91\u63a8\u7406\u8bc4\u4f30\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u66f4\u51c6\u786e\u5730\u8861\u91cfLLMs\u7684\u903b\u8f91\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2509.15620", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15620", "abs": "https://arxiv.org/abs/2509.15620", "authors": ["Bofu Dong", "Pritesh Shah", "Sumedh Sonawane", "Tiyasha Banerjee", "Erin Brady", "Xinya Du", "Ming Jiang"], "title": "SciEvent: Benchmarking Multi-domain Scientific Event Extraction", "comment": "9 pages, 8 figures (main); 22 pages, 11 figures (appendix). Accepted\n  to EMNLP 2025 (Main Conference)", "summary": "Scientific information extraction (SciIE) has primarily relied on\nentity-relation extraction in narrow domains, limiting its applicability to\ninterdisciplinary research and struggling to capture the necessary context of\nscientific information, often resulting in fragmented or conflicting\nstatements. In this paper, we introduce SciEvent, a novel multi-domain\nbenchmark of scientific abstracts annotated via a unified event extraction (EE)\nschema designed to enable structured and context-aware understanding of\nscientific content. It includes 500 abstracts across five research domains,\nwith manual annotations of event segments, triggers, and fine-grained\narguments. We define SciIE as a multi-stage EE pipeline: (1) segmenting\nabstracts into core scientific activities--Background, Method, Result, and\nConclusion; and (2) extracting the corresponding triggers and arguments.\nExperiments with fine-tuned EE models, large language models (LLMs), and human\nannotators reveal a performance gap, with current models struggling in domains\nsuch as sociology and humanities. SciEvent serves as a challenging benchmark\nand a step toward generalizable, multi-domain SciIE.", "AI": {"tldr": "SciEvent\u662f\u4e00\u4e2a\u65b0\u7684\u591a\u9886\u57df\u79d1\u5b66\u4e8b\u4ef6\u62bd\u53d6\u57fa\u51c6\uff0c\u5305\u542b500\u7bc7\u8de85\u4e2a\u7814\u7a76\u9886\u57df\u7684\u6458\u8981\uff0c\u91c7\u7528\u7edf\u4e00\u7684\u4e8b\u4ef6\u62bd\u53d6\u6a21\u5f0f\u6765\u89e3\u51b3\u4f20\u7edf\u79d1\u5b66\u4fe1\u606f\u62bd\u53d6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u79d1\u5b66\u4fe1\u606f\u62bd\u53d6\u4e3b\u8981\u4f9d\u8d56\u7a84\u9886\u57df\u7684\u5b9e\u4f53\u5173\u7cfb\u62bd\u53d6\uff0c\u96be\u4ee5\u9002\u5e94\u8de8\u5b66\u79d1\u7814\u7a76\uff0c\u4e14\u65e0\u6cd5\u6355\u6349\u79d1\u5b66\u4fe1\u606f\u7684\u5fc5\u8981\u4e0a\u4e0b\u6587\uff0c\u5bfc\u81f4\u4fe1\u606f\u788e\u7247\u5316\u6216\u51b2\u7a81\u3002", "method": "\u5c06\u79d1\u5b66\u4fe1\u606f\u62bd\u53d6\u5b9a\u4e49\u4e3a\u591a\u9636\u6bb5\u4e8b\u4ef6\u62bd\u53d6\u6d41\u7a0b\uff1a1\uff09\u5c06\u6458\u8981\u5206\u6bb5\u4e3a\u6838\u5fc3\u79d1\u5b66\u6d3b\u52a8\uff08\u80cc\u666f\u3001\u65b9\u6cd5\u3001\u7ed3\u679c\u3001\u7ed3\u8bba\uff09\uff1b2\uff09\u63d0\u53d6\u76f8\u5e94\u89e6\u53d1\u8bcd\u548c\u7ec6\u7c92\u5ea6\u8bba\u5143\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5f53\u524d\u6a21\u578b\u5728\u793e\u4f1a\u79d1\u5b66\u548c\u4eba\u6587\u5b66\u79d1\u7b49\u9886\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "SciEvent\u4f5c\u4e3a\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\uff0c\u662f\u8fc8\u5411\u53ef\u6cdb\u5316\u3001\u591a\u9886\u57df\u79d1\u5b66\u4fe1\u606f\u62bd\u53d6\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2509.15621", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15621", "abs": "https://arxiv.org/abs/2509.15621", "authors": ["Tomoya Yamashita", "Yuuki Yamanaka", "Masanori Yamada", "Takayuki Miura", "Toshiki Shibahara", "Tomoharu Iwata"], "title": "Concept Unlearning in Large Language Models via Self-Constructed Knowledge Triplets", "comment": null, "summary": "Machine Unlearning (MU) has recently attracted considerable attention as a\nsolution to privacy and copyright issues in large language models (LLMs).\nExisting MU methods aim to remove specific target sentences from an LLM while\nminimizing damage to unrelated knowledge. However, these approaches require\nexplicit target sentences and do not support removing broader concepts, such as\npersons or events. To address this limitation, we introduce Concept Unlearning\n(CU) as a new requirement for LLM unlearning. We leverage knowledge graphs to\nrepresent the LLM's internal knowledge and define CU as removing the forgetting\ntarget nodes and associated edges. This graph-based formulation enables a more\nintuitive unlearning and facilitates the design of more effective methods. We\npropose a novel method that prompts the LLM to generate knowledge triplets and\nexplanatory sentences about the forgetting target and applies the unlearning\nprocess to these representations. Our approach enables more precise and\ncomprehensive concept removal by aligning the unlearning process with the LLM's\ninternal knowledge representations. Experiments on real-world and synthetic\ndatasets demonstrate that our method effectively achieves concept-level\nunlearning while preserving unrelated knowledge.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u6982\u5ff5\u9057\u5fd8\uff08CU\uff09\u4f5c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9057\u5fd8\u7684\u65b0\u9700\u6c42\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u8868\u793aLLM\u5185\u90e8\u77e5\u8bc6\uff0c\u5b9e\u73b0\u66f4\u76f4\u89c2\u6709\u6548\u7684\u6982\u5ff5\u7ea7\u9057\u5fd8\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\u4ec5\u80fd\u79fb\u9664\u7279\u5b9a\u76ee\u6807\u53e5\u5b50\uff0c\u65e0\u6cd5\u652f\u6301\u66f4\u5e7f\u6cdb\u7684\u6982\u5ff5\uff08\u5982\u4eba\u7269\u6216\u4e8b\u4ef6\uff09\u9057\u5fd8\uff0c\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u8868\u793aLLM\u5185\u90e8\u77e5\u8bc6\uff0c\u5b9a\u4e49CU\u4e3a\u79fb\u9664\u9057\u5fd8\u76ee\u6807\u8282\u70b9\u53ca\u76f8\u5173\u8fb9\uff1b\u901a\u8fc7\u63d0\u793aLLM\u751f\u6210\u77e5\u8bc6\u4e09\u5143\u7ec4\u548c\u89e3\u91ca\u6027\u53e5\u5b50\uff0c\u5bf9\u8fd9\u4e9b\u8868\u793a\u5e94\u7528\u9057\u5fd8\u8fc7\u7a0b\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5b9e\u73b0\u6982\u5ff5\u7ea7\u9057\u5fd8\uff0c\u540c\u65f6\u4fdd\u7559\u65e0\u5173\u77e5\u8bc6\u3002", "conclusion": "\u63d0\u51fa\u7684\u6982\u5ff5\u9057\u5fd8\u65b9\u6cd5\u901a\u8fc7\u56fe\u57fa\u8868\u793a\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u548c\u5168\u9762\u7684\u6982\u5ff5\u79fb\u9664\uff0c\u4e0eLLM\u5185\u90e8\u77e5\u8bc6\u8868\u793a\u5bf9\u9f50\uff0c\u4e3aLLM\u9057\u5fd8\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.15631", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15631", "abs": "https://arxiv.org/abs/2509.15631", "authors": ["Tomoya Yamashita", "Akira Ito", "Yuuki Yamanaka", "Masanori Yamada", "Takayuki Miura", "Toshiki Shibahara"], "title": "Sparse-Autoencoder-Guided Internal Representation Unlearning for Large Language Models", "comment": null, "summary": "As large language models (LLMs) are increasingly deployed across various\napplications, privacy and copyright concerns have heightened the need for more\neffective LLM unlearning techniques. Many existing unlearning methods aim to\nsuppress undesirable outputs through additional training (e.g., gradient\nascent), which reduces the probability of generating such outputs. While such\nsuppression-based approaches can control model outputs, they may not eliminate\nthe underlying knowledge embedded in the model's internal activations; muting a\nresponse is not the same as forgetting it. Moreover, such suppression-based\nmethods often suffer from model collapse. To address these issues, we propose a\nnovel unlearning method that directly intervenes in the model's internal\nactivations. In our formulation, forgetting is defined as a state in which the\nactivation of a forgotten target is indistinguishable from that of ``unknown''\nentities. Our method introduces an unlearning objective that modifies the\nactivation of the target entity away from those of known entities and toward\nthose of unknown entities in a sparse autoencoder latent space. By aligning the\ntarget's internal activation with those of unknown entities, we shift the\nmodel's recognition of the target entity from ``known'' to ``unknown'',\nachieving genuine forgetting while avoiding over-suppression and model\ncollapse. Empirically, we show that our method effectively aligns the internal\nactivations of the forgotten target, a result that the suppression-based\napproaches do not reliably achieve. Additionally, our method effectively\nreduces the model's recall of target knowledge in question-answering tasks\nwithout significant damage to the non-target knowledge.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684LLM\u9057\u5fd8\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f4\u63a5\u5e72\u9884\u6a21\u578b\u5185\u90e8\u6fc0\u6d3b\u6765\u5b9e\u73b0\u771f\u6b63\u7684\u9057\u5fd8\uff0c\u800c\u4e0d\u662f\u4ec5\u4ec5\u6291\u5236\u8f93\u51fa", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6291\u5236\u7684\u9057\u5fd8\u65b9\u6cd5\u65e0\u6cd5\u6d88\u9664\u6a21\u578b\u5185\u90e8\u5d4c\u5165\u7684\u77e5\u8bc6\uff0c\u4e14\u5bb9\u6613\u5bfc\u81f4\u6a21\u578b\u5d29\u6e83\u3002\u9700\u8981\u4e00\u79cd\u80fd\u771f\u6b63\u5b9e\u73b0\u9057\u5fd8\u800c\u975e\u7b80\u5355\u6291\u5236\u7684\u65b9\u6cd5", "method": "\u5728\u7a00\u758f\u81ea\u7f16\u7801\u5668\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u901a\u8fc7\u4fee\u6539\u76ee\u6807\u5b9e\u4f53\u7684\u6fc0\u6d3b\uff0c\u4f7f\u5176\u4ece\u5df2\u77e5\u5b9e\u4f53\u8f6c\u5411\u672a\u77e5\u5b9e\u4f53\uff0c\u5b9e\u73b0\u4ece\"\u5df2\u77e5\"\u5230\"\u672a\u77e5\"\u7684\u8f6c\u53d8", "result": "\u8be5\u65b9\u6cd5\u6709\u6548\u5bf9\u9f50\u4e86\u88ab\u9057\u5fd8\u76ee\u6807\u7684\u5185\u90e8\u6fc0\u6d3b\uff0c\u5728\u95ee\u7b54\u4efb\u52a1\u4e2d\u663e\u8457\u964d\u4f4e\u76ee\u6807\u77e5\u8bc6\u7684\u53ec\u56de\u7387\uff0c\u540c\u65f6\u5bf9\u975e\u76ee\u6807\u77e5\u8bc6\u5f71\u54cd\u8f83\u5c0f", "conclusion": "\u901a\u8fc7\u76f4\u63a5\u5e72\u9884\u5185\u90e8\u6fc0\u6d3b\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u771f\u6b63\u7684\u9057\u5fd8\uff0c\u907f\u514d\u4e86\u8fc7\u5ea6\u6291\u5236\u548c\u6a21\u578b\u5d29\u6e83\u95ee\u9898"}}
{"id": "2509.15640", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15640", "abs": "https://arxiv.org/abs/2509.15640", "authors": ["Nhu Vo", "Nu-Uyen-Phuong Le", "Dung D. Le", "Massimo Piccardi", "Wray Buntine"], "title": "Multilingual LLM Prompting Strategies for Medical English-Vietnamese Machine Translation", "comment": "The work is under peer review", "summary": "Medical English-Vietnamese machine translation (En-Vi MT) is essential for\nhealthcare access and communication in Vietnam, yet Vietnamese remains a\nlow-resource and under-studied language. We systematically evaluate prompting\nstrategies for six multilingual LLMs (0.5B-9B parameters) on the MedEV dataset,\ncomparing zero-shot, few-shot, and dictionary-augmented prompting with Meddict,\nan English-Vietnamese medical lexicon. Results show that model scale is the\nprimary driver of performance: larger LLMs achieve strong zero-shot results,\nwhile few-shot prompting yields only marginal improvements. In contrast,\nterminology-aware cues and embedding-based example retrieval consistently\nimprove domain-specific translation. These findings underscore both the promise\nand the current limitations of multilingual LLMs for medical En-Vi MT.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u516d\u79cd\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u82f1\u8bed-\u8d8a\u5357\u8bed\u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\u4e0a\u7684\u63d0\u793a\u7b56\u7565\uff0c\u53d1\u73b0\u6a21\u578b\u89c4\u6a21\u662f\u6027\u80fd\u7684\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\uff0c\u800c\u672f\u8bed\u611f\u77e5\u63d0\u793a\u548c\u5d4c\u5165\u68c0\u7d22\u80fd\u6709\u6548\u63d0\u5347\u4e13\u4e1a\u9886\u57df\u7ffb\u8bd1\u8d28\u91cf\u3002", "motivation": "\u8d8a\u5357\u8bed\u4f5c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728\u533b\u5b66\u673a\u5668\u7ffb\u8bd1\u9886\u57df\u7814\u7a76\u4e0d\u8db3\uff0c\u800c\u533b\u5b66\u82f1\u8bed-\u8d8a\u5357\u8bed\u7ffb\u8bd1\u5bf9\u8d8a\u5357\u7684\u533b\u7597\u4fdd\u5065\u8bbf\u95ee\u548c\u6c9f\u901a\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5728MedEV\u6570\u636e\u96c6\u4e0a\u6bd4\u8f83\u4e86\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u57fa\u4e8eMeddict\u533b\u5b66\u8bcd\u5178\u589e\u5f3a\u7684\u63d0\u793a\u7b56\u7565\uff0c\u8bc4\u4f30\u4e86\u516d\u79cd\u591a\u8bed\u8a00LLM\uff080.5B-9B\u53c2\u6570\uff09\u7684\u6027\u80fd\u3002", "result": "\u8f83\u5927\u89c4\u6a21\u7684LLM\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u5c11\u6837\u672c\u63d0\u793a\u4ec5\u5e26\u6765\u8fb9\u9645\u6539\u8fdb\uff1b\u672f\u8bed\u611f\u77e5\u63d0\u793a\u548c\u57fa\u4e8e\u5d4c\u5165\u7684\u793a\u4f8b\u68c0\u7d22\u80fd\u6301\u7eed\u63d0\u5347\u9886\u57df\u7279\u5b9a\u7ffb\u8bd1\u8d28\u91cf\u3002", "conclusion": "\u591a\u8bed\u8a00LLM\u5728\u533b\u5b66\u82f1\u8bed-\u8d8a\u5357\u8bed\u7ffb\u8bd1\u65b9\u9762\u5177\u6709\u6f5c\u529b\u4f46\u4e5f\u5b58\u5728\u5f53\u524d\u5c40\u9650\u6027\uff0c\u6a21\u578b\u89c4\u6a21\u548c\u9886\u57df\u9002\u5e94\u6027\u662f\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2509.15655", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15655", "abs": "https://arxiv.org/abs/2509.15655", "authors": ["Linyang He", "Qiaolin Wang", "Xilin Jiang", "Nima Mesgarani"], "title": "Layer-wise Minimal Pair Probing Reveals Contextual Grammatical-Conceptual Hierarchy in Speech Representations", "comment": "EMNLP 2025 Main Conference (Oral)", "summary": "Transformer-based speech language models (SLMs) have significantly improved\nneural speech recognition and understanding. While existing research has\nexamined how well SLMs encode shallow acoustic and phonetic features, the\nextent to which SLMs encode nuanced syntactic and conceptual features remains\nunclear. By drawing parallels with linguistic competence assessments for large\nlanguage models, this study is the first to systematically evaluate the\npresence of contextual syntactic and semantic features across SLMs for\nself-supervised learning (S3M), automatic speech recognition (ASR), speech\ncompression (codec), and as the encoder for auditory large language models\n(AudioLLMs). Through minimal pair designs and diagnostic feature analysis\nacross 71 tasks spanning diverse linguistic levels, our layer-wise and\ntime-resolved analysis uncovers that 1) all speech encode grammatical features\nmore robustly than conceptual ones.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e0d\u540c\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u4e2d\u4e0a\u4e0b\u6587\u53e5\u6cd5\u548c\u8bed\u4e49\u7279\u5f81\u7684\u7f16\u7801\u60c5\u51b5\uff0c\u53d1\u73b0\u6240\u6709\u8bed\u97f3\u6a21\u578b\u5bf9\u8bed\u6cd5\u7279\u5f81\u7684\u7f16\u7801\u90fd\u6bd4\u6982\u5ff5\u7279\u5f81\u66f4\u7a33\u5065\u3002", "motivation": "\u867d\u7136\u73b0\u6709\u7814\u7a76\u5df2\u7ecf\u68c0\u9a8c\u4e86SLMs\u5bf9\u6d45\u5c42\u58f0\u5b66\u548c\u8bed\u97f3\u7279\u5f81\u7684\u7f16\u7801\u80fd\u529b\uff0c\u4f46SLMs\u7f16\u7801\u7ec6\u5fae\u53e5\u6cd5\u548c\u6982\u5ff5\u7279\u5f81\u7684\u7a0b\u5ea6\u4ecd\u4e0d\u6e05\u695a\u3002\u672c\u7814\u7a76\u501f\u9274\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u8a00\u80fd\u529b\u8bc4\u4f30\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u6700\u5c0f\u5bf9\u8bbe\u8ba1\u548c\u8bca\u65ad\u7279\u5f81\u5206\u6790\uff0c\u572871\u4e2a\u6db5\u76d6\u4e0d\u540c\u8bed\u8a00\u5c42\u6b21\u7684\u4efb\u52a1\u4e2d\u8fdb\u884c\u5206\u5c42\u548c\u65f6\u95f4\u5206\u8fa8\u5206\u6790\uff0c\u8bc4\u4f30\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\uff08S3M\uff09\u3001\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u3001\u8bed\u97f3\u538b\u7f29\uff08codec\uff09\u4ee5\u53ca\u542c\u89c9\u5927\u8bed\u8a00\u6a21\u578b\uff08AudioLLMs\uff09\u7f16\u7801\u5668\u7b49\u591a\u79cdSLMs\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6240\u6709\u8bed\u97f3\u6a21\u578b\u5bf9\u8bed\u6cd5\u7279\u5f81\u7684\u7f16\u7801\u90fd\u6bd4\u6982\u5ff5\u7279\u5f81\u66f4\u7a33\u5065\uff0c\u8fd9\u662f\u901a\u8fc7\u5c42\u95f4\u548c\u65f6\u95f4\u5206\u8fa8\u5206\u6790\u5f97\u51fa\u7684\u5173\u952e\u53d1\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3SLMs\u7684\u8bed\u8a00\u7f16\u7801\u80fd\u529b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u8bed\u97f3\u6a21\u578b\u5728\u8bed\u6cd5\u7279\u5f81\u7f16\u7801\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u4e3a\u540e\u7eed\u6a21\u578b\u6539\u8fdb\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2509.15714", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15714", "abs": "https://arxiv.org/abs/2509.15714", "authors": ["Jonas Mayer Martins", "Ali Hamza Bashir", "Muhammad Rehan Khalid", "Lisa Beinborn"], "title": "Once Upon a Time: Interactive Learning for Storytelling with Small Language Models", "comment": "EMNLP 2025, BabyLM Challenge; 16 pages, 6 figures", "summary": "Children efficiently acquire language not just by listening, but by\ninteracting with others in their social environment. Conversely, large language\nmodels are typically trained with next-word prediction on massive amounts of\ntext. Motivated by this contrast, we investigate whether language models can be\ntrained with less data by learning not only from next-word prediction but also\nfrom high-level, cognitively inspired feedback. We train a student model to\ngenerate stories, which a teacher model rates on readability, narrative\ncoherence, and creativity. By varying the amount of pretraining before the\nfeedback loop, we assess the impact of this interactive learning on formal and\nfunctional linguistic competence. We find that the high-level feedback is\nhighly data efficient: With just 1 M words of input in interactive learning,\nstorytelling skills can improve as much as with 410 M words of next-word\nprediction.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u901a\u8fc7\u8ba4\u77e5\u542f\u53d1\u7684\u9ad8\u5c42\u6b21\u53cd\u9988\u6765\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u53d1\u73b0\u4ec5\u9700100\u4e07\u8bcd\u7684\u4ea4\u4e92\u5f0f\u5b66\u4e60\u5c31\u80fd\u8fbe\u52304.1\u4ebf\u8bcd\u4e0b\u4e00\u8bcd\u9884\u6d4b\u8bad\u7ec3\u7684\u6548\u679c\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u6548\u7387\u3002", "motivation": "\u513f\u7ae5\u901a\u8fc7\u793e\u4ea4\u4e92\u52a8\u9ad8\u6548\u5b66\u4e60\u8bed\u8a00\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u901a\u8fc7\u6d77\u91cf\u6587\u672c\u7684\u4e0b\u4e00\u8bcd\u9884\u6d4b\u8bad\u7ec3\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u7ed3\u5408\u9ad8\u5c42\u6b21\u8ba4\u77e5\u53cd\u9988\u6765\u51cf\u5c11\u6570\u636e\u9700\u6c42\u3002", "method": "\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\u751f\u6210\u6545\u4e8b\uff0c\u7531\u6559\u5e08\u6a21\u578b\u5bf9\u53ef\u8bfb\u6027\u3001\u53d9\u4e8b\u8fde\u8d2f\u6027\u548c\u521b\u9020\u6027\u8fdb\u884c\u8bc4\u5206\u3002\u901a\u8fc7\u8c03\u6574\u53cd\u9988\u5faa\u73af\u524d\u7684\u9884\u8bad\u7ec3\u91cf\uff0c\u8bc4\u4f30\u4ea4\u4e92\u5f0f\u5b66\u4e60\u5bf9\u8bed\u8a00\u80fd\u529b\u7684\u5f71\u54cd\u3002", "result": "\u9ad8\u5c42\u6b21\u53cd\u9988\u5177\u6709\u6781\u9ad8\u7684\u6570\u636e\u6548\u7387\uff1a\u4ec5\u4f7f\u7528100\u4e07\u8bcd\u7684\u4ea4\u4e92\u5f0f\u5b66\u4e60\u8f93\u5165\uff0c\u6545\u4e8b\u8bb2\u8ff0\u80fd\u529b\u7684\u63d0\u5347\u6548\u679c\u76f8\u5f53\u4e8e4.1\u4ebf\u8bcd\u7684\u4e0b\u4e00\u8bcd\u9884\u6d4b\u8bad\u7ec3\u3002", "conclusion": "\u4ea4\u4e92\u5f0f\u5b66\u4e60\u7ed3\u5408\u9ad8\u5c42\u6b21\u8ba4\u77e5\u53cd\u9988\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u7684\u6570\u636e\u6548\u7387\uff0c\u4e3a\u66f4\u9ad8\u6548\u7684\u8bed\u8a00\u5b66\u4e60\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.15667", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15667", "abs": "https://arxiv.org/abs/2509.15667", "authors": ["Dimitrios Damianos", "Leon Voukoutis", "Georgios Paraskevopoulos", "Vassilis Katsouros"], "title": "VOX-KRIKRI: Unifying Speech and Language through Continuous Fusion", "comment": null, "summary": "We present a multimodal fusion framework that bridges pre-trained\ndecoder-based large language models (LLM) and acoustic encoder-decoder\narchitectures such as Whisper, with the aim of building speech-enabled LLMs.\nInstead of directly using audio embeddings, we explore an intermediate\naudio-conditioned text space as a more effective mechanism for alignment. Our\nmethod operates fully in continuous text representation spaces, fusing\nWhisper's hidden decoder states with those of an LLM through cross-modal\nattention, and supports both offline and streaming modes. We introduce\n\\textit{VoxKrikri}, the first Greek speech LLM, and show through analysis that\nour approach effectively aligns representations across modalities. These\nresults highlight continuous space fusion as a promising path for multilingual\nand low-resource speech LLMs, while achieving state-of-the-art results for\nAutomatic Speech Recognition in Greek, providing an average $\\sim20\\%$ relative\nimprovement across benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u878d\u5408\u6846\u67b6\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u57fa\u4e8e\u89e3\u7801\u5668\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4e0eWhisper\u7b49\u58f0\u5b66\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u76f8\u7ed3\u5408\uff0c\u6784\u5efa\u652f\u6301\u8bed\u97f3\u7684LLM\u3002\u901a\u8fc7\u97f3\u9891\u6761\u4ef6\u6587\u672c\u7a7a\u95f4\u8fdb\u884c\u5bf9\u9f50\uff0c\u5728\u8fde\u7eed\u6587\u672c\u8868\u793a\u7a7a\u95f4\u4e2d\u878d\u5408\u4e24\u79cd\u6a21\u578b\u7684\u9690\u85cf\u72b6\u6001\uff0c\u5e76\u652f\u6301\u79bb\u7ebf\u548c\u6d41\u5f0f\u6a21\u5f0f\u3002", "motivation": "\u6784\u5efa\u652f\u6301\u8bed\u97f3\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u63a2\u7d22\u66f4\u6709\u6548\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u673a\u5236\uff0c\u4e3a\u591a\u8bed\u8a00\u548c\u4f4e\u8d44\u6e90\u8bed\u97f3LLM\u63d0\u4f9b\u53ef\u884c\u8def\u5f84\u3002", "method": "\u4f7f\u7528\u4e2d\u95f4\u97f3\u9891\u6761\u4ef6\u6587\u672c\u7a7a\u95f4\u4f5c\u4e3a\u5bf9\u9f50\u673a\u5236\uff0c\u5728\u8fde\u7eed\u6587\u672c\u8868\u793a\u7a7a\u95f4\u4e2d\u901a\u8fc7\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u878d\u5408Whisper\u7684\u9690\u85cf\u89e3\u7801\u5668\u72b6\u6001\u548cLLM\u7684\u72b6\u6001\uff0c\u652f\u6301\u79bb\u7ebf\u548c\u6d41\u5f0f\u5904\u7406\u6a21\u5f0f\u3002", "result": "\u5f00\u53d1\u4e86\u9996\u4e2a\u5e0c\u814a\u8bed\u8bed\u97f3LLM VoxKrikri\uff0c\u5728\u5e0c\u814a\u8bed\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u7ea620%\u7684\u76f8\u5bf9\u6027\u80fd\u63d0\u5347\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u8fde\u7eed\u7a7a\u95f4\u878d\u5408\u662f\u591a\u8bed\u8a00\u548c\u4f4e\u8d44\u6e90\u8bed\u97f3LLM\u7684\u6709\u524d\u666f\u8def\u5f84\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u5b9e\u73b0\u4e86\u8de8\u6a21\u6001\u8868\u793a\u5bf9\u9f50\u3002"}}
{"id": "2509.15811", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15811", "abs": "https://arxiv.org/abs/2509.15811", "authors": ["Sara Rajaee", "Rochelle Choenni", "Ekaterina Shutova", "Christof Monz"], "title": "Best-of-L: Cross-Lingual Reward Modeling for Mathematical Reasoning", "comment": null, "summary": "While the reasoning abilities of large language models (LLMs) continue to\nadvance, it remains unclear how such ability varies across languages in\nmultilingual LLMs and whether different languages produce reasoning paths that\ncomplement each other. To investigate this question, we train a reward model to\nrank generated responses for a given question across languages. Our results\nshow that our cross-lingual reward model substantially improves mathematical\nreasoning performance compared to using reward modeling within a single\nlanguage, benefiting even high-resource languages. While English often exhibits\nthe highest performance in multilingual models, we find that cross-lingual\nsampling particularly benefits English under low sampling budgets. Our findings\nreveal new opportunities to improve multilingual reasoning by leveraging the\ncomplementary strengths of diverse languages.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u63a8\u7406\u80fd\u529b\u5728\u4e0d\u540c\u8bed\u8a00\u95f4\u7684\u5dee\u5f02\uff0c\u53d1\u73b0\u8de8\u8bed\u8a00\u5956\u52b1\u6a21\u578b\u80fd\u663e\u8457\u63d0\u5347\u6570\u5b66\u63a8\u7406\u6027\u80fd\uff0c\u5373\u4f7f\u5bf9\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e5f\u6709\u76ca\u3002", "motivation": "\u63a2\u7a76\u591a\u8bed\u8a00LLMs\u4e2d\u63a8\u7406\u80fd\u529b\u5982\u4f55\u968f\u8bed\u8a00\u53d8\u5316\uff0c\u4ee5\u53ca\u4e0d\u540c\u8bed\u8a00\u662f\u5426\u4ea7\u751f\u4e92\u8865\u7684\u63a8\u7406\u8def\u5f84\u3002", "method": "\u8bad\u7ec3\u4e00\u4e2a\u8de8\u8bed\u8a00\u5956\u52b1\u6a21\u578b\u6765\u5bf9\u591a\u8bed\u8a00\u751f\u6210\u7684\u56de\u7b54\u8fdb\u884c\u6392\u5e8f\u3002", "result": "\u8de8\u8bed\u8a00\u5956\u52b1\u6a21\u578b\u76f8\u6bd4\u5355\u8bed\u8a00\u5956\u52b1\u5efa\u6a21\u663e\u8457\u63d0\u5347\u4e86\u6570\u5b66\u63a8\u7406\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u4f4e\u91c7\u6837\u9884\u7b97\u4e0b\u5bf9\u82f1\u8bed\u5c24\u4e3a\u6709\u76ca\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u901a\u8fc7\u5229\u7528\u4e0d\u540c\u8bed\u8a00\u7684\u4e92\u8865\u4f18\u52bf\u6765\u6539\u8fdb\u591a\u8bed\u8a00\u63a8\u7406\u7684\u65b0\u673a\u4f1a\u3002"}}
{"id": "2509.15701", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15701", "abs": "https://arxiv.org/abs/2509.15701", "authors": ["Ke Wang", "Wenning Wei", "Yan Deng", "Lei He", "Sheng Zhao"], "title": "Fine-Tuning Large Multimodal Models for Automatic Pronunciation Assessment", "comment": "submitted to ICASSP2026", "summary": "Automatic Pronunciation Assessment (APA) is critical for Computer-Assisted\nLanguage Learning (CALL), requiring evaluation across multiple granularities\nand aspects. Large Multimodal Models (LMMs) present new opportunities for APA,\nbut their effectiveness in fine-grained assessment remains uncertain. This work\ninvestigates fine-tuning LMMs for APA using the Speechocean762 dataset and a\nprivate corpus. Fine-tuning significantly outperforms zero-shot settings and\nachieves competitive results on single-granularity tasks compared to public and\ncommercial systems. The model performs well at word and sentence levels, while\nphoneme-level assessment remains challenging. We also observe that the Pearson\nCorrelation Coefficient (PCC) reaches 0.9, whereas Spearman's rank Correlation\nCoefficient (SCC) remains around 0.6, suggesting that SCC better reflects\nordinal consistency. These findings highlight both the promise and limitations\nof LMMs for APA and point to future work on fine-grained modeling and\nrank-aware evaluation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u4f7f\u7528\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u8fdb\u884c\u81ea\u52a8\u53d1\u97f3\u8bc4\u4f30\uff0c\u901a\u8fc7\u5fae\u8c03\u5728Speechocean762\u6570\u636e\u96c6\u548c\u79c1\u6709\u8bed\u6599\u5e93\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u4f18\u4e8e\u96f6\u6837\u672c\u8bbe\u7f6e\u7684\u6027\u80fd\uff0c\u5728\u5355\u8bcd\u548c\u53e5\u5b50\u7ea7\u522b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u97f3\u7d20\u7ea7\u522b\u8bc4\u4f30\u4ecd\u5177\u6311\u6218\u6027\u3002", "motivation": "\u81ea\u52a8\u53d1\u97f3\u8bc4\u4f30\u5728\u8ba1\u7b97\u673a\u8f85\u52a9\u8bed\u8a00\u5b66\u4e60\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u8bc4\u4f30\u591a\u4e2a\u7c92\u5ea6\u548c\u65b9\u9762\u3002\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u4e3aAPA\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\uff0c\u4f46\u5176\u5728\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u4e2d\u7684\u6709\u6548\u6027\u5c1a\u4e0d\u786e\u5b9a\u3002", "method": "\u4f7f\u7528Speechocean762\u6570\u636e\u96c6\u548c\u79c1\u6709\u8bed\u6599\u5e93\u5bf9\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u6bd4\u8f83\u5fae\u8c03\u4e0e\u96f6\u6837\u672c\u8bbe\u7f6e\u7684\u6027\u80fd\u5dee\u5f02\u3002", "result": "\u5fae\u8c03\u663e\u8457\u4f18\u4e8e\u96f6\u6837\u672c\u8bbe\u7f6e\uff0c\u5728\u5355\u7c92\u5ea6\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e0e\u516c\u5171\u548c\u5546\u4e1a\u7cfb\u7edf\u7ade\u4e89\u7684\u7ed3\u679c\u3002\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u8fbe\u52300.9\uff0c\u800c\u65af\u76ae\u5c14\u66fc\u7b49\u7ea7\u76f8\u5173\u7cfb\u6570\u7ea6\u4e3a0.6\uff0c\u8868\u660eSCC\u66f4\u597d\u5730\u53cd\u6620\u4e86\u987a\u5e8f\u4e00\u81f4\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u51f8\u663e\u4e86LMMs\u5728APA\u4e2d\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\uff0c\u6307\u51fa\u4e86\u672a\u6765\u5728\u7ec6\u7c92\u5ea6\u5efa\u6a21\u548c\u79e9\u611f\u77e5\u8bc4\u4f30\u65b9\u9762\u7684\u5de5\u4f5c\u65b9\u5411\u3002"}}
{"id": "2509.15888", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15888", "abs": "https://arxiv.org/abs/2509.15888", "authors": ["Senkang Hu", "Xudong Han", "Jinqi Jiang", "Yihang Tao", "Zihan Fang", "Sam Tak Wu Kwong", "Yuguang Fang"], "title": "Distribution-Aligned Decoding for Efficient LLM Task Adaptation", "comment": "Accepted by NeurIPS'25", "summary": "Adapting billion-parameter language models to a downstream task is still\ncostly, even with parameter-efficient fine-tuning (PEFT). We re-cast task\nadaptation as output-distribution alignment: the objective is to steer the\noutput distribution toward the task distribution directly during decoding\nrather than indirectly through weight updates. Building on this view, we\nintroduce Steering Vector Decoding (SVD), a lightweight, PEFT-compatible, and\ntheoretically grounded method. We start with a short warm-start fine-tune and\nextract a task-aware steering vector from the Kullback-Leibler (KL) divergence\ngradient between the output distribution of the warm-started and pre-trained\nmodels. This steering vector is then used to guide the decoding process to\nsteer the model's output distribution towards the task distribution. We\ntheoretically prove that SVD is first-order equivalent to the gradient step of\nfull fine-tuning and derive a globally optimal solution for the strength of the\nsteering vector. Across three tasks and nine benchmarks, SVD paired with four\nstandard PEFT methods improves multiple-choice accuracy by up to 5 points and\nopen-ended truthfulness by 2 points, with similar gains (1-2 points) on\ncommonsense datasets without adding trainable parameters beyond the PEFT\nadapter. SVD thus offers a lightweight, theoretically grounded path to stronger\ntask adaptation for large language models.", "AI": {"tldr": "\u63d0\u51faSteering Vector Decoding (SVD)\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f93\u51fa\u5206\u5e03\u5bf9\u9f50\u800c\u975e\u6743\u91cd\u66f4\u65b0\u6765\u6307\u5bfc\u89e3\u7801\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u4efb\u52a1\u9002\u5e94", "motivation": "\u4f20\u7edf\u53c2\u6570\u9ad8\u6548\u5fae\u8c03(PEFT)\u65b9\u6cd5\u4ecd\u5b58\u5728\u6210\u672c\u95ee\u9898\uff0c\u9700\u8981\u66f4\u8f7b\u91cf\u5316\u7684\u4efb\u52a1\u9002\u5e94\u65b9\u6848", "method": "\u5148\u8fdb\u884c\u77ed\u65f6\u9884\u70ed\u5fae\u8c03\uff0c\u4eceKL\u6563\u5ea6\u68af\u5ea6\u4e2d\u63d0\u53d6\u4efb\u52a1\u611f\u77e5\u7684\u8f6c\u5411\u5411\u91cf\uff0c\u5728\u89e3\u7801\u8fc7\u7a0b\u4e2d\u5f15\u5bfc\u6a21\u578b\u8f93\u51fa\u5206\u5e03\u5411\u4efb\u52a1\u5206\u5e03\u5bf9\u9f50", "result": "\u5728\u4e09\u4e2a\u4efb\u52a1\u4e5d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSVD\u4e0e\u56db\u79cd\u6807\u51c6PEFT\u65b9\u6cd5\u7ed3\u5408\uff0c\u591a\u9879\u9009\u62e9\u51c6\u786e\u7387\u63d0\u5347\u8fbe5\u5206\uff0c\u5f00\u653e\u751f\u6210\u771f\u5b9e\u6027\u63d0\u53472\u5206", "conclusion": "SVD\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u3001\u7406\u8bba\u57fa\u7840\u7684\u5f3a\u4efb\u52a1\u9002\u5e94\u8def\u5f84"}}
{"id": "2509.15901", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15901", "abs": "https://arxiv.org/abs/2509.15901", "authors": ["Frederic Kirstein", "Sonu Kumar", "Terry Ruas", "Bela Gipp"], "title": "Re-FRAME the Meeting Summarization SCOPE: Fact-Based Summarization and Personalization via Questions", "comment": "Accepted at EMNLP 2025", "summary": "Meeting summarization with large language models (LLMs) remains error-prone,\noften producing outputs with hallucinations, omissions, and irrelevancies. We\npresent FRAME, a modular pipeline that reframes summarization as a semantic\nenrichment task. FRAME extracts and scores salient facts, organizes them\nthematically, and uses these to enrich an outline into an abstractive summary.\nTo personalize summaries, we introduce SCOPE, a reason-out-loud protocol that\nhas the model build a reasoning trace by answering nine questions before\ncontent selection. For evaluation, we propose P-MESA, a multi-dimensional,\nreference-free evaluation framework to assess if a summary fits a target\nreader. P-MESA reliably identifies error instances, achieving >= 89% balanced\naccuracy against human annotations and strongly aligns with human severity\nratings (r >= 0.70). On QMSum and FAME, FRAME reduces hallucination and\nomission by 2 out of 5 points (measured with MESA), while SCOPE improves\nknowledge fit and goal alignment over prompt-only baselines. Our findings\nadvocate for rethinking summarization to improve control, faithfulness, and\npersonalization.", "AI": {"tldr": "FRAME\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6d41\u6c34\u7ebf\uff0c\u5c06\u4f1a\u8bae\u6458\u8981\u91cd\u6784\u4e3a\u8bed\u4e49\u4e30\u5bcc\u4efb\u52a1\uff0c\u901a\u8fc7\u63d0\u53d6\u548c\u7ec4\u7ec7\u5173\u952e\u4e8b\u5b9e\u6765\u51cf\u5c11\u5e7b\u89c9\u548c\u9057\u6f0f\u3002SCOPE\u534f\u8bae\u901a\u8fc7\u95ee\u7b54\u63a8\u7406\u5b9e\u73b0\u4e2a\u6027\u5316\u6458\u8981\u3002P-MESA\u8bc4\u4f30\u6846\u67b6\u80fd\u53ef\u9760\u8bc6\u522b\u9519\u8bef\uff0cFRAME\u5728QMSum\u548cFAME\u6570\u636e\u96c6\u4e0a\u663e\u8457\u51cf\u5c11\u4e86\u5e7b\u89c9\u548c\u9057\u6f0f\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4f1a\u8bae\u6458\u8981\u4efb\u52a1\u4e2d\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\u3001\u9057\u6f0f\u548c\u4e0d\u76f8\u5173\u5185\u5bb9\uff0c\u9700\u8981\u6539\u8fdb\u6458\u8981\u7684\u5fe0\u5b9e\u5ea6\u3001\u53ef\u63a7\u6027\u548c\u4e2a\u6027\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faFRAME\u6a21\u5757\u5316\u6d41\u6c34\u7ebf\uff1a\u63d0\u53d6\u548c\u8bc4\u5206\u5173\u952e\u4e8b\u5b9e\uff0c\u6309\u4e3b\u9898\u7ec4\u7ec7\uff0c\u7528\u8fd9\u4e9b\u4e8b\u5b9e\u4e30\u5bcc\u5927\u7eb2\u751f\u6210\u62bd\u8c61\u6458\u8981\u3002SCOPE\u534f\u8bae\u8ba9\u6a21\u578b\u901a\u8fc7\u56de\u7b54\u4e5d\u4e2a\u95ee\u9898\u6784\u5efa\u63a8\u7406\u8f68\u8ff9\u6765\u5b9e\u73b0\u4e2a\u6027\u5316\u3002P-MESA\u4f5c\u4e3a\u591a\u7ef4\u65e0\u53c2\u8003\u8bc4\u4f30\u6846\u67b6\u3002", "result": "P-MESA\u8bc4\u4f30\u6846\u67b6\u8fbe\u5230\u226589%\u7684\u5e73\u8861\u51c6\u786e\u7387\uff0c\u4e0e\u4eba\u7c7b\u4e25\u91cd\u6027\u8bc4\u7ea7\u5f3a\u76f8\u5173(r\u22650.70)\u3002FRAME\u5c06\u5e7b\u89c9\u548c\u9057\u6f0f\u51cf\u5c11\u4e862/5\u5206\uff0cSCOPE\u5728\u77e5\u8bc6\u5951\u5408\u5ea6\u548c\u76ee\u6807\u5bf9\u9f50\u65b9\u9762\u4f18\u4e8e\u4ec5\u63d0\u793a\u7684\u57fa\u7ebf\u3002", "conclusion": "\u7814\u7a76\u4e3b\u5f20\u91cd\u65b0\u601d\u8003\u6458\u8981\u65b9\u6cd5\uff0c\u4ee5\u6539\u8fdb\u53ef\u63a7\u6027\u3001\u5fe0\u5b9e\u6027\u548c\u4e2a\u6027\u5316\uff0c\u4e3a\u4f1a\u8bae\u6458\u8981\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u5b9a\u5236\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.15723", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15723", "abs": "https://arxiv.org/abs/2509.15723", "authors": ["Nannan Huang", "Haytham M. Fayek", "Xiuzhen Zhang"], "title": "REFER: Mitigating Bias in Opinion Summarisation via Frequency Framed Prompting", "comment": "Accepted to the 5th New Frontiers in Summarization Workshop\n  (NewSumm@EMNLP 2025)", "summary": "Individuals express diverse opinions, a fair summary should represent these\nviewpoints comprehensively. Previous research on fairness in opinion\nsummarisation using large language models (LLMs) relied on hyperparameter\ntuning or providing ground truth distributional information in prompts.\nHowever, these methods face practical limitations: end-users rarely modify\ndefault model parameters, and accurate distributional information is often\nunavailable. Building upon cognitive science research demonstrating that\nfrequency-based representations reduce systematic biases in human statistical\nreasoning by making reference classes explicit and reducing cognitive load,\nthis study investigates whether frequency framed prompting (REFER) can\nsimilarly enhance fairness in LLM opinion summarisation. Through systematic\nexperimentation with different prompting frameworks, we adapted techniques\nknown to improve human reasoning to elicit more effective information\nprocessing in language models compared to abstract probabilistic\nrepresentations.Our results demonstrate that REFER enhances fairness in\nlanguage models when summarising opinions. This effect is particularly\npronounced in larger language models and using stronger reasoning instructions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aREFER\u7684\u9891\u7387\u6846\u67b6\u63d0\u793a\u65b9\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u501f\u9274\u4eba\u7c7b\u8ba4\u77e5\u79d1\u5b66\u4e2d\u7684\u9891\u7387\u8868\u793a\u6765\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u610f\u89c1\u6458\u8981\u4e2d\u7684\u516c\u5e73\u6027\uff0c\u907f\u514d\u4e86\u5bf9\u8d85\u53c2\u6570\u8c03\u6574\u6216\u771f\u5b9e\u5206\u5e03\u4fe1\u606f\u7684\u4f9d\u8d56\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u516c\u5e73\u610f\u89c1\u6458\u8981\u65b9\u6cd5\u9700\u8981\u8d85\u53c2\u6570\u8c03\u6574\u6216\u63d0\u4f9b\u771f\u5b9e\u5206\u5e03\u4fe1\u606f\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff1a\u7ec8\u7aef\u7528\u6237\u5f88\u5c11\u4fee\u6539\u9ed8\u8ba4\u53c2\u6570\uff0c\u4e14\u51c6\u786e\u7684\u5206\u5e03\u4fe1\u606f\u901a\u5e38\u96be\u4ee5\u83b7\u5f97\u3002", "method": "\u57fa\u4e8e\u8ba4\u77e5\u79d1\u5b66\u7814\u7a76\uff0c\u5c06\u9891\u7387\u8868\u793a\u65b9\u6cd5\u5e94\u7528\u4e8e\u63d0\u793a\u6846\u67b6\uff08REFER\uff09\uff0c\u901a\u8fc7\u4f7f\u53c2\u8003\u7c7b\u522b\u660e\u786e\u5316\u5e76\u51cf\u5c11\u8ba4\u77e5\u8d1f\u8377\uff0c\u6765\u6539\u5584\u8bed\u8a00\u6a21\u578b\u7684\u4fe1\u606f\u5904\u7406\u6548\u679c\uff0c\u4e0e\u62bd\u8c61\u6982\u7387\u8868\u793a\u76f8\u6bd4\u66f4\u6709\u6548\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cREFER\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5728\u610f\u89c1\u6458\u8981\u4e2d\u7684\u516c\u5e73\u6027\uff0c\u7279\u522b\u662f\u5728\u66f4\u5927\u7684\u8bed\u8a00\u6a21\u578b\u548c\u4f7f\u7528\u66f4\u5f3a\u63a8\u7406\u6307\u4ee4\u65f6\u6548\u679c\u66f4\u4e3a\u660e\u663e\u3002", "conclusion": "\u9891\u7387\u6846\u67b6\u63d0\u793a\uff08REFER\uff09\u662f\u4e00\u79cd\u6709\u6548\u7684\u6280\u672f\uff0c\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u8d85\u53c2\u6570\u8c03\u6574\u6216\u771f\u5b9e\u5206\u5e03\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u610f\u89c1\u6458\u8981\u4efb\u52a1\u4e2d\u7684\u516c\u5e73\u6027\u8868\u73b0\u3002"}}
{"id": "2509.15974", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15974", "abs": "https://arxiv.org/abs/2509.15974", "authors": ["Baichuan Huang", "Ananth Balashankar", "Amir Aminifar"], "title": "BEFT: Bias-Efficient Fine-Tuning of Language Models", "comment": null, "summary": "Fine-tuning all-bias-terms stands out among various parameter-efficient\nfine-tuning (PEFT) techniques, owing to its out-of-the-box usability and\ncompetitive performance, especially in low-data regimes. Bias-only fine-tuning\nhas the potential for unprecedented parameter efficiency. However, the link\nbetween fine-tuning different bias terms (i.e., bias terms in the query, key,\nor value projections) and downstream performance remains unclear. The existing\napproaches, e.g., based on the magnitude of bias change or empirical Fisher\ninformation, provide limited guidance for selecting the particular bias term\nfor effective fine-tuning. In this paper, we propose an approach for selecting\nthe bias term to be fine-tuned, forming the foundation of our bias-efficient\nfine-tuning (BEFT). We extensively evaluate our bias-efficient approach against\nother bias-selection approaches, across a wide range of large language models\n(LLMs) spanning encoder-only and decoder-only architectures from 110M to 6.7B\nparameters. Our results demonstrate the effectiveness and superiority of our\nbias-efficient approach on diverse downstream tasks, including classification,\nmultiple-choice, and generation tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u504f\u7f6e\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5730\u5fae\u8c03\u7279\u5b9a\u504f\u7f6e\u9879\u6765\u63d0\u5347\u53c2\u6570\u6548\u7387\uff0c\u5728\u591a\u79cdLLM\u548c\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027", "motivation": "\u73b0\u6709\u7684\u504f\u7f6e\u9879\u5fae\u8c03\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u9009\u62e9\u54ea\u4e2a\u504f\u7f6e\u9879\u8fdb\u884c\u5fae\u8c03\u7684\u7cfb\u7edf\u6307\u5bfc\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u504f\u7f6e\u9009\u62e9\u7b56\u7565", "method": "\u63d0\u51fa\u504f\u7f6e\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u57fa\u4e8e\u7279\u5b9a\u6807\u51c6\u9009\u62e9query\u3001key\u6216value\u6295\u5f71\u4e2d\u7684\u504f\u7f6e\u9879\u8fdb\u884c\u5fae\u8c03", "result": "\u5728110M\u52306.7B\u53c2\u6570\u7684\u591a\u79cdLLM\u4e0a\u9a8c\u8bc1\uff0c\u5728\u5206\u7c7b\u3001\u591a\u9009\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u504f\u7f6e\u9009\u62e9\u65b9\u6cd5", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u504f\u7f6e\u9879\u5fae\u8c03\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u9009\u62e9\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u53c2\u6570\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u4e86\u6027\u80fd"}}
{"id": "2509.15739", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15739", "abs": "https://arxiv.org/abs/2509.15739", "authors": ["Reza Sanayei", "Srdjan Vesic", "Eduardo Blanco", "Mihai Surdeanu"], "title": "Can LLMs Judge Debates? Evaluating Non-Linear Reasoning via Argumentation Theory Semantics", "comment": "Accepted to EMNLP 2025 Findings", "summary": "Large Language Models (LLMs) excel at linear reasoning tasks but remain\nunderexplored on non-linear structures such as those found in natural debates,\nwhich are best expressed as argument graphs. We evaluate whether LLMs can\napproximate structured reasoning from Computational Argumentation Theory (CAT).\nSpecifically, we use Quantitative Argumentation Debate (QuAD) semantics, which\nassigns acceptability scores to arguments based on their attack and support\nrelations. Given only dialogue-formatted debates from two NoDE datasets, models\nare prompted to rank arguments without access to the underlying graph. We test\nseveral LLMs under advanced instruction strategies, including Chain-of-Thought\nand In-Context Learning. While models show moderate alignment with QuAD\nrankings, performance degrades with longer inputs or disrupted discourse flow.\nAdvanced prompting helps mitigate these effects by reducing biases related to\nargument length and position. Our findings highlight both the promise and\nlimitations of LLMs in modeling formal argumentation semantics and motivate\nfuture work on graph-aware reasoning.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8ba1\u7b97\u8bba\u8bc1\u7406\u8bba\u4e2d\u7684\u7ed3\u6784\u5316\u63a8\u7406\u80fd\u529b\uff0c\u7279\u522b\u5173\u6ce8\u5b9a\u91cf\u8bba\u8bc1\u8fa9\u8bba\u8bed\u4e49\uff0c\u6d4b\u8bd5\u6a21\u578b\u4ec5\u57fa\u4e8e\u5bf9\u8bdd\u683c\u5f0f\u7684\u8fa9\u8bba\u6765\u6392\u540d\u8bba\u8bc1\uff0c\u800c\u4e0d\u8bbf\u95ee\u5e95\u5c42\u56fe\u7ed3\u6784\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7ebf\u6027\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u81ea\u7136\u8fa9\u8bba\u7b49\u975e\u7ebf\u6027\u7ed3\u6784\u4e0a\u7684\u80fd\u529b\u4ecd\u6709\u5f85\u63a2\u7d22\uff0c\u8fd9\u4e9b\u7ed3\u6784\u6700\u597d\u7528\u8bba\u8bc1\u56fe\u8868\u793a\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30LLMs\u662f\u5426\u80fd\u8fd1\u4f3c\u8ba1\u7b97\u8bba\u8bc1\u7406\u8bba\u4e2d\u7684\u7ed3\u6784\u5316\u63a8\u7406\u3002", "method": "\u4f7f\u7528\u5b9a\u91cf\u8bba\u8bc1\u8fa9\u8bba\u8bed\u4e49\uff0c\u57fa\u4e8e\u4e24\u4e2aNoDE\u6570\u636e\u96c6\u7684\u5bf9\u8bdd\u683c\u5f0f\u8fa9\u8bba\uff0c\u6d4b\u8bd5\u591a\u4e2aLLM\u5728\u9ad8\u7ea7\u6307\u4ee4\u7b56\u7565\u4e0b\u7684\u8868\u73b0\uff0c\u5305\u62ec\u601d\u7ef4\u94fe\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u3002\u6a21\u578b\u88ab\u63d0\u793a\u5728\u6ca1\u6709\u5e95\u5c42\u56fe\u7ed3\u6784\u7684\u60c5\u51b5\u4e0b\u5bf9\u8bba\u8bc1\u8fdb\u884c\u6392\u540d\u3002", "result": "\u6a21\u578b\u4e0eQuAD\u6392\u540d\u663e\u793a\u51fa\u4e2d\u7b49\u7a0b\u5ea6\u7684\u4e00\u81f4\u6027\uff0c\u4f46\u968f\u7740\u8f93\u5165\u957f\u5ea6\u589e\u52a0\u6216\u8bdd\u8bed\u6d41\u88ab\u6253\u65ad\uff0c\u6027\u80fd\u4f1a\u4e0b\u964d\u3002\u9ad8\u7ea7\u63d0\u793a\u901a\u8fc7\u51cf\u5c11\u4e0e\u8bba\u8bc1\u957f\u5ea6\u548c\u4f4d\u7f6e\u76f8\u5173\u7684\u504f\u89c1\u6765\u5e2e\u52a9\u7f13\u89e3\u8fd9\u4e9b\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u51fa\u4e86LLMs\u5728\u5efa\u6a21\u5f62\u5f0f\u8bba\u8bc1\u8bed\u4e49\u65b9\u9762\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\uff0c\u5e76\u6fc0\u52b1\u672a\u6765\u5173\u4e8e\u56fe\u611f\u77e5\u63a8\u7406\u7684\u7814\u7a76\u3002"}}
{"id": "2509.16025", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16025", "abs": "https://arxiv.org/abs/2509.16025", "authors": ["Hong-Yun Lin", "Jhen-Ke Lin", "Chung-Chun Wang", "Hao-Chien Lu", "Berlin Chen"], "title": "Session-Level Spoken Language Assessment with a Multimodal Foundation Model via Multi-Target Learning", "comment": "Copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Spoken Language Assessment (SLA) estimates a learner's oral proficiency from\nspontaneous speech. The growing population of L2 English speakers has\nintensified the demand for reliable SLA, a critical component of Computer\nAssisted Language Learning (CALL). Existing efforts often rely on cascaded\npipelines, which are prone to error propagation, or end-to-end models that\noften operate on a short audio window, which might miss discourse-level\nevidence. This paper introduces a novel multimodal foundation model approach\nthat performs session-level evaluation in a single pass. Our approach couples\nmulti-target learning with a frozen, Whisper ASR model-based speech prior for\nacoustic-aware calibration, allowing for jointly learning holistic and\ntrait-level objectives of SLA without resorting to handcrafted features. By\ncoherently processing the entire response session of an L2 speaker, the model\nexcels at predicting holistic oral proficiency. Experiments conducted on the\nSpeak & Improve benchmark demonstrate that our proposed approach outperforms\nthe previous state-of-the-art cascaded system and exhibits robust cross-part\ngeneralization, producing a compact deployable grader that is tailored for CALL\napplications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u65b9\u6cd5\uff0c\u7528\u4e8e\u53e3\u8bed\u8bed\u8a00\u8bc4\u4f30\uff0c\u901a\u8fc7\u4f1a\u8bdd\u7ea7\u8bc4\u4f30\u548c\u58f0\u5b66\u611f\u77e5\u6821\u51c6\uff0c\u5728Speak & Improve\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u7ea7\u8054\u7cfb\u7edf\u3002", "motivation": "\u968f\u7740L2\u82f1\u8bed\u4f7f\u7528\u8005\u6570\u91cf\u7684\u589e\u957f\uff0c\u5bf9\u53ef\u9760\u53e3\u8bed\u8bc4\u4f30\u7684\u9700\u6c42\u65e5\u76ca\u589e\u52a0\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u9519\u8bef\u4f20\u64ad\u6216\u5ffd\u7565\u8bed\u7bc7\u7ea7\u8bc1\u636e\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u591a\u76ee\u6807\u5b66\u4e60\u4e0e\u57fa\u4e8eWhisper ASR\u6a21\u578b\u7684\u51bb\u7ed3\u8bed\u97f3\u5148\u9a8c\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u8fdb\u884c\u58f0\u5b66\u611f\u77e5\u6821\u51c6\uff0c\u65e0\u9700\u624b\u5de5\u7279\u5f81\u5373\u53ef\u8054\u5408\u5b66\u4e60\u6574\u4f53\u548c\u7279\u8d28\u7ea7\u76ee\u6807\u3002", "result": "\u5728Speak & Improve\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6240\u63d0\u65b9\u6cd5\u4f18\u4e8e\u5148\u524d\u6700\u5148\u8fdb\u7684\u7ea7\u8054\u7cfb\u7edf\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u90e8\u5206\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8ba1\u7b97\u673a\u8f85\u52a9\u8bed\u8a00\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7d27\u51d1\u53ef\u90e8\u7f72\u7684\u8bc4\u5206\u5668\uff0c\u80fd\u591f\u8fde\u8d2f\u5904\u7406\u6574\u4e2a\u54cd\u5e94\u4f1a\u8bdd\uff0c\u5728\u9884\u6d4b\u6574\u4f53\u53e3\u8bed\u719f\u7ec3\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.15763", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15763", "abs": "https://arxiv.org/abs/2509.15763", "authors": ["Chenlong Deng", "Zhisong Zhang", "Kelong Mao", "Shuaiyi Li", "Tianqing Fang", "Hongming Zhang", "Haitao Mi", "Dong Yu", "Zhicheng Dou"], "title": "UniGist: Towards General and Hardware-aligned Sequence-level Long Context Compression", "comment": "15 pages, 7 figures", "summary": "Large language models are increasingly capable of handling long-context\ninputs, but the memory overhead of key-value (KV) cache remains a major\nbottleneck for general-purpose deployment. While various compression strategies\nhave been explored, sequence-level compression, which drops the full KV caches\nfor certain tokens, is particularly challenging as it can lead to the loss of\nimportant contextual information. To address this, we introduce UniGist, a\nsequence-level long-context compression framework that efficiently preserves\ncontext information by replacing raw tokens with special compression tokens\n(gists) in a fine-grained manner. We adopt a chunk-free training strategy and\ndesign an efficient kernel with a gist shift trick, enabling optimized GPU\ntraining. Our scheme also supports flexible inference by allowing the actual\nremoval of compressed tokens, resulting in real-time memory savings.\nExperiments across multiple long-context tasks demonstrate that UniGist\nsignificantly improves compression quality, with especially strong performance\nin detail-recalling tasks and long-range dependency modeling.", "AI": {"tldr": "UniGist\u662f\u4e00\u4e2a\u5e8f\u5217\u7ea7\u957f\u4e0a\u4e0b\u6587\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u7528\u7279\u6b8a\u538b\u7f29\u6807\u8bb0\u66ff\u6362\u539f\u59cb\u6807\u8bb0\u6765\u9ad8\u6548\u4fdd\u7559\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u89e3\u51b3\u4e86KV\u7f13\u5b58\u5185\u5b58\u5f00\u9500\u5927\u7684\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u8f93\u5165\u65f6\uff0c\u952e\u503c\u7f13\u5b58\u7684\u5185\u5b58\u5f00\u9500\u6210\u4e3a\u4e3b\u8981\u74f6\u9888\u3002\u73b0\u6709\u7684\u5e8f\u5217\u7ea7\u538b\u7f29\u65b9\u6cd5\u5bb9\u6613\u4e22\u5931\u91cd\u8981\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "method": "\u91c7\u7528\u65e0\u5206\u5757\u8bad\u7ec3\u7b56\u7565\uff0c\u8bbe\u8ba1\u5e26\u6709gist\u79fb\u4f4d\u6280\u5de7\u7684\u9ad8\u6548\u5185\u6838\uff0c\u652f\u6301\u901a\u8fc7\u5b9e\u9645\u79fb\u9664\u538b\u7f29\u6807\u8bb0\u5b9e\u73b0\u7075\u6d3b\u63a8\u7406\u3002", "result": "\u5728\u591a\u4e2a\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cUniGist\u663e\u8457\u63d0\u9ad8\u4e86\u538b\u7f29\u8d28\u91cf\uff0c\u5728\u7ec6\u8282\u56de\u5fc6\u4efb\u52a1\u548c\u957f\u8ddd\u79bb\u4f9d\u8d56\u5efa\u6a21\u65b9\u9762\u8868\u73b0\u5c24\u5176\u51fa\u8272\u3002", "conclusion": "UniGist\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u538b\u7f29\u4e2d\u7684\u4fe1\u606f\u4fdd\u7559\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u5185\u5b58\u8282\u7701\u548c\u4f18\u5316\u7684GPU\u8bad\u7ec3\u3002"}}
{"id": "2509.16028", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16028", "abs": "https://arxiv.org/abs/2509.16028", "authors": ["Sang Hoon Woo", "Sehun Lee", "Kang-wook Kim", "Gunhee Kim"], "title": "Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech", "comment": "EMNLP 2025 Main. Project page: https://yhytoto12.github.io/TVS-ReVerT", "summary": "Spoken dialogue systems increasingly employ large language models (LLMs) to\nleverage their advanced reasoning capabilities. However, direct application of\nLLMs in spoken communication often yield suboptimal results due to mismatches\nbetween optimal textual and verbal delivery. While existing approaches adapt\nLLMs to produce speech-friendly outputs, their impact on reasoning performance\nremains underexplored. In this work, we propose Think-Verbalize-Speak, a\nframework that decouples reasoning from spoken delivery to preserve the full\nreasoning capacity of LLMs. Central to our method is verbalizing, an\nintermediate step that translates thoughts into natural, speech-ready text. We\nalso introduce ReVerT, a latency-efficient verbalizer based on incremental and\nasynchronous summarization. Experiments across multiple benchmarks show that\nour method enhances speech naturalness and conciseness with minimal impact on\nreasoning. The project page with the dataset and the source code is available\nat https://yhytoto12.github.io/TVS-ReVerT", "AI": {"tldr": "\u63d0\u51fa\u4e86Think-Verbalize-Speak\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u63a8\u7406\u4e0e\u8bed\u97f3\u8f93\u51fa\u89e3\u8026\u6765\u4fdd\u6301LLMs\u7684\u5b8c\u6574\u63a8\u7406\u80fd\u529b\uff0c\u5f15\u5165ReVerT\u4f5c\u4e3a\u5ef6\u8fdf\u9ad8\u6548\u7684verbalizer\u3002", "motivation": "LLMs\u5728\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u76f4\u63a5\u5e94\u7528\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u4e3a\u6587\u672c\u548c\u8bed\u97f3\u4f20\u9012\u65b9\u5f0f\u5b58\u5728\u4e0d\u5339\u914d\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u9002\u5e94LLMs\u4ea7\u751f\u8bed\u97f3\u53cb\u597d\u8f93\u51fa\u65f6\u5bf9\u63a8\u7406\u6027\u80fd\u7684\u5f71\u54cd\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u63d0\u51faTVS\u6846\u67b6\uff0c\u5305\u542b\u63a8\u7406\u3001verbalizing\uff08\u5c06\u601d\u60f3\u8f6c\u5316\u4e3a\u81ea\u7136\u3001\u8bed\u97f3\u5c31\u7eea\u7684\u6587\u672c\uff09\u548c\u8bed\u97f3\u8f93\u51fa\u4e09\u4e2a\u6b65\u9aa4\uff0c\u5e76\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u589e\u91cf\u5f02\u6b65\u6458\u8981\u7684ReVerT verbalizer\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8bed\u97f3\u81ea\u7136\u5ea6\u548c\u7b80\u6d01\u6027\uff0c\u540c\u65f6\u5bf9\u63a8\u7406\u6027\u80fd\u5f71\u54cd\u6700\u5c0f\u3002", "conclusion": "Think-Verbalize-Speak\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLMs\u5728\u8bed\u97f3\u5bf9\u8bdd\u4e2d\u7684\u9002\u914d\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\u6539\u5584\u4e86\u8bed\u97f3\u8f93\u51fa\u8d28\u91cf\u3002"}}
{"id": "2509.15789", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15789", "abs": "https://arxiv.org/abs/2509.15789", "authors": ["Qiuyang Lu", "Fangjian Shen", "Zhengkai Tang", "Qiang Liu", "Hexuan Cheng", "Hui Liu", "Wushao Wen"], "title": "UPRPRC: Unified Pipeline for Reproducing Parallel Resources -- Corpus from the United Nations", "comment": "5 pages, 1 figure, submitted to ICASSP2026", "summary": "The quality and accessibility of multilingual datasets are crucial for\nadvancing machine translation. However, previous corpora built from United\nNations documents have suffered from issues such as opaque process, difficulty\nof reproduction, and limited scale. To address these challenges, we introduce a\ncomplete end-to-end solution, from data acquisition via web scraping to text\nalignment. The entire process is fully reproducible, with a minimalist\nsingle-machine example and optional distributed computing steps for\nscalability. At its core, we propose a new Graph-Aided Paragraph Alignment\n(GAPA) algorithm for efficient and flexible paragraph-level alignment. The\nresulting corpus contains over 713 million English tokens, more than doubling\nthe scale of prior work. To the best of our knowledge, this represents the\nlargest publicly available parallel corpus composed entirely of\nhuman-translated, non-AI-generated content. Our code and corpus are accessible\nunder the MIT License.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u6784\u5efa\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u5e73\u884c\u8bed\u6599\u5e93\uff0c\u89e3\u51b3\u4e86\u4ee5\u5f80\u8054\u5408\u56fd\u6587\u6863\u8bed\u6599\u5e93\u5b58\u5728\u7684\u4e0d\u900f\u660e\u3001\u96be\u4ee5\u590d\u73b0\u548c\u89c4\u6a21\u6709\u9650\u7b49\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8054\u5408\u56fd\u6587\u6863\u7684\u591a\u8bed\u8a00\u6570\u636e\u96c6\u5b58\u5728\u8fc7\u7a0b\u4e0d\u900f\u660e\u3001\u96be\u4ee5\u590d\u73b0\u548c\u89c4\u6a21\u6709\u9650\u7684\u95ee\u9898\uff0c\u8fd9\u9650\u5236\u4e86\u673a\u5668\u7ffb\u8bd1\u7684\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u5b8c\u6574\u7684\u7aef\u5230\u7aef\u6d41\u7a0b\uff0c\u5305\u62ec\u7f51\u7edc\u722c\u866b\u83b7\u53d6\u6570\u636e\u548c\u6587\u672c\u5bf9\u9f50\u3002\u6838\u5fc3\u662f\u63d0\u51fa\u65b0\u7684\u56fe\u8f85\u52a9\u6bb5\u843d\u5bf9\u9f50\uff08GAPA\uff09\u7b97\u6cd5\uff0c\u652f\u6301\u9ad8\u6548\u7075\u6d3b\u7684\u6bb5\u843d\u7ea7\u5bf9\u9f50\u3002\u6574\u4e2a\u8fc7\u7a0b\u5b8c\u5168\u53ef\u590d\u73b0\uff0c\u63d0\u4f9b\u5355\u673a\u793a\u4f8b\u548c\u53ef\u9009\u7684\u5206\u5e03\u5f0f\u8ba1\u7b97\u6b65\u9aa4\u3002", "result": "\u6784\u5efa\u7684\u8bed\u6599\u5e93\u5305\u542b\u8d85\u8fc77.13\u4ebf\u4e2a\u82f1\u6587\u8bcd\u5143\uff0c\u89c4\u6a21\u662f\u5148\u524d\u5de5\u4f5c\u7684\u4e24\u500d\u4ee5\u4e0a\uff0c\u662f\u76ee\u524d\u6700\u5927\u7684\u5b8c\u5168\u7531\u4eba\u5de5\u7ffb\u8bd1\u3001\u975eAI\u751f\u6210\u5185\u5bb9\u7684\u516c\u5f00\u5e73\u884c\u8bed\u6599\u5e93\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u590d\u73b0\u7684\u5927\u89c4\u6a21\u5e73\u884c\u8bed\u6599\u5e93\u6784\u5efa\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u8bed\u6599\u5e93\u5728MIT\u8bb8\u53ef\u4e0b\u516c\u5f00\u53ef\u7528\uff0c\u5c06\u4fc3\u8fdb\u673a\u5668\u7ffb\u8bd1\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.16093", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16093", "abs": "https://arxiv.org/abs/2509.16093", "authors": ["Fangyi Yu", "Nabeel Seedat", "Dasha Herrmannova", "Frank Schilder", "Jonathan Richard Schwarz"], "title": "Beyond Pointwise Scores: Decomposed Criteria-Based Evaluation of LLM Responses", "comment": null, "summary": "Evaluating long-form answers in high-stakes domains such as law or medicine\nremains a fundamental challenge. Standard metrics like BLEU and ROUGE fail to\ncapture semantic correctness, and current LLM-based evaluators often reduce\nnuanced aspects of answer quality into a single undifferentiated score. We\nintroduce DeCE, a decomposed LLM evaluation framework that separates precision\n(factual accuracy and relevance) and recall (coverage of required concepts),\nusing instance-specific criteria automatically extracted from gold answer\nrequirements. DeCE is model-agnostic and domain-general, requiring no\npredefined taxonomies or handcrafted rubrics. We instantiate DeCE to evaluate\ndifferent LLMs on a real-world legal QA task involving multi-jurisdictional\nreasoning and citation grounding. DeCE achieves substantially stronger\ncorrelation with expert judgments ($r=0.78$), compared to traditional metrics\n($r=0.12$), pointwise LLM scoring ($r=0.35$), and modern multidimensional\nevaluators ($r=0.48$). It also reveals interpretable trade-offs: generalist\nmodels favor recall, while specialized models favor precision. Importantly,\nonly 11.95% of LLM-generated criteria required expert revision, underscoring\nDeCE's scalability. DeCE offers an interpretable and actionable LLM evaluation\nframework in expert domains.", "AI": {"tldr": "DeCE\u662f\u4e00\u4e2a\u5206\u89e3\u5f0fLLM\u8bc4\u4f30\u6846\u67b6\uff0c\u5c06\u7b54\u6848\u8d28\u91cf\u5206\u89e3\u4e3a\u7cbe\u786e\u5ea6\uff08\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u76f8\u5173\u6027\uff09\u548c\u53ec\u56de\u7387\uff08\u6240\u9700\u6982\u5ff5\u7684\u8986\u76d6\u5ea6\uff09\uff0c\u4f7f\u7528\u4ece\u9ec4\u91d1\u7b54\u6848\u8981\u6c42\u4e2d\u81ea\u52a8\u63d0\u53d6\u7684\u5b9e\u4f8b\u7279\u5b9a\u6807\u51c6\u3002", "motivation": "\u5728\u9ad8\u98ce\u9669\u9886\u57df\uff08\u5982\u6cd5\u5f8b\u6216\u533b\u5b66\uff09\u8bc4\u4f30\u957f\u7bc7\u7b54\u6848\u5b58\u5728\u6311\u6218\uff0c\u4f20\u7edf\u6307\u6807\u5982BLEU\u548cROUGE\u65e0\u6cd5\u6355\u6349\u8bed\u4e49\u6b63\u786e\u6027\uff0c\u73b0\u6709LLM\u8bc4\u4f30\u5668\u5f80\u5f80\u5c06\u7b54\u6848\u8d28\u91cf\u7684\u7ec6\u5fae\u5dee\u522b\u7b80\u5316\u4e3a\u5355\u4e00\u5206\u6570\u3002", "method": "DeCE\u6846\u67b6\u6a21\u578b\u65e0\u5173\u4e14\u9886\u57df\u901a\u7528\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u5206\u7c7b\u6cd5\u6216\u624b\u5de5\u5236\u4f5c\u7684\u8bc4\u5206\u6807\u51c6\uff0c\u901a\u8fc7\u81ea\u52a8\u4ece\u9ec4\u91d1\u7b54\u6848\u8981\u6c42\u4e2d\u63d0\u53d6\u5b9e\u4f8b\u7279\u5b9a\u6807\u51c6\u6765\u8bc4\u4f30LLM\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6cd5\u5f8bQA\u4efb\u52a1\u4e2d\uff0cDeCE\u4e0e\u4e13\u5bb6\u5224\u65ad\u7684\u76f8\u5173\u6027\u8fbe\u52300.78\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6307\u6807\uff080.12\uff09\u3001\u9010\u70b9LLM\u8bc4\u5206\uff080.35\uff09\u548c\u73b0\u4ee3\u591a\u7ef4\u8bc4\u4f30\u5668\uff080.48\uff09\u3002", "conclusion": "DeCE\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u4e14\u53ef\u64cd\u4f5c\u7684LLM\u8bc4\u4f30\u6846\u67b6\uff0c\u5728\u4e13\u5bb6\u9886\u57df\u4e2d\u5177\u6709\u53ef\u6269\u5c55\u6027\uff0c\u4ec5\u970011.95%\u7684LLM\u751f\u6210\u6807\u51c6\u9700\u8981\u4e13\u5bb6\u4fee\u8ba2\u3002"}}
{"id": "2509.15793", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15793", "abs": "https://arxiv.org/abs/2509.15793", "authors": ["Yufeng Li", "Arkaitz Zubiaga"], "title": "RAVE: Retrieval and Scoring Aware Verifiable Claim Detection", "comment": "5 pages, 1 figure", "summary": "The rapid spread of misinformation on social media underscores the need for\nscalable fact-checking tools. A key step is claim detection, which identifies\nstatements that can be objectively verified. Prior approaches often rely on\nlinguistic cues or claim check-worthiness, but these struggle with vague\npolitical discourse and diverse formats such as tweets. We present RAVE\n(Retrieval and Scoring Aware Verifiable Claim Detection), a framework that\ncombines evidence retrieval with structured signals of relevance and source\ncredibility. Experiments on CT22-test and PoliClaim-test show that RAVE\nconsistently outperforms text-only and retrieval-based baselines in both\naccuracy and F1.", "AI": {"tldr": "RAVE\u6846\u67b6\u7ed3\u5408\u8bc1\u636e\u68c0\u7d22\u4e0e\u7ed3\u6784\u5316\u4fe1\u53f7\uff0c\u5728\u793e\u4ea4\u5a92\u4f53\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u4e2d\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u793e\u4ea4\u5a92\u4f53\u865a\u5047\u4fe1\u606f\u5feb\u901f\u4f20\u64ad\uff0c\u9700\u8981\u53ef\u6269\u5c55\u7684\u4e8b\u5b9e\u6838\u67e5\u5de5\u5177\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u6a21\u7cca\u653f\u6cbb\u8a00\u8bba\u548c\u591a\u6837\u5316\u683c\u5f0f", "method": "\u63d0\u51faRAVE\u6846\u67b6\uff0c\u7ed3\u5408\u8bc1\u636e\u68c0\u7d22\u4e0e\u76f8\u5173\u6027\u3001\u6765\u6e90\u53ef\u4fe1\u5ea6\u7684\u7ed3\u6784\u5316\u4fe1\u53f7\u8fdb\u884c\u53ef\u9a8c\u8bc1\u58f0\u660e\u68c0\u6d4b", "result": "\u5728CT22-test\u548cPoliClaim-test\u6570\u636e\u96c6\u4e0a\uff0cRAVE\u5728\u51c6\u786e\u7387\u548cF1\u5206\u6570\u4e0a\u5747\u4f18\u4e8e\u7eaf\u6587\u672c\u548c\u57fa\u4e8e\u68c0\u7d22\u7684\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "RAVE\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u8bc1\u636e\u68c0\u7d22\u548c\u7ed3\u6784\u5316\u4fe1\u53f7\uff0c\u6709\u6548\u63d0\u5347\u4e86\u793e\u4ea4\u5a92\u4f53\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u7684\u6027\u80fd"}}
{"id": "2509.16188", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16188", "abs": "https://arxiv.org/abs/2509.16188", "authors": ["Jinghao Zhang", "Sihang Jiang", "Shiwei Guo", "Shisong Chen", "Yanghua Xiao", "Hongwei Feng", "Jiaqing Liang", "Minggui HE", "Shimin Tao", "Hongxia Ma"], "title": "CultureScope: A Dimensional Lens for Probing Cultural Understanding in LLMs", "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in diverse cultural\nenvironments, evaluating their cultural understanding capability has become\nessential for ensuring trustworthy and culturally aligned applications.\nHowever, most existing benchmarks lack comprehensiveness and are challenging to\nscale and adapt across different cultural contexts, because their frameworks\noften lack guidance from well-established cultural theories and tend to rely on\nexpert-driven manual annotations. To address these issues, we propose\nCultureScope, the most comprehensive evaluation framework to date for assessing\ncultural understanding in LLMs. Inspired by the cultural iceberg theory, we\ndesign a novel dimensional schema for cultural knowledge classification,\ncomprising 3 layers and 140 dimensions, which guides the automated construction\nof culture-specific knowledge bases and corresponding evaluation datasets for\nany given languages and cultures. Experimental results demonstrate that our\nmethod can effectively evaluate cultural understanding. They also reveal that\nexisting large language models lack comprehensive cultural competence, and\nmerely incorporating multilingual data does not necessarily enhance cultural\nunderstanding. All code and data files are available at\nhttps://github.com/HoganZinger/Culture", "AI": {"tldr": "\u63d0\u51fa\u4e86CultureScope\u6846\u67b6\uff0c\u57fa\u4e8e\u6587\u5316\u51b0\u5c71\u7406\u8bba\u6784\u5efa3\u5c42140\u7ef4\u5ea6\u7684\u6587\u5316\u77e5\u8bc6\u5206\u7c7b\u4f53\u7cfb\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6587\u5316\u7406\u89e3\u80fd\u529b", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u6587\u5316\u73af\u5883\u4e2d\u7684\u90e8\u7f72\uff0c\u8bc4\u4f30\u5176\u6587\u5316\u7406\u89e3\u80fd\u529b\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u7f3a\u4e4f\u5168\u9762\u6027\u4e14\u96be\u4ee5\u8de8\u6587\u5316\u6269\u5c55", "method": "\u57fa\u4e8e\u6587\u5316\u51b0\u5c71\u7406\u8bba\u8bbe\u8ba1\u7ef4\u5ea6\u5316\u5206\u7c7b\u6a21\u5f0f\uff0c\u81ea\u52a8\u5316\u6784\u5efa\u7279\u5b9a\u6587\u5316\u7684\u77e5\u8bc6\u5e93\u548c\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u652f\u6301\u4efb\u610f\u8bed\u8a00\u548c\u6587\u5316\u7684\u8bc4\u4f30", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8bc4\u4f30\u6587\u5316\u7406\u89e3\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u5168\u9762\u7684\u6587\u5316\u80fd\u529b\uff0c\u4ec5\u589e\u52a0\u591a\u8bed\u8a00\u6570\u636e\u5e76\u4e0d\u80fd\u63d0\u5347\u6587\u5316\u7406\u89e3", "conclusion": "CultureScope\u662f\u8fc4\u4eca\u4e3a\u6b62\u6700\u5168\u9762\u7684\u6587\u5316\u7406\u89e3\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u6587\u5316\u7406\u89e3\u65b9\u9762\u7684\u5c40\u9650\u6027"}}
{"id": "2509.16198", "categories": ["cs.CL", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16198", "abs": "https://arxiv.org/abs/2509.16198", "authors": ["Jane Luo", "Xin Zhang", "Steven Liu", "Jie Wu", "Yiming Huang", "Yangyu Huang", "Chengyu Yin", "Ying Xin", "Jianfeng Liu", "Yuefeng Zhan", "Hao Sun", "Qi Chen", "Scarlett Li", "Mao Yang"], "title": "RPG: A Repository Planning Graph for Unified and Scalable Codebase Generation", "comment": null, "summary": "Large language models excel at function- and file-level code generation, yet\ngenerating complete repositories from scratch remains a fundamental challenge.\nThis process demands coherent and reliable planning across proposal- and\nimplementation-level stages, while natural language, due to its ambiguity and\nverbosity, is ill-suited for faithfully representing complex software\nstructures. To address this, we introduce the Repository Planning Graph (RPG),\na persistent representation that unifies proposal- and implementation-level\nplanning by encoding capabilities, file structures, data flows, and functions\nin one graph. RPG replaces ambiguous natural language with an explicit\nblueprint, enabling long-horizon planning and scalable repository generation.\nBuilding on RPG, we develop ZeroRepo, a graph-driven framework for repository\ngeneration from scratch. It operates in three stages: proposal-level planning\nand implementation-level refinement to construct the graph, followed by\ngraph-guided code generation with test validation. To evaluate this setting, we\nconstruct RepoCraft, a benchmark of six real-world projects with 1,052 tasks.\nOn RepoCraft, ZeroRepo produces repositories averaging nearly 36K LOC, roughly\n3.9$\\times$ the strongest baseline (Claude Code) and about 64$\\times$ other\nbaselines. It attains 81.5% functional coverage and a 69.7% pass rate,\nexceeding Claude Code by 27.3 and 35.8 percentage points, respectively. Further\nanalysis shows that RPG models complex dependencies, enables progressively more\nsophisticated planning through near-linear scaling, and enhances LLM\nunderstanding of repositories, thereby accelerating agent localization.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Repository Planning Graph (RPG)\u548cZeroRepo\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u4ece\u96f6\u751f\u6210\u5b8c\u6574\u4ee3\u7801\u4ed3\u5e93\u7684\u6311\u6218\u3002RPG\u901a\u8fc7\u56fe\u5f62\u5316\u8868\u793a\u7edf\u4e00\u4e86\u63d0\u6848\u7ea7\u548c\u5b9e\u73b0\u7ea7\u89c4\u5212\uff0c\u53d6\u4ee3\u4e86\u6a21\u7cca\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u3002\u5728RepoCraft\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cZeroRepo\u751f\u6210\u7684\u4ee3\u7801\u91cf\u662fClaude Code\u76843.9\u500d\uff0c\u529f\u80fd\u8986\u76d6\u7387\u548c\u901a\u8fc7\u7387\u5206\u522b\u8fbe\u523081.5%\u548c69.7%\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u51fd\u6570\u548c\u6587\u4ef6\u7ea7\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4ece\u96f6\u751f\u6210\u5b8c\u6574\u4ee3\u7801\u4ed3\u5e93\u4ecd\u9762\u4e34\u6311\u6218\u3002\u81ea\u7136\u8bed\u8a00\u7531\u4e8e\u5176\u6a21\u7cca\u6027\u548c\u5197\u957f\u6027\uff0c\u4e0d\u9002\u5408\u51c6\u786e\u8868\u793a\u590d\u6742\u7684\u8f6f\u4ef6\u7ed3\u6784\u3002", "method": "\u63d0\u51fa\u4e86Repository Planning Graph (RPG)\u4f5c\u4e3a\u6301\u4e45\u5316\u8868\u793a\uff0c\u7edf\u4e00\u4e86\u63d0\u6848\u7ea7\u548c\u5b9e\u73b0\u7ea7\u89c4\u5212\u3002\u57fa\u4e8eRPG\u5f00\u53d1\u4e86ZeroRepo\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1a\u63d0\u6848\u7ea7\u89c4\u5212\u3001\u5b9e\u73b0\u7ea7\u7ec6\u5316\u548c\u56fe\u5f62\u5f15\u5bfc\u7684\u4ee3\u7801\u751f\u6210\u4e0e\u6d4b\u8bd5\u9a8c\u8bc1\u3002", "result": "\u5728RepoCraft\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u542b6\u4e2a\u771f\u5b9e\u9879\u76ee1052\u4e2a\u4efb\u52a1\uff09\u4e0a\uff0cZeroRepo\u751f\u6210\u7684\u4ed3\u5e93\u5e73\u5747\u63a5\u8fd136K\u884c\u4ee3\u7801\uff0c\u662fClaude Code\u76843.9\u500d\uff0c\u529f\u80fd\u8986\u76d6\u7387\u8fbe\u523081.5%\uff0c\u901a\u8fc7\u7387\u4e3a69.7%\uff0c\u5206\u522b\u6bd4Claude Code\u9ad8\u51fa27.3\u548c35.8\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "RPG\u80fd\u591f\u6709\u6548\u5efa\u6a21\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\uff0c\u901a\u8fc7\u8fd1\u4f3c\u7ebf\u6027\u7f29\u653e\u5b9e\u73b0\u6e10\u8fdb\u5f0f\u590d\u6742\u89c4\u5212\uff0c\u5e76\u589e\u5f3aLLM\u5bf9\u4ed3\u5e93\u7684\u7406\u89e3\uff0c\u4ece\u800c\u52a0\u901f\u667a\u80fd\u4f53\u5b9a\u4f4d\u3002"}}
{"id": "2509.15837", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.15837", "abs": "https://arxiv.org/abs/2509.15837", "authors": ["Adrian Sauter", "Willem Zuidema", "Marianne de Heer Kloots"], "title": "The Curious Case of Visual Grounding: Different Effects for Speech- and Text-based Language Encoders", "comment": "5 pages, 3 figures, Submitted to ICASSP 2026", "summary": "How does visual information included in training affect language processing\nin audio- and text-based deep learning models? We explore how such visual\ngrounding affects model-internal representations of words, and find\nsubstantially different effects in speech- vs. text-based language encoders.\nFirstly, global representational comparisons reveal that visual grounding\nincreases alignment between representations of spoken and written language, but\nthis effect seems mainly driven by enhanced encoding of word identity rather\nthan meaning. We then apply targeted clustering analyses to probe for phonetic\nvs. semantic discriminability in model representations. Speech-based\nrepresentations remain phonetically dominated with visual grounding, but in\ncontrast to text-based representations, visual grounding does not improve\nsemantic discriminability. Our findings could usefully inform the development\nof more efficient methods to enrich speech-based models with visually-informed\nsemantics.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u89c6\u89c9\u4fe1\u606f\u5728\u8bad\u7ec3\u4e2d\u5bf9\u97f3\u9891\u548c\u6587\u672c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bed\u8a00\u5904\u7406\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u89c6\u89c9\u57fa\u7840\u5bf9\u8bed\u97f3\u548c\u6587\u672c\u7f16\u7801\u5668\u4ea7\u751f\u4e0d\u540c\u6548\u679c\uff1a\u589e\u5f3a\u8bed\u97f3\u548c\u6587\u672c\u8868\u793a\u7684\u5bf9\u9f50\uff0c\u4f46\u4e3b\u8981\u662f\u901a\u8fc7\u6539\u8fdb\u8bcd\u6c47\u8eab\u4efd\u7f16\u7801\u800c\u975e\u8bed\u4e49\u7f16\u7801\u3002", "motivation": "\u63a2\u7d22\u89c6\u89c9\u57fa\u7840\u5982\u4f55\u5f71\u54cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\u8bed\u8a00\u5904\u7406\u7684\u5185\u90e8\u8868\u793a\uff0c\u7279\u522b\u662f\u6bd4\u8f83\u8bed\u97f3\u548c\u6587\u672c\u7f16\u7801\u5668\u5728\u89c6\u89c9\u57fa\u7840\u4e0b\u7684\u8868\u73b0\u5dee\u5f02\u3002", "method": "\u4f7f\u7528\u5168\u5c40\u8868\u793a\u6bd4\u8f83\u548c\u76ee\u6807\u805a\u7c7b\u5206\u6790\uff0c\u8bc4\u4f30\u89c6\u89c9\u57fa\u7840\u5bf9\u8bed\u97f3\u548c\u6587\u672c\u8868\u793a\u4e2d\u8bed\u97f3\u7279\u5f81\u4e0e\u8bed\u4e49\u53ef\u533a\u5206\u6027\u7684\u5f71\u54cd\u3002", "result": "\u89c6\u89c9\u57fa\u7840\u63d0\u9ad8\u4e86\u8bed\u97f3\u548c\u6587\u672c\u8868\u793a\u7684\u5bf9\u9f50\u5ea6\uff0c\u4f46\u4e3b\u8981\u6539\u5584\u8bcd\u6c47\u8eab\u4efd\u7f16\u7801\uff1b\u8bed\u97f3\u8868\u793a\u4ecd\u4ee5\u8bed\u97f3\u7279\u5f81\u4e3a\u4e3b\uff0c\u89c6\u89c9\u57fa\u7840\u672a\u80fd\u663e\u8457\u63d0\u5347\u8bed\u4e49\u53ef\u533a\u5206\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u53ef\u4e3a\u5f00\u53d1\u66f4\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5c06\u89c6\u89c9\u4fe1\u606f\u8bed\u4e49\u878d\u5165\u8bed\u97f3\u6a21\u578b\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2509.15839", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15839", "abs": "https://arxiv.org/abs/2509.15839", "authors": ["Zhongze Luo", "Zhenshuai Yin", "Yongxin Guo", "Zhichao Wang", "Jionghao Zhu", "Xiaoying Tang"], "title": "Multi-Physics: A Comprehensive Benchmark for Multimodal LLMs Reasoning on Chinese Multi-Subject Physics Problems", "comment": null, "summary": "While multimodal LLMs (MLLMs) demonstrate remarkable reasoning progress,\ntheir application in specialized scientific domains like physics reveals\nsignificant gaps in current evaluation benchmarks. Specifically, existing\nbenchmarks often lack fine-grained subject coverage, neglect the step-by-step\nreasoning process, and are predominantly English-centric, failing to\nsystematically evaluate the role of visual information. Therefore, we introduce\n\\textbf {Multi-Physics} for Chinese physics reasoning, a comprehensive\nbenchmark that includes 5 difficulty levels, featuring 1,412 image-associated,\nmultiple-choice questions spanning 11 high-school physics subjects. We employ a\ndual evaluation framework to evaluate 20 different MLLMs, analyzing both final\nanswer accuracy and the step-by-step integrity of their chain-of-thought.\nFurthermore, we systematically study the impact of difficulty level and visual\ninformation by comparing the model performance before and after changing the\ninput mode. Our work provides not only a fine-grained resource for the\ncommunity but also offers a robust methodology for dissecting the multimodal\nreasoning process of state-of-the-art MLLMs, and our dataset and code have been\nopen-sourced: https://github.com/luozhongze/Multi-Physics.", "AI": {"tldr": "\u63d0\u51fa\u4e86Multi-Physics\u4e2d\u6587\u7269\u7406\u63a8\u7406\u57fa\u51c6\uff0c\u5305\u542b5\u4e2a\u96be\u5ea6\u7ea7\u522b\u30011,412\u9053\u56fe\u50cf\u76f8\u5173\u9009\u62e9\u9898\uff0c\u8986\u76d611\u4e2a\u9ad8\u4e2d\u7269\u7406\u79d1\u76ee\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u57fa\u51c6\u5728\u4e13\u4e1a\u79d1\u5b66\u9886\u57df\uff08\u5982\u7269\u7406\uff09\u5b58\u5728\u4e0d\u8db3\uff1a\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u79d1\u76ee\u8986\u76d6\u3001\u5ffd\u89c6\u9010\u6b65\u63a8\u7406\u8fc7\u7a0b\u3001\u4ee5\u82f1\u8bed\u4e3a\u4e2d\u5fc3\u3001\u672a\u80fd\u7cfb\u7edf\u8bc4\u4f30\u89c6\u89c9\u4fe1\u606f\u7684\u4f5c\u7528\u3002", "method": "\u91c7\u7528\u53cc\u8bc4\u4f30\u6846\u67b6\u8bc4\u4f3020\u4e2a\u4e0d\u540cMLLM\uff0c\u5206\u6790\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u6027\u548c\u9010\u6b65\u63a8\u7406\u5b8c\u6574\u6027\uff1b\u901a\u8fc7\u6539\u53d8\u8f93\u5165\u6a21\u5f0f\u7cfb\u7edf\u7814\u7a76\u96be\u5ea6\u7ea7\u522b\u548c\u89c6\u89c9\u4fe1\u606f\u7684\u5f71\u54cd\u3002", "result": "\u63d0\u4f9b\u4e86\u7ec6\u7c92\u5ea6\u8d44\u6e90\u548c\u5206\u6790\u65b9\u6cd5\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e0d\u4ec5\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u7ec6\u7c92\u5ea6\u8d44\u6e90\uff0c\u8fd8\u63d0\u4f9b\u4e86\u5256\u6790\u6700\u5148\u8fdbMLLM\u591a\u6a21\u6001\u63a8\u7406\u8fc7\u7a0b\u7684\u7a33\u5065\u65b9\u6cd5\u3002"}}
{"id": "2509.15896", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.15896", "abs": "https://arxiv.org/abs/2509.15896", "authors": ["Arghodeep Nandi", "Megha Sundriyal", "Euna Mehnaz Khan", "Jikai Sun", "Emily Vraga", "Jaideep Srivastava", "Tanmoy Chakraborty"], "title": "The Psychology of Falsehood: A Human-Centric Survey of Misinformation Detection", "comment": "Accepted in EMNLP'25 Main", "summary": "Misinformation remains one of the most significant issues in the digital age.\nWhile automated fact-checking has emerged as a viable solution, most current\nsystems are limited to evaluating factual accuracy. However, the detrimental\neffect of misinformation transcends simple falsehoods; it takes advantage of\nhow individuals perceive, interpret, and emotionally react to information. This\nunderscores the need to move beyond factuality and adopt more human-centered\ndetection frameworks. In this survey, we explore the evolving interplay between\ntraditional fact-checking approaches and psychological concepts such as\ncognitive biases, social dynamics, and emotional responses. By analyzing\nstate-of-the-art misinformation detection systems through the lens of human\npsychology and behavior, we reveal critical limitations of current methods and\nidentify opportunities for improvement. Additionally, we outline future\nresearch directions aimed at creating more robust and adaptive frameworks, such\nas neuro-behavioural models that integrate technological factors with the\ncomplexities of human cognition and social influence. These approaches offer\npromising pathways to more effectively detect and mitigate the societal harms\nof misinformation.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u8d85\u8d8a\u4f20\u7edf\u4e8b\u5b9e\u6838\u67e5\u7684\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5f3a\u8c03\u9700\u8981\u7ed3\u5408\u4eba\u7c7b\u5fc3\u7406\u5b66\u56e0\u7d20\uff08\u5982\u8ba4\u77e5\u504f\u89c1\u3001\u793e\u4f1a\u52a8\u6001\u548c\u60c5\u7eea\u53cd\u5e94\uff09\u6765\u6784\u5efa\u66f4\u6709\u6548\u7684\u4eba\u7c7b\u4e2d\u5fc3\u68c0\u6d4b\u6846\u67b6\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u4f46\u865a\u5047\u4fe1\u606f\u7684\u5371\u5bb3\u8d85\u8d8a\u4e86\u7b80\u5355\u7684\u4e8b\u5b9e\u9519\u8bef\uff0c\u5b83\u5229\u7528\u4e86\u4eba\u4eec\u7684\u611f\u77e5\u3001\u89e3\u91ca\u548c\u60c5\u7eea\u53cd\u5e94\u65b9\u5f0f\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u4eba\u6027\u5316\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u8c03\u67e5\u5206\u6790\u6700\u5148\u8fdb\u7684\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u7cfb\u7edf\uff0c\u7ed3\u5408\u5fc3\u7406\u5b66\u6982\u5ff5\u5982\u8ba4\u77e5\u504f\u89c1\u3001\u793e\u4f1a\u52a8\u6001\u548c\u60c5\u7eea\u53cd\u5e94\uff0c\u8bc4\u4f30\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u5e76\u8bc6\u522b\u6539\u8fdb\u673a\u4f1a\u3002", "result": "\u63ed\u793a\u4e86\u5f53\u524d\u68c0\u6d4b\u65b9\u6cd5\u7684\u5173\u952e\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u6574\u5408\u6280\u672f\u56e0\u7d20\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u548c\u793e\u4f1a\u5f71\u54cd\u590d\u6742\u6027\u7684\u795e\u7ecf\u884c\u4e3a\u6a21\u578b\u7b49\u6539\u8fdb\u65b9\u5411\u3002", "conclusion": "\u57fa\u4e8e\u4eba\u7c7b\u5fc3\u7406\u5b66\u7684\u65b9\u6cd5\u4e3a\u66f4\u6709\u6548\u5730\u68c0\u6d4b\u548c\u51cf\u8f7b\u865a\u5047\u4fe1\u606f\u7684\u793e\u4f1a\u5371\u5bb3\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\uff0c\u9700\u8981\u521b\u5efa\u66f4\u5f3a\u5927\u548c\u81ea\u9002\u5e94\u7684\u68c0\u6d4b\u6846\u67b6\u3002"}}
{"id": "2509.15926", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15926", "abs": "https://arxiv.org/abs/2509.15926", "authors": ["Ahmed Karim", "Qiao Wang", "Zheng Yuan"], "title": "Beyond the Score: Uncertainty-Calibrated LLMs for Automated Essay Assessment", "comment": "Accepted at EMNLP 2025 (Main Conference). Camera-ready version", "summary": "Automated Essay Scoring (AES) systems now reach near human agreement on some\npublic benchmarks, yet real-world adoption, especially in high-stakes\nexaminations, remains limited. A principal obstacle is that most models output\na single score without any accompanying measure of confidence or explanation.\nWe address this gap with conformal prediction, a distribution-free wrapper that\nequips any classifier with set-valued outputs and formal coverage guarantees.\nTwo open-source large language models (Llama-3 8B and Qwen-2.5 3B) are\nfine-tuned on three diverse corpora (ASAP, TOEFL11, Cambridge-FCE) and\ncalibrated at a 90 percent risk level. Reliability is assessed with UAcc, an\nuncertainty-aware accuracy that rewards models for being both correct and\nconcise. To our knowledge, this is the first work to combine conformal\nprediction and UAcc for essay scoring. The calibrated models consistently meet\nthe coverage target while keeping prediction sets compact, indicating that\nopen-source, mid-sized LLMs can already support teacher-in-the-loop AES; we\ndiscuss scaling and broader user studies as future work.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u4fdd\u5f62\u9884\u6d4b\u65b9\u6cd5\u4e3a\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u7cfb\u7edf\u63d0\u4f9b\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\uff0c\u901a\u8fc7\u5fae\u8c03\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5e76\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u4e8690%\u98ce\u9669\u6c34\u5e73\u4e0b\u7684\u53ef\u9760\u8bc4\u5206\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u7cfb\u7edf\u867d\u7136\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63a5\u8fd1\u4eba\u7c7b\u8bc4\u5206\u6c34\u5e73\uff0c\u4f46\u5728\u9ad8\u98ce\u9669\u8003\u8bd5\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u4ecd\u53d7\u9650\uff0c\u4e3b\u8981\u969c\u788d\u662f\u6a21\u578b\u53ea\u8f93\u51fa\u5355\u4e00\u5206\u6570\u800c\u7f3a\u4e4f\u7f6e\u4fe1\u5ea6\u6216\u89e3\u91ca\u3002", "method": "\u4f7f\u7528\u4fdd\u5f62\u9884\u6d4b\u4f5c\u4e3a\u5206\u5e03\u65e0\u5173\u7684\u5305\u88c5\u5668\uff0c\u4e3a\u4efb\u4f55\u5206\u7c7b\u5668\u63d0\u4f9b\u96c6\u5408\u503c\u8f93\u51fa\u548c\u6b63\u5f0f\u8986\u76d6\u4fdd\u8bc1\u3002\u5fae\u8c03\u4e24\u4e2a\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\uff08Llama-3 8B\u548cQwen-2.5 3B\uff09\u5728\u4e09\u4e2a\u4e0d\u540c\u8bed\u6599\u5e93\u4e0a\uff0c\u5e76\u572890%\u98ce\u9669\u6c34\u5e73\u4e0b\u8fdb\u884c\u6821\u51c6\u3002", "result": "\u6821\u51c6\u540e\u7684\u6a21\u578b\u59cb\u7ec8\u6ee1\u8db3\u8986\u76d6\u76ee\u6807\uff0c\u540c\u65f6\u4fdd\u6301\u9884\u6d4b\u96c6\u5408\u7d27\u51d1\uff0c\u8868\u660e\u5f00\u6e90\u4e2d\u7b49\u89c4\u6a21\u5927\u8bed\u8a00\u6a21\u578b\u5df2\u80fd\u652f\u6301\u6559\u5e08\u53c2\u4e0e\u7684\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5c06\u4fdd\u5f62\u9884\u6d4b\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u51c6\u786e\u7387\u7ed3\u5408\u7528\u4e8e\u4f5c\u6587\u8bc4\u5206\u7684\u5de5\u4f5c\uff0c\u8ba8\u8bba\u4e86\u6269\u5c55\u548c\u66f4\u5e7f\u6cdb\u7684\u7528\u6237\u7814\u7a76\u4f5c\u4e3a\u672a\u6765\u5de5\u4f5c\u3002"}}
{"id": "2509.15958", "categories": ["cs.CL", "cs.LG", "math.DS", "math.OC", "68T07, 68T50, 37N35, 37B25"], "pdf": "https://arxiv.org/pdf/2509.15958", "abs": "https://arxiv.org/abs/2509.15958", "authors": ["Henri Cimeti\u00e8re", "Maria Teresa Chiri", "Bahman Gharesifard"], "title": "Localmax dynamics for attention in transformers and its asymptotic behavior", "comment": "28 pages, 5 figures", "summary": "We introduce a new discrete-time attention model, termed the localmax\ndynamics, which interpolates between the classic softmax dynamics and the\nhardmax dynamics, where only the tokens that maximize the influence toward a\ngiven token have a positive weight. As in hardmax, uniform weights are\ndetermined by a parameter controlling neighbor influence, but the key extension\nlies in relaxing neighborhood interactions through an alignment-sensitivity\nparameter, which allows controlled deviations from pure hardmax behavior. As we\nprove, while the convex hull of the token states still converges to a convex\npolytope, its structure can no longer be fully described by a maximal alignment\nset, prompting the introduction of quiescent sets to capture the invariant\nbehavior of tokens near vertices. We show that these sets play a key role in\nunderstanding the asymptotic behavior of the system, even under time-varying\nalignment sensitivity parameters. We further show that localmax dynamics does\nnot exhibit finite-time convergence and provide results for vanishing, nonzero,\ntime-varying alignment-sensitivity parameters, recovering the limiting behavior\nof hardmax as a by-product. Finally, we adapt Lyapunov-based methods from\nclassical opinion dynamics, highlighting their limitations in the asymmetric\nsetting of localmax interactions and outlining directions for future research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u79bb\u6563\u65f6\u95f4\u6ce8\u610f\u529b\u6a21\u578b\u2014\u2014localmax\u52a8\u6001\uff0c\u5b83\u5728softmax\u548chardmax\u52a8\u6001\u4e4b\u95f4\u8fdb\u884c\u63d2\u503c\uff0c\u901a\u8fc7\u90bb\u57df\u5f71\u54cd\u53c2\u6570\u548c\u5bf9\u9f50\u654f\u611f\u5ea6\u53c2\u6570\u63a7\u5236\u6ce8\u610f\u529b\u6743\u91cd\u5206\u914d\u3002", "motivation": "\u4f20\u7edfsoftmax\u548chardmax\u6ce8\u610f\u529b\u6a21\u578b\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7075\u6d3b\u63a7\u5236\u6ce8\u610f\u529b\u5206\u914d\u673a\u5236\u7684\u65b0\u6a21\u578b\uff0c\u65e2\u80fd\u4fdd\u6301hardmax\u7684\u9009\u62e9\u6027\uff0c\u53c8\u80fd\u5141\u8bb8\u4e00\u5b9a\u7684\u7075\u6d3b\u6027\u3002", "method": "\u5f15\u5165localmax\u52a8\u6001\u6a21\u578b\uff0c\u4f7f\u7528\u90bb\u57df\u5f71\u54cd\u53c2\u6570\u63a7\u5236\u6743\u91cd\u5206\u914d\uff0c\u901a\u8fc7\u5bf9\u9f50\u654f\u611f\u5ea6\u53c2\u6570\u8c03\u8282\u4e0ehardmax\u7684\u504f\u79bb\u7a0b\u5ea6\uff0c\u5e76\u5206\u6790\u7cfb\u7edf\u7684\u6e10\u8fd1\u884c\u4e3a\u548c\u6536\u655b\u7279\u6027\u3002", "result": "\u8bc1\u660etoken\u72b6\u6001\u7684\u51f8\u5305\u6536\u655b\u5230\u51f8\u591a\u9762\u4f53\uff0c\u4f46\u7ed3\u6784\u4e0d\u80fd\u5b8c\u5168\u7531\u6700\u5927\u5bf9\u9f50\u96c6\u63cf\u8ff0\uff1b\u5f15\u5165\u9759\u6b62\u96c6\u6765\u63cf\u8ff0\u9876\u70b9\u9644\u8fd1token\u7684\u4e0d\u53d8\u884c\u4e3a\uff1b\u6a21\u578b\u4e0d\u4f1a\u51fa\u73b0\u6709\u9650\u65f6\u95f4\u6536\u655b\u3002", "conclusion": "localmax\u52a8\u6001\u4e3a\u6ce8\u610f\u529b\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\uff0c\u6269\u5c55\u4e86\u4f20\u7edf\u6a21\u578b\u7684\u80fd\u529b\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u975e\u5bf9\u79f0\u8bbe\u7f6e\u4e0b\u7684Lyapunov\u65b9\u6cd5\u5e94\u7528\u3002"}}
{"id": "2509.16105", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16105", "abs": "https://arxiv.org/abs/2509.16105", "authors": ["Sikai Bai", "Haoxi Li", "Jie Zhang", "Zicong Hong", "Song Guo"], "title": "DiEP: Adaptive Mixture-of-Experts Compression through Differentiable Expert Pruning", "comment": "18 pages", "summary": "Despite the significant breakthrough of Mixture-of-Experts (MoE), the\nincreasing scale of these MoE models presents huge memory and storage\nchallenges. Existing MoE pruning methods, which involve reducing parameter size\nwith a uniform sparsity across all layers, often lead to suboptimal outcomes\nand performance degradation due to varying expert redundancy in different MoE\nlayers. To address this, we propose a non-uniform pruning strategy, dubbed\n\\textbf{Di}fferentiable \\textbf{E}xpert \\textbf{P}runing (\\textbf{DiEP}), which\nadaptively adjusts pruning rates at the layer level while jointly learning\ninter-layer importance, effectively capturing the varying redundancy across\ndifferent MoE layers. By transforming the global discrete search space into a\ncontinuous one, our method handles exponentially growing non-uniform expert\ncombinations, enabling adaptive gradient-based pruning. Extensive experiments\non five advanced MoE models demonstrate the efficacy of our method across\nvarious NLP tasks. Notably, \\textbf{DiEP} retains around 92\\% of original\nperformance on Mixtral 8$\\times$7B with only half the experts, outperforming\nother pruning methods by up to 7.1\\% on the challenging MMLU dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDiEP\u7684\u975e\u5747\u5300\u526a\u679d\u7b56\u7565\uff0c\u7528\u4e8e\u89e3\u51b3MoE\u6a21\u578b\u7684\u5185\u5b58\u548c\u5b58\u50a8\u6311\u6218\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u4e0d\u540c\u5c42\u7684\u526a\u679d\u7387\u6765\u4fdd\u7559\u7ea692%\u7684\u539f\u59cb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684MoE\u526a\u679d\u65b9\u6cd5\u91c7\u7528\u7edf\u4e00\u7684\u7a00\u758f\u5ea6\u4f1a\u5bfc\u81f4\u6b21\u4f18\u7ed3\u679c\u548c\u6027\u80fd\u4e0b\u964d\uff0c\u56e0\u4e3a\u4e0d\u540cMoE\u5c42\u7684\u4e13\u5bb6\u5197\u4f59\u5ea6\u4e0d\u540c\u3002", "method": "DiEP\u65b9\u6cd5\u5c06\u79bb\u6563\u641c\u7d22\u7a7a\u95f4\u8f6c\u5316\u4e3a\u8fde\u7eed\u7a7a\u95f4\uff0c\u81ea\u9002\u5e94\u8c03\u6574\u5c42\u7ea7\u526a\u679d\u7387\u5e76\u8054\u5408\u5b66\u4e60\u5c42\u95f4\u91cd\u8981\u6027\uff0c\u5b9e\u73b0\u57fa\u4e8e\u68af\u5ea6\u7684\u81ea\u9002\u5e94\u526a\u679d\u3002", "result": "\u5728\u4e94\u4e2a\u5148\u8fdbMoE\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDiEP\u5728Mixtral 8\u00d77B\u6a21\u578b\u4e0a\u4ec5\u4fdd\u7559\u4e00\u534a\u4e13\u5bb6\u5c31\u80fd\u4fdd\u6301\u7ea692%\u7684\u539f\u59cb\u6027\u80fd\uff0c\u5728MMLU\u6570\u636e\u96c6\u4e0a\u6bd4\u5176\u4ed6\u526a\u679d\u65b9\u6cd5\u9ad8\u51fa7.1%\u3002", "conclusion": "DiEP\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u4e0d\u540cMoE\u5c42\u7684\u5197\u4f59\u5ea6\u5dee\u5f02\uff0c\u5728\u663e\u8457\u51cf\u5c11\u53c2\u6570\u7684\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002"}}
{"id": "2509.16107", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16107", "abs": "https://arxiv.org/abs/2509.16107", "authors": ["Lukas Ellinger", "Georg Groh"], "title": "It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge", "comment": "Accepted by UncertaiNLP workshop @ EMNLP 2025", "summary": "Ambiguous words or underspecified references require interlocutors to resolve\nthem, often by relying on shared context and commonsense knowledge. Therefore,\nwe systematically investigate whether Large Language Models (LLMs) can leverage\ncommonsense to resolve referential ambiguity in multi-turn conversations and\nanalyze their behavior when ambiguity persists. Further, we study how requests\nfor simplified language affect this capacity. Using a novel multilingual\nevaluation dataset, we test DeepSeek v3, GPT-4o, Qwen3-32B, GPT-4o-mini, and\nLlama-3.1-8B via LLM-as-Judge and human annotations. Our findings indicate that\ncurrent LLMs struggle to resolve ambiguity effectively: they tend to commit to\na single interpretation or cover all possible references, rather than hedging\nor seeking clarification. This limitation becomes more pronounced under\nsimplification prompts, which drastically reduce the use of commonsense\nreasoning and diverse response strategies. Fine-tuning Llama-3.1-8B with Direct\nPreference Optimization substantially improves ambiguity resolution across all\nrequest types. These results underscore the need for advanced fine-tuning to\nimprove LLMs' handling of ambiguity and to ensure robust performance across\ndiverse communication styles.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u5229\u7528\u5e38\u8bc6\u89e3\u51b3\u6307\u4ee3\u6b67\u4e49\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524dLLMs\u5728\u6b67\u4e49\u5904\u7406\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u503e\u5411\u4e8e\u5355\u4e00\u89e3\u91ca\u6216\u8986\u76d6\u6240\u6709\u53ef\u80fd\uff0c\u7b80\u5316\u63d0\u793a\u4f1a\u8fdb\u4e00\u6b65\u524a\u5f31\u5176\u5e38\u8bc6\u63a8\u7406\u80fd\u529b\uff0c\u800c\u901a\u8fc7DPO\u5fae\u8c03\u53ef\u663e\u8457\u6539\u5584\u6b67\u4e49\u89e3\u51b3\u6548\u679c\u3002", "motivation": "\u7814\u7a76LLMs\u662f\u5426\u80fd\u591f\u5229\u7528\u5e38\u8bc6\u77e5\u8bc6\u6765\u89e3\u51b3\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u6307\u4ee3\u6b67\u4e49\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6b67\u4e49\u6301\u7eed\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\uff0c\u4ee5\u53ca\u7b80\u5316\u8bed\u8a00\u8bf7\u6c42\u5982\u4f55\u5f71\u54cd\u8fd9\u79cd\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u65b0\u9896\u7684\u591a\u8bed\u8a00\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u901a\u8fc7LLM-as-Judge\u548c\u4eba\u5de5\u6807\u6ce8\u6d4b\u8bd5\u4e86DeepSeek v3\u3001GPT-4o\u3001Qwen3-32B\u3001GPT-4o-mini\u548cLlama-3.1-8B\u7b49\u6a21\u578b\uff0c\u5e76\u5bf9Llama-3.1-8B\u8fdb\u884c\u4e86\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u5fae\u8c03\u3002", "result": "\u5f53\u524dLLMs\u5728\u6709\u6548\u89e3\u51b3\u6b67\u4e49\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff1a\u503e\u5411\u4e8e\u627f\u8bfa\u5355\u4e00\u89e3\u91ca\u6216\u8986\u76d6\u6240\u6709\u53ef\u80fd\u5f15\u7528\uff0c\u800c\u4e0d\u662f\u91c7\u53d6\u5bf9\u51b2\u6216\u5bfb\u6c42\u6f84\u6e05\u7684\u7b56\u7565\u3002\u7b80\u5316\u63d0\u793a\u663e\u8457\u51cf\u5c11\u4e86\u5e38\u8bc6\u63a8\u7406\u548c\u591a\u6837\u5316\u54cd\u5e94\u7b56\u7565\u7684\u4f7f\u7528\u3002DPO\u5fae\u8c03\u663e\u8457\u6539\u5584\u4e86\u6240\u6709\u8bf7\u6c42\u7c7b\u578b\u7684\u6b67\u4e49\u89e3\u51b3\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u9700\u8981\u5148\u8fdb\u7684\u5fae\u8c03\u6280\u672f\u6765\u6539\u8fdbLLMs\u5904\u7406\u6b67\u4e49\u7684\u80fd\u529b\uff0c\u5e76\u786e\u4fdd\u5728\u4e0d\u540c\u6c9f\u901a\u98ce\u683c\u4e0b\u7684\u7a33\u5065\u6027\u80fd\u3002"}}
{"id": "2509.16112", "categories": ["cs.CL", "cs.IR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16112", "abs": "https://arxiv.org/abs/2509.16112", "authors": ["Sheng Zhang", "Yifan Ding", "Shuquan Lian", "Shun Song", "Hui Li"], "title": "CodeRAG: Finding Relevant and Necessary Knowledge for Retrieval-Augmented Repository-Level Code Completion", "comment": "EMNLP 2025", "summary": "Repository-level code completion automatically predicts the unfinished code\nbased on the broader information from the repository. Recent strides in Code\nLarge Language Models (code LLMs) have spurred the development of\nrepository-level code completion methods, yielding promising results.\nNevertheless, they suffer from issues such as inappropriate query construction,\nsingle-path code retrieval, and misalignment between code retriever and code\nLLM. To address these problems, we introduce CodeRAG, a framework tailored to\nidentify relevant and necessary knowledge for retrieval-augmented\nrepository-level code completion. Its core components include log probability\nguided query construction, multi-path code retrieval, and preference-aligned\nBestFit reranking. Extensive experiments on benchmarks ReccEval and CCEval\ndemonstrate that CodeRAG significantly and consistently outperforms\nstate-of-the-art methods. The implementation of CodeRAG is available at\nhttps://github.com/KDEGroup/CodeRAG.", "AI": {"tldr": "CodeRAG\u662f\u4e00\u4e2a\u9488\u5bf9\u4ed3\u5e93\u7ea7\u4ee3\u7801\u8865\u5168\u7684\u68c0\u7d22\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u67e5\u8be2\u6784\u5efa\u3001\u591a\u8def\u5f84\u4ee3\u7801\u68c0\u7d22\u548c\u504f\u597d\u5bf9\u9f50\u91cd\u6392\u5e8f\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u95ee\u9898\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u4ed3\u5e93\u7ea7\u4ee3\u7801\u8865\u5168\u65b9\u6cd5\u5b58\u5728\u4e0d\u5408\u9002\u7684\u67e5\u8be2\u6784\u5efa\u3001\u5355\u8def\u5f84\u4ee3\u7801\u68c0\u7d22\u4ee5\u53ca\u4ee3\u7801\u68c0\u7d22\u5668\u4e0e\u4ee3\u7801LLM\u4e4b\u95f4\u7684\u4e0d\u5bf9\u9f50\u7b49\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "CodeRAG\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u57fa\u4e8e\u5bf9\u6570\u6982\u7387\u7684\u67e5\u8be2\u6784\u5efa\u3001\u591a\u8def\u5f84\u4ee3\u7801\u68c0\u7d22\u3001\u4ee5\u53ca\u504f\u597d\u5bf9\u9f50\u7684BestFit\u91cd\u6392\u5e8f\u65b9\u6cd5\u3002", "result": "\u5728ReccEval\u548cCCEval\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCodeRAG\u663e\u8457\u4e14\u6301\u7eed\u5730\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "CodeRAG\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u89e3\u51b3\u4ed3\u5e93\u7ea7\u4ee3\u7801\u8865\u5168\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u68c0\u7d22\u589e\u5f3a\u6846\u67b6\uff0c\u4ee3\u7801\u5b9e\u73b0\u5df2\u5f00\u6e90\u3002"}}
