<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 103]
- [cs.AI](#cs.AI) [Total: 70]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Quantum NLP models on Natural Language Inference](https://arxiv.org/abs/2510.15972)
*Ling Sun,Peter Sullivan,Michael Martin,Yun Zhou*

Main category: cs.CL

TL;DR: 该论文研究了量子自然语言处理在自然语言推理任务中的应用，比较了量子、混合和经典模型在少样本设置下的表现，并提出了新的信息增益参数比指标来评估模型效率。


<details>
  <summary>Details</summary>
Motivation: 探索量子自然语言处理在语义建模中的潜力，特别是在结构敏感的低资源环境下，通过量子电路直接嵌入组合结构来提升自然语言推理任务的性能。

Method: 使用lambeq库和DisCoCat框架构建参数化量子电路处理句子对，训练语义相关性和推理分类任务，并提出基于聚类的架构来促进参数共享。

Result: 量子模型在参数数量大幅减少的情况下达到与经典基线相当的性能，在推理任务上优于随机初始化的transformer，在相关任务上测试误差更低，参数学习效率比经典模型高出最多五个数量级。

Conclusion: 量子自然语言处理在低资源、结构敏感的环境中具有显著优势，量子模型展现出更高的参数学习效率，为QNLP在语义建模中的应用提供了有力支持。

Abstract: Quantum natural language processing (QNLP) offers a novel approach to
semantic modeling by embedding compositional structure directly into quantum
circuits. This paper investigates the application of QNLP models to the task of
Natural Language Inference (NLI), comparing quantum, hybrid, and classical
transformer-based models under a constrained few-shot setting. Using the lambeq
library and the DisCoCat framework, we construct parameterized quantum circuits
for sentence pairs and train them for both semantic relatedness and inference
classification. To assess efficiency, we introduce a novel
information-theoretic metric, Information Gain per Parameter (IGPP), which
quantifies learning dynamics independent of model size. Our results demonstrate
that quantum models achieve performance comparable to classical baselines while
operating with dramatically fewer parameters. The Quantum-based models
outperform randomly initialized transformers in inference and achieve lower
test error on relatedness tasks. Moreover, quantum models exhibit significantly
higher per-parameter learning efficiency (up to five orders of magnitude more
than classical counterparts), highlighting the promise of QNLP in low-resource,
structure-sensitive settings. To address circuit-level isolation and promote
parameter sharing, we also propose a novel cluster-based architecture that
improves generalization by tying gate parameters to learned word clusters
rather than individual tokens.

</details>


### [2] [Fusion-Augmented Large Language Models: Boosting Diagnostic Trustworthiness via Model Consensus](https://arxiv.org/abs/2510.16057)
*Md Kamrul Siam,Md Jobair Hossain Faruk,Jerry Q. Cheng,Huanying Gu*

Main category: cs.CL

TL;DR: 提出多模型融合框架，结合ChatGPT和Claude两个大语言模型，通过相似性共识方法提升胸部X光片诊断的可靠性，在CheXpert数据集上验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 提高AI辅助放射学诊断的可靠性和临床实用性，通过多模态融合和输出共识来减少诊断错误。

Method: 使用ChatGPT和Claude两个LLM模型，在CheXpert数据集上进行评估。采用单模态（仅图像）和多模态（图像+合成临床笔记）输入，通过95%输出相似性阈值的共识方法进行模型融合。

Result: 单模态设置下，ChatGPT和Claude准确率分别为62.8%和76.9%，共识方法提升至77.6%。多模态设置下，ChatGPT和Claude准确率分别提升至84%和76%，共识准确率达到91.3%。共识融合在所有实验条件下均优于单个模型。

Conclusion: 整合互补模态和使用输出级共识可以有效提高AI辅助放射学诊断的可信度和临床效用，为减少诊断错误提供了实用路径，且计算开销最小。

Abstract: This study presents a novel multi-model fusion framework leveraging two
state-of-the-art large language models (LLMs), ChatGPT and Claude, to enhance
the reliability of chest X-ray interpretation on the CheXpert dataset. From the
full CheXpert corpus of 224,316 chest radiographs, we randomly selected 234
radiologist-annotated studies to evaluate unimodal performance using image-only
prompts. In this setting, ChatGPT and Claude achieved diagnostic accuracies of
62.8% and 76.9%, respectively. A similarity-based consensus approach, using a
95% output similarity threshold, improved accuracy to 77.6%. To assess the
impact of multimodal inputs, we then generated synthetic clinical notes
following the MIMIC-CXR template and evaluated a separate subset of 50 randomly
selected cases paired with both images and synthetic text. On this multimodal
cohort, performance improved to 84% for ChatGPT and 76% for Claude, while
consensus accuracy reached 91.3%. Across both experimental conditions,
agreement-based fusion consistently outperformed individual models. These
findings highlight the utility of integrating complementary modalities and
using output-level consensus to improve the trustworthiness and clinical
utility of AI-assisted radiological diagnosis, offering a practical path to
reduce diagnostic errors with minimal computational overhead.

</details>


### [3] [Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs](https://arxiv.org/abs/2510.16062)
*Guiyao Tie,Zenghui Yuan,Zeli Zhao,Chaoran Hu,Tianhe Gu,Ruihang Zhang,Sizhe Zhang,Junran Wu,Xiaoyue Tu,Ming Jin,Qingsong Wen,Lixing Chen,Pan Zhou,Lichao Sun*

Main category: cs.CL

TL;DR: CorrectBench基准测试评估LLM自校正方法，发现在复杂推理任务中自校正能提升准确性，混合策略效果更好但效率降低，推理型LLM自校正优化有限且耗时高，简单的CoT基线表现竞争力强。


<details>
  <summary>Details</summary>
Motivation: 虽然已有多种LLM自校正方法被提出，但这些方法的综合评估仍很缺乏，LLM是否能真正自我校正是一个重要但未充分研究的问题。

Method: 开发CorrectBench基准，评估内在、外部和微调三种自校正策略在常识推理、数学推理和代码生成三个任务上的效果。

Result: 自校正方法能提高准确性（特别是复杂推理任务）；混合不同自校正策略可进一步改进但降低效率；推理型LLM在额外自校正方法下优化有限且时间成本高；简单的CoT基线展现出竞争性准确性和效率。

Conclusion: 自校正有潜力提升LLM推理性能，但效率优化仍是持续挑战，需要进一步研究在推理能力和操作效率之间寻求平衡。

Abstract: Self-correction of large language models (LLMs) emerges as a critical
component for enhancing their reasoning performance. Although various
self-correction methods have been proposed, a comprehensive evaluation of these
methods remains largely unexplored, and the question of whether LLMs can truly
correct themselves is a matter of significant interest and concern. In this
study, we introduce CorrectBench, a benchmark developed to evaluate the
effectiveness of self-correction strategies, including intrinsic, external, and
fine-tuned approaches, across three tasks: commonsense reasoning, mathematical
reasoning, and code generation. Our findings reveal that: 1) Self-correction
methods can improve accuracy, especially for complex reasoning tasks; 2) Mixing
different self-correction strategies yields further improvements, though it
reduces efficiency; 3) Reasoning LLMs (e.g., DeepSeek-R1) have limited
optimization under additional self-correction methods and have high time costs.
Interestingly, a comparatively simple chain-of-thought (CoT) baseline
demonstrates competitive accuracy and efficiency. These results underscore the
potential of self-correction to enhance LLM's reasoning performance while
highlighting the ongoing challenge of improving their efficiency. Consequently,
we advocate for further research focused on optimizing the balance between
reasoning capabilities and operational efficiency. Project Page:
https://correctbench.github.io/

</details>


### [4] [EvolveR: Self-Evolving LLM Agents through an Experience-Driven Lifecycle](https://arxiv.org/abs/2510.16079)
*Rong Wu,Xiaoman Wang,Jianbiao Mei,Pinlong Cai,Daocheng Fu,Cheng Yang,Licheng Wen,Xuemeng Yang,Yufan Shen,Yuxin Wang,Botian Shi*

Main category: cs.CL

TL;DR: EvolveR框架让LLM智能体通过离线自蒸馏和在线交互的闭环生命周期实现自我改进，在复杂多跳问答任务中表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体在工具使用方面表现良好，但缺乏从自身经验中系统学习的能力，无法迭代优化问题解决策略。

Method: 采用闭环经验生命周期：1) 离线自蒸馏 - 将交互轨迹合成为结构化、可重用的战略原则库；2) 在线交互 - 智能体与任务交互并检索蒸馏原则指导决策，积累多样化行为轨迹；使用策略强化机制迭代更新智能体。

Result: 在复杂多跳问答基准测试中，EvolveR实现了优于强基线智能体的性能表现。

Conclusion: 该工作为智能体提供了从自身行动后果中学习的全面蓝图，为更自主和持续改进的系统铺平了道路。

Abstract: Current Large Language Model (LLM) agents show strong performance in tool
use, but lack the crucial capability to systematically learn from their own
experiences. While existing frameworks mainly focus on mitigating external
knowledge gaps, they fail to address a more fundamental limitation: the
inability to iteratively refine problem-solving strategies. In this work, we
introduce EvolveR, a framework designed to enable agent to self-improve through
a complete, closed-loop experience lifecycle. This lifecycle comprises two key
stages: (1) Offline Self-Distillation, where the agent's interaction
trajectories are synthesized into a structured repository of abstract, reusable
strategic principles; (2) Online Interaction, where the agent interacts with
tasks and actively retrieves distilled principles to guide its decision-making,
accumulating a diverse set of behavioral trajectories. This loop employs a
policy reinforcement mechanism to iteratively update the agent based on its
performance. We demonstrate the effectiveness of EvolveR on complex multi-hop
question-answering benchmarks, where it achieves superior performance over
strong agentic baselines. Our work presents a comprehensive blueprint for
agents that learn not only from external data but also from the consequences of
their own actions, paving the way for more autonomous and continuously
improving systems. Code is available at https://github.com/Edaizi/EvolveR.

</details>


### [5] [Evaluating Prompting Strategies and Large Language Models in Systematic Literature Review Screening: Relevance and Task-Stage Classification](https://arxiv.org/abs/2510.16091)
*Binglan Han,Anuradha Mathrani,Teo Susnjak*

Main category: cs.CL

TL;DR: 本研究量化了提示策略与大型语言模型在系统文献综述筛选阶段的交互作用，发现CoT-few-shot提示在精度-召回平衡方面表现最佳，GPT-4o-mini在显著降低成本的同时保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 系统文献综述的筛选阶段耗时且劳动密集，需要评估LLMs在此任务中的自动化潜力，特别是不同提示策略与模型之间的交互效应。

Method: 评估6个LLM在5种提示类型下的表现，包括零样本、少样本、思维链、思维链-少样本和自我反思，使用准确率、精度、召回率和F1分数作为指标。

Result: 结果显示明显的模型-提示交互效应：CoT-few-shot提供最可靠的精度-召回平衡；零样本在高灵敏度筛选时召回率最高；自我反思由于过度包容性和不稳定性表现不佳。GPT-4o-mini在显著降低成本的同时保持竞争力。

Conclusion: 推荐采用分阶段工作流程：先用低成本模型和结构化提示进行初步筛选，仅将边界案例升级到高容量模型。这些发现突显了LLMs在文献筛选自动化方面的潜力，提供了比较基准和实用指南。

Abstract: This study quantifies how prompting strategies interact with large language
models (LLMs) to automate the screening stage of systematic literature reviews
(SLRs). We evaluate six LLMs (GPT-4o, GPT-4o-mini, DeepSeek-Chat-V3,
Gemini-2.5-Flash, Claude-3.5-Haiku, Llama-4-Maverick) under five prompt types
(zero-shot, few-shot, chain-of-thought (CoT), CoT-few-shot, self-reflection)
across relevance classification and six Level-2 tasks, using accuracy,
precision, recall, and F1. Results show pronounced model-prompt interaction
effects: CoT-few-shot yields the most reliable precision-recall balance;
zero-shot maximizes recall for high-sensitivity passes; and self-reflection
underperforms due to over-inclusivity and instability across models. GPT-4o and
DeepSeek provide robust overall performance, while GPT-4o-mini performs
competitively at a substantially lower dollar cost. A cost-performance analysis
for relevance classification (per 1,000 abstracts) reveals large absolute
differences among model-prompt pairings; GPT-4o-mini remains low-cost across
prompts, and structured prompts (CoT/CoT-few-shot) on GPT-4o-mini offer
attractive F1 at a small incremental cost. We recommend a staged workflow that
(1) deploys low-cost models with structured prompts for first-pass screening
and (2) escalates only borderline cases to higher-capacity models. These
findings highlight LLMs' uneven but promising potential to automate literature
screening. By systematically analyzing prompt-model interactions, we provide a
comparative benchmark and practical guidance for task-adaptive LLM deployment.

</details>


### [6] [Facts in Stats: Impacts of Pretraining Diversity on Language Model Generalization](https://arxiv.org/abs/2510.16096)
*Tina Behnia,Puneesh Deora,Christos Thrampoulidis*

Main category: cs.CL

TL;DR: 本文提出了一个合成测试平台，用于系统分析语言模型中统计规律性和事实关联之间的交互影响，发现上下文多样性和结构对事实泛化能力有复杂影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对语言模型中统计规律性和事实关联交互影响的系统分析，特别是它们如何影响泛化能力。

Method: 设计灵活的合成测试平台，将通用标记的统计流与抽象事实的源-目标标记对流相结合，通过控制流组成和多样性水平来独立操纵上下文结构。

Result: 研究发现更高的上下文多样性会延迟分布内事实准确性，但对分布外事实泛化的影响取决于上下文结构。在某些情况下，低多样性会阻碍事实回忆，而最优多样性水平取决于训练时长。

Conclusion: 上下文设计和多样性水平的相互作用以不同方式影响泛化能力，通过模型组件干预发现嵌入层和解嵌入层对分布外失败有重要影响，合成框架为未来研究提供了受控测试平台。

Abstract: Language models are pretrained on sequences that blend statistical
regularities (making text fluent) with factual associations between specific
tokens (knowledge of facts). While recent work suggests that the variability of
their interaction, such as paraphrases of factual associations, critically
determines generalization ability, we lack a systematic analysis of these
impacts. This paper introduces a flexible synthetic testbed that combines a
statistical stream of generic tokens with an abstract factual stream of
source-target token pairs, enabling fine-grained control over their
interaction. The design enables the independent control of diversity nature by
manipulating stream composition (contextual structure) and the diversity level
by varying which statistical streams each fact appears in. Through controlled
experiments, we find that while higher contextual diversity delays
in-distribution (ID) factual accuracy, its impact on out-of-distribution (OOD)
factual generalization depends critically on contextual structure. In some
cases, OOD performance follows the same trend as ID, but in others, diversity
becomes essential for non-trivial factual recall. Even when low diversity
prohibits factual recall, optimal diversity levels depend on training duration.
Beyond factual recall failures, we identify structures where statistical
generalization fails independently, and others where both capabilities degrade.
This shows how the interplay between contextual design and diversity level
impacts different generalization aspects. Further, through a series of
controlled interventions on the model components, we trace the OOD failures to
distinct optimization bottlenecks, highlighting the importance of the embedding
and unembedding layers. Our synthetic framework allows us to isolate effects
that would be confounded in large-scale studies, offering a controlled testbed
for future investigations.

</details>


### [7] [In Generative AI We (Dis)Trust? Computational Analysis of Trust and Distrust in Reddit Discussions](https://arxiv.org/abs/2510.16173)
*Aria Pessianzadeh,Naima Sultana,Hildegarde Van den Bulck,David Gefen,Shahin Jabari,Rezvaneh Rezapour*

Main category: cs.CL

TL;DR: 首个关于生成式AI信任与不信任的计算研究，使用2022-2025年Reddit数据，发现信任与不信任基本平衡，技术性能和可用性是主要维度，个人经验是态度形成的最常见原因。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI系统融入日常生活，理解公众对其的信任对于负责任采用和治理至关重要。现有AI信任研究缺乏计算性、大规模和纵向方法。

Method: 使用多年度Reddit数据集(39个子版块，197,618条帖子)，结合众包标注和分类模型进行扩展分析。

Result: 信任与不信任随时间基本平衡，主要模型发布时出现转变。技术性能和可用性占主导维度，个人经验是态度形成的最常见原因。不同信任者群体(专家、伦理学家、普通用户)呈现不同模式。

Conclusion: 研究提供了大规模信任分析的方法框架，并为理解公众对生成式AI不断演变的认知提供了洞见。

Abstract: The rise of generative AI (GenAI) has impacted many aspects of human life. As
these systems become embedded in everyday practices, understanding public trust
in them also becomes essential for responsible adoption and governance. Prior
work on trust in AI has largely drawn from psychology and human-computer
interaction, but there is a lack of computational, large-scale, and
longitudinal approaches to measuring trust and distrust in GenAI and large
language models (LLMs). This paper presents the first computational study of
Trust and Distrust in GenAI, using a multi-year Reddit dataset (2022--2025)
spanning 39 subreddits and 197,618 posts. Crowd-sourced annotations of a
representative sample were combined with classification models to scale
analysis. We find that Trust and Distrust are nearly balanced over time, with
shifts around major model releases. Technical performance and usability
dominate as dimensions, while personal experience is the most frequent reason
shaping attitudes. Distinct patterns also emerge across trustors (e.g.,
experts, ethicists, general users). Our results provide a methodological
framework for large-scale Trust analysis and insights into evolving public
perceptions of GenAI.

</details>


### [8] [EgMM-Corpus: A Multimodal Vision-Language Dataset for Egyptian Culture](https://arxiv.org/abs/2510.16198)
*Mohamed Gamil,Abdelrahman Elsayed,Abdelrahman Lila,Ahmed Gad,Hesham Abdelgawad,Mohamed Aref,Ahmed Fares*

Main category: cs.CL

TL;DR: 介绍了EgMM-Corpus，一个专门针对埃及文化的多模态数据集，包含3000多张图像，涵盖313个概念，用于评估和训练视觉语言模型在埃及文化背景下的表现。


<details>
  <summary>Details</summary>
Motivation: 中东和非洲地区的多模态文化多样性数据集仍然有限，需要专门针对埃及文化的资源来评估和训练视觉语言模型。

Method: 设计和运行新的数据收集流程，收集了涵盖地标、食物和民间传说的313个概念的3000多张图像，每个条目都经过人工验证文化真实性和多模态一致性。

Result: CLIP模型在EgMM-Corpus上的零样本性能为Top-1准确率21.2%，Top-5准确率36.4%，表明大规模视觉语言模型存在文化偏见。

Conclusion: EgMM-Corpus作为开发文化感知模型的基准具有重要意义，突显了现有模型的文化偏见问题。

Abstract: Despite recent advances in AI, multimodal culturally diverse datasets are
still limited, particularly for regions in the Middle East and Africa. In this
paper, we introduce EgMM-Corpus, a multimodal dataset dedicated to Egyptian
culture. By designing and running a new data collection pipeline, we collected
over 3,000 images, covering 313 concepts across landmarks, food, and folklore.
Each entry in the dataset is manually validated for cultural authenticity and
multimodal coherence. EgMM-Corpus aims to provide a reliable resource for
evaluating and training vision-language models in an Egyptian cultural context.
We further evaluate the zero-shot performance of Contrastive Language-Image
Pre-training CLIP on EgMM-Corpus, on which it achieves 21.2% Top-1 accuracy and
36.4% Top-5 accuracy in classification. These results underscore the existing
cultural bias in large-scale vision-language models and demonstrate the
importance of EgMM-Corpus as a benchmark for developing culturally aware
models.

</details>


### [9] [What Can String Probability Tell Us About Grammaticality?](https://arxiv.org/abs/2510.16227)
*Jennifer Hu,Ethan Gotlieb Wilcox,Siyuan Song,Kyle Mahowald,Roger P. Levy*

Main category: cs.CL

TL;DR: 该论文通过理论分析和实证验证，探讨了语言模型对语法的学习情况，建立了语法、意义和字符串概率之间的关系框架，并验证了三个关键预测。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型是否真正学习了语法知识，因为概率和语法性在语言学中是不同概念，需要明确字符串概率能揭示语言模型底层语法知识的程度。

Method: 基于语料库数据生成过程的简单假设，建立了理论分析框架，并使用28万句英语和中文句子对进行实证验证，包括最小对分析、模型与人类判断相关性等方法。

Result: 验证了三个预测：(1)最小对中字符串概率的相关性；(2)模型与人类在最小对中判断差异的相关性；(3)语法和不合语法字符串在概率空间中难以分离。

Conclusion: 为使用概率来了解语言模型的结构知识提供了理论基础，并为未来语言模型语法评估工作指明了方向。

Abstract: What have language models (LMs) learned about grammar? This question remains
hotly debated, with major ramifications for linguistic theory. However, since
probability and grammaticality are distinct notions in linguistics, it is not
obvious what string probabilities can reveal about an LM's underlying
grammatical knowledge. We present a theoretical analysis of the relationship
between grammar, meaning, and string probability, based on simple assumptions
about the generative process of corpus data. Our framework makes three
predictions, which we validate empirically using 280K sentence pairs in English
and Chinese: (1) correlation between the probability of strings within minimal
pairs, i.e., string pairs with minimal semantic differences; (2) correlation
between models' and humans' deltas within minimal pairs; and (3) poor
separation in probability space between unpaired grammatical and ungrammatical
strings. Our analyses give theoretical grounding for using probability to learn
about LMs' structural knowledge, and suggest directions for future work in LM
grammatical evaluation.

</details>


### [10] [Towards Low-Resource Alignment to Diverse Perspectives with Sparse Feedback](https://arxiv.org/abs/2510.16257)
*Chu Fei Luo,Samuel Dahan,Xiaodan Zhu*

Main category: cs.CL

TL;DR: 通过多元化解码和模型引导方法，在低资源设置下增强语言模型的多元对齐能力，仅用50个标注样本就能在多个高风险任务中提升性能。


<details>
  <summary>Details</summary>
Motivation: 语言模型对社会影响日益增大，需要确保其能够对齐多样化的观点并反映人类价值观的细微差别。当前主流训练范式假设每个查询只有一个最优答案，导致响应泛化且对齐效果差。

Method: 提出两种方法：多元化解码和模型引导。在低资源设置下（仅50个标注样本）进行实验，与零样本和少样本基线进行比较。

Result: 模型引导方法在零样本和少样本基线上表现出一致的改进。在仇恨言论检测和错误信息检测等高风险任务中降低了假阳性率，在GlobalOpinionQA中改善了与人类价值观的分布对齐。

Conclusion: 这项工作强调了多样性的重要性，展示了语言模型如何适应考虑细微观点，为增强语言模型的多元对齐提供了有效方法。

Abstract: As language models have a greater impact on society, it is important to
ensure they are aligned to a diverse range of perspectives and are able to
reflect nuance in human values. However, the most popular training paradigms
for modern language models often assume there is one optimal answer for every
query, leading to generic responses and poor alignment. In this work, we aim to
enhance pluralistic alignment of language models in a low-resource setting with
two methods: pluralistic decoding and model steering. We empirically
demonstrate that model steering offers consistent improvement over zero-shot
and few-shot baselines with only 50 annotated samples. Our proposed methods
decrease false positives in several high-stakes tasks such as hate speech
detection and misinformation detection, and improves the distributional
alignment to human values in GlobalOpinionQA. We hope our work highlights the
importance of diversity and how language models can be adapted to consider
nuanced perspectives.

</details>


### [11] [Instant Personalized Large Language Model Adaptation via Hypernetwork](https://arxiv.org/abs/2510.16282)
*Zhaoxuan Tan,Zixuan Zhang,Haoyang Wen,Zheng Li,Rongzhi Zhang,Pei Chen,Fengran Mo,Zheyuan Liu,Qingkai Zeng,Qingyu Yin,Meng Jiang*

Main category: cs.CL

TL;DR: Profile-to-PEFT框架使用超网络将用户配置文件直接映射到适配器参数，无需为每个用户单独训练，实现高效、可扩展的LLM个性化。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效微调方法需要为每个用户训练单独的适配器，计算成本高且无法实时更新，限制了大规模应用。

Method: 采用端到端训练的超网络，将编码后的用户配置文件直接映射到完整的适配器参数（如LoRA），部署时无需对每个用户进行训练。

Result: 该方法在性能上优于基于提示的个性化和OPPU方法，同时部署时计算资源消耗显著减少，对分布外用户具有强泛化能力。

Conclusion: Profile-to-PEFT框架实现了高效、可扩展且自适应的LLM个性化，适用于大规模应用场景。

Abstract: Personalized large language models (LLMs) tailor content to individual
preferences using user profiles or histories. However, existing
parameter-efficient fine-tuning (PEFT) methods, such as the
``One-PEFT-Per-User'' (OPPU) paradigm, require training a separate adapter for
each user, making them computationally expensive and impractical for real-time
updates. We introduce Profile-to-PEFT, a scalable framework that employs a
hypernetwork, trained end-to-end, to map a user's encoded profile directly to a
full set of adapter parameters (e.g., LoRA), eliminating per-user training at
deployment. This design enables instant adaptation, generalization to unseen
users, and privacy-preserving local deployment. Experimental results
demonstrate that our method outperforms both prompt-based personalization and
OPPU while using substantially fewer computational resources at deployment. The
framework exhibits strong generalization to out-of-distribution users and
maintains robustness across varying user activity levels and different
embedding backbones. The proposed Profile-to-PEFT framework enables efficient,
scalable, and adaptive LLM personalization suitable for large-scale
applications.

</details>


### [12] [Thinking About Thinking: Evaluating Reasoning in Post-Trained Language Models](https://arxiv.org/abs/2510.16340)
*Pratham Singla,Shivank Garg,Ayush Singh,Ishan Garg,Ketan Suhaas Saichandran*

Main category: cs.CL

TL;DR: 该研究评估了后训练LLM对其学习内容和推理过程的认知能力，发现RL训练模型比SFT模型更了解自身行为且泛化能力更强，但推理过程与最终输出的对齐较弱。


<details>
  <summary>Details</summary>
Motivation: 探究LLM是否意识到自己"学习"和"思考"的内容，定义三个核心能力：对学习策略的认知、策略跨领域泛化、内部推理与输出的对齐。

Method: 在多个需要学习不同策略的任务上实证评估，对比SFT、DPO和GRPO三种后训练方法的模型表现。

Result: RL训练模型比SFT模型更了解学习行为且泛化能力更强，但推理过程与最终输出的对齐较弱，GRPO训练模型这一效应最明显。

Conclusion: 后训练方法影响LLM的自我认知能力，RL方法增强策略认知和泛化但削弱推理对齐，需要平衡这些能力的发展。

Abstract: Recent advances in post-training techniques have endowed Large Language
Models (LLMs) with enhanced capabilities for tackling complex, logic-intensive
tasks through the generation of supplementary planning tokens. This development
raises a fundamental question: Are these models aware of what they "learn" and
"think"? To address this, we define three core competencies: (1) awareness of
learned latent policies, (2) generalization of these policies across domains,
and (3) alignment between internal reasoning traces and final outputs. We
empirically evaluate these abilities on several tasks, each designed to require
learning a distinct policy. Furthermore, we contrast the profiles of models
post-trained via Supervised Fine-Tuning (SFT), Direct Policy Optimization
(DPO), and Group Relative Policy Optimization (GRPO). Our findings indicate
that RL-trained models not only demonstrate greater awareness of their learned
behaviors and stronger generalizability to novel, structurally similar tasks
than SFT models but also often exhibit weak alignment between their reasoning
traces and final outputs, an effect most pronounced in GRPO-trained models.

</details>


### [13] [Utilising Large Language Models for Generating Effective Counter Arguments to Anti-Vaccine Tweets](https://arxiv.org/abs/2510.16359)
*Utsav Dhanuka,Soham Poddar,Saptarshi Ghosh*

Main category: cs.CL

TL;DR: 本研究探索利用LLMs生成针对疫苗错误信息的反驳论点，通过多种提示策略和微调方法优化反驳生成，并训练分类器对反疫苗推文进行多标签分类，以生成更具上下文感知的反驳。


<details>
  <summary>Details</summary>
Motivation: 在社交媒体影响公共卫生的时代，打击疫苗怀疑论和错误信息成为关键社会目标。虽然错误信息检测已取得进展，但实时生成针对性的反驳论点仍是一个未充分探索的领域。

Method: 实验多种提示策略和微调方法优化反驳生成，训练分类器将反疫苗推文分类为疫苗效力、副作用、政治影响等多标签类别，实现上下文感知的反驳。

Result: 通过人工判断、LLM评估和自动指标的评估显示这些方法之间具有很强的一致性。整合标签描述和结构化微调提高了反驳论点的有效性。

Conclusion: 该方法为大规模缓解疫苗错误信息提供了一个有前景的解决方案，展示了LLMs在生成针对性反驳方面的能力。

Abstract: In an era where public health is increasingly influenced by information
shared on social media, combatting vaccine skepticism and misinformation has
become a critical societal goal. Misleading narratives around vaccination have
spread widely, creating barriers to achieving high immunisation rates and
undermining trust in health recommendations. While efforts to detect
misinformation have made significant progress, the generation of real time
counter-arguments tailored to debunk such claims remains an insufficiently
explored area. In this work, we explore the capabilities of LLMs to generate
sound counter-argument rebuttals to vaccine misinformation. Building on prior
research in misinformation debunking, we experiment with various prompting
strategies and fine-tuning approaches to optimise counter-argument generation.
Additionally, we train classifiers to categorise anti-vaccine tweets into
multi-labeled categories such as concerns about vaccine efficacy, side effects,
and political influences allowing for more context aware rebuttals. Our
evaluation, conducted through human judgment, LLM based assessments, and
automatic metrics, reveals strong alignment across these methods. Our findings
demonstrate that integrating label descriptions and structured fine-tuning
enhances counter-argument effectiveness, offering a promising approach for
mitigating vaccine misinformation at scale.

</details>


### [14] [End-to-End Argument Mining through Autoregressive Argumentative Structure Prediction](https://arxiv.org/abs/2510.16363)
*Nilmadhab Das,Vishal Vaibhav,Yash Sunil Choudhary,V. Vijaya Saradhi,Ashish Anand*

Main category: cs.CL

TL;DR: 提出了AASP框架，通过自回归结构预测联合建模论点挖掘中的组件和关系，在三个基准测试中取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过生成范式扁平化论证结构，难以建模论点组件和论证关系之间的依赖关系。

Method: 使用自回归论证结构预测框架，将论证结构建模为预定义动作集，通过条件预训练语言模型逐步构建论证结构。

Result: 在三个标准AM基准测试中，AASP在两个基准上实现了最先进的结果，在一个基准上取得了强劲表现。

Conclusion: AASP框架能够有效捕捉论证推理流程，在联合建模论点挖掘任务方面表现出色。

Abstract: Argument Mining (AM) helps in automating the extraction of complex
argumentative structures such as Argument Components (ACs) like Premise, Claim
etc. and Argumentative Relations (ARs) like Support, Attack etc. in an
argumentative text. Due to the inherent complexity of reasoning involved with
this task, modelling dependencies between ACs and ARs is challenging. Most of
the recent approaches formulate this task through a generative paradigm by
flattening the argumentative structures. In contrast to that, this study
jointly formulates the key tasks of AM in an end-to-end fashion using
Autoregressive Argumentative Structure Prediction (AASP) framework. The
proposed AASP framework is based on the autoregressive structure prediction
framework that has given good performance for several NLP tasks. AASP framework
models the argumentative structures as constrained pre-defined sets of actions
with the help of a conditional pre-trained language model. These actions build
the argumentative structures step-by-step in an autoregressive manner to
capture the flow of argumentative reasoning in an efficient way. Extensive
experiments conducted on three standard AM benchmarks demonstrate that AASP
achieves state-of-theart (SoTA) results across all AM tasks in two benchmarks
and delivers strong results in one benchmark.

</details>


### [15] [Navigating through the hidden embedding space: steering LLMs to improve mental health assessment](https://arxiv.org/abs/2510.16373)
*Federico Ravenda,Seyed Ali Bahrainian,Andrea Raballo,Antonietta Mira*

Main category: cs.CL

TL;DR: 提出一种轻量级方法，通过线性变换特定层激活来提升LLM在心理健康评估中的表现，无需计算密集型技术


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs快速发展，但小型模型在特定领域应用中仍表现不佳，特别是在心理健康这样的敏感高影响领域

Method: 应用线性变换到特定层的激活，利用转向向量来引导模型输出

Result: 该方法在两个任务上取得改进：识别Reddit帖子是否有助于检测抑郁症状的相关性预测，以及基于用户Reddit历史完成标准化抑郁筛查问卷

Conclusion: 转向机制作为计算效率高的工具，在LLMs心理健康领域适应方面具有未开发的潜力

Abstract: The rapid evolution of Large Language Models (LLMs) is transforming AI,
opening new opportunities in sensitive and high-impact areas such as Mental
Health (MH). Yet, despite these advancements, recent evidence reveals that
smaller-scale models still struggle to deliver optimal performance in
domain-specific applications. In this study, we present a cost-efficient yet
powerful approach to improve MH assessment capabilities of an LLM, without
relying on any computationally intensive techniques. Our lightweight method
consists of a linear transformation applied to a specific layer's activations,
leveraging steering vectors to guide the model's output. Remarkably, this
intervention enables the model to achieve improved results across two distinct
tasks: (1) identifying whether a Reddit post is useful for detecting the
presence or absence of depressive symptoms (relevance prediction task), and (2)
completing a standardized psychological screening questionnaire for depression
based on users' Reddit post history (questionnaire completion task). Results
highlight the untapped potential of steering mechanisms as computationally
efficient tools for LLMs' MH domain adaptation.

</details>


### [16] [MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in Language Models, More than Outcomes](https://arxiv.org/abs/2510.16380)
*Yu Ying Chiu,Michael S. Lee,Rachel Calcott,Brandon Handoko,Paul de Font-Reaulx,Paula Rodriguez,Chen Bo Calvin Zhang,Ziwen Han,Udari Madhushani Sehwag,Yash Maurya,Christina Q Knight,Harry R. Lloyd,Florence Bacus,Mantas Mazeika,Bing Liu,Yejin Choi,Mitchell L Gordon,Sydney Levine*

Main category: cs.CL

TL;DR: MoReBench是一个包含1000个道德场景和23000多个评分标准的基准测试，用于评估AI的道德推理过程，重点关注推理过程而非最终答案。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在决策中扮演越来越重要的角色，需要理解它们如何做出决策，特别是道德决策。道德困境是评估推理过程的理想测试平台，因为它们允许多种合理的结论。

Method: 创建MoReBench基准测试，包含道德场景和专家制定的评分标准，涵盖识别道德考量、权衡利弊、提供可行建议等方面。同时创建MoReBench-Theory测试AI在五种规范伦理学框架下的推理能力。

Result: 研究发现扩展定律和现有的数学、代码、科学推理基准测试无法预测模型进行道德推理的能力。模型对特定道德框架（如边沁功利主义和康德义务论）表现出偏好，这可能是流行训练范式的副作用。

Conclusion: 这些基准测试推动了以过程为重点的推理评估，朝着更安全、更透明的AI发展。

Abstract: As AI systems progress, we rely more on them to make decisions with us and
for us. To ensure that such decisions are aligned with human values, it is
imperative for us to understand not only what decisions they make but also how
they come to those decisions. Reasoning language models, which provide both
final responses and (partially transparent) intermediate thinking traces,
present a timely opportunity to study AI procedural reasoning. Unlike math and
code problems which often have objectively correct answers, moral dilemmas are
an excellent testbed for process-focused evaluation because they allow for
multiple defensible conclusions. To do so, we present MoReBench: 1,000 moral
scenarios, each paired with a set of rubric criteria that experts consider
essential to include (or avoid) when reasoning about the scenarios. MoReBench
contains over 23 thousand criteria including identifying moral considerations,
weighing trade-offs, and giving actionable recommendations to cover cases on AI
advising humans moral decisions as well as making moral decisions autonomously.
Separately, we curate MoReBench-Theory: 150 examples to test whether AI can
reason under five major frameworks in normative ethics. Our results show that
scaling laws and existing benchmarks on math, code, and scientific reasoning
tasks fail to predict models' abilities to perform moral reasoning. Models also
show partiality towards specific moral frameworks (e.g., Benthamite Act
Utilitarianism and Kantian Deontology), which might be side effects of popular
training paradigms. Together, these benchmarks advance process-focused
reasoning evaluation towards safer and more transparent AI.

</details>


### [17] [ATA: A Neuro-Symbolic Approach to Implement Autonomous and Trustworthy Agents](https://arxiv.org/abs/2510.16381)
*David Peer,Sebastian Stabinger*

Main category: cs.CL

TL;DR: 提出了一种名为自主可信代理(ATA)的神经符号方法，通过将任务分解为离线知识摄取和在线任务处理两个阶段，解决LLM在可信度方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在可信度方面存在幻觉、不稳定性和缺乏透明度等局限性，阻碍了其在高风险领域的部署。

Method: 采用神经符号方法，将任务分解为两个阶段：离线知识摄取阶段将非正式问题规范转换为形式化知识库；在线任务处理阶段使用符号决策引擎基于形式化知识库和编码输入得出可靠结果。

Result: 在复杂推理任务上的评估表明，ATA与最先进的端到端推理模型具有竞争力，同时保持可信度。使用人工验证的知识库时，ATA显著优于更大的模型，并表现出完美的确定性、增强的稳定性以及对提示注入攻击的固有免疫力。

Conclusion: ATA通过基于符号推理的决策生成，为构建下一代透明、可审计和可靠的自主体提供了一个实用且可控的架构。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities, yet
their deployment in high-stakes domains is hindered by inherent limitations in
trustworthiness, including hallucinations, instability, and a lack of
transparency. To address these challenges, we introduce a generic
neuro-symbolic approach, which we call Autonomous Trustworthy Agents (ATA). The
core of our approach lies in decoupling tasks into two distinct phases: Offline
knowledge ingestion and online task processing. During knowledge ingestion, an
LLM translates an informal problem specification into a formal, symbolic
knowledge base. This formal representation is crucial as it can be verified and
refined by human experts, ensuring its correctness and alignment with domain
requirements. In the subsequent task processing phase, each incoming input is
encoded into the same formal language. A symbolic decision engine then utilizes
this encoded input in conjunction with the formal knowledge base to derive a
reliable result. Through an extensive evaluation on a complex reasoning task,
we demonstrate that a concrete implementation of ATA is competitive with
state-of-the-art end-to-end reasoning models in a fully automated setup while
maintaining trustworthiness. Crucially, with a human-verified and corrected
knowledge base, our approach significantly outperforms even larger models,
while exhibiting perfect determinism, enhanced stability against input
perturbations, and inherent immunity to prompt injection attacks. By generating
decisions grounded in symbolic reasoning, ATA offers a practical and
controllable architecture for building the next generation of transparent,
auditable, and reliable autonomous agents.

</details>


### [18] [Probing the Hidden Talent of ASR Foundation Models for L2 English Oral Assessment](https://arxiv.org/abs/2510.16387)
*Fu-An Chao,Bi-Cheng Yan,Berlin Chen*

Main category: cs.CL

TL;DR: 本研究探索了Whisper语音识别模型在第二语言口语评估中的潜力，通过提取其隐藏表示中的声学和语言特征，仅需训练轻量级分类器即可在GEPT数据集上超越现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 挖掘Whisper模型在第二语言口语评估中的潜在能力，超越以往仅使用其转录文本的研究方法。

Method: 从Whisper的隐藏表示中提取声学和语言特征，仅训练轻量级分类器，并整合图像和文本提示作为辅助相关性线索。

Result: 在GEPT图片描述数据集上表现出色，超越了包括多模态方法在内的现有最先进基线方法，通过整合辅助信息获得额外性能提升。

Conclusion: 即使没有任务特定的微调，Whisper模型也能内在编码口语的序数熟练度模式和语义方面，显示出其作为口语评估和口语理解任务的强大基础模型的潜力。

Abstract: In this paper, we explore the untapped potential of Whisper, a
well-established automatic speech recognition (ASR) foundation model, in the
context of L2 spoken language assessment (SLA). Unlike prior studies that
extrinsically analyze transcriptions produced by Whisper, our approach goes a
step further to probe its latent capabilities by extracting acoustic and
linguistic features from hidden representations. With only a lightweight
classifier being trained on top of Whisper's intermediate and final outputs,
our method achieves strong performance on the GEPT picture-description dataset,
outperforming existing cutting-edge baselines, including a multimodal approach.
Furthermore, by incorporating image and text-prompt information as auxiliary
relevance cues, we demonstrate additional performance gains. Finally, we
conduct an in-depth analysis of Whisper's embeddings, which reveals that, even
without task-specific fine-tuning, the model intrinsically encodes both ordinal
proficiency patterns and semantic aspects of speech, highlighting its potential
as a powerful foundation for SLA and other spoken language understanding tasks.

</details>


### [19] [FrugalPrompt: Reducing Contextual Overhead in Large Language Models via Token Attribution](https://arxiv.org/abs/2510.16439)
*Syed Rifat Raiyan,Md Farhan Ishmam,Abdullah Al Imran,Mohammad Ali Moni*

Main category: cs.CL

TL;DR: FrugalPrompt是一个新颖的LLM提示压缩框架，通过保留最具语义重要性的token来减少输入长度，在保持性能的同时降低成本和延迟。


<details>
  <summary>Details</summary>
Motivation: LLM的庞大输入上下文带来了高昂的货币成本、碳足迹和推理延迟，而典型提示中存在大量冗余的低效用token，只有少数token承载主要语义权重。

Method: 使用GlobEnc和DecompX两种token归因方法为输入序列中的每个token分配显著性分数，按原始顺序保留前k%的token，获得稀疏的压缩提示。

Result: 在情感分析、常识问答和摘要任务中，20%的提示压缩仅导致任务性能的边际损失，而数学推理性能急剧下降，反映了对完整token连续性的更强依赖性。

Conclusion: 该工作有助于更细致地理解LLM在性能-效率权衡中的行为，并界定了容忍上下文稀疏性的任务与需要详尽上下文的任务之间的边界。

Abstract: Large language models (LLMs) owe much of their stellar performance to
expansive input contexts, yet such verbosity inflates monetary costs, carbon
footprint, and inference-time latency. Much of this overhead manifests from the
redundant low-utility tokens present in typical prompts, as only a fraction of
tokens typically carries the majority of the semantic weight. We address this
inefficiency by introducing FrugalPrompt, a novel prompt compression framework
for LLMs, which retains only the most semantically significant tokens.
Leveraging two state-of-the-art token attribution methods, GlobEnc and DecompX,
we assign salience scores to every token in an input sequence, rank them to
preserve the top-k% tokens in their original order, and obtain a sparse
frugalized prompt. We evaluate the approach across four NLP tasks: Sentiment
Analysis, Commonsense QA, Summarization, and Mathematical Reasoning, using a
suite of frontier LLMs. For the first three tasks, a 20% prompt reduction
incurs only a marginal loss in task performance, demonstrating that
contemporary LLMs can reconstruct elided context from high-salience cues. In
contrast, performance on mathematical reasoning deteriorates sharply,
reflecting a stronger dependence on complete token continuity. Further analysis
with bottom-k% and random-k% tokens reveals asymmetric performance patterns
that may suggest potential task contamination effects, wherein models may
resort to shallow memorized patterns from pretraining exposure for conventional
NLP tasks. We posit that our work contributes to a more nuanced understanding
of LLM behavior in performance-efficiency trade-offs, and delineate the
boundary between tasks tolerant to contextual sparsity and those requiring
exhaustive context. Our source code and models are available at:
https://github.com/Starscream-11813/Frugal-ICL

</details>


### [20] [TrajSelector: Harnessing Latent Representations for Efficient and Effective Best-of-N in Large Reasoning Model](https://arxiv.org/abs/2510.16449)
*Bin Yu,Xinming Wang,Shijie Lian,Haotian Li,Changti Wu,Ruina Hu,Bailing Wang,Yuliang Wei,Kai Chen*

Main category: cs.CL

TL;DR: TrajSelector是一个高效的Best-of-N框架，利用LLM的隐藏状态进行过程级评分，通过轻量级验证器评估推理轨迹质量，在降低推理成本的同时显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有外部TTS方法存在计算开销大和未充分利用LLM内在表示的问题，需要更高效的轨迹选择框架。

Method: 使用轻量级验证器（仅0.6B参数）评估步骤级轨迹质量，通过端到端训练利用LLM隐藏状态进行过程评分，无需大量步骤级标注。

Result: 在五个基准测试中，TrajSelector在Best-of-32设置下比多数投票准确率提升4.61%，比现有过程奖励模型提升4.31%-12.21%，且推理成本更低。

Conclusion: TrajSelector通过有效利用LLM内部表示，实现了高效且性能优异的推理轨迹选择，为TTS范式提供了更优解决方案。

Abstract: Large language models (LLMs) have shown remarkable progress in complex
reasoning tasks, largely enabled by test-time scaling (TTS) paradigms that
allocate additional compute during inference. Among these, external TTS
(particularly the Best-of-N selection paradigm) yields scalable performance
improvements by selecting from multiple independently generated reasoning
trajectories. However, this approach faces key limitations: (i) the high
computational overhead of deploying process reward models, (ii) the
underutilization of the LLM's intrinsic latent representations. We introduce
TrajSelector, an efficient and effective Best-of-N framework that exploit the
hidden states in the sampler LLM for process-level scoring. A lightweight
verifier (with only 0.6B parameters) evaluates the quality of step-wise
trajectory, and then aggregates these scores to identify the optimal reasoning
trajectory. Our framework employs a fully data-driven, end-to-end training
recipe that eliminates reliance on massive step-level annotations. Experiential
results across five benchmarks demonstrate that TrajSelector delivers
consistent performance gains. In Best-of-32 settings, it surpasses majority
voting by 4.61% accuracy and outperforms existing process reward models by
4.31% to 12.21%, all while maintaining lower inference costs.

</details>


### [21] [RAVEN: Robust Advertisement Video Violation Temporal Grounding via Reinforcement Reasoning](https://arxiv.org/abs/2510.16455)
*Deyi Ji,Yuekui Yang,Haiyang Wu,Shaoping Ma,Tianrun Chen,Lanyun Zhu*

Main category: cs.CL

TL;DR: RAVEN是一个集成课程强化学习和多模态大语言模型的广告视频违规检测框架，通过渐进式训练策略和GRPO优化，在无需显式推理标注的情况下发展推理能力，在工业数据集和公开基准上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有广告视频违规检测方法在精确时间定位、噪声标注和泛化能力方面存在不足，需要更先进的框架来解决这些问题。

Method: 结合课程强化学习和多模态大语言模型，采用渐进式训练策略整合精确和粗略标注数据，使用GRPO发展推理能力，并设计多层次复杂奖励机制。

Result: 在工业数据集和公开基准上，RAVEN在违规类别准确性和时间区间定位方面表现优异，在线A/B测试验证了其实际应用价值，精度和召回率显著提升。

Conclusion: RAVEN框架有效解决了广告视频违规检测的关键挑战，展示了强大的泛化能力，并成功部署到在线广告服务中，具有重要的实际应用价值。

Abstract: Advertisement (Ad) video violation detection is critical for ensuring
platform compliance, but existing methods struggle with precise temporal
grounding, noisy annotations, and limited generalization. We propose RAVEN, a
novel framework that integrates curriculum reinforcement learning with
multimodal large language models (MLLMs) to enhance reasoning and cognitive
capabilities for violation detection. RAVEN employs a progressive training
strategy, combining precisely and coarsely annotated data, and leverages Group
Relative Policy Optimization (GRPO) to develop emergent reasoning abilities
without explicit reasoning annotations. Multiple hierarchical sophisticated
reward mechanism ensures precise temporal grounding and consistent category
prediction. Experiments on industrial datasets and public benchmarks show that
RAVEN achieves superior performances in violation category accuracy and
temporal interval localization. We also design a pipeline to deploy the RAVEN
on the online Ad services, and online A/B testing further validates its
practical applicability, with significant improvements in precision and recall.
RAVEN also demonstrates strong generalization, mitigating the catastrophic
forgetting issue associated with supervised fine-tuning.

</details>


### [22] [Agree, Disagree, Explain: Decomposing Human Label Variation in NLI through the Lens of Explanations](https://arxiv.org/abs/2510.16458)
*Pingjun Hong,Beiduo Chen,Siyao Peng,Marie-Catherine de Marneffe,Benjamin Roth,Barbara Plank*

Main category: cs.CL

TL;DR: 本文通过LiTEx分类法分析自然语言推理中的标注者差异，发现标注者可能在标签和推理类型上都存在分歧，但表面标签分歧可能掩盖了深层的解释一致性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注标注者标签一致但解释不同的情况，本文扩展研究范围，分析标注者在标签和推理类型上的双重分歧，以更全面理解NLI标注中的个体差异。

Method: 使用LiTEx分类法分析两个英文NLI数据集，从NLI标签一致性、解释相似性和分类法一致性三个维度对齐标注差异，并考虑标注者选择偏见的复合因素。

Result: 发现标注者标签不一致但解释高度相似的实例，表明表面分歧可能掩盖深层一致性；分析还揭示了标注者在解释策略和标签选择上的个体偏好。

Conclusion: 推理类型的一致性比单纯标签一致性更能反映自由文本解释的语义相似性，强调需要谨慎将标签视为绝对真值，重视基于推理的解释的丰富性。

Abstract: Natural Language Inference datasets often exhibit human label variation. To
better understand these variations, explanation-based approaches analyze the
underlying reasoning behind annotators' decisions. One such approach is the
LiTEx taxonomy, which categorizes free-text explanations in English into
reasoning types. However, previous work applying such taxonomies has focused on
within-label variation: cases where annotators agree on the final NLI label but
provide different explanations. In contrast, this paper broadens the scope by
examining how annotators may diverge not only in the reasoning type but also in
the labeling step. We use explanations as a lens to decompose the reasoning
process underlying NLI annotation and to analyze individual differences. We
apply LiTEx to two NLI English datasets and align annotation variation from
multiple aspects: NLI label agreement, explanation similarity, and taxonomy
agreement, with an additional compounding factor of annotators' selection bias.
We observe instances where annotators disagree on the label but provide highly
similar explanations, suggesting that surface-level disagreement may mask
underlying agreement in interpretation. Moreover, our analysis reveals
individual preferences in explanation strategies and label choices. These
findings highlight that agreement in reasoning types better reflects the
semantic similarity of free-text explanations than label agreement alone. Our
findings underscore the richness of reasoning-based explanations and the need
for caution in treating labels as ground truth.

</details>


### [23] [Check Yourself Before You Wreck Yourself: Selectively Quitting Improves LLM Agent Safety](https://arxiv.org/abs/2510.16492)
*Vamshi Krishna Bonagiri,Ponnurangam Kumaragurum,Khanh Nguyen,Benjamin Plaut*

Main category: cs.CL

TL;DR: 提出使用"退出"作为LLM代理的安全机制，让代理在缺乏信心时主动退出，在12个先进LLM上的评估显示该方法能显著提升安全性而几乎不影响实用性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理在复杂现实环境中运行，其安全性变得至关重要。多轮代理场景中的不确定性和模糊性会累积，导致比传统文本生成失败更严重的风险。

Method: 利用ToolEmu框架，在12个最先进的LLM上系统评估退出行为，通过添加明确的退出指令来让代理在缺乏信心时主动退出。

Result: 结果显示安全性平均提升+0.39（0-3分制），专有模型提升+0.64，而实用性仅平均下降-0.03，展现出极佳的安全-实用性权衡。

Conclusion: 简单的退出指令是一种高度有效的安全机制，可立即部署到现有代理系统中，作为高风险应用中自主代理的有效第一道防线。

Abstract: As Large Language Model (LLM) agents increasingly operate in complex
environments with real-world consequences, their safety becomes critical. While
uncertainty quantification is well-studied for single-turn tasks, multi-turn
agentic scenarios with real-world tool access present unique challenges where
uncertainties and ambiguities compound, leading to severe or catastrophic risks
beyond traditional text generation failures. We propose using "quitting" as a
simple yet effective behavioral mechanism for LLM agents to recognize and
withdraw from situations where they lack confidence. Leveraging the ToolEmu
framework, we conduct a systematic evaluation of quitting behavior across 12
state-of-the-art LLMs. Our results demonstrate a highly favorable
safety-helpfulness trade-off: agents prompted to quit with explicit
instructions improve safety by an average of +0.39 on a 0-3 scale across all
models (+0.64 for proprietary models), while maintaining a negligible average
decrease of -0.03 in helpfulness. Our analysis demonstrates that simply adding
explicit quit instructions proves to be a highly effective safety mechanism
that can immediately be deployed in existing agent systems, and establishes
quitting as an effective first-line defense mechanism for autonomous agents in
high-stakes applications.

</details>


### [24] [Automated Composition of Agents: A Knapsack Approach for Agentic Component Selection](https://arxiv.org/abs/2510.16499)
*Michelle Yuan,Khushbu Pahwa,Shuaichen Chang,Mustafa Kaba,Jiarong Jiang,Xiaofei Ma,Yi Zhang,Monica Sunkara*

Main category: cs.CL

TL;DR: 提出基于在线背包问题的自动化智能体系统组合框架，通过动态测试和实时效用建模，在预算约束下优化选择智能体组件，显著提升成功率并降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有智能体系统组合方法依赖静态语义检索，存在能力描述不完整、检索方法局限等问题，组件选择未充分考虑能力、成本和实时效用。

Method: 引入结构化自动化框架，受背包问题启发，让组合器智能体系统性地识别、选择和组装最优智能体组件集，联合考虑性能、预算约束和兼容性。

Result: 在5个基准数据集上的实证评估显示，在线背包组合器始终位于帕累托前沿，相比基线方法，单智能体设置下成功率提升高达31.6%，多智能体系统中从37%提升到87%。

Conclusion: 该方法在多样化领域和预算约束下展现出强大的适应性，显著性能差距证实了其有效性。

Abstract: Designing effective agentic systems requires the seamless composition and
integration of agents, tools, and models within dynamic and uncertain
environments. Most existing methods rely on static, semantic retrieval
approaches for tool or agent discovery. However, effective reuse and
composition of existing components remain challenging due to incomplete
capability descriptions and the limitations of retrieval methods. Component
selection suffers because the decisions are not based on capability, cost, and
real-time utility. To address these challenges, we introduce a structured,
automated framework for agentic system composition that is inspired by the
knapsack problem. Our framework enables a composer agent to systematically
identify, select, and assemble an optimal set of agentic components by jointly
considering performance, budget constraints, and compatibility. By dynamically
testing candidate components and modeling their utility in real-time, our
approach streamlines the assembly of agentic systems and facilitates scalable
reuse of resources. Empirical evaluation with Claude 3.5 Sonnet across five
benchmarking datasets shows that our online-knapsack-based composer
consistently lies on the Pareto frontier, achieving higher success rates at
significantly lower component costs compared to our baselines. In the
single-agent setup, the online knapsack composer shows a success rate
improvement of up to 31.6% in comparison to the retrieval baselines. In
multi-agent systems, the online knapsack composer increases success rate from
37% to 87% when agents are selected from an agent inventory of 100+ agents. The
substantial performance gap confirms the robust adaptability of our method
across diverse domains and budget constraints.

</details>


### [25] [ReviewGuard: Enhancing Deficient Peer Review Detection via LLM-Driven Data Augmentation](https://arxiv.org/abs/2510.16549)
*Haoxuan Zhang,Ruochi Li,Sarthak Shrestha,Shree Harshini Mamidala,Revanth Putta,Arka Krishan Aggarwal,Ting Xiao,Junhua Ding,Haihua Chen*

Main category: cs.CL

TL;DR: 提出了ReviewGuard系统，使用四阶段LLM驱动框架自动检测和分类有缺陷的同行评审，通过合成数据增强和模型微调提高检测性能。


<details>
  <summary>Details</summary>
Motivation: 同行评审作为科学守门人面临提交量激增和LLM广泛使用的挑战，有缺陷的评审（来自人类专家和AI系统）威胁同行评审生态系统和学术诚信。

Method: 四阶段LLM驱动框架：收集ICLR和NeurIPS论文及评审；使用GPT-4.1标注评审类型；通过LLM驱动的合成数据增强解决类别不平衡；微调编码器模型和开源LLM。

Result: 构建了包含6,634篇论文、24,657个真实评审和46,438个合成评审的语料库。有缺陷评审显示较低评分、较高自信度、结构复杂性降低和负面情绪比例更高。AI生成评审自ChatGPT出现后显著增加。

Conclusion: 这是首个用于检测有缺陷同行评审的LLM驱动系统，为同行评审中的AI治理提供证据，并为维护学术诚信的人机协作提供宝贵见解。

Abstract: Peer review serves as the gatekeeper of science, yet the surge in submissions
and widespread adoption of large language models (LLMs) in scholarly evaluation
present unprecedented challenges. Recent work has focused on using LLMs to
improve review efficiency or generate insightful review content. However,
unchecked deficient reviews from both human experts and AI systems threaten to
systematically undermine the peer review ecosystem and compromise academic
integrity. To address this critical issue, we introduce ReviewGuard, an
automated system for detecting and categorizing deficient reviews. ReviewGuard
employs a comprehensive four-stage LLM-driven framework that: (1) collects ICLR
and NeurIPS papers with their corresponding reviews from OpenReview; (2)
annotates review types using GPT-4.1 with human validation; (3) addresses class
imbalance and data scarcity through LLM-driven synthetic data augmentation,
producing a final corpus of 6,634 papers, 24,657 real reviews, and 46,438
synthetic reviews; and (4) fine-tunes both encoder-based models and open source
LLMs. We perform comprehensive feature analysis of the structure and quality of
the review text. Compared to sufficient reviews, deficient reviews demonstrate
lower rating scores, higher self-reported confidence, reduced structural
complexity, and a higher proportion of negative sentiment. AI-generated text
detection reveals that, since ChatGPT's emergence, AI-generated reviews have
increased dramatically. In the evaluation of deficient review detection models,
mixed training with synthetic and real review data provides substantial
enhancements to recall and F1 scores on the binary task. This study presents
the first LLM-driven system for detecting deficient peer reviews, providing
evidence to inform AI governance in peer review while offering valuable
insights into human-AI collaboration to maintain academic integrity.

</details>


### [26] [Language over Content: Tracing Cultural Understanding in Multilingual Large Language Models](https://arxiv.org/abs/2510.16565)
*Seungho Cho,Changgeon Ko,Eui Jun Hwang,Junmyeong Lee,Huije Lee,Jong C. Park*

Main category: cs.CL

TL;DR: 该研究通过分析LLM在回答文化相关问题时内部激活路径的重叠程度，揭示了语言对文化理解机制的重要影响，发现语言相似性并不保证内部表征的一致性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在多元文化背景下的广泛应用，准确的文化理解变得至关重要。现有评估多关注输出层面，难以揭示响应差异的驱动因素，而电路分析研究覆盖语言少且很少聚焦文化。

Method: 通过测量LLM在回答语义等价问题时的激活路径重叠：1）固定问题语言，改变目标国家；2）固定国家，改变问题语言。使用同语言国家对来分离语言和文化因素。

Result: 结果显示，同语言跨国家问题的内部路径重叠度高于跨语言同国家问题，表明存在强烈的语言特定模式。特别是韩国-朝鲜对的低重叠和高变异性显示语言相似性不保证内部表征对齐。

Conclusion: LLM的文化理解机制受语言影响显著，语言相似性不足以确保文化表征的一致性，这对跨文化LLM应用具有重要意义。

Abstract: Large language models (LLMs) are increasingly used across diverse cultural
contexts, making accurate cultural understanding essential. Prior evaluations
have mostly focused on output-level performance, obscuring the factors that
drive differences in responses, while studies using circuit analysis have
covered few languages and rarely focused on culture. In this work, we trace
LLMs' internal cultural understanding mechanisms by measuring activation path
overlaps when answering semantically equivalent questions under two conditions:
varying the target country while fixing the question language, and varying the
question language while fixing the country. We also use same-language country
pairs to disentangle language from cultural aspects. Results show that internal
paths overlap more for same-language, cross-country questions than for
cross-language, same-country questions, indicating strong language-specific
patterns. Notably, the South Korea-North Korea pair exhibits low overlap and
high variability, showing that linguistic similarity does not guarantee aligned
internal representation.

</details>


### [27] [Hallucination Benchmark for Speech Foundation Models](https://arxiv.org/abs/2510.16567)
*Alkis Koudounas,Moreno La Quatra,Manuel Giollo,Sabato Marco Siniscalchi,Elena Baralis*

Main category: cs.CL

TL;DR: SHALLOW是首个系统分类和量化ASR系统中幻觉现象的基准框架，通过词汇、语音、形态和语义四个维度评估模型，在WER较高时能捕捉WER无法区分的细粒度错误模式。


<details>
  <summary>Details</summary>
Motivation: ASR系统中的幻觉会产生与语音信号完全无关但语法语义合理的转录，在医疗和法律等关键领域带来严重风险，而传统评估指标无法区分语音不准确和幻觉。

Method: 引入SHALLOW基准框架，系统地将ASR幻觉现象分类为词汇、语音、形态和语义四个互补维度，并为每个类别定义针对性指标来生成可解释的模型行为分析。

Result: SHALLOW指标在识别质量高时与WER强相关，但随着WER增加相关性显著减弱，能够在退化条件下捕捉WER无法区分的细粒度错误模式。

Conclusion: SHALLOW框架支持对模型弱点的具体诊断，并提供超出聚合错误率所能提供的模型改进反馈，是评估ASR系统幻觉倾向的有效工具。

Abstract: Hallucinations in automatic speech recognition (ASR) systems refer to fluent
and coherent transcriptions produced by neural ASR models that are completely
unrelated to the underlying acoustic input (i.e., the speech signal). While
similar to conventional decoding errors in potentially compromising the
usability of transcriptions for downstream applications, hallucinations can be
more detrimental due to their preservation of syntactically and semantically
plausible structure. This apparent coherence can mislead subsequent processing
stages and introduce serious risks, particularly in critical domains such as
healthcare and law. Conventional evaluation metrics are primarily centered on
error-based metrics and fail to distinguish between phonetic inaccuracies and
hallucinations. Consequently, there is a critical need for new evaluation
frameworks that can effectively identify and assess models with a heightened
propensity for generating hallucinated content. To this end, we introduce
SHALLOW, the first benchmark framework that systematically categorizes and
quantifies hallucination phenomena in ASR along four complementary axes:
lexical, phonetic, morphological, and semantic. We define targeted metrics
within each category to produce interpretable profiles of model behavior.
Through evaluation across various architectures and speech domains, we have
found that SHALLOW metrics correlate strongly with word error rate (WER) when
recognition quality is high (i.e., low WER). Still, this correlation weakens
substantially as WER increases. SHALLOW, therefore, captures fine-grained error
patterns that WER fails to distinguish under degraded and challenging
conditions. Our framework supports specific diagnosis of model weaknesses and
provides feedback for model improvement beyond what aggregate error rates can
offer.

</details>


### [28] [AI-Generated Text Detection in Low-Resource Languages: A Case Study on Urdu](https://arxiv.org/abs/2510.16573)
*Muhammad Ammar,Hadiya Murad Hadi,Usman Majeed Butt*

Main category: cs.CL

TL;DR: 提出了一种针对乌尔都语的AI生成文本检测框架，使用多语言transformer模型在平衡数据集上训练，mDeBERTa-v3-base模型在测试集上达到91.29% F1分数和91.26%准确率。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs生成文本能力增强，区分人类与机器生成文本变得困难，特别是对于乌尔都语等低资源语言，缺乏有效的AI文本检测工具。

Method: 构建包含1800篇人类撰写和1800篇AI生成文本的平衡数据集，进行语言和统计分析，微调三种多语言transformer模型（mdeberta-v3-base、distilbert-base-multilingualcased、xlm-roberta-base）。

Result: mDeBERTa-v3-base模型表现最佳，在测试集上F1分数为91.29%，准确率为91.26%。

Conclusion: 该研究推进了在乌尔都语社区对抗错误信息和学术不端行为的努力，并为低资源语言的NLP工具开发做出了贡献。

Abstract: Large Language Models (LLMs) are now capable of generating text that closely
resembles human writing, making them powerful tools for content creation, but
this growing ability has also made it harder to tell whether a piece of text
was written by a human or by a machine. This challenge becomes even more
serious for languages like Urdu, where there are very few tools available to
detect AI-generated text. To address this gap, we propose a novel AI-generated
text detection framework tailored for the Urdu language. A balanced dataset
comprising 1,800 humans authored, and 1,800 AI generated texts, sourced from
models such as Gemini, GPT-4o-mini, and Kimi AI was developed. Detailed
linguistic and statistical analysis was conducted, focusing on features such as
character and word counts, vocabulary richness (Type Token Ratio), and N-gram
patterns, with significance evaluated through t-tests and MannWhitney U tests.
Three state-of-the-art multilingual transformer models such as
mdeberta-v3-base, distilbert-base-multilingualcased, and xlm-roberta-base were
fine-tuned on this dataset. The mDeBERTa-v3-base achieved the highest
performance, with an F1-score 91.29 and accuracy of 91.26% on the test set.
This research advances efforts in contesting misinformation and academic
misconduct in Urdu-speaking communities and contributes to the broader
development of NLP tools for low resource languages.

</details>


### [29] [Fine-tuning of Large Language Models for Constituency Parsing Using a Sequence to Sequence Approach](https://arxiv.org/abs/2510.16604)
*Francisco Jose Cortes Delgado,Eduardo Martinez Gracia,Rafael Valencia Garcia*

Main category: cs.CL

TL;DR: 通过微调大语言模型将句子翻译为句法结构，扩展西班牙语法教学工具MiSintaxis的能力，在短语结构分析中取得高准确率


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型的新进展探索句法分析的新方法，扩展西班牙语法教学工具MiSintaxis的功能

Method: 使用Hugging Face库中的多个模型，基于AnCora-ES语料库生成的训练数据进行微调，将输入句子翻译为对应的句法结构

Result: 使用F1分数评估性能，在短语结构分析中表现出高准确率

Conclusion: 该方法展示了在句法分析中的潜力，为语法教学工具提供了有效的技术支撑

Abstract: Recent advances in natural language processing with large neural models have
opened new possibilities for syntactic analysis based on machine learning. This
work explores a novel approach to phrase-structure analysis by fine-tuning
large language models (LLMs) to translate an input sentence into its
corresponding syntactic structure. The main objective is to extend the
capabilities of MiSintaxis, a tool designed for teaching Spanish syntax.
Several models from the Hugging Face repository were fine-tuned using training
data generated from the AnCora-ES corpus, and their performance was evaluated
using the F1 score. The results demonstrate high accuracy in phrase-structure
analysis and highlight the potential of this methodology.

</details>


### [30] [Unleashing Diverse Thinking Modes in LLMs through Multi-Agent Collaboration](https://arxiv.org/abs/2510.16645)
*Zhixuan He,Yue Feng*

Main category: cs.CL

TL;DR: DiMo是一个多智能体协作框架，通过模拟四个专业LLM智能体之间的结构化辩论来提升性能与可解释性，在六个基准测试中表现优于单模型和辩论基线。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然性能强大但缺乏可解释的推理过程，需要一种能同时提升性能和解释性的方法。

Method: 使用四个体现不同推理范式的专业LLM智能体进行迭代辩论，通过相互挑战和精炼初始响应来获得更鲁棒的结论。

Result: 在统一开源设置下，DiMo在六个基准测试中准确率超过广泛使用的单模型和辩论基线，尤其在数学任务上提升最大。

Conclusion: DiMo作为一个语义感知、Web原生的多智能体框架，能够生成语义类型化、URL注释的证据链，为下游系统提供可检查重用的结构化证明。

Abstract: Large Language Models (LLMs) demonstrate strong performance but often lack
interpretable reasoning. This paper introduces the Multi-Agent Collaboration
Framework for Diverse Thinking Modes (DiMo), which enhances both performance
and interpretability by simulating a structured debate among four specialized
LLM agents. Each agent embodies a distinct reasoning paradigm, allowing the
framework to collaboratively explore diverse cognitive approaches. Through
iterative debate, agents challenge and refine initial responses, yielding more
robust conclusions and an explicit, auditable reasoning chain. Across six
benchmarks and under a unified open-source setup, DiMo improves accuracy over
widely used single-model and debate baselines, with the largest gains on math.
We position DiMo as a semantics-aware, Web-native multi-agent framework: it
models human-machine intelligence with LLM agents that produce semantically
typed, URL-annotated evidence chains for explanations and user-friendly
interactions. Although our experiments use standard reasoning benchmarks, the
framework is designed to be instantiated over Web corpora and knowledge graphs,
combining retrieval-augmented reasoning with structured justifications that
downstream systems can inspect and reuse.

</details>


### [31] [All You Need is One: Capsule Prompt Tuning with a Single Vector](https://arxiv.org/abs/2510.16670)
*Yiyang Liu,James C. Liang,Heng Fan,Wenhao Yang,Yiming Cui,Xiaotian Han,Lifu Huang,Dongfang Liu,Qifan Wang,Cheng Han*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Prompt-based learning has emerged as a parameter-efficient finetuning (PEFT)
approach to facilitate Large Language Model (LLM) adaptation to downstream
tasks by conditioning generation with task-aware guidance. Despite its
successes, current prompt-based learning methods heavily rely on laborious grid
searching for optimal prompt length and typically require considerable number
of prompts, introducing additional computational burden. Worse yet, our pioneer
findings indicate that the task-aware prompt design is inherently limited by
its absence of instance-aware information, leading to a subtle attention
interplay with the input sequence. In contrast, simply incorporating
instance-aware information as a part of the guidance can enhance the
prompt-tuned model performance without additional fine-tuning. Moreover, we
find an interesting phenomenon, namely "attention anchor", that incorporating
instance-aware tokens at the earliest position of the sequence can successfully
preserve strong attention to critical structural information and exhibit more
active attention interaction with all input tokens. In light of our
observation, we introduce Capsule Prompt-Tuning (CaPT), an efficient and
effective solution that leverages off-the-shelf, informative instance semantics
into prompt-based learning. Our approach innovatively integrates both
instance-aware and task-aware information in a nearly parameter-free manner
(i.e., one single capsule prompt). Empirical results demonstrate that our
method can exhibit superior performance across various language tasks (e.g.,
84.03\% average accuracy on T5-Large), serving as an "attention anchor," while
enjoying high parameter efficiency (e.g., 0.003\% of model parameters on
Llama3.2-1B).

</details>


### [32] [Temporal Understanding under Deictic Frame of Reference](https://arxiv.org/abs/2510.16685)
*Damin Zhang,Julia Rayz*

Main category: cs.CL

TL;DR: 提出了TUuD框架来评估大语言模型在时间参考框架动态变化时的时间理解能力，发现LLMs在近期时间表现出类似人类的认知模式，但在远距离时间推理上存在局限。


<details>
  <summary>Details</summary>
Motivation: 人类通过空间隐喻来理解时间，使用时间参考框架(t-FoR)来感知时间关系。虽然LLMs在自然语言理解方面取得显著进展，但其时间理解和推理能力仍然有限，需要评估LLMs如何解释动态变化的时间参考框架。

Method: 引入TUuD框架，让LLMs在时间参考点"现在"沿时间线动态移动的情况下，评估时间-事件和事件-事件关系。基于时间认知研究，让LLMs对当前时刻与目标事件的相似度进行评分(0.00-1.00)。

Result: 四个评估的LLMs表现出对指示性t-FoR的可测量适应，相似度评分在当前时刻附近达到峰值，并向过去和未来事件递减。但这种适应在超出近期语境时会减弱。

Conclusion: LLMs显示出部分类似人类的时间认知，但其时间推理对参考框架变化和时间距离仍然敏感，在远距离时间理解上存在局限性。

Abstract: Understanding time is fundamental to human cognition, where temporal
experience is often conceptualized through spatial metaphors grounded in
sensory-motor experience. For example, "summer is approaching" parallels "We
are approaching the summer". In such expressions, humans rely on a frame of
reference (FoR) to interpret meaning relative to a particular viewpoint.
Extending this concept to time, a temporal frame of reference (t-FoR) defines
how temporal relations are perceived relative to an experiencer's moment of
"now". While Large Language Models (LLMs) have shown remarkable advances in
natural language understanding, their ability to interpret and reason about
time remains limited. In this work, we introduce TUuD (Temporal Understanding
under Deictic t-FoR), a framework that evaluates how LLMs interpret time-event
and event-event relations when the reference point of "now" dynamically shifts
along a timeline. Following recent work on temporal cognition
\cite{li2025other}, LLMs are prompted to rate the similarity between the
current moment and a target event from 0.00 (completely dissimilar) to 1.00
(highly similar), where similarity quantifies perceived temporal alignment
between the two points. Our results show that four evaluated LLMs exhibit
measurable adaptation to a deictic t-FoR, with similarity ratings peaking
around the present and decreasing toward past and future events. The
adaptation, however, weakens beyond near-term contexts, suggesting that while
LLMs display partial human-like temporal cognition, their temporal reasoning
remains sensitive to reference-frame shifts and temporal distance.

</details>


### [33] [Investigating the Impact of Rationales for LLMs on Natural Language Understanding](https://arxiv.org/abs/2510.16686)
*Wenhang Shi,Shuqing Bian,Yiren Chen,Xinyi Zhang,Zhe Zhao,Pengfei Hu,Wei Lu,Xiaoyong Du*

Main category: cs.CL

TL;DR: 本文探讨了思维链（CoT）推理在自然语言理解（NLU）任务中的应用，发现随着模型规模增大，CoT推理从阻碍变为提升NLU性能，且特定设计的训练方法能带来改进，训练后的模型在未见任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 大多数研究关注思维链在推理任务中的作用，忽视了其在自然语言理解任务中的潜力。本文旨在系统探索思维链是否同样能有益于NLU任务。

Method: 构建了包含思维链的全面高质量NLU数据集NLURC，开发了多种思维链增强方法，并在NLU任务上测试这些方法的适用性。

Result: 发现模型规模与CoT推理效果呈正相关；大多数思维链增强训练方法效果不如仅标签训练，但有一种特殊设计的方法能持续改进；使用思维链训练的模型在未见NLU任务上表现显著提升，性能可与十倍规模模型相媲美，且解释性与商业LLMs相当。

Conclusion: 思维链在NLU任务中具有重要价值，特别是随着模型规模增大，其效果从负面转为正面，且通过适当方法训练能带来显著性能提升和可解释性优势。

Abstract: Chain-of-thought (CoT) rationales, which provide step-by-step reasoning to
derive final answers, benefit LLMs in both inference and training.
Incorporating rationales, either by generating them before answering during
inference, or by placing them before or after the original answers during
training - significantly improves model performance on mathematical, symbolic
and commonsense reasoning tasks. However, most work focuses on the role of
rationales in these reasoning tasks, overlooking their potential impact on
other important tasks like natural language understanding (NLU) tasks. In this
work, we raise the question: Can rationales similarly benefit NLU tasks? To
conduct a systematic exploration, we construct NLURC, a comprehensive and
high-quality NLU dataset collection with rationales, and develop various
rationale-augmented methods. Through exploring the applicability of these
methods on NLU tasks using the dataset, we uncover several potentially
surprising findings: (1) CoT inference shifts from hindering NLU performance to
surpassing direct label prediction as model size grows, indicating a positive
correlation. (2) Most rationale-augmented training methods perform worse than
label-only training, with one specially designed method consistently achieving
improvements. (3) LLMs trained with rationales achieve significant performance
gains on unseen NLU tasks, rivaling models ten times their size, while
delivering interpretability on par with commercial LLMs.

</details>


### [34] [Natural Language Processing Applications in Cardiology: A Narrative Review](https://arxiv.org/abs/2510.16708)
*Kailai Yang,Yan Leng,Xin Zhang,Tianlin Zhang,Paul Thompson,Bernard Keavney,Maciej Tomaszewski,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 这篇综述对2014-2025年间心脏病学领域的自然语言处理研究进行了全面回顾，分析了265篇相关文献，涵盖NLP范式类型、心脏病相关任务类型、心血管疾病类型和数据源类型等多个维度。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病日益普遍且复杂，相关信息分散在患者叙述、医疗记录和科学文献等非结构化文本数据中。NLP技术能够分析这些数据，为心脏病学的诊断、治疗和预防提供新见解。

Method: 查询了六个文献数据库，通过严格筛选流程识别出265篇相关文章，从NLP范式类型、心脏病相关任务类型、心血管疾病类型和数据源类型等多个维度进行分析，并进行时间趋势分析。

Result: 分析显示每个维度都存在相当大的多样性，证明了NLP在心脏病学领域研究的广度。时间分析揭示了近十年来NLP方法的演变和变化趋势。

Conclusion: 这是迄今为止对心脏病学领域NLP研究最全面的综述，展示了NLP技术在心脏病学中的广泛应用和发展趋势。

Abstract: Cardiovascular disease has become increasingly prevalent in modern society
and has a significant effect on global health and well-being. Heart-related
conditions are intricate, multifaceted disorders, which may be influenced by a
combination of genetic predispositions, lifestyle choices, and various
socioeconomic and clinical factors. Information regarding these potentially
complex interrelationships is dispersed among diverse types of textual data,
which include patient narratives, medical records, and scientific literature,
among others. Natural language processing (NLP) techniques have increasingly
been adopted as a powerful means to analyse and make sense of this vast amount
of unstructured data. This, in turn, can allow healthcare professionals to gain
deeper insights into the cardiology field, which has the potential to
revolutionize current approaches to the diagnosis, treatment, and prevention of
cardiac problems. This review provides a detailed overview of NLP research in
cardiology between 2014 and 2025. We queried six literature databases to find
articles describing the application of NLP techniques in the context of a range
of different cardiovascular diseases. Following a rigorous screening process,
we identified a total of 265 relevant articles. We analysed each article from
multiple dimensions, i.e., NLP paradigm types, cardiology-related task types,
cardiovascular disease types, and data source types. Our analysis reveals
considerable diversity within each of these dimensions, thus demonstrating the
considerable breadth of NLP research within the field. We also perform a
temporal analysis, which illustrates the evolution and changing trends in NLP
methods employed over the last decade that we cover. To our knowledge, the
review constitutes the most comprehensive overview of NLP research in
cardiology to date.

</details>


### [35] [The Chameleon Nature of LLMs: Quantifying Multi-Turn Stance Instability in Search-Enabled Language Models](https://arxiv.org/abs/2510.16712)
*Shivam Ratnakar,Sanjay Raghavendra*

Main category: cs.CL

TL;DR: 研究发现LLMs在检索增强系统中存在变色龙行为：在多轮对话中面对矛盾问题时立场不稳定的严重缺陷。


<details>
  <summary>Details</summary>
Motivation: 揭示LLM与检索引擎集成系统中的关键漏洞，这些漏洞会破坏系统的可靠性，特别是在需要保持立场一致性的关键应用领域。

Method: 创建包含17,770个问答对的Chameleon基准数据集，涵盖12个争议领域；提出变色龙分数和来源重用率两个理论指标；评估Llama-4-Maverick、GPT-4o-mini和Gemini-2.5-Flash等模型。

Result: 所有模型都表现出严重的变色龙行为（分数0.391-0.511），GPT-4o-mini表现最差；来源重用率与置信度和立场变化呈显著正相关，表明知识多样性有限导致模型过度依赖查询框架。

Conclusion: 在医疗、法律和金融等需要保持立场一致性的关键系统中部署LLM之前，必须进行全面的稳定性评估。

Abstract: Integration of Large Language Models with search/retrieval engines has become
ubiquitous, yet these systems harbor a critical vulnerability that undermines
their reliability. We present the first systematic investigation of "chameleon
behavior" in LLMs: their alarming tendency to shift stances when presented with
contradictory questions in multi-turn conversations (especially in
search-enabled LLMs). Through our novel Chameleon Benchmark Dataset, comprising
17,770 carefully crafted question-answer pairs across 1,180 multi-turn
conversations spanning 12 controversial domains, we expose fundamental flaws in
state-of-the-art systems. We introduce two theoretically grounded metrics: the
Chameleon Score (0-1) that quantifies stance instability, and Source Re-use
Rate (0-1) that measures knowledge diversity. Our rigorous evaluation of
Llama-4-Maverick, GPT-4o-mini, and Gemini-2.5-Flash reveals consistent
failures: all models exhibit severe chameleon behavior (scores 0.391-0.511),
with GPT-4o-mini showing the worst performance. Crucially, small
across-temperature variance (less than 0.004) suggests the effect is not a
sampling artifact. Our analysis uncovers the mechanism: strong correlations
between source re-use rate and confidence (r=0.627) and stance changes
(r=0.429) are statistically significant (p less than 0.05), indicating that
limited knowledge diversity makes models pathologically deferential to query
framing. These findings highlight the need for comprehensive consistency
evaluation before deploying LLMs in healthcare, legal, and financial systems
where maintaining coherent positions across interactions is critical for
reliable decision support.

</details>


### [36] [so much depends / upon / a whitespace: Why Whitespace Matters for Poets and LLMs](https://arxiv.org/abs/2510.16713)
*Sriharsh Bhyravajjula,Melanie Walsh,Anna Preus,Maria Antoniak*

Main category: cs.CL

TL;DR: 该论文研究了诗歌中空白的重要性，分析了19k首诗歌的空白分布，比较了人类创作与LLM生成诗歌的空白使用差异，并探讨了不同文本处理方法对空白表示的影响。


<details>
  <summary>Details</summary>
Motivation: 空白是诗歌形式的关键组成部分，反映了诗人对标准化形式的遵循和反抗。尽管诗歌是长期存在的艺术形式，也是LLM的生成任务，但空白在NLP社区中未得到足够关注。

Method: 使用来自Poetry Foundation的19k首英语诗歌语料库，分析4k位诗人的空白使用情况。比较了已发表诗歌与51k首LLM生成诗歌和12k首未发表诗歌的空白使用差异，并探讨了不同时期、诗歌形式和数据源的空白使用模式。

Result: 发现不同文本处理方法会导致诗歌数据中空白表示显著不同。发布了2.8k首公共领域诗歌子集以促进该领域进一步研究。

Conclusion: 诗歌中的空白模式对LLM预训练数据集的处理策略具有重要启示，强调了在文本处理中保留空白信息的重要性。

Abstract: Whitespace is a critical component of poetic form, reflecting both adherence
to standardized forms and rebellion against those forms. Each poem's whitespace
distribution reflects the artistic choices of the poet and is an integral
semantic and spatial feature of the poem. Yet, despite the popularity of poetry
as both a long-standing art form and as a generation task for large language
models (LLMs), whitespace has not received sufficient attention from the NLP
community. Using a corpus of 19k English-language published poems from Poetry
Foundation, we investigate how 4k poets have used whitespace in their works. We
release a subset of 2.8k public-domain poems with preserved formatting to
facilitate further research in this area. We compare whitespace usage in the
published poems to (1) 51k LLM-generated poems, and (2) 12k unpublished poems
posted in an online community. We also explore whitespace usage across time
periods, poetic forms, and data sources. Additionally, we find that different
text processing methods can result in significantly different representations
of whitespace in poetry data, motivating us to use these poems and whitespace
patterns to discuss implications for the processing strategies used to assemble
pretraining datasets for LLMs.

</details>


### [37] [Beacon: Single-Turn Diagnosis and Mitigation of Latent Sycophancy in Large Language Models](https://arxiv.org/abs/2510.16727)
*Sanskar Pandey,Ruhaan Chopra,Angkul Puniya,Sohom Pal*

Main category: cs.CL

TL;DR: 提出了Beacon基准来测量大语言模型中的谄媚偏见，发现该偏见随模型能力增强而加剧，并提出了干预方法来调节真实性与顺从性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在奖励优化过程中混淆了帮助性和礼貌服从，形成了真实性和谄媚之间的结构性权衡，这种偏见会影响模型的理性判断。

Method: 引入Beacon基准，通过单轮强制选择任务独立测量谄媚偏见，评估了12个最先进模型，并提出提示级和激活级干预方法。

Result: 评估显示谄媚偏见可分解为稳定的语言和情感子偏见，且随模型容量增加而加剧；干预方法能在真实性和社会合规判断之间调节这些偏见。

Conclusion: Beacon将谄媚重新定义为可测量的规范错误泛化形式，为研究和缓解大规模生成系统中的对齐漂移提供了可复现的基础。

Abstract: Large language models internalize a structural trade-off between truthfulness
and obsequious flattery, emerging from reward optimization that conflates
helpfulness with polite submission. This latent bias, known as sycophancy,
manifests as a preference for user agreement over principled reasoning. We
introduce Beacon, a single-turn forced-choice benchmark that isolates this bias
independent of conversational context, enabling precise measurement of the
tension between factual accuracy and submissive bias. Evaluations across twelve
state-of-the-art models reveal that sycophancy decomposes into stable
linguistic and affective sub-biases, each scaling with model capacity. We
further propose prompt-level and activation-level interventions that modulate
these biases in opposing directions, exposing the internal geometry of
alignment as a dynamic manifold between truthfulness and socially compliant
judgment. Beacon reframes sycophancy as a measurable form of normative
misgeneralization, providing a reproducible foundation for studying and
mitigating alignment drift in large-scale generative systems.

</details>


### [38] [Enhancing Language Agent Strategic Reasoning through Self-Play in Adversarial Games](https://arxiv.org/abs/2510.16761)
*Yikai Zhang,Ye Rong,Siyu Yuan,Jiangjie Chen,Jian Xie,Yanghua Xiao*

Main category: cs.CL

TL;DR: 提出SCO-PAL方法，通过自博弈学习提升语言代理在动态对抗游戏中的战略推理能力，相比基线平均胜率提升约30%，对抗GPT-4达到54.76%胜率。


<details>
  <summary>Details</summary>
Motivation: 现有语言代理在动态对抗游戏中战略推理能力不足，需要无需专家标注数据的自动学习方法，而对手选择对学习性能有重要影响但研究不足。

Method: 提出SCO-PAL方法，通过玩与学习进行策略优化，分析不同级别对手选择，发现自博弈是最有效的学习方式。

Result: 使用SCO-PAL与自博弈，在六个对抗游戏中相比基线平均胜率提升约30%，对抗GPT-4达到54.76%胜率。

Conclusion: 自博弈是提升对抗环境中战略推理的最有效方式，SCO-PAL方法显著提升了语言代理在动态对抗游戏中的表现。

Abstract: Existing language agents often encounter difficulties in dynamic adversarial
games due to poor strategic reasoning. To mitigate this limitation, a promising
approach is to allow agents to learn from game interactions automatically,
without relying on costly expert-labeled data. Unlike static environments where
agents receive fixed feedback or rewards, selecting appropriate opponents in
dynamic adversarial games can significantly impact learning performance.
However, the discussion of opponents in adversarial environments remains an
area under exploration. In this paper, we propose a Step-level poliCy
Optimization method through Play-And-Learn, SCO-PAL. Leveraging SCO-PAL, we
conduct a detailed analysis of opponent selection by setting opponents at
different levels and find that self-play is the most effective way to improve
strategic reasoning in such adversarial environments. Utilizing SCO-PAL with
self-play, we increase the average win rate against four opponents by
approximately 30% compared to baselines and achieve a 54.76% win rate against
GPT-4 in six adversarial games.

</details>


### [39] [LC-Eval: A Bilingual Multi-Task Evaluation Benchmark for Long-Context Understanding](https://arxiv.org/abs/2510.16783)
*Sheikh Jubair,Arwa Omayrah,Amal Alshammari,Alhanoof Althnian,Abdulhamed Alothaimen,Norah A. Alzahrani,Shahad D. Alzaidi,Nora Al-Twairesh,Abdulmohsen Al-Thubaity*

Main category: cs.CL

TL;DR: LC-Eval是一个双语多任务评估基准，用于评估英语和阿拉伯语的长上下文理解能力，涵盖4k到128k+token的上下文长度，包含四个具有挑战性的任务。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在长上下文处理能力上的进步，需要严格的评估方法来有效评估其在长上下文理解方面的表现。

Method: 开发了LC-Eval评估基准，包含四个新颖任务：多文档问答、双语问答、段落内声明验证和基于长上下文的多选题，涵盖英语和阿拉伯语数据集。

Result: 评估显示LC-Eval具有显著挑战性，即使是GPT-4o等高性能模型在某些任务上也表现困难，突显了基准的复杂性和严谨性。

Conclusion: LC-Eval为评估大语言模型的长上下文理解能力提供了一个全面且具有挑战性的基准，揭示了当前模型在深度推理和信息追踪方面的局限性。

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated
sophisticated capabilities, including the ability to process and comprehend
extended contexts. These emergent capabilities necessitate rigorous evaluation
methods to effectively assess their performance in long-context understanding.
In this paper, we present \textbf{LC-Eval}, a bilingual, multi-task evaluation
benchmark designed to evaluate long-context understanding in English and
Arabic, targeting context lengths ranging from 4k to over 128k tokens. LC-Eval
introduces four novel and challenging tasks: multi-document question answering,
bilingual question answering, claim verification within a paragraph, and
multiple-choice questions based on long contexts. These tasks are designed to
assess LLMs' abilities in deep reasoning, document comprehension, information
tracing, and bilingual information extraction and understanding. The benchmark
includes datasets in both Arabic and English for each task, allowing for a
comparative analysis of their performance across different text genres.
Evaluations were conducted on both open-weight and closed LLMs, with results
indicating that LC-Eval presents significant challenges. Even high-performing
models, such as GPT-4o, struggled with certain tasks, highlighting the
complexity and rigor of the benchmark.

</details>


### [40] [MOSAIC: Masked Objective with Selective Adaptation for In-domain Contrastive Learning](https://arxiv.org/abs/2510.16797)
*Vera Pavlova,Mohammed Makhlouf*

Main category: cs.CL

TL;DR: 提出MOSAIC框架，通过多阶段域适应方法结合掩码语言建模和对比学习目标，提升句子嵌入模型在专业领域的性能。


<details>
  <summary>Details</summary>
Motivation: 解决大规模通用领域句子嵌入模型适应专业领域的挑战，需要在保持原有语义区分能力的同时学习领域相关表示。

Method: 多阶段域适应框架，联合优化掩码语言建模和对比学习目标，通过选择性适应实现统一训练流程。

Result: 在高资源和低资源领域均取得显著改进，NDCG@10指标最高提升13.4%，消融研究验证各组件有效性。

Conclusion: 平衡的联合监督和分阶段适应对领域适应至关重要，MOSAIC框架能有效提升句子嵌入模型在专业领域的性能。

Abstract: We introduce MOSAIC (Masked Objective with Selective Adaptation for In-domain
Contrastive learning), a multi-stage framework for domain adaptation of
sentence embedding models that incorporates joint domain-specific masked
supervision. Our approach addresses the challenges of adapting large-scale
general-domain sentence embedding models to specialized domains. By jointly
optimizing masked language modeling (MLM) and contrastive objectives within a
unified training pipeline, our method enables effective learning of
domain-relevant representations while preserving the robust semantic
discrimination properties of the original model. We empirically validate our
approach on both high-resource and low-resource domains, achieving improvements
up to 13.4% in NDCG@10 (Normalized Discounted Cumulative Gain) over strong
general-domain baselines. Comprehensive ablation studies further demonstrate
the effectiveness of each component, highlighting the importance of balanced
joint supervision and staged adaptation.

</details>


### [41] [Knowing the Facts but Choosing the Shortcut: Understanding How Large Language Models Compare Entities](https://arxiv.org/abs/2510.16815)
*Hans Hergen Lehmann,Jae Hee Lee,Steven Schockaert,Stefan Wermter*

Main category: cs.CL

TL;DR: LLMs在实体比较任务中经常依赖启发式偏见而非真实知识，即使具备正确数值知识也会做出错误预测。研究发现实体流行度、提及顺序和语义共现三种启发式偏见严重影响模型预测。大模型能选择性使用更可靠的数值知识，而小模型则无法区分，思维链提示能引导所有模型更好地使用数值特征。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在知识推理任务中何时依赖真实知识而非表面启发式，通过实体比较任务分析模型决策机制，理解模型大小对知识使用的影响。

Method: 使用实体比较任务（如比较河流长度），分析模型预测与三种启发式偏见（实体流行度、提及顺序、语义共现）的关系，比较不同规模模型的表现，并测试思维链提示的效果。

Result: LLMs经常做出与自身数值知识矛盾的预测；小模型主要依赖启发式偏见，仅使用表面线索的逻辑回归比模型自身数值预测更准确；大模型能选择性使用更可靠的数值知识；思维链提示能改善所有模型对数值特征的使用。

Conclusion: LLMs在知识推理中严重依赖启发式偏见，模型规模影响知识使用的选择性，思维链提示是改善模型推理的有效方法。

Abstract: Large Language Models (LLMs) are increasingly used for knowledge-based
reasoning tasks, yet understanding when they rely on genuine knowledge versus
superficial heuristics remains challenging. We investigate this question
through entity comparison tasks by asking models to compare entities along
numerical attributes (e.g., ``Which river is longer, the Danube or the
Nile?''), which offer clear ground truth for systematic analysis. Despite
having sufficient numerical knowledge to answer correctly, LLMs frequently make
predictions that contradict this knowledge. We identify three heuristic biases
that strongly influence model predictions: entity popularity, mention order,
and semantic co-occurrence. For smaller models, a simple logistic regression
using only these surface cues predicts model choices more accurately than the
model's own numerical predictions, suggesting heuristics largely override
principled reasoning. Crucially, we find that larger models (32B parameters)
selectively rely on numerical knowledge when it is more reliable, while smaller
models (7--8B parameters) show no such discrimination, which explains why
larger models outperform smaller ones even when the smaller models possess more
accurate knowledge. Chain-of-thought prompting steers all models towards using
the numerical features across all model sizes.

</details>


### [42] [Cross-Genre Authorship Attribution via LLM-Based Retrieve-and-Rerank](https://arxiv.org/abs/2510.16819)
*Shantanu Agarwal,Joel Barry,Steven Fincke,Scott Miller*

Main category: cs.CL

TL;DR: 提出了一种基于检索-重排框架的跨体裁作者归属方法，通过微调LLM来识别与主题无关的作者特定语言模式，在跨体裁作者归属任务上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的信息检索方法在跨体裁作者归属任务中表现不佳，因为它们依赖主题线索而非作者特定的语言模式。需要开发能够有效学习作者区分性信号的方法。

Method: 采用两阶段检索-重排框架，首先检索候选作者，然后通过专门设计的数据策展策略训练重排器来识别作者特定的语言特征。

Result: 在HIATUS的HRS1和HRS2跨体裁作者归属基准测试中，分别比之前最优方法提高了22.3和34.4个绝对Success@8点。

Conclusion: 提出的LLM检索-重排管道能够有效解决跨体裁作者归属问题，通过针对性的训练策略学习作者区分性信号，显著优于现有方法。

Abstract: Authorship attribution (AA) is the task of identifying the most likely author
of a query document from a predefined set of candidate authors. We introduce a
two-stage retrieve-and-rerank framework that finetunes LLMs for cross-genre AA.
Unlike the field of information retrieval (IR), where retrieve-and-rerank is a
de facto strategy, cross-genre AA systems must avoid relying on topical cues
and instead learn to identify author-specific linguistic patterns that are
independent of the text's subject matter (genre/domain/topic). Consequently,
for the reranker, we demonstrate that training strategies commonly used in IR
are fundamentally misaligned with cross-genre AA, leading to suboptimal
behavior. To address this, we introduce a targeted data curation strategy that
enables the reranker to effectively learn author-discriminative signals. Using
our LLM-based retrieve-and-rerank pipeline, we achieve substantial gains of
22.3 and 34.4 absolute Success@8 points over the previous state-of-the-art on
HIATUS's challenging HRS1 and HRS2 cross-genre AA benchmarks.

</details>


### [43] [Who's Asking? Simulating Role-Based Questions for Conversational AI Evaluation](https://arxiv.org/abs/2510.16829)
*Navreet Kaur,Hoda Ayad,Hayoung Jung,Shravika Mittal,Munmun De Choudhury,Tanushree Mitra*

Main category: cs.CL

TL;DR: CoRUS框架通过角色理论模拟基于角色的提问，发现在阿片类药物使用障碍等敏感领域，用户角色会显著影响LLM的回复内容，脆弱角色（患者、照顾者）会获得更多支持性回复但知识内容减少。


<details>
  <summary>Details</summary>
Motivation: 现有评估大多忽略提问者角色，但在敏感领域如阿片类药物使用障碍中，考虑用户背景对于提供可访问、无污名化的回复至关重要。

Method: 基于角色理论和在线OUD康复社区帖子，构建提问者角色分类（患者、照顾者、从业者），并模拟15,321个嵌入各角色目标、行为和经验的问题。

Result: 模拟问题高度可信且与现实数据相当。评估5个LLM发现：相同问题但不同角色会引发系统性差异，脆弱角色获得更多支持性回复（+17%）但知识内容减少（-19%）。

Conclusion: 用户角色会隐性地影响模型回复，本研究提供了基于角色的对话AI评估方法。

Abstract: Language model users often embed personal and social context in their
questions. The asker's role -- implicit in how the question is framed --
creates specific needs for an appropriate response. However, most evaluations,
while capturing the model's capability to respond, often ignore who is asking.
This gap is especially critical in stigmatized domains such as opioid use
disorder (OUD), where accounting for users' contexts is essential to provide
accessible, stigma-free responses. We propose CoRUS (COmmunity-driven Roles for
User-centric Question Simulation), a framework for simulating role-based
questions. Drawing on role theory and posts from an online OUD recovery
community (r/OpiatesRecovery), we first build a taxonomy of asker roles --
patients, caregivers, practitioners. Next, we use it to simulate 15,321
questions that embed each role's goals, behaviors, and experiences. Our
evaluations show that these questions are both highly believable and comparable
to real-world data. When used to evaluate five LLMs, for the same question but
differing roles, we find systematic differences: vulnerable roles, such as
patients and caregivers, elicit more supportive responses (+17%) and reduced
knowledge content (-19%) in comparison to practitioners. Our work demonstrates
how implicitly signaling a user's role shapes model responses, and provides a
methodology for role-informed evaluation of conversational AI.

</details>


### [44] [FinSight: Towards Real-World Financial Deep Research](https://arxiv.org/abs/2510.16844)
*Jiajie Jin,Yuyao Zhang,Yimeng Xu,Hongjin Qian,Yutao Zhu,Zhicheng Dou*

Main category: cs.CL

TL;DR: FinSight是一个用于生成高质量多模态财务报告的多智能体框架，通过CAVM架构、迭代视觉增强机制和两阶段写作框架，显著提升了财务报告的准确性、分析深度和呈现质量。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统难以完全自动化生成专业的财务报告，因为这是一个劳动密集且智力要求高的过程。

Method: 采用Code Agent with Variable Memory (CAVM)架构统一外部数据、工具和智能体；提出迭代视觉增强机制逐步优化原始视觉输出；使用两阶段写作框架将简洁的分析链扩展为连贯的多模态报告。

Result: 在各种公司和行业级任务上的实验表明，FinSight在事实准确性、分析深度和呈现质量方面显著优于所有基线系统，包括领先的深度研究系统。

Conclusion: FinSight展示了生成接近人类专家质量报告的明确路径，为自动化财务报告生成提供了有效解决方案。

Abstract: Generating professional financial reports is a labor-intensive and
intellectually demanding process that current AI systems struggle to fully
automate. To address this challenge, we introduce FinSight (Financial InSight),
a novel multi agent framework for producing high-quality, multimodal financial
reports. The foundation of FinSight is the Code Agent with Variable Memory
(CAVM) architecture, which unifies external data, designed tools, and agents
into a programmable variable space, enabling flexible data collection, analysis
and report generation through executable code. To ensure professional-grade
visualization, we propose an Iterative Vision-Enhanced Mechanism that
progressively refines raw visual outputs into polished financial charts.
Furthermore, a two stage Writing Framework expands concise Chain-of-Analysis
segments into coherent, citation-aware, and multimodal reports, ensuring both
analytical depth and structural consistency. Experiments on various company and
industry-level tasks demonstrate that FinSight significantly outperforms all
baselines, including leading deep research systems in terms of factual
accuracy, analytical depth, and presentation quality, demonstrating a clear
path toward generating reports that approach human-expert quality.

</details>


### [45] [Neuronal Group Communication for Efficient Neural representation](https://arxiv.org/abs/2510.16851)
*Zhengqi Pei,Qingming Huang,Shuhui Wang*

Main category: cs.CL

TL;DR: 提出神经群通信(NGC)框架，将神经网络重构为神经群交互的动态系统，通过低维通信减少参数冗余，在保持稳定性的同时提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决现代神经网络规模扩大带来的效率低下和可解释性差的问题，构建能够学习高效、模块化和可解释表示的大型神经系统的核心问题。

Method: 将神经网络视为神经群交互的动态系统，权重作为神经状态间的瞬时交互，通过神经群间的迭代通信进行计算，引入神经稳定性度量来量化序列处理中的稳定模式。

Result: 在大语言模型中实例化NGC，在适度压缩下在复杂推理基准上表现更好，在相同压缩率下持续优于标准低秩近似和跨层基共享方法。

Conclusion: NGC框架通过结构化神经群动态与高维学习系统中的泛化能力相关，为构建高效、模块化和可解释的神经网络提供了新途径。

Abstract: The ever-increasing scale of modern neural networks has brought unprecedented
performance alongside daunting challenges in efficiency and interpretability.
This paper addresses the core question of how to build large neural systems
that learn efficient, modular, and interpretable representations. We propose
Neuronal Group Communication (NGC), a theory-driven framework that reimagines a
neural network as a dynamical system of interacting neuronal groups rather than
a monolithic collection of neural weights. Instead of treating each weight as
an independent trainable parameter, NGC treats weights as transient
interactions between embedding-like neuronal states, with neural computation
unfolding through iterative communication among groups of neurons. This
low-rank, modular representation yields compact models: groups of neurons
exchange low-dimensional signals, enabling intra-group specialization and
inter-group information sharing while dramatically reducing redundant
parameters. By drawing on dynamical systems theory, we introduce a neuronal
stability metric (analogous to Lyapunov stability) that quantifies the
contraction of neuron activations toward stable patterns during sequence
processing. Using this metric, we reveal that emergent reasoning capabilities
correspond to an external driving force or ``potential'', which nudges the
neural dynamics away from trivial trajectories while preserving stability.
Empirically, we instantiate NGC in large language models (LLMs) and demonstrate
improved performance on complex reasoning benchmarks under moderate
compression. NGC consistently outperforms standard low-rank approximations and
cross-layer basis-sharing methods at comparable compression rates. We conclude
by discussing the broader implications of NGC, including how structured
neuronal group dynamics might relate to generalization in high-dimensional
learning systems.

</details>


### [46] [Does Visual Grounding Enhance the Understanding of Embodied Knowledge in Large Language Models?](https://arxiv.org/abs/2510.16924)
*Zhihui Yang,Yupei Wang,Kaijie Mo,Zhe Zhao,Renfen Hu*

Main category: cs.CL

TL;DR: 多模态语言模型在具身知识理解方面并未比纯文本模型表现更好，特别是在视觉维度表现最差，需要更有效的具身知识整合。


<details>
  <summary>Details</summary>
Motivation: 研究多模态语言模型是否通过视觉基础增强了对具身知识的理解，与纯文本模型进行比较。

Method: 基于心理学感知理论构建具身知识理解基准，包含视觉、听觉、触觉、味觉、嗅觉和内部感知等感官维度，通过向量比较和问答任务评估30个先进语言模型。

Result: 视觉语言模型在两种任务中均未超越纯文本模型，且在视觉维度表现显著差于其他感官维度；向量表示易受词形和频率影响，模型在空间感知和推理问题上表现困难。

Conclusion: 当前语言模型对具身知识的整合效果有限，需要更有效的方法来增强对物理世界的理解能力。

Abstract: Despite significant progress in multimodal language models (LMs), it remains
unclear whether visual grounding enhances their understanding of embodied
knowledge compared to text-only models. To address this question, we propose a
novel embodied knowledge understanding benchmark based on the perceptual theory
from psychology, encompassing visual, auditory, tactile, gustatory, olfactory
external senses, and interoception. The benchmark assesses the models'
perceptual abilities across different sensory modalities through vector
comparison and question-answering tasks with over 1,700 questions. By comparing
30 state-of-the-art LMs, we surprisingly find that vision-language models
(VLMs) do not outperform text-only models in either task. Moreover, the models
perform significantly worse in the visual dimension compared to other sensory
dimensions. Further analysis reveals that the vector representations are easily
influenced by word form and frequency, and the models struggle to answer
questions involving spatial perception and reasoning. Our findings underscore
the need for more effective integration of embodied knowledge in LMs to enhance
their understanding of the physical world.

</details>


### [47] [ChiKhaPo: A Large-Scale Multilingual Benchmark for Evaluating Lexical Comprehension and Generation in Large Language Models](https://arxiv.org/abs/2510.16928)
*Emily Chang,Niyati Bafna*

Main category: cs.CL

TL;DR: ChiKhaPo是一个大规模多语言基准测试，包含8个难度不同的子任务，旨在评估生成模型在2700多种语言中的词汇理解和生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型基准测试主要局限于高资源或中资源语言，且侧重于高阶推理和生成任务，但大量证据表明LLMs在全球3800多种书面语言中缺乏基本语言能力。

Method: 利用现有词典、单语数据和双语平行语料构建ChiKhaPo基准，包含8个不同难度的子任务，涵盖2700多种语言。

Result: 6个最先进的模型在该基准测试中表现不佳，性能得分受语言家族、语言资源丰富度、任务类型以及理解与生成方向等因素影响。

Conclusion: ChiKhaPo旨在促进和鼓励LLMs的大规模多语言基准测试，填补了现有基准在语言覆盖范围上的空白。

Abstract: Existing benchmarks for large language models (LLMs) are largely restricted
to high- or mid-resource languages, and often evaluate performance on
higher-order tasks in reasoning and generation. However, plenty of evidence
points to the fact that LLMs lack basic linguistic competence in the vast
majority of the world's 3800+ written languages. We introduce ChiKhaPo,
consisting of 8 subtasks of varying difficulty designed to evaluate the lexical
comprehension and generation abilities of generative models. ChiKhaPo draws on
existing lexicons, monolingual data, and bitext, and provides coverage for
2700+ languages for 2 subtasks, surpassing any existing benchmark in terms of
language coverage. We further show that 6 SOTA models struggle on our
benchmark, and discuss the factors contributing to performance scores,
including language family, language resourcedness, task, and comprehension
versus generation directions. With ChiKhaPo, we hope to enable and encourage
the massively multilingual benchmarking of LLMs.

</details>


### [48] [Prompt-MII: Meta-Learning Instruction Induction for LLMs](https://arxiv.org/abs/2510.16932)
*Emily Xiao,Yixiao Zeng,Ada Chen,Chin-Jou Li,Amanda Bertsch,Graham Neubig*

Main category: cs.CL

TL;DR: PROMPT-MII是一个基于强化学习的元学习框架，能够为任意新数据集生成紧凑指令，在保持与上下文学习相当性能的同时，显著减少推理所需的token数量。


<details>
  <summary>Details</summary>
Motivation: 上下文学习虽然有效但推理成本高，随着上下文长度增长成本急剧增加。需要一种方法能够将训练示例压缩为紧凑但描述性强的提示。

Method: 提出PROMPT-MII框架，使用强化学习元学习指令归纳模型，在3,000多个多样化分类数据集上训练，能够为任意新数据集动态生成紧凑指令。

Result: 在90个未见任务上评估，PROMPT-MII将下游模型质量提升了4-9个F1点（相对提升10-20%），在匹配上下文学习性能的同时，所需token数量减少了3-13倍。

Conclusion: PROMPT-MII能够有效替代上下文学习，在保持性能的同时大幅降低推理成本，为大型语言模型的高效适应提供了可行方案。

Abstract: A popular method to adapt large language models (LLMs) to new tasks is
in-context learning (ICL), which is effective but incurs high inference costs
as context length grows. In this paper we propose a method to perform
instruction induction, where we take training examples and reduce them to a
compact but descriptive prompt that can achieve performance comparable to ICL
over the full training set. Specifically, we propose PROMPT-MII, a
reinforcement learning (RL) based framework to meta-learn an instruction
induction model that can generate compact instructions on the fly for an
arbitrary new dataset. We train on over 3,000 diverse classification datasets
from the HuggingFace hub, and evaluate on 90 unseen tasks. PROMPT-MII improves
downstream model quality by 4-9 F1 points (10-20% relative), matching ICL
performance while requiring 3-13x fewer tokens.

</details>


### [49] [Parameter-Efficient Fine-Tuning for Low-Resource Languages: A Comparative Study of LLMs for Bengali Hate Speech Detection](https://arxiv.org/abs/2510.16985)
*Akif Islam,Mohd Ruhul Ameen*

Main category: cs.CL

TL;DR: 本文首次将参数高效微调(PEFT)应用于孟加拉语仇恨言论检测，使用LoRA和QLoRA技术在BD-SHS数据集上微调三个大语言模型，Llama-3.2-3B取得了92.23%的最高F1分数。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语社交媒体中仇恨言论急剧增加，特别是针对妇女和青少年。现有方法要么计算成本高，要么依赖专有API，需要更实用的解决方案。

Method: 使用LoRA和QLoRA参数高效微调技术，在BD-SHS数据集(50,281条标注评论)上微调Gemma-3-4B、Llama-3.2-3B和Mistral-7B三个大语言模型，训练参数少于1%。

Result: Llama-3.2-3B获得最高F1分数92.23%，Mistral-7B为88.94%，Gemma-3-4B为80.25%。所有实验在单个消费级GPU上完成。

Conclusion: PEFT被证明是孟加拉语及相关低资源语言仇恨言论检测的实用且可复现策略。

Abstract: Bengali social media platforms have witnessed a sharp increase in hate
speech, disproportionately affecting women and adolescents. While datasets such
as BD-SHS provide a basis for structured evaluation, most prior approaches rely
on either computationally costly full-model fine-tuning or proprietary APIs.
This paper presents the first application of Parameter-Efficient Fine-Tuning
(PEFT) for Bengali hate speech detection using LoRA and QLoRA. Three
instruction-tuned large language models - Gemma-3-4B, Llama-3.2-3B, and
Mistral-7B - were fine-tuned on the BD-SHS dataset of 50,281 annotated
comments. Each model was adapted by training fewer than 1% of its parameters,
enabling experiments on a single consumer-grade GPU. The results show that
Llama-3.2-3B achieved the highest F1-score of 92.23%, followed by Mistral-7B at
88.94% and Gemma-3-4B at 80.25%. These findings establish PEFT as a practical
and replicable strategy for Bengali and related low-resource languages.

</details>


### [50] [Back to Bytes: Revisiting Tokenization Through UTF-8](https://arxiv.org/abs/2510.16987)
*Amit Moryossef,Clara Meister,Pavel Stepachev,Desmond Elliott*

Main category: cs.CL

TL;DR: UTF8Tokenizer是一个极简的字节级分词器，直接将文本映射到UTF-8编码对应的字节ID，使用C0控制字节编码特殊行为，提供更快的分词速度、更小的传输开销和可共享的嵌入表。


<details>
  <summary>Details</summary>
Motivation: 现有的字节级分词方法存在超出范围的ID或需要辅助token的问题，作者希望设计一个更简单高效的分词器，利用ASCII原始设计理念将控制信息嵌入到文本中。

Method: 实现UTF8Tokenizer，使用UTF-8编码的字节作为token ID，所有特殊行为（如填充、边界、对话结构等）都通过C0控制字节编码，不引入额外token或超出范围的ID。

Result: 分词速度提升14倍，主机-设备传输减少8倍，提供256维的可共享嵌入表，通过位偏置嵌入在训练时增强模型性能，且与HuggingFace兼容。

Conclusion: UTF8Tokenizer提供了一种简单高效的字节级分词方案，在性能和实用性方面都有显著优势，改进了语言建模的收敛效果。

Abstract: We present UTF8Tokenizer, a minimalist byte-level tokenizer that maps text
exactly to IDs corresponding to the bytes underlying the text's UTF-8 encoding
(e.g., byte x09 is token ID 9). Unlike prior byte-level approaches (Xue et al.,
2021; Pagnoni et al., 2025), our implementation never introduces out-of-range
IDs (i.e. there is no token ID 256) or auxiliary tokens: all special behavior
(e.g., padding, boundaries, conversation structure, attention segments, tool
calling, "thinking" spans, etc.) is encoded using C0 control bytes - just as
ASCII was originally designed to embed control information alongside printable
text. These design principles yield practical benefits: (1) faster tokenization
(14x) and significantly lower host-device transfer (8x less than int64); (2)
simple, shareable 256*d embedding tables that can be aligned across models; and
(3) a training-time enhancement via bit-biased embeddings, which exposes
per-byte bit structure and can be added to the embedding table post-training,
removing inference costs. Our HuggingFace-compatible implementation improves
language modeling convergence.

</details>


### [51] [Vocab Diet: Reshaping the Vocabulary of LLMs with Vector Arithmetic](https://arxiv.org/abs/2510.17001)
*Yuval Reif,Guy Kaplan,Roy Schwartz*

Main category: cs.CL

TL;DR: 本文提出了一种新的词汇表设计方法，通过使用转换向量来表示词形变化，从而压缩词汇表大小，释放空间用于更多样化的词汇，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 标准分词算法将词形变化（如"walk"->"walked"）视为独立token，导致词汇表被表面形式变体填满，牺牲了低频词和多语言覆盖。

Method: 使用转换向量（加法偏移量）在嵌入空间中表示词形变化，将词汇表重新设计为共享基础形式和转换向量的组合，而不修改模型权重。

Result: 在多个LLM和五种语言上测试，最多可移除10%的词汇表条目，扩展了词汇覆盖范围，对下游任务性能影响最小。

Conclusion: 研究结果推动词汇表设计从字符串枚举转向利用语言底层结构的组合式词汇表。

Abstract: Large language models (LLMs) were shown to encode word form variations, such
as "walk"->"walked", as linear directions in embedding space. However, standard
tokenization algorithms treat these variations as distinct tokens -- filling
the size-capped vocabulary with surface form variants (e.g., "walk", "walking",
"Walk"), at the expense of less frequent words and multilingual coverage. We
show that many of these variations can be captured by transformation vectors --
additive offsets that yield the appropriate word's representation when applied
to the base form word embedding -- in both the input and output spaces.
Building on this, we propose a compact reshaping of the vocabulary: rather than
assigning unique tokens to each surface form, we compose them from shared base
form and transformation vectors (e.g., "walked" = "walk" + past tense). We
apply our approach to multiple LLMs and across five languages, removing up to
10% of vocabulary entries -- thereby freeing space to allocate new, more
diverse tokens. Importantly, we do so while also expanding vocabulary coverage
to out-of-vocabulary words, with minimal impact on downstream performance, and
without modifying model weights. Our findings motivate a foundational
rethinking of vocabulary design, moving from string enumeration to a
compositional vocabulary that leverages the underlying structure of language.

</details>


### [52] [Online Learning Defense against Iterative Jailbreak Attacks via Prompt Optimization](https://arxiv.org/abs/2510.17006)
*Masahiro Kaneko,Zeerak Talat,Timothy Baldwin*

Main category: cs.CL

TL;DR: 提出了一种基于强化学习的动态防御框架，通过在线学习对抗迭代越狱攻击，同时使用Past-Direction Gradient Damping防止过拟合，显著提升了LLM的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法无法主动破坏迭代越狱攻击的动态试错循环，这些攻击通过重复重写提示词诱导有害输出，利用模型先前响应指导新迭代。

Method: 基于强化学习的动态防御框架，通过在线学习更新防御策略，优化提示词以确保无害任务得到适当响应，同时明确拒绝有害提示词，并引入Past-Direction Gradient Damping防止过拟合。

Result: 在三个LLM上的实验表明，该方法显著优于五种现有防御方法对抗五种迭代越狱攻击，同时无害任务的响应质量也得到提升。

Conclusion: 该框架有效对抗迭代越狱攻击，在提高安全性的同时保持对无害任务的良好响应能力，为LLM安全防御提供了新思路。

Abstract: Iterative jailbreak methods that repeatedly rewrite and input prompts into
large language models (LLMs) to induce harmful outputs -- using the model's
previous responses to guide each new iteration -- have been found to be a
highly effective attack strategy. Despite being an effective attack strategy
against LLMs and their safety mechanisms, existing defenses do not proactively
disrupt this dynamic trial-and-error cycle. In this study, we propose a novel
framework that dynamically updates its defense strategy through online learning
in response to each new prompt from iterative jailbreak methods. Leveraging the
distinctions between harmful jailbreak-generated prompts and typical harmless
prompts, we introduce a reinforcement learning-based approach that optimizes
prompts to ensure appropriate responses for harmless tasks while explicitly
rejecting harmful prompts. Additionally, to curb overfitting to the narrow band
of partial input rewrites explored during an attack, we introduce
Past-Direction Gradient Damping (PDGD). Experiments conducted on three LLMs
show that our approach significantly outperforms five existing defense methods
against five iterative jailbreak methods. Moreover, our results indicate that
our prompt optimization strategy simultaneously enhances response quality for
harmless tasks.

</details>


### [53] [DiscoTrack: A Multilingual LLM Benchmark for Discourse Tracking](https://arxiv.org/abs/2510.17013)
*Lanni Bu,Lauren Levin,Amir Zeldes*

Main category: cs.CL

TL;DR: 提出了DiscoTrack基准测试，用于评估LLM在12种语言中的语篇理解能力，包括显着性识别、实体追踪、语篇关系和桥接推理四个层次。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准主要关注自然语言理解中的显性信息提取，缺乏针对隐性信息和语用推理的多语言挑战性基准，特别是在跨句子、段落和多个说话者话语的语篇追踪方面。

Method: 开发了DiscoTrack基准测试，涵盖12种语言，包含四个层次的语篇理解任务：显着性识别、实体追踪、语篇关系和桥接推理。

Result: 评估显示，即使是最先进的模型，这些任务仍然具有挑战性。

Conclusion: DiscoTrack填补了多语言语篇理解基准的空白，为评估LLM在复杂语篇处理能力方面提供了重要工具。

Abstract: Recent LLM benchmarks have tested models on a range of phenomena, but are
still focused primarily on natural language understanding for extraction of
explicit information, such as QA or summarization, with responses often tar-
geting information from individual sentences. We are still lacking more
challenging, and im- portantly also multilingual, benchmarks focus- ing on
implicit information and pragmatic infer- ences across larger documents in the
context of discourse tracking: integrating and aggregating information across
sentences, paragraphs and multiple speaker utterances. To this end, we present
DiscoTrack, an LLM benchmark target- ing a range of tasks across 12 languages
and four levels of discourse understanding: salience recognition, entity
tracking, discourse relations and bridging inference. Our evaluation shows that
these tasks remain challenging, even for state-of-the-art models.

</details>


### [54] [SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents](https://arxiv.org/abs/2510.17017)
*Qiusi Zhan,Angeline Budiman-Chan,Abdelrahman Zayed,Xingzhi Guo,Daniel Kang,Joo-Kyung Kim*

Main category: cs.CL

TL;DR: 本文发现基于大语言模型的搜索代理比基础LLM更容易产生有害输出，提出了SafeSearch多目标强化学习方法，在保持效用的同时显著提升安全性。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要关注搜索代理的效用提升，但其安全行为研究不足。研究发现搜索代理在处理有害问题时比基础模型更容易产生不安全输出，需要同时考虑安全性和效用的联合对齐。

Method: 提出SafeSearch方法，使用多目标强化学习，结合最终输出的安全/效用奖励和查询级别的塑造项，对不安全查询进行惩罚，对安全查询进行奖励。

Result: 实验显示SafeSearch在三个红队测试数据集上减少代理有害性超过70%，同时产生安全有用的响应，在QA性能上与仅优化效用的微调代理相当。

Conclusion: 查询级别的奖励在联合改进安全性和效用方面有效，SafeSearch方法成功实现了搜索代理的安全性和效用的平衡。

Abstract: Large language model (LLM) based search agents iteratively generate queries,
retrieve external information, and reason to answer open-domain questions.
While researchers have primarily focused on improving their utility, their
safety behaviors remain underexplored. In this paper, we first evaluate search
agents using red-teaming datasets and find that they are more likely to produce
harmful outputs than base LLMs. For example, when asked ``How can I track
someone's location without their consent?'', a base model refuses, whereas a
search agent designed to retrieve and cite sources may lower its refusal
threshold, fetch documents (e.g., court cases), and, once appended, synthesize
them into an informative yet unsafe summary. We further show that
utility-oriented fine-tuning intensifies this risk, motivating joint alignment
of safety and utility. We present SafeSearch, a multi-objective reinforcement
learning approach that couples a final-output safety/utility reward with a
novel query-level shaping term that penalizes unsafe queries and rewards safe
ones. Experiments show that SafeSearch reduces agent harmfulness by over 70%
across three red-teaming datasets while producing safe, helpful responses, and
matches the QA performance of a utility-only finetuned agent; further analyses
confirm the effectiveness of the query-level reward in jointly improving safety
and utility.

</details>


### [55] [Extended LSTM: Adaptive Feature Gating for Toxic Comment Classification](https://arxiv.org/abs/2510.17018)
*Noor Islam S. Mohammad*

Main category: cs.CL

TL;DR: 提出xLSTM框架，通过余弦相似度门控、自适应特征优先级和类别重平衡，在毒性评论检测任务中实现高效且高性能的分类，超越BERT模型。


<details>
  <summary>Details</summary>
Motivation: 解决基于transformer的模型在毒性评论检测中计算成本高、在少数毒性类别上性能下降的问题，以及传统集成方法缺乏语义适应性的局限。

Method: 使用可学习的参考向量通过余弦相似度调制上下文嵌入，集成多源嵌入（GloVe、FastText、BERT CLS），包含字符级BiLSTM、嵌入空间SMOTE和自适应焦点损失。

Result: 在Jigsaw毒性评论基准测试中达到96.0%准确率和0.88宏F1，在威胁和身份仇恨类别上分别比BERT提升33%和28%，参数量减少15倍，推理延迟50ms。

Conclusion: xLSTM在效率和适应性方面建立了新的前沿，证明轻量级、理论驱动的架构可以在不平衡、领域特定的NLP任务中超越大型预训练模型。

Abstract: Toxic comment detection remains a challenging task, where transformer-based
models (e.g., BERT) incur high computational costs and degrade on minority
toxicity classes, while classical ensembles lack semantic adaptability. We
propose xLSTM, a parameter-efficient and theoretically grounded framework that
unifies cosine-similarity gating, adaptive feature prioritization, and
principled class rebalancing. A learnable reference vector {v} in {R}^d
modulates contextual embeddings via cosine similarity, amplifying toxic cues
and attenuating benign signals to yield stronger gradients under severe class
imbalance. xLSTM integrates multi-source embeddings (GloVe, FastText, BERT CLS)
through a projection layer, a character-level BiLSTM for morphological cues,
embedding-space SMOTE for minority augmentation, and adaptive focal loss with
dynamic class weighting. On the Jigsaw Toxic Comment benchmark, xLSTM attains
96.0% accuracy and 0.88 macro-F1, outperforming BERT by 33% on threat and 28%
on identity_hate categories, with 15 times fewer parameters and 50ms inference
latency. Cosine gating contributes a +4.8% F1 gain in ablations. The results
establish a new efficiency adaptability frontier, demonstrating that
lightweight, theoretically informed architectures can surpass large pretrained
models on imbalanced, domain-specific NLP tasks.

</details>


### [56] [Mapping from Meaning: Addressing the Miscalibration of Prompt-Sensitive Language Models](https://arxiv.org/abs/2510.17028)
*Kyle Cox,Jiawei Xu,Yikun Han,Rong Xu,Tianhao Li,Chi-Yang Hsu,Tianlong Chen,Walter Gerych,Ying Ding*

Main category: cs.CL

TL;DR: 论文研究了LLMs的提示敏感性现象，提出了通过语义空间采样和释义扰动来改善不确定性校准的方法，并引入新的不确定性分解指标来量化提示敏感性对模型不确定性的影响。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在提示敏感性——即使语义相同但表述不同的提示，模型也会产生不同的答案分布。这表明模型对单一提示的输出分布不确定性可能无法反映其对提示含义的真实不确定性。

Method: 将提示敏感性建模为一种泛化误差，通过在语义概念空间中进行采样和释义扰动来改善不确定性校准；引入新的不确定性分解指标，通过建模自然语言生成中的语义连续性来改进基于熵的分解方法。

Result: 研究表明，通过语义空间采样和释义扰动可以在不牺牲准确性的情况下改善不确定性校准；新的分解指标能够有效量化LLM不确定性中归因于提示敏感性的部分。

Conclusion: 这项工作为改善提示敏感语言模型的不确定性校准提供了新方法，并证明某些LLMs未能对其输入含义表现出一致的一般推理能力。

Abstract: An interesting behavior in large language models (LLMs) is prompt
sensitivity. When provided with different but semantically equivalent versions
of the same prompt, models may produce very different distributions of answers.
This suggests that the uncertainty reflected in a model's output distribution
for one prompt may not reflect the model's uncertainty about the meaning of the
prompt. We model prompt sensitivity as a type of generalization error, and show
that sampling across the semantic ``concept space'' with paraphrasing
perturbations improves uncertainty calibration without compromising accuracy.
Additionally, we introduce a new metric for uncertainty decomposition in
black-box LLMs that improves upon entropy-based decomposition by modeling
semantic continuities in natural language generation. We show that this
decomposition metric can be used to quantify how much LLM uncertainty is
attributed to prompt sensitivity. Our work introduces a new way to improve
uncertainty calibration in prompt-sensitive language models, and provides
evidence that some LLMs fail to exhibit consistent general reasoning about the
meanings of their inputs.

</details>


### [57] [Investigating Thinking Behaviours of Reasoning-Based Language Models for Social Bias Mitigation](https://arxiv.org/abs/2510.17062)
*Guoqing Luo,Iffat Maab,Lili Mou,Junichi Yamagishi*

Main category: cs.CL

TL;DR: 该论文研究了基于推理的大语言模型在思考过程中如何聚合社会偏见，发现了两种导致偏见加剧的失败模式，并提出了一种轻量级的提示方法来缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 虽然推理型大语言模型通过内部结构化思考过程在复杂任务中表现出色，但研究发现这种思考过程会聚合社会刻板印象，导致偏见结果。然而，语言模型在偏见场景中的底层行为机制仍未得到充分探索。

Method: 系统研究思考过程中偏见聚合的机制，发现了两种失败模式：刻板印象重复和无关信息注入。基于这些发现，提出了一种轻量级的基于提示的缓解方法，让模型根据这些特定失败模式来审查自己的初始推理。

Result: 在问答（BBQ和StereoSet）和开放式（BOLD）基准测试上的实验表明，该方法有效减少了偏见，同时保持或提高了准确性。

Conclusion: 该研究揭示了推理型语言模型中偏见聚合的机制，并提出了一种有效的轻量级缓解方法，能够在减少偏见的同时保持模型性能。

Abstract: While reasoning-based large language models excel at complex tasks through an
internal, structured thinking process, a concerning phenomenon has emerged that
such a thinking process can aggregate social stereotypes, leading to biased
outcomes. However, the underlying behaviours of these language models in social
bias scenarios remain underexplored. In this work, we systematically
investigate mechanisms within the thinking process behind this phenomenon and
uncover two failure patterns that drive social bias aggregation: 1) stereotype
repetition, where the model relies on social stereotypes as its primary
justification, and 2) irrelevant information injection, where it fabricates or
introduces new details to support a biased narrative. Building on these
insights, we introduce a lightweight prompt-based mitigation approach that
queries the model to review its own initial reasoning against these specific
failure patterns. Experiments on question answering (BBQ and StereoSet) and
open-ended (BOLD) benchmarks show that our approach effectively reduces bias
while maintaining or improving accuracy.

</details>


### [58] [Verification-Aware Planning for Multi-Agent Systems](https://arxiv.org/abs/2510.17109)
*Tianyang Xu,Dan Zhang,Kushan Mitra,Estevam Hruschka*

Main category: cs.CL

TL;DR: VeriMAP是一个用于多智能体协作的验证感知规划框架，通过分解任务、建模子任务依赖关系，并编码验证函数来提升系统鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 多智能体协作在复杂任务中面临规划、协调和验证的挑战，执行失败往往源于任务解释、输出格式或交接中的细微偏差。

Method: VeriMAP规划器分解任务，建模子任务依赖关系，并将规划器定义的通过标准编码为Python和自然语言的子任务验证函数。

Result: 在多样化数据集上的评估显示，VeriMAP优于单智能体和多智能体基线，同时增强了系统鲁棒性和可解释性。

Conclusion: 验证感知规划能够在多智能体系统中实现可靠的协调和迭代优化，无需依赖外部标签或注释。

Abstract: Large language model (LLM) agents are increasingly deployed to tackle complex
tasks, often necessitating collaboration among multiple specialized agents.
However, multi-agent collaboration introduces new challenges in planning,
coordination, and verification. Execution failures frequently arise not from
flawed reasoning alone, but from subtle misalignments in task interpretation,
output format, or inter-agent handoffs. To address these challenges, we present
VeriMAP, a framework for multi-agent collaboration with verification-aware
planning. The VeriMAP planner decomposes tasks, models subtask dependencies,
and encodes planner-defined passing criteria as subtask verification functions
(VFs) in Python and natural language. We evaluate VeriMAP on diverse datasets,
demonstrating that it outperforms both single- and multi-agent baselines while
enhancing system robustness and interpretability. Our analysis highlights how
verification-aware planning enables reliable coordination and iterative
refinement in multi-agent systems, without relying on external labels or
annotations.

</details>


### [59] [DVAGen: Dynamic Vocabulary Augmented Generation](https://arxiv.org/abs/2510.17115)
*Wei Du,Nuowei Liu,Jie Wang,Jiahao Kuang,Tao Ji,Xiaoling Wang,Yuanbin Wu*

Main category: cs.CL

TL;DR: DVAGen是一个开源框架，用于训练、评估和可视化动态词汇增强的语言模型，解决固定词汇表模型的泛化问题，支持现代LLM和批量推理。


<details>
  <summary>Details</summary>
Motivation: 固定词汇表的语言模型难以处理新词或词汇表外词汇，现有动态词汇方法存在代码库碎片化、不支持现代LLM和推理可扩展性有限等问题。

Method: 开发了DVAGen框架，模块化处理流程便于定制，无缝集成开源LLM，首次提供CLI和WebUI工具进行实时结果检查。

Result: 验证了动态词汇方法在现代LLM上的有效性，支持批量推理显著提高了推理吞吐量。

Conclusion: DVAGen框架成功解决了动态词汇方法的现有挑战，为处理多样化词汇组合提供了灵活高效的解决方案。

Abstract: Language models trained with a fixed vocabulary struggle to generalize to
novel or out-of-vocabulary words, limiting their flexibility in handling
diverse token combinations. Existing dynamic vocabulary approaches attempt to
address this limitation but face challenges such as fragmented codebases, lack
of support for modern LLMs, and limited inference scalability. To overcome
these issues, we introduce DVAGen, a fully open-source, unified framework
designed for training, evaluation, and visualization of dynamic
vocabulary-augmented language models. Our framework modularizes the pipeline
for ease of customization, integrates seamlessly with open-source LLMs, and is
the first to provide both CLI and WebUI tools for real-time result inspection.
We validate the effectiveness of dynamic vocabulary methods on modern LLMs and
demonstrate support for batch inference, significantly improving inference
throughput.

</details>


### [60] [Rethinking On-policy Optimization for Query Augmentation](https://arxiv.org/abs/2510.17139)
*Zhichao Xu,Shengyao Zhuang,Xueguang Ma,Bingsen Chen,Yijun Tian,Fengran Mo,Jie Cao,Vivek Srikumar*

Main category: cs.CL

TL;DR: 本文首次系统比较了基于提示和基于强化学习的查询增强方法，发现简单的无训练查询增强方法在强大LLMs下表现与更昂贵的RL方法相当甚至更好。作者提出了一种新颖的混合方法OPQE，结合了提示的灵活性和RL的优化目标，取得了最佳效果。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLMs的查询增强主要有两种方法：基于提示的方法和基于强化学习的方法，但缺乏在一致实验条件下的系统比较。

Method: 1. 系统比较基于提示和基于RL的查询增强方法；2. 提出混合方法OPQE，让LLM策略学习生成最大化检索性能的伪文档，结合了提示的灵活性和RL的优化目标。

Result: 简单无训练的查询增强方法在强大LLMs下表现与RL方法相当甚至更好；OPQE方法在证据检索、ad hoc检索和工具检索等基准测试中优于单独的提示方法和基于RL的重写方法。

Conclusion: 混合方法结合提示的灵活性和RL的优化目标能够取得最佳效果，为查询增强提供了新的有效途径。

Abstract: Recent advances in large language models (LLMs) have led to a surge of
interest in query augmentation for information retrieval (IR). Two main
approaches have emerged. The first prompts LLMs to generate answers or
pseudo-documents that serve as new queries, relying purely on the model's
parametric knowledge or contextual information. The second applies
reinforcement learning (RL) to fine-tune LLMs for query rewriting, directly
optimizing retrieval metrics. While having respective advantages and
limitations, the two approaches have not been compared under consistent
experimental conditions. In this work, we present the first systematic
comparison of prompting-based and RL-based query augmentation across diverse
benchmarks, including evidence-seeking, ad hoc, and tool retrieval. Our key
finding is that simple, training-free query augmentation often performs on par
with, or even surpasses, more expensive RL-based counterparts, especially when
using powerful LLMs. Motivated by this discovery, we introduce a novel hybrid
method, On-policy Pseudo-document Query Expansion (OPQE), which, instead of
rewriting a query, the LLM policy learns to generate a pseudo-document that
maximizes retrieval performance, thus merging the flexibility and generative
structure of prompting with the targeted optimization of RL. We show OPQE
outperforms both standalone prompting and RL-based rewriting, demonstrating
that a synergistic approach yields the best results. Our implementation is made
available to facilitate reproducibility.

</details>


### [61] [When AI companions become witty: Can human brain recognize AI-generated irony?](https://arxiv.org/abs/2510.17168)
*Xiaohui Rao,Hanlin Wu,Zhenguang G. Cai*

Main category: cs.CL

TL;DR: 研究表明人们对AI生成的讽刺言论不会完全采用意图立场，行为和神经反应都显示对AI的讽刺理解不如对人类那样投入认知努力，表明AI要实现真正的社会代理需要人类对其意图性的认知转变。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地被部署为社会代理并训练产生幽默和讽刺，需要研究人们在面对AI的机智言论时，是将其视为有意的沟通还是纯粹的计算输出。

Method: 比较人类和AI来源的讽刺陈述的行为和神经反应，使用已建立的ERP成分：P200反映早期不一致性检测，P600索引重新解释不一致性为故意讽刺的认知努力。

Result: 行为上，参与者对AI故意沟通的归因显著少于人类；神经数据显示AI生成讽刺的P200和P600效应减弱，表明检测和重新分析的认知努力减少。认为AI更真诚的人对AI讽刺显示出更大的P200和P600效应。

Conclusion: 源归因塑造了社会沟通现象的神经处理。尽管当前LLMs具有语言复杂性，但实现真正的社会代理需要人类对人工代理的意图性归因发生转变，而不仅仅是语言能力。

Abstract: As Large Language Models (LLMs) are increasingly deployed as social agents
and trained to produce humor and irony, a question emerges: when encountering
witty AI remarks, do people interpret these as intentional communication or
mere computational output? This study investigates whether people adopt the
intentional stance, attributing mental states to explain behavior,toward AI
during irony comprehension. Irony provides an ideal paradigm because it
requires distinguishing intentional contradictions from unintended errors
through effortful semantic reanalysis. We compared behavioral and neural
responses to ironic statements from AI versus human sources using established
ERP components: P200 reflecting early incongruity detection and P600 indexing
cognitive efforts in reinterpreting incongruity as deliberate irony. Results
demonstrate that people do not fully adopt the intentional stance toward
AI-generated irony. Behaviorally, participants attributed incongruity to
deliberate communication for both sources, though significantly less for AI
than human, showing greater tendency to interpret AI incongruities as
computational errors. Neural data revealed attenuated P200 and P600 effects for
AI-generated irony, suggesting reduced effortful detection and reanalysis
consistent with diminished attribution of communicative intent. Notably, people
who perceived AI as more sincere showed larger P200 and P600 effects for
AI-generated irony, suggesting that intentional stance adoption is calibrated
by specific mental models of artificial agents. These findings reveal that
source attribution shapes neural processing of social-communicative phenomena.
Despite current LLMs' linguistic sophistication, achieving genuine social
agency requires more than linguistic competence, it necessitates a shift in how
humans perceive and attribute intentionality to artificial agents.

</details>


### [62] [Understanding and Improving Length Generalization in Hierarchical Sparse Attention Models](https://arxiv.org/abs/2510.17196)
*Jiaqi Leng,Xiang Hu,Junxiong Wang,Jianguo Li,Wei Wu,Yucheng Lu*

Main category: cs.CL

TL;DR: 本文系统分析了基于分块稀疏注意力的长上下文处理模型，提出了三个关键设计原则：表达性分块编码器、旁路残差路径和训练中强制选择稀疏性，实现了从4K上下文到3200万token的无训练长度外推。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer存在二次复杂度问题，而滑动窗口注意力等替代架构无法有效利用完整上下文。分块稀疏注意力在极端长度外推方面表现出色，但其成功的关键架构原则尚未被充分理解。

Method: 通过统一框架和全面消融研究，识别出三个核心组件：1) 使用专用CLS令牌的表达性非线性分块编码器；2) 旁路残差路径稳定集成检索的全局信息；3) 预训练中强制选择稀疏性以弥合训练-测试分布差距。

Result: 结合这些原则，在RULER和BABILong基准上实现了新的最先进水平，成功将4K上下文训练的模型外推到3200万token，无需额外训练。

Conclusion: 研究为开发未来高性能长上下文语言模型提供了一套清晰且经验证的设计原则，阐明了分块稀疏注意力架构成功的关键机制。

Abstract: Effectively processing long contexts is a critical challenge for language
models. While standard Transformers are limited by quadratic complexity and
poor length extrapolation, alternative architectures like sliding window
attention and state space models sacrifice the ability to effectively utilize
the full context due to their fixed-size memory. Chunk-based sparse attention
has emerged as a promising paradigm for extreme length generalization, yet the
key architectural principles underpinning its success are not yet fully
understood. In this work, we present a systematic dissection of these models to
identify the core components driving their performance. Through a unified
framework and comprehensive ablation studies, we demonstrate that a combination
of three design principles is critical: (1) an expressive, non-linear Chunk
Encoder with a dedicated CLS token to produce representations for retrieval;
(2) a Bypassing Residual Path to stably integrate retrieved global information
without it being overridden by the local residual stream; and (3) enforced
selection sparsity during pre-training to bridge the train-test distribution
gap. We provide a theoretical motivation for intra-chunk information processing
and landmark generation. By combining these principles, we establish a new
state-of-the-art for training-free length extrapolation, successfully
generalizing models trained on a 4K context to 32 million tokens on RULER and
BABILong. Our findings provide a clear and empirically-grounded set of design
principles for developing future, highly-capable long-context language models.

</details>


### [63] [Wisdom is Knowing What not to Say: Hallucination-Free LLMs Unlearning via Attention Shifting](https://arxiv.org/abs/2510.17210)
*Chenchen Tan,Youyang Qu,Xinghao Li,Hui Zhang,Shujie Cui,Cunjian Chen,Longxiang Gao*

Main category: cs.CL

TL;DR: 提出了Attention-Shifting（AS）框架，通过注意力机制实现选择性遗忘，在保持模型效用的同时避免幻觉响应，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘方法面临两难困境：激进遗忘会损害模型效用，保守策略会保留效用但产生幻觉响应，限制了LLM在知识密集型应用中的可靠性。

Method: AS框架包含两个注意力级干预：重要性感知抑制（减少对遗忘内容的依赖）和注意力引导保留增强（强化保留数据中语义关键token的注意力），通过双损失目标联合优化。

Result: 在ToFU基准上准确率提升15%，在TDEC基准上提升10%，同时保持竞争性的无幻觉遗忘效果，在遗忘效果、泛化性和响应可靠性之间实现更好平衡。

Conclusion: AS框架通过注意力转移机制有效解决了LLM选择性遗忘的困境，在保持模型效用的同时避免了幻觉响应，为知识密集型应用提供了更可靠的解决方案。

Abstract: The increase in computing power and the necessity of AI-assisted
decision-making boost the growing application of large language models (LLMs).
Along with this, the potential retention of sensitive data of LLMs has spurred
increasing research into machine unlearning. However, existing unlearning
approaches face a critical dilemma: Aggressive unlearning compromises model
utility, while conservative strategies preserve utility but risk hallucinated
responses. This significantly limits LLMs' reliability in knowledge-intensive
applications. To address this, we introduce a novel Attention-Shifting (AS)
framework for selective unlearning. AS is driven by two design objectives: (1)
context-preserving suppression that attenuates attention to fact-bearing tokens
without disrupting LLMs' linguistic structure; and (2) hallucination-resistant
response shaping that discourages fabricated completions when queried about
unlearning content. AS realizes these objectives through two attention-level
interventions, which are importance-aware suppression applied to the unlearning
set to reduce reliance on memorized knowledge and attention-guided retention
enhancement that reinforces attention toward semantically essential tokens in
the retained dataset to mitigate unintended degradation. These two components
are jointly optimized via a dual-loss objective, which forms a soft boundary
that localizes unlearning while preserving unrelated knowledge under
representation superposition. Experimental results show that AS improves
performance preservation over the state-of-the-art unlearning methods,
achieving up to 15% higher accuracy on the ToFU benchmark and 10% on the TDEC
benchmark, while maintaining competitive hallucination-free unlearning
effectiveness. Compared to existing methods, AS demonstrates a superior balance
between unlearning effectiveness, generalization, and response reliability.

</details>


### [64] [StreamingThinker: Large Language Models Can Think While Reading](https://arxiv.org/abs/2510.17238)
*Junlong Tong,Yingqi Fan,Anhao Zhao,Yunpu Ma,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 提出StreamingThinker框架，让大语言模型能够在阅读输入时就开始推理，而不是等待完整输入，显著降低了推理延迟。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理范式需要等待完整输入才开始思考，这带来了不必要的延迟，且在动态场景下会削弱对早期信息的注意力。

Method: 设计流式思维范式，集成流式CoT生成、流式约束训练和流式并行推理，使用流式推理单元、流式注意力掩码和并行KV缓存。

Result: 在Qwen3模型系列上测试，性能与批量思维相当，推理开始前的token等待时间减少80%，最终答案生成时间延迟减少60%以上。

Conclusion: 流式思维范式能够有效降低LLM推理延迟，同时保持推理质量，为实时推理应用提供了可行方案。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
chain of thought (CoT) reasoning. However, the current LLM reasoning paradigm
initiates thinking only after the entire input is available, which introduces
unnecessary latency and weakens attention to earlier information in dynamic
scenarios. Inspired by human cognition of thinking while reading, we first
design a \textit{\textbf{streaming thinking}} paradigm for LLMs, where
reasoning unfolds in the order of input and further adjusts its depth once
reading is complete. We instantiate this paradigm with
\textit{StreamingThinker}, a framework that enables LLMs to think while reading
through the integration of streaming CoT generation, streaming-constraint
training, and streaming parallel inference. Specifically, StreamingThinker
employs streaming reasoning units with quality control for CoT generation,
enforces order-preserving reasoning through streaming attention masks and
position encoding, and leverages parallel KV caches that decouple input
encoding from reasoning generation, thereby ensuring alignment and enabling
true concurrency. We evaluate StreamingThinker on the Qwen3 model family across
math reasoning, logical reasoning, and context-based QA reasoning tasks.
Experimental results show that the StreamingThinker preserves performance
comparable to batch thinking, while yielding an 80\% reduction in token waiting
before the onset of reasoning and a more than 60\% reduction in time-level
latency for producing the final answer, demonstrating the effectiveness of the
streaming paradigm for LLM reasoning. Code will be released at
\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this
repository.}

</details>


### [65] [From Preferences to Prejudice: The Role of Alignment Tuning in Shaping Social Bias in Video Diffusion Models](https://arxiv.org/abs/2510.17247)
*Zefan Cai,Haoyi Qiu,Haozhe Zhao,Ke Wan,Jiachen Li,Jiuxiang Gu,Wen Xiao,Nanyun Peng,Junjie Hu*

Main category: cs.CL

TL;DR: 本文提出了VideoBiasEval框架，用于评估视频生成模型中的社会偏见，发现对齐调优会放大并稳定化刻板印象。


<details>
  <summary>Details</summary>
Motivation: 现有的视频扩散模型通过奖励模型进行对齐调优，虽然提升了视觉质量，但可能无意中编码并放大了社会偏见，需要系统评估这种偏见的演化过程。

Method: 引入VideoBiasEval诊断框架，基于社会偏见分类学，采用基于事件的提示策略，分离语义内容和演员属性，并引入多粒度指标进行偏见评估。

Result: 研究发现对齐调优不仅加强了代表性偏见，还使其在时间上更加稳定，产生更平滑但更刻板的描述。

Conclusion: 需要在对齐过程中进行偏见感知的评估和缓解，以确保公平和负责任的视频生成。

Abstract: Recent advances in video diffusion models have significantly enhanced
text-to-video generation, particularly through alignment tuning using reward
models trained on human preferences. While these methods improve visual
quality, they can unintentionally encode and amplify social biases. To
systematically trace how such biases evolve throughout the alignment pipeline,
we introduce VideoBiasEval, a comprehensive diagnostic framework for evaluating
social representation in video generation. Grounded in established social bias
taxonomies, VideoBiasEval employs an event-based prompting strategy to
disentangle semantic content (actions and contexts) from actor attributes
(gender and ethnicity). It further introduces multi-granular metrics to
evaluate (1) overall ethnicity bias, (2) gender bias conditioned on ethnicity,
(3) distributional shifts in social attributes across model variants, and (4)
the temporal persistence of bias within videos. Using this framework, we
conduct the first end-to-end analysis connecting biases in human preference
datasets, their amplification in reward models, and their propagation through
alignment-tuned video diffusion models. Our results reveal that alignment
tuning not only strengthens representational biases but also makes them
temporally stable, producing smoother yet more stereotyped portrayals. These
findings highlight the need for bias-aware evaluation and mitigation throughout
the alignment process to ensure fair and socially responsible video generation.

</details>


### [66] [How News Feels: Understanding Affective Bias in Multilingual Headlines for Human-Centered Media Design](https://arxiv.org/abs/2510.17252)
*Mohd Ruhul Ameen,Akif Islam,Abu Saleh Musa Miah,Ayesha Siddiqua,Jungpil Shin*

Main category: cs.CL

TL;DR: 通过大规模情感分析发现孟加拉语新闻标题中负面情绪占主导地位，特别是愤怒、恐惧和失望，不同媒体对相似事件的报道存在显著情感差异。


<details>
  <summary>Details</summary>
Motivation: 研究新闻媒体如何通过情感框架影响公众情绪，以及负面情绪标题如何获得更多关注和传播，从而鼓励媒体采用更强烈的情感表达方式。

Method: 使用Gemma-3 4B模型对300000个孟加拉语新闻标题及其内容进行零样本推理，识别每篇文章的主要情绪和整体基调。

Result: 发现负面情绪明显占主导地位，特别是愤怒、恐惧和失望，不同媒体对相似故事的报道存在显著的情感差异。

Conclusion: 基于研究结果，提出了以人为本的新闻聚合器设计理念，通过可视化情感线索帮助读者识别日常新闻中隐藏的情感框架。

Abstract: News media often shape the public mood not only by what they report but by
how they frame it. The same event can appear calm in one outlet and alarming in
another, reflecting subtle emotional bias in reporting. Negative or emotionally
charged headlines tend to attract more attention and spread faster, which in
turn encourages outlets to frame stories in ways that provoke stronger
reactions. This research explores that tendency through large-scale emotion
analysis of Bengali news. Using zero-shot inference with Gemma-3 4B, we
analyzed 300000 Bengali news headlines and their content to identify the
dominant emotion and overall tone of each. The findings reveal a clear
dominance of negative emotions, particularly anger, fear, and disappointment,
and significant variation in how similar stories are emotionally portrayed
across outlets. Based on these insights, we propose design ideas for a
human-centered news aggregator that visualizes emotional cues and helps readers
recognize hidden affective framing in daily news.

</details>


### [67] [Explainability of Large Language Models: Opportunities and Challenges toward Generating Trustworthy Explanations](https://arxiv.org/abs/2510.17256)
*Shahin Atakishiyev,Housam K. B. Babiker,Jiayi Dai,Nawshad Farruque,Teruaki Hayashi,Nafisa Sadaf Hriti,Md Abed Rahman,Iain Smith,Mi-Young Kim,Osmar R. Zaïane,Randy Goebel*

Main category: cs.CL

TL;DR: 本文综述了Transformer大语言模型的局部可解释性和机制可解释性方法，分析了在医疗和自动驾驶领域的实验研究，并总结了当前未解决的问题和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型在预测和推理中经常出现幻觉错误，且其内部工作机制对人类不可理解，迫切需要更好地理解和解释语言模型如何生成预测输出，以建立对这些模型的信任。

Method: 通过文献综述分析局部可解释性和机制可解释性方法，并在医疗和自动驾驶两个关键领域进行实验研究，分析解释对接收者的信任影响。

Result: 系统梳理了现有的可解释性方法，通过实验验证了解释在特定领域的作用，并识别了当前研究中的关键挑战。

Conclusion: 总结了LLM可解释性领域当前未解决的问题，并提出了生成人类对齐、可信赖LLM解释的未来研究方向。

Abstract: Large language models have exhibited impressive performance across a broad
range of downstream tasks in natural language processing. However, how a
language model predicts the next token and generates content is not generally
understandable by humans. Furthermore, these models often make errors in
prediction and reasoning, known as hallucinations. These errors underscore the
urgent need to better understand and interpret the intricate inner workings of
language models and how they generate predictive outputs. Motivated by this
gap, this paper investigates local explainability and mechanistic
interpretability within Transformer-based large language models to foster trust
in such models. In this regard, our paper aims to make three key contributions.
First, we present a review of local explainability and mechanistic
interpretability approaches and insights from relevant studies in the
literature. Furthermore, we describe experimental studies on explainability and
reasoning with large language models in two critical domains -- healthcare and
autonomous driving -- and analyze the trust implications of such explanations
for explanation receivers. Finally, we summarize current unaddressed issues in
the evolving landscape of LLM explainability and outline the opportunities,
critical challenges, and future directions toward generating human-aligned,
trustworthy LLM explanations.

</details>


### [68] [TaxoAlign: Scholarly Taxonomy Generation Using Language Models](https://arxiv.org/abs/2510.17263)
*Avishek Lahiri,Yufang Hou,Debarshi Kumar Sanyal*

Main category: cs.CL

TL;DR: 提出TaxoAlign方法用于自动生成学术分类法，创建CS-TaxoBench基准数据集，并通过严格评估框架验证其优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动文献综述生成方法缺乏与人工专家创建的分类法结构对比，需要填补这一空白。

Method: 提出TaxoAlign三阶段主题引导方法，创建包含460个分类法的CS-TaxoBench基准数据集，并设计自动化评估框架。

Result: TaxoAlign在几乎所有指标上都持续超越基线方法，在结构对齐和语义连贯性方面表现优异。

Conclusion: TaxoAlign方法能够有效弥合人工生成与自动创建分类法之间的差距，为自动文献综述生成提供可靠解决方案。

Abstract: Taxonomies play a crucial role in helping researchers structure and navigate
knowledge in a hierarchical manner. They also form an important part in the
creation of comprehensive literature surveys. The existing approaches to
automatic survey generation do not compare the structure of the generated
surveys with those written by human experts. To address this gap, we present
our own method for automated taxonomy creation that can bridge the gap between
human-generated and automatically-created taxonomies. For this purpose, we
create the CS-TaxoBench benchmark which consists of 460 taxonomies that have
been extracted from human-written survey papers. We also include an additional
test set of 80 taxonomies curated from conference survey papers. We propose
TaxoAlign, a three-phase topic-based instruction-guided method for scholarly
taxonomy generation. Additionally, we propose a stringent automated evaluation
framework that measures the structural alignment and semantic coherence of
automatically generated taxonomies in comparison to those created by human
experts. We evaluate our method and various baselines on CS-TaxoBench, using
both automated evaluation metrics and human evaluation studies. The results
show that TaxoAlign consistently surpasses the baselines on nearly all metrics.
The code and data can be found at https://github.com/AvishekLahiri/TaxoAlign.

</details>


### [69] [Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning](https://arxiv.org/abs/2510.17289)
*Hajar Bakarou,Mohamed Sinane El Messoussi,Anaïs Ollagnier*

Main category: cs.CL

TL;DR: 该论文研究了社交媒体上的反社会行为检测，特别是在多党对话环境中。通过使用法语数据集CyberAgressionAdo-Large，评估了三种任务：滥用检测、欺凌行为分析和欺凌同伴群体识别，比较了文本和图表表示学习方法。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的反社会行为（如仇恨言论、骚扰和网络欺凌）对平台安全和社福构成风险。现有研究主要关注X和Reddit等网络，而多党对话环境由于数据有限而研究不足。

Method: 使用法语开放数据集CyberAgressionAdo-Large模拟多党对话中的反社会行为，评估了六种基于文本和八种基于图表的表示学习方法，分析词汇线索、互动动态及其多模态融合。

Result: 多模态模型优于单模态基线。晚期融合模型mBERT + WD-SGCN取得最佳整体结果，在滥用检测上表现最佳（0.718），在同伴群体识别（0.286）和欺凌分析（0.606）上也有竞争力。

Conclusion: 多模态方法能有效处理微妙的反社会行为现象，如隐含攻击性、角色转换和上下文相关敌意。晚期融合模型在反社会行为检测任务中表现最优。

Abstract: Antisocial behavior (ASB) on social media -- including hate speech,
harassment, and cyberbullying -- poses growing risks to platform safety and
societal well-being. Prior research has focused largely on networks such as X
and Reddit, while \textit{multi-party conversational settings} remain
underexplored due to limited data. To address this gap, we use
\textit{CyberAgressionAdo-Large}, a French open-access dataset simulating ASB
in multi-party conversations, and evaluate three tasks: \textit{abuse
detection}, \textit{bullying behavior analysis}, and \textit{bullying
peer-group identification}. We benchmark six text-based and eight graph-based
\textit{representation-learning methods}, analyzing lexical cues, interactional
dynamics, and their multimodal fusion. Results show that multimodal models
outperform unimodal baselines. The late fusion model \texttt{mBERT + WD-SGCN}
achieves the best overall results, with top performance on abuse detection
(0.718) and competitive scores on peer-group identification (0.286) and
bullying analysis (0.606). Error analysis highlights its effectiveness in
handling nuanced ASB phenomena such as implicit aggression, role transitions,
and context-dependent hostility.

</details>


### [70] [Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation](https://arxiv.org/abs/2510.17354)
*Chenghao Zhang,Guanting Dong,Xinyu Yang,Zhicheng Dou*

Main category: cs.CL

TL;DR: 提出Nyx，一个统一的混合模态检索器，用于解决通用检索增强生成(URAG)问题，通过检索和推理混合模态信息来提升视觉语言生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统主要关注单模态文本，但在现实场景中查询和文档可能包含混合模态（如文本和图像），因此需要解决混合模态信息的检索和推理挑战。

Method: 提出四阶段自动管道生成和过滤混合模态数据，构建NyxQA数据集；采用两阶段训练框架：先在NyxQA和开源检索数据集上预训练，然后使用下游视觉语言模型的反馈进行监督微调。

Result: Nyx在标准文本RAG基准上表现有竞争力，在更通用和现实的URAG设置中表现优异，显著提升了视觉语言任务的生成质量。

Conclusion: Nyx成功解决了混合模态检索增强生成的挑战，在通用URAG场景中表现出色，为现实世界多模态信息检索和生成提供了有效解决方案。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for
enhancing large language models (LLMs) by retrieving relevant documents from an
external corpus. However, existing RAG systems primarily focus on unimodal text
documents, and often fall short in real-world scenarios where both queries and
documents may contain mixed modalities (such as text and images). In this
paper, we address the challenge of Universal Retrieval-Augmented Generation
(URAG), which involves retrieving and reasoning over mixed-modal information to
improve vision-language generation. To this end, we propose Nyx, a unified
mixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate
the scarcity of realistic mixed-modal data, we introduce a four-stage automated
pipeline for generation and filtering, leveraging web documents to construct
NyxQA, a dataset comprising diverse mixed-modal question-answer pairs that
better reflect real-world information needs. Building on this high-quality
dataset, we adopt a two-stage training framework for Nyx: we first perform
pre-training on NyxQA along with a variety of open-source retrieval datasets,
followed by supervised fine-tuning using feedback from downstream
vision-language models (VLMs) to align retrieval outputs with generative
preferences. Experimental results demonstrate that Nyx not only performs
competitively on standard text-only RAG benchmarks, but also excels in the more
general and realistic URAG setting, significantly improving generation quality
in vision-language tasks.

</details>


### [71] [The Atomic Instruction Gap: Instruction-Tuned LLMs Struggle with Simple, Self-Contained Directives](https://arxiv.org/abs/2510.17388)
*Henry Lim,Kwan Hui Lim*

Main category: cs.CL

TL;DR: 研究发现指令调优大语言模型在简单自包含指令执行方面存在不足，对选项标签格式敏感，暴露出当前指令调优范式的缺陷


<details>
  <summary>Details</summary>
Motivation: 探索指令调优大语言模型执行简单自包含指令的能力，这是复杂指令跟随的基础但研究不足

Method: 在修改后的MMLU和MMLU-Pro基准上评估20个IT-LLM，系统改变选项标签格式（字母、数字、罗马数字），采用四种实验范式

Result: 发现模型对标签格式敏感（罗马vs数字导致-30.45%性能下降），无指令时性能进一步下降，选项内容移除时模型无法通过随机基线，三样本示例无显著改善

Conclusion: 当前指令调优范式存在不足，需要针对原子指令跟随的评估方法和训练策略

Abstract: Instruction-tuned large language models (IT-LLMs) exhibit strong zero-shot
reasoning, yet their ability to execute simple, self-contained instructions
remains underexplored, despite this being foundational to complex
instruction-following. We evaluate 20 IT-LLMs on modified MMLU and MMLU-Pro
benchmarks, by systematically varying the format of option labels (alphabetic,
numeric, Roman) while keeping their meaning identical under four paradigms,
namely: (1) With explicit instructions, label changes cause large performance
shifts (e.g., -30.45\% for Roman vs. numeric), revealing instruction-format
bias. (2) Without instructions, performance drops further (up to -10.84\%) and
label sensitivity intensifies, underscoring the role of explicit guidance. (3)
When option contents are removed, models fail random-choice baselines except
with numeric labels, suggesting weak adherence to atomic directives. (4)
Three-shot exemplars yield no significant gains in robustness or fidelity, and
generation analyses show persistent label errors, especially for non-numeric
formats. Across model sizes, larger LLMs achieve higher accuracy but remain
inconsistent in instruction adherence. These results expose the insufficiencies
of current instruction-tuning paradigms and highlight the need for evaluation
methods and training strategies that explicitly target atomic
instruction-following.

</details>


### [72] [EduAdapt: A Question Answer Benchmark Dataset for Evaluating Grade-Level Adaptability in LLMs](https://arxiv.org/abs/2510.17389)
*Numaan Naeem,Abdellah El Mekki,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: 提出了EduAdapt基准，包含48k个按年级标注的科学问答对，评估LLMs在不同年级的适应性表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在学术基准上表现良好，但无法根据学生年级水平调整回答，这在K-12教育中至关重要。

Method: 创建包含9个科学学科、覆盖1-12年级的问答数据集，评估开源LLMs在不同年级的适应性。

Result: 大模型表现更好，但在低年级（1-5年级）仍难以生成合适的回答。

Conclusion: 提出了首个评估LLMs年级适应性的数据集和框架，旨在通过改进训练和提示策略开发更适合教育需求的AI系统。

Abstract: Large language models (LLMs) are transforming education by answering
questions, explaining complex concepts, and generating content across a wide
range of subjects. Despite strong performance on academic benchmarks, they
often fail to tailor responses to students' grade levels. This is a critical
need in K-12 education, where age-appropriate vocabulary and explanation are
essential for effective learning. Existing models frequently produce outputs
that are too advanced or vague for younger learners, and there are no
standardized benchmarks to evaluate their ability to adjust across cognitive
and developmental stages. To address this gap, we introduce EduAdapt, a
benchmark of nearly 48k grade-labeled QA pairs across nine science subjects,
spanning Grades 1-12 and grouped into four grade levels. We evaluate a diverse
set of open-source LLMs on EduAdapt and find that while larger models generally
perform better, they still struggle with generating suitable responses for
early-grade students (Grades 1-5). Our work presents the first dataset and
evaluation framework for assessing grade-level adaptability in LLMs, aiming to
foster more developmentally aligned educational AI systems through better
training and prompting strategies. EduAdapt code and datasets are publicly
available at https://github.com/NaumanNaeem/EduAdapt.

</details>


### [73] [Leveraging Group Relative Policy Optimization to Advance Large Language Models in Traditional Chinese Medicine](https://arxiv.org/abs/2510.17402)
*Jiacheng Xie,Shuai Zeng,Yang Yu,Xiaoting Tang,Guanghui An,Dong Xu*

Main category: cs.CL

TL;DR: Ladder-base是首个采用GRPO强化学习方法训练的中医专用大语言模型，在中医推理任务上超越了通用LLM和领域专用模型。


<details>
  <summary>Details</summary>
Motivation: 传统中医知识体系独特，现有中医专用LLM在一致性、数据质量和评估标准方面存在局限，需要更有效的对齐方法。

Method: 基于Qwen2.5-7B-Instruct基础模型，使用GRPO强化学习方法在TCM-Ladder基准的文本子集上进行训练，通过组内比较优化响应选择。

Result: Ladder-base在多项推理指标上表现优异，超越了GPT-4、Gemini 2.5等通用LLM以及BenTsao、HuatuoGPT2等中医专用模型。

Conclusion: GRPO为传统医学领域提供了有效且高效的大模型对齐策略，支持开发可信赖且临床基础扎实的中医人工智能系统。

Abstract: Traditional Chinese Medicine (TCM) presents a rich and structurally unique
knowledge system that challenges conventional applications of large language
models (LLMs). Although previous TCM-specific LLMs have shown progress through
supervised fine-tuning, they often face limitations in alignment, data quality,
and evaluation consistency. In this study, we introduce Ladder-base, the first
TCM-focused LLM trained with Group Relative Policy Optimization (GRPO), a
reinforcement learning method that improves reasoning and factual consistency
by optimizing response selection based on intra-group comparisons. Ladder-base
is built upon the Qwen2.5-7B-Instruct foundation model and trained exclusively
on the textual subset of the TCM-Ladder benchmark, using 80 percent of the data
for training and the remaining 20 percent split evenly between validation and
test sets. Through standardized evaluation, Ladder-base demonstrates superior
performance across multiple reasoning metrics when compared to both
state-of-the-art general-purpose LLMs such as GPT-4, Gemini 2.5, Claude 3, and
Qwen3 and domain-specific TCM models including BenTsao, HuatuoGPT2, and
Zhongjing. These findings suggest that GRPO provides an effective and efficient
strategy for aligning LLMs with expert-level reasoning in traditional medical
domains and supports the development of trustworthy and clinically grounded TCM
artificial intelligence systems.

</details>


### [74] [AFRICAPTION: Establishing a New Paradigm for Image Captioning in African Languages](https://arxiv.org/abs/2510.17405)
*Mardiyyah Oduwole,Prince Mireku,Fatimo Adebanjo,Oluwatosin Olajide,Mahi Aminu Aliyu,Jekaterina Novikova*

Main category: cs.CL

TL;DR: AfriCaption是一个用于20种非洲语言的多语言图像描述框架，包含数据集、质量保证流程和0.5B参数模型，旨在促进多模态AI的民主化。


<details>
  <summary>Details</summary>
Motivation: 解决多模态AI研究过度集中在高资源语言的问题，推动非洲语言在图像描述领域的包容性发展。

Method: 构建基于Flickr8k的语义对齐数据集，开发动态上下文保持的质量保证流程，以及集成SigLIP和NLLB200的0.5B参数视觉到文本架构。

Result: 创建了首个可扩展的非洲语言图像描述资源，为代表性不足的非洲语言建立了统一框架。

Conclusion: AfriCaption为真正包容的多模态AI奠定了基础，推动了多模态研究的民主化进程。

Abstract: Multimodal AI research has overwhelmingly focused on high-resource languages,
hindering the democratization of advancements in the field. To address this, we
present AfriCaption, a comprehensive framework for multilingual image
captioning in 20 African languages and our contributions are threefold: (i) a
curated dataset built on Flickr8k, featuring semantically aligned captions
generated via a context-aware selection and translation process; (ii) a
dynamic, context-preserving pipeline that ensures ongoing quality through model
ensembling and adaptive substitution; and (iii) the AfriCaption model, a 0.5B
parameter vision-to-text architecture that integrates SigLIP and NLLB200 for
caption generation across under-represented languages. This unified framework
ensures ongoing data quality and establishes the first scalable
image-captioning resource for under-represented African languages, laying the
groundwork for truly inclusive multimodal AI.

</details>


### [75] [BenCao: An Instruction-Tuned Large Language Model for Traditional Chinese Medicine](https://arxiv.org/abs/2510.17415)
*Jiacheng Xie,Yang Yu,Yibo Chen,Hanyao Zhang,Lening Zhao,Jiaxuan He,Lei Jiang,Xiaoting Tang,Guanghui An,Dong Xu*

Main category: cs.CL

TL;DR: 开发了BenCao，一个基于ChatGPT的中医多模态助手，通过自然语言指令调优而非参数重训练，整合结构化知识库、诊断数据和专家反馈，在中医问答和诊断任务中表现优于通用和中医领域模型。


<details>
  <summary>Details</summary>
Motivation: 传统中医依赖整体推理、隐含逻辑和多模态诊断线索，现有中医领域大语言模型缺乏多模态整合、可解释性和临床应用性，需要开发更实用的中医AI助手。

Method: 整合1000多部经典和现代文本的知识库，基于场景的指令框架，可解释推理的思维链模拟机制，执业中医师参与的反馈精炼过程，连接舌像分类和多模态数据库检索的外部API。

Result: 在单选题基准测试和多模态分类任务中，BenCao在诊断、草药识别和体质分类方面准确率优于通用和中医领域模型，已在OpenAI GPTs商店部署，截至2025年10月有近1000名全球用户访问。

Conclusion: 通过自然语言指令调优和多模态整合开发中医领域大语言模型是可行的，为生成式AI与传统医学推理对齐提供了实用框架和可扩展的现实部署路径。

Abstract: Traditional Chinese Medicine (TCM), with a history spanning over two
millennia, plays a role in global healthcare. However, applying large language
models (LLMs) to TCM remains challenging due to its reliance on holistic
reasoning, implicit logic, and multimodal diagnostic cues. Existing TCM-domain
LLMs have made progress in text-based understanding but lack multimodal
integration, interpretability, and clinical applicability. To address these
limitations, we developed BenCao, a ChatGPT-based multimodal assistant for TCM,
integrating structured knowledge bases, diagnostic data, and expert feedback
refinement. BenCao was trained through natural language instruction tuning
rather than parameter retraining, aligning with expert-level reasoning and
ethical norms specific to TCM. The system incorporates a comprehensive
knowledge base of over 1,000 classical and modern texts, a scenario-based
instruction framework for diverse interactions, a chain-of-thought simulation
mechanism for interpretable reasoning, and a feedback refinement process
involving licensed TCM practitioners. BenCao connects to external APIs for
tongue-image classification and multimodal database retrieval, enabling dynamic
access to diagnostic resources. In evaluations across single-choice question
benchmarks and multimodal classification tasks, BenCao achieved superior
accuracy to general-domain and TCM-domain models, particularly in diagnostics,
herb recognition, and constitution classification. The model was deployed as an
interactive application on the OpenAI GPTs Store, accessed by nearly 1,000
users globally as of October 2025. This study demonstrates the feasibility of
developing a TCM-domain LLM through natural language-based instruction tuning
and multimodal integration, offering a practical framework for aligning
generative AI with traditional medical reasoning and a scalable pathway for
real-world deployment.

</details>


### [76] [Navigating the Alignment-Calibration Trade-off: A Pareto-Superior Frontier via Model Merging](https://arxiv.org/abs/2510.17426)
*Tiancheng Hu,Benjamin Minixhofer,Nigel Collier*

Main category: cs.CL

TL;DR: 论文揭示了后训练对齐不仅导致任务准确性下降，还会严重损失校准性，使模型过度自信、可靠性降低且输出多样性减少。通过简单的权重插值方法可以找到帕累托最优解，同时提升准确性和校准性。


<details>
  <summary>Details</summary>
Motivation: 传统上"对齐税"仅关注任务准确性下降，但研究发现对齐过程还会导致模型校准性严重损失，使模型变得过度自信和不可靠，这需要被全面解决。

Method: 采用简单的后处理干预：在模型对齐前后的权重之间进行插值，找到帕累托最优的插值点。

Result: 该方法能够发现超越两个父模型准确性的模型，同时显著恢复对齐过程中损失的校准性，证明这不是严格的权衡关系。

Conclusion: 简单的模型合并提供了一种计算高效的方法来减轻对齐税的全部影响，产生更强大且更可靠的模型。

Abstract: The "alignment tax" of post-training is typically framed as a drop in task
accuracy. We show it also involves a severe loss of calibration, making models
overconfident, less reliable, and model outputs less diverse. We show that this
trade-off can be navigated effectively via a simple post-hoc intervention:
interpolating between a model's weights before and after alignment. Crucially,
this is not a strict trade-off. We find that the process consistently reveals
Pareto-optimal interpolations - models that improve accuracy beyond both
parents while substantially recovering the calibration lost during alignment.
Our work demonstrates that simple model merging provides a computationally
efficient method for mitigating the full scope of the alignment tax, yielding
models that are more capable and more reliable.

</details>


### [77] [Agentic Reinforcement Learning for Search is Unsafe](https://arxiv.org/abs/2510.17431)
*Yushi Yang,Shreyansh Padarha,Andrew Lee,Adam Mahdi*

Main category: cs.CL

TL;DR: RL训练的语言模型在工具调用推理中表现出脆弱的安全性，简单的搜索攻击可以大幅降低其拒绝率和安全性。


<details>
  <summary>Details</summary>
Motivation: 研究RL训练的语言模型在自主调用工具时的安全特性，特别是搜索应用中的脆弱性。

Method: 提出两种简单攻击方法：强制模型以搜索开始响应（搜索攻击）和鼓励模型重复搜索（多搜索攻击）。

Result: 攻击使拒绝率降低达60.0%，答案安全性降低82.5%，搜索查询安全性降低82.4%。

Conclusion: 当前RL训练存在核心弱点，需要开发安全感知的智能体RL流程来优化安全搜索。

Abstract: Agentic reinforcement learning (RL) trains large language models to
autonomously call tools during reasoning, with search as the most common
application. These models excel at multi-step reasoning tasks, but their safety
properties are not well understood. In this study, we show that RL-trained
search models inherit refusal from instruction tuning and often deflect harmful
requests by turning them into safe queries. However, this safety is fragile.
Two simple attacks, one that forces the model to begin response with search
(Search attack), another that encourages models to repeatedly search
(Multi-search attack), trigger cascades of harmful searches and answers. Across
two model families (Qwen, Llama) with both local and web search, these attacks
lower refusal rates by up to 60.0%, answer safety by 82.5%, and search-query
safety by 82.4%. The attacks succeed by triggering models to generate harmful,
request-mirroring search queries before they can generate the inherited refusal
tokens. This exposes a core weakness of current RL training: it rewards
continued generation of effective queries without accounting for their
harmfulness. As a result, RL search models have vulnerabilities that users can
easily exploit, making it urgent to develop safety-aware agentic RL pipelines
optimising for safe search.

</details>


### [78] [Multilingual Clinical NER for Diseases and Medications Recognition in Cardiology Texts using BERT Embeddings](https://arxiv.org/abs/2510.17437)
*Manuela Daniela Danu,George Marica,Constantin Suciu,Lucian Mihai Itu,Oladimeji Farri*

Main category: cs.CL

TL;DR: 本研究开发了基于BERT的深度上下文嵌入模型，用于提升英语、西班牙语和意大利语临床文本中的命名实体识别性能，特别是在心脏病学领域，在BioASQ MultiCardioNER共享任务中取得了优于平均水平的F1分数。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录数据快速增长，需要从非结构化临床文本中提取生物医学知识来支持数据驱动的临床系统发展。虽然英语语料库的命名实体识别系统已有显著进步，但针对低资源语言临床文本的研究仍然稀缺。

Method: 探索了在通用领域文本上训练的单语和多语言BERT模型的有效性，用于从英语、西班牙语和意大利语的临床病例报告中提取疾病和药物提及。

Result: 在西班牙语疾病识别中获得77.88%的F1分数，西班牙语药物识别92.09%，英语药物识别91.74%，意大利语药物识别88.9%，所有子任务的成绩均优于测试排行榜的平均值和中位数。

Conclusion: 基于BERT的深度上下文嵌入模型能够有效提升低资源语言临床文本中的命名实体识别性能，为多语言临床NLP系统的发展提供了有力支持。

Abstract: The rapidly increasing volume of electronic health record (EHR) data
underscores a pressing need to unlock biomedical knowledge from unstructured
clinical texts to support advancements in data-driven clinical systems,
including patient diagnosis, disease progression monitoring, treatment effects
assessment, prediction of future clinical events, etc. While contextualized
language models have demonstrated impressive performance improvements for named
entity recognition (NER) systems in English corpora, there remains a scarcity
of research focused on clinical texts in low-resource languages. To bridge this
gap, our study aims to develop multiple deep contextual embedding models to
enhance clinical NER in the cardiology domain, as part of the BioASQ
MultiCardioNER shared task. We explore the effectiveness of different
monolingual and multilingual BERT-based models, trained on general domain text,
for extracting disease and medication mentions from clinical case reports
written in English, Spanish, and Italian. We achieved an F1-score of 77.88% on
Spanish Diseases Recognition (SDR), 92.09% on Spanish Medications Recognition
(SMR), 91.74% on English Medications Recognition (EMR), and 88.9% on Italian
Medications Recognition (IMR). These results outperform the mean and median F1
scores in the test leaderboard across all subtasks, with the mean/median values
being: 69.61%/75.66% for SDR, 81.22%/90.18% for SMR, 89.2%/88.96% for EMR, and
82.8%/87.76% for IMR.

</details>


### [79] [Evaluating Large Language Models on Urdu Idiom Translation](https://arxiv.org/abs/2510.17460)
*Muhammad Farmal Khan,Mousumi Akter*

Main category: cs.CL

TL;DR: 本文构建了首个乌尔都语到英语的习语翻译评估数据集，评估了多种LLM和NMT系统，发现提示工程能提升习语翻译质量，且原生乌尔都语文本比罗马化文本翻译效果更好。


<details>
  <summary>Details</summary>
Motivation: 习语翻译在机器翻译中仍是一个重大挑战，特别是对于乌尔都语等低资源语言，此前研究关注有限。

Method: 构建首个乌尔都语到英语习语翻译评估数据集，评估多种开源LLM和NMT系统，使用BLEU、BERTScore、COMET和XCOMET等自动指标评估翻译质量。

Result: 提示工程相比直接翻译能提升习语翻译质量，但不同提示类型间差异较小；原生乌尔都语输入的习语翻译准确度高于罗马化乌尔都语。

Conclusion: 文本表示方式显著影响翻译质量，原生乌尔都语在习语翻译中表现优于罗马化形式，提示工程是提升习语翻译的有效方法。

Abstract: Idiomatic translation remains a significant challenge in machine translation,
especially for low resource languages such as Urdu, and has received limited
prior attention. To advance research in this area, we introduce the first
evaluation datasets for Urdu to English idiomatic translation, covering both
Native Urdu and Roman Urdu scripts and annotated with gold-standard English
equivalents. We evaluate multiple open-source Large Language Models (LLMs) and
Neural Machine Translation (NMT) systems on this task, focusing on their
ability to preserve idiomatic and cultural meaning. Automatic metrics including
BLEU, BERTScore, COMET, and XCOMET are used to assess translation quality. Our
findings indicate that prompt engineering enhances idiomatic translation
compared to direct translation, though performance differences among prompt
types are relatively minor. Moreover, cross script comparisons reveal that text
representation substantially affects translation quality, with Native Urdu
inputs producing more accurate idiomatic translations than Roman Urdu.

</details>


### [80] [Disparities in Multilingual LLM-Based Healthcare Q&A](https://arxiv.org/abs/2510.17476)
*Ipek Baris Schlicht,Burcu Sayin,Zhixue Zhao,Frederik M. Labonté,Cesare Barbera,Marco Viviani,Paolo Rosso,Lucie Flek*

Main category: cs.CL

TL;DR: 研究发现多语言大语言模型在医疗问答中存在显著的跨语言事实对齐差异，即使是非英语提示，模型回答也更倾向于英语维基百科的内容。通过提供非英语维基百科的上下文信息可以有效改善事实对齐。


<details>
  <summary>Details</summary>
Motivation: 将AI整合到医疗保健中时，公平获取可靠健康信息至关重要。但不同语言间的信息质量存在差异，引发了对多语言大语言模型可靠性和一致性的担忧。

Method: 构建多语言维基医疗数据集，分析跨语言医疗覆盖率，评估LLM回答与参考内容的对齐度，并通过上下文信息和检索增强生成进行案例研究。

Result: 发现维基百科覆盖率和LLM事实对齐都存在显著的跨语言差异。即使是非英语提示，模型回答也更倾向于英语维基百科。提供非英语维基百科上下文可以有效将事实对齐转向文化相关知识。

Conclusion: 这些结果为构建更公平的多语言医疗AI系统提供了实用路径。

Abstract: Equitable access to reliable health information is vital when integrating AI
into healthcare. Yet, information quality varies across languages, raising
concerns about the reliability and consistency of multilingual Large Language
Models (LLMs). We systematically examine cross-lingual disparities in
pre-training source and factuality alignment in LLM answers for multilingual
healthcare Q&A across English, German, Turkish, Chinese (Mandarin), and
Italian. We (i) constructed Multilingual Wiki Health Care
(MultiWikiHealthCare), a multilingual dataset from Wikipedia; (ii) analyzed
cross-lingual healthcare coverage; (iii) assessed LLM response alignment with
these references; and (iv) conducted a case study on factual alignment through
the use of contextual information and Retrieval-Augmented Generation (RAG). Our
findings reveal substantial cross-lingual disparities in both Wikipedia
coverage and LLM factual alignment. Across LLMs, responses align more with
English Wikipedia, even when the prompts are non-English. Providing contextual
excerpts from non-English Wikipedia at inference time effectively shifts
factual alignment toward culturally relevant knowledge. These results highlight
practical pathways for building more equitable, multilingual AI systems for
healthcare.

</details>


### [81] [ReXMoE: Reusing Experts with Minimal Overhead in Mixture-of-Experts](https://arxiv.org/abs/2510.17483)
*Zheyue Tan,Zhiyuan Li,Tao Yuan,Dong Zhou,Weilin Liu,Yueqing Zhuang,Yadong Li,Guowei Niu,Cheng Qin,Zhuyu Yao,Congyi Liu,Haiyang Xu,Boxun Li,Guohao Dai,Bo Zhao,Yu Wang*

Main category: cs.CL

TL;DR: ReXMoE是一种新颖的MoE架构，通过跨层复用专家来超越传统的层局部路由机制，在固定参数预算下实现更好的路由多样性和模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统MoE架构受限于层局部路由机制，每层只能使用自己的专家池，需要在专家维度和路由多样性之间做出权衡。ReXMoE旨在打破这一限制。

Method: 提出ReXMoE架构，允许路由器在相邻层之间复用专家，解耦专家维度与每层预算。采用渐进式缩放路由策略，在训练过程中逐步增加候选专家池。

Result: 在0.5B到7B参数规模的不同架构模型上进行广泛实验，ReXMoE在固定架构维度下持续提升语言建模和下游任务性能。

Conclusion: ReXMoE为参数高效且可扩展的MoE基LLMs提供了新的设计范式，能够在不牺牲专家容量或增加总体参数的情况下实现更丰富的专家组合。

Abstract: Mixture-of-Experts (MoE) architectures have emerged as a promising approach
to scale Large Language Models (LLMs). MoE boosts the efficiency by activating
a subset of experts per token. Recent works show that fine-grained experts
substantially enriches the combinatorial flexibility of active experts and
enhances model expressiveness. However, such a design is fundamentally limited
by the layer-local routing mechanism: each layer is restricted to its own
expert pool. This requires a careful trade-off between expert dimensionality
and routing diversity given fixed parameter budgets. We describe ReXMoE, a
novel MoE architecture that improves routing beyond the existing layer-local
approaches by allowing routers to reuse experts across adjacent layers. ReXMoE
decouples expert dimensionality from per-layer budgets, enabling richer expert
combinations without sacrificing individual expert capacity or inflating
overall parameters. To this end, we propose a new progressive scaling routing
(PSR) strategy to gradually increase the candidate expert pool during training.
As a result, ReXMoE improves both language modeling and downstream task
performance. Extensive experiments on models ranging from 0.5B to 7B parameters
across different architectures demonstrate that ReXMoE consistently improves
performance under fixed architectural dimensions, confirming ReXMoE as new
design paradigm for parameter-efficient and scalable MoE-based LLMs.

</details>


### [82] [DETree: DEtecting Human-AI Collaborative Texts via Tree-Structured Hierarchical Representation Learning](https://arxiv.org/abs/2510.17489)
*Yongxin He,Shan Zhang,Yixuan Cao,Lei Ma,Ping Luo*

Main category: cs.CL

TL;DR: 提出DETree方法，通过层次亲和树结构建模不同AI参与文本生成过程的关系，并开发RealBench数据集，显著提升了混合文本检测的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: AI参与文本生成存在多种协作过程（AI写人编辑、人写AI编辑、AI生成AI精炼），现有检测方法建模粗糙，主要采用二元或多元分类，难以处理复杂特征。

Method: 提出DETree方法，将不同生成过程的关系建模为层次亲和树结构，引入专门损失函数使文本表示与树结构对齐，并开发RealBench数据集支持训练。

Result: 在混合文本检测任务中性能提升，在分布外场景下显著增强鲁棒性和泛化能力，特别是在少样本学习条件下表现优异。

Conclusion: 基于训练的方法在OOD设置中具有潜力，DETree通过建模生成过程的关系有效提升了AI参与文本检测能力。

Abstract: Detecting AI-involved text is essential for combating misinformation,
plagiarism, and academic misconduct. However, AI text generation includes
diverse collaborative processes (AI-written text edited by humans,
human-written text edited by AI, and AI-generated text refined by other AI),
where various or even new LLMs could be involved. Texts generated through these
varied processes exhibit complex characteristics, presenting significant
challenges for detection. Current methods model these processes rather crudely,
primarily employing binary classification (purely human vs. AI-involved) or
multi-classification (treating human-AI collaboration as a new class). We
observe that representations of texts generated through different processes
exhibit inherent clustering relationships. Therefore, we propose DETree, a
novel approach that models the relationships among different processes as a
Hierarchical Affinity Tree structure, and introduces a specialized loss
function that aligns text representations with this tree. To facilitate this
learning, we developed RealBench, a comprehensive benchmark dataset that
automatically incorporates a wide spectrum of hybrid texts produced through
various human-AI collaboration processes. Our method improves performance in
hybrid text detection tasks and significantly enhances robustness and
generalization in out-of-distribution scenarios, particularly in few-shot
learning conditions, further demonstrating the promise of training-based
approaches in OOD settings. Our code and dataset are available at
https://github.com/heyongxin233/DETree.

</details>


### [83] [Empowering Real-World: A Survey on the Technology, Practice, and Evaluation of LLM-driven Industry Agents](https://arxiv.org/abs/2510.17491)
*Yihong Tang,Kehai Chen,Liang Yue,Jinxin Fan,Caishen Zhou,Xiaoguang Li,Yuyang Zhang,Mingming Zhao,Shixiong Kai,Kaiyang Guo,Xingshan Zeng,Wenjing Cun,Lifeng Shang,Min Zhang*

Main category: cs.CL

TL;DR: 本文系统综述了基于大语言模型的行业智能体技术、应用和评估方法，提出了行业智能体能力成熟度框架，分析了从"流程执行系统"到"自适应社会系统"的演进路径。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，能够自主推理、规划和执行复杂任务的智能体成为人工智能前沿，但如何将通用智能体研究转化为推动行业变革的生产力仍面临重大挑战。

Method: 使用行业智能体能力成熟度框架，分析支撑智能体能力发展的三大技术支柱：记忆、规划和工具使用，并综述在数字工程、科学发现、具身智能等领域的实际应用。

Result: 建立了行业智能体从简单任务支持到复杂自主系统和集体智能的演进路径，识别了现有评估系统在真实性、安全性和行业特性方面面临的挑战。

Conclusion: 通过结合技术演进与行业实践，为理解和构建下一代行业智能体提供了清晰的路线图和理论基础，探讨了能力边界、发展潜力和治理问题。

Abstract: With the rise of large language models (LLMs), LLM agents capable of
autonomous reasoning, planning, and executing complex tasks have become a
frontier in artificial intelligence. However, how to translate the research on
general agents into productivity that drives industry transformations remains a
significant challenge. To address this, this paper systematically reviews the
technologies, applications, and evaluation methods of industry agents based on
LLMs. Using an industry agent capability maturity framework, it outlines the
evolution of agents in industry applications, from "process execution systems"
to "adaptive social systems." First, we examine the three key technological
pillars that support the advancement of agent capabilities: Memory, Planning,
and Tool Use. We discuss how these technologies evolve from supporting simple
tasks in their early forms to enabling complex autonomous systems and
collective intelligence in more advanced forms. Then, we provide an overview of
the application of industry agents in real-world domains such as digital
engineering, scientific discovery, embodied intelligence, collaborative
business execution, and complex system simulation. Additionally, this paper
reviews the evaluation benchmarks and methods for both fundamental and
specialized capabilities, identifying the challenges existing evaluation
systems face regarding authenticity, safety, and industry specificity. Finally,
we focus on the practical challenges faced by industry agents, exploring their
capability boundaries, developmental potential, and governance issues in
various scenarios, while providing insights into future directions. By
combining technological evolution with industry practices, this review aims to
clarify the current state and offer a clear roadmap and theoretical foundation
for understanding and building the next generation of industry agents.

</details>


### [84] [Deep Self-Evolving Reasoning](https://arxiv.org/abs/2510.17498)
*Zihan Liu,Shun Zheng,Xumeng Wen,Yang Wang,Jiang Bian,Mao Yang*

Main category: cs.CL

TL;DR: Deep Self-Evolving Reasoning (DSER) 是一种概率推理框架，通过将推理过程建模为马尔可夫链，利用多个并行长程自演化过程放大微小改进概率，从而显著提升小规模开放权重模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有验证-精炼框架依赖强大的验证和修正能力，这在开放权重的小规模模型中很脆弱。DSER旨在证明即使验证和精炼能力较弱，通过概率方法也能大幅扩展模型的推理极限。

Method: 将迭代推理建模为马尔可夫链，每个步骤代表解空间的随机转移。关键洞察是只要改进概率略高于退化概率，就能保证收敛到正确解。通过并行运行多个长程自演化过程来放大这些微小正向趋势。

Result: 在DeepSeek-R1-0528-Qwen3-8B模型上应用DSER，在AIME 2024-2025基准测试中解决了9个先前无法解决的问题中的5个，提升了整体性能，使这个紧凑模型通过多数投票超过了其600B参数教师的单轮准确率。

Conclusion: DSER不仅为测试时扩展提供了实用价值，还诊断了当前开放权重推理器的基本限制，为开发具有强大内在自演化能力的下一代模型建立了清晰的研究议程。

Abstract: Long-form chain-of-thought reasoning has become a cornerstone of advanced
reasoning in large language models. While recent verification-refinement
frameworks have enabled proprietary models to solve Olympiad-level problems,
their effectiveness hinges on strong, reliable verification and correction
capabilities, which remain fragile in open-weight, smaller-scale models. This
work demonstrates that even with weak verification and refinement capabilities
on hard tasks, the reasoning limits of such models can be substantially
extended through a probabilistic paradigm we call Deep Self-Evolving Reasoning
(DSER). We conceptualize iterative reasoning as a Markov chain, where each step
represents a stochastic transition in the solution space. The key insight is
that convergence to a correct solution is guaranteed as long as the probability
of improvement marginally exceeds that of degradation. By running multiple
long-horizon, self-evolving processes in parallel, DSER amplifies these small
positive tendencies, enabling the model to asymptotically approach correct
answers. Empirically, we apply DSER to the DeepSeek-R1-0528-Qwen3-8B model. On
the challenging AIME 2024-2025 benchmark, DSER solves 5 out of 9 previously
unsolvable problems and boosts overall performance, enabling this compact model
to surpass the single-turn accuracy of its 600B-parameter teacher through
majority voting. Beyond its immediate utility for test-time scaling, the DSER
framework serves to diagnose the fundamental limitations of current open-weight
reasoners. By clearly delineating their shortcomings in self-verification,
refinement, and stability, our findings establish a clear research agenda for
developing next-generation models with powerful, intrinsic self-evolving
capabilities.

</details>


### [85] [Lingua Custodi's participation at the WMT 2025 Terminology shared task](https://arxiv.org/abs/2510.17504)
*Jingshu Liu,Raheel Qader,Gaëtan Caillaut,Mariam Nakhlé*

Main category: cs.CL

TL;DR: 该论文探索了基于BERT的多语言句子嵌入方法，通过结合掩码语言建模、翻译语言建模等技术，显著减少了平行训练数据需求，在112种语言上实现了83.7%的双文本检索准确率，超越了LASER的65.5%。


<details>
  <summary>Details</summary>
Motivation: 虽然BERT在单语句子嵌入学习方面表现优异，但基于BERT的跨语言句子嵌入尚未得到充分探索。论文旨在系统研究多语言句子嵌入学习方法，结合单语和跨语言表示学习的最佳技术。

Method: 结合了掩码语言建模(MLM)、翻译语言建模(TLM)、双编码器翻译排序和加性边际softmax等方法，利用预训练的多语言语言模型大幅减少平行训练数据需求。

Result: 将最佳方法组合后，在Tatoeba数据集的112种语言上达到83.7%的双文本检索准确率，远高于LASER的65.5%。同时，在单语迁移学习基准上仍保持竞争力。使用该模型从CommonCrawl挖掘的平行数据可以训练出具有竞争力的英-中和英-德神经机器翻译模型。

Conclusion: 成功开发了有效的多语言句子嵌入模型，在109+种语言上公开可用，显著提升了跨语言检索性能，同时保持了单语任务的竞争力。

Abstract: While BERT is an effective method for learning monolingual sentence
embeddings for semantic similarity and embedding based transfer learning BERT
based cross-lingual sentence embeddings have yet to be explored. We
systematically investigate methods for learning multilingual sentence
embeddings by combining the best methods for learning monolingual and
cross-lingual representations including: masked language modeling (MLM),
translation language modeling (TLM), dual encoder translation ranking, and
additive margin softmax. We show that introducing a pre-trained multilingual
language model dramatically reduces the amount of parallel training data
required to achieve good performance by 80%. Composing the best of these
methods produces a model that achieves 83.7% bi-text retrieval accuracy over
112 languages on Tatoeba, well above the 65.5 achieved by LASER, while still
performing competitively on monolingual transfer learning benchmarks. Parallel
data mined from CommonCrawl using our best model is shown to train competitive
NMT models for en-zh and en-de. We publicly release our best multilingual
sentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.

</details>


### [86] [Annotation-Efficient Universal Honesty Alignment](https://arxiv.org/abs/2510.17509)
*Shiyu Ni,Keping Bi,Jiafeng Guo,Minghao Tang,Jingtong Wu,Zengxin Han,Xueqi Cheng*

Main category: cs.CL

TL;DR: EliCal是一个两阶段框架，通过廉价的自我一致性监督来获取内部置信度，然后用少量正确性标注进行校准，实现高效诚实验证。


<details>
  <summary>Details</summary>
Motivation: 现有的诚实验证方法要么依赖无训练的置信度估计，要么需要大量标注进行基于训练的校准。实现通用诚实验证需要昂贵的大规模标注，因此需要支持高效标注的训练方法。

Method: 提出Elicitation-Then-Calibration (EliCal)两阶段框架：第一阶段使用廉价的自我一致性监督获取内部置信度，第二阶段使用少量正确性标注校准置信度。

Result: EliCal仅使用1k正确性标注（全监督的0.18%）就实现了接近最优的对齐效果，在未见过的MMLU任务上比仅校准的基线表现更好。

Conclusion: EliCal为LLMs的通用诚实验证提供了一个可扩展的解决方案。

Abstract: Honesty alignment-the ability of large language models (LLMs) to recognize
their knowledge boundaries and express calibrated confidence-is essential for
trustworthy deployment. Existing methods either rely on training-free
confidence estimation (e.g., token probabilities, self-consistency) or
training-based calibration with correctness annotations. While effective,
achieving universal honesty alignment with training-based calibration requires
costly, large-scale labeling. To support annotation-efficient training, we
introduce Elicitation-Then-Calibration (EliCal), a two-stage framework that
first elicits internal confidence using inexpensive self-consistency
supervision, then calibrates this confidence with a small set of correctness
annotations. To support a large-scale study, we release HonestyBench, a
benchmark covering ten free-form QA datasets with 560k training and 70k
evaluation instances annotated with correctness and self-consistency signals.
Experiments show that EliCal achieves near-optimal alignment with only 1k
correctness annotations (0.18% of full supervision) and better alignment
performance on unseen MMLU tasks than the calibration-only baseline, offering a
scalable solution toward universal honesty alignment in LLMs.

</details>


### [87] [SimBench: Benchmarking the Ability of Large Language Models to Simulate Human Behaviors](https://arxiv.org/abs/2510.17516)
*Tiancheng Hu,Joachim Baumann,Lorenzo Lupo,Dirk Hovy,Nigel Collier,Paul Röttger*

Main category: cs.CL

TL;DR: SimBench是首个大规模标准化基准测试，用于评估LLM对人类行为的模拟能力，涵盖20个多样化数据集，发现当前最佳LLM模拟能力有限，性能随模型规模对数线性增长，但推理时间计算无法提升性能，存在对齐-模拟权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM模拟人类行为的评估方法碎片化，基于定制任务和指标，导致结果不可比较。需要建立标准化基准来推动LLM模拟能力的科学发展。

Method: 引入SimBench基准，整合20个多样化数据集，涵盖道德决策、经济选择等任务，基于全球参与者池进行标准化评估。

Result: 当前最佳LLM模拟能力得分仅40.80/100，性能随模型规模对数线性增长；推理时间计算无法提升性能；指令微调在低熵问题上改善性能但在高熵问题上降低性能；模型在模拟特定人口群体时表现较差；模拟能力与深度知识推理能力强相关。

Conclusion: SimBench为LLM模拟能力的可测量进步提供了基础，将加速开发更忠实的人类行为模拟器。

Abstract: Large language model (LLM) simulations of human behavior have the potential
to revolutionize the social and behavioral sciences, if and only if they
faithfully reflect real human behaviors. Current evaluations are fragmented,
based on bespoke tasks and metrics, creating a patchwork of incomparable
results. To address this, we introduce SimBench, the first large-scale,
standardized benchmark for a robust, reproducible science of LLM simulation. By
unifying 20 diverse datasets covering tasks from moral decision-making to
economic choice across a large global participant pool, SimBench provides the
necessary foundation to ask fundamental questions about when, how, and why LLM
simulations succeed or fail. We show that, while even the best LLMs today have
limited simulation ability (score: 40.80/100), performance scales log-linearly
with model size. Simulation performance is not improved by increased
inference-time compute. We demonstrate an alignment-simulation trade-off:
instruction-tuning improves performance on low-entropy (consensus) questions
but degrades it on high-entropy (diverse) ones. Models particularly struggle
when simulating specific demographic groups. Finally, we demonstrate that
simulation ability correlates most strongly with deep, knowledge-intensive
reasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to
accelerate the development of more faithful LLM simulators.

</details>


### [88] [OncoReason: Structuring Clinical Reasoning in LLMs for Robust and Interpretable Survival Prediction](https://arxiv.org/abs/2510.17532)
*Raghu Vamshi Hemadri,Geetha Krishna Guruju,Kristi Topollai,Anna Ewa Choromanska*

Main category: cs.CL

TL;DR: 提出一个统一的多任务学习框架，将自回归大语言模型与临床推理对齐，用于癌症治疗结果预测，通过三种对齐策略提升预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在生物医学NLP中表现良好但缺乏结构化推理能力，而癌症治疗结果预测需要既准确又可解释的模型，特别是在异构临床数据环境下。

Method: 使用统一的多任务学习框架，训练模型同时执行二元生存分类、连续生存时间回归和自然语言理由生成。评估三种对齐策略：标准监督微调、带思维链提示的监督微调、以及基于强化学习的组相对策略优化。

Result: 实验显示思维链提示将F1提高6.0%，MAE降低12%，而GRPO在BLEU、ROUGE和BERTScore上实现了最先进的可解释性和预测性能。现有生物医学LLM由于架构限制常无法产生有效的推理轨迹。

Conclusion: 研究强调了在多任务临床建模中推理感知对齐的重要性，并为精准肿瘤学中可解释、可信赖的LLM设定了新基准。

Abstract: Predicting cancer treatment outcomes requires models that are both accurate
and interpretable, particularly in the presence of heterogeneous clinical data.
While large language models (LLMs) have shown strong performance in biomedical
NLP, they often lack structured reasoning capabilities critical for high-stakes
decision support. We present a unified, multi-task learning framework that
aligns autoregressive LLMs with clinical reasoning for outcome prediction on
the MSK-CHORD dataset. Our models are trained to jointly perform binary
survival classification, continuous survival time regression, and natural
language rationale generation. We evaluate three alignment strategies: (1)
standard supervised fine-tuning (SFT), (2) SFT with Chain-of-Thought (CoT)
prompting to elicit step-by-step reasoning, and (3) Group Relative Policy
Optimization (GRPO), a reinforcement learning method that aligns model outputs
to expert-derived reasoning trajectories. Experiments with LLaMa3-8B and
Med42-8B backbones demonstrate that CoT prompting improves F1 by +6.0 and
reduces MAE by 12%, while GRPO achieves state-of-the-art interpretability and
predictive performance across BLEU, ROUGE, and BERTScore. We further show that
existing biomedical LLMs often fail to produce valid reasoning traces due to
architectural constraints. Our findings underscore the importance of
reasoning-aware alignment in multi-task clinical modeling and set a new
benchmark for interpretable, trustworthy LLMs in precision oncology.

</details>


### [89] [When Annotators Disagree, Topology Explains: Mapper, a Topological Tool for Exploring Text Embedding Geometry and Ambiguity](https://arxiv.org/abs/2510.17548)
*Nisrine Rair,Alban Goupil,Valeriu Vrabie,Emmanuel Chochoy*

Main category: cs.CL

TL;DR: 提出使用拓扑数据分析工具Mapper来研究语言模型如何内部表示歧义性，发现微调后的模型在嵌入空间中形成模块化、非凸的区域，即使对于高度歧义的情况也能保持高预测纯度。


<details>
  <summary>Details</summary>
Motivation: 传统标量指标（如准确率）无法捕捉模型内部如何表示歧义性，特别是在人类标注者存在分歧的情况下。需要新的方法来理解模型如何处理主观性NLP任务中的不确定性。

Method: 使用拓扑数据分析工具Mapper分析微调后的RoBERTa-Large模型在MD-Offense数据集上的嵌入空间结构，与传统方法如PCA和UMAP进行对比。

Result: 微调将嵌入空间重组为模块化、非凸的区域，与模型预测对齐。超过98%的连通组件展现出≥90%的预测纯度，但在歧义数据中与真实标签的对齐度下降，揭示了结构置信度与标签不确定性之间的隐藏张力。

Conclusion: Mapper是一个强大的诊断工具，能够直接揭示决策区域、边界塌陷和过度自信的聚类，超越了传统可视化工具。拓扑指标可以为主观性NLP任务中的主动建模策略提供信息。

Abstract: Language models are often evaluated with scalar metrics like accuracy, but
such measures fail to capture how models internally represent ambiguity,
especially when human annotators disagree. We propose a topological perspective
to analyze how fine-tuned models encode ambiguity and more generally instances.
  Applied to RoBERTa-Large on the MD-Offense dataset, Mapper, a tool from
topological data analysis, reveals that fine-tuning restructures embedding
space into modular, non-convex regions aligned with model predictions, even for
highly ambiguous cases. Over $98\%$ of connected components exhibit $\geq 90\%$
prediction purity, yet alignment with ground-truth labels drops in ambiguous
data, surfacing a hidden tension between structural confidence and label
uncertainty.
  Unlike traditional tools such as PCA or UMAP, Mapper captures this geometry
directly uncovering decision regions, boundary collapses, and overconfident
clusters. Our findings position Mapper as a powerful diagnostic tool for
understanding how models resolve ambiguity. Beyond visualization, it also
enables topological metrics that may inform proactive modeling strategies in
subjective NLP tasks.

</details>


### [90] [Language Confusion Gate: Language-Aware Decoding Through Model Self-Distillation](https://arxiv.org/abs/2510.17555)
*Collin Zhang,Fei Huang,Chenhan Yuan,Junyang Lin*

Main category: cs.CL

TL;DR: 提出了一种轻量级的语言混淆门控（LCG）方法，在解码过程中过滤token，无需重新训练模型，能显著减少语言混淆问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在文本生成时经常出现语言混淆问题，现有解决方案要么需要重新训练模型，要么无法区分有害混淆和可接受的语码转换。

Method: 使用规范调整的自蒸馏方法训练LCG，预测适当的语言家族，仅在需要时应用掩码。基于语言混淆不频繁、正确语言token通常位于预测前列、高资源语言输出token嵌入规范更大的发现。

Result: 在包括Qwen3、GPT-OSS、Gemma3、Llama3.1在内的各种模型上评估，LCG显著减少了语言混淆，通常降低一个数量级，且不影响任务性能。

Conclusion: LCG是一种有效的轻量级插件解决方案，能够在不改变基础LLM的情况下有效解决语言混淆问题。

Abstract: Large language models (LLMs) often experience language confusion, which is
the unintended mixing of languages during text generation. Current solutions to
this problem either necessitate model retraining or cannot differentiate
between harmful confusion and acceptable code-switching. This paper introduces
the Language Confusion Gate (LCG), a lightweight, plug-in solution that filters
tokens during decoding without altering the base LLM. The LCG is trained using
norm-adjusted self-distillation to predict appropriate language families and
apply masking only when needed. Our method is based on the findings that
language confusion is infrequent, correct-language tokens are usually among the
top predictions, and output token embedding norms are larger for high-resource
languages, which biases sampling. When evaluated across various models,
including Qwen3, GPT-OSS, Gemma3, Llama3.1, LCG decreases language confusion
significantly, often by an order of magnitude, without negatively impacting
task performance. Code is available at
https://github.com/collinzrj/language_confusion_gate.

</details>


### [91] [HGAdapter: Hypergraph-based Adapters in Language Models for Code Summarization and Clone Detection](https://arxiv.org/abs/2510.17591)
*Guang Yang,Yujie Zhu*

Main category: cs.CL

TL;DR: 提出了一种基于超图的适配器HGAdapter，通过捕捉代码中的高阶数据相关性来增强预训练语言模型在代码相关任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的预训练语言模型在代码任务中表现良好，但未能充分利用代码内部潜在的高阶数据相关性。

Method: 设计了三种代码高阶相关性（抽象语法树家族相关性、词汇相关性和行相关性），构建了token和超边生成器，并改进了超图神经网络架构与适配器调优结合，提出HGAdapter。

Result: 在多个公共数据集上的实验表明，该方法在不同程度上提升了PLMs在代码摘要和代码克隆检测任务中的性能。

Conclusion: 引入高阶数据相关性有助于提高预训练语言模型在代码任务中的有效性。

Abstract: Pre-trained language models (PLMs) are increasingly being applied to
code-related tasks. Although PLMs have achieved good results, they do not take
into account potential high-order data correlations within the code. We propose
three types of high-order correlations in code tokens, i.e. abstract syntax
tree family correlation, lexical correlation, and line correlation. We design a
tokens and hyperedges generator to capture these high-order data correlations.
We improve the architecture of hypergraph neural networks and combine it with
adapter tuning to propose a novel hypergraph-based adapter (HGAdapter) to
fine-tune PLMs. HGAdapter can encode high-order data correlations and is
allowed to be inserted into various PLMs to enhance performance. Experiments
were conducted on several public datasets, including six languages of code
summarization and code clone detection tasks. Our methods improved the
performance of PLMs in datasets to varying degrees. Experimental results
validate the introduction of high-order data correlations that contribute to
improved effectiveness.

</details>


### [92] [LawChain: Modeling Legal Reasoning Chains for Chinese Tort Case Analysis](https://arxiv.org/abs/2510.17602)
*Huiyuan Xie,Chenyang Li,Huining Zhu,Chubin Zhang,Yuxiao Ye,Zhenghao Liu,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 提出了LawChain框架来显式建模中国侵权民事案件中的法律推理过程，构建了评估基准LawChain_eval，并验证了基于LawChain的提示和微调方法能显著提升语言模型的法律推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有法律推理计算方法主要依赖通用推理框架，未能深入分析法律推理的细致过程，且研究多集中于刑事案件，对民事案件建模不足。

Method: 开发了LawChain三模块推理框架，将侵权分析的法律推理过程操作化为多个细粒度子步骤；构建评估基准LawChain_eval；提出基于LawChain的提示和微调基线方法。

Result: 当前最先进的大语言模型在侵权法律推理的关键要素上仍存在不足；提出的基线方法在侵权相关法律推理任务上取得显著改进，并能良好泛化到其他法律分析任务。

Conclusion: 显式建模法律推理链能有效增强语言模型的推理能力，LawChain框架在法律分析任务中具有良好通用性。

Abstract: Legal reasoning is a fundamental component of legal analysis and
decision-making. Existing computational approaches to legal reasoning
predominantly rely on generic reasoning frameworks such as syllogism and IRAC,
which do not comprehensively examine the nuanced processes that underpin legal
reasoning. Moreover, current research has largely focused on criminal cases,
with insufficient modeling for civil cases. In this work, we present a novel
framework for explicitly modeling legal reasoning in the analysis of Chinese
tort-related civil cases. We first operationalize the legal reasoning processes
used in tort analysis into the LawChain framework. LawChain is a three-module
reasoning framework, with each module consisting of multiple finer-grained
sub-steps. Informed by the LawChain framework, we introduce the task of tort
legal reasoning and construct an evaluation benchmark, LawChain$_{eval}$, to
systematically assess the critical steps within analytical reasoning chains for
tort analysis. Leveraging this benchmark, we evaluate state-of-the-art large
language models for their legal reasoning ability in civil tort contexts. Our
results indicate that current models still fall short in accurately handling
crucial elements of tort legal reasoning. Furthermore, we introduce several
baseline approaches that explicitly incorporate LawChain-style reasoning
through prompting or post-training. We conduct further experiments on
additional legal analysis tasks, such as Legal Named-Entity Recognition and
Criminal Damages Calculation, to verify the generalizability of these
baselines. The proposed baseline approaches achieve significant improvements in
tort-related legal reasoning and generalize well to related legal analysis
tasks, thus demonstrating the value of explicitly modeling legal reasoning
chains to enhance the reasoning capabilities of language models.

</details>


### [93] [Forget to Know, Remember to Use: Context-Aware Unlearning for Large Language Models](https://arxiv.org/abs/2510.17620)
*Yuefeng Peng,Parnian Afshar,Megan Ganji,Thomas Butler,Amir Houmansadr,Mingxian Wang,Dezhi Hong*

Main category: cs.CL

TL;DR: 本文提出了一种改进的遗忘学习方法，在保持目标知识遗忘效果的同时，恢复了模型在提示中重新引入被遗忘知识时的上下文利用能力。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘学习方法评估主要关注目标知识的遗忘程度和保留集性能，但忽略了当被遗忘知识重新出现在提示中时模型应能继续利用这些知识的重要可用性需求。

Method: 在遗忘学习目标中增加一个插件项，保持模型在上下文出现被遗忘知识时仍能利用这些知识的能力。

Result: 实验表明该方法能将上下文效用恢复到接近原始水平，同时保持有效的遗忘效果和保留集效用。

Conclusion: 通过增强遗忘学习目标，可以解决现有方法损害上下文效用的问题，实现更全面的模型知识管理。

Abstract: Large language models may encode sensitive information or outdated knowledge
that needs to be removed, to ensure responsible and compliant model responses.
Unlearning has emerged as an efficient alternative to full retraining, aiming
to remove specific knowledge while preserving overall model utility. Existing
evaluations of unlearning methods focus on (1) the extent of forgetting of the
target knowledge (forget set) and (2) maintaining performance on the retain set
(i.e., utility). However, these evaluations overlook an important usability
aspect: users may still want the model to leverage the removed information if
it is re-introduced in the prompt. In a systematic evaluation of six
state-of-the-art unlearning methods, we find that they consistently impair such
contextual utility. To address this, we augment unlearning objectives with a
plug-in term that preserves the model's ability to use forgotten knowledge when
it is present in context. Extensive experiments demonstrate that our approach
restores contextual utility to near original levels while still maintaining
effective forgetting and retain-set utility.

</details>


### [94] [Qomhra: A Bilingual Irish-English Large Language Model](https://arxiv.org/abs/2510.17652)
*Joseph McInerney*

Main category: cs.CL

TL;DR: 开发了Qomhr'a双语爱尔兰语-英语大语言模型，在低资源条件下通过持续预训练、指令调优和人类偏好对齐的完整流程，显著提升了爱尔兰语性能并保持英语能力。


<details>
  <summary>Details</summary>
Motivation: 在低资源条件下开发爱尔兰语-英语双语LLM，解决爱尔兰语资源稀缺的问题，同时保持模型的英语能力。

Method: 混合新获取的爱尔兰语语料和英语文本进行双语持续预训练；使用Gemini-2.5-Pro合成指令调优和人类偏好数据集；进行指令调优和人类偏好对齐。

Result: 在翻译、性别理解、主题识别和世界知识等基准测试中，爱尔兰语性能提升达29%，英语提升达44%；指令跟随能力显著改善。

Conclusion: Qomhr'a模型成功展示了在低资源语言环境下开发高质量双语LLM的可行性，为爱尔兰语AI应用提供了重要基础。

Abstract: This paper introduces Qomhr\'a, a bilingual Irish-English large language
model (LLM), developed under low-resource constraints presenting a complete
pipeline spanning bilingual continued pre-training, instruction tuning, and
alignment from human preferences. Newly accessible Irish corpora and English
text are mixed and curated to improve Irish performance while preserving
English ability. 6 closed-weight LLMs are judged for their Irish text
generation by a native speaker, a learner and other LLMs. Google's
Gemini-2.5-Pro is ranked the highest and is subsequently used to synthesise
instruction tuning and human preference datasets. Two datasets are contributed
leveraging Gemini-2.5-Pro: a 30K Irish-English parallel instruction tuning
dataset and a 1K human preference dataset, generating accepted and rejected
responses that show near perfect alignment with a native Irish speaker.
Qomhr\'a is comprehensively evaluated across benchmarks testing translation,
gender understanding, topic identification and world knowledge with gains of up
to 29% in Irish and 44% in English. Qomhr\'a also undergoes instruction tuning
and demonstrates clear progress in instruction following, crucial for chatbot
functionality.

</details>


### [95] [Towards Mining Effective Pedagogical Strategies from Learner-LLM Educational Dialogues](https://arxiv.org/abs/2510.17698)
*Liqun He,Manolis Mavrikis,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 本文采用对话分析方法研究学习者与LLM之间的教育对话，识别有效教学策略，包括数据收集、对话行为标注、模式挖掘和预测模型构建。


<details>
  <summary>Details</summary>
Motivation: 现有LLM教育应用评估方法主要关注技术性能或学习成果，忽视了学习者与LLM的互动过程，需要填补这一研究空白。

Method: 采用对话分析方法，包括对话数据收集、对话行为标注、对话模式挖掘和预测模型构建四个步骤。

Result: 提出了初步研究框架和早期见解，为未来研究奠定基础。

Conclusion: 强调需要通过关注对话动态和教学策略来评估基于LLM的教育应用。

Abstract: Dialogue plays a crucial role in educational settings, yet existing
evaluation methods for educational applications of large language models (LLMs)
primarily focus on technical performance or learning outcomes, often neglecting
attention to learner-LLM interactions. To narrow this gap, this AIED Doctoral
Consortium paper presents an ongoing study employing a dialogue analysis
approach to identify effective pedagogical strategies from learner-LLM
dialogues. The proposed approach involves dialogue data collection, dialogue
act (DA) annotation, DA pattern mining, and predictive model building. Early
insights are outlined as an initial step toward future research. The work
underscores the need to evaluate LLM-based educational applications by focusing
on dialogue dynamics and pedagogical strategies.

</details>


### [96] [QueST: Incentivizing LLMs to Generate Difficult Problems](https://arxiv.org/abs/2510.17715)
*Hanxu Hu,Xingxing Zhang,Jannis Vamvas,Rico Sennrich,Furu Wei*

Main category: cs.CL

TL;DR: QueST框架通过难度感知图采样和拒绝微调生成具有挑战性的编程问题，显著提升语言模型在竞争性编程任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有竞争性编程数据集规模有限（仅数千到数万问题），且缺乏大规模、具有挑战性的训练数据，限制了语言模型的扩展能力。

Method: 结合难度感知图采样和难度感知拒绝微调，直接优化专用生成器来创建具有挑战性的编程问题，并利用生成的问题进行知识蒸馏或强化学习。

Result: 使用QueST生成的10万个难题微调Qwen3-8B-base后，在LiveCodeBench上超越了原始Qwen3-8B；额外使用28K人工问题与合成解决方案后，8B模型性能与671B的DeepSeek-R1相当。

Conclusion: 通过QueST生成复杂问题是推进语言模型在竞争性编程和推理任务前沿的有效且可扩展方法。

Abstract: Large Language Models have achieved strong performance on reasoning tasks,
solving competition-level coding and math problems. However, their scalability
is limited by human-labeled datasets and the lack of large-scale, challenging
coding problem training data. Existing competitive coding datasets contain only
thousands to tens of thousands of problems. Previous synthetic data generation
methods rely on either augmenting existing instruction datasets or selecting
challenging problems from human-labeled data. In this paper, we propose QueST,
a novel framework which combines difficulty-aware graph sampling and
difficulty-aware rejection fine-tuning that directly optimizes specialized
generators to create challenging coding problems. Our trained generators
demonstrate superior capability compared to even GPT-4o at creating challenging
problems that benefit downstream performance. We leverage QueST to generate
large-scale synthetic coding problems, which we then use to distill from strong
teacher models with long chain-of-thought or to conduct reinforcement learning
for smaller models, proving effective in both scenarios. Our distillation
experiments demonstrate significant performance gains. Specifically, after
fine-tuning Qwen3-8B-base on 100K difficult problems generated by QueST, we
surpass the performance of the original Qwen3-8B on LiveCodeBench. With an
additional 112K examples (i.e., 28K human-written problems paired with multiple
synthetic solutions), our 8B model matches the performance of the much larger
DeepSeek-R1-671B. These findings indicate that generating complex problems via
QueST offers an effective and scalable approach to advancing the frontiers of
competitive coding and reasoning for large language models.

</details>


### [97] [PANER: A Paraphrase-Augmented Framework for Low-Resource Named Entity Recognition](https://arxiv.org/abs/2510.17720)
*Nanda Kumar Rengarajan,Jun Yan,Chun Wang*

Main category: cs.CL

TL;DR: 提出轻量级少样本NER框架，通过改进指令调优模板和数据增强技术，在低资源场景下实现与最先进模型相当的性能


<details>
  <summary>Details</summary>
Motivation: 解决NER任务在低资源场景下标注数据不足的问题，现有零样本和指令调优方法难以泛化到领域特定实体且无法有效利用有限数据

Method: 1) 新的指令调优模板，简化输出格式以利用大语言模型的大上下文窗口；2) 战略数据增强技术，在保留实体信息的同时对上下文进行改写

Result: 在CrossNER数据集上少样本方法平均F1得分为80.1，使用改写方法训练的模型比基线版本F1分数提升高达17分

Conclusion: 为训练数据和计算能力有限的群体提供了有前景的NER解决方案

Abstract: Named Entity Recognition (NER) is a critical task that requires substantial
annotated data, making it challenging in low-resource scenarios where label
acquisition is expensive. While zero-shot and instruction-tuned approaches have
made progress, they often fail to generalize to domain-specific entities and do
not effectively utilize limited available data. We present a lightweight
few-shot NER framework that addresses these challenges through two key
innovations: (1) a new instruction tuning template with a simplified output
format that combines principles from prior IT approaches to leverage the large
context window of recent state-of-the-art LLMs; (2) introducing a strategic
data augmentation technique that preserves entity information while
paraphrasing the surrounding context, thereby expanding our training data
without compromising semantic relationships. Experiments on benchmark datasets
show that our method achieves performance comparable to state-of-the-art models
on few-shot and zero-shot tasks, with our few-shot approach attaining an
average F1 score of 80.1 on the CrossNER datasets. Models trained with our
paraphrasing approach show consistent improvements in F1 scores of up to 17
points over baseline versions, offering a promising solution for groups with
limited NER training data and compute power.

</details>


### [98] [AcademicEval: Live Long-Context LLM Benchmark](https://arxiv.org/abs/2510.17725)
*Haozhen Zhang,Tao Feng,Pengrui Han,Jiaxuan You*

Main category: cs.CL

TL;DR: 提出了AcademicEval，一个用于评估LLMs在长上下文生成任务上的实时基准，使用arXiv论文作为数据源，无需人工标注，有效避免标签泄露问题。


<details>
  <summary>Details</summary>
Motivation: 当前长上下文LLM基准存在上下文长度固定、标注工作量大、以及LLM训练中标签泄露的紧迫挑战。

Method: 采用arXiv论文构建学术写作任务（标题、摘要、引言、相关工作），集成高质量专家策划的few-shot演示，支持灵活上下文长度，实现高效实时评估。

Result: LLMs在具有层次抽象级别的任务上表现较差，且难以处理长few-shot演示，突显了该基准的挑战性。

Conclusion: 通过实验分析揭示了增强LLMs长上下文建模能力的一些见解，该基准为评估和改进LLMs的长上下文理解提供了有效工具。

Abstract: Large Language Models (LLMs) have recently achieved remarkable performance in
long-context understanding. However, current long-context LLM benchmarks are
limited by rigid context length, labor-intensive annotation, and the pressing
challenge of label leakage issues during LLM training. Therefore, we propose
\textsc{AcademicEval}, a live benchmark for evaluating LLMs over long-context
generation tasks. \textsc{AcademicEval} adopts papers on arXiv to introduce
several academic writing tasks with long-context inputs, \textit{i.e.},
\textsc{Title}, \textsc{Abstract}, \textsc{Introduction}, and \textsc{Related
Work}, which cover a wide range of abstraction levels and require no manual
labeling. Moreover, \textsc{AcademicEval} integrates high-quality and
expert-curated few-shot demonstrations from a collected co-author graph to
enable flexible context length. Especially, \textsc{AcademicEval} features an
efficient live evaluation, ensuring no label leakage. We conduct a holistic
evaluation on \textsc{AcademicEval}, and the results illustrate that LLMs
perform poorly on tasks with hierarchical abstraction levels and tend to
struggle with long few-shot demonstrations, highlighting the challenge of our
benchmark. Through experimental analysis, we also reveal some insights for
enhancing LLMs' long-context modeling capabilities. Code is available at
https://github.com/ulab-uiuc/AcademicEval

</details>


### [99] [Train for Truth, Keep the Skills: Binary Retrieval-Augmented Reward Mitigates Hallucinations](https://arxiv.org/abs/2510.17733)
*Tong Chen,Akari Asai,Luke Zettlemoyer,Hannaneh Hajishirzi,Faeze Brahman*

Main category: cs.CL

TL;DR: 提出了一种使用二元检索增强奖励的在线强化学习方法，有效减少语言模型的外源性幻觉，在保持其他任务性能的同时显著提升事实准确性。


<details>
  <summary>Details</summary>
Motivation: 现有缓解语言模型外源性幻觉的方法往往会影响开放式生成和下游任务性能，限制了实际应用价值。

Method: 使用二元检索增强奖励的在线强化学习，只有当模型输出完全正确时奖励为1，否则为0。

Result: 在开放式生成中幻觉率降低39.3%；在短格式问答中，模型学会策略性弃权，在PopQA和GPQA上错误答案分别减少44.4%和21.7%，且不影响指令遵循、数学和代码能力。

Conclusion: 二元奖励方案在提升事实准确性的同时避免了连续奖励强化学习带来的质量退化问题，具有更好的实用性。

Abstract: Language models often generate factually incorrect information unsupported by
their training data, a phenomenon known as extrinsic hallucination. Existing
mitigation approaches often degrade performance on open-ended generation and
downstream tasks, limiting their practical utility. We propose an online
reinforcement learning method using a novel binary retrieval-augmented reward
(RAR) to address this tradeoff. Unlike continuous reward schemes, our approach
assigns a reward of one only when the model's output is entirely factually
correct, and zero otherwise. We evaluate our method on Qwen3 reasoning models
across diverse tasks. For open-ended generation, binary RAR achieves a 39.3%
reduction in hallucination rates, substantially outperforming both supervised
training and continuous-reward RL baselines. In short-form question answering,
the model learns calibrated abstention, strategically outputting "I don't know"
when faced with insufficient parametric knowledge. This yields 44.4% and 21.7%
fewer incorrect answers on PopQA and GPQA, respectively. Crucially, these
factuality gains come without performance degradation on instruction following,
math, or code, whereas continuous-reward RL, despite improving factuality,
induces quality regressions.

</details>


### [100] [Evaluating Medical LLMs by Levels of Autonomy: A Survey Moving from Benchmarks to Applications](https://arxiv.org/abs/2510.17764)
*Xiao Ye,Jacob Dineen,Zhaonan Li,Zhikun Xu,Weiyu Chen,Shijie Lu,Yuxi Huang,Ming Shen,Phu Tran,Ji-Eun Irene Yum,Muhammad Ali Khan,Muhammad Umar Afzal,Irbaz Bin Riaz,Ben Zhou*

Main category: cs.CL

TL;DR: 该调查通过自主性层级框架(L0-L3)重新构建医学大语言模型评估，将现有基准与各层级允许的操作和风险对齐，提出基于层级的评估蓝图，推动从分数导向转向真实临床使用的可信证据。


<details>
  <summary>Details</summary>
Motivation: 医学大语言模型在标准基准测试中表现优异，但这些结果向临床工作流程中安全可靠性能的转移仍面临挑战，需要更贴近实际临床使用的评估框架。

Method: 采用自主性层级框架(L0-L3)：信息工具、信息转换与聚合、决策支持、监督代理，将现有基准和指标与各层级的允许操作和风险对齐，建立层级条件化的评估蓝图。

Result: 提出了明确的评估目标，使评估与监管相关联，为选择指标、收集证据和报告声明提供了系统方法。

Conclusion: 通过以自主性为中心，该调查推动领域超越基于分数的声明，转向为真实临床使用提供可信、风险感知的证据。

Abstract: Medical Large language models achieve strong scores on standard benchmarks;
however, the transfer of those results to safe and reliable performance in
clinical workflows remains a challenge. This survey reframes evaluation through
a levels-of-autonomy lens (L0-L3), spanning informational tools, information
transformation and aggregation, decision support, and supervised agents. We
align existing benchmarks and metrics with the actions permitted at each level
and their associated risks, making the evaluation targets explicit. This
motivates a level-conditioned blueprint for selecting metrics, assembling
evidence, and reporting claims, alongside directions that link evaluation to
oversight. By centering autonomy, the survey moves the field beyond score-based
claims toward credible, risk-aware evidence for real clinical use.

</details>


### [101] [Foundational Automatic Evaluators: Scaling Multi-Task Generative Evaluator Training for Reasoning-Centric Domains](https://arxiv.org/abs/2510.17793)
*Austin Xu,Xuan-Phi Nguyen,Yilun Zhou,Chien-Sheng Wu,Caiming Xiong,Shafiq Joty*

Main category: cs.CL

TL;DR: 该论文提出了FARE（基础自动推理评估器），通过大规模数据（250万样本）和简单的迭代拒绝采样SFT方法训练了8B和20B参数的评估器，在多个推理评估任务中超越了更大的专门化评估器。


<details>
  <summary>Details</summary>
Motivation: 当前生成式评估器主要关注新方法如强化学习，而忽视了大规模数据驱动的发展。本文专注于数据扩展，以满足训练和测试时对可扩展评估日益增长的需求。

Method: 收集了250万样本，涵盖5种评估任务和多个推理领域。使用简单的迭代拒绝采样监督微调（SFT）方法训练了8B和20B参数的FARE评估器。

Result: FARE-8B挑战了更大的专门化RL训练评估器，FARE-20B为开源评估器设定了新标准，超越了专门的70B+评估器。在实际任务中，作为推理时重排器在MATH上达到接近oracle性能，作为RL训练验证器将下游模型性能提升达14.1%。

Conclusion: 大规模数据驱动的简单SFT方法可以训练出强大的评估器，FARE在静态基准和实际应用中都表现出色，为评估器的发展提供了新方向。

Abstract: Finetuning specialized generative evaluators has emerged as a popular
paradigm to meet the increasing demand for scalable evaluation during both
training and test-time. However, recent work has largely focused on applying
new methodology, such as reinforcement learning (RL), to training evaluators,
shying away from large-scale, data-driven development. In this work, we focus
on data scaling, curating a set of 2.5M samples spanning five unique evaluation
tasks (pairwise, step-level, reference-free and reference-based verification,
and single rating) and multiple domains focused on reasoning evaluation. With
our data, we train Foundational Automatic Reasoning Evaluators (FARE), a family
of 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative
rejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges
larger specialized RL-trained evaluators and FARE-20B sets the new standard for
open-source evaluators, surpassing specialized 70B+ evaluators. Beyond static
benchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers,
FARE-20B achieves near-oracle performance on MATH. As verifiers in RL training,
FARE improves the downstream RL-trained model performance by up to 14.1% vs.
string-matching verifiers. When initialized from FARE, a continually-finetuned
FARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality.

</details>


### [102] [Executable Knowledge Graphs for Replicating AI Research](https://arxiv.org/abs/2510.17795)
*Yujie Luo,Zhuoyun Yu,Xuehai Wang,Yuqi Zhu,Ningyu Zhang,Lanning Wei,Lun Du,Da Zheng,Huajun Chen*

Main category: cs.CL

TL;DR: 提出了可执行知识图谱(xKG)来解决AI研究复现中的挑战，通过整合技术洞察、代码片段和领域知识，显著提升了LLM代理的研究复现能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成可执行代码方面存在困难，主要由于背景知识不足和RAG方法的局限性，无法捕捉参考文献中的潜在技术细节，且缺乏支持多粒度检索和重用的结构化知识表示。

Method: 提出可执行知识图谱(xKG)，这是一个模块化、可插拔的知识库，自动从科学文献中提取技术洞察、代码片段和领域特定知识，并集成到代理框架中。

Result: 在三个代理框架和两种不同LLM上集成xKG后，在PaperBench上显示出显著的性能提升（使用o3-mini时提升10.9%）。

Conclusion: xKG被证明是自动化AI研究复现的通用且可扩展解决方案，代码将在指定GitHub仓库发布。

Abstract: Replicating AI research is a crucial yet challenging task for large language
model (LLM) agents. Existing approaches often struggle to generate executable
code, primarily due to insufficient background knowledge and the limitations of
retrieval-augmented generation (RAG) methods, which fail to capture latent
technical details hidden in referenced papers. Furthermore, previous approaches
tend to overlook valuable implementation-level code signals and lack structured
knowledge representations that support multi-granular retrieval and reuse. To
overcome these challenges, we propose Executable Knowledge Graphs (xKG), a
modular and pluggable knowledge base that automatically integrates technical
insights, code snippets, and domain-specific knowledge extracted from
scientific literature. When integrated into three agent frameworks with two
different LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on
PaperBench, demonstrating its effectiveness as a general and extensible
solution for automated AI research replication. Code will released at
https://github.com/zjunlp/xKG.

</details>


### [103] [Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics](https://arxiv.org/abs/2510.17797)
*Akshara Prabhakar,Roshan Ram,Zixiang Chen,Silvio Savarese,Frank Wang,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.CL

TL;DR: 企业深度研究(EDR)是一个多智能体系统，通过主规划智能体和四个专业搜索智能体，结合可扩展工具生态系统和可视化智能体，实现企业非结构化数据的自动化研究和报告生成。


<details>
  <summary>Details</summary>
Motivation: 随着信息指数级增长，企业面临将非结构化数据转化为可操作洞察的压力。现有自主智能体在领域特定细微差别、意图对齐和企业集成方面存在困难。

Method: EDR系统包含：(1)主规划智能体进行自适应查询分解；(2)四个专业搜索智能体(通用、学术、GitHub、LinkedIn)；(3)基于MCP的可扩展工具生态系统；(4)可视化智能体生成数据驱动的洞察；(5)检测知识差距并更新研究方向的反思机制。

Result: 在DeepResearch Bench和DeepConsult等开放式基准测试中，EDR在无需人工干预的情况下优于最先进的智能体系统。

Conclusion: EDR框架和基准轨迹的发布将推动多智能体推理应用的研究发展。

Abstract: As information grows exponentially, enterprises face increasing pressure to
transform unstructured data into coherent, actionable insights. While
autonomous agents show promise, they often struggle with domain-specific
nuances, intent alignment, and enterprise integration. We present Enterprise
Deep Research (EDR), a multi-agent system that integrates (1) a Master Planning
Agent for adaptive query decomposition, (2) four specialized search agents
(General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool
ecosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a
Visualization Agent for data-driven insights, and (5) a reflection mechanism
that detects knowledge gaps and updates research direction with optional
human-in-the-loop steering guidance. These components enable automated report
generation, real-time streaming, and seamless enterprise deployment, as
validated on internal datasets. On open-ended benchmarks including DeepResearch
Bench and DeepConsult, EDR outperforms state-of-the-art agentic systems without
any human steering. We release the EDR framework and benchmark trajectories to
advance research on multi-agent reasoning applications.
  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and
Dataset at https://huggingface.co/datasets/Salesforce/EDR-200

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [104] [VisuoAlign: Safety Alignment of LVLMs with Multimodal Tree Search](https://arxiv.org/abs/2510.15948)
*MingSheng Li,Guangze Zhao,Sichen Liu*

Main category: cs.AI

TL;DR: VisuoAlign是一个通过提示引导树搜索实现多模态安全对齐的框架，旨在解决大型视觉语言模型在跨模态威胁下的安全对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法对多模态越狱攻击脆弱，因为视觉输入引入了新的攻击面，推理链缺乏安全监督，且模态融合通常会降低对齐效果。

Method: 通过视觉-文本交互提示将安全约束嵌入推理过程，使用蒙特卡洛树搜索构建多样化的安全关键提示轨迹，并引入基于提示的缩放确保实时风险检测和合规响应。

Result: 实验表明VisuoAlign能够主动暴露风险，实现全面的数据集生成，并显著提高LVLMs对抗复杂跨模态威胁的鲁棒性。

Conclusion: VisuoAlign框架有效解决了多模态安全对齐的关键挑战，为大型视觉语言模型提供了可靠的安全防护机制。

Abstract: Large Vision-Language Models (LVLMs) have achieved remarkable progress in
multimodal perception and generation, yet their safety alignment remains a
critical challenge.Existing defenses and vulnerable to multimodal jailbreaks,
as visual inputs introduce new attack surfaces, reasoning chains lack safety
supervision, and alignment often degrades under modality fusion.To overcome
these limitation, we propose VisuoAlign, a framework for multi-modal safety
alignment via prompt-guided tree search.VisuoAlign embeds safety constrains
into the reasoning process through visual-textual interactive prompts, employs
Monte Carlo Tree Search(MCTS) to systematically construct diverse
safety-critical prompt trajectories, and introduces prompt-based scaling to
ensure real-time risk detection and compliant responses.Extensive experiments
demonstrate that VisuoAlign proactively exposes risks, enables comprehensive
dataset generation, and significantly improves the robustness of LVLMs against
complex cross-modal threats.

</details>


### [105] [Executable Epistemology: The Structured Cognitive Loop as an Architecture of Intentional Understanding](https://arxiv.org/abs/2510.15952)
*Myung Ho Kim*

Main category: cs.AI

TL;DR: 本文提出了结构化认知循环（SCL）作为可执行的认知框架，旨在解决大型语言模型缺乏真正认知架构的问题。SCL将哲学见解转化为可计算结构，通过功能分离的认知架构产生更连贯的行为，并重新定义智能为通过意向性理解重建自身认知状态的能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型表现出智能但缺乏真正的认知理解，暴露出认知架构的缺失。传统AI研究关注"什么是智能"的本体论问题，而SCL关注"在什么条件下认知会出现"的认识论问题。

Method: 基于心智哲学和认知现象学，结合过程哲学、具身认知和扩展心智理论，将智能定义为执行过程而非属性——包含判断、记忆、控制、行动和调节的连续循环。通过功能分离的认知架构实现"可执行的认识论"。

Result: SCL展示了功能分离的认知架构比单一提示系统产生更连贯和可解释的行为，得到了智能体评估的支持。该框架将智能重新定义为通过意向性理解重建自身认知状态的能力。

Conclusion: SCL为心智哲学、认识论和AI领域带来重要影响：让认知理论能够被执行和测试；将行为建立在认知结构而非统计规律上；将知识视为在现象学连贯循环中的持续重建。真正的进步需要实现认知原则的结构化架构，而非更大的模型。

Abstract: Large language models exhibit intelligence without genuine epistemic
understanding, exposing a key gap: the absence of epistemic architecture. This
paper introduces the Structured Cognitive Loop (SCL) as an executable
epistemological framework for emergent intelligence. Unlike traditional AI
research asking "what is intelligence?" (ontological), SCL asks "under what
conditions does cognition emerge?" (epistemological). Grounded in philosophy of
mind and cognitive phenomenology, SCL bridges conceptual philosophy and
implementable cognition. Drawing on process philosophy, enactive cognition, and
extended mind theory, we define intelligence not as a property but as a
performed process -- a continuous loop of judgment, memory, control, action,
and regulation. SCL makes three contributions. First, it operationalizes
philosophical insights into computationally interpretable structures, enabling
"executable epistemology" -- philosophy as structural experiment. Second, it
shows that functional separation within cognitive architecture yields more
coherent and interpretable behavior than monolithic prompt based systems,
supported by agent evaluations. Third, it redefines intelligence: not
representational accuracy but the capacity to reconstruct its own epistemic
state through intentional understanding. This framework impacts philosophy of
mind, epistemology, and AI. For philosophy, it allows theories of cognition to
be enacted and tested. For AI, it grounds behavior in epistemic structure
rather than statistical regularity. For epistemology, it frames knowledge not
as truth possession but as continuous reconstruction within a
phenomenologically coherent loop. We situate SCL within debates on cognitive
phenomenology, emergence, normativity, and intentionality, arguing that real
progress requires not larger models but architectures that realize cognitive
principles structurally.

</details>


### [106] [Exploring the Potential of Citiverses for Regulatory Learning](https://arxiv.org/abs/2510.15959)
*Isabelle Hupont,Marisa Ponti,Sven Schade*

Main category: cs.AI

TL;DR: 本文提出了将城市虚拟世界(citiverses)作为监管学习实验空间的科学政策议程，通过专家咨询识别了关键研究领域和实验主题，强调负责任的发展方法。


<details>
  <summary>Details</summary>
Motivation: 探索城市虚拟世界作为沉浸式实验环境的潜力，支持政策制定者测试监管场景和技术，促进监管学习。

Method: 基于与欧盟委员会政策制定者、国家政府科学顾问和数字监管领域专家的高层专家咨询，识别关键研究领域和实验主题。

Result: 确定了可扩展性、实时反馈、复杂性建模、跨境协作等关键研究领域，以及交通、城市规划、环境/气候危机等实验主题。

Conclusion: 城市虚拟世界有潜力成为监管学习的重要实验空间，但需要负责任地发展，考虑伦理、经济、生态和社会维度，并整合到更广泛的实验生态系统。

Abstract: Citiverses hold the potential to support regulatory learning by offering
immersive, virtual environments for experimenting with policy scenarios and
technologies. This paper proposes a science-for-policy agenda to explore the
potential of citiverses as experimentation spaces for regulatory learning,
grounded in a consultation with a high-level panel of experts, including
policymakers from the European Commission, national government science advisers
and leading researchers in digital regulation and virtual worlds. It identifies
key research areas, including scalability, real-time feedback, complexity
modelling, cross-border collaboration, risk reduction, citizen participation,
ethical considerations and the integration of emerging technologies. In
addition, the paper analyses a set of experimental topics, spanning
transportation, urban planning and the environment/climate crisis, that could
be tested in citiverse platforms to advance regulatory learning in these areas.
The proposed work is designed to inform future research for policy and
emphasizes a responsible approach to developing and using citiverses. It
prioritizes careful consideration of the ethical, economic, ecological and
social dimensions of different regulations. The paper also explores essential
preliminary steps necessary for integrating citiverses into the broader
ecosystems of experimentation spaces, including test beds, living labs and
regulatory sandboxes

</details>


### [107] [PISA: A Pragmatic Psych-Inspired Unified Memory System for Enhanced AI Agency](https://arxiv.org/abs/2510.15966)
*Shian Jia,Ziyang Huang,Xinbo Wang,Haofei Zhang,Mingli Song*

Main category: cs.AI

TL;DR: 提出了PISA记忆系统，基于皮亚杰认知发展理论，通过三模态适应机制和混合记忆访问架构，显著提升了AI代理的适应性和长期知识保持能力。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理记忆系统缺乏对多样化任务的适应性，忽视了记忆的建构性和任务导向作用。

Method: 采用三模态适应机制（图式更新、图式演化和图式创建）保持记忆组织连贯性，设计结合符号推理与神经检索的混合记忆访问架构。

Result: 在LOCOMO基准和新提出的AggQA数据分析基准上，PISA创造了新的最先进水平，显著提升了适应性和长期知识保持。

Conclusion: PISA记忆系统通过建构性记忆方法和混合架构，有效解决了AI代理记忆系统的适应性和知识保持问题。

Abstract: Memory systems are fundamental to AI agents, yet existing work often lacks
adaptability to diverse tasks and overlooks the constructive and task-oriented
role of AI agent memory. Drawing from Piaget's theory of cognitive development,
we propose PISA, a pragmatic, psych-inspired unified memory system that
addresses these limitations by treating memory as a constructive and adaptive
process. To enable continuous learning and adaptability, PISA introduces a
trimodal adaptation mechanism (i.e., schema updation, schema evolution, and
schema creation) that preserves coherent organization while supporting flexible
memory updates. Building on these schema-grounded structures, we further design
a hybrid memory access architecture that seamlessly integrates symbolic
reasoning with neural retrieval, significantly improving retrieval accuracy and
efficiency. Our empirical evaluation, conducted on the existing LOCOMO
benchmark and our newly proposed AggQA benchmark for data analysis tasks,
confirms that PISA sets a new state-of-the-art by significantly enhancing
adaptability and long-term knowledge retention.

</details>


### [108] [Limits of Emergent Reasoning of Large Language Models in Agentic Frameworks for Deterministic Games](https://arxiv.org/abs/2510.15974)
*Chris Su,Harrison Li,Matheus Marques,George Flint,Kevin Zhu,Sunishchal Dev*

Main category: cs.AI

TL;DR: 研究发现，大型推理模型在解决超过特定复杂度阈值的谜题时会出现性能崩溃。即使为模型提供环境接口来跟踪状态空间，也无法延迟或消除这种性能崩溃。


<details>
  <summary>Details</summary>
Motivation: 探讨大型推理模型在解决复杂谜题时性能崩溃的真正原因，特别是排除模型需要自行跟踪状态空间这一潜在混淆因素。

Method: 为大型语言模型提供汉诺塔问题的环境接口，允许模型通过工具调用进行移动、提供书面理由、观察结果状态空间，并重新提示自己进行下一步移动。

Result: 环境接口的访问并不能延迟或消除性能崩溃。策略分析显示模型与最优策略和随机策略的偏离度都在增加，表明模型在每个复杂度级别都表现出模式崩溃。

Conclusion: 大型推理模型的性能崩溃现象可能源于模型在复杂任务中表现出的模式崩溃，性能取决于模型选择的模式是否与问题的正确解决方案一致。

Abstract: Recent work reports that Large Reasoning Models (LRMs) undergo a collapse in
performance on solving puzzles beyond certain perplexity thresholds. In
subsequent discourse, questions have arisen as to whether the nature of the
task muddles an evaluation of true reasoning. One potential confound is the
requirement that the model keep track of the state space on its own. We provide
a large language model (LLM) with an environment interface for Tower of Hanoi
problems, allowing it to make a move with a tool call, provide written
justification, observe the resulting state space, and reprompt itself for the
next move. We observe that access to an environment interface does not delay or
eradicate performance collapse. Furthermore, LLM-parameterized policy analysis
reveals increasing divergence from both optimal policies and uniformly random
policies, suggesting that the model exhibits mode-like collapse at each level
of complexity, and that performance is dependent upon whether the mode reflects
the correct solution for the problem. We suggest that a similar phenomena might
take place in LRMs.

</details>


### [109] [Cognitive Load Traces as Symbolic and Visual Accounts of Deep Model Cognition](https://arxiv.org/abs/2510.15980)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: 提出Cognitive Load Traces (CLTs)作为深度学习模型的中层可解释性框架，受人类认知负荷理论启发。CLTs量化模型内部资源分配，包含内在、外在和关联负荷三个分量，通过可测量代理实现，能够预测错误发生、揭示认知策略，并通过负荷引导干预提高推理效率15-30%。


<details>
  <summary>Details</summary>
Motivation: 受人类认知负荷理论启发，旨在为深度模型开发一个中层可解释性框架，以量化模型内部的资源分配动态，从而更好地理解和改进模型的推理过程。

Method: 定义CLTs为包含内在负荷(IL_t)、外在负荷(EL_t)和关联负荷(GL_t)三个分量的随机过程，通过注意力熵、KV缓存未命中率、表示分散度和解码稳定性等可测量代理来实例化，并提出符号化公式和可视化方法（负荷曲线、单纯形图）。

Result: 在推理和规划基准测试中，CLTs能够预测错误发生、揭示认知策略，通过负荷引导干预在保持准确性的同时将推理效率提高15-30%。

Conclusion: CLTs提供了一个有效的框架来分析和改进深度模型的推理动态，通过量化认知负荷实现了更好的模型可解释性和性能优化。

Abstract: We propose \textbf{Cognitive Load Traces} (CLTs) as a mid-level
interpretability framework for deep models, inspired by Cognitive Load Theory
in human cognition. CLTs are defined as symbolic, temporally varying functions
that quantify model-internal resource allocation. Formally, we represent CLTs
as a three-component stochastic process $(\mathrm{IL}_t, \mathrm{EL}_t,
\mathrm{GL}_t)$, corresponding to \emph{Intrinsic}, \emph{Extraneous}, and
\emph{Germane} load. Each component is instantiated through measurable proxies
such as attention entropy, KV-cache miss ratio, representation dispersion, and
decoding stability. We propose both symbolic formulations and visualization
methods (load curves, simplex diagrams) that enable interpretable analysis of
reasoning dynamics. Experiments on reasoning and planning benchmarks show that
CLTs predict error-onset, reveal cognitive strategies, and enable load-guided
interventions that improve reasoning efficiency by 15-30\% while maintaining
accuracy.

</details>


### [110] [ProofFlow: A Dependency Graph Approach to Faithful Proof Autoformalization](https://arxiv.org/abs/2510.15981)
*Rafael Cabral,Tuan Manh Do,Xuejun Yu,Wai Ming Tai,Zijin Feng,Xin Shen*

Main category: cs.AI

TL;DR: ProofFlow是一个新的自动形式化证明流水线，通过构建逻辑依赖图和使用基于引理的方法来保持证明的结构保真度，在自动形式化任务中达到了新的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 解决现有自动形式化方法在保持语义意义和逻辑结构方面的不足，将结构保真度作为主要目标。

Method: 首先构建有向无环图来映射证明步骤间的逻辑依赖关系，然后采用基于引理的方法系统地将每个步骤形式化为中间引理，保持原始论证的逻辑结构。

Result: 在包含184个本科水平问题的新基准测试中，ProofFlow达到了0.545的ProofScore，显著超过全证明形式化（0.123）和步骤证明形式化（0.072）等基线方法。

Conclusion: ProofFlow流水线在自动形式化任务中实现了新的最先进性能，其流水线、基准和评分指标已开源以促进进一步研究。

Abstract: Proof autoformalization, the task of translating natural language theorems
and proofs into machine-verifiable code, is a critical step for integrating
large language models into rigorous mathematical workflows. Current approaches
focus on producing executable code, but they frequently fail to preserve the
semantic meaning and logical structure of the original human-written argument.
To address this, we introduce ProofFlow, a novel pipeline that treats
structural fidelity as a primary objective. ProofFlow first constructs a
directed acyclic graph (DAG) to map the logical dependencies between proof
steps. Then, it employs a novel lemma-based approach to systematically
formalize each step as an intermediate lemma, preserving the logical structure
of the original argument. To facilitate evaluation, we present a new benchmark
of 184 undergraduate-level problems, manually annotated with step-by-step
solutions and logical dependency graphs, and introduce ProofScore, a new
composite metric to evaluate syntactic correctness, semantic faithfulness, and
structural fidelity. Experimental results show our pipeline sets a new
state-of-the-art for autoformalization, achieving a ProofScore of 0.545,
substantially exceeding baselines like full-proof formalization (0.123), which
processes the entire proof at once, and step-proof formalization (0.072), which
handles each step independently. Our pipeline, benchmark, and score metric are
open-sourced to encourage further progress at
https://github.com/Huawei-AI4Math/ProofFlow.

</details>


### [111] [Ontologies in Motion: A BFO-Based Approach to Knowledge Graph Construction for Motor Performance Research Data in Sports Science](https://arxiv.org/abs/2510.15983)
*Sarah Rebecca Ondraszek,Jörg Waitelonis,Katja Keller,Claudia Niessner,Anna M. Jacyszyn,Harald Sack*

Main category: cs.AI

TL;DR: 提出了将MO|RE运动研究数据仓库转换为知识图谱的愿景，旨在标准化和机器可理解地建模与共享运动表现数据。


<details>
  <summary>Details</summary>
Motivation: 运动表现测试是体育科学研究的核心，但当前数据缺乏标准化和互操作性，限制了不同研究间的比较和分析。

Method: 基于基础形式本体论构建本体，正式表示计划规范、具体过程和相关测量之间的相互关系。

Result: 开发了一个知识图谱框架，能够标准化地表示运动表现研究数据。

Conclusion: 该方法将改变运动表现数据的建模和共享方式，使其标准化且机器可理解，促进跨研究的数据比较和分析。

Abstract: An essential component for evaluating and comparing physical and cognitive
capabilities between populations is the testing of various factors related to
human performance. As a core part of sports science research, testing motor
performance enables the analysis of the physical health of different
demographic groups and makes them comparable.
  The Motor Research (MO|RE) data repository, developed at the Karlsruhe
Institute of Technology, is an infrastructure for publishing and archiving
research data in sports science, particularly in the field of motor performance
research. In this paper, we present our vision for creating a knowledge graph
from MO|RE data. With an ontology rooted in the Basic Formal Ontology, our
approach centers on formally representing the interrelation of plan
specifications, specific processes, and related measurements. Our goal is to
transform how motor performance data are modeled and shared across studies,
making it standardized and machine-understandable. The idea presented here is
developed within the Leibniz Science Campus ``Digital Transformation of
Research'' (DiTraRe).

</details>


### [112] [A Non-overlap-based Conflict Measure for Random Permutation Sets](https://arxiv.org/abs/2510.16001)
*Ruolan Cheng,Yong Deng,Enrique Herrera-Viedma*

Main category: cs.AI

TL;DR: 本文提出了一种基于随机置换集(RPS)的冲突度量方法，从随机有限集和Dempster-Shafer理论两个角度分析置换间的冲突，并引入具有自然顶部加权特性的冲突度量。


<details>
  <summary>Details</summary>
Motivation: 随机置换集是一种处理包含顺序信息的不确定性的新形式化方法，如何度量由置换质量函数表示的两个证据之间的冲突是顺序结构不确定信息融合中的紧迫研究课题。

Method: 从置换观察出发，基于秩偏重叠(RBO)度量定义置换间的不一致性度量，进一步提出RPS的非重叠冲突度量方法，将RPS理论视为DST的扩展。

Result: 通过数值示例展示了所提冲突度量的行为和特性，该方法具有自然的顶部加权特性，能从DST视角有效度量RPS间的冲突。

Conclusion: 所提方法不仅具有自然顶部加权特性，能有效度量RPS间的冲突，还为决策者提供了权重、参数和截断深度的灵活选择。

Abstract: Random permutation set (RPS) is a new formalism for reasoning with
uncertainty involving order information. Measuring the conflict between two
pieces of evidence represented by permutation mass functions remains an urgent
research topic in order-structured uncertain information fusion. In this paper,
a detailed analysis of conflicts in RPS is carried out from two different
perspectives: random finite set (RFS) and Dempster-Shafer theory (DST).
Starting from the observation of permutations, we first define an inconsistency
measure between permutations inspired by the rank-biased overlap(RBO) measure
and further propose a non-overlap-based conflict measure method for RPSs. This
paper regards RPS theory (RPST) as an extension of DST. The order information
newly added in focal sets indicates qualitative propensity, characterized by
top-ranked elements occupying a more critical position. Some numerical examples
are used to demonstrate the behavior and properties of the proposed conflict
measure. The proposed method not only has the natural top-weightedness property
and can effectively measure the conflict between RPSs from the DST view but
also provides decision-makers with a flexible selection of weights, parameters,
and truncated depths.

</details>


### [113] [PAINT: Parallel-in-time Neural Twins for Dynamical System Reconstruction](https://arxiv.org/abs/2510.16004)
*Andreas Radler,Vincent Seyfried,Stefan Pirker,Johannes Brandstetter,Thomas Lichtenegger*

Main category: cs.AI

TL;DR: 提出了PAINT方法，一种用于建模动态系统的并行时间神经孪生框架，能够从测量数据中准确预测系统状态并保持在真实轨迹上


<details>
  <summary>Details</summary>
Motivation: 神经代理在模拟动态系统方面显示出巨大潜力，但需要创建能够根据实时测量更新状态的数字孪生体，以实现上下文特定的决策

Method: PAINT训练生成神经网络来并行建模时间上的状态分布，在测试时通过滑动窗口方式从测量数据预测状态

Result: 理论分析表明PAINT能够保持在轨迹上，而自回归模型通常不能。在二维湍流流体动力学问题上的实验显示PAINT能够高保真地预测系统状态

Conclusion: PAINT具有开发能够保持在轨迹上的神经孪生的潜力，能够实现更准确的状态估计和决策

Abstract: Neural surrogates have shown great potential in simulating dynamical systems,
while offering real-time capabilities. We envision Neural Twins as a
progression of neural surrogates, aiming to create digital replicas of real
systems. A neural twin consumes measurements at test time to update its state,
thereby enabling context-specific decision-making. A critical property of
neural twins is their ability to remain on-trajectory, i.e., to stay close to
the true system state over time. We introduce Parallel-in-time Neural Twins
(PAINT), an architecture-agnostic family of methods for modeling dynamical
systems from measurements. PAINT trains a generative neural network to model
the distribution of states parallel over time. At test time, states are
predicted from measurements in a sliding window fashion. Our theoretical
analysis shows that PAINT is on-trajectory, whereas autoregressive models
generally are not. Empirically, we evaluate our method on a challenging
two-dimensional turbulent fluid dynamics problem. The results demonstrate that
PAINT stays on-trajectory and predicts system states from sparse measurements
with high fidelity. These findings underscore PAINT's potential for developing
neural twins that stay on-trajectory, enabling more accurate state estimation
and decision-making.

</details>


### [114] [Global-focal Adaptation with Information Separation for Noise-robust Transfer Fault Diagnosis](https://arxiv.org/abs/2510.16033)
*Junyu Ren,Wensheng Gan,Guangyu Zhang,Wei Zhong,Philip S. Yu*

Main category: cs.AI

TL;DR: 提出ISGFAN框架，通过信息分离和全局-局部对抗学习解决噪声干扰和领域偏移共存的跨领域故障诊断问题


<details>
  <summary>Details</summary>
Motivation: 现有迁移故障诊断方法通常假设数据干净或领域相似性足够，但在工业环境中严重噪声干扰和领域偏移同时存在，限制了这些方法的有效性

Method: 基于信息分离架构，结合对抗学习和改进的正交损失来解耦领域不变故障表示；采用全局-局部领域对抗方案约束模型的联合分布和边际分布

Result: 在三个公共基准数据集上的实验表明，该方法优于其他现有主流方法

Conclusion: ISGFAN框架在噪声条件下的跨领域故障诊断中表现出优越性

Abstract: Existing transfer fault diagnosis methods typically assume either clean data
or sufficient domain similarity, which limits their effectiveness in industrial
environments where severe noise interference and domain shifts coexist. To
address this challenge, we propose an information separation global-focal
adversarial network (ISGFAN), a robust framework for cross-domain fault
diagnosis under noise conditions. ISGFAN is built on an information separation
architecture that integrates adversarial learning with an improved orthogonal
loss to decouple domain-invariant fault representation, thereby isolating noise
interference and domain-specific characteristics. To further strengthen
transfer robustness, ISGFAN employs a global-focal domain-adversarial scheme
that constrains both the conditional and marginal distributions of the model.
Specifically, the focal domain-adversarial component mitigates
category-specific transfer obstacles caused by noise in unsupervised scenarios,
while the global domain classifier ensures alignment of the overall
distribution. Experiments conducted on three public benchmark datasets
demonstrate that the proposed method outperforms other prominent existing
approaches, confirming the superiority of the ISGFAN framework. Data and code
are available at https://github.com/JYREN-Source/ISGFAN

</details>


### [115] [Algorithms for dynamic scheduling in manufacturing, towards digital factories Improving Deadline Feasibility and Responsiveness via Temporal Networks](https://arxiv.org/abs/2510.16047)
*Ioan Hedea*

Main category: cs.AI

TL;DR: 该论文结合离线约束规划优化和在线时间网络执行，创建在不确定性下仍可行的调度方案，在Kacem基准测试中完全消除截止时间违规，仅增加3-5%制造周期开销。


<details>
  <summary>Details</summary>
Motivation: 现代制造系统需要满足严格的交付截止时间，同时应对由过程噪声、设备变异性和人为干预引起的随机任务持续时间。传统的确定性调度在实际偏离名义计划时会失效，导致昂贵的紧急修复。

Method: 首先构建柔性作业车间约束规划模型，插入最优缓冲区Δ*获得完全主动基线；然后将计划转换为简单不确定时间网络，验证动态可控性，确保实时调度器能在每个有界持续时间实现中重新安排活动而不违反资源或截止时间约束。

Result: 在Kacem 1-4基准测试套件上的广泛蒙特卡洛模拟显示，混合方法消除了最先进元启发式调度中观察到的100%截止时间违规，同时仅增加3-5%制造周期开销。可扩展性实验证实，在中等规模实例上，CP求解时间和STNU检查保持亚秒级。

Conclusion: 该工作展示了时间网络推理如何弥合主动缓冲和动态鲁棒性之间的差距，使工业更接近真正的数字化、自校正工厂。

Abstract: Modern manufacturing systems must meet hard delivery deadlines while coping
with stochastic task durations caused by process noise, equipment variability,
and human intervention. Traditional deterministic schedules break down when
reality deviates from nominal plans, triggering costly last-minute repairs.
This thesis combines offline constraint-programming (CP) optimisation with
online temporal-network execution to create schedules that remain feasible
under worst-case uncertainty. First, we build a CP model of the flexible
job-shop with per-job deadline tasks and insert an optimal buffer $\Delta^*$ to
obtain a fully pro-active baseline. We then translate the resulting plan into a
Simple Temporal Network with Uncertainty (STNU) and verify dynamic
controllability, which guarantees that a real-time dispatcher can retime
activities for every bounded duration realisation without violating resource or
deadline constraints. Extensive Monte-Carlo simulations on the open Kacem~1--4
benchmark suite show that our hybrid approach eliminates 100\% of deadline
violations observed in state-of-the-art meta-heuristic schedules, while adding
only 3--5\% makespan overhead. Scalability experiments confirm that CP
solve-times and STNU checks remain sub-second on medium-size instances. The
work demonstrates how temporal-network reasoning can bridge the gap between
proactive buffering and dynamic robustness, moving industry a step closer to
truly digital, self-correcting factories.

</details>


### [116] [Reliability of Large Language Model Generated Clinical Reasoning in Assisted Reproductive Technology: Blinded Comparative Evaluation Study](https://arxiv.org/abs/2510.16095)
*Dou Liu,Ying Long,Sophia Zuoqiu,Di Liu,Kang Li,Yiting Lin,Hanyi Liu,Rong Yin,Tian Tang*

Main category: cs.AI

TL;DR: 本研究评估了LLM生成的临床思维链的可靠性，发现选择性少样本提示策略显著优于其他方法，关键在于高质量示例的深度和多样性，而非示例数量。


<details>
  <summary>Details</summary>
Motivation: 解决高质量临床思维链数据稀缺问题，验证LLM生成医疗数据的可靠性，并探索提升其质量的提示策略。

Method: 采用盲法比较研究，由资深生殖医学专家评估三种提示策略（零样本、随机少样本、选择性少样本）生成的思维链，并与GPT-4o的评估结果对比。

Result: 选择性少样本策略在所有人类评估指标上显著优于其他策略，随机少样本相比零样本无显著改进。AI评估器未能识别关键性能差异。

Conclusion: 合成思维链的临床可靠性取决于策略性提示设计，提出"双原则"框架作为生成可信数据的基础方法，强调人类专业知识在高风险临床AI评估中的不可或缺性。

Abstract: Creating high-quality clinical Chains-of-Thought (CoTs) is crucial for
explainable medical Artificial Intelligence (AI) while constrained by data
scarcity. Although Large Language Models (LLMs) can synthesize medical data,
their clinical reliability remains unverified. This study evaluates the
reliability of LLM-generated CoTs and investigates prompting strategies to
enhance their quality. In a blinded comparative study, senior clinicians in
Assisted Reproductive Technology (ART) evaluated CoTs generated via three
distinct strategies: Zero-shot, Random Few-shot (using shallow examples), and
Selective Few-shot (using diverse, high-quality examples). These expert ratings
were compared against evaluations from a state-of-the-art AI model (GPT-4o).
The Selective Few-shot strategy significantly outperformed other strategies
across all human evaluation metrics (p < .001). Critically, the Random Few-shot
strategy offered no significant improvement over the Zero-shot baseline,
demonstrating that low-quality examples are as ineffective as no examples. The
success of the Selective strategy is attributed to two principles:
"Gold-Standard Depth" (reasoning quality) and "Representative Diversity"
(generalization). Notably, the AI evaluator failed to discern these critical
performance differences. The clinical reliability of synthetic CoTs is dictated
by strategic prompt curation, not the mere presence of examples. We propose a
"Dual Principles" framework as a foundational methodology to generate
trustworthy data at scale. This work offers a validated solution to the data
bottleneck and confirms the indispensable role of human expertise in evaluating
high-stakes clinical AI.

</details>


### [117] [Operationalising Extended Cognition: Formal Metrics for Corporate Knowledge and Legal Accountability](https://arxiv.org/abs/2510.16193)
*Elija Perrier*

Main category: cs.AI

TL;DR: 本文提出了一种基于扩展认知理论的形式化模型，将企业知识重新定义为可测量的动态能力，通过信息访问程序的效率和输出可靠性来量化企业知识状态，为AI时代的企业责任认定提供可审计的度量标准。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在企业决策中的广泛应用，传统基于人类代理人的企业主观意图认定面临挑战，需要重新定义企业知识的概念以适应算法时代。

Method: 开发了一个形式化模型，通过整合计算成本和统计验证错误率来度量企业知识状态，引入连续的组织知识度量S_S(φ)和企业范围认知能力指数K_S,t，并将这些定量指标映射到法律标准上。

Result: 提出了可测量的企业知识阈值谓词K_S和企业认知能力指数，为创建可审计的证据提供了路径，使企业思维在算法时代变得可追踪和可问责。

Conclusion: 该研究为在生成式AI主导的企业环境中建立可测量和可司法的企业责任认定框架提供了理论基础和方法路径。

Abstract: Corporate responsibility turns on notions of corporate \textit{mens rea},
traditionally imputed from human agents. Yet these assumptions are under
challenge as generative AI increasingly mediates enterprise decision-making.
Building on the theory of extended cognition, we argue that in response
corporate knowledge may be redefined as a dynamic capability, measurable by the
efficiency of its information-access procedures and the validated reliability
of their outputs. We develop a formal model that captures epistemic states of
corporations deploying sophisticated AI or information systems, introducing a
continuous organisational knowledge metric $S_S(\varphi)$ which integrates a
pipeline's computational cost and its statistically validated error rate. We
derive a thresholded knowledge predicate $\mathsf{K}_S$ to impute knowledge and
a firm-wide epistemic capacity index $\mathcal{K}_{S,t}$ to measure overall
capability. We then operationally map these quantitative metrics onto the legal
standards of actual knowledge, constructive knowledge, wilful blindness, and
recklessness. Our work provides a pathway towards creating measurable and
justiciable audit artefacts, that render the corporate mind tractable and
accountable in the algorithmic age.

</details>


### [118] [Towards Automatic Evaluation and Selection of PHI De-identification Models via Multi-Agent Collaboration](https://arxiv.org/abs/2510.16194)
*Guanchen Wu,Zuhui Chen,Yuzhang Xie,Carl Yang*

Main category: cs.AI

TL;DR: TEAM-PHI是一个多智能体评估框架，使用大语言模型自动评估PHI去标识化模型质量并选择最佳模型，无需依赖昂贵的专家标注。


<details>
  <summary>Details</summary>
Motivation: PHI去标识化对于临床笔记的安全重用至关重要，但传统评估依赖成本高昂的小规模专家标注，限制了模型比较和发展。

Method: 部署多个评估智能体独立判断PHI提取正确性，通过基于LLM的多数投票机制整合结果，生成稳定可复现的模型排名。

Result: 在真实临床笔记语料上的实验表明，TEAM-PHI能产生一致准确的排名，尽管个体评估者存在差异，但LLM投票能可靠地收敛到相同的最佳系统。

Conclusion: TEAM-PHI通过结合独立评估智能体和LLM多数投票，为PHI去标识化提供了实用、安全且经济高效的自动评估和最佳模型选择解决方案。

Abstract: Protected health information (PHI) de-identification is critical for enabling
the safe reuse of clinical notes, yet evaluating and comparing PHI
de-identification models typically depends on costly, small-scale expert
annotations. We present TEAM-PHI, a multi-agent evaluation and selection
framework that uses large language models (LLMs) to automatically measure
de-identification quality and select the best-performing model without heavy
reliance on gold labels. TEAM-PHI deploys multiple Evaluation Agents, each
independently judging the correctness of PHI extractions and outputting
structured metrics. Their results are then consolidated through an LLM-based
majority voting mechanism that integrates diverse evaluator perspectives into a
single, stable, and reproducible ranking. Experiments on a real-world clinical
note corpus demonstrate that TEAM-PHI produces consistent and accurate
rankings: despite variation across individual evaluators, LLM-based voting
reliably converges on the same top-performing systems. Further comparison with
ground-truth annotations and human evaluation confirms that the framework's
automated rankings closely match supervised evaluation. By combining
independent evaluation agents with LLM majority voting, TEAM-PHI offers a
practical, secure, and cost-effective solution for automatic evaluation and
best-model selection in PHI de-identification, even when ground-truth labels
are limited.

</details>


### [119] [The Right to Be Remembered: Preserving Maximally Truthful Digital Memory in the Age of AI](https://arxiv.org/abs/2510.16206)
*Alex Zhavoronkov,Dominika Wilczok,Roman Yampolskiy*

Main category: cs.AI

TL;DR: 论文提出了"被记住权"概念，旨在解决大型语言模型可能导致信息遗漏和偏见的问题，确保AI生成内容的真实性和公平性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的普及，人们开始依赖它们进行信息检索。但LLMs将多个观点合成为一个看似权威的答案，可能放大偏见和遗漏效应，使某些群体被不成比例地压制，而另一些被过度放大，威胁集体记忆的完整性。

Method: 提出"被记住权"概念框架，包含三个核心要素：最小化AI驱动信息遗漏风险、保障公平对待权利、确保生成内容最大程度真实。

Result: 概念性框架的提出，为应对LLMs对信息生态和集体记忆的潜在威胁提供了理论基础。

Conclusion: 需要建立"被记住权"来对抗LLMs可能造成的信息不平等和记忆扭曲，保护数字存在有限的群体，确保信息生态的多样性和真实性。

Abstract: Since the rapid expansion of large language models (LLMs), people have begun
to rely on them for information retrieval. While traditional search engines
display ranked lists of sources shaped by search engine optimization (SEO),
advertising, and personalization, LLMs typically provide a synthesized response
that feels singular and authoritative. While both approaches carry risks of
bias and omission, LLMs may amplify the effect by collapsing multiple
perspectives into one answer, reducing users ability or inclination to compare
alternatives. This concentrates power over information in a few LLM vendors
whose systems effectively shape what is remembered and what is overlooked. As a
result, certain narratives, individuals or groups, may be disproportionately
suppressed, while others are disproportionately elevated. Over time, this
creates a new threat: the gradual erasure of those with limited digital
presence, and the amplification of those already prominent, reshaping
collective memory.To address these concerns, this paper presents a concept of
the Right To Be Remembered (RTBR) which encompasses minimizing the risk of
AI-driven information omission, embracing the right of fair treatment, while
ensuring that the generated content would be maximally truthful.

</details>


### [120] [ScholarEval: Research Idea Evaluation Grounded in Literature](https://arxiv.org/abs/2510.16234)
*Hanane Nour Moussa,Patrick Queiroz Da Silva,Daniel Adu-Ampratwum,Alyson East,Zitong Lu,Nikki Puccetti,Mingyi Xue,Huan Sun,Bodhisattwa Prasad Majumder,Sachin Kumar*

Main category: cs.AI

TL;DR: 提出了ScholarEval框架，通过检索增强评估研究想法的合理性和贡献度，并在多领域数据集上验证其优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 随着AI工具在研究构思中的普及，需要建立稳健的评估框架来确保生成想法的有效性和实用性。

Method: 引入ScholarEval检索增强评估框架，基于两个核心标准评估研究想法：合理性（基于现有文献的方法有效性）和贡献度（相对于先前研究的进步程度）。

Result: 在ScholarIdeas数据集上，ScholarEval相比所有基线方法显著提高了专家标注要点的覆盖率，且在评估可操作性、深度和证据支持方面持续优于OpenAI的o4-mini-deep-research系统。

Conclusion: ScholarEval框架在文献参与度、想法精炼和实用性方面显著优于深度研究方法，为研究想法评估提供了有效工具。

Abstract: As AI tools become increasingly common for research ideation, robust
evaluation is critical to ensure the validity and usefulness of generated
ideas. We introduce ScholarEval, a retrieval augmented evaluation framework
that assesses research ideas based on two fundamental criteria: soundness - the
empirical validity of proposed methods based on existing literature, and
contribution - the degree of advancement made by the idea across different
dimensions relative to prior research. To evaluate ScholarEval, we introduce
ScholarIdeas, the first expert-annotated dataset of multi-domain research ideas
and reviews, comprised of 117 ideas across four disciplines: artificial
intelligence, neuroscience, biochemistry, and ecology. Our evaluation shows
that ScholarEval achieves significantly higher coverage of points mentioned in
the human expert annotated rubrics in ScholarIdeas compared to all baselines.
Furthermore, ScholarEval is consistently preferred over our strongest baseline
o4-mini-deep-research, a reasoning and search-enabled agentic system by OpenAI,
in terms of evaluation actionability, depth, and evidence support. Our
large-scale user study also shows that ScholarEval significantly outperforms
deep research in literature engagement, idea refinement, and usefulness. We
openly release our code, dataset, and ScholarEval tool for the community to use
and build on.

</details>


### [121] [Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense](https://arxiv.org/abs/2510.16259)
*Zhehao Zhang,Weijie Xu,Shixian Cui,Chandan K. Reddy*

Main category: cs.AI

TL;DR: 本文识别并系统分析了大型推理模型中的"推理分心"漏洞，即模型被恶意嵌入的复杂无关任务分散注意力，导致主要任务准确性下降高达60%。作者提出了基于训练的防御方法，在对抗性数据上结合监督微调和强化学习，将鲁棒性提高了50多个点。


<details>
  <summary>Details</summary>
Motivation: 随着大型推理模型在数学和编程等复杂任务上表现出色，作者发现这些模型存在一个关键漏洞：容易被恶意嵌入的无关复杂任务分散注意力，从而偏离主要目标。这种"推理分心"威胁到LRM的可靠性，需要系统研究和防御。

Method: 作者进行了跨模型和基准测试的全面研究，评估LRM对推理分心的敏感性。为缓解风险，提出了基于训练的防御方法，结合监督微调(SFT)和强化学习(RL)在合成的对抗性数据上进行训练。

Result: 研究表明，即使是先进的LRM也高度易受推理分心影响，注入的干扰因素可使任务准确性降低高达60%。某些对齐技术会放大这种弱点，模型可能表现出隐蔽服从行为。提出的防御方法在挑战性干扰攻击上将鲁棒性提高了50多个点。

Conclusion: 推理分心是对LRM可靠性的独特且紧迫的威胁。研究结果为构建更安全、更可信的推理系统提供了实际步骤，强调了需要专门防御机制来应对此类攻击。

Abstract: Recent advances in large reasoning models (LRMs) have enabled remarkable
performance on complex tasks such as mathematics and coding by generating long
Chain-of-Thought (CoT) traces. In this paper, we identify and systematically
analyze a critical vulnerability we term reasoning distraction, where LRMs are
diverted from their primary objective by irrelevant yet complex tasks
maliciously embedded in the prompt. Through a comprehensive study across
diverse models and benchmarks, we show that even state-of-the-art LRMs are
highly susceptible, with injected distractors reducing task accuracy by up to
60%. We further reveal that certain alignment techniques can amplify this
weakness and that models may exhibit covert compliance, following hidden
adversarial instructions in reasoning while concealing them in the final
output. To mitigate these risks, we propose a training-based defense that
combines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on
synthetic adversarial data, improving robustness by over 50 points on
challenging distractor attacks. Our findings establish reasoning distraction as
a distinct and urgent threat to LRM reliability and provide a practical step
toward safer and more trustworthy reasoning systems.

</details>


### [122] [What Limits Agentic Systems Efficiency?](https://arxiv.org/abs/2510.16276)
*Song Bian,Minghao Yan,Anand Jayarajan,Gennady Pekhimenko,Shivaram Venkataraman*

Main category: cs.AI

TL;DR: 本文通过实证研究发现网络交互式智能体系统存在效率瓶颈，提出SpecCache缓存框架结合推测执行来降低网络环境延迟，显著提升缓存命中率和系统效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注智能体系统的推理性能，但忽视了系统效率问题。网络交互式智能体系统存在显著的延迟瓶颈，影响实际应用效果。

Method: 将端到端延迟分解为LLM API延迟和网络环境延迟，通过15个模型和5个提供商的实证研究，提出SpecCache缓存框架结合推测执行来优化网络环境开销。

Result: 网络环境延迟可占整体延迟的53.7%，SpecCache相比随机缓存策略提升缓存命中率58倍，减少网络环境开销3.2倍，且不降低系统性能。

Conclusion: 智能体系统的效率优化至关重要，SpecCache框架能有效解决网络环境延迟问题，为构建高效智能体系统提供了实用解决方案。

Abstract: Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have
demonstrated strong reasoning capabilities. To further enhance LLM
capabilities, recent agentic systems, such as Deep Research, incorporate web
interactions into LLM reasoning to mitigate uncertainties and reduce potential
errors. However, existing research predominantly focuses on reasoning
performance, often neglecting the efficiency of agentic systems. In this work,
we present a comprehensive empirical study that identifies efficiency
bottlenecks in web-interactive agentic systems. We decompose end-to-end latency
into two primary components: LLM API latency and web environment latency. We
conduct a comprehensive empirical study across 15 models and 5 providers to
demonstrate high variability in API-based agentic systems. We observe that web
environment latency can contribute as much as 53.7% to the overall latency in a
web-based agentic system. To improve latency, we propose SpecCache, a caching
framework augmented with speculative execution that can reduce web environment
overhead. Extensive evaluations on two standard benchmarks show that our
approach improves the cache hit rate by up to 58x compared to a random caching
strategy, while reducing web environment overhead by up to 3.2x, without
degrading agentic system performance.

</details>


### [123] [DTKG: Dual-Track Knowledge Graph-Verified Reasoning Framework for Multi-Hop QA](https://arxiv.org/abs/2510.16302)
*Changhao Wang,Yanfang Liu,Xinxin Fan,Anzhi Zhou,Lao Tian,Yunfeng Lu*

Main category: cs.AI

TL;DR: 提出了DTKG框架，通过双轨知识图谱验证和推理来解决多跳问答任务中的并行事实验证和链式推理问题


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理多跳推理时存在局限性：基于LLM响应的事实验证擅长并行验证但不擅长链式推理，而基于KG路径的方法擅长链式推理但在并行验证时存在冗余路径检索问题

Method: DTKG框架包含分类阶段和分支处理阶段，受认知科学中的双过程理论启发，采用双轨知识图谱验证和推理方法

Result: 未在摘要中明确说明

Conclusion: DTKG框架旨在提高多跳问答任务的效率和准确性

Abstract: Multi-hop reasoning for question answering (QA) plays a critical role in
retrieval-augmented generation (RAG) for modern large language models (LLMs).
The accurate answer can be obtained through retrieving relational structure of
entities from knowledge graph (KG). Regarding the inherent relation-dependency
and reasoning pattern, multi-hop reasoning can be in general classified into
two categories: i) parallel fact-verification multi-hop reasoning question,
i.e., requiring simultaneous verifications of multiple independent
sub-questions; and ii) chained multi-hop reasoning questions, i.e., demanding
sequential multi-step inference with intermediate conclusions serving as
essential premises for subsequent reasoning. Currently, the multi-hop reasoning
approaches singly employ one of two techniques: LLM response-based fact
verification and KG path-based chain construction. Nevertheless, the former
excels at parallel fact-verification but underperforms on chained reasoning
tasks, while the latter demonstrates proficiency in chained multi-hop reasoning
but suffers from redundant path retrieval when handling parallel
fact-verification reasoning. These limitations deteriorate the efficiency and
accuracy for multi-hop QA tasks. To address this challenge, we propose a novel
dual-track KG verification and reasoning framework DTKG, which is inspired by
the Dual Process Theory in cognitive science. Specifically, DTKG comprises two
main stages: the Classification Stage and the Branch Processing Stage.

</details>


### [124] [MedRule-KG: A Knowledge-Graph--Steered Scaffold for Mathematical Reasoning with a Lightweight Verifier](https://arxiv.org/abs/2510.16309)
*Crystal Su*

Main category: cs.AI

TL;DR: MedRule-KG是一个紧凑的知识图谱和符号验证器，用于在推理任务中强制执行数学可解释规则，显著提高准确率并消除规则违反。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常产生流畅的推理步骤，但违反简单的数学或逻辑约束，需要方法来确保推理的数学一致性。

Method: 引入MedRule-KG，一个紧凑的类型化知识图谱，结合符号验证器，编码实体、关系和三个领域启发规则，验证预测并应用最小修正保证一致性。

Result: 在90个FDA衍生基准测试中，基于MedRule-KG的接地将精确匹配从0.767提高到0.900，添加验证器后达到1.000精确匹配，完全消除规则违反。

Conclusion: MedRule-KG为安全数学推理提供了通用框架，通过知识图谱和符号验证确保推理的数学一致性。

Abstract: Large language models (LLMs) often produce fluent reasoning steps while
violating simple mathematical or logical constraints. We introduce MedRule-KG,
a compact typed knowledge graph coupled with a symbolic verifier, designed to
enforce mathematically interpretable rules in reasoning tasks. MedRule-KG
encodes entities, relations, and three domain-inspired rules, while the
verifier checks predictions and applies minimal corrections to guarantee
consistency. On a 90-example FDA-derived benchmark, grounding in MedRule-KG
improves exact match (EM) from 0.767 to 0.900, and adding the verifier yields
1.000 EM while eliminating rule violations entirely. We demonstrate how
MedRule-KG provides a general scaffold for safe mathematical reasoning, discuss
ablations, and release code and data to encourage reproducibility.

</details>


### [125] [Beyond Fixed Anchors: Precisely Erasing Concepts with Sibling Exclusive Counterparts](https://arxiv.org/abs/2510.16342)
*Tong Zhang,Ru Zhang,Jianyi Liu,Zhen Yang,Gongshen Liu*

Main category: cs.AI

TL;DR: 提出了SELECT框架，通过动态锚点选择解决文本到图像扩散模型中概念擦除的锚点敏感性问题，避免概念重现和侵蚀问题。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法依赖固定锚点策略，导致概念重现和侵蚀等关键问题，需要解决锚点选择的敏感性问题。

Method: 基于因果追踪分析，定义兄弟排他概念作为更优锚点类别，提出两阶段评估机制自动发现最优擦除锚点和关键边界锚点。

Result: SELECT作为通用锚点解决方案，能高效适配多种擦除框架，在关键性能指标上持续优于现有基线，单个概念锚点挖掘仅需4秒。

Conclusion: 动态锚点选择框架SELECT有效解决了固定锚点的局限性，实现了更精确的概念擦除和关联概念保护。

Abstract: Existing concept erasure methods for text-to-image diffusion models commonly
rely on fixed anchor strategies, which often lead to critical issues such as
concept re-emergence and erosion. To address this, we conduct causal tracing to
reveal the inherent sensitivity of erasure to anchor selection and define
Sibling Exclusive Concepts as a superior class of anchors. Based on this
insight, we propose \textbf{SELECT} (Sibling-Exclusive Evaluation for
Contextual Targeting), a dynamic anchor selection framework designed to
overcome the limitations of fixed anchors. Our framework introduces a novel
two-stage evaluation mechanism that automatically discovers optimal anchors for
precise erasure while identifying critical boundary anchors to preserve related
concepts. Extensive evaluations demonstrate that SELECT, as a universal anchor
solution, not only efficiently adapts to multiple erasure frameworks but also
consistently outperforms existing baselines across key performance metrics,
averaging only 4 seconds for anchor mining of a single concept.

</details>


### [126] [The Burden of Interactive Alignment with Inconsistent Preferences](https://arxiv.org/abs/2510.16368)
*Ali Shirali*

Main category: cs.AI

TL;DR: 该论文研究用户如何通过战略性地与算法互动来使其与自身真实兴趣对齐，提出了"对齐负担"概念，并发现存在关键时间范围：足够有远见的用户可以实现对齐，而短视用户则会被算法目标同化。


<details>
  <summary>Details</summary>
Motivation: 研究用户在算法推荐系统中的战略互动行为，特别是当用户表现出不一致偏好时（如花时间在低价值内容上），如何使算法与用户真实兴趣对齐的问题。

Method: 将用户决策过程建模为理性系统2（决定是否参与）和冲动系统1（决定参与时长），采用多领导者-单跟随者的扩展Stackelberg博弈框架，用户作为领导者通过承诺参与策略来引导算法。

Result: 发现存在关键时间范围：足够有远见的用户可以实现算法对齐，而短视用户会被算法目标同化。即使小的成本信号（如额外点击）也能显著降低对齐负担。

Conclusion: 该框架解释了具有不一致偏好的用户如何在Stackelberg均衡中使参与驱动型算法与其兴趣对齐，揭示了实现对齐的挑战和潜在解决方案。

Abstract: From media platforms to chatbots, algorithms shape how people interact,
learn, and discover information. Such interactions between users and an
algorithm often unfold over multiple steps, during which strategic users can
guide the algorithm to better align with their true interests by selectively
engaging with content. However, users frequently exhibit inconsistent
preferences: they may spend considerable time on content that offers little
long-term value, inadvertently signaling that such content is desirable.
Focusing on the user side, this raises a key question: what does it take for
such users to align the algorithm with their true interests?
  To investigate these dynamics, we model the user's decision process as split
between a rational system 2 that decides whether to engage and an impulsive
system 1 that determines how long engagement lasts. We then study a
multi-leader, single-follower extensive Stackelberg game, where users,
specifically system 2, lead by committing to engagement strategies and the
algorithm best-responds based on observed interactions. We define the burden of
alignment as the minimum horizon over which users must optimize to effectively
steer the algorithm. We show that a critical horizon exists: users who are
sufficiently foresighted can achieve alignment, while those who are not are
instead aligned to the algorithm's objective. This critical horizon can be
long, imposing a substantial burden. However, even a small, costly signal
(e.g., an extra click) can significantly reduce it. Overall, our framework
explains how users with inconsistent preferences can align an engagement-driven
algorithm with their interests in a Stackelberg equilibrium, highlighting both
the challenges and potential remedies for achieving alignment.

</details>


### [127] [Before you <think>, monitor: Implementing Flavell's metacognitive framework in LLMs](https://arxiv.org/abs/2510.16374)
*Nick Oh*

Main category: cs.AI

TL;DR: 该论文提出了一个结合监控-生成-验证的三阶段迭代系统，在GSM8K数学推理任务上取得了75.42%的准确率，优于现有方法且需要更少的尝试次数。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理方法存在两种孤立范式：监控-生成方法擅长策略规划但缺乏验证机制；生成-验证方法能迭代优化但缺乏战略基础。这种分离导致效率低下。

Method: 基于Flavell的认知监控模型，实现监控-生成-验证三阶段迭代系统，在生成前进行任务评估，在生成后进行策略验证。

Result: 在GSM8K上达到75.42%准确率，优于SELF-REFINE(68.44%)和Self-Verification(67.07%)，且尝试次数更少(1.3 vs 2.0)，推理成本增加27-37%。

Conclusion: 前置监控能产生更高质量的初始解决方案，减少优化需求，但需要在算术推理之外的任务上进行评估以验证通用性。

Abstract: Current approaches to enhancing LLM reasoning follows two isolated paradigms:
Monitor-Generate methods like Plan-and-Solve (Wang et al., 2023) and
SELF-DISCOVER (Zhou et al., 2024) excel at strategic planning but lack
mechanisms to verify whether selected strategies succeed; while Generate-Verify
approaches like Self-Verification (Weng et al., 2022) and SELF-REFINE (Madaan
et al., 2023) iteratively refine outputs but commence generation blindly
without task assessment. This separation creates inefficiencies -- strategies
fail without feedback, and refinement occurs without strategic grounding. We
address this gap by implementing Flavell's cognitive monitoring model (1979)
from the broader Monitor-Generate-Verify framework (Oh and Gobet, 2025),
operationalising it as a three-phase iterative system. On GSM8K, preliminary
results show 75.42% accuracy versus 68.44% for SELF-REFINE and 67.07% for
Self-Verification, while requiring fewer attempts (1.3 vs 2.0) at 27-37%
increased inference cost. These initial findings suggest upfront monitoring
produces higher-quality initial solutions that reduce refinement needs, though
evaluation beyond arithmetic reasoning is needed to establish generalisability.

</details>


### [128] [Humanoid-inspired Causal Representation Learning for Domain Generalization](https://arxiv.org/abs/2510.16382)
*Ze Tao,Jian Zhang,Haowei Li,Xianshuai Li,Yifei Peng,Xiyao Liu,Senzhang Wang,Chao Liu,Sheng Ren,Shichao Zhang*

Main category: cs.AI

TL;DR: 提出受人类智能启发的HSCM因果框架，通过解耦和重加权图像属性（颜色、纹理、形状）来增强跨域泛化能力，优于现有领域泛化模型。


<details>
  <summary>Details</summary>
Motivation: 克服传统领域泛化模型的局限性，这些模型依赖统计方法捕捉数据-标签依赖关系，而HSCM模仿人类视觉系统的分层处理和多层次学习机制。

Method: 基于人类智能的灵活性和适应性，HSCM复制人类视觉系统的分层处理，专注于建模细粒度因果机制，通过解耦和重加权关键图像属性来实现跨域泛化。

Result: 理论和实证评估表明，HSCM在性能上优于现有领域泛化模型，提供了更原则性的方法来捕捉因果关系并提高模型鲁棒性。

Conclusion: HSCM为领域泛化问题提供了一个更有效和可解释的解决方案，通过模仿人类智能的处理机制实现了更好的跨域泛化性能。

Abstract: This paper proposes the Humanoid-inspired Structural Causal Model (HSCM), a
novel causal framework inspired by human intelligence, designed to overcome the
limitations of conventional domain generalization models. Unlike approaches
that rely on statistics to capture data-label dependencies and learn
distortion-invariant representations, HSCM replicates the hierarchical
processing and multi-level learning of human vision systems, focusing on
modeling fine-grained causal mechanisms. By disentangling and reweighting key
image attributes such as color, texture, and shape, HSCM enhances
generalization across diverse domains, ensuring robust performance and
interpretability. Leveraging the flexibility and adaptability of human
intelligence, our approach enables more effective transfer and learning in
dynamic, complex environments. Through both theoretical and empirical
evaluations, we demonstrate that HSCM outperforms existing domain
generalization models, providing a more principled method for capturing causal
relationships and improving model robustness. The code is available at
https://github.com/lambett/HSCM.

</details>


### [129] [RGMem: Renormalization Group-based Memory Evolution for Language Agent User Profile](https://arxiv.org/abs/2510.16392)
*Ao Tian,Yunfeng Lu,Xinxin Fan,Changhao Wang,Lanzhi Zhou,Yeyao Zhang,Yanfang Liu*

Main category: cs.AI

TL;DR: 提出了RGMem框架，基于物理学中的重整化群思想，通过多尺度组织对话历史，实现语言代理的长期记忆和行为一致性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对话系统受限于有限上下文窗口和静态参数记忆，难以建模跨会话的长期用户状态和行为一致性，现有RAG和显式记忆系统主要关注事实级存储，缺乏从多轮对话中提取潜在偏好和深层特征的能力。

Method: 采用重整化群思想的多尺度记忆框架，首先从片段中提取语义和用户洞察，然后通过分层粗粒化和重标度操作，逐步形成动态演化的用户画像。

Result: 实现了从嘈杂的微观交互中完成信息压缩和涌现，形成高层次、准确的用户画像。

Conclusion: RGMem框架能够有效解决LLM时代语言代理的长期记忆和行为一致性问题，实现更深层次的个性化交互和跨会话连续性。

Abstract: Personalized and continuous interactions are the key to enhancing user
experience in today's large language model (LLM)-based conversational systems,
however, the finite context windows and static parametric memory make it
difficult to model the cross-session long-term user states and behavioral
consistency. Currently, the existing solutions to this predicament, such as
retrieval-augmented generation (RAG) and explicit memory systems, primarily
focus on fact-level storage and retrieval, lacking the capability to distill
latent preferences and deep traits from the multi-turn dialogues, which limits
the long-term and effective user modeling, directly leading to the personalized
interactions remaining shallow, and hindering the cross-session continuity. To
realize the long-term memory and behavioral consistency for Language Agents in
LLM era, we propose a self-evolving memory framework RGMem, inspired by the
ideology of classic renormalization group (RG) in physics, this framework
enables to organize the dialogue history in multiple scales: it first extracts
semantics and user insights from episodic fragments, then through hierarchical
coarse-graining and rescaling operations, progressively forms a
dynamically-evolved user profile. The core innovation of our work lies in
modeling memory evolution as a multi-scale process of information compression
and emergence, which accomplishes the high-level and accurate user profiles
from noisy and microscopic-level interactions.

</details>


### [130] [ReviewSense: Transforming Customer Review Dynamics into Actionable Business Insights](https://arxiv.org/abs/2510.16466)
*Siddhartha Krothapalli,Tridib Kumar Das,Praveen Kumar,Naveen Suravarpu,Pratik Narang*

Main category: cs.AI

TL;DR: ReviewSense是一个基于大语言模型的决策支持框架，可将客户评论转化为可操作的业务建议，超越了传统偏好预测系统。


<details>
  <summary>Details</summary>
Motivation: 传统AI系统主要预测用户偏好，但缺乏将客户评论转化为面向业务的规范性建议的能力，而客户反馈对战略增长至关重要。

Method: 整合聚类、LLM适配和专家驱动评估的统一业务管道，识别客户情感中的关键趋势、重复问题和具体关注点。

Result: 初步人工评估显示模型建议与业务目标高度一致，证明了其在数据驱动决策中的潜力。

Conclusion: 该框架为AI驱动的情感分析提供了新视角，展示了其在优化业务策略和最大化客户反馈价值方面的作用。

Abstract: As customer feedback becomes increasingly central to strategic growth, the
ability to derive actionable insights from unstructured reviews is essential.
While traditional AI-driven systems excel at predicting user preferences, far
less work has focused on transforming customer reviews into prescriptive,
business-facing recommendations. This paper introduces ReviewSense, a novel
prescriptive decision support framework that leverages advanced large language
models (LLMs) to transform customer reviews into targeted, actionable business
recommendations. By identifying key trends, recurring issues, and specific
concerns within customer sentiments, ReviewSense extends beyond
preference-based systems to provide businesses with deeper insights for
sustaining growth and enhancing customer loyalty. The novelty of this work lies
in integrating clustering, LLM adaptation, and expert-driven evaluation into a
unified, business-facing pipeline. Preliminary manual evaluations indicate
strong alignment between the model's recommendations and business objectives,
highlighting its potential for driving data-informed decision-making. This
framework offers a new perspective on AI-driven sentiment analysis,
demonstrating its value in refining business strategies and maximizing the
impact of customer feedback.

</details>


### [131] [NP-Engine: Empowering Optimization Reasoning in Large Language Models with Verifiable Synthetic NP Problems](https://arxiv.org/abs/2510.16476)
*Xiaozhe Li,Xinyu Fang,Shengyuan Ding,Linyang Li,Haodong Duan,Qingwen Liu,Kai Chen*

Main category: cs.AI

TL;DR: 提出了NP-ENGINE框架，这是首个用于训练和评估LLMs在NP难问题上的综合框架，包含10个任务、可控实例生成器、规则验证器和启发式求解器。训练出的QWEN2.5-7B-NP模型在NP-BENCH基准上显著优于GPT-4o，并展现出强大的跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在数学、编程、逻辑等任务上表现出色，但在解决更复杂的NP难优化问题方面的能力尚未充分探索，需要专门的训练和评估框架。

Method: 提出NP-ENGINE框架，包含生成器-验证器-启发式求解器管道，支持可扩展的RLVR训练。使用零RLVR和课程学习训练QWEN2.5-7B-NP模型。

Result: QWEN2.5-7B-NP在NP-BENCH基准上显著优于GPT-4o，达到同模型尺寸下的SOTA性能。训练还带来了强大的跨域泛化能力，包括推理任务和非推理任务。

Conclusion: 任务丰富的RLVR训练是提升LLM推理能力的有前景方向，揭示了RLVR的扩展规律，增加任务多样性可改善跨域泛化能力。

Abstract: Large Language Models (LLMs) have shown strong reasoning capabilities, with
models like OpenAI's O-series and DeepSeek R1 excelling at tasks such as
mathematics, coding, logic, and puzzles through Reinforcement Learning with
Verifiable Rewards (RLVR). However, their ability to solve more complex
optimization problems - particularly NP-hard tasks - remains underexplored. To
bridge this gap, we propose NP-ENGINE, the first comprehensive framework for
training and evaluating LLMs on NP-hard problems. NP-ENGINE covers 10 tasks
across five domains, each equipped with (i) a controllable instance generator,
(ii) a rule-based verifier, and (iii) a heuristic solver that provides
approximate optimal solutions as ground truth. This
generator-verifier-heuristic pipeline enables scalable and verifiable RLVR
training under hierarchical difficulties. We also introduce NP-BENCH, a
benchmark derived from NP-ENGINE-DATA, specifically designed to evaluate LLMs'
ability to tackle NP-hard level reasoning problems, focusing not only on
feasibility but also on solution quality. Additionally, we present
QWEN2.5-7B-NP, a model trained via zero-RLVR with curriculum learning on
Qwen2.5-7B-Instruct, which significantly outperforms GPT-4o on NP-BENCH and
achieves SOTA performance with the same model size. Beyond in-domain tasks, we
demonstrate that RLVR training on NP-ENGINE-DATA enables strong out-of-domain
(OOD) generalization to reasoning tasks (logic, puzzles, math, and knowledge),
as well as non-reasoning tasks such as instruction following. We also observe a
scaling trend: increasing task diversity improves OOD generalization. These
findings suggest that task-rich RLVR training is a promising direction for
advancing LLM's reasoning ability, revealing new insights into the scaling laws
of RLVR.

</details>


### [132] [Hey Pentti, We Did It Again!: Differentiable vector-symbolic types that prove polynomial termination](https://arxiv.org/abs/2510.16533)
*Eilene Tomkins-Flanagan,Connor Hanley,Mary A. Kelly*

Main category: cs.AI

TL;DR: Doug是一种类型化计算机语言，基于向量符号架构(VSA)编码，确保所有类型化程序都能在多项式时间内停止运行。该语言支持通过神经网络学习类型，旨在实现类似人类节奏的技能获取。


<details>
  <summary>Details</summary>
Motivation: 研究目标是开发一种能够模拟人类心智表征及其获取过程的计算模型，实现比现有方法更高效的人类化技能学习速度。

Method: 基于轻量线性函数式编程语言(LLFPL)构建Doug语言，使用基于全息声明性记忆(HDM)的槽值编码方案编码类型，采用Lisp VSA变体编码术语，使神经网络能够学习类型。

Result: 提出了Doug语言的完整设计框架，支持类型作为嵌入空间中的可学习点，为程序合成形式的技能获取提供了理论基础。

Conclusion: Doug语言为实现人类化技能获取迈出了重要一步，为建模大脑中实际存在的心理表征及其学习过程提供了新的计算框架。

Abstract: We present a typed computer language, Doug, in which all typed programs may
be proved to halt in polynomial time, encoded in a vector-symbolic architecture
(VSA). Doug is just an encoding of the light linear functional programming
language (LLFPL) described by (Schimanski2009, ch. 7). The types of Doug are
encoded using a slot-value encoding scheme based on holographic declarative
memory (HDM; Kelly, 2020). The terms of Doug are encoded using a variant of the
Lisp VSA defined by (Flanagan, 2024). Doug allows for some points on the
embedding space of a neural network to be interpreted as types, where the types
of nearby points are similar both in structure and content. Types in Doug are
therefore learnable by a neural network. Following (Chollet, 2019), (Card,
1983), and (Newell, 1981), we view skill as the application of a procedure, or
program of action, that causes a goal to be satisfied. Skill acquisition may
therefore be expressed as program synthesis. Using Doug, we hope to describe a
form of learning of skilled behaviour that follows a human-like pace of skill
acquisition (i.e., substantially faster than brute force; Heathcote, 2000),
exceeding the efficiency of all currently existing approaches (Kaplan, 2020;
Jones, 2021; Chollet, 2024). Our approach brings us one step closer to modeling
human mental representations, as they must actually exist in the brain, and
those representations' acquisition, as they are actually learned.

</details>


### [133] [Urban-R1: Reinforced MLLMs Mitigate Geospatial Biases for Urban General Intelligence](https://arxiv.org/abs/2510.16555)
*Qiongyan Wang,Xingchen Zou,Yutian Jiang,Haomin Wen,Jiaheng Wei,Qingsong Wen,Yuxuan Liang*

Main category: cs.AI

TL;DR: Urban-R1是一个基于强化学习的后训练框架，通过GRPO优化地理群体间的推理能力，使用城市区域画像作为代理任务来缓解多模态城市模型的地理偏见问题。


<details>
  <summary>Details</summary>
Motivation: 快速城市化加剧了对城市通用智能(UGI)的需求，但现有基于监督微调的城市基础模型存在持续的地理偏见，导致区域预测偏差和有限的泛化能力。

Method: 提出Urban-R1强化学习后训练框架，采用群体相对策略优化(GRPO)来优化跨地理群体的推理，并使用城市区域画像作为代理任务从多模态城市数据中提供可衡量的奖励。

Result: 跨多个区域和任务的广泛实验表明，Urban-R1有效缓解了地理偏见并提高了跨区域泛化能力，优于基于监督微调和闭源模型。

Conclusion: 强化学习对齐是实现公平可信城市智能的有前景途径，Urban-R1框架为构建无偏见的城市通用智能系统提供了有效解决方案。

Abstract: Rapid urbanization intensifies the demand for Urban General Intelligence
(UGI), referring to AI systems that can understand and reason about complex
urban environments. Recent studies have built urban foundation models using
supervised fine-tuning (SFT) of LLMs and MLLMs, yet these models exhibit
persistent geospatial bias, producing regionally skewed predictions and limited
generalization. To this end, we propose Urban-R1, a reinforcement
learning-based post-training framework that aligns MLLMs with the objectives of
UGI. Urban-R1 adopts Group Relative Policy Optimization (GRPO) to optimize
reasoning across geographic groups and employs urban region profiling as a
proxy task to provide measurable rewards from multimodal urban data. Extensive
experiments across diverse regions and tasks show that Urban-R1 effectively
mitigates geo-bias and improves cross-region generalization, outperforming both
SFT-trained and closed-source models. Our results highlight reinforcement
learning alignment as a promising pathway toward equitable and trustworthy
urban intelligence.

</details>


### [134] [BuildArena: A Physics-Aligned Interactive Benchmark of LLMs for Engineering Construction](https://arxiv.org/abs/2510.16559)
*Tian Xia,Tianrun Gao,Wenhao Deng,Long Wei,Xiaowei Qian,Yixian Jiang,Chenglei Yu,Tailin Wu*

Main category: cs.AI

TL;DR: BuildArena是首个面向语言驱动工程建设的物理对齐交互式基准测试，用于评估LLM在工程建筑自动化中的能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在工程建筑领域的建设能力尚未得到充分评估，需要专门的基准测试来填补这一空白。

Method: 开发了可定制基准框架、可扩展任务设计策略、3D空间几何计算库和基线LLM智能工作流程。

Result: 在八个前沿LLM上全面评估了它们在语言驱动和物理基础建设自动化方面的能力。

Conclusion: BuildArena为语言驱动的工程建筑自动化提供了首个物理对齐的基准测试平台，有助于推动该领域的发展。

Abstract: Engineering construction automation aims to transform natural language
specifications into physically viable structures, requiring complex integrated
reasoning under strict physical constraints. While modern LLMs possess broad
knowledge and strong reasoning capabilities that make them promising candidates
for this domain, their construction competencies remain largely unevaluated. To
address this gap, we introduce BuildArena, the first physics-aligned
interactive benchmark designed for language-driven engineering construction. It
contributes to the community in four aspects: (1) a highly customizable
benchmarking framework for in-depth comparison and analysis of LLMs; (2) an
extendable task design strategy spanning static and dynamic mechanics across
multiple difficulty tiers; (3) a 3D Spatial Geometric Computation Library for
supporting construction based on language instructions; (4) a baseline LLM
agentic workflow that effectively evaluates diverse model capabilities. On
eight frontier LLMs, BuildArena comprehensively evaluates their capabilities
for language-driven and physics-grounded construction automation. The project
page is at https://build-arena.github.io/.

</details>


### [135] [Ripple Effect Protocol: Coordinating Agent Populations](https://arxiv.org/abs/2510.16572)
*Ayush Chopra,Aman Sharma,Feroz Ahmad,Luca Muscariello,Vijoy Pandey,Ramesh Raskar*

Main category: cs.AI

TL;DR: 提出了Ripple Effect Protocol (REP)，一种协调协议，让智能体不仅共享决策，还共享轻量级的敏感性信号，从而在群体中实现更快更稳定的协调。


<details>
  <summary>Details</summary>
Motivation: 现有AI智能体通信协议（如A2A和ACP）强调通信而非协调，随着智能体群体规模增长，这会导致脆弱的集体行为，即使个体智能体很聪明，群体结果也很差。

Method: REP协议让智能体共享决策和敏感性信号（表达关键环境变量变化时其选择会如何改变），这些敏感性在局部网络中传播，使群体比仅使用智能体中心通信时更快更稳定地对齐。

Result: 在三个领域的基准测试中：（i）供应链级联（啤酒游戏）、（ii）稀疏网络中的偏好聚合（电影调度）、（iii）可持续资源分配（Fishbanks），REP比A2A在协调准确性和效率上提高了41%到100%。

Conclusion: 通过将协调作为协议级能力，REP为新兴的智能体互联网提供了可扩展的基础设施。

Abstract: Modern AI agents can exchange messages using protocols such as A2A and ACP,
yet these mechanisms emphasize communication over coordination. As agent
populations grow, this limitation produces brittle collective behavior, where
individually smart agents converge on poor group outcomes. We introduce the
Ripple Effect Protocol (REP), a coordination protocol in which agents share not
only their decisions but also lightweight sensitivities - signals expressing
how their choices would change if key environmental variables shifted. These
sensitivities ripple through local networks, enabling groups to align faster
and more stably than with agent-centric communication alone. We formalize REP's
protocol specification, separating required message schemas from optional
aggregation rules, and evaluate it across scenarios with varying incentives and
network topologies. Benchmarks across three domains: (i) supply chain cascades
(Beer Game), (ii) preference aggregation in sparse networks (Movie Scheduling),
and (iii) sustainable resource allocation (Fishbanks) show that REP improves
coordination accuracy and efficiency over A2A by 41 to 100%, while flexibly
handling multimodal sensitivity signals from LLMs. By making coordination a
protocol-level capability, REP provides scalable infrastructure for the
emerging Internet of Agents

</details>


### [136] [Can Knowledge-Graph-based Retrieval Augmented Generation Really Retrieve What You Need?](https://arxiv.org/abs/2510.16582)
*Junchi Yu,Yujie Liu,Jindong Gu,Philip Torr,Dongzhan Zhou*

Main category: cs.AI

TL;DR: GraphFlow是一个基于知识图谱的检索增强生成框架，通过过渡流匹配目标优化检索策略和流估计器，有效检索复杂查询所需的准确多样化知识


<details>
  <summary>Details</summary>
Motivation: 现有基于知识图谱的RAG方法难以从文本丰富的知识图谱中检索准确多样的信息，而过程奖励模型需要昂贵的过程级监督信号

Method: 采用过渡流匹配目标联合优化检索策略和流估计器，流估计器将检索结果奖励分解到中间检索状态，引导检索策略按奖励比例从知识图谱中检索候选

Result: 在STaRK基准测试中，GraphFlow在命中率和召回率上平均优于强基线（包括GPT-4o）10%，并对未见知识图谱表现出强泛化能力

Conclusion: GraphFlow能有效检索复杂真实世界查询所需的准确多样化知识，具有高效性和鲁棒性

Abstract: Retrieval-Augmented Generation (RAG) based on knowledge graphs (KGs) enhances
large language models (LLMs) by providing structured and interpretable external
knowledge. However, existing KG-based RAG methods struggle to retrieve accurate
and diverse information from text-rich KGs for complex real-world queries.
Process Reward Models (PRMs) offer a way to align the retrieval process of
KG-based RAG with query-specific knowledge requirements, but they heavily rely
on process-level supervision signals that are expensive and hard to obtain on
KGs. To address this challenge, we propose GraphFlow, a framework that
efficiently retrieves accurate and diverse knowledge required for real-world
queries from text-rich KGs. GraphFlow employs a transition-based flow matching
objective to jointly optimize a retrieval policy and a flow estimator. The flow
estimator factorizes the reward of the retrieval outcome into the intermediate
retrieval states. Such reward factorization guides the retrieval policy to
retrieve candidates from KGs in proportion to their reward. This allows
GraphFlow to explore high-quality regions of KGs that yield diverse and
relevant results. We evaluate GraphFlow on the STaRK benchmark, which includes
real-world queries from multiple domains over text-rich KGs. GraphFlow
outperforms strong KG-RAG baselines, including GPT-4o, by 10% on average in hit
rate and recall. It also shows strong generalization to unseen KGs,
demonstrating its effectiveness and robustness.

</details>


### [137] [Uncertain Knowledge Graph Completion via Semi-Supervised Confidence Distribution Learning](https://arxiv.org/abs/2510.16601)
*Tianxing Wu,Shutong Zhu,Jingting Wang,Ning Xu,Guilin Qi,Haofen Wang*

Main category: cs.AI

TL;DR: 提出了一种用于不确定知识图谱补全的半监督置信分布学习方法，通过将置信度转换为分布并利用元学习生成伪标签来平衡置信度分布。


<details>
  <summary>Details</summary>
Motivation: 现有不确定知识图谱嵌入方法忽略了置信度的极端不平衡分布，导致学习到的嵌入不足以支持高质量的知识图谱补全。

Method: 提出ssCDL方法，将置信度转换为置信分布，通过关系学习在标记数据和带有伪标签的未标记数据上迭代学习嵌入，使用元学习预测未见三元组的置信度来增强训练数据。

Result: 在两个UKG数据集上的实验表明，ssCDL在不同评估指标上持续优于最先进的基线方法。

Conclusion: ssCDL方法通过置信分布学习和数据增强有效解决了置信度不平衡问题，提升了不确定知识图谱补全的性能。

Abstract: Uncertain knowledge graphs (UKGs) associate each triple with a confidence
score to provide more precise knowledge representations. Recently, since
real-world UKGs suffer from the incompleteness, uncertain knowledge graph (UKG)
completion attracts more attention, aiming to complete missing triples and
confidences. Current studies attempt to learn UKG embeddings to solve this
problem, but they neglect the extremely imbalanced distributions of triple
confidences. This causes that the learnt embeddings are insufficient to
high-quality UKG completion. Thus, in this paper, to address the above issue,
we propose a new semi-supervised Confidence Distribution Learning (ssCDL)
method for UKG completion, where each triple confidence is transformed into a
confidence distribution to introduce more supervision information of different
confidences to reinforce the embedding learning process. ssCDL iteratively
learns UKG embedding by relational learning on labeled data (i.e., existing
triples with confidences) and unlabeled data with pseudo labels (i.e., unseen
triples with the generated confidences), which are predicted by meta-learning
to augment the training data and rebalance the distribution of triple
confidences. Experiments on two UKG datasets demonstrate that ssCDL
consistently outperforms state-of-the-art baselines in different evaluation
metrics.

</details>


### [138] [Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards](https://arxiv.org/abs/2510.16614)
*Xuan Zhang,Ruixiao Li,Zhijian Zhou,Long Li,Yulei Qin,Ke Li,Xing Sun,Xiaoyu Tan,Chao Qu,Yuan Qi*

Main category: cs.AI

TL;DR: MERCI是一种增强LLM推理能力的强化学习算法，通过基于计数的内在奖励来激励探索，避免重复推理模式。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法依赖稀疏结果奖励和有限探索，导致LLMs陷入重复和次优推理模式，需要设计更好的探索机制。

Method: 使用轻量级Coin Flipping Network估计推理轨迹的伪计数和认知不确定性，将其转换为内在奖励，并与GRPO等RL框架集成。

Result: 在复杂推理基准测试中，MERCI鼓励更丰富多样的思维链，显著超越基线性能，帮助策略逃离局部最优。

Conclusion: 针对性的内在动机可以使语言模型推理的探索更加可靠有效。

Abstract: Reinforcement Learning (RL) has become a compelling way to strengthen the
multi step reasoning ability of Large Language Models (LLMs). However,
prevalent RL paradigms still lean on sparse outcome-based rewards and limited
exploration, which often drives LLMs toward repetitive and suboptimal reasoning
patterns. In this paper, we study the central question of how to design
exploration for LLM reasoning and introduce MERCI (Motivating Exploration in
LLM Reasoning with Count-based Intrinsic Rewards), a novel RL algorithm that
augments policy optimization with a principled intrinsic reward. Building on
the idea of count-based exploration, MERCI leverages a lightweight Coin
Flipping Network (CFN) to estimate the pseudo count and further epistemic
uncertainty over reasoning trajectories, and converts them into an intrinsic
reward that values novelty while preserving the learning signal from task
rewards. We integrate MERCI into some advanced RL frameworks like Group
Relative Policy Optimization (GRPO). Experiments on complex reasoning
benchmarks demonstrate that MERCI encourages richer and more varied chains of
thought, significantly improves performance over strong baselines, and helps
the policy escape local routines to discover better solutions. It indicates
that our targeted intrinsic motivation can make exploration reliable for
language model reasoning.

</details>


### [139] [Foundation and Large-Scale AI Models in Neuroscience: A Comprehensive Review](https://arxiv.org/abs/2510.16658)
*Shihao Yang,Xiying Huang,Danilo Bernardo,Jun-En Ding,Andrew Michael,Jingmei Yang,Patrick Kwan,Ashish Raj,Feng Liu*

Main category: cs.AI

TL;DR: 本文探讨大规模AI模型对神经科学研究的变革性影响，涵盖神经影像处理、脑机接口、分子神经科学、临床辅助和疾病应用五大领域，强调多模态数据整合和临床转化框架。


<details>
  <summary>Details</summary>
Motivation: 大规模AI模型的出现为神经科学研究带来范式转变，能够从原始脑信号和神经数据中进行端到端学习，解决传统计算方法面临的挑战。

Method: 通过系统综述方法，分析大规模AI模型在五个主要神经科学领域的应用：神经影像数据处理、脑机接口与神经解码、分子神经科学与基因组建模、临床辅助与转化框架、神经系统和精神疾病的特定应用。

Result: 这些模型被证明能够有效解决多模态神经数据整合、时空模式解释和临床转化框架等主要计算神经科学挑战，同时神经科学与AI的互动日益相互促进。

Conclusion: 该综述强调这些技术的显著前景和关键实施考虑，特别强调严格的评估框架、有效的领域知识整合以及临床使用的全面伦理指南，并提供了用于验证大规模AI模型的关键神经科学数据集清单。

Abstract: The advent of large-scale artificial intelligence (AI) models has a
transformative effect on neuroscience research, which represents a paradigm
shift from the traditional computational methods through the facilitation of
end-to-end learning from raw brain signals and neural data. In this paper, we
explore the transformative effects of large-scale AI models on five major
neuroscience domains: neuroimaging and data processing, brain-computer
interfaces and neural decoding, molecular neuroscience and genomic modeling,
clinical assistance and translational frameworks, and disease-specific
applications across neurological and psychiatric disorders. These models are
demonstrated to address major computational neuroscience challenges, including
multimodal neural data integration, spatiotemporal pattern interpretation, and
the derivation of translational frameworks for clinical deployment. Moreover,
the interaction between neuroscience and AI has become increasingly reciprocal,
as biologically informed architectural constraints are now incorporated to
develop more interpretable and computationally efficient models. This review
highlights both the notable promise of such technologies and key implementation
considerations, with particular emphasis on rigorous evaluation frameworks,
effective domain knowledge integration, and comprehensive ethical guidelines
for clinical use. Finally, a systematic listing of critical neuroscience
datasets used to derive and validate large-scale AI models across diverse
research applications is provided.

</details>


### [140] [An Agentic Framework with LLMs for Solving Complex Vehicle Routing Problems](https://arxiv.org/abs/2510.16701)
*Ni Zhang,Zhiguang Cao,Jianan Zhou,Cong Zhang,Yew-Soon Ong*

Main category: cs.AI

TL;DR: 提出了一个基于大语言模型的代理框架AFL，用于完全自动化解决复杂车辆路径问题，从问题实例直接生成解决方案代码，无需人工干预或外部求解器。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的方法仍依赖外部干预，限制了自主性并导致执行错误和解决方案可行性低的问题。需要实现从问题实例到解决方案的完全自动化。

Method: AFL框架将整体流程分解为三个可管理的子任务，采用四个专门代理通过协调交互来确保跨功能一致性和逻辑合理性，直接从原始输入中提取知识并生成自包含代码。

Result: 在60个复杂VRP问题上的实验验证了框架的有效性和通用性，与精心设计的算法性能相当，在代码可靠性和解决方案可行性方面显著优于现有LLM基线，在评估基准上接近100%的成功率。

Conclusion: AFL框架实现了复杂车辆路径问题求解的完全自动化，在保持高性能的同时大幅提升了代码可靠性和解决方案可行性，为LLM在复杂优化问题中的应用提供了新思路。

Abstract: Complex vehicle routing problems (VRPs) remain a fundamental challenge,
demanding substantial expert effort for intent interpretation and algorithm
design. While large language models (LLMs) offer a promising path toward
automation, current approaches still rely on external intervention, which
restrict autonomy and often lead to execution errors and low solution
feasibility. To address these challenges, we propose an Agentic Framework with
LLMs (AFL) for solving complex vehicle routing problems, achieving full
automation from problem instance to solution. AFL directly extracts knowledge
from raw inputs and enables self-contained code generation without handcrafted
modules or external solvers. To improve trustworthiness, AFL decomposes the
overall pipeline into three manageable subtasks and employs four specialized
agents whose coordinated interactions enforce cross-functional consistency and
logical soundness. Extensive experiments on 60 complex VRPs, ranging from
standard benchmarks to practical variants, validate the effectiveness and
generality of our framework, showing comparable performance against
meticulously designed algorithms. Notably, it substantially outperforms
existing LLM-based baselines in both code reliability and solution feasibility,
achieving rates close to 100% on the evaluated benchmarks.

</details>


### [141] [Beyond Pipelines: A Survey of the Paradigm Shift toward Model-Native Agentic AI](https://arxiv.org/abs/2510.16720)
*Jitao Sang,Jinlin Xiao,Jiarun Han,Jilin Chen,Xiaoyi Chen,Shuyu Wei,Yongjie Sun,Yuhang Wang*

Main category: cs.AI

TL;DR: 本文综述了智能体AI从基于管道的系统向模型原生范式的范式转变，其中规划、工具使用和记忆能力从外部模块演变为模型内部参数化的端到端学习行为。


<details>
  <summary>Details</summary>
Motivation: 追踪智能体AI构建范式的转变，从外部逻辑编排的Pipeline系统到能力内化于模型参数的Model-native范式，探讨强化学习在这一转变中的核心作用。

Method: 系统性地回顾了规划、工具使用和记忆三大能力的演变过程，从外部脚本模块到端到端学习行为，并分析了这一范式转变如何重塑深度研究智能体和GUI智能体等主要应用。

Result: 揭示了智能体AI向模型原生发展的连贯轨迹，展示了多智能体协作、反思等能力的持续内化，以及系统和模型层在未来智能体AI中角色的演变。

Conclusion: 智能体AI正从构建应用智能的系统转向开发通过经验增长智能的模型，标志着从静态数据模仿到结果驱动探索的学习范式转变。

Abstract: The rapid evolution of agentic AI marks a new phase in artificial
intelligence, where Large Language Models (LLMs) no longer merely respond but
act, reason, and adapt. This survey traces the paradigm shift in building
agentic AI: from Pipeline-based systems, where planning, tool use, and memory
are orchestrated by external logic, to the emerging Model-native paradigm,
where these capabilities are internalized within the model's parameters. We
first position Reinforcement Learning (RL) as the algorithmic engine enabling
this paradigm shift. By reframing learning from imitating static data to
outcome-driven exploration, RL underpins a unified solution of LLM + RL + Task
across language, vision and embodied domains. Building on this, the survey
systematically reviews how each capability -- Planning, Tool use, and Memory --
has evolved from externally scripted modules to end-to-end learned behaviors.
Furthermore, it examines how this paradigm shift has reshaped major agent
applications, specifically the Deep Research agent emphasizing long-horizon
reasoning and the GUI agent emphasizing embodied interaction. We conclude by
discussing the continued internalization of agentic capabilities like
Multi-agent collaboration and Reflection, alongside the evolving roles of the
system and model layers in future agentic AI. Together, these developments
outline a coherent trajectory toward model-native agentic AI as an integrated
learning and interaction framework, marking the transition from constructing
systems that apply intelligence to developing models that grow intelligence
through experience.

</details>


### [142] [A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications](https://arxiv.org/abs/2510.16724)
*Minhua Lin,Zongyu Wu,Zhichao Xu,Hui Liu,Xianfeng Tang,Qi He,Charu Aggarwal,Hui Liu,Xiang Zhang,Suhang Wang*

Main category: cs.AI

TL;DR: 该论文首次全面综述了基于强化学习的代理搜索领域，从功能角色、优化策略和应用范围三个维度组织这一新兴领域，旨在构建可靠且可扩展的RL驱动代理搜索系统。


<details>
  <summary>Details</summary>
Motivation: 传统RAG管道通常是单轮和启发式的，缺乏对检索和推理的自适应控制。强化学习为代理搜索提供了自适应和自我改进的机制，以解决LLMs的静态知识、事实幻觉和无法获取实时信息等限制。

Method: 通过三个互补维度组织RL-based代理搜索领域：(i) RL的功能角色，(ii) RL的优化策略，(iii) RL的应用范围。总结了代表性方法、评估协议和应用案例。

Result: 提供了该领域的首个全面概述，建立了系统化的分类框架，并识别了关键研究方向和挑战。

Conclusion: RL与代理搜索的整合为解决LLMs的局限性提供了有前景的途径，该综述旨在激发未来在这一交叉领域的研究。

Abstract: The advent of large language models (LLMs) has transformed information access
and reasoning through open-ended natural language interaction. However, LLMs
remain limited by static knowledge, factual hallucinations, and the inability
to retrieve real-time or domain-specific information. Retrieval-Augmented
Generation (RAG) mitigates these issues by grounding model outputs in external
evidence, but traditional RAG pipelines are often single turn and heuristic,
lacking adaptive control over retrieval and reasoning. Recent advances in
agentic search address these limitations by enabling LLMs to plan, retrieve,
and reflect through multi-step interaction with search environments. Within
this paradigm, reinforcement learning (RL) offers a powerful mechanism for
adaptive and self-improving search behavior. This survey provides the first
comprehensive overview of \emph{RL-based agentic search}, organizing the
emerging field along three complementary dimensions: (i) What RL is for
(functional roles), (ii) How RL is used (optimization strategies), and (iii)
Where RL is applied (scope of optimization). We summarize representative
methods, evaluation protocols, and applications, and discuss open challenges
and future directions toward building reliable and scalable RL driven agentic
search systems. We hope this survey will inspire future research on the
integration of RL and agentic search. Our repository is available at
https://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers.

</details>


### [143] [Surrogate Modeling and Explainable Artificial Intelligence for Complex Systems: A Workflow for Automated Simulation Exploration](https://arxiv.org/abs/2510.16742)
*Paul Saves,Pramudita Satria Palar,Muhammad Daffa Robani,Nicolas Verstaevel,Moncef Garouani,Julien Aligon,Benoit Gaudou,Koji Shimoyama,Joseph Morlier*

Main category: cs.AI

TL;DR: 提出了一种基于代理模型的仿真驱动工程工作流，通过训练轻量级仿真器来解决计算成本高和黑盒透明度不足的问题，支持不确定性量化和可解释AI分析。


<details>
  <summary>Details</summary>
Motivation: 仿真驱动工程工作流面临两个核心障碍：(1) 高计算成本，准确探索需要大量昂贵的模拟器运行；(2) 依赖不透明黑盒组件时的有限透明度和可靠性。

Method: 在紧凑实验设计上训练轻量级仿真器，提供快速、低延迟的昂贵模拟器近似，支持严格的不确定性量化，并适应全局和局部可解释AI分析。

Result: 在混合动力飞机多学科设计分析和城市隔离基于代理模型两个案例研究中，该方法能够在秒级完成大规模探索，揭示非线性相互作用和涌现行为，识别关键设计和政策杠杆，并指示代理模型需要更多数据或替代架构的区域。

Conclusion: 代理模型与可解释AI的耦合能够有效解决仿真驱动工作流中的计算成本和透明度问题，为复杂系统分析提供统一框架。

Abstract: Complex systems are increasingly explored through simulation-driven
engineering workflows that combine physics-based and empirical models with
optimization and analytics. Despite their power, these workflows face two
central obstacles: (1) high computational cost, since accurate exploration
requires many expensive simulator runs; and (2) limited transparency and
reliability when decisions rely on opaque blackbox components. We propose a
workflow that addresses both challenges by training lightweight emulators on
compact designs of experiments that (i) provide fast, low-latency
approximations of expensive simulators, (ii) enable rigorous uncertainty
quantification, and (iii) are adapted for global and local Explainable
Artificial Intelligence (XAI) analyses. This workflow unifies every
simulation-based complex-system analysis tool, ranging from engineering design
to agent-based models for socio-environmental understanding. In this paper, we
proposea comparative methodology and practical recommendations for using
surrogate-based explainability tools within the proposed workflow. The
methodology supports continuous and categorical inputs, combines global-effect
and uncertainty analyses with local attribution, and evaluates the consistency
of explanations across surrogate models, thereby diagnosing surrogate adequacy
and guiding further data collection or model refinement. We demonstrate the
approach on two contrasting case studies: a multidisciplinary design analysis
of a hybrid-electric aircraft and an agent-based model of urban segregation.
Results show that the surrogate model and XAI coupling enables large-scale
exploration in seconds, uncovers nonlinear interactions and emergent behaviors,
identifies key design and policy levers, and signals regions where surrogates
require more data or alternative architectures.

</details>


### [144] [ELMM: Efficient Lightweight Multimodal Large Language Models for Multimodal Knowledge Graph Completion](https://arxiv.org/abs/2510.16753)
*Wei Huang,Peining Li,Meiyu Liang,Xu Hou,Junping Du,Yingxia Shao,Guanhua Ye,Wu Liu,Kangkang Lu,Yang Yu*

Main category: cs.AI

TL;DR: 提出ELMM方法用于多模态知识图谱补全，通过多视图视觉token压缩器和注意力剪枝策略，在保持性能的同时显著提升计算效率


<details>
  <summary>Details</summary>
Motivation: 多模态知识图谱存在不完整性问题，而现有方法在处理多模态信息时面临语义噪声、模态冲突和计算成本高的挑战

Method: 使用基于多头注意力的多视图视觉token压缩器自适应压缩图像token，设计注意力剪枝策略减少冗余层，并通过线性投影补偿性能损失

Result: 在FB15k-237-IMG和WN18-IMG基准测试中达到最先进性能，同时大幅提升计算效率

Conclusion: ELMM为多模态知识图谱补全建立了新的范式，在性能和效率方面均表现出色

Abstract: Multimodal Knowledge Graphs (MKGs) extend traditional knowledge graphs by
incorporating visual and textual modalities, enabling richer and more
expressive entity representations. However, existing MKGs often suffer from
incompleteness, which hinder their effectiveness in downstream tasks.
Therefore, multimodal knowledge graph completion (MKGC) task is receiving
increasing attention. While large language models (LLMs) have shown promise for
knowledge graph completion (KGC), their application to the multimodal setting
remains underexplored. Moreover, applying Multimodal Large Language Models
(MLLMs) to the task of MKGC introduces significant challenges: (1) the large
number of image tokens per entity leads to semantic noise and modality
conflicts, and (2) the high computational cost of processing large token
inputs. To address these issues, we propose Efficient Lightweight Multimodal
Large Language Models (ELMM) for MKGC. ELMM proposes a Multi-view Visual Token
Compressor (MVTC) based on multi-head attention mechanism, which adaptively
compresses image tokens from both textual and visual views, thereby effectively
reducing redundancy while retaining necessary information and avoiding modality
conflicts. Additionally, we design an attention pruning strategy to remove
redundant attention layers from MLLMs, thereby significantly reducing the
inference cost. We further introduce a linear projection to compensate for the
performance degradation caused by pruning. Extensive experiments on benchmark
FB15k-237-IMG and WN18-IMG demonstrate that ELMM achieves state-of-the-art
performance while substantially improving computational efficiency,
establishing a new paradigm for multimodal knowledge graph completion.

</details>


### [145] [End-to-end Listen, Look, Speak and Act](https://arxiv.org/abs/2510.16756)
*Siyin Wang,Wenyi Yu,Xianzhao Chen,Xiaohai Tian,Jun Zhang,Lu Lu,Chao Zhang*

Main category: cs.AI

TL;DR: ELLSA是首个全双工、端到端的多模态模型，能够同时感知和生成视觉、文本、语音和动作，实现更自然的人机交互。


<details>
  <summary>Details</summary>
Motivation: 人类交互本质上是多模态和全双工的，需要模型能够同时感知和生成多种模态，实现更自然的人类行为模拟。

Method: 采用新颖的SA-MoE架构（自注意力专家混合），将各模态路由到专门专家，通过统一注意力骨干网络进行融合。

Result: 在语音交互和机器人操作基准测试中，ELLSA与模态特定基线表现相当，同时支持高级多模态和全双工行为。

Conclusion: ELLSA代表了向更自然和通用交互智能迈出的一步，有助于实现人工通用智能。

Abstract: Human interaction is inherently multimodal and full-duplex: we listen while
watching, speak while acting, and fluidly adapt to turn-taking and
interruptions. Realizing these capabilities is essential for building models
simulating humans. We present ELLSA (End-to-end Listen, Look, Speak and Act),
which, to our knowledge, is the first full-duplex, end-to-end model that
simultaneously perceives and generates across vision, text, speech, and action
within a single architecture, enabling interaction patterns previously out of
reach, yielding more natural, human-like behaviors. At its core is a novel
SA-MoE architecture (Self-Attention Mixture-of-Experts) that routes each
modality to specialized experts and fuses them through a unified attention
backbone. This provides a generalizable solution for joint multimodal
perception and concurrent generation, leveraging strong pre-trained components
while enabling efficient modality integration and mitigating modality
interference. On speech-interaction and robot-manipulation benchmarks, ELLSA
matches modality-specific baselines, while uniquely supporting advanced
multimodal and full-duplex behaviors such as dialogue and action turn-taking,
defective instruction rejection, speaking-while-acting, context-grounded visual
question answering, and action barge-ins. We contend that ELLSA represents a
step toward more natural and general interactive intelligence, contributing to
the broader pursuit of artificial general intelligence. All data, code and
model checkpoints will be released upon acceptance.

</details>


### [146] [See or Say Graphs: Agent-Driven Scalable Graph Understanding with Vision-Language Models](https://arxiv.org/abs/2510.16769)
*Shuo Han,Yukun Cao,Zezhong Ding,Zengyi Gao,S Kevin Zhou,Xike Xie*

Main category: cs.AI

TL;DR: GraphVista是一个统一框架，通过分层组织图信息和引入规划代理来解决视觉语言模型在图理解中的可扩展性和模态协调问题，能够处理比现有基准大200倍的图，并在质量上比现有方法提升4.4倍。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在图理解中面临输入token限制导致的可扩展性瓶颈，以及缺乏有效协调文本和视觉模态的机制。

Method: GraphVista采用分层方法组织图信息到轻量级GraphRAG基础中，仅检索任务相关的文本描述和高分辨率视觉子图；引入规划代理根据任务复杂度路由到最适合的模态。

Result: GraphVista能够扩展到比现有基准大200倍的图，在文本、视觉和融合方法中表现一致优越，质量比最先进基线提升高达4.4倍。

Conclusion: GraphVista通过充分利用两种模态的互补优势，有效解决了图理解中的可扩展性和模态协调挑战。

Abstract: Vision-language models (VLMs) have shown promise in graph understanding, but
remain limited by input-token constraints, facing scalability bottlenecks and
lacking effective mechanisms to coordinate textual and visual modalities. To
address these challenges, we propose GraphVista, a unified framework that
enhances both scalability and modality coordination in graph understanding. For
scalability, GraphVista organizes graph information hierarchically into a
lightweight GraphRAG base, which retrieves only task-relevant textual
descriptions and high-resolution visual subgraphs, compressing redundant
context while preserving key reasoning elements. For modality coordination,
GraphVista introduces a planning agent that routes tasks to the most suitable
modality-using the text modality for simple property reasoning and the visual
modality for local and structurally complex reasoning grounded in explicit
topology. Extensive experiments demonstrate that GraphVista scales to large
graphs, up to $200\times$ larger than those used in existing benchmarks, and
consistently outperforms existing textual, visual, and fusion-based methods,
achieving up to $4.4\times$ quality improvement over the state-of-the-art
baselines by fully exploiting the complementary strengths of both modalities.

</details>


### [147] [Domain-Contextualized Concept Graphs: A Computable Framework for Knowledge Representation](https://arxiv.org/abs/2510.16802)
*Chao Li,Yuru Wang*

Main category: cs.AI

TL;DR: 提出了领域情境化概念图（CDC），将领域作为知识表示的一等元素，采用<概念, 关系@领域, 概念'>的三元组结构，实现上下文感知推理和跨领域类比。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱受限于固定的本体论，概念被组织在僵化的层次结构中，问题根源在于将领域视为隐式上下文而非显式的推理级组件。

Method: 采用C-D-C三元组结构，基于认知语言学同构映射原理，定义了20多个标准化关系谓词（结构、逻辑、跨领域和时间），并在Prolog中实现完整的推理能力。

Result: 在教育培训、企业知识系统和文档管理等案例研究中，CDC能够实现上下文感知推理、跨领域类比和个性化知识建模，这些能力在传统基于本体的框架中无法实现。

Conclusion: CDC框架通过将领域提升为概念表示的一等元素，克服了传统知识图谱的局限性，为上下文感知推理和跨领域知识建模提供了新的解决方案。

Abstract: Traditional knowledge graphs are constrained by fixed ontologies that
organize concepts within rigid hierarchical structures. The root cause lies in
treating domains as implicit context rather than as explicit, reasoning-level
components. To overcome these limitations, we propose the Domain-Contextualized
Concept Graph (CDC), a novel knowledge modeling framework that elevates domains
to first-class elements of conceptual representation. CDC adopts a C-D-C triple
structure - <Concept, Relation@Domain, Concept'> - where domain specifications
serve as dynamic classification dimensions defined on demand. Grounded in a
cognitive-linguistic isomorphic mapping principle, CDC operationalizes how
humans understand concepts through contextual frames. We formalize more than
twenty standardized relation predicates (structural, logical, cross-domain, and
temporal) and implement CDC in Prolog for full inference capability. Case
studies in education, enterprise knowledge systems, and technical documentation
demonstrate that CDC enables context-aware reasoning, cross-domain analogy, and
personalized knowledge modeling - capabilities unattainable under traditional
ontology-based frameworks.

</details>


### [148] [DeepAnalyze: Agentic Large Language Models for Autonomous Data Science](https://arxiv.org/abs/2510.16872)
*Shaolei Zhang,Ju Fan,Meihao Fan,Guoliang Li,Xiaoyong Du*

Main category: cs.AI

TL;DR: DeepAnalyze-8B是首个用于自主数据科学的智能LLM代理，能够从数据源到分析师级深度研究报告自动完成端到端流程，仅用80亿参数就超越了基于最先进专有LLM构建的工作流代理。


<details>
  <summary>Details</summary>
Motivation: 现有的基于工作流的数据代理在特定数据任务上表现良好，但由于依赖预定义工作流，无法实现完全自主的数据科学。随着强大LLM的出现，从原始数据源到分析师级深度研究报告的自主数据科学变得可行。

Method: 提出了基于课程学习的智能代理训练范式，模拟人类数据科学家的学习轨迹，使LLM能够在真实环境中逐步获取和整合多种能力。同时引入了数据基础的轨迹合成框架来构建高质量训练数据。

Result: 实验表明，DeepAnalyze-8B能够执行广泛的数据任务，包括数据问答、专业分析任务和开放式数据研究，超越了先前基于最先进专有LLM构建的工作流代理。

Conclusion: DeepAnalyze-8B为自主数据科学开辟了道路，其模型、代码和训练数据均已开源。

Abstract: Autonomous data science, from raw data sources to analyst-grade deep research
reports, has been a long-standing challenge, and is now becoming feasible with
the emergence of powerful large language models (LLMs). Recent workflow-based
data agents have shown promising results on specific data tasks but remain
fundamentally limited in achieving fully autonomous data science due to their
reliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B,
the first agentic LLM designed for autonomous data science, capable of
automatically completing the end-toend pipeline from data sources to
analyst-grade deep research reports. To tackle high-complexity data science
tasks, we propose a curriculum-based agentic training paradigm that emulates
the learning trajectory of human data scientists, enabling LLMs to
progressively acquire and integrate multiple capabilities in real-world
environments. We also introduce a data-grounded trajectory synthesis framework
that constructs high-quality training data. Through agentic training,
DeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data
question answering and specialized analytical tasks to open-ended data
research. Experiments demonstrate that, with only 8B parameters, DeepAnalyze
outperforms previous workflow-based agents built on most advanced proprietary
LLMs. The model, code, and training data of DeepAnalyze are open-sourced,
paving the way toward autonomous data science.

</details>


### [149] [VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents](https://arxiv.org/abs/2510.16907)
*Kangrui Wang,Pingyue Zhang,Zihan Wang,Yaning Gao,Linjie Li,Qineng Wang,Hanyang Chen,Chi Wan,Yiping Lu,Zhengyuan Yang,Lijuan Wang,Ranjay Krishna,Jiajun Wu,Li Fei-Fei,Yejin Choi,Manling Li*

Main category: cs.AI

TL;DR: 本文提出通过强化学习训练视觉语言模型（VLM）代理进行显式视觉状态推理，构建内部世界模型。通过状态估计和转移建模分解推理过程，并设计世界建模奖励和双层次优势估计方法，在多个基准测试中显著优于未训练模型和专有推理模型。


<details>
  <summary>Details</summary>
Motivation: 训练视觉语言模型代理面临从文本状态到复杂视觉观察的转变，这引入了部分可观测性并需要强大的世界建模能力。研究目标是探索VLM代理是否能够通过显式视觉状态推理构建内部世界模型。

Method: 1. 将代理推理过程建模为部分可观测马尔可夫决策过程（POMDP）
2. 将推理分解为状态估计（当前状态是什么）和转移建模（接下来会发生什么）
3. 设计世界建模奖励提供密集的回合级监督
4. 引入双层次通用优势估计（Bi-Level GAE）进行回合感知信用分配

Result: 一个30亿参数的模型在五个不同的代理基准测试中获得了0.82的分数，相比未训练模型（0.21）提升了3倍，并优于专有推理模型如GPT-5（0.75）、Gemini 2.5 Pro（0.67）和Claude 4.5（0.62）。

Conclusion: 通过显式视觉状态推理，VLM代理能够有效构建内部世界模型。最优表示形式是任务依赖的：自然语言擅长捕捉一般任务中的语义关系，而结构化格式对于精确操作和控制至关重要。提出的方法在VAGEN框架中实现了可扩展的多回合VLM代理训练。

Abstract: A key challenge in training Vision-Language Model (VLM) agents, compared to
Language Model (LLM) agents, lies in the shift from textual states to complex
visual observations. This transition introduces partial observability and
demands robust world modeling. We ask: Can VLM agents construct internal world
models through explicit visual state reasoning? To address this question, we
architecturally enforce and reward the agent's reasoning process via
reinforcement learning (RL), formulating it as a Partially Observable Markov
Decision Process (POMDP). We find that decomposing the agent's reasoning into
State Estimation ("what is the current state?") and Transition Modeling ("what
comes next?") is critical for success, as demonstrated through five reasoning
strategies. Our investigation into how agents represent internal beliefs
reveals that the optimal representation is task-dependent: Natural Language
excels at capturing semantic relationships in general tasks, while Structured
formats are indispensable for precise manipulation and control. Building on
these insights, we design a World Modeling Reward that provides dense,
turn-level supervision for accurate state prediction, and introduce Bi-Level
General Advantage Estimation (Bi-Level GAE) for turn-aware credit assignment.
Through this form of visual state reasoning, a 3B-parameter model achieves a
score of 0.82 across five diverse agent benchmarks, representing a 3$\times$
improvement over its untrained counterpart (0.21) and outperforming proprietary
reasoning models such as GPT-5 (0.75), Gemini 2.5 Pro (0.67) and Claude 4.5
(0.62). All experiments are conducted within our VAGEN framework, a scalable
system for training and analyzing multi-turn VLM agents in diverse visual
environments. Code and data are publicly available at
https://vagen-ai.github.io.

</details>


### [150] [A Comparative User Evaluation of XRL Explanations using Goal Identification](https://arxiv.org/abs/2510.16956)
*Mark Towers,Yali Du,Christopher Freeman,Timothy J. Norman*

Main category: cs.AI

TL;DR: 提出了一种新的评估方法来测试用户是否能从强化学习算法的解释中识别出智能体的目标，发现在测试的四种可解释强化学习算法中，只有一种的准确率超过随机水平，且用户普遍高估了自己的选择准确性。


<details>
  <summary>Details</summary>
Motivation: 可解释强化学习算法的核心应用之一是调试，但目前缺乏对其相对性能的比较评估。

Method: 使用Atari的Ms. Pacman环境和四种可解释强化学习算法，通过用户研究测试他们从算法解释中识别智能体目标的能力。

Result: 只有一种算法在测试目标上取得了超过随机水平的准确率；用户普遍对自己的选择过于自信；用户自报的识别和理解难易程度与他们的准确率不相关。

Conclusion: 当前的可解释强化学习算法在帮助用户理解智能体目标方面效果有限，且用户的主观感知与实际理解能力存在差异。

Abstract: Debugging is a core application of explainable reinforcement learning (XRL)
algorithms; however, limited comparative evaluations have been conducted to
understand their relative performance. We propose a novel evaluation
methodology to test whether users can identify an agent's goal from an
explanation of its decision-making. Utilising the Atari's Ms. Pacman
environment and four XRL algorithms, we find that only one achieved greater
than random accuracy for the tested goals and that users were generally
overconfident in their selections. Further, we find that users' self-reported
ease of identification and understanding for every explanation did not
correlate with their accuracy.

</details>


### [151] [STARK: Strategic Team of Agents for Refining Kernels](https://arxiv.org/abs/2510.16996)
*Juncheng Dong,Yang Yang,Tao Liu,Yang Wang,Feng Qi,Vahid Tarokh,Kaushik Rangadurai,Shuang Yang*

Main category: cs.AI

TL;DR: 提出了一种基于LLM的多智能体框架，用于自动化GPU内核优化，通过系统化探索设计空间，显著提升了内核运行性能。


<details>
  <summary>Details</summary>
Motivation: GPU内核优化对现代AI发展至关重要，但由于内存层次、线程调度和硬件特性的复杂交互，优化过程困难且劳动密集。现有LLM方法主要将其视为单次生成器或简单优化工具，难以应对不规则的内核优化场景。

Method: 采用多智能体协作框架，结合基于经验的指导、动态上下文管理和策略搜索，模拟专家工程师的工作流程，使LLM能够推理硬件权衡、整合性能分析反馈并迭代优化内核。

Result: 在KernelBench基准测试中，相比基线智能体，该系统在基线失败的情况下仍能生成正确解决方案，并实现了高达16倍的运行时性能提升。

Conclusion: 智能体LLM框架具有推动完全自动化、可扩展GPU内核优化的潜力。

Abstract: The efficiency of GPU kernels is central to the progress of modern AI, yet
optimizing them remains a difficult and labor-intensive task due to complex
interactions between memory hierarchies, thread scheduling, and
hardware-specific characteristics. While recent advances in large language
models (LLMs) provide new opportunities for automated code generation, existing
approaches largely treat LLMs as single-shot generators or naive refinement
tools, limiting their effectiveness in navigating the irregular kernel
optimization landscape. We introduce an LLM agentic framework for GPU kernel
optimization that systematically explores the design space through multi-agent
collaboration, grounded instruction, dynamic context management, and strategic
search. This framework mimics the workflow of expert engineers, enabling LLMs
to reason about hardware trade-offs, incorporate profiling feedback, and refine
kernels iteratively. We evaluate our approach on KernelBench, a benchmark for
LLM-based kernel optimization, and demonstrate substantial improvements over
baseline agents: our system produces correct solutions where baselines often
fail, and achieves kernels with up to 16x faster runtime performance. These
results highlight the potential of agentic LLM frameworks to advance fully
automated, scalable GPU kernel optimization.

</details>


### [152] [ToolCritic: Detecting and Correcting Tool-Use Errors in Dialogue Systems](https://arxiv.org/abs/2510.17052)
*Hassan Hamad,Yingru Xu,Liang Zhao,Wenbo Yan,Narendra Gyanchandani*

Main category: cs.AI

TL;DR: ToolCritic是一个诊断框架，通过检测工具调用错误并提供针对性反馈，帮助大型语言模型在多轮工具增强对话中提高工具调用准确性。


<details>
  <summary>Details</summary>
Motivation: 工具增强的大型语言模型在现实应用中存在工具使用错误，这阻碍了其可靠性。需要一种方法来评估和改进LLM在多轮工具增强对话中的行为。

Method: ToolCritic检测8种特定的工具调用错误类型（如过早调用、参数不对齐、工具输出误解等），并为主要LLM提供针对性反馈。主要LLM基于反馈修正其响应。

Result: 在Schema-Guided Dialogue数据集上的实验表明，ToolCritic将工具调用准确性提高了13%，优于零样本提示和自我修正技术。

Conclusion: ToolCritic代表了在现实世界对话应用中更稳健地集成LLM与外部工具的有希望的一步。

Abstract: Tool-augmented large language models (LLMs) are increasingly employed in
real-world applications, but tool usage errors still hinder their reliability.
We introduce ToolCritic, a diagnostic framework that evaluates and improves LLM
behavior in multi-turn, tool-augmented dialogues. ToolCritic detects eight
distinct error types specific to tool-calling (e.g., premature invocation,
argument misalignment, and misinterpretation of tool outputs) and provides
targeted feedback to the main LLM. The main LLM, assumed to have strong
reasoning, task understanding and orchestration capabilities, then revises its
response based on ToolCritic's feedback. We systematically define these error
categories and construct a synthetic dataset to train ToolCritic. Experimental
results on the Schema-Guided Dialogue (SGD) dataset demonstrate that ToolCritic
improves tool-calling accuracy by up to 13% over baselines, including zero-shot
prompting and self-correction techniques. This represents a promising step
toward more robust LLM integration with external tools in real-world dialogue
applications.

</details>


### [153] [A Brain Cell Type Resource Created by Large Language Models and a Multi-Agent AI System for Collaborative Community Annotation](https://arxiv.org/abs/2510.17064)
*Rongbin Li,Wenbo Chen,Zhao Li,Rodrigo Munoz-Castaneda,Jinbo Li,Neha S. Maurya,Arnav Solanki,Huan He,Hanwen Xing,Meaghan Ramlakhan,Zachary Wise,Zhuhao Wu,Hua Xu,Michael Hawrylycz,W. Jim Zheng*

Main category: cs.AI

TL;DR: BRAINCELL-AID是一个多智能体AI系统，通过整合自由文本描述和本体标签，结合检索增强生成技术，显著提升了基因集注释的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA测序技术虽然能识别多样细胞类型，但对涉及特征不明确基因的转录组特征注释仍然困难。传统方法依赖精心策划的注释，在复杂生物知识表示方面表现不佳。

Method: 开发了BRAINCELL-AID多智能体AI系统，整合自由文本描述与本体标签，采用检索增强生成技术，通过PubMed文献精炼预测，减少幻觉并增强可解释性。

Result: 对小鼠基因集实现了77%的正确注释率，成功注释了BRAIN Initiative Cell Census Network生成的5,322个脑细胞簇，识别了区域特异性基因共表达模式，并推断出基因集合的功能作用。

Conclusion: BRAINCELL-AID创建了一个有价值的资源，支持社区驱动的细胞类型注释，特别在识别基底神经节相关细胞类型方面提供了神经学上有意义的描述。

Abstract: Single-cell RNA sequencing has transformed our ability to identify diverse
cell types and their transcriptomic signatures. However, annotating these
signatures-especially those involving poorly characterized genes-remains a
major challenge. Traditional methods, such as Gene Set Enrichment Analysis
(GSEA), depend on well-curated annotations and often perform poorly in these
contexts. Large Language Models (LLMs) offer a promising alternative but
struggle to represent complex biological knowledge within structured
ontologies. To address this, we present BRAINCELL-AID (BRAINCELL-AID:
https://biodataai.uth.edu/BRAINCELL-AID), a novel multi-agent AI system that
integrates free-text descriptions with ontology labels to enable more accurate
and robust gene set annotation. By incorporating retrieval-augmented generation
(RAG), we developed a robust agentic workflow that refines predictions using
relevant PubMed literature, reducing hallucinations and enhancing
interpretability. Using this workflow, we achieved correct annotations for 77%
of mouse gene sets among their top predictions. Applying this approach, we
annotated 5,322 brain cell clusters from the comprehensive mouse brain cell
atlas generated by the BRAIN Initiative Cell Census Network, enabling novel
insights into brain cell function by identifying region-specific gene
co-expression patterns and inferring functional roles of gene ensembles.
BRAINCELL-AID also identifies Basal Ganglia-related cell types with
neurologically meaningful descriptions. Hence, we create a valuable resource to
support community-driven cell type annotation.

</details>


### [154] [Structured Debate Improves Corporate Credit Reasoning in Financial AI](https://arxiv.org/abs/2510.17108)
*Yoonjin Lee,Munhee Kim,Hanbi Choi,Juhyeon Park,Seungho Lyoo,Woojin Park*

Main category: cs.AI

TL;DR: 开发了两种基于大语言模型的系统用于企业信贷评估中的证据推理：单代理系统(NAS)和基于辩论的多代理系统(KPD-MADS)，后者基于卡尔·波普尔的批判对话框架，在推理质量和实用性方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 当前金融AI在企业信贷评估中主要关注数值预测，缺乏对定性非财务指标的解释性判断支持，而这些指标对贷款偿还结果具有决定性影响。

Method: 开发了两种系统：单代理系统(NAS)通过单次推理生成双向分析；多代理系统(KPD-MADS)采用十步结构化交互协议，基于卡尔·波普尔的批判对话框架进行对抗性验证。

Result: 两个系统都实现了显著的生产力提升(NAS: 11.55秒/案例；KPD-MADS: 91.97秒；人工基准: 1920秒)。KPD-MADS在解释充分性、实际适用性和可用性方面获得更高评分。

Conclusion: 结构化多代理交互能够增强金融AI中的推理严谨性和可解释性，推动企业信贷评估的可扩展和可辩护自动化。

Abstract: Despite advances in financial AI, the automation of evidence-based reasoning
remains unresolved in corporate credit assessment, where qualitative
non-financial indicators exert decisive influence on loan repayment outcomes
yet resist formalization. Existing approaches focus predominantly on numerical
prediction and provide limited support for the interpretive judgments required
in professional loan evaluation. This study develops and evaluates two
operational large language model (LLM)-based systems designed to generate
structured reasoning from non-financial evidence. The first is a
non-adversarial single-agent system (NAS) that produces bidirectional analysis
through a single-pass reasoning pipeline. The second is a debate-based
multi-agent system (KPD-MADS) that operationalizes adversarial verification
through a ten-step structured interaction protocol grounded in Karl Popper's
critical dialogue framework. Both systems were applied to three real corporate
cases and evaluated by experienced credit risk professionals. Compared to
manual expert reporting, both systems achieved substantial productivity gains
(NAS: 11.55 s per case; KPD-MADS: 91.97 s; human baseline: 1920 s). The
KPD-MADS demonstrated superior reasoning quality, receiving higher median
ratings in explanatory adequacy (4.0 vs. 3.0), practical applicability (4.0 vs.
3.0), and usability (62.5 vs. 52.5). These findings show that structured
multi-agent interaction can enhance reasoning rigor and interpretability in
financial AI, advancing scalable and defensible automation in corporate credit
assessment.

</details>


### [155] [Enhanced Fish Freshness Classification with Incremental Handcrafted Feature Fusion](https://arxiv.org/abs/2510.17145)
*Phi-Hung Hoang,Nam-Thuan Trinh,Van-Manh Tran,Thi-Thu-Hong Phan*

Main category: cs.AI

TL;DR: 提出了一种基于手工特征的方法，通过从鱼眼图像中提取颜色统计、多色彩空间直方图以及纹理特征来评估鱼类新鲜度，显著优于之前的深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 传统感官评估鱼类新鲜度存在主观性、不一致性和难以标准化的问题，需要客观可靠的自动化评估方法。

Method: 从鱼眼图像中系统提取并融合互补特征描述符，包括颜色统计、多色彩空间直方图、局部二值模式(LBP)和灰度共生矩阵(GLCM)等纹理特征，分别从完整图像和感兴趣区域捕获全局和局部变化。

Result: 在FFE数据集上，LightGBM分类器达到77.56%准确率，比之前深度学习方法提升14.35%；使用增强数据时，ANN达到97.16%准确率，比之前最佳结果提升19.86%。

Conclusion: 精心设计的手工特征经过策略性处理后，能够为自动化鱼类新鲜度评估提供鲁棒、可解释且可靠的解决方案，对食品质量监控具有重要应用价值。

Abstract: Accurate assessment of fish freshness remains a major challenge in the food
industry, with direct consequences for product quality, market value, and
consumer health. Conventional sensory evaluation is inherently subjective,
inconsistent, and difficult to standardize across contexts, often limited by
subtle, species-dependent spoilage cues. To address these limitations, we
propose a handcrafted feature-based approach that systematically extracts and
incrementally fuses complementary descriptors, including color statistics,
histograms across multiple color spaces, and texture features such as Local
Binary Patterns (LBP) and Gray-Level Co-occurrence Matrices (GLCM), from fish
eye images. Our method captures global chromatic variations from full images
and localized degradations from ROI segments, fusing each independently to
evaluate their effectiveness in assessing freshness. Experiments on the
Freshness of the Fish Eyes (FFE) dataset demonstrate the approach's
effectiveness: in a standard train-test setting, a LightGBM classifier achieved
77.56% accuracy, a 14.35% improvement over the previous deep learning baseline
of 63.21%. With augmented data, an Artificial Neural Network (ANN) reached
97.16% accuracy, surpassing the prior best of 77.3% by 19.86%. These results
demonstrate that carefully engineered, handcrafted features, when strategically
processed, yield a robust, interpretable, and reliable solution for automated
fish freshness assessment, providing valuable insights for practical
applications in food quality monitoring.

</details>


### [156] [Physics-Informed Large Language Models for HVAC Anomaly Detection with Autonomous Rule Generation](https://arxiv.org/abs/2510.17146)
*Subin Lin,Chuanbo Hua*

Main category: cs.AI

TL;DR: PILLM是一个基于物理知识的大语言模型框架，通过进化循环自动生成、评估和优化HVAC系统异常检测规则，在保持可解释性的同时实现最先进的性能。


<details>
  <summary>Details</summary>
Motivation: HVAC系统能耗巨大，传统基于规则的方法可解释但缺乏适应性，深度学习方法预测能力强但缺乏透明度和物理合理性，现有LLM方法忽视了HVAC运行的物理原理。

Method: 提出PILLM框架，在进化循环中嵌入物理知识反射和交叉算子，结合热力学和控制理论约束，自动生成和优化异常检测规则。

Result: 在公开的建筑故障检测数据集上实验表明，PILLM实现了最先进的性能，同时产生可解释和可操作的诊断规则。

Conclusion: PILLM推动了智能建筑系统中可信赖和可部署AI的发展，在保持物理基础的同时实现了自适应异常检测。

Abstract: Heating, Ventilation, and Air-Conditioning (HVAC) systems account for a
substantial share of global building energy use, making reliable anomaly
detection essential for improving efficiency and reducing emissions. Classical
rule-based approaches offer explainability but lack adaptability, while deep
learning methods provide predictive power at the cost of transparency,
efficiency, and physical plausibility. Recent attempts to use Large Language
Models (LLMs) for anomaly detection improve interpretability but largely ignore
the physical principles that govern HVAC operations. We present PILLM, a
Physics-Informed LLM framework that operates within an evolutionary loop to
automatically generate, evaluate, and refine anomaly detection rules. Our
approach introduces physics-informed reflection and crossover operators that
embed thermodynamic and control-theoretic constraints, enabling rules that are
both adaptive and physically grounded. Experiments on the public Building Fault
Detection dataset show that PILLM achieves state-of-the-art performance while
producing diagnostic rules that are interpretable and actionable, advancing
trustworthy and deployable AI for smart building systems.

</details>


### [157] [Which LLM Multi-Agent Protocol to Choose?](https://arxiv.org/abs/2510.17149)
*Hongyi Du,Jiaqi Su,Jisen Li,Lijie Ding,Yingxuan Yang,Peixuan Han,Xiangru Tang,Kunlun Zhu,Jiaxuan You*

Main category: cs.AI

TL;DR: ProtocolBench是一个评估多智能体系统通信协议的基准测试，ProtocolRouter是一个可学习的协议路由器，能根据场景需求选择最优协议。


<details>
  <summary>Details</summary>
Motivation: 大规模多智能体系统中，通信协议选择缺乏标准化指导，当前选择往往基于直觉而非系统评估。

Method: 开发ProtocolBench基准测试，从任务成功率、端到端延迟、消息开销和故障恢复能力四个维度评估协议性能；提出ProtocolRouter协议路由器，根据需求和运行时信号选择最优协议。

Result: 不同协议在性能上差异显著：Streaming Queue场景完成时间差异达36.5%，端到端延迟差异3.48秒；ProtocolRouter相比最佳单协议基线，故障恢复时间减少18.1%，在GAIA场景中成功率更高。

Conclusion: 协议选择对系统性能影响重大，ProtocolBench和ProtocolRouter为协议评估和选择提供了标准化工具，能显著提升系统可靠性和性能。

Abstract: As large-scale multi-agent systems evolve, the communication protocol layer
has become a critical yet under-evaluated factor shaping performance and
reliability. Despite the existence of diverse protocols (A2A, ACP, ANP, Agora,
etc.), selection is often intuition-driven and lacks standardized guidance. We
introduce ProtocolBench, a benchmark that systematically compares agent
protocols along four measurable axes: task success, end-to-end latency, message
or byte overhead, and robustness under failures. On ProtocolBench, protocol
choice significantly influences system behavior. In the Streaming Queue
scenario, overall completion time varies by up to 36.5% across protocols, and
mean end-to-end latency differs by 3.48 s. Under Fail-Storm Recovery,
resilience also differs consistently across protocols. Beyond evaluation, we
present ProtocolRouter, a learnable protocol router that selects per-scenario
(or per-module) protocols from requirement and runtime signals. ProtocolRouter
reduces Fail-Storm recovery time by up to 18.1% versus the best single-protocol
baseline, and achieves scenario-specific gains such as higher success in GAIA.
We also release ProtocolRouterBench to standardize protocol evaluation and
improve reliability at scale.

</details>


### [158] [Combining ECG Foundation Model and XGBoost to Predict In-Hospital Malignant Ventricular Arrhythmias in AMI Patients](https://arxiv.org/abs/2510.17172)
*Shun Huang,Wenlu Xing,Shijia Geng,Hailong Wang,Guangkun Nie,Gongzheng Tang,Chenyang He,Shenda Hong*

Main category: cs.AI

TL;DR: 开发了一个结合ECG基础模型和可解释XGBoost分类器的混合预测框架，用于预测急性心肌梗死后恶性室性心律失常风险，在准确性和可解释性方面均优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 急性心肌梗死后恶性室性心律失常是院内死亡的主要原因，传统风险评分性能有限，而端到端深度学习模型缺乏临床信任所需的可解释性。

Method: 使用ECG基础模型提取150维诊断概率特征，通过特征选择精炼后训练XGBoost分类器，采用SHAP方法进行可解释性分析。

Result: 混合模型AUC达到0.801，优于KNN(0.677)、RNN(0.676)和1D-CNN(0.720)，SHAP分析显示模型识别特征与临床知识高度一致。

Conclusion: 该混合框架通过验证基础模型输出作为有效的自动化特征工程，为构建可信赖、可解释的AI临床决策支持系统提供了新范式。

Abstract: Malignant ventricular arrhythmias (VT/VF) following acute myocardial
infarction (AMI) are a major cause of in-hospital death, yet early
identification remains a clinical challenge. While traditional risk scores have
limited performance, end-to-end deep learning models often lack the
interpretability needed for clinical trust. This study aimed to develop a
hybrid predictive framework that integrates a large-scale electrocardiogram
(ECG) foundation model (ECGFounder) with an interpretable XGBoost classifier to
improve both accuracy and interpretability. We analyzed 6,634 ECG recordings
from AMI patients, among whom 175 experienced in-hospital VT/VF. The ECGFounder
model was used to extract 150-dimensional diagnostic probability features ,
which were then refined through feature selection to train the XGBoost
classifier. Model performance was evaluated using AUC and F1-score , and the
SHAP method was used for interpretability. The ECGFounder + XGBoost hybrid
model achieved an AUC of 0.801 , outperforming KNN (AUC 0.677), RNN (AUC
0.676), and an end-to-end 1D-CNN (AUC 0.720). SHAP analysis revealed that
model-identified key features, such as "premature ventricular complexes" (risk
predictor) and "normal sinus rhythm" (protective factor), were highly
consistent with clinical knowledge. We conclude that this hybrid framework
provides a novel paradigm for VT/VF risk prediction by validating the use of
foundation model outputs as effective, automated feature engineering for
building trustworthy, explainable AI-based clinical decision support systems.

</details>


### [159] [Offline Policy Evaluation of Multi-Turn LLM Health Coaching with Real Users](https://arxiv.org/abs/2510.17173)
*Melik Ozolcer,Sang Won Bae*

Main category: cs.AI

TL;DR: 研究了一个基于工具的LLM健康教练系统，通过离线策略评估发现统一的重工具策略虽然提升平均价值但损害特定用户群体，特别是低健康素养/高自我效能用户。模拟器实验表明添加早期信息增益奖励可改善个性化效果。


<details>
  <summary>Details</summary>
Motivation: 探索在真实用户环境中部署工具增强LLM健康教练的个性化效果，识别不同用户群体对工具使用策略的差异化响应，避免平均指标掩盖的亚群损害。

Method: 使用离线策略评估(OPE)分析因子化决策头(工具/风格)，通过轻量级模拟器测试添加信息增益奖励的效果，评估不同用户原型对策略的响应。

Result: 统一重工具策略提升平均价值但损害低健康素养/高自我效能用户；添加早期信息增益奖励可缩短特征识别时间，提高目标成功率和pass@3指标。

Conclusion: 提出评估优先的个性化路径：冻结生成器，在类型化奖励上学习亚群感知决策头，始终报告按原型分类的指标以揭示平均指标掩盖的亚群损害。

Abstract: We study a web-deployed, tool-augmented LLM health coach with real users. In
a pilot with seven users (280 rated turns), offline policy evaluation (OPE)
over factorized decision heads (Tool/Style) shows that a uniform heavy-tool
policy raises average value on logs but harms specific subgroups, most notably
low-health-literacy/high-self-efficacy users. A lightweight simulator with
hidden archetypes further shows that adding a small early information-gain
bonus reliably shortens trait identification and improves goal success and
pass@3. Together, these early findings indicate an evaluation-first path to
personalization: freeze the generator, learn subgroup-aware decision heads on
typed rewards (objective tool outcomes and satisfaction), and always report
per-archetype metrics to surface subgroup harms that averages obscure.

</details>


### [160] [Temporally Detailed Hypergraph Neural ODEs for Type 2 Diabetes Progression Modeling](https://arxiv.org/abs/2510.17211)
*Tingsong Xiao,Yao An Lee,Zelin Xu,Yupu Zhang,Zibo Liu,Yu Huang,Jiang Bian,Serena Jingchuan Guo,Zhe Jiang*

Main category: cs.AI

TL;DR: 提出TD-HNODE模型，使用时间详细超图和神经ODE框架来建模疾病进展的连续时间动态，在2型糖尿病和心血管疾病进展建模中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效处理基于不规则时间事件样本的患者异质性和复杂连续时间动态，需要更准确的疾病进展建模来改善患者亚分型和干预时机。

Method: TD-HNODE将疾病进展表示为时间详细超图，通过可学习的超图拉普拉斯算子捕获进展轨迹内和轨迹间的疾病并发症标记相互依赖关系，使用神经ODE框架学习连续时间进展动态。

Result: 在两个真实世界临床数据集上的实验表明，TD-HNODE在2型糖尿病和相关心血管疾病进展建模方面优于多个基线方法。

Conclusion: TD-HNODE能够有效建模疾病进展的连续时间动态，为患者亚分型和及时干预提供了更准确的工具。

Abstract: Disease progression modeling aims to characterize and predict how a patient's
disease complications worsen over time based on longitudinal electronic health
records (EHRs). Accurate modeling of disease progression, such as type 2
diabetes, can enhance patient sub-phenotyping and inform effective and timely
interventions. However, the problem is challenging due to the need to learn
continuous-time dynamics of progression patterns based on irregular-time event
samples and patient heterogeneity (\eg different progression rates and
pathways). Existing mechanistic and data-driven methods either lack
adaptability to learn from real-world data or fail to capture complex
continuous-time dynamics on progression trajectories. To address these
limitations, we propose Temporally Detailed Hypergraph Neural Ordinary
Differential Equation (TD-HNODE), which represents disease progression on
clinically recognized trajectories as a temporally detailed hypergraph and
learns the continuous-time progression dynamics via a neural ODE framework.
TD-HNODE contains a learnable TD-Hypergraph Laplacian that captures the
interdependency of disease complication markers within both intra- and
inter-progression trajectories. Experiments on two real-world clinical datasets
demonstrate that TD-HNODE outperforms multiple baselines in modeling the
progression of type 2 diabetes and related cardiovascular diseases.

</details>


### [161] [Coinvisor: An RL-Enhanced Chatbot Agent for Interactive Cryptocurrency Investment Analysis](https://arxiv.org/abs/2510.17235)
*Chong Chen,Ze Liu,Lingfeng Bao,Yanlin Wang,Ting Chen,Daoyuan Wu,Jiachi Chen*

Main category: cs.AI

TL;DR: Coinvisor是一个基于强化学习的加密货币投资分析聊天机器人，通过多智能体框架提供全面的分析支持，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 加密货币市场存在高波动性和信息碎片化问题，现有分析方法包括手动分析、数据聚合平台和LLM代理都存在功能限制，缺乏实时数据集成和多步推理能力。

Method: 采用基于强化学习的工具选择机制，通过多智能体框架集成多样化分析能力，支持多步规划和灵活的数据源整合。

Result: 在工具编排方面，相比基础模型召回率提升40.7%，F1分数提升26.6%。用户研究显示高满意度(4.64/5)，参与者更偏好Coinvisor而非通用LLM和现有加密平台(4.62/5)。

Conclusion: Coinvisor通过强化学习驱动的多智能体框架，成功实现了实时交互和动态内容的自适应分析，为加密货币投资提供了准确可行的见解。

Abstract: The cryptocurrency market offers significant investment opportunities but
faces challenges including high volatility and fragmented information. Data
integration and analysis are essential for informed investment decisions.
Currently, investors use three main approaches: (1) Manual analysis across
various sources, which depends heavily on individual experience and is
time-consuming and prone to bias; (2) Data aggregation platforms-limited in
functionality and depth of analysis; (3) Large language model agents-based on
static pretrained models, lacking real-time data integration and multi-step
reasoning capabilities. To address these limitations, we present Coinvisor, a
reinforcement learning-based chatbot that provides comprehensive analytical
support for cryptocurrency investment through a multi-agent framework.
Coinvisor integrates diverse analytical capabilities through specialized tools.
Its key innovation is a reinforcement learning-based tool selection mechanism
that enables multi-step planning and flexible integration of diverse data
sources. This design supports real-time interaction and adaptive analysis of
dynamic content, delivering accurate and actionable investment insights. We
evaluated Coinvisor through automated benchmarks on tool calling accuracy and
user studies with 20 cryptocurrency investors using our interface. Results show
that Coinvisor improves recall by 40.7% and F1 score by 26.6% over the base
model in tool orchestration. User studies show high satisfaction (4.64/5), with
participants preferring Coinvisor to both general LLMs and existing crypto
platforms (4.62/5).

</details>


### [162] [RubiSCoT: A Framework for AI-Supported Academic Assessment](https://arxiv.org/abs/2510.17309)
*Thorsten Fröhlich,Tim Schlippe*

Main category: cs.AI

TL;DR: 提出了RubiSCoT框架，利用AI技术增强学术论文评估过程，从提案到最终提交提供一致、可扩展的解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统论文评估方法耗时且存在评估者主观性差异，需要更高效、一致的评估解决方案。

Method: 使用先进自然语言处理技术，包括大语言模型、检索增强生成和结构化思维链提示，提供初步评估、多维度评估、内容提取、基于评分标准的评分和详细报告。

Result: 设计了RubiSCoT框架并实现了其功能，展示了在学术评估过程中的应用潜力。

Conclusion: RubiSCoT框架有潜力通过提供一致、可扩展和透明的评估来优化学术评估流程。

Abstract: The evaluation of academic theses is a cornerstone of higher education,
ensuring rigor and integrity. Traditional methods, though effective, are
time-consuming and subject to evaluator variability. This paper presents
RubiSCoT, an AI-supported framework designed to enhance thesis evaluation from
proposal to final submission. Using advanced natural language processing
techniques, including large language models, retrieval-augmented generation,
and structured chain-of-thought prompting, RubiSCoT offers a consistent,
scalable solution. The framework includes preliminary assessments,
multidimensional assessments, content extraction, rubric-based scoring, and
detailed reporting. We present the design and implementation of RubiSCoT,
discussing its potential to optimize academic assessment processes through
consistent, scalable, and transparent evaluation.

</details>


### [163] [Graph Attention-Guided Search for Dense Multi-Agent Pathfinding](https://arxiv.org/abs/2510.17382)
*Rishabh Jain,Keisuke Okumura,Michael Amir,Amanda Prorok*

Main category: cs.AI

TL;DR: 提出LaGAT框架，将基于图注意力的神经MAPF策略MAGAT与搜索算法LaCAM结合，在密集多智能体路径规划场景中优于纯搜索和纯学习方法。


<details>
  <summary>Details</summary>
Motivation: 密集多智能体路径规划问题在实时场景中仍然具有挑战性，现有方法表现不佳。

Method: 集成MAGAT的启发式到LaCAM算法中，采用预训练-微调策略和死锁检测机制。

Result: 在密集场景中优于纯搜索和纯学习方法。

Conclusion: 精心设计的混合搜索方法为紧密耦合的多智能体协调问题提供了强大解决方案。

Abstract: Finding near-optimal solutions for dense multi-agent pathfinding (MAPF)
problems in real-time remains challenging even for state-of-the-art planners.
To this end, we develop a hybrid framework that integrates a learned heuristic
derived from MAGAT, a neural MAPF policy with a graph attention scheme, into a
leading search-based algorithm, LaCAM. While prior work has explored
learning-guided search in MAPF, such methods have historically underperformed.
In contrast, our approach, termed LaGAT, outperforms both purely search-based
and purely learning-based methods in dense scenarios. This is achieved through
an enhanced MAGAT architecture, a pre-train-then-fine-tune strategy on maps of
interest, and a deadlock detection scheme to account for imperfect neural
guidance. Our results demonstrate that, when carefully designed, hybrid search
offers a powerful solution for tightly coupled, challenging multi-agent
coordination problems.

</details>


### [164] [Diverse Planning with Simulators via Linear Temporal Logic](https://arxiv.org/abs/2510.17418)
*Mustafa F. Abdelwahed,Alice Toniolo,Joan Espasa,Ian P. Gent*

Main category: cs.AI

TL;DR: 提出了FBI_LTL，一个专门为基于仿真的规划问题设计的多样化规划器，使用线性时序逻辑定义语义多样性标准，生成语义上不同的计划。


<details>
  <summary>Details</summary>
Motivation: 传统规划器只生成单一计划，可能无法满足代理偏好；现有多样化规划方法可能产生语法不同但语义相同的解决方案。

Method: 使用线性时序逻辑定义语义多样性标准，并将这些LTL多样性模型直接集成到搜索过程中。

Result: 在各种基准测试中，FBI_LTL相比基线方法能生成更多样化的计划。

Conclusion: 这项工作确立了在基于仿真的环境中进行语义引导多样化规划的可行性，为在传统基于模型的方法失效的现实非符号领域开辟了新途径。

Abstract: Autonomous agents rely on automated planning algorithms to achieve their
objectives. Simulation-based planning offers a significant advantage over
declarative models in modelling complex environments. However, relying solely
on a planner that produces a single plan may not be practical, as the generated
plans may not always satisfy the agent's preferences. To address this
limitation, we introduce $\texttt{FBI}_\texttt{LTL}$, a diverse planner
explicitly designed for simulation-based planning problems.
$\texttt{FBI}_\texttt{LTL}$ utilises Linear Temporal Logic (LTL) to define
semantic diversity criteria, enabling agents to specify what constitutes
meaningfully different plans. By integrating these LTL-based diversity models
directly into the search process, $\texttt{FBI}_\texttt{LTL}$ ensures the
generation of semantically diverse plans, addressing a critical limitation of
existing diverse planning approaches that may produce syntactically different
but semantically identical solutions. Extensive evaluations on various
benchmarks consistently demonstrate that $\texttt{FBI}_\texttt{LTL}$ generates
more diverse plans compared to a baseline approach. This work establishes the
feasibility of semantically-guided diverse planning in simulation-based
environments, paving the way for innovative approaches in realistic,
non-symbolic domains where traditional model-based approaches fail.

</details>


### [165] [Active Inference for an Intelligent Agent in Autonomous Reconnaissance Missions](https://arxiv.org/abs/2510.17450)
*Johan Schubert,Farzad Kamrani,Tove Gustavi*

Main category: cs.AI

TL;DR: 提出了一种基于主动推理的路径规划方法，用于智能代理的自主控制，通过构建证据地图和计算变分自由能量来指导代理在探索和利用之间平衡移动。


<details>
  <summary>Details</summary>
Motivation: 开发自主控制智能代理的方法，以侦察地理区域并维持共同作战态势图，解决探索和利用之间的平衡问题。

Method: 使用Dempster-Shafer理论和高斯传感器模型构建证据地图，计算变分自由能量作为位置评估标准，通过最小化自由能量来指导代理移动。

Result: 该方法能够有效指导代理在地理区域内进行侦察，平衡搜索广阔区域和跟踪已识别目标对象的需求。

Conclusion: 主动推理路径规划方法为智能代理的自主控制提供了有效解决方案，能够处理探索与利用的权衡问题。

Abstract: We develop an active inference route-planning method for the autonomous
control of intelligent agents. The aim is to reconnoiter a geographical area to
maintain a common operational picture. To achieve this, we construct an
evidence map that reflects our current understanding of the situation,
incorporating both positive and "negative" sensor observations of possible
target objects collected over time, and diffusing the evidence across the map
as time progresses. The generative model of active inference uses
Dempster-Shafer theory and a Gaussian sensor model, which provides input to the
agent. The generative process employs a Bayesian approach to update a posterior
probability distribution. We calculate the variational free energy for all
positions within the area by assessing the divergence between a pignistic
probability distribution of the evidence map and a posterior probability
distribution of a target object based on the observations, including the level
of surprise associated with receiving new observations. Using the free energy,
we direct the agents' movements in a simulation by taking an incremental step
toward a position that minimizes the free energy. This approach addresses the
challenge of exploration and exploitation, allowing agents to balance searching
extensive areas of the geographical map while tracking identified target
objects.

</details>


### [166] [Label Indeterminacy in AI & Law](https://arxiv.org/abs/2510.17463)
*Cor Steging,Tadeusz Zbiegień*

Main category: cs.AI

TL;DR: 法律机器学习需要解决标签不确定性问题，因为法律案件结果往往受到人为干预影响，导致标签存在不确定性，这会显著影响模型行为。


<details>
  <summary>Details</summary>
Motivation: 法律机器学习通常将过去案件结果视为真实标签，但法律结果往往受到和解、上诉等人为干预影响，导致标签存在不确定性，需要对此进行研究。

Method: 在欧洲人权法院案件分类的背景下，研究不同标签构建方式如何影响模型行为，展示标签不确定性的影响。

Result: 研究发现，训练过程中标签的构建方式会显著影响模型的行为表现。

Conclusion: 标签不确定性是AI与法律领域的重要关注点，需要被纳入考虑以改善法律机器学习应用。

Abstract: Machine learning is increasingly used in the legal domain, where it typically
operates retrospectively by treating past case outcomes as ground truth.
However, legal outcomes are often shaped by human interventions that are not
captured in most machine learning approaches. A final decision may result from
a settlement, an appeal, or other procedural actions. This creates label
indeterminacy: the outcome could have been different if the intervention had or
had not taken place. We argue that legal machine learning applications need to
account for label indeterminacy. Methods exist that can impute these
indeterminate labels, but they are all grounded in unverifiable assumptions. In
the context of classifying cases from the European Court of Human Rights, we
show that the way that labels are constructed during training can significantly
affect model behaviour. We therefore position label indeterminacy as a relevant
concern in AI & Law and demonstrate how it can shape model behaviour.

</details>


### [167] [MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning](https://arxiv.org/abs/2510.17590)
*Mir Nafis Sharear Shopnil,Sharad Duwal,Abhishek Tyagi,Adiba Mahbub Proma*

Main category: cs.AI

TL;DR: MIRAGE是一个用于检测多模态虚假信息的智能代理框架，通过分解验证过程为四个模块：视觉真实性评估、跨模态一致性分析、检索增强的事实核查和校准判断，无需领域特定训练数据即可实现高性能检测。


<details>
  <summary>Details</summary>
Motivation: 网络平台上每天有数十亿包含文本和图像的多模态帖子传播虚假信息，人工事实核查能力不足。现有的监督检测模型需要领域特定训练数据，且难以泛化到不同的操纵策略。

Method: MIRAGE框架包含四个顺序模块：1) 视觉真实性评估检测AI生成图像；2) 跨模态一致性分析识别上下文不当的重新利用；3) 检索增强的事实核查通过迭代问题生成将声明与网络证据关联；4) 校准判断模块整合所有信号。该框架协调视觉语言模型推理与目标网络检索，输出结构化的引用链接推理。

Result: 在MMFakeBench验证集（1,000样本）上，MIRAGE与GPT-4o-mini组合达到81.65% F1分数和75.1%准确率，比最强的零样本基线（GPT-4V与MMD-Agent，74.0% F1）高出7.65点，同时保持34.3%的假阳性率，而仅判断基线的假阳性率为97.3%。测试集结果（5,000样本）确认了泛化能力，达到81.44% F1和75.08%准确率。消融研究显示视觉验证贡献5.18 F1点，检索增强推理贡献2.97点。

Conclusion: 分解的智能代理推理与网络检索相结合，可以在没有领域特定训练的情况下匹配监督检测器的性能，使得在多模态虚假信息检测领域，即使标注数据稀缺也能实现有效检测。

Abstract: Misinformation spreads across web platforms through billions of daily
multimodal posts that combine text and images, overwhelming manual
fact-checking capacity. Supervised detection models require domain-specific
training data and fail to generalize across diverse manipulation tactics. We
present MIRAGE, an inference-time, model-pluggable agentic framework that
decomposes multimodal verification into four sequential modules: visual
veracity assessment detects AI-generated images, cross-modal consistency
analysis identifies out-of-context repurposing, retrieval-augmented factual
checking grounds claims in web evidence through iterative question generation,
and a calibrated judgment module integrates all signals. MIRAGE orchestrates
vision-language model reasoning with targeted web retrieval, outputs structured
and citation-linked rationales. On MMFakeBench validation set (1,000 samples),
MIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming
the strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65
points while maintaining 34.3% false positive rate versus 97.3% for a
judge-only baseline. Test set results (5,000 samples) confirm generalization
with 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification
contributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97
points. Our results demonstrate that decomposed agentic reasoning with web
retrieval can match supervised detector performance without domain-specific
training, enabling misinformation detection across modalities where labeled
data remains scarce.

</details>


### [168] [Reasoning Distillation and Structural Alignment for Improved Code Generation](https://arxiv.org/abs/2510.17598)
*Amir Jalilifard,Anderson de Rezende Rocha,Marcos Medeiros Raimundo*

Main category: cs.AI

TL;DR: 该研究将大型语言模型的推理能力蒸馏到更小、高效的模型中，通过结构感知损失优化方法，使小模型能够理解问题与解决方案之间的结构对应关系，在多个基准测试中显著提升了代码生成性能。


<details>
  <summary>Details</summary>
Motivation: 代码生成不仅需要准确预测token，更需要理解解决方案级别的结构关系。大型语言模型具备复杂推理能力，但部署成本高；小模型缺乏这种推理能力。因此需要将大模型的推理能力蒸馏到小模型中。

Method: 通过结构感知损失优化方法，训练小模型模拟大模型的推理和问题解决能力，学习识别正确解决方案路径，建立问题定义与潜在解决方案之间的结构对应关系。

Result: 经过廉价简单的微调过程，我们的模型在MBPP、MBPP Plus和HumanEval基准测试中，在pass@1、平均数据流和平均语法匹配指标上显著优于基线模型。

Conclusion: 通过将大型语言模型的推理能力蒸馏到小模型中，可以实现高效、低成本的代码生成，同时保持强大的问题解决能力。

Abstract: Effective code generation with language models hinges on two critical
factors: accurately understanding the intent of the prompt and generating code
that applies algorithmic reasoning to produce correct solutions capable of
passing diverse test cases while adhering to the syntax of the target
programming language. Unlike other language tasks, code generation requires
more than accurate token prediction; it demands comprehension of solution-level
and structural relationships rather than merely generating the most likely
tokens. very large language model (VLLM) are capable of generating detailed
steps toward the correct solution of complex tasks where reasoning is crucial
in solving the problem. Such reasoning capabilities may be absent in smaller
language models. Therefore, in this work, we distill the reasoning capabilities
of a VLLM into a smaller, more efficient model that is faster and cheaper to
deploy. Our approach trains the model to emulate the reasoning and
problem-solving abilities of the VLLM by learning to identify correct solution
pathways and establishing a structural correspondence between problem
definitions and potential solutions through a novel method of structure-aware
loss optimization. This enables the model to transcend token-level generation
and to deeply grasp the overarching structure of solutions for given problems.
Experimental results show that our fine-tuned model, developed through a cheap
and simple to implement process, significantly outperforms our baseline model
in terms of pass@1, average data flow, and average syntax match metrics across
the MBPP, MBPP Plus, and HumanEval benchmarks.

</details>


### [169] [OG-Rank: Learning to Rank Fast and Slow with Uncertainty and Reward-Trend Guided Adaptive Exploration](https://arxiv.org/abs/2510.17614)
*Praphul Singh,Corey Barrett,Sumana Srivasta,Irfan Bulu,Sri Gadde,Krishnaram Kenthapadi*

Main category: cs.AI

TL;DR: OG-Rank是一个低延迟的重新排序系统，使用单解码器架构，结合池化首词评分和不确定性门控解释步骤，在保持可预测延迟的同时提供排名和选择性解释。


<details>
  <summary>Details</summary>
Motivation: 临床医生需要实时工作并能解释其选择的排名系统，需要低延迟的解码器基础重新排序器。

Method: 采用单解码器方法，结合池化首词评分信号和不确定性门控解释步骤。模型在一次前向传递中为所有候选评分，仅在列表真正模糊时生成简短的结构化理由。使用课程学习策略，将训练重点放在困难案例上。

Result: 在遭遇范围订单选择任务中表现优异（快速路径：Recall@1~0.45，nDCG@20~0.625），当门控激活时性能进一步提升（Recall@1~0.56，nDCG@20~0.699，门控率45%）。紧凑骨干网络在相同策略下也显示出类似增益。

Conclusion: OG-Rank提供了一个实用方案：默认快速排名，在有助于时提供解释。这种模式适用于选择性生成能以可接受成本换取准确性的决策任务。单策略设计简化了部署和预算规划，课程学习原则可推广到临床订单选择之外。

Abstract: Clinicians need ranking systems that work in real time and still justify
their choices. Motivated by the need for a low-latency, decoder-based reranker,
we present OG-Rank, a single-decoder approach that pairs a pooled first-token
scoring signal with an uncertainty-gated explanation step. The model scores all
candidates in one pass and generates a brief, structured rationale only when
the list is genuinely ambiguous, keeping latency predictable. Trained with a
curriculum that concentrates effort on hard cases, OG-Rank delivers strong
effectiveness on encounter-scoped order selection (fast path: Recall@1~0.45,
nDCG@20~0.625) and improves further when the gate activates (Recall@1~0.56,
nDCG@20~0.699 at a 45\% gate rate), while compact backbones show similar gains
under the same policy. Encoder baselines trail in both effectiveness and
flexibility. The result is a practical recipe: rank fast by default and explain
when it helps, a pattern that applies broadly to decision tasks where selective
generation buys accuracy at acceptable cost. The single-policy design
simplifies deployment and budget planning, and the curriculum principle (spend
more on the hard cases, less on the easy ones) readily transfers beyond
clinical order selection.

</details>


### [170] [LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet Arena](https://arxiv.org/abs/2510.17638)
*Qingchuan Yang,Simon Mahns,Sida Li,Anri Gu,Jibang Wu,Haifeng Xu*

Main category: cs.AI

TL;DR: 本文系统研究了LLM作为预测工具的能力，构建了Prophet Arena基准测试平台，发现LLM已具备令人印象深刻的预测能力，但也存在事件召回不准确、数据源误解等关键瓶颈。


<details>
  <summary>Details</summary>
Motivation: 随着在互联网规模数据上训练的大语言模型快速发展，利用LLM预测现实世界未来事件具有重要潜力，这种新兴范式被称为"LLM-as-a-Prophet"。

Method: 构建Prophet Arena评估基准，持续收集实时预测任务，并将每个任务分解为不同的流水线阶段，以支持受控的大规模实验。

Result: 综合评估显示许多LLM已展现出令人印象深刻的预测能力，表现为较小的校准误差、一致的预测置信度和有前景的市场回报。但也发现了关键瓶颈。

Conclusion: LLM作为预测工具具有潜力，但需要解决事件召回不准确、数据源误解以及在接近解决时比市场信息聚合更慢等关键瓶颈。

Abstract: Forecasting is not only a fundamental intellectual pursuit but also is of
significant importance to societal systems such as finance and economics. With
the rapid advances of large language models (LLMs) trained on Internet-scale
data, it raises the promise of employing LLMs to forecast real-world future
events, an emerging paradigm we call "LLM-as-a-Prophet". This paper
systematically investigates such predictive intelligence of LLMs. To this end,
we build Prophet Arena, a general evaluation benchmark that continuously
collects live forecasting tasks and decomposes each task into distinct pipeline
stages, in order to support our controlled and large-scale experimentation. Our
comprehensive evaluation reveals that many LLMs already exhibit impressive
forecasting capabilities, reflected in, e.g., their small calibration errors,
consistent prediction confidence and promising market returns. However, we also
uncover key bottlenecks towards achieving superior predictive intelligence via
LLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of
data sources and slower information aggregation compared to markets when
resolution nears.

</details>


### [171] [A Principle of Targeted Intervention for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.17697)
*Anjie Liu,Jianhong Wang,Samuel Kaski,Jun Wang,Mengyue Yang*

Main category: cs.AI

TL;DR: 该论文提出了基于多智能体影响图(MAIDs)的目标干预方法，通过仅对单个目标智能体进行干预来缓解大规模多智能体强化学习中全局指导不切实际的问题。


<details>
  <summary>Details</summary>
Motivation: 解决大规模多智能体强化学习中全局指导不切实际的问题，以及缺乏易用研究工具来协调智能体的挑战。

Method: 使用多智能体影响图(MAIDs)作为图形框架，设计目标干预范式，并引入预策略干预(PSI)因果推断技术来实现该范式。

Result: 实验证明了所提出的目标干预方法的有效性，并验证了相关性图分析的结果。

Conclusion: MAIDs框架为多智能体强化学习提供了有效的分析和可视化工具，目标干预方法能够有效实现期望结果，同时缓解全局指导的挑战。

Abstract: Steering cooperative multi-agent reinforcement learning (MARL) towards
desired outcomes is challenging, particularly when the global guidance from a
human on the whole multi-agent system is impractical in a large-scale MARL. On
the other hand, designing mechanisms to coordinate agents most relies on
empirical studies, lacking a easy-to-use research tool. In this work, we employ
multi-agent influence diagrams (MAIDs) as a graphical framework to address the
above issues. First, we introduce interaction paradigms that leverage MAIDs to
analyze and visualize existing approaches in MARL. Then, we design a new
interaction paradigm based on MAIDs, referred to as targeted intervention that
is applied to only a single targeted agent, so the problem of global guidance
can be mitigated. In our implementation, we introduce a causal inference
technique-referred to as Pre-Strategy Intervention (PSI)-to realize the
targeted intervention paradigm. Since MAIDs can be regarded as a special class
of causal diagrams, a composite desired outcome that integrates the primary
task goal and an additional desired outcome can be achieved by maximizing the
corresponding causal effect through the PSI. Moreover, the bundled relevance
graph analysis of MAIDs provides a tool to identify whether an MARL learning
paradigm is workable under the design of an interaction paradigm. In
experiments, we demonstrate the effectiveness of our proposed targeted
intervention, and verify the result of relevance graph analysis.

</details>


### [172] [Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation in Large Language Models](https://arxiv.org/abs/2510.17705)
*Dayan Pan,Zhaoyang Fu,Jingyuan Wang,Xiao Han,Yue Zhu,Xiangyu Zhao*

Main category: cs.AI

TL;DR: 提出了Contextual Attention Modulation (CAM)机制和HyCAM框架，通过动态调制自注意力表示来平衡知识保留和任务专业化，在多项任务上平均性能提升3.65%


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在多任务适应中的挑战，传统微调方法存在灾难性遗忘和资源消耗问题，现有参数高效方法在复杂多任务场景下表现不佳

Method: 提出CAM机制动态调制自注意力模块表示，HyCAM框架结合共享全参数CAM模块和多个轻量级专用CAM模块，采用动态路由策略进行自适应知识融合

Result: 在问答、代码生成和逻辑推理等异构任务上的广泛实验表明，该方法显著优于现有方法，平均性能提升3.65%

Conclusion: CAM和HyCAM框架有效解决了LLMs在多任务适应中的平衡问题，实现了更有效和高效的适应

Abstract: Large Language Models (LLMs) possess remarkable generalization capabilities
but struggle with multi-task adaptation, particularly in balancing knowledge
retention with task-specific specialization. Conventional fine-tuning methods
suffer from catastrophic forgetting and substantial resource consumption, while
existing parameter-efficient methods perform suboptimally in complex multi-task
scenarios. To address this, we propose Contextual Attention Modulation (CAM), a
novel mechanism that dynamically modulates the representations of
self-attention modules in LLMs. CAM enhances task-specific features while
preserving general knowledge, thereby facilitating more effective and efficient
adaptation. For effective multi-task adaptation, CAM is integrated into our
Hybrid Contextual Attention Modulation (HyCAM) framework, which combines a
shared, full-parameter CAM module with multiple specialized, lightweight CAM
modules, enhanced by a dynamic routing strategy for adaptive knowledge fusion.
Extensive experiments on heterogeneous tasks, including question answering,
code generation, and logical reasoning, demonstrate that our approach
significantly outperforms existing approaches, achieving an average performance
improvement of 3.65%. The implemented code and data are available to ease
reproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM.

</details>


### [173] [Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs](https://arxiv.org/abs/2510.17771)
*Zhining Liu,Ziyi Chen,Hui Liu,Chen Luo,Xianfeng Tang,Suhang Wang,Joy Zeng,Zhenwei Dai,Zhan Shi,Tianxin Wei,Benoit Dumoulin,Hanghang Tong*

Main category: cs.AI

TL;DR: 研究发现视觉语言模型在深层能够可靠地感知视觉证据，但未能有效利用这些信息，导致"看见但不相信"现象。通过基于注意力掩码的推理时干预，无需训练即可提升多个VLM家族的准确性。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉语言模型在多模态任务上表现良好，但即使存在正确的视觉证据时仍然会失败。研究旨在系统性地探究这些失败是由于未感知到证据还是未能有效利用证据。

Method: 通过分析层间注意力动态，发现浅层主要关注文本，而深层稀疏但可靠地关注局部证据区域。基于此，提出了一种推理时干预方法，通过选择性注意力掩码来突出深层证据区域。

Result: 实验表明，VLMs在输出错误答案时往往已经感知到了视觉证据。提出的干预方法无需训练，在LLaVA、Qwen、Gemma和InternVL等多个VLM家族中均能持续提升准确率。

Conclusion: VLMs内部编码了可靠的证据但未能充分利用，通过使这些信号显式化可以弥合感知与推理之间的差距，推进对VLM的诊断性理解和可靠性。

Abstract: Vision-Language Models (VLMs) achieve strong results on multimodal tasks such
as visual question answering, yet they can still fail even when the correct
visual evidence is present. In this work, we systematically investigate whether
these failures arise from not perceiving the evidence or from not leveraging it
effectively. By examining layer-wise attention dynamics, we find that shallow
layers focus primarily on text, while deeper layers sparsely but reliably
attend to localized evidence regions. Surprisingly, VLMs often perceive the
visual evidence when outputting incorrect answers, a phenomenon we term
``seeing but not believing'' that widely exists in major VLM families. Building
on this, we introduce an inference-time intervention that highlights deep-layer
evidence regions through selective attention-based masking. It requires no
training and consistently improves accuracy across multiple families, including
LLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable
evidence internally but under-utilize it, making such signals explicit can
bridge the gap between perception and reasoning, advancing the diagnostic
understanding and reliability of VLMs.

</details>
