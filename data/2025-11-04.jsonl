{"id": "2511.00010", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00010", "abs": "https://arxiv.org/abs/2511.00010", "authors": ["Jiajun Zhang", "Jianke Zhang", "Zeyu Cui", "Jiaxi Yang", "Lei Zhang", "Binyuan Hui", "Qiang Liu", "Zilei Wang", "Liang Wang", "Junyang Lin"], "title": "PlotCraft: Pushing the Limits of LLMs for Complex and Interactive Data Visualization", "comment": null, "summary": "Recent Large Language Models (LLMs) have demonstrated remarkable profi-\nciency in code generation. However, their ability to create complex visualiza-\ntions for scaled and structured data remains largely unevaluated and\nunderdevel- oped. To address this gap, we introduce PlotCraft, a new benchmark\nfeaturing 1k challenging visualization tasks that cover a wide range of topics,\nsuch as fi- nance, scientific research, and sociology. The benchmark is\nstructured around seven high-level visualization tasks and encompasses 48\ndistinct chart types. Cru- cially, it is the first to systematically evaluate\nboth single-turn generation and multi-turn refinement across a diverse spectrum\nof task complexities. Our com- prehensive evaluation of 23 leading LLMs on\nPlotCraft reveals obvious per- formance deficiencies in handling sophisticated\nvisualization tasks. To bridge this performance gap, we develope SynthVis-30K,\na large-scale, high-quality dataset of complex visualization code synthesized\nvia a collaborative agent frame- work. Building upon this dataset, we develope\nPlotCraftor, a novel code gener- ation model that achieves strong capabilities\nin complex data visualization with a remarkably small size. Across VisEval,\nPandasPlotBench, and our proposed PlotCraft, PlotCraftor shows performance\ncomparable to that of leading propri- etary approaches. Especially, on hard\ntask, Our model achieves over 50% per- formance improvement. We will release\nthe benchmark, dataset, and code at\nhttps://github.com/Speakn0w/PlotCraft-Benchmark."}
{"id": "2511.00115", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00115", "abs": "https://arxiv.org/abs/2511.00115", "authors": ["Haoyuan Li", "Yuanbo Tong", "Yuchen Li", "Zirui Wang", "Chunhou Liu", "Jiamou Liu"], "title": "Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference", "comment": null, "summary": "Personality recognition from text is typically cast as hard-label\nclassification, which obscures the graded, prototype-like nature of human\npersonality judgments. We present ProtoMBTI, a cognitively aligned framework\nfor MBTI inference that operationalizes prototype theory within an LLM-based\npipeline. First, we construct a balanced, quality-controlled corpus via\nLLM-guided multi-dimensional augmentation (semantic, linguistic, sentiment).\nNext, we LoRA-fine-tune a lightweight (<=2B) encoder to learn discriminative\nembeddings and to standardize a bank of personality prototypes. At inference,\nwe retrieve top-k prototypes for a query post and perform a\nretrieve--reuse--revise--retain cycle: the model aggregates prototype evidence\nvia prompt-based voting, revises when inconsistencies arise, and, upon correct\nprediction, retains the sample to continually enrich the prototype library.\nAcross Kaggle and Pandora benchmarks, ProtoMBTI improves over baselines on both\nthe four MBTI dichotomies and the full 16-type task, and exhibits robust\ncross-dataset generalization. Our results indicate that aligning the inference\nprocess with psychological prototype reasoning yields gains in accuracy,\ninterpretability, and transfer for text-based personality modeling."}
{"id": "2511.00180", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00180", "abs": "https://arxiv.org/abs/2511.00180", "authors": ["Nicky Pochinkov", "Yulia Volkova", "Anna Vasileva", "Sai V R Chereddy"], "title": "ParaScopes: What do Language Models Activations Encode About Future Text?", "comment": "Main paper: 9 pages, 10 figures. Total 24 pages", "summary": "Interpretability studies in language models often investigate forward-looking\nrepresentations of activations. However, as language models become capable of\ndoing ever longer time horizon tasks, methods for understanding activations\noften remain limited to testing specific concepts or tokens. We develop a\nframework of Residual Stream Decoders as a method of probing model activations\nfor paragraph-scale and document-scale plans. We test several methods and find\ninformation can be decoded equivalent to 5+ tokens of future context in small\nmodels. These results lay the groundwork for better monitoring of language\nmodels and better understanding how they might encode longer-term planning\ninformation."}
{"id": "2511.00198", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00198", "abs": "https://arxiv.org/abs/2511.00198", "authors": ["Chun-Hao Yang", "Bo-Han Feng", "Tzu-Yuan Lai", "Yan Yu Chen", "Yin-Kai Dean Huang", "Shou-De Lin"], "title": "Training LLMs Beyond Next Token Prediction - Filling the Mutual Information Gap", "comment": null, "summary": "Optimizing training performance in large language models (LLMs) remains an\nessential challenge, particularly in improving model performance while\nmaintaining computational costs. This work challenges the conventional approach\nof training LLMs using next-token prediction (NTP), arguing that by predicting\ninformation-rich tokens during training, there is a more effective way to train\nLLMs. We investigate the impact of the proposed solution in three kinds of\ntasks for LLMs: arithmetic, multi-label classification of text, and\nnatural-language generation. This work offers a principled approach to\noptimizing LLM training, advancing both model performance and theoretical\nunderstanding of the target-token selection strategies."}
{"id": "2511.00020", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00020", "abs": "https://arxiv.org/abs/2511.00020", "authors": ["Suhasnadh Reddy Veluru", "Sai Teja Erukude", "Viswa Chaitanya Marella"], "title": "Multimodal Detection of Fake Reviews using BERT and ResNet-50", "comment": "Published in IEEE", "summary": "In the current digital commerce landscape, user-generated reviews play a\ncritical role in shaping consumer behavior, product reputation, and platform\ncredibility. However, the proliferation of fake or misleading reviews often\ngenerated by bots, paid agents, or AI models poses a significant threat to\ntrust and transparency within review ecosystems. Existing detection models\nprimarily rely on unimodal, typically textual, data and therefore fail to\ncapture semantic inconsistencies across different modalities. To address this\ngap, a robust multimodal fake review detection framework is proposed,\nintegrating textual features encoded with BERT and visual features extracted\nusing ResNet-50. These representations are fused through a classification head\nto jointly predict review authenticity. To support this approach, a curated\ndataset comprising 21,142 user-uploaded images across food delivery,\nhospitality, and e-commerce domains was utilized. Experimental results indicate\nthat the multimodal model outperforms unimodal baselines, achieving an F1-score\nof 0.934 on the test set. Additionally, the confusion matrix and qualitative\nanalysis highlight the model's ability to detect subtle inconsistencies, such\nas exaggerated textual praise paired with unrelated or low-quality images,\ncommonly found in deceptive content. This study demonstrates the critical role\nof multimodal learning in safeguarding digital trust and offers a scalable\nsolution for content moderation across various online platforms."}
{"id": "2511.00222", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00222", "abs": "https://arxiv.org/abs/2511.00222", "authors": ["Marwa Abdulhai", "Ryan Cheng", "Donovan Clay", "Tim Althoff", "Sergey Levine", "Natasha Jaques"], "title": "Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to simulate human users in\ninteractive settings such as therapy, education, and social role-play. While\nthese simulations enable scalable training and evaluation of AI agents,\noff-the-shelf LLMs often drift from their assigned personas, contradict earlier\nstatements, or abandon role-appropriate behavior. We introduce a unified\nframework for evaluating and improving persona consistency in LLM-generated\ndialogue. We define three automatic metrics: prompt-to-line consistency,\nline-to-line consistency, and Q&A consistency, that capture different types of\npersona drift and validate each against human annotations. Using these metrics\nas reward signals, we apply multi-turn reinforcement learning to fine-tune LLMs\nfor three user roles: a patient, a student, and a social chat partner. Our\nmethod reduces inconsistency by over 55%, resulting in more coherent and\nfaithful simulated users."}
{"id": "2511.00039", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00039", "abs": "https://arxiv.org/abs/2511.00039", "authors": ["Krishna Kumar Neelakanta Pillai Santha Kumari Amma"], "title": "Graph-Attentive MAPPO for Dynamic Retail Pricing", "comment": null, "summary": "Dynamic pricing in retail requires policies that adapt to shifting demand\nwhile coordinating decisions across related products. We present a systematic\nempirical study of multi-agent reinforcement learning for retail price\noptimization, comparing a strong MAPPO baseline with a\ngraph-attention-augmented variant (MAPPO+GAT) that leverages learned\ninteractions among products. Using a simulated pricing environment derived from\nreal transaction data, we evaluate profit, stability across random seeds,\nfairness across products, and training efficiency under a standardized\nevaluation protocol. The results indicate that MAPPO provides a robust and\nreproducible foundation for portfolio-level price control, and that MAPPO+GAT\nfurther enhances performance by sharing information over the product graph\nwithout inducing excessive price volatility. These results indicate that\ngraph-integrated MARL provides a more scalable and stable solution than\nindependent learners for dynamic retail pricing, offering practical advantages\nin multi-product decision-making."}
{"id": "2511.00265", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.00265", "abs": "https://arxiv.org/abs/2511.00265", "authors": ["Arman Anwar", "Zefang Liu"], "title": "AgentBnB: A Browser-Based Cybersecurity Tabletop Exercise with Large Language Model Support and Retrieval-Aligned Scaffolding", "comment": null, "summary": "Traditional cybersecurity tabletop exercises (TTXs) provide valuable training\nbut are often scripted, resource-intensive, and difficult to scale. We\nintroduce AgentBnB, a browser-based re-imagining of the Backdoors & Breaches\ngame that integrates large language model teammates with a Bloom-aligned,\nretrieval-augmented copilot (C2D2). The system expands a curated corpus into\nfactual, conceptual, procedural, and metacognitive snippets, delivering\non-demand, cognitively targeted hints. Prompt-engineered agents employ a\nscaffolding ladder that gradually fades as learner confidence grows. In a\nsolo-player pilot with four graduate students, participants reported greater\nintention to use the agent-based version compared to the physical card deck and\nviewed it as more scalable, though a ceiling effect emerged on a simple\nknowledge quiz. Despite limitations of small sample size, single-player focus,\nand narrow corpus, these early findings suggest that large language model\naugmented TTXs can provide lightweight, repeatable practice without the\nlogistical burden of traditional exercises. Planned extensions include\nmulti-player modes, telemetry-driven coaching, and comparative studies with\nlarger cohorts."}
{"id": "2511.00048", "categories": ["cs.AI", "cs.CY", "62-11", "E.5; G.3; I.6.4; I.6.6; J.3; J.4"], "pdf": "https://arxiv.org/pdf/2511.00048", "abs": "https://arxiv.org/abs/2511.00048", "authors": ["Martin Bicher", "Maximilian Viehauser", "Daniele Giannandrea", "Hannah Kastinger", "Dominik Brunmeir", "Claire Rippinger", "Christoph Urach", "Niki Popper"], "title": "GEPOC Parameters - Open Source Parametrisation and Validation for Austria, Version 2.0", "comment": "134 pages, 75 figures, 19 tables", "summary": "GEPOC, short for Generic Population Concept, is a collection of models and\nmethods for analysing population-level research questions. For the valid\napplication of the models for a specific country or region, stable and\nreproducible data processes are necessary, which provide valid and ready-to-use\nmodel parameters. This work contains a complete description of the\ndata-processing methods for computation of model parameters for Austria, based\nexclusively on freely and publicly accessible data. In addition to the\ndescription of the source data used, this includes all algorithms used for\naggregation, disaggregation, fusion, cleansing or scaling of the data, as well\nas a description of the resulting parameter files. The document places\nparticular emphasis on the computation of parameters for the most important\nGEPOC model, GEPOC ABM, a continuous-time agent-based population model. An\nextensive validation study using this particular model was made and is\npresented at the end of this work."}
{"id": "2511.00268", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00268", "abs": "https://arxiv.org/abs/2511.00268", "authors": ["Shounak Paul", "Dhananjay Ghumare", "Pawan Goyal", "Saptarshi Ghosh", "Ashutosh Modi"], "title": "IL-PCSR: Legal Corpus for Prior Case and Statute Retrieval", "comment": "Accepted at EMNLP 2025 (Main)", "summary": "Identifying/retrieving relevant statutes and prior cases/precedents for a\ngiven legal situation are common tasks exercised by law practitioners.\nResearchers to date have addressed the two tasks independently, thus developing\ncompletely different datasets and models for each task; however, both retrieval\ntasks are inherently related, e.g., similar cases tend to cite similar statutes\n(due to similar factual situation). In this paper, we address this gap. We\npropose IL-PCR (Indian Legal corpus for Prior Case and Statute Retrieval),\nwhich is a unique corpus that provides a common testbed for developing models\nfor both the tasks (Statute Retrieval and Precedent Retrieval) that can exploit\nthe dependence between the two. We experiment extensively with several baseline\nmodels on the tasks, including lexical models, semantic models and ensemble\nbased on GNNs. Further, to exploit the dependence between the two tasks, we\ndevelop an LLM-based re-ranking approach that gives the best performance."}
{"id": "2511.00092", "categories": ["cs.AI", "cs.CL", "cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2511.00092", "abs": "https://arxiv.org/abs/2511.00092", "authors": ["Shunya Minami", "Tatsuya Ishigaki", "Ikko Hamamura", "Taku Mikuriya", "Youmi Ma", "Naoaki Okazaki", "Hiroya Takamura", "Yohichi Suzuki", "Tadashi Kadowaki"], "title": "QuantumBench: A Benchmark for Quantum Problem Solving", "comment": "11 pages, 8 figures", "summary": "Large language models are now integrated into many scientific workflows,\naccelerating data analysis, hypothesis generation, and design space\nexploration. In parallel with this growth, there is a growing need to carefully\nevaluate whether models accurately capture domain-specific knowledge and\nnotation, since general-purpose benchmarks rarely reflect these requirements.\nThis gap is especially clear in quantum science, which features non-intuitive\nphenomena and requires advanced mathematics. In this study, we introduce\nQuantumBench, a benchmark for the quantum domain that systematically examine\nhow well LLMs understand and can be applied to this non-intuitive field. Using\npublicly available materials, we compiled approximately 800 questions with\ntheir answers spanning nine areas related to quantum science and organized them\ninto an eight-option multiple-choice dataset. With this benchmark, we evaluate\nseveral existing LLMs and analyze their performance in the quantum domain,\nincluding sensitivity to changes in question format. QuantumBench is the first\nLLM evaluation dataset built for the quantum domain, and it is intended to\nguide the effective use of LLMs in quantum research."}
{"id": "2511.00270", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00270", "abs": "https://arxiv.org/abs/2511.00270", "authors": ["Abhinav Joshi", "Vaibhav Sharma", "Sanjeet Singh", "Ashutosh Modi"], "title": "POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation", "comment": "Accepted at EMNLP 2025 (Main)", "summary": "Sign language translation remains a challenging task due to the scarcity of\nlarge-scale, sentence-aligned datasets. Prior arts have focused on various\nfeature extraction and architectural changes to support neural machine\ntranslation for sign languages. We propose POSESTITCH-SLT, a novel pre-training\nscheme that is inspired by linguistic-templates-based sentence generation\ntechnique. With translation comparison on two sign language datasets, How2Sign\nand iSign, we show that a simple transformer-based encoder-decoder architecture\noutperforms the prior art when considering template-generated sentence pairs in\ntraining. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign\nand from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for\npose-based gloss-free translation. The results demonstrate the effectiveness of\ntemplate-driven synthetic supervision in low-resource sign language settings."}
{"id": "2511.00122", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00122", "abs": "https://arxiv.org/abs/2511.00122", "authors": ["Ran Xu", "Yupeng Qi", "Jingsen Feng", "Xu Chu"], "title": "Engineering.ai: A Platform for Teams of AI Engineers in Computational Design", "comment": null, "summary": "In modern engineering practice, human engineers collaborate in specialized\nteams to design complex products, with each expert completing their respective\ntasks while communicating and exchanging results and data with one another.\nWhile this division of expertise is essential for managing multidisciplinary\ncomplexity, it demands substantial development time and cost. Recently, we\nintroduced OpenFOAMGPT (1.0, 2.0), which functions as an autonomous AI engineer\nfor computational fluid dynamics, and turbulence.ai, which can conduct\nend-to-end research in fluid mechanics draft publications and PhD theses.\nBuilding upon these foundations, we present Engineering.ai, a platform for\nteams of AI engineers in computational design. The framework employs a\nhierarchical multi-agent architecture where a Chief Engineer coordinates\nspecialized agents consisting of Aerodynamics, Structural, Acoustic, and\nOptimization Engineers, each powered by LLM with domain-specific knowledge.\nAgent-agent collaboration is achieved through file-mediated communication for\ndata provenance and reproducibility, while a comprehensive memory system\nmaintains project context, execution history, and retrieval-augmented domain\nknowledge to ensure reliable decision-making across the workflow. The system\nintegrates FreeCAD, Gmsh, OpenFOAM, CalculiX, and BPM acoustic analysis,\nenabling parallel multidisciplinary simulations while maintaining computational\naccuracy. The framework is validated through UAV wing optimization. This work\ndemonstrates that agentic-AI-enabled AI engineers has the potential to perform\ncomplex engineering tasks autonomously. Remarkably, the automated workflow\nachieved a 100% success rate across over 400 parametric configurations, with\nzero mesh generation failures, solver convergence issues, or manual\ninterventions required, validating that the framework is trustworthy."}
{"id": "2511.00315", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00315", "abs": "https://arxiv.org/abs/2511.00315", "authors": ["Lee Xiong", "Maksim Tkachenko", "Johanes Effendi", "Ting Cai"], "title": "Language Modeling With Factorization Memory", "comment": null, "summary": "We propose Factorization Memory, an efficient recurrent neural network (RNN)\narchitecture that achieves performance comparable to Transformer models on\nshort-context language modeling tasks while also demonstrating superior\ngeneralization in long-context scenarios. Our model builds upon Mamba-2,\nenabling Factorization Memory to exploit parallel computations during training\nwhile preserving constant computational and memory complexity during inference.\nTo further optimize model efficiency and representational capacity, we develop\na sparse formulation of Factorization Memory that updates only a subset of\nrecurrent states at each step while preserving the strong performance of its\ndense counterpart. To our knowledge, this represents the first RNN architecture\nthat successfully combines sparse memory activation with competitive\nperformance across both short and long-context settings. This work provides a\nsystematic empirical analysis of Factorization Memory in comparison to\nTransformer and Mamba-2 architectures."}
{"id": "2511.00162", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00162", "abs": "https://arxiv.org/abs/2511.00162", "authors": ["Michael D. Moffitt"], "title": "ARC-GEN: A Mimetic Procedural Benchmark Generator for the Abstraction and Reasoning Corpus", "comment": null, "summary": "The Abstraction and Reasoning Corpus remains one of the most compelling and\nchallenging benchmarks for tracking progress toward achieving Artificial\nGeneral Intelligence. In contrast to other evaluation datasets designed to\nassess an agent's task-specific skills or accumulated knowledge, the ARC-AGI\nsuite is specifically targeted at measuring skill acquisition efficiency, a\ntrait that has (so far) been lacking in even the most sophisticated machine\nlearning systems. For algorithms that require extensive intra-task exemplars, a\nsignificant constraint imposed by ARC-AGI is the modest cardinality of its\ndemonstration set, comprising a small number of $\\langle$ input, output\n$\\rangle$ grids per task specifying the corresponding transformation. To\nembellish the space of viable sample pairs, this paper introduces ARC-GEN, an\nopen-source procedural generator aimed at extending the original ARC-AGI\ntraining dataset as faithfully as possible. Unlike prior efforts, our generator\nis both exhaustive (covering all four-hundred tasks) and mimetic (more closely\nhonoring the distributional properties and characteristics embodied in the\ninitial ARC-AGI-1 release). We also discuss the use of this generator in\nestablishing a static benchmark suite to verify the correctness of programs\nsubmitted to the 2025 Google Code Golf Championship."}
{"id": "2511.00341", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00341", "abs": "https://arxiv.org/abs/2511.00341", "authors": ["Mihir Sahasrabudhe"], "title": "Reversal Invariance in Autoregressive Language Models", "comment": "7 pages, theoretical note", "summary": "We formalize a structural property of the causal (autoregressive) language\nmodeling (CLM) objective: reversal invariance. Formally, the next-token\nprediction loss assigns identical likelihood to a corpus and its reversal,\nimplying that standard CLM pretraining is direction-blind. This symmetry\nexplains why models trained on reversed text can achieve comparable performance\nto those trained on forward text, despite the inherently time-asymmetric nature\nof human language and reasoning. We argue that this invariance represents a\nlimitation of current pretraining objectives rather than a benign artifact. If\nnatural language encodes directional dependencies - phonological,\nmorphological, or causal - a symmetric objective may fail to capture them. We\ntherefore propose viewing pretraining through the lens of temporal asymmetry,\nmotivating future work on loss functions and architectures that explicitly\nmodel the arrow of language while retaining standard language modeling\ncapacity."}
{"id": "2511.00194", "categories": ["cs.AI", "F.2.2, F.4.1"], "pdf": "https://arxiv.org/pdf/2511.00194", "abs": "https://arxiv.org/abs/2511.00194", "authors": ["Jovial Cheukam Ngouonou", "Ramiz Gindullin", "Claude-Guy Quimper", "Nicolas Beldiceanu", "Remi Douence"], "title": "Incremental Selection of Most-Filtering Conjectures and Proofs of the Selected Conjectures", "comment": null, "summary": "We present an improved incremental selection algorithm of the selection\nalgorithm presented in [1] and prove all the selected conjectures."}
{"id": "2511.00343", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00343", "abs": "https://arxiv.org/abs/2511.00343", "authors": ["Changbing Yang", "Franklin Ma", "Freda Shi", "Jian Zhu"], "title": "LingGym: How Far Are LLMs from Thinking Like Field Linguists?", "comment": "EMNLP 2025 Main", "summary": "This paper introduces LingGym, a new benchmark that evaluates LLMs' capacity\nfor meta-linguistic reasoning using Interlinear Glossed Text (IGT) and\ngrammatical descriptions extracted from 18 typologically diverse reference\ngrammars. Unlike previous work that focuses on specific downstream tasks, we\nassess whether LLMs can generalize linguistic inference across low-resource\nlanguages and structures not seen during training. We present a controlled\nevaluation task: Word-Gloss Inference, in which the model must infer a missing\nword and gloss from context using varying levels of linguistic information\n(e.g., glosses, grammatical explanations, translations). Our results show that\nincorporating structured linguistic cues leads to consistent improvements in\nreasoning performance across all models. This work highlights both the promise\nand current limitations of using LLMs for typologically informed linguistic\nanalysis and low-resource language documentation."}
{"id": "2511.00206", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00206", "abs": "https://arxiv.org/abs/2511.00206", "authors": ["Dirk U. Wulff", "Rui Mata"], "title": "Advancing Cognitive Science with LLMs", "comment": null, "summary": "Cognitive science faces ongoing challenges in knowledge synthesis and\nconceptual clarity, in part due to its multifaceted and interdisciplinary\nnature. Recent advances in artificial intelligence, particularly the\ndevelopment of large language models (LLMs), offer tools that may help to\naddress these issues. This review examines how LLMs can support areas where the\nfield has historically struggled, including establishing cross-disciplinary\nconnections, formalizing theories, developing clear measurement taxonomies,\nachieving generalizability through integrated modeling frameworks, and\ncapturing contextual and individual variation. We outline the current\ncapabilities and limitations of LLMs in these domains, including potential\npitfalls. Taken together, we conclude that LLMs can serve as tools for a more\nintegrative and cumulative cognitive science when used judiciously to\ncomplement, rather than replace, human expertise."}
{"id": "2511.00371", "categories": ["cs.CL", "cs.CY", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.00371", "abs": "https://arxiv.org/abs/2511.00371", "authors": ["Erfan Al-Hossami", "Razvan Bunescu"], "title": "Reasoning Trajectories for Socratic Debugging of Student Code: From Misconceptions to Contradictions and Updated Beliefs", "comment": "25 pages, 2 tables, 13 figures", "summary": "In Socratic debugging, instructors guide students towards identifying and\nfixing a bug on their own, instead of providing the bug fix directly. Most\nnovice programmer bugs are caused by programming misconceptions, namely false\nbeliefs about a programming concept. In this context, Socratic debugging can be\nformulated as a guided Reasoning Trajectory (RT) leading to a statement about\nthe program behavior that contradicts the bug-causing misconception. Upon\nreaching this statement, the ensuing cognitive dissonance leads the student to\nfirst identify and then update their false belief. In this paper, we introduce\nthe task of reasoning trajectory generation, together with a dataset of\ndebugging problems manually annotated with RTs. We then describe LLM-based\nsolutions for generating RTs and Socratic conversations that are anchored on\nthem. A large-scale LLM-as-judge evaluation shows that frontier models can\ngenerate up to 91% correct reasoning trajectories and 98.7% valid conversation\nturns."}
{"id": "2511.00267", "categories": ["cs.AI", "cs.CY", "cs.GL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00267", "abs": "https://arxiv.org/abs/2511.00267", "authors": ["Christian Prothmann", "Vijay Gadepally", "Jeremy Kepner", "Koley Borchard", "Luca Carlone", "Zachary Folcik", "J. Daniel Grith", "Michael Houle", "Jonathan P. How", "Nathan Hughes", "Ifueko Igbinedion", "Hayden Jananthan", "Tejas Jayashankar", "Michael Jones", "Sertac Karaman", "Binoy G. Kurien", "Alejandro Lancho", "Giovanni Lavezzi", "Gary C. F. Lee", "Charles E. Leiserson", "Richard Linares", "Lindsey McEvoy", "Peter Michaleas", "Chasen Milner", "Alex Pentland", "Yury Polyanskiy", "Jovan Popovich", "Jeffrey Price", "Tim W. Reid", "Stephanie Riley", "Siddharth Samsi", "Peter Saunders", "Olga Simek", "Mark S. Veillette", "Amir Weiss", "Gregory W. Wornell", "Daniela Rus", "Scott T. Ruppel"], "title": "Advancing AI Challenges for the United States Department of the Air Force", "comment": "8 pages, 8 figures, 59 references. To appear in IEEE HPEC 2025", "summary": "The DAF-MIT AI Accelerator is a collaboration between the United States\nDepartment of the Air Force (DAF) and the Massachusetts Institute of Technology\n(MIT). This program pioneers fundamental advances in artificial intelligence\n(AI) to expand the competitive advantage of the United States in the defense\nand civilian sectors. In recent years, AI Accelerator projects have developed\nand launched public challenge problems aimed at advancing AI research in\npriority areas. Hallmarks of AI Accelerator challenges include large, publicly\navailable, and AI-ready datasets to stimulate open-source solutions and engage\nthe wider academic and private sector AI ecosystem. This article supplements\nour previous publication, which introduced AI Accelerator challenges. We\nprovide an update on how ongoing and new challenges have successfully\ncontributed to AI research and applications of AI technologies."}
{"id": "2511.00416", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00416", "abs": "https://arxiv.org/abs/2511.00416", "authors": ["Yiwei Zha", "Rui Min", "Shanu Sushmita"], "title": "PADBen: A Comprehensive Benchmark for Evaluating AI Text Detectors Against Paraphrase Attacks", "comment": null, "summary": "While AI-generated text (AIGT) detectors achieve over 90\\% accuracy on direct\nLLM outputs, they fail catastrophically against iteratively-paraphrased\ncontent. We investigate why iteratively-paraphrased text -- itself AI-generated\n-- evades detection systems designed for AIGT identification. Through intrinsic\nmechanism analysis, we reveal that iterative paraphrasing creates an\nintermediate laundering region characterized by semantic displacement with\npreserved generation patterns, which brings up two attack categories:\nparaphrasing human-authored text (authorship obfuscation) and paraphrasing\nLLM-generated text (plagiarism evasion). To address these vulnerabilities, we\nintroduce PADBen, the first benchmark systematically evaluating detector\nrobustness against both paraphrase attack scenarios. PADBen comprises a\nfive-type text taxonomy capturing the full trajectory from original content to\ndeeply laundered text, and five progressive detection tasks across\nsentence-pair and single-sentence challenges. We evaluate 11 state-of-the-art\ndetectors, revealing critical asymmetry: detectors successfully identify the\nplagiarism evasion problem but fail for the case of authorship obfuscation. Our\nfindings demonstrate that current detection approaches cannot effectively\nhandle the intermediate laundering region, necessitating fundamental advances\nin detection architectures beyond existing semantic and stylistic\ndiscrimination methods. For detailed code implementation, please see\nhttps://github.com/JonathanZha47/PadBen-Paraphrase-Attack-Benchmark."}
{"id": "2511.00340", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00340", "abs": "https://arxiv.org/abs/2511.00340", "authors": ["Manan Roy Choudhury", "Adithya Chandramouli", "Mannan Anand", "Vivek Gupta"], "title": "Better Call CLAUSE: A Discrepancy Benchmark for Auditing LLMs Legal Reasoning Capabilities", "comment": "41 pages, 4 images", "summary": "The rapid integration of large language models (LLMs) into high-stakes legal\nwork has exposed a critical gap: no benchmark exists to systematically\nstress-test their reliability against the nuanced, adversarial, and often\nsubtle flaws present in real-world contracts. To address this, we introduce\nCLAUSE, a first-of-its-kind benchmark designed to evaluate the fragility of an\nLLM's legal reasoning. We study the capabilities of LLMs to detect and reason\nabout fine-grained discrepancies by producing over 7500 real-world perturbed\ncontracts from foundational datasets like CUAD and ContractNLI. Our novel,\npersona-driven pipeline generates 10 distinct anomaly categories, which are\nthen validated against official statutes using a Retrieval-Augmented Generation\n(RAG) system to ensure legal fidelity. We use CLAUSE to evaluate leading LLMs'\nability to detect embedded legal flaws and explain their significance. Our\nanalysis shows a key weakness: these models often miss subtle errors and\nstruggle even more to justify them legally. Our work outlines a path to\nidentify and correct such reasoning failures in legal AI."}
{"id": "2511.00421", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00421", "abs": "https://arxiv.org/abs/2511.00421", "authors": ["Naoto Iwase", "Hiroki Okuyama", "Junichiro Iwasawa"], "title": "MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts", "comment": null, "summary": "Large language models (LLMs) show increasing promise in medical applications,\nbut their ability to detect and correct errors in clinical texts -- a\nprerequisite for safe deployment -- remains under-evaluated, particularly\nbeyond English. We introduce MedRECT, a cross-lingual benchmark\n(Japanese/English) that formulates medical error handling as three subtasks:\nerror detection, error localization (sentence extraction), and error\ncorrection. MedRECT is built with a scalable, automated pipeline from the\nJapanese Medical Licensing Examinations (JMLE) and a curated English\ncounterpart, yielding MedRECT-ja (663 texts) and MedRECT-en (458 texts) with\ncomparable error/no-error balance. We evaluate 9 contemporary LLMs spanning\nproprietary, open-weight, and reasoning families. Key findings: (i) reasoning\nmodels substantially outperform standard architectures, with up to 13.5%\nrelative improvement in error detection and 51.0% in sentence extraction; (ii)\ncross-lingual evaluation reveals 5-10% performance gaps from English to\nJapanese, with smaller disparities for reasoning models; (iii) targeted LoRA\nfine-tuning yields asymmetric improvements in error correction performance\n(Japanese: +0.078, English: +0.168) while preserving reasoning capabilities;\nand (iv) our fine-tuned model exceeds human expert performance on structured\nmedical error correction tasks. To our knowledge, MedRECT is the first\ncomprehensive cross-lingual benchmark for medical error correction, providing a\nreproducible framework and resources for developing safer medical LLMs across\nlanguages."}
{"id": "2511.00379", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00379", "abs": "https://arxiv.org/abs/2511.00379", "authors": ["Jiahao Wang", "Songkai Xue", "Jinghui Li", "Xiaozhen Wang"], "title": "Diverse Human Value Alignment for Large Language Models via Ethical Reasoning", "comment": "Accepted by AIES 2025, camera-ready version", "summary": "Ensuring that Large Language Models (LLMs) align with the diverse and\nevolving human values across different regions and cultures remains a critical\nchallenge in AI ethics. Current alignment approaches often yield superficial\nconformity rather than genuine ethical understanding, failing to address the\ncomplex, context-dependent nature of human values. In this paper, we propose a\nnovel ethical reasoning paradigm for LLMs inspired by well-established ethical\ndecision-making models, aiming at enhancing diverse human value alignment\nthrough deliberative ethical reasoning. Our framework consists of a structured\nfive-step process, including contextual fact gathering, hierarchical social\nnorm identification, option generation, multiple-lens ethical impact analysis,\nand reflection. This theory-grounded approach guides LLMs through an\ninterpretable reasoning process that enhances their ability to understand\nregional specificities and perform nuanced ethical analysis, which can be\nimplemented with either prompt engineering or supervised fine-tuning methods.\nWe perform evaluations on the SafeWorld benchmark that specially designed for\nregional value alignment. Experimental results demonstrate our framework\nsignificantly improves LLM alignment with diverse human values compared to\nbaseline methods, enabling more accurate social norm identification and more\nculturally appropriate reasoning. Our work provides a concrete pathway toward\ndeveloping LLMs that align more effectively with the multifaceted values of\nglobal societies through interdisciplinary research."}
{"id": "2511.00432", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00432", "abs": "https://arxiv.org/abs/2511.00432", "authors": ["Zhiwen Ruan", "Yixia Li", "Yefeng Liu", "Yun Chen", "Weihua Luo", "Peng Li", "Yang Liu", "Guanhua Chen"], "title": "G2: Guided Generation for Enhanced Output Diversity in LLMs", "comment": "EMNLP 2025", "summary": "Large Language Models (LLMs) have demonstrated exceptional performance across\ndiverse natural language processing tasks. However, these models exhibit a\ncritical limitation in output diversity, often generating highly similar\ncontent across multiple attempts. This limitation significantly affects tasks\nrequiring diverse outputs, from creative writing to reasoning. Existing\nsolutions, like temperature scaling, enhance diversity by modifying probability\ndistributions but compromise output quality. We propose Guide-to-Generation\n(G2), a training-free plug-and-play method that enhances output diversity while\npreserving generation quality. G2 employs a base generator alongside dual\nGuides, which guide the generation process through decoding-based interventions\nto encourage more diverse outputs conditioned on the original query.\nComprehensive experiments demonstrate that G2 effectively improves output\ndiversity while maintaining an optimal balance between diversity and quality."}
{"id": "2511.00382", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00382", "abs": "https://arxiv.org/abs/2511.00382", "authors": ["Mina Taraghi", "Yann Pequignot", "Amin Nikanjam", "Mohamed Amine Merzouk", "Foutse Khomh"], "title": "Efficiency vs. Alignment: Investigating Safety and Fairness Risks in Parameter-Efficient Fine-Tuning of LLMs", "comment": null, "summary": "Organizations are increasingly adopting and adapting Large Language Models\n(LLMs) hosted on public repositories such as HuggingFace. Although these\nadaptations often improve performance on specialized downstream tasks, recent\nevidence indicates that they can also degrade a model's safety or fairness.\nSince different fine-tuning techniques may exert distinct effects on these\ncritical dimensions, this study undertakes a systematic assessment of their\ntrade-offs. Four widely used Parameter-Efficient Fine-Tuning methods, LoRA,\nIA3, Prompt-Tuning, and P-Tuning, are applied to four instruction-tuned model\nfamilies (Meta-Llama-3-8B, Qwen2.5-7B, Mistral-7B, and Gemma-7B). In total, 235\nfine-tuned variants are evaluated across eleven safety hazard categories and\nnine demographic fairness dimensions. The results show that adapter-based\napproaches (LoRA, IA3) tend to improve safety scores and are the least\ndisruptive to fairness, retaining higher accuracy and lower bias scores. In\ncontrast, prompt-based methods (Prompt-Tuning and P-Tuning) generally reduce\nsafety and cause larger fairness regressions, with decreased accuracy and\nincreased bias. Alignment shifts are strongly moderated by base model type:\nLLaMA remains stable, Qwen records modest gains, Gemma experiences the steepest\nsafety decline, and Mistral, which is released without an internal moderation\nlayer, displays the greatest variance. Improvements in safety do not\nnecessarily translate into improvements in fairness, and no single\nconfiguration optimizes all fairness metrics simultaneously, indicating an\ninherent trade-off between these objectives. These findings suggest a practical\nguideline for safety-critical deployments: begin with a well-aligned base\nmodel, favour adapter-based PEFT, and conduct category-specific audits of both\nsafety and fairness."}
{"id": "2511.00476", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00476", "abs": "https://arxiv.org/abs/2511.00476", "authors": ["Ghazal Kalhor", "Afra Mashhadi"], "title": "Remembering Unequally: Global and Disciplinary Bias in LLM-Generated Co-Authorship Networks", "comment": null, "summary": "Ongoing breakthroughs in Large Language Models (LLMs) are reshaping search\nand recommendation platforms at their core. While this shift unlocks powerful\nnew scientometric tools, it also exposes critical fairness and bias issues that\ncould erode the integrity of the information ecosystem. Additionally, as LLMs\nbecome more integrated into web-based searches for scholarly tools, their\nability to generate summarized research work based on memorized data introduces\nnew dimensions to these challenges. The extent of memorization in LLMs can\nimpact the accuracy and fairness of the co-authorship networks they produce,\npotentially reflecting and amplifying existing biases within the scientific\ncommunity and across different regions. This study critically examines the\nimpact of LLM memorization on the co-authorship networks. To this end, we\nassess memorization effects across three prominent models, DeepSeek R1, Llama 4\nScout, and Mixtral 8x7B, analyzing how memorization-driven outputs vary across\nacademic disciplines and world regions. While our global analysis reveals a\nconsistent bias favoring highly cited researchers, this pattern is not\nuniformly observed. Certain disciplines, such as Clinical Medicine, and\nregions, including parts of Africa, show more balanced representation, pointing\nto areas where LLM training data may reflect greater equity. These findings\nunderscore both the risks and opportunities in deploying LLMs for scholarly\ndiscovery."}
{"id": "2511.00424", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00424", "abs": "https://arxiv.org/abs/2511.00424", "authors": ["Ashutosh Anshul", "Gumpili Sai Pranav", "Mohammad Zia Ur Rehman", "Nagendra Kumar"], "title": "A Multimodal Framework for Depression Detection during Covid-19 via Harvesting Social Media: A Novel Dataset and Method", "comment": null, "summary": "The recent coronavirus disease (Covid-19) has become a pandemic and has\naffected the entire globe. During the pandemic, we have observed a spike in\ncases related to mental health, such as anxiety, stress, and depression.\nDepression significantly influences most diseases worldwide, making it\ndifficult to detect mental health conditions in people due to unawareness and\nunwillingness to consult a doctor. However, nowadays, people extensively use\nonline social media platforms to express their emotions and thoughts. Hence,\nsocial media platforms are now becoming a large data source that can be\nutilized for detecting depression and mental illness. However, existing\napproaches often overlook data sparsity in tweets and the multimodal aspects of\nsocial media. In this paper, we propose a novel multimodal framework that\ncombines textual, user-specific, and image analysis to detect depression among\nsocial media users. To provide enough context about the user's emotional state,\nwe propose (i) an extrinsic feature by harnessing the URLs present in tweets\nand (ii) extracting textual content present in images posted in tweets. We also\nextract five sets of features belonging to different modalities to describe a\nuser. Additionally, we introduce a Deep Learning model, the Visual Neural\nNetwork (VNN), to generate embeddings of user-posted images, which are used to\ncreate the visual feature vector for prediction. We contribute a curated\nCovid-19 dataset of depressed and non-depressed users for research purposes and\ndemonstrate the effectiveness of our model in detecting depression during the\nCovid-19 outbreak. Our model outperforms existing state-of-the-art methods over\na benchmark dataset by 2%-8% and produces promising results on the Covid-19\ndataset. Our analysis highlights the impact of each modality and provides\nvaluable insights into users' mental and emotional states."}
{"id": "2511.00486", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00486", "abs": "https://arxiv.org/abs/2511.00486", "authors": ["Pooja Singh", "Shashwat Bhardwaj", "Vaibhav Sharma", "Sandeep Kumar"], "title": "Leveraging the Cross-Domain & Cross-Linguistic Corpus for Low Resource NMT: A Case Study On Bhili-Hindi-English Parallel Corpus", "comment": "Accepted in EMNLP 2025", "summary": "The linguistic diversity of India poses significant machine translation\nchallenges, especially for underrepresented tribal languages like Bhili, which\nlack high-quality linguistic resources. This paper addresses the gap by\nintroducing Bhili-Hindi-English Parallel Corpus (BHEPC), the first and largest\nparallel corpus worldwide comprising 110,000 meticulously curated sentences\nacross Bhili, Hindi, and English. The corpus was created with the assistance of\nexpert human translators. BHEPC spans critical domains such as education,\nadministration, and news, establishing a valuable benchmark for research in low\nresource machine translation. To establish a comprehensive Bhili Machine\nTranslation benchmark, we evaluated a wide range of proprietary and open-source\nMultilingual Large Language Models (MLLMs) on bidirectional translation tasks\nbetween English/Hindi and Bhili. Comprehensive evaluation demonstrates that the\nfine-tuned NLLB-200 distilled 600M variant model outperforms others,\nhighlighting the potential of multilingual models in low resource scenarios.\nFurthermore, we investigated the generative translation capabilities of\nmultilingual LLMs on BHEPC using in-context learning, assessing performance\nunder cross-domain generalization and quantifying distributional divergence.\nThis work bridges a critical resource gap and promotes inclusive natural\nlanguage processing technologies for low-resource and marginalized languages\nglobally."}
{"id": "2511.00457", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00457", "abs": "https://arxiv.org/abs/2511.00457", "authors": ["Chunyu Wei", "Wenji Hu", "Xingjia Hao", "Xin Wang", "Yifan Yang", "Yueguo Chen", "Yang Tian", "Yunhai Wang"], "title": "GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining", "comment": null, "summary": "Large Language Models (LLMs) face significant limitations when applied to\nlarge-scale graphs, struggling with context constraints and inflexible\nreasoning. We present GraphChain, a framework that enables LLMs to analyze\ncomplex graphs through dynamic sequences of specialized tools, mimicking human\nexploratory intelligence. Our approach introduces two key innovations: (1)\nProgressive Graph Distillation, a reinforcement learning mechanism that\ngenerates optimized tool sequences balancing task relevance with information\ncompression, and (2) Structure-aware Test-Time Adaptation, which efficiently\ntailors tool selection strategies to diverse graph topologies using spectral\nproperties and lightweight adapters without costly retraining. Experiments show\nGraphChain significantly outperforms prior methods, enabling scalable and\nadaptive LLM-driven graph analysis."}
{"id": "2511.00487", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00487", "abs": "https://arxiv.org/abs/2511.00487", "authors": ["Stephen Meisenbacher", "Florian Matthes"], "title": "With Privacy, Size Matters: On the Importance of Dataset Size in Differentially Private Text Rewriting", "comment": "11 pages, 1 figure, 5 tables. Accepted to IJCNLP-AACL 2025 (Main)", "summary": "Recent work in Differential Privacy with Natural Language Processing (DP NLP)\nhas proposed numerous promising techniques in the form of text rewriting\nmechanisms. In the evaluation of these mechanisms, an often-ignored aspect is\nthat of dataset size, or rather, the effect of dataset size on a mechanism's\nefficacy for utility and privacy preservation. In this work, we are the first\nto introduce this factor in the evaluation of DP text privatization, where we\ndesign utility and privacy tests on large-scale datasets with dynamic split\nsizes. We run these tests on datasets of varying size with up to one million\ntexts, and we focus on quantifying the effect of increasing dataset size on the\nprivacy-utility trade-off. Our findings reveal that dataset size plays an\nintegral part in evaluating DP text rewriting mechanisms; additionally, these\nfindings call for more rigorous evaluation procedures in DP NLP, as well as\nshed light on the future of DP NLP in practice and at scale."}
{"id": "2511.00509", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.00509", "abs": "https://arxiv.org/abs/2511.00509", "authors": ["Yifan Xia", "Guorui Chen", "Wenqian Yu", "Zhijiang Li", "Philip Torr", "Jindong Gu"], "title": "Reimagining Safety Alignment with An Image", "comment": null, "summary": "Large language models (LLMs) excel in diverse applications but face dual\nchallenges: generating harmful content under jailbreak attacks and over-refusal\nof benign queries due to rigid safety mechanisms. These issues are further\ncomplicated by the need to accommodate different value systems and precisely\nalign with given safety preferences. Moreover, traditional methods like SFT and\nRLHF lack this capability due to their costly parameter tuning requirements and\ninability to support multiple value systems within a single model. These\nproblems are more obvious in multimodal large language models (MLLMs),\nespecially in terms of heightened over-refusal in cross-modal tasks and new\nsecurity risks arising from expanded attack surfaces. We propose Magic Image,\nan optimization-driven visual prompt framework that enhances security while\nreducing over-refusal. By optimizing image prompts using harmful/benign\nsamples, our method enables a single model to adapt to different value systems\nand better align with given safety preferences without parameter updates.\nExperiments demonstrate improved safety-effectiveness balance across diverse\ndatasets while preserving model performance, offering a practical solution for\ndeployable MLLM safety alignment."}
{"id": "2511.00489", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00489", "abs": "https://arxiv.org/abs/2511.00489", "authors": ["Jiani Guo", "Zuchao Li", "Jie Wu", "Qianren Wang", "Yun Li", "Lefei Zhang", "Hai Zhao", "Yujiu Yang"], "title": "ToM: Leveraging Tree-oriented MapReduce for Long-Context Reasoning in Large Language Models", "comment": "EMNLP 2025 Main Conference", "summary": "Large Language Models (LLMs), constrained by limited context windows, often\nface significant performance degradation when reasoning over long contexts. To\naddress this, Retrieval-Augmented Generation (RAG) retrieves and reasons over\nchunks but frequently sacrifices logical coherence due to its reliance on\nsimilarity-based rankings. Similarly, divide-and-conquer frameworks (DCF) split\ndocuments into small chunks for independent reasoning and aggregation. While\neffective for local reasoning, DCF struggles to capture long-range dependencies\nand risks inducing conflicts by processing chunks in isolation. To overcome\nthese limitations, we propose ToM, a novel Tree-oriented MapReduce framework\nfor long-context reasoning. ToM leverages the inherent hierarchical structure\nof long documents (e.g., main headings and subheadings) by constructing a\nDocTree through hierarchical semantic parsing and performing bottom-up\naggregation. Using a Tree MapReduce approach, ToM enables recursive reasoning:\nin the Map step, rationales are generated at child nodes; in the Reduce step,\nthese rationales are aggregated across sibling nodes to resolve conflicts or\nreach consensus at parent nodes. Experimental results on 70B+ LLMs show that\nToM significantly outperforms existing divide-and-conquer frameworks and\nretrieval-augmented generation methods, achieving better logical coherence and\nlong-context reasoning. Our code is available at\nhttps://github.com/gjn12-31/ToM ."}
{"id": "2511.00547", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00547", "abs": "https://arxiv.org/abs/2511.00547", "authors": ["Alain Riou"], "title": "Efficient Generation of Binary Magic Squares", "comment": null, "summary": "We propose a simple algorithm for generating Binary Magic Squares (BMS),\ni.e., square binary matrices where the sum of all rows and all columns are\nequal. We show by induction that our algorithm always returns valid BMS with\noptimal theoretical complexity. We then extend our study to non-square Binary\nMagic Squares, formalize conditions on the sum of rows and columns for these\nBMS to exist, and show that a slight variant of our first algorithm can\ngenerate provably generate them. Finally, we publicly release two\nimplementations of our algorithm as Python packages, including one that can\ngenerate several BMS in parallel using GPU acceleration."}
{"id": "2511.00505", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00505", "abs": "https://arxiv.org/abs/2511.00505", "authors": ["Qi Luo", "Xiaonan Li", "Junqi Dai", "Shuang Cheng", "Xipeng Qiu"], "title": "Zero-RAG: Towards Retrieval-Augmented Generation with Zero Redundant Knowledge", "comment": null, "summary": "Retrieval-Augmented Generation has shown remarkable results to address Large\nLanguage Models' hallucinations, which usually uses a large external corpus to\nsupplement knowledge to LLMs. However, with the development of LLMs, the\ninternal knowledge of LLMs has expanded significantly, thus causing significant\nknowledge redundancy between the external corpus and LLMs. On the one hand, the\nindexing cost of dense retrieval is highly related to the corpus size and thus\nsignificant redundant knowledge intensifies the dense retrieval's workload. On\nthe other hand, the redundant knowledge in the external corpus is not helpful\nto LLMs and our exploratory analysis shows that it instead hurts the RAG\nperformance on those questions which the LLM can answer by itself. To address\nthese issues, we propose Zero-RAG to tackle these challenges. Specifically, we\nfirst propose the Mastery-Score metric to identify redundant knowledge in the\nRAG corpus to prune it. After pruning, answers to \"mastered\" questions rely\nprimarily on internal knowledge of the LLM. To better harness the internal\ncapacity, we propose Query Router and Noise-Tolerant Tuning to avoid the\nirrelevant documents' distraction and thus further improve the LLM's\nutilization of internal knowledge with pruned corpus. Experimental results show\nthat Zero-RAG prunes the Wikipedia corpus by 30\\% and accelerates the retrieval\nstage by 22\\%, without compromising RAG's performance."}
{"id": "2511.00551", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00551", "abs": "https://arxiv.org/abs/2511.00551", "authors": ["Qiang Li", "Ningjing Zeng", "Lina Yu"], "title": "Single-agent Reinforcement Learning Model for Regional Adaptive Traffic Signal Control", "comment": null, "summary": "Several studies have employed reinforcement learning (RL) to address the\nchallenges of regional adaptive traffic signal control (ATSC) and achieved\npromising results. In this field, existing research predominantly adopts\nmulti-agent frameworks. However, the adoption of multi-agent frameworks\npresents challenges for scalability. Instead, the Traffic signal control (TSC)\nproblem necessitates a single-agent framework. TSC inherently relies on\ncentralized management by a single control center, which can monitor traffic\nconditions across all roads in the study area and coordinate the control of all\nintersections. This work proposes a single-agent RL-based regional ATSC model\ncompatible with probe vehicle technology. Key components of the RL design\ninclude state, action, and reward function definitions. To facilitate learning\nand manage congestion, both state and reward functions are defined based on\nqueue length, with action designed to regulate queue dynamics. The queue length\ndefinition used in this study differs slightly from conventional definitions\nbut is closely correlated with congestion states. More importantly, it allows\nfor reliable estimation using link travel time data from probe vehicles. With\nprobe vehicle data already covering most urban roads, this feature enhances the\nproposed method's potential for widespread deployment. The method was\ncomprehensively evaluated using the SUMO simulation platform. Experimental\nresults demonstrate that the proposed model effectively mitigates large-scale\nregional congestion levels via coordinated multi-intersection control."}
{"id": "2511.00514", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00514", "abs": "https://arxiv.org/abs/2511.00514", "authors": ["Birat Poudel", "Satyam Ghimire", "Er. Prakash Chandra Prasad"], "title": "Fine-Tuning DialoGPT on Common Diseases in Rural Nepal for Medical Conversations", "comment": "6 pages, 6 figures, 3 tables", "summary": "Conversational agents are increasingly being explored to support healthcare\ndelivery, particularly in resource-constrained settings such as rural Nepal.\nLarge-scale conversational models typically rely on internet connectivity and\ncloud infrastructure, which may not be accessible in rural areas. In this\nstudy, we fine-tuned DialoGPT, a lightweight generative dialogue model that can\noperate offline, on a synthetically constructed dataset of doctor-patient\ninteractions covering ten common diseases prevalent in rural Nepal, including\ncommon cold, seasonal fever, diarrhea, typhoid fever, gastritis, food\npoisoning, malaria, dengue fever, tuberculosis, and pneumonia. Despite being\ntrained on a limited, domain-specific dataset, the fine-tuned model produced\ncoherent, contextually relevant, and medically appropriate responses,\ndemonstrating an understanding of symptoms, disease context, and empathetic\ncommunication. These results highlight the adaptability of compact,\noffline-capable dialogue models and the effectiveness of targeted datasets for\ndomain adaptation in low-resource healthcare environments, offering promising\ndirections for future rural medical conversational AI."}
{"id": "2511.00609", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00609", "abs": "https://arxiv.org/abs/2511.00609", "authors": ["Shengqi Xu", "Xinpeng Zhou", "Yabo Zhang", "Ming Liu", "Tao Liang", "Tianyu Zhang", "Yalong Bai", "Zuxuan Wu", "Wangmeng Zuo"], "title": "PreferThinker: Reasoning-based Personalized Image Preference Assessment", "comment": null, "summary": "Personalized image preference assessment aims to evaluate an individual\nuser's image preferences by relying only on a small set of reference images as\nprior information. Existing methods mainly focus on general preference\nassessment, training models with large-scale data to tackle well-defined tasks\nsuch as text-image alignment. However, these approaches struggle to handle\npersonalized preference because user-specific data are scarce and not easily\nscalable, and individual tastes are often diverse and complex. To overcome\nthese challenges, we introduce a common preference profile that serves as a\nbridge across users, allowing large-scale user data to be leveraged for\ntraining profile prediction and capturing complex personalized preferences.\nBuilding on this idea, we propose a reasoning-based personalized image\npreference assessment framework that follows a \\textit{predict-then-assess}\nparadigm: it first predicts a user's preference profile from reference images,\nand then provides interpretable, multi-dimensional scores and assessments of\ncandidate images based on the predicted profile. To support this, we first\nconstruct a large-scale Chain-of-Thought (CoT)-style personalized assessment\ndataset annotated with diverse user preference profiles and high-quality\nCoT-style reasoning, enabling explicit supervision of structured reasoning.\nNext, we adopt a two-stage training strategy: a cold-start supervised\nfine-tuning phase to empower the model with structured reasoning capabilities,\nfollowed by reinforcement learning to incentivize the model to explore more\nreasonable assessment paths and enhance generalization. Furthermore, we propose\na similarity-aware prediction reward to encourage better prediction of the\nuser's preference profile, which facilitates more reasonable assessments\nexploration. Extensive experiments demonstrate the superiority of the proposed\nmethod."}
{"id": "2511.00519", "categories": ["cs.CL", "I.2.7; I.7.1; K.4.1"], "pdf": "https://arxiv.org/pdf/2511.00519", "abs": "https://arxiv.org/abs/2511.00519", "authors": ["Ariyan Hossain", "Khondokar Mohammad Ahanaf Hannan", "Rakinul Haque", "Nowreen Tarannum Rafa", "Humayra Musarrat", "Shoaib Ahmed Dipu", "Farig Yousuf Sadeque"], "title": "Exploring and Mitigating Gender Bias in Encoder-Based Transformer Models", "comment": "25 pages, 20 figures", "summary": "Gender bias in language models has gained increasing attention in the field\nof natural language processing. Encoder-based transformer models, which have\nachieved state-of-the-art performance in various language tasks, have been\nshown to exhibit strong gender biases inherited from their training data. This\npaper investigates gender bias in contextualized word embeddings, a crucial\ncomponent of transformer-based models. We focus on prominent architectures such\nas BERT, ALBERT, RoBERTa, and DistilBERT to examine their vulnerability to\ngender bias. To quantify the degree of bias, we introduce a novel metric,\nMALoR, which assesses bias based on model probabilities for filling masked\ntokens. We further propose a mitigation approach involving continued\npre-training on a gender-balanced dataset generated via Counterfactual Data\nAugmentation. Our experiments reveal significant reductions in gender bias\nscores across different pronoun pairs. For instance, in BERT-base, bias scores\nfor \"he-she\" dropped from 1.27 to 0.08, and \"his-her\" from 2.51 to 0.36\nfollowing our mitigation approach. We also observed similar improvements across\nother models, with \"male-female\" bias decreasing from 1.82 to 0.10 in\nBERT-large. Our approach effectively reduces gender bias without compromising\nmodel performance on downstream tasks."}
{"id": "2511.00640", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00640", "abs": "https://arxiv.org/abs/2511.00640", "authors": ["Zicheng Xu", "Guanchu Wang", "Yu-Neng Chuang", "Guangyao Zheng", "Alexander S. Szalay", "Zirui Liu", "Vladimir Braverman"], "title": "DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching", "comment": null, "summary": "Large Reasoning Models (LRMs) demonstrate strong performance on complex\nreasoning tasks, yet they often suffer from overthinking, producing excessively\nlong chain-of-thought (CoT) traces that increase inference cost and may degrade\naccuracy. Our analysis reveals a clear anti-correlation between reasoning\nlength and accuracy, where across multiple stochastic decodes, the short\nreasoning paths consistently achieve the highest correctness, while longer ones\naccumulate errors and repetitions. These short optimal reasoning paths can be\nfound ideally through full enumeration of the reasoning space. However, the\ntree-structured reasoning space grows exponentially with sequence length,\nrendering exhaustive exploration infeasible. To address this, we propose DTS, a\nmodel-agnostic decoding framework that sketches the reasoning space by\nselectively branching at high-entropy tokens and applies early stopping to\nselect the shortest completed reasoning path. This approach approximates the\noptimal solution that enhances both efficiency and accuracy, without requiring\nadditional training or supervision. Experiments on AIME2024 and AIME2025\ndatasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves\naccuracy by up to 8%, reduces average reasoning length by 23%, and decreases\nrepetition frequency by 12%, demonstrating DTS's ability for scalable and\nefficient LRM reasoning."}
{"id": "2511.00536", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00536", "abs": "https://arxiv.org/abs/2511.00536", "authors": ["Wenya Xie", "Shaochen", "Zhong", "Hoang Anh Duy Le", "Zhaozhuo Xu", "Jianwen Xie", "Zirui Liu"], "title": "Word Salad Chopper: Reasoning Models Waste A Ton Of Decoding Budget On Useless Repetitions, Self-Knowingly", "comment": null, "summary": "Large Reasoning Models (LRMs) are often bottlenecked by the high cost of\noutput tokens. We show that a significant portion of these tokens are useless\nself-repetitions - what we call \"word salad\" - that exhaust the decoding budget\nwithout adding value. Interestingly, we observe that LRMs are self-aware when\ntrapped in these loops: the hidden states of <\\n\\n> tokens trailing each\nreasoning chunk exhibit patterns that allow us to detect word salad behavior\non-the-fly via a single-layer linear classifier. Once detected, a simple chop\nappended by a straightforward regeneration prompt yields substantial length\nsavings with minimal quality loss. Our work offers WordSaladChopper (WSC) - a\nlightweight, turnkey component for LRM that is minimally invasive to its\nreasoning trajectory by only removing semantically redundant tokens. Given its\nlow overhead, strong savings, and the lack of semantic value of word salad\ntokens, we believe it is not too far-fetched to argue that WSC - or a similar\ncomponent - is a must-have for all LRM applications with user experience in\nmind. Our code is publicly available at\nhttps://github.com/wenyaxie023/WordSaladChopper."}
{"id": "2511.00651", "categories": ["cs.AI", "cs.CL", "cs.IT", "cs.MA", "cs.NI", "math.IT"], "pdf": "https://arxiv.org/pdf/2511.00651", "abs": "https://arxiv.org/abs/2511.00651", "authors": ["Chenhua Shi", "Bhavika Jalli", "Gregor Macdonald", "John Zou", "Wanlu Lei", "Mridul Jain", "Joji Philip"], "title": "Leveraging Multi-Agent System (MAS) and Fine-Tuned Small Language Models (SLMs) for Automated Telecom Network Troubleshooting", "comment": "6 pages, 7 figures, 1 table", "summary": "Telecom networks are rapidly growing in scale and complexity, making\neffective management, operation, and optimization increasingly challenging.\nAlthough Artificial Intelligence (AI) has been applied to many telecom tasks,\nexisting models are often narrow in scope, require large amounts of labeled\ndata, and struggle to generalize across heterogeneous deployments.\nConsequently, network troubleshooting continues to rely heavily on Subject\nMatter Experts (SMEs) to manually correlate various data sources to identify\nroot causes and corrective actions. To address these limitations, we propose a\nMulti-Agent System (MAS) that employs an agentic workflow, with Large Language\nModels (LLMs) coordinating multiple specialized tools for fully automated\nnetwork troubleshooting. Once faults are detected by AI/ML-based monitors, the\nframework dynamically activates agents such as an orchestrator, solution\nplanner, executor, data retriever, and root-cause analyzer to diagnose issues\nand recommend remediation strategies within a short time frame. A key component\nof this system is the solution planner, which generates appropriate remediation\nplans based on internal documentation. To enable this, we fine-tuned a Small\nLanguage Model (SLM) on proprietary troubleshooting documents to produce\ndomain-grounded solution plans. Experimental results demonstrate that the\nproposed framework significantly accelerates troubleshooting automation across\nboth Radio Access Network (RAN) and Core network domains."}
{"id": "2511.00537", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00537", "abs": "https://arxiv.org/abs/2511.00537", "authors": ["Peter Atandoh", "Jie Zou", "Weikang Guo", "Jiwei Wei", "Zheng Wang"], "title": "Multi-refined Feature Enhanced Sentiment Analysis Using Contextual Instruction", "comment": null, "summary": "Sentiment analysis using deep learning and pre-trained language models (PLMs)\nhas gained significant traction due to their ability to capture rich contextual\nrepresentations. However, existing approaches often underperform in scenarios\ninvolving nuanced emotional cues, domain shifts, and imbalanced sentiment\ndistributions. We argue that these limitations stem from inadequate semantic\ngrounding, poor generalization to diverse linguistic patterns, and biases\ntoward dominant sentiment classes. To overcome these challenges, we propose\nCISEA-MRFE, a novel PLM-based framework integrating Contextual Instruction\n(CI), Semantic Enhancement Augmentation (SEA), and Multi-Refined Feature\nExtraction (MRFE). CI injects domain-aware directives to guide sentiment\ndisambiguation; SEA improves robustness through sentiment-consistent\nparaphrastic augmentation; and MRFE combines a Scale-Adaptive Depthwise Encoder\n(SADE) for multi-scale feature specialization with an Emotion Evaluator Context\nEncoder (EECE) for affect-aware sequence modeling. Experimental results on four\nbenchmark datasets demonstrate that CISEA-MRFE consistently outperforms strong\nbaselines, achieving relative improvements in accuracy of up to 4.6% on IMDb,\n6.5% on Yelp, 30.3% on Twitter, and 4.1% on Amazon. These results validate the\neffectiveness and generalization ability of our approach for sentiment\nclassification across varied domains."}
{"id": "2511.00673", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00673", "abs": "https://arxiv.org/abs/2511.00673", "authors": ["Dominik Drexler"], "title": "Lifted Successor Generation in Numeric Planning", "comment": null, "summary": "Most planners ground numeric planning tasks, given in a first-order-like\nlanguage, into a ground task representation. However, this can lead to an\nexponential blowup in task representation size, which occurs in practice for\nhard-to-ground tasks. We extend a state-of-the-art lifted successor generator\nfor classical planning to support numeric precondition applicability. The\nmethod enumerates maximum cliques in a substitution consistency graph. Each\nmaximum clique represents a substitution for the variables of the action\nschema, yielding a ground action. We augment this graph with numeric action\npreconditions and prove the successor generator is exact under formally\nspecified conditions. When the conditions fail, our generator may list\ninapplicable ground actions; a final applicability check filters these without\naffecting completeness. However, this cannot happen in 23 of 25 benchmark\ndomains, and it occurs only in 1 domain. To the authors' knowledge, no other\nlifted successor generator supports numeric action preconditions. This enables\nfuture research on lifted planning for a very rich planning fragment."}
{"id": "2511.00556", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00556", "abs": "https://arxiv.org/abs/2511.00556", "authors": ["Peng Ding", "Jun Kuang", "Wen Sun", "Zongyu Wang", "Xuezhi Cao", "Xunliang Cai", "Jiajun Chen", "Shujian Huang"], "title": "Friend or Foe: How LLMs' Safety Mind Gets Fooled by Intent Shift Attack", "comment": "Preprint, 14 pages, 5 figures, 7 tables", "summary": "Large language models (LLMs) remain vulnerable to jailbreaking attacks\ndespite their impressive capabilities. Investigating these weaknesses is\ncrucial for robust safety mechanisms. Existing attacks primarily distract LLMs\nby introducing additional context or adversarial tokens, leaving the core\nharmful intent unchanged. In this paper, we introduce ISA (Intent Shift\nAttack), which obfuscates LLMs about the intent of the attacks. More\nspecifically, we establish a taxonomy of intent transformations and leverage\nthem to generate attacks that may be misperceived by LLMs as benign requests\nfor information. Unlike prior methods relying on complex tokens or lengthy\ncontext, our approach only needs minimal edits to the original request, and\nyields natural, human-readable, and seemingly harmless prompts. Extensive\nexperiments on both open-source and commercial LLMs show that ISA achieves over\n70% improvement in attack success rate compared to direct harmful prompts. More\ncritically, fine-tuning models on only benign data reformulated with ISA\ntemplates elevates success rates to nearly 100%. For defense, we evaluate\nexisting methods and demonstrate their inadequacy against ISA, while exploring\nboth training-free and training-based mitigation strategies. Our findings\nreveal fundamental challenges in intent inference for LLMs safety and\nunderscore the need for more effective defenses. Our code and datasets are\navailable at https://github.com/NJUNLP/ISA."}
{"id": "2511.00710", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00710", "abs": "https://arxiv.org/abs/2511.00710", "authors": ["Minghe Shen", "Zhuo Zhi", "Chonghan Liu", "Shuo Xing", "Zhengzhong Tu", "Che Liu"], "title": "Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries", "comment": null, "summary": "While Vision-Language Models (VLMs) post-trained with Reinforcement Learning\n(RL) show impressive general reasoning, their evaluation is often confined to\nlanguage-dominant tasks (e.g., math). This raises a critical question: can RL\npost-training truly extend the inherent capability boundary of a base VLM,\nparticularly for visual-centric spatial tasks where it initially fails? To\ninvestigate this, we introduce Ariadne, a framework utilizing synthetic mazes\nfor multi-step spatial reasoning where task difficulty (e.g., path length,\nturns) is precisely controlled. We leverage this controllable environment to\ntrain VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a\ndifficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves\nover 50% accuracy on a problem set where the base model scored 0%,\ndemonstrating that our approach expands the model's initial capability\nboundary. To assess real-world viability, we evaluate out-of-distribution (OOD)\ngeneralization on practical benchmarks. Despite training only on synthetic maze\nsamples, Ariadne achieves significant zero-shot improvements, averaging 16% on\nMapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer\ntasks). These results confirm that our method not only broadens the model's\nfundamental limits but also enhances its generalization to real-world spatial\nreasoning. We acknowledge our study is limited to the post-training phase,\ngiven the opaqueness of pre-training data, and hope our research motivates\nfurther work on specialized, capability-extending alignment."}
{"id": "2511.00576", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00576", "abs": "https://arxiv.org/abs/2511.00576", "authors": ["Juan Gabriel Kostelec", "Qinghai Guo"], "title": "FlashEVA: Accelerating LLM inference via Efficient Attention", "comment": "Technical Report", "summary": "Transformer models have revolutionized natural language processing, achieving\nstate-of-the-art performance and demonstrating remarkable scalability. However,\ntheir memory demands, particularly due to maintaining full context in memory,\npose significant challenges for inference. In this paper, we present FlashEVA,\nan efficient implementation of EVA (Efficient Attention via Control Variates),\nand demonstrate how to finetune transformers to adapt to FlashEVA attention.\nOur method enables fine-tuning of Transformer models with as few as 1.5B tokens\nwhile preserving effectiveness across various downstream tasks. Notably,\nFlashEVA achieves up to 6.7x higher throughput and 5x lower peak GPU memory\nusage during inference compared to standard Transformer implementations.\nDespite these improvements, we observe limitations in retrieval-focused tasks.\nOur implementation offers control over the trade-off between throughput and\naccuracy through adjustable hyperparameters, providing flexibility for diverse\nuse cases. This work represents a significant step towards more efficient and\nadaptable Transformer-based models for inference."}
{"id": "2511.00739", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.00739", "abs": "https://arxiv.org/abs/2511.00739", "authors": ["Ritik Raj", "Hong Wang", "Tushar Krishna"], "title": "A CPU-Centric Perspective on Agentic AI", "comment": null, "summary": "Agentic AI frameworks add a decision-making orchestrator embedded with\nexternal tools, including web search, Python interpreter, contextual database,\nand others, on top of monolithic LLMs, turning them from passive text oracles\ninto autonomous problem-solvers that can plan, call tools, remember past steps,\nand adapt on the fly.\n  This paper aims to characterize and understand the system bottlenecks\nintroduced by agentic AI workloads from a largely overlooked CPU-centric\nperspective. We first systematically characterize Agentic AI on the basis of\norchestrator/decision making component, inference path dynamics and\nrepetitiveness of the agentic flow which directly influences the system-level\nperformance. Thereafter, based on the characterization, we choose five\nrepresentative agentic AI workloads- Haystack RAG, Toolformer, ChemCrow,\nLangchain and SWE-Agent to profile latency, throughput and energy metrics and\ndemystify the significant impact of CPUs on these metrics relative to GPUs. We\nobserve that - 1. Tool processing on CPUs can take up to 90.6% of the total\nlatency; 2. Agentic throughput gets bottlenecked either by CPU factors -\ncoherence, synchronization and over-subscription of cores or GPU factors - main\nmemory capacity and bandwidth; \\circled{3} CPU dynamic energy consumes up to\n44% of the total dynamic energy at large batch sizes. Based on the profiling\ninsights, we present two key optimizations- 1. CPU and GPU-Aware Micro-batching\n(CGAM) and 2. Mixed Agentic Workload Scheduling (MAWS) for homogeneous and\nheterogeneous agentic workloads respectively to demonstrate the potential to\nimprove the performance, efficiency, and scalability of agentic AI. We achieve\nup to 2.1x and 1.41x P50 latency speedup compared to the multi-processing\nbenchmark for homogeneous and heterogeneous agentic workloads respectively."}
{"id": "2511.00602", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00602", "abs": "https://arxiv.org/abs/2511.00602", "authors": ["Wai-Chung Kwan", "Joshua Ong Jun Leang", "Pavlos Vougiouklis", "Jeff Z. Pan", "Marco Valentino", "Pasquale Minervini"], "title": "OpenSIR: Open-Ended Self-Improving Reasoner", "comment": null, "summary": "Recent advances in large language model (LLM) reasoning through reinforcement\nlearning rely on annotated datasets for verifiable rewards, which may limit\nmodels' ability to surpass human-level performance. While self-play offers a\npromising alternative, existing approaches depend on external verifiers or\ncannot learn open-endedly. We present Open-Ended Self-Improving Reasoner\n(OpenSIR), a self-play framework where an LLM learns to generate and solve\nnovel problems by alternating teacher and student roles without external\nsupervision. To generate novel problems, OpenSIR optimises for both difficulty\nand diversity, rewarding problems that challenge appropriately while exploring\ndistinct concepts, enabling open-ended mathematical discovery. Starting from a\nsingle trivial seed problem, OpenSIR substantially improves instruction models:\nLlama-3.2-3B-Instruct advances from 73.9 to 78.3 on GSM8K, and from 28.8 to\n34.4 on College Math, while Gemma-2-2B-Instruct rises from 38.5 to 58.7 on\nGSM8K. Our analyses reveal that OpenSIR achieves open-ended learning through\nco-evolving teacher-student roles that adaptively calibrate difficulty and\ndrive diverse exploration, progressing autonomously from basic to advanced\nmathematics."}
{"id": "2511.00751", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00751", "abs": "https://arxiv.org/abs/2511.00751", "authors": ["Chiyan Loo"], "title": "Reevaluating Self-Consistency Scaling in Multi-Agent Systems", "comment": "7 pages, 3 figures", "summary": "This study examines the trade-offs of increasing sampled reasoning paths in\nself-consistency for modern large language models (LLMs). Earlier research with\nolder models showed that combining multiple reasoning chains improves results\nbefore reaching a plateau. Using Gemini 2.5 models on HotpotQA and Math-500, we\nrevisit those claims under current model conditions. Each configuration pooled\noutputs from varying sampled reasoning paths and compared them to a single\nchain-of-thought (CoT) baseline. Larger models exhibited a more stable and\nconsistent improvement curve. The results confirm that performance gains taper\noff after moderate sampling, aligning with past findings. This plateau suggests\ndiminishing returns driven by overlap among reasoning paths. Self-consistency\nremains useful, but high-sample configurations offer little benefit relative to\ntheir computational cost."}
{"id": "2511.00606", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00606", "abs": "https://arxiv.org/abs/2511.00606", "authors": ["Jameson Sandler", "Jacob K. Christopher", "Thomas Hartvigsen", "Nando Fioretto"], "title": "SpecDiff-2: Scaling Diffusion Drafter Alignment For Faster Speculative Decoding", "comment": null, "summary": "Speculative decoding has become the standard approach for accelerating Large\nLanguage Model (LLM) inference. It exploits a lossless draft-then-verify\nprocedure to circumvent the latency of autoregressive decoding, achieving\nimpressive speed-ups. Yet, current speculative decoding approaches remain\nlimited by two fundamental bottlenecks: (1) the autoregressive dependency\nduring drafting which limits parallelism, and (2) frequent rejections of draft\ntokens caused by misalignment between the draft and verify models. This paper\nproposes SpecDiff-2, a novel framework to jointly address these two\nbottlenecks. It leverages discrete diffusion as a non-autoregressive drafter to\naddress bottleneck (1) and develops novel techniques to calibrate discrete\ndiffusion drafters with autoregressive verifiers, addressing bottleneck (2).\nExperimental results across a comprehensive benchmark suite show that\nSpecDiff-2 achieves a new state-of-the-art across reasoning, coding, and\nmathematical benchmarks, improving tokens-per-second by up to an average of\n+55% over previous baselines and obtaining up to 5.5x average speed-up over\nstandard decoding, without any loss of accuracy."}
{"id": "2511.00758", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00758", "abs": "https://arxiv.org/abs/2511.00758", "authors": ["Hong Su"], "title": "Active Thinking Model: A Goal-Directed Self-Improving Framework for Real-World Adaptive Intelligence", "comment": null, "summary": "Real-world artificial intelligence (AI) systems are increasingly required to\noperate autonomously in dynamic, uncertain, and continuously changing\nenvironments. However, most existing AI models rely on predefined objectives,\nstatic training data, and externally supplied feedback, which restrict their\nability to adapt, reflect, and improve independently. In this paper, we propose\nthe Active Thinking Model (ATM)- a unified cognitive framework that integrates\ngoal reasoning, dynamic task generation, and self-reflective learning into an\nadaptive architecture. Unlike conventional systems that passively execute fixed\nprocedures, ATM actively evaluates its performance through logical reasoning\nand environmental indicators, reuses effective methods to solve new problems,\nand generates novel strategies for unseen situations via a continuous\nself-improvement loop. A mathematically grounded theoretical analysis\ndemonstrates that ATM can autonomously evolve from suboptimal to optimal\nbehavior without external supervision and maintain bounded tracking regret\nunder changing environmental conditions."}
{"id": "2511.00620", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00620", "abs": "https://arxiv.org/abs/2511.00620", "authors": ["Autumn Toney-Wails", "Ryan Wails"], "title": "Certain but not Probable? Differentiating Certainty from Probability in LLM Token Outputs for Probabilistic Scenarios", "comment": "To appear at the Second Workshop on Uncertainty-Aware NLP @EMNLP 2025\n  (UncertaiNLP '25)", "summary": "Reliable uncertainty quantification (UQ) is essential for ensuring\ntrustworthy downstream use of large language models, especially when they are\ndeployed in decision-support and other knowledge-intensive applications. Model\ncertainty can be estimated from token logits, with derived probability and\nentropy values offering insight into performance on the prompt task. However,\nthis approach may be inadequate for probabilistic scenarios, where the\nprobabilities of token outputs are expected to align with the theoretical\nprobabilities of the possible outcomes. We investigate the relationship between\ntoken certainty and alignment with theoretical probability distributions in\nwell-defined probabilistic scenarios. Using GPT-4.1 and DeepSeek-Chat, we\nevaluate model responses to ten prompts involving probability (e.g., roll a\nsix-sided die), both with and without explicit probability cues in the prompt\n(e.g., roll a fair six-sided die). We measure two dimensions: (1) response\nvalidity with respect to scenario constraints, and (2) alignment between\ntoken-level output probabilities and theoretical probabilities. Our results\nindicate that, while both models achieve perfect in-domain response accuracy\nacross all prompt scenarios, their token-level probability and entropy values\nconsistently diverge from the corresponding theoretical distributions."}
{"id": "2511.00763", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00763", "abs": "https://arxiv.org/abs/2511.00763", "authors": ["Wanda Hou", "Leon Zhou", "Hong-Ye Hu", "Yi-Zhuang You", "Xiao-Liang Qi"], "title": "How Focused Are LLMs? A Quantitative Study via Repetitive Deterministic Prediction Tasks", "comment": null, "summary": "We investigate the performance of large language models on repetitive\ndeterministic prediction tasks and study how the sequence accuracy rate scales\nwith output length. Each such task involves repeating the same operation n\ntimes. Examples include letter replacement in strings following a given rule,\ninteger addition, and multiplication of string operators in many body quantum\nmechanics. If the model performs the task through a simple repetition\nalgorithm, the success rate should decay exponentially with sequence length. In\ncontrast, our experiments on leading large language models reveal a sharp\ndouble exponential drop beyond a characteristic length scale, forming an\naccuracy cliff that marks the transition from reliable to unstable generation.\nThis indicates that the models fail to execute each operation independently. To\nexplain this phenomenon, we propose a statistical physics inspired model that\ncaptures the competition between external conditioning from the prompt and\ninternal interference among generated tokens. The model quantitatively\nreproduces the observed crossover and provides an interpretable link between\nattention induced interference and sequence level failure. Fitting the model to\nempirical results across multiple models and tasks yields effective parameters\nthat characterize the intrinsic error rate and error accumulation factor for\neach model task pair, offering a principled framework for understanding the\nlimits of deterministic accuracy in large language models."}
{"id": "2511.00627", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00627", "abs": "https://arxiv.org/abs/2511.00627", "authors": ["Jean Barr", "Olga Seminck", "Antoine Bourgois", "Thierry Poibeau"], "title": "Modeling the Construction of a Literary Archetype: The Case of the Detective Figure in French Literature", "comment": "19 pages, 2 tables, 5 figures Conference Computational Humanities\n  Research 2025", "summary": "This research explores the evolution of the detective archetype in French\ndetective fiction through computational analysis. Using quantitative methods\nand character-level embeddings, we show that a supervised model is able to\ncapture the unity of the detective archetype across 150 years of literature,\nfrom M. Lecoq (1866) to Commissaire Adamsberg (2017). Building on this finding,\nthe study demonstrates how the detective figure evolves from a secondary\nnarrative role to become the central character and the \"reasoning machine\" of\nthe classical detective story. In the aftermath of the Second World War, with\nthe importation of the hardboiled tradition into France, the archetype becomes\nmore complex, navigating the genre's turn toward social violence and moral\nambiguity."}
{"id": "2511.00782", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00782", "abs": "https://arxiv.org/abs/2511.00782", "authors": ["Jifan Gao", "Michael Rosenthal", "Brian Wolpin", "Simona Cristea"], "title": "Count-Based Approaches Remain Strong: A Benchmark Against Transformer and LLM Pipelines on Structured EHR", "comment": null, "summary": "Structured electronic health records (EHR) are essential for clinical\nprediction. While count-based learners continue to perform strongly on such\ndata, no benchmarking has directly compared them against more recent\nmixture-of-agents LLM pipelines, which have been reported to outperform single\nLLMs in various NLP tasks. In this study, we evaluated three categories of\nmethodologies for EHR prediction using the EHRSHOT dataset: count-based models\nbuilt from ontology roll-ups with two time bins, based on LightGBM and the\ntabular foundation model TabPFN; a pretrained sequential transformer (CLMBR);\nand a mixture-of-agents pipeline that converts tabular histories to\nnatural-language summaries followed by a text classifier. We assessed eight\noutcomes using the EHRSHOT dataset. Across the eight evaluation tasks,\nhead-to-head wins were largely split between the count-based and the\nmixture-of-agents methods. Given their simplicity and interpretability,\ncount-based models remain a strong candidate for structured EHR benchmarking.\nThe source code is available at:\nhttps://github.com/cristea-lab/Structured_EHR_Benchmark."}
{"id": "2511.00657", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00657", "abs": "https://arxiv.org/abs/2511.00657", "authors": ["Eshaan Tanwar", "Anwoy Chatterjee", "Michael Saxon", "Alon Albalak", "William Yang Wang", "Tanmoy Chakraborty"], "title": "Do You Know About My Nation? Investigating Multilingual Language Models' Cultural Literacy Through Factual Knowledge", "comment": "Accepted in EMNLP 2025. Code at: https://github.com/EshaanT/XNationQA", "summary": "Most multilingual question-answering benchmarks, while covering a diverse\npool of languages, do not factor in regional diversity in the information they\ncapture and tend to be Western-centric. This introduces a significant gap in\nfairly evaluating multilingual models' comprehension of factual information\nfrom diverse geographical locations. To address this, we introduce XNationQA\nfor investigating the cultural literacy of multilingual LLMs. XNationQA\nencompasses a total of 49,280 questions on the geography, culture, and history\nof nine countries, presented in seven languages. We benchmark eight standard\nmultilingual LLMs on XNationQA and evaluate them using two novel transference\nmetrics. Our analyses uncover a considerable discrepancy in the models'\naccessibility to culturally specific facts across languages. Notably, we often\nfind that a model demonstrates greater knowledge of cultural information in\nEnglish than in the dominant language of the respective culture. The models\nexhibit better performance in Western languages, although this does not\nnecessarily translate to being more literate for Western countries, which is\ncounterintuitive. Furthermore, we observe that models have a very limited\nability to transfer knowledge across languages, particularly evident in\nopen-source models."}
{"id": "2511.00808", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00808", "abs": "https://arxiv.org/abs/2511.00808", "authors": ["Bowen Fang", "Ruijian Zha", "Xuan Di"], "title": "Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events?", "comment": null, "summary": "Predicting public transit incident duration from unstructured text alerts is\na critical but challenging task. Addressing the domain sparsity of transit\noperations with standard Supervised Fine-Tuning (SFT) is difficult, as the task\ninvolves noisy, continuous labels and lacks reliable expert demonstrations for\nreasoning. While Reinforcement Learning from Verifiable Rewards (RLVR) excels\nat tasks with binary correctness, like mathematics, its applicability to noisy,\ncontinuous forecasting is an open question. This work, to our knowledge, is the\nfirst to bridge the gap between RLVR LLM training with the critical, real-world\nforecasting challenges in public transit operations. We adapt RLVR to this task\nby introducing a tolerance-based, shaped reward function that grants partial\ncredit within a continuous error margin, rather than demanding a single correct\nanswer. We systematically evaluate this framework on a curated dataset of NYC\nMTA service alerts. Our findings show that general-purpose, instruction-tuned\nLLMs significantly outperform specialized math-reasoning models, which struggle\nwith the ambiguous, real-world text. We empirically demonstrate that the binary\nreward is unstable and degrades performance, whereas our shaped reward design\nis critical and allows our model to dominate on the most challenging metrics.\nWhile classical regressors are superior at minimizing overall MAE or MSE, our\nRLVR approach achieved a 35\\% relative improvement in 5-minute accuracy (Acc@5)\nover the strongest baseline. This demonstrates that RLVR can be successfully\nadapted to real-world, noisy forecasting, but requires a verifier design that\nreflects the continuous nature of the problem."}
{"id": "2511.00689", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00689", "abs": "https://arxiv.org/abs/2511.00689", "authors": ["Berk Atil", "Rebecca J. Passonneau", "Fred Morstatter"], "title": "Do Methods to Jailbreak and Defend LLMs Generalize Across Languages?", "comment": null, "summary": "Large language models (LLMs) undergo safety alignment after training and\ntuning, yet recent work shows that safety can be bypassed through jailbreak\nattacks. While many jailbreaks and defenses exist, their cross-lingual\ngeneralization remains underexplored. This paper presents the first systematic\nmultilingual evaluation of jailbreaks and defenses across ten\nlanguages--spanning high-, medium-, and low-resource languages--using six LLMs\non HarmBench and AdvBench. We assess two jailbreak types:\nlogical-expression-based and adversarial-prompt-based. For both types, attack\nsuccess and defense robustness vary across languages: high-resource languages\nare safer under standard queries but more vulnerable to adversarial ones.\nSimple defenses can be effective, but are language- and model-dependent. These\nfindings call for language-aware and cross-lingual safety benchmarks for LLMs."}
{"id": "2511.00926", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00926", "abs": "https://arxiv.org/abs/2511.00926", "authors": ["Kyung-Hoon Kim"], "title": "LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory", "comment": "19 pages, 6 figures, 28 models tested across 4,200 trials", "summary": "As Large Language Models (LLMs) grow in capability, do they develop\nself-awareness as an emergent behavior? And if so, can we measure it? We\nintroduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for\nmeasuring self-awareness through strategic differentiation. Using the \"Guess\n2/3 of Average\" game, we test 28 models (OpenAI, Anthropic, Google) across\n4,200 trials with three opponent framings: (A) against humans, (B) against\nother AI models, and (C) against AI models like you. We operationalize\nself-awareness as the capacity to differentiate strategic reasoning based on\nopponent type. Finding 1: Self-awareness emerges with model advancement. The\nmajority of advanced models (21/28, 75%) demonstrate clear self-awareness,\nwhile older/smaller models show no differentiation. Finding 2: Self-aware\nmodels rank themselves as most rational. Among the 21 models with\nself-awareness, a consistent rationality hierarchy emerges: Self > Other AIs >\nHumans, with large AI attribution effects and moderate self-preferencing. These\nfindings reveal that self-awareness is an emergent capability of advanced LLMs,\nand that self-aware models systematically perceive themselves as more rational\nthan humans. This has implications for AI alignment, human-AI collaboration,\nand understanding AI beliefs about human capabilities."}
{"id": "2511.00819", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00819", "abs": "https://arxiv.org/abs/2511.00819", "authors": ["Yuxuan Hu", "Jianchao Tan", "Jiaqi Zhang", "Wen Zan", "Pingwei Sun", "Yifan Lu", "Yerui Sun", "Yuchen Xie", "Xunliang Cai", "Jing Zhang"], "title": "Optimizing Native Sparse Attention with Latent Attention and Local Global Alternating Strategies", "comment": null, "summary": "In this work, we conduct a systematic analysis of Native Sparse Attention\n(NSA) and propose targeted improvements that enhance long-context modeling. A\nkey insight is that alternating between local (sliding-window) and global\n(compression, selective) attention across layers, rather than using fixed\npatterns, enables more effective propagation of long-range dependencies and\nsubstantially boosts performance on long-sequence tasks. Meanwhile, we further\nrefine NSA's branches with Latent Attention that the sliding-window branch is\nenhanced with Multi-head Latent Attention (MLA) while compression and selective\nbranches adopt Group-head Latent Attention (GLA). These changes reduce KV-cache\nmemory by 50\\% versus NSA while improving the model's common-sense reasoning\nand long-text understanding capabilities. Experiments on models from 340M to\n1.3B parameters (trained on 15B and 100B tokens) show our method matches or\nexceeds full attention and native sparse attention in both common-sense\nreasoning and long-context understanding tasks."}
{"id": "2511.00993", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00993", "abs": "https://arxiv.org/abs/2511.00993", "authors": ["Tianming Liu", "Jirong Yang", "Yafeng Yin", "Manzi Li", "Linghao Wang", "Zheng Zhu"], "title": "Aligning LLM agents with human learning and adjustment behavior: a dual agent approach", "comment": "32 pages, 6 figures, 7 tables", "summary": "Effective modeling of how human travelers learn and adjust their travel\nbehavior from interacting with transportation systems is critical for system\nassessment and planning. However, this task is also difficult due to the\ncomplex cognition and decision-making involved in such behavior. Recent\nresearch has begun to leverage Large Language Model (LLM) agents for this task.\nBuilding on this, we introduce a novel dual-agent framework that enables\ncontinuous learning and alignment between LLM agents and human travelers on\nlearning and adaptation behavior from online data streams. Our approach\ninvolves a set of LLM traveler agents, equipped with a memory system and a\nlearnable persona, which serve as simulators for human travelers. To ensure\nbehavioral alignment, we introduce an LLM calibration agent that leverages the\nreasoning and analytical capabilities of LLMs to train the personas of these\ntraveler agents. Working together, this dual-agent system is designed to track\nand align the underlying decision-making mechanisms of travelers and produce\nrealistic, adaptive simulations. Using a real-world dataset from a day-to-day\nroute choice experiment, we show our approach significantly outperforms\nexisting LLM-based methods in both individual behavioral alignment and\naggregate simulation accuracy. Furthermore, we demonstrate that our method\nmoves beyond simple behavioral mimicry to capture the evolution of underlying\nlearning processes, a deeper alignment that fosters robust generalization.\nOverall, our framework provides a new approach for creating adaptive and\nbehaviorally realistic agents to simulate travelers' learning and adaptation\nthat can benefit transportation simulation and policy analysis."}
{"id": "2511.00854", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00854", "abs": "https://arxiv.org/abs/2511.00854", "authors": ["Chong Lyu", "Lin Li", "Shiqing Wu", "Jingling Yuan"], "title": "TriCon-Fair: Triplet Contrastive Learning for Mitigating Social Bias in Pre-trained Language Models", "comment": null, "summary": "The increasing utilization of large language models raises significant\nconcerns about the propagation of social biases, which may result in harmful\nand unfair outcomes. However, existing debiasing methods treat the biased and\nunbiased samples independently, thus ignoring their mutual relationship. This\noversight enables a hidden negative-positive coupling, where improvements for\none group inadvertently compromise the other, allowing residual social bias to\npersist. In this paper, we introduce TriCon-Fair, a contrastive learning\nframework that employs a decoupled loss that combines triplet and language\nmodeling terms to eliminate positive-negative coupling. Our TriCon-Fair assigns\neach anchor an explicitly biased negative and an unbiased positive, decoupling\nthe push-pull dynamics and avoiding positive-negative coupling, and jointly\noptimizes a language modeling (LM) objective to preserve general capability.\nExperimental results demonstrate that TriCon-Fair reduces discriminatory output\nbeyond existing debiasing baselines while maintaining strong downstream\nperformance. This suggests that our proposed TriCon-Fair offers a practical and\nethical solution for sensitive NLP applications."}
{"id": "2511.01018", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01018", "abs": "https://arxiv.org/abs/2511.01018", "authors": ["Hui-Lee Ooi", "Nicholas Mitsakakis", "Margerie Huet Dastarac", "Roger Zemek", "Amy C. Plint", "Jeff Gilchrist", "Khaled El Emam", "Dhenuka Radhakrishnan"], "title": "AI for pRedicting Exacerbations in KIDs with aSthma (AIRE-KIDS)", "comment": null, "summary": "Recurrent exacerbations remain a common yet preventable outcome for many\nchildren with asthma. Machine learning (ML) algorithms using electronic medical\nrecords (EMR) could allow accurate identification of children at risk for\nexacerbations and facilitate referral for preventative comprehensive care to\navoid this morbidity. We developed ML algorithms to predict repeat severe\nexacerbations (i.e. asthma-related emergency department (ED) visits or future\nhospital admissions) for children with a prior asthma ED visit at a tertiary\ncare children's hospital.\n  Retrospective pre-COVID19 (Feb 2017 - Feb 2019, N=2716) Epic EMR data from\nthe Children's Hospital of Eastern Ontario (CHEO) linked with environmental\npollutant exposure and neighbourhood marginalization information was used to\ntrain various ML models. We used boosted trees (LGBM, XGB) and 3 open-source\nlarge language model (LLM) approaches (DistilGPT2, Llama 3.2 1B and\nLlama-8b-UltraMedical). Models were tuned and calibrated then validated in a\nsecond retrospective post-COVID19 dataset (Jul 2022 - Apr 2023, N=1237) from\nCHEO. Models were compared using the area under the curve (AUC) and F1 scores,\nwith SHAP values used to determine the most predictive features.\n  The LGBM ML model performed best with the most predictive features in the\nfinal AIRE-KIDS_ED model including prior asthma ED visit, the Canadian triage\nacuity scale, medical complexity, food allergy, prior ED visits for non-asthma\nrespiratory diagnoses, and age for an AUC of 0.712, and F1 score of 0.51. This\nis a nontrivial improvement over the current decision rule which has F1=0.334.\nWhile the most predictive features in the AIRE-KIDS_HOSP model included medical\ncomplexity, prior asthma ED visit, average wait time in the ED, the pediatric\nrespiratory assessment measure score at triage and food allergy."}
{"id": "2511.00879", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00879", "abs": "https://arxiv.org/abs/2511.00879", "authors": ["Hyeon Hwang", "Yewon Cho", "Chanwoong Yoon", "Yein Park", "Minju Song", "Kyungjae Lee", "Gangwoo Kim", "Jaewoo Kang"], "title": "Assessing LLM Reasoning Steps via Principal Knowledge Grounding", "comment": "Accepted to EMNLP 2025 Findings", "summary": "Step-by-step reasoning has become a standard approach for large language\nmodels (LLMs) to tackle complex tasks. While this paradigm has proven\neffective, it raises a fundamental question: How can we verify that an LLM's\nreasoning is accurately grounded in knowledge? To address this question, we\nintroduce a novel evaluation suite that systematically assesses the knowledge\ngrounding of intermediate reasoning. Our framework comprises three key\ncomponents. (1) Principal Knowledge Collection, a large-scale repository of\natomic knowledge essential for reasoning. Based on the collection, we propose\n(2) knowledge-grounded evaluation metrics designed to measure how well models\nrecall and apply prerequisite knowledge in reasoning. These metrics are\ncomputed by our (3) evaluator LLM, a lightweight model optimized for\ncost-effective and reliable metric computation. Our evaluation suite\ndemonstrates remarkable effectiveness in identifying missing or misapplied\nknowledge elements, providing crucial insights for uncovering fundamental\nreasoning deficiencies in LLMs. Beyond evaluation, we demonstrate how these\nmetrics can be integrated into preference optimization, showcasing further\napplications of knowledge-grounded evaluation."}
{"id": "2511.01033", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01033", "abs": "https://arxiv.org/abs/2511.01033", "authors": ["Tiberiu Musat", "Tiago Pimentel", "Lorenzo Noci", "Alessandro Stolfo", "Mrinmaya Sachan", "Thomas Hofmann"], "title": "On the Emergence of Induction Heads for In-Context Learning", "comment": null, "summary": "Transformers have become the dominant architecture for natural language\nprocessing. Part of their success is owed to a remarkable capability known as\nin-context learning (ICL): they can acquire and apply novel associations solely\nfrom their input context, without any updates to their weights. In this work,\nwe study the emergence of induction heads, a previously identified mechanism in\ntwo-layer transformers that is particularly important for in-context learning.\nWe uncover a relatively simple and interpretable structure of the weight\nmatrices implementing the induction head. We theoretically explain the origin\nof this structure using a minimal ICL task formulation and a modified\ntransformer architecture. We give a formal proof that the training dynamics\nremain constrained to a 19-dimensional subspace of the parameter space.\nEmpirically, we validate this constraint while observing that only 3 dimensions\naccount for the emergence of an induction head. By further studying the\ntraining dynamics inside this 3-dimensional subspace, we find that the time\nuntil the emergence of an induction head follows a tight asymptotic bound that\nis quadratic in the input context length."}
{"id": "2511.00903", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00903", "abs": "https://arxiv.org/abs/2511.00903", "authors": ["Ahmed Masry", "Megh Thakkar", "Patrice Bechard", "Sathwik Tejaswi Madhusudhan", "Rabiul Awal", "Shambhavi Mishra", "Akshay Kalkunte Suresh", "Srivatsava Daruru", "Enamul Hoque", "Spandana Gella", "Torsten Scholak", "Sai Rajeswar"], "title": "ColMate: Contrastive Late Interaction and Masked Text for Multimodal Document Retrieval", "comment": null, "summary": "Retrieval-augmented generation has proven practical when models require\nspecialized knowledge or access to the latest data. However, existing methods\nfor multimodal document retrieval often replicate techniques developed for\ntext-only retrieval, whether in how they encode documents, define training\nobjectives, or compute similarity scores. To address these limitations, we\npresent ColMate, a document retrieval model that bridges the gap between\nmultimodal representation learning and document retrieval. ColMate utilizes a\nnovel OCR-based pretraining objective, a self-supervised masked contrastive\nlearning objective, and a late interaction scoring mechanism more relevant to\nmultimodal document structures and visual characteristics. ColMate obtains\n3.61% improvements over existing retrieval models on the ViDoRe V2 benchmark,\ndemonstrating stronger generalization to out-of-domain benchmarks."}
{"id": "2511.01052", "categories": ["cs.AI", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2511.01052", "abs": "https://arxiv.org/abs/2511.01052", "authors": ["Yeawon Lee", "Christopher C. Yang", "Chia-Hsuan Chang", "Grace Lu-Yao"], "title": "Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports", "comment": null, "summary": "Cancer staging is critical for patient prognosis and treatment planning, yet\nextracting pathologic TNM staging from unstructured pathology reports poses a\npersistent challenge. Existing natural language processing (NLP) and machine\nlearning (ML) strategies often depend on large annotated datasets, limiting\ntheir scalability and adaptability. In this study, we introduce two Knowledge\nElicitation methods designed to overcome these limitations by enabling large\nlanguage models (LLMs) to induce and apply domain-specific rules for cancer\nstaging. The first, Knowledge Elicitation with Long-Term Memory (KEwLTM), uses\nan iterative prompting strategy to derive staging rules directly from\nunannotated pathology reports, without requiring ground-truth labels. The\nsecond, Knowledge Elicitation with Retrieval-Augmented Generation (KEwRAG),\nemploys a variation of RAG where rules are pre-extracted from relevant\nguidelines in a single step and then applied, enhancing interpretability and\navoiding repeated retrieval overhead. We leverage the ability of LLMs to apply\nbroad knowledge learned during pre-training to new tasks. Using breast cancer\npathology reports from the TCGA dataset, we evaluate their performance in\nidentifying T and N stages, comparing them against various baseline approaches\non two open-source LLMs. Our results indicate that KEwLTM outperforms KEwRAG\nwhen Zero-Shot Chain-of-Thought (ZSCOT) inference is effective, whereas KEwRAG\nachieves better performance when ZSCOT inference is less effective. Both\nmethods offer transparent, interpretable interfaces by making the induced rules\nexplicit. These findings highlight the promise of our Knowledge Elicitation\nmethods as scalable, high-performing solutions for automated cancer staging\nwith enhanced interpretability, particularly in clinical settings with limited\nannotated data."}
{"id": "2511.00924", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00924", "abs": "https://arxiv.org/abs/2511.00924", "authors": ["Jianzhou Yao", "Shunchang Liu", "Guillaume Drui", "Rikard Pettersson", "Alessandro Blasimme", "Sara Kijewski"], "title": "The Biased Oracle: Assessing LLMs' Understandability and Empathy in Medical Diagnoses", "comment": "Accepted by NeurIPS 2025 GenAI4Health Workshop", "summary": "Large language models (LLMs) show promise for supporting clinicians in\ndiagnostic communication by generating explanations and guidance for patients.\nYet their ability to produce outputs that are both understandable and\nempathetic remains uncertain. We evaluate two leading LLMs on medical\ndiagnostic scenarios, assessing understandability using readability metrics as\na proxy and empathy through LLM-as-a-Judge ratings compared to human\nevaluations. The results indicate that LLMs adapt explanations to\nsocio-demographic variables and patient conditions. However, they also generate\noverly complex content and display biased affective empathy, leading to uneven\naccessibility and support. These patterns underscore the need for systematic\ncalibration to ensure equitable patient communication. The code and data are\nreleased: https://github.com/Jeffateth/Biased_Oracle"}
{"id": "2511.01059", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01059", "abs": "https://arxiv.org/abs/2511.01059", "authors": ["Hailong Yin", "Bin Zhu", "Jingjing Chen", "Chong-Wah Ngo"], "title": "Efficient Test-Time Retrieval Augmented Generation", "comment": null, "summary": "Although Large Language Models (LLMs) demonstrate significant capabilities,\ntheir reliance on parametric knowledge often leads to inaccuracies. Retrieval\nAugmented Generation (RAG) mitigates this by incorporating external knowledge,\nbut these methods may introduce irrelevant retrieved documents, leading to\ninaccurate responses. While the integration methods filter out incorrect\nanswers from multiple responses, but lack external knowledge like RAG methods,\nand their high costs require balancing overhead with performance gains. To\naddress these issues, we propose an Efficient Test-Time Retrieval-Augmented\nGeneration Framework named ET2RAG to improve the performance of LLMs while\nmaintaining efficiency. Specifically, ET2RAG is a training-free method, that\nfirst retrieves the most relevant documents and augments the LLMs to\nefficiently generate diverse candidate responses by managing response length.\nThen we compute the similarity of candidate responses and employ a majority\nvoting mechanism to select the most suitable response as the final output. In\nparticular, we discover that partial generation is sufficient to capture the\nkey information necessary for consensus calculation, allowing us to effectively\nperform majority voting without the need for fully generated responses. Thus,\nwe can reach a balance between computational cost and performance by managing\nthe response length for the number of retrieved documents for majority voting.\nExperimental results demonstrate that ET2RAG significantly enhances performance\nacross three tasks, including open-domain question answering, recipe generation\nand image captioning."}
{"id": "2511.00960", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00960", "abs": "https://arxiv.org/abs/2511.00960", "authors": ["Abhinav P M", "Ojasva Saxena", "Oswald C", "Parameswari Krishnamurthy"], "title": "The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in Multilingual LLMs using Indian Riddles", "comment": null, "summary": "The extent to which large language models (LLMs) can perform culturally\ngrounded reasoning across non-English languages remains underexplored. This\npaper examines the reasoning and self-assessment abilities of LLMs across seven\nmajor Indian languages-Bengali, Gujarati, Hindi, Kannada, Malayalam, Tamil, and\nTelugu. We introduce a multilingual riddle dataset combining traditional\nriddles with context-reconstructed variants and evaluate five LLMs-Gemini 2.5\nPro, Gemini 2.5 Flash, Mistral-Saba, LLaMA 4 Scout, and LLaMA 4 Maverick-under\nseven prompting strategies. In the first stage, we assess riddle-solving\nperformance and find that while Gemini 2.5 Pro performs best overall, few-shot\nmethods yield only marginal gains, and accuracy varies notably across\nlanguages. In the second stage, we conduct a self-evaluation experiment to\nmeasure reasoning consistency. The results reveal a key finding: a model's\ninitial accuracy is inversely correlated with its ability to identify its own\nmistakes. Top-performing models such as Gemini 2.5 Pro are overconfident (4.34%\nTrue Negative Rate), whereas lower-performing models like LLaMA 4 Scout are\nsubstantially more self-aware (42.09% True Negative Rate). These results point\nto clear gaps in multilingual reasoning and highlight the need for models that\nnot only reason effectively but also recognize their own limitations."}
{"id": "2511.01149", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01149", "abs": "https://arxiv.org/abs/2511.01149", "authors": ["Shuaidong Pan", "Di Wu"], "title": "Modular Task Decomposition and Dynamic Collaboration in Multi-Agent Systems Driven by Large Language Models", "comment": null, "summary": "This paper addresses the limitations of a single agent in task decomposition\nand collaboration during complex task execution, and proposes a multi-agent\narchitecture for modular task decomposition and dynamic collaboration based on\nlarge language models. The method first converts natural language task\ndescriptions into unified semantic representations through a large language\nmodel. On this basis, a modular decomposition mechanism is introduced to break\ndown the overall goal into multiple hierarchical sub-tasks. Then, dynamic\nscheduling and routing mechanisms enable reasonable division of labor and\nrealtime collaboration among agents, allowing the system to adjust strategies\ncontinuously according to environmental feedback, thus maintaining efficiency\nand stability in complex tasks. Furthermore, a constraint parsing and global\nconsistency mechanism is designed to ensure coherent connections between\nsub-tasks and balanced workload, preventing performance degradation caused by\nredundant communication or uneven resource allocation. The experiments validate\nthe architecture across multiple dimensions, including task success rate,\ndecomposition efficiency, sub-task coverage, and collaboration balance. The\nresults show that the proposed method outperforms existing approaches in both\noverall performance and robustness, achieving a better balance between task\ncomplexity and communication overhead. In conclusion, this study demonstrates\nthe effectiveness and feasibility of language-driven task decomposition and\ndynamic collaboration in multi-agent systems, providing a systematic solution\nfor task execution in complex environments."}
{"id": "2511.00988", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00988", "abs": "https://arxiv.org/abs/2511.00988", "authors": ["Chenwang Wu", "Yiu-ming Cheung", "Bo Han", "Defu Lian"], "title": "Advancing Machine-Generated Text Detection from an Easy to Hard Supervision Perspective", "comment": null, "summary": "Existing machine-generated text (MGT) detection methods implicitly assume\nlabels as the \"golden standard\". However, we reveal boundary ambiguity in MGT\ndetection, implying that traditional training paradigms are inexact. Moreover,\nlimitations of human cognition and the superintelligence of detectors make\ninexact learning widespread and inevitable. To this end, we propose an\neasy-to-hard enhancement framework to provide reliable supervision under such\ninexact conditions. Distinct from knowledge distillation, our framework employs\nan easy supervisor targeting relatively simple longer-text detection tasks\n(despite weaker capabilities), to enhance the more challenging target detector.\nFirstly, longer texts targeted by supervisors theoretically alleviate the\nimpact of inexact labels, laying the foundation for reliable supervision.\nSecondly, by structurally incorporating the detector into the supervisor, we\ntheoretically model the supervisor as a lower performance bound for the\ndetector. Thus, optimizing the supervisor indirectly optimizes the detector,\nultimately approximating the underlying \"golden\" labels. Extensive experiments\nacross diverse practical scenarios, including cross-LLM, cross-domain, mixed\ntext, and paraphrase attacks, demonstrate the framework's significant detection\neffectiveness. The code is available at:\nhttps://github.com/tmlr-group/Easy2Hard."}
{"id": "2511.01170", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01170", "abs": "https://arxiv.org/abs/2511.01170", "authors": ["Ruofan Zhang", "Bin Xia", "Zhen Cheng", "Cairen Jian", "Minglun Yang", "Ngai Wong", "Yuan Cheng"], "title": "DART: Difficulty-Adaptive Reasoning Truncation for Efficient Large Language Models", "comment": null, "summary": "Adaptive reasoning is essential for aligning the computational effort of\nlarge language models (LLMs) with the intrinsic difficulty of problems. Current\nchain-of-thought methods boost reasoning ability but indiscriminately generate\nlong explanations, leading to evident inefficiency. However, existing\nreinforcement learning approaches to adaptive thinking remain unstable and\nheavily reward-dependent. Here we propose \\textbf{DART}, a supervised\n\\textbf{D}ifficulty-\\textbf{A}daptive \\textbf{R}easoning \\textbf{T}runcation\nframework that adjusts thinking length according to problem difficulty. By\ndistilling concise reasoning patterns from stronger models, interpolating them\ninto a continuum of reasoning styles, and curating optimal training data that\nbalances correctness and compactness, DART learns when to ``stop thinking''.\nAcross multiple mathematical benchmarks, experimental results demonstrate its\nremarkable efficiency while preserving or improving accuracy, achieving a\nsignificant 81.2\\% reasoning truncation (DeepSeek-R1-Distill-Qwen-7B on GSM8K\ndataset) with 5.33$\\times$ computational acceleration. DART provides a stable\nand general paradigm for efficient reasoning, advancing the development of\nadaptive intelligence in LLMs."}
{"id": "2511.01008", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01008", "abs": "https://arxiv.org/abs/2511.01008", "authors": ["Haolin Yang", "Jipeng Zhang", "Zhitao He", "Yi R. Fung"], "title": "MARS-SQL: A multi-agent reinforcement learning framework for Text-to-SQL", "comment": null, "summary": "Translating natural language to SQL remains difficult for complex queries.\nSuch queries often need environmental interaction and self-correction. To\naddress this, we introduce MARS-SQL, a novel multi-agent framework that\ncombines principled task decomposition and interactive reinforcement learning\n(RL). Our system comprises three specialized agents: a Grounding Agent for\nschema linking, a Generation Agent for query generation, and a Validation Agent\nfor final selection. The core of our framework is the Generation agent, which\nis trained via a multi-turn RL policy. Adopting a ReAct-style Think-Act-Observe\nloop, the agent iteratively generates thoughts, executes SQL actions against a\nlive database, and revises its strategy based on execution feedback, enabling\ndynamic, stateful reasoning and self-correction. At inference time, we generate\nmultiple interaction trajectories to explore diverse reasoning paths. The\nValidation agent, then selects the optimal trajectory by modeling verification\nas a next-token prediction task and choosing the solution with the highest\ngeneration probability. This structured workflow pipelines specialized agents.\nIt combines interactive RL for generation with generative modeling for\nverification. The approach proves highly effective for robust and accurate SQL\ngeneration. Experiments show that MARS-SQL achieves state-of-the-art Execution\nAccuracy of 77.84% on the BIRD dev set and 89.75% on the Spider test set. Our\ncode is available at https://github.com/YangHaolin0526/MARS-SQL."}
{"id": "2511.01182", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01182", "abs": "https://arxiv.org/abs/2511.01182", "authors": ["Cuong Van Duc", "Thai Tran Quoc", "Minh Nguyen Dinh Tuan", "Tam Vu Duc", "Son Nguyen Van", "Hanh Nguyen Thi"], "title": "MiRAGE: Misconception Detection with Retrieval-Guided Multi-Stage Reasoning and Ensemble Fusion", "comment": null, "summary": "Detecting student misconceptions in open-ended responses is a longstanding\nchallenge, demanding semantic precision and logical reasoning. We propose\nMiRAGE - Misconception Detection with Retrieval-Guided Multi-Stage Reasoning\nand Ensemble Fusion, a novel framework for automated misconception detection in\nmathematics. MiRAGE operates in three stages: (1) a Retrieval module narrows a\nlarge candidate pool to a semantically relevant subset; (2) a Reasoning module\nemploys chain-of-thought generation to expose logical inconsistencies in\nstudent solutions; and (3) a Reranking module refines predictions by aligning\nthem with the reasoning. These components are unified through an\nensemble-fusion strategy that enhances robustness and interpretability. On\nmathematics datasets, MiRAGE achieves Mean Average Precision scores of\n0.82/0.92/0.93 at levels 1/3/5, consistently outperforming individual modules.\nBy coupling retrieval guidance with multi-stage reasoning, MiRAGE reduces\ndependence on large-scale language models while delivering a scalable and\neffective solution for educational assessment."}
{"id": "2511.01014", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01014", "abs": "https://arxiv.org/abs/2511.01014", "authors": ["Bosi Wen", "Yilin Niu", "Cunxiang Wang", "Pei Ke", "Xiaoying Ling", "Ying Zhang", "Aohan Zeng", "Hongning Wang", "Minlie Huang"], "title": "IF-CRITIC: Towards a Fine-Grained LLM Critic for Instruction-Following Evaluation", "comment": "21 pages, 5 figures", "summary": "Instruction following is a fundamental ability of Large Language Models\n(LLMs), requiring their generated outputs to follow multiple constraints\nimposed in input instructions. Numerous studies have attempted to enhance this\nability through preference optimization or reinforcement learning based on\nreward signals from LLM-as-a-Judge. However, existing evaluation models for\ninstruction following still possess many deficiencies, such as substantial\ncosts and unreliable assessments. To this end, we propose IF-CRITIC, an LLM\ncritic that can provide efficient and reliable assessments of constraint\nfollowing in the instructions. We first develop a checklist generator to\ndecompose instructions and generate constraint checklists. With the assistance\nof the checklists, we collect high-quality critique training data through a\nmulti-stage critique filtering mechanism and employ a constraint-level\npreference optimization method to train IF-CRITIC. Extensive experiments\ndemonstrate that the evaluation performance of IF-CRITIC can beat strong\nLLM-as-a-Judge baselines, including Deepseek-R1 and o4-mini. With the scalable\nreward signals provided by IF-CRITIC, LLMs can achieve substantial performance\ngains in instruction-following optimization under lower computational overhead\ncompared to strong LLM critic baselines."}
{"id": "2511.01183", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01183", "abs": "https://arxiv.org/abs/2511.01183", "authors": ["Hainan Fang", "Yuanbo Wen", "Jun Bi", "Yihan Wang", "Tonghui He", "Yanlin Tang", "Di Huang", "Jiaming Guo", "Rui Zhang", "Qi Guo", "Yunji Chen"], "title": "QiMeng-NeuComBack: Self-Evolving Translation from IR to Assembly Code", "comment": "Accepted at NeurIPS 2025", "summary": "Compilers, while essential, are notoriously complex systems that demand\nprohibitively expensive human expertise to develop and maintain. The recent\nadvancements in Large Language Models (LLMs) offer a compelling new paradigm:\nNeural Compilation, which could potentially simplify compiler development for\nnew architectures and facilitate the discovery of innovative optimization\ntechniques. However, several critical obstacles impede its practical adoption.\nFirstly, a significant lack of dedicated benchmarks and robust evaluation\nmethodologies hinders objective assessment and tracking of progress in the\nfield. Secondly, systematically enhancing the reliability and performance of\nLLM-generated assembly remains a critical challenge. Addressing these\nchallenges, this paper introduces NeuComBack, a novel benchmark dataset\nspecifically designed for IR-to-assembly compilation. Leveraging this dataset,\nwe first define a foundational Neural Compilation workflow and conduct a\ncomprehensive evaluation of the capabilities of recent frontier LLMs on Neural\nCompilation, establishing new performance baselines. We further propose a\nself-evolving prompt optimization method that enables LLMs to iteratively\nevolve their internal prompt strategies by extracting insights from prior\nself-debugging traces, thereby enhancing their neural compilation capabilities.\nExperiments demonstrate that our method significantly improves both the\nfunctional correctness and the performance of LLM-generated assembly code.\nCompared to baseline prompts, the functional correctness rates improved from\n44% to 64% on x86_64 and from 36% to 58% on aarch64, respectively. More\nsignificantly, among the 16 correctly generated x86_64 programs using our\nmethod, 14 (87.5%) surpassed clang-O3 performance."}
{"id": "2511.01016", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01016", "abs": "https://arxiv.org/abs/2511.01016", "authors": ["Wenjin Liu", "Haoran Luo", "Xueyuan Lin", "Haoming Liu", "Tiesunlong Shen", "Jiapu Wang", "Rui Mao", "Erik Cambria"], "title": "Prompt-R1: Collaborative Automatic Prompting Framework via End-to-end Reinforcement Learning", "comment": null, "summary": "Recently, advanced large language models (LLMs) have emerged at an\nincreasingly rapid pace. However, when faced with complex problems, most users\nare often unable to provide accurate and effective prompts to interact with\nLLMs, thus limiting the performance of LLMs. To address this challenge, we\npropose Prompt-R1, an end-to-end reinforcement learning framework that uses a\nsmall-scale LLM to collaborate with large-scale LLMs, replacing user\ninteraction to solve problems better. This collaboration is cast as a\nmulti-turn prompt interaction, where the small-scale LLM thinks and generates\nprompts, and the large-scale LLM performs complex reasoning. A dual-constrained\nreward is designed to optimize for correctness, generation quality, and\nreasoning accuracy. Prompt-R1 provides a plug-and-play framework that supports\nboth inference and training with various large-scale LLMs. Experiments on\nmultiple public datasets show that Prompt-R1 significantly outperforms baseline\nmodels across tasks. Our code is publicly available at\nhttps://github.com/QwenQKing/Prompt-R1."}
{"id": "2511.01258", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01258", "abs": "https://arxiv.org/abs/2511.01258", "authors": ["Chuyue Lou", "M. Amine Atoui"], "title": "Graph Neural Network-Based Semi-Supervised Open-Set Fault Diagnosis for Marine Machinery Systems", "comment": null, "summary": "Recently, fault diagnosis methods for marine machinery systems based on deep\nlearning models have attracted considerable attention in the shipping industry.\nMost existing studies assume fault classes are consistent and known between the\ntraining and test datasets, and these methods perform well under controlled\nenvironment. In practice, however, previously unseen or unknown fault types\n(i.e., out-of-distribution or open-set observations not present during\ntraining) can occur, causing such methods to fail and posing a significant\nchallenge to their widespread industrial deployment. To address this challenge,\nthis paper proposes a semi-supervised open-set fault diagnosis (SOFD) framework\nthat enhances and extends the applicability of deep learning models in open-set\nfault diagnosis scenarios. The framework includes a reliability subset\nconstruction process, which uses a multi-layer fusion feature representation\nextracted by a supervised feature learning model to select an unlabeled test\nsubset. The labeled training set and pseudo-labeled test subset are then fed\ninto a semi-supervised diagnosis model to learn discriminative features for\neach class, enabling accurate classification of known faults and effective\ndetection of unknown samples. Experimental results on a public maritime\nbenchmark dataset demonstrate the effectiveness and superiority of the proposed\nSOFD framework."}
{"id": "2511.01019", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2511.01019", "abs": "https://arxiv.org/abs/2511.01019", "authors": ["Bowen Chen", "Jayesh Gajbhar", "Gregory Dusek", "Rob Redmon", "Patrick Hogan", "Paul Liu", "DelWayne Bohnenstiehl", "Dongkuan", "Xu", "Ruoying He"], "title": "OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights", "comment": "A related presentation will be given at the AGU(American Geophysical\n  Union) and AMS(American Meteorological Society) Annual Meetings", "summary": "Artificial intelligence is transforming the sciences, yet general\nconversational AI systems often generate unverified \"hallucinations\"\nundermining scientific rigor. We present OceanAI, a conversational platform\nthat integrates the natural-language fluency of open-source large language\nmodels (LLMs) with real-time, parameterized access to authoritative\noceanographic data streams hosted by the National Oceanic and Atmospheric\nAdministration (NOAA). Each query such as \"What was Boston Harbor's highest\nwater level in 2024?\" triggers real-time API calls that identify, parse, and\nsynthesize relevant datasets into reproducible natural-language responses and\ndata visualizations. In a blind comparison with three widely used AI\nchat-interface products, only OceanAI produced NOAA-sourced values with\noriginal data references; others either declined to answer or provided\nunsupported results. Designed for extensibility, OceanAI connects to multiple\nNOAA data products and variables, supporting applications in marine hazard\nforecasting, ecosystem assessment, and water-quality monitoring. By grounding\noutputs and verifiable observations, OceanAI advances transparency,\nreproducibility, and trust, offering a scalable framework for AI-enabled\ndecision support within the oceans. A public demonstration is available at\nhttps://oceanai.ai4ocean.xyz."}
{"id": "2511.01311", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01311", "abs": "https://arxiv.org/abs/2511.01311", "authors": ["Filip Naudot", "Tobias Sundqvist", "Timotheus Kampik"], "title": "llmSHAP: A Principled Approach to LLM Explainability", "comment": null, "summary": "Feature attribution methods help make machine learning-based inference\nexplainable by determining how much one or several features have contributed to\na model's output. A particularly popular attribution method is based on the\nShapley value from cooperative game theory, a measure that guarantees the\nsatisfaction of several desirable principles, assuming deterministic inference.\nWe apply the Shapley value to feature attribution in large language model\n(LLM)-based decision support systems, where inference is, by design, stochastic\n(non-deterministic). We then demonstrate when we can and cannot guarantee\nShapley value principle satisfaction across different implementation variants\napplied to LLM-based decision support, and analyze how the stochastic nature of\nLLMs affects these guarantees. We also highlight trade-offs between explainable\ninference speed, agreement with exact Shapley value attributions, and principle\nattainment."}
{"id": "2511.01046", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01046", "abs": "https://arxiv.org/abs/2511.01046", "authors": ["Vedant Acharya", "Abhay Pisharodi", "Rishabh Mondal", "Mohammad Rafiuddin", "Nipun Batra"], "title": "VayuChat: An LLM-Powered Conversational Interface for Air Quality Data Analytics", "comment": "4 Pages, 4 Figures", "summary": "Air pollution causes about 1.6 million premature deaths each year in India,\nyet decision makers struggle to turn dispersed data into decisions. Existing\ntools require expertise and provide static dashboards, leaving key policy\nquestions unresolved. We present VayuChat, a conversational system that answers\nnatural language questions on air quality, meteorology, and policy programs,\nand responds with both executable Python code and interactive visualizations.\nVayuChat integrates data from Central Pollution Control Board (CPCB) monitoring\nstations, state-level demographics, and National Clean Air Programme (NCAP)\nfunding records into a unified interface powered by large language models. Our\nlive demonstration will show how users can perform complex environmental\nanalytics through simple conversations, making data science accessible to\npolicymakers, researchers, and citizens. The platform is publicly deployed at\nhttps://huggingface.co/spaces/SustainabilityLabIITGN/ VayuChat. For further\ninformation check out video uploaded on\nhttps://www.youtube.com/watch?v=d6rklL05cs4."}
{"id": "2511.01320", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01320", "abs": "https://arxiv.org/abs/2511.01320", "authors": ["Ziqi Wang", "Hailiang Zhao", "Yuhao Yang", "Daojiang Hu", "Cheng Bao", "Mingyi Liu", "Kai Di", "Schahram Dustdar", "Zhongjie Wang", "Shuiguang Deng"], "title": "OmniFuser: Adaptive Multimodal Fusion for Service-Oriented Predictive Maintenance", "comment": null, "summary": "Accurate and timely prediction of tool conditions is critical for intelligent\nmanufacturing systems, where unplanned tool failures can lead to quality\ndegradation and production downtime. In modern industrial environments,\npredictive maintenance is increasingly implemented as an intelligent service\nthat integrates sensing, analysis, and decision support across production\nprocesses. To meet the demand for reliable and service-oriented operation, we\npresent OmniFuser, a multimodal learning framework for predictive maintenance\nof milling tools that leverages both visual and sensor data. It performs\nparallel feature extraction from high-resolution tool images and cutting-force\nsignals, capturing complementary spatiotemporal patterns across modalities. To\neffectively integrate heterogeneous features, OmniFuser employs a\ncontamination-free cross-modal fusion mechanism that disentangles shared and\nmodality-specific components, allowing for efficient cross-modal interaction.\nFurthermore, a recursive refinement pathway functions as an anchor mechanism,\nconsistently retaining residual information to stabilize fusion dynamics. The\nlearned representations can be encapsulated as reusable maintenance service\nmodules, supporting both tool-state classification (e.g., Sharp, Used, Dulled)\nand multi-step force signal forecasting. Experiments on real-world milling\ndatasets demonstrate that OmniFuser consistently outperforms state-of-the-art\nbaselines, providing a dependable foundation for building intelligent\nindustrial maintenance services."}
{"id": "2511.01053", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01053", "abs": "https://arxiv.org/abs/2511.01053", "authors": ["Qing Ding", "Eric Hua Qing Zhang", "Felix Jozsa", "Julia Ive"], "title": "Building a Silver-Standard Dataset from NICE Guidelines for Clinical LLMs", "comment": "Submitted to EFMI Medical Informatics Europe 2026", "summary": "Large language models (LLMs) are increasingly used in healthcare, yet\nstandardised benchmarks for evaluating guideline-based clinical reasoning are\nmissing. This study introduces a validated dataset derived from publicly\navailable guidelines across multiple diagnoses. The dataset was created with\nthe help of GPT and contains realistic patient scenarios, as well as clinical\nquestions. We benchmark a range of recent popular LLMs to showcase the validity\nof our dataset. The framework supports systematic evaluation of LLMs' clinical\nutility and guideline adherence."}
{"id": "2511.01329", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01329", "abs": "https://arxiv.org/abs/2511.01329", "authors": ["Ying Song", "Yijing Wang", "Hui Yang", "Weihan Jin", "Jun Xiong", "Congyi Zhou", "Jialin Zhu", "Xiang Gao", "Rong Chen", "HuaGuang Deng", "Ying Dai", "Fei Xiao", "Haihong Tang", "Bo Zheng", "KaiFu Zhang"], "title": "Unbiased Platform-Level Causal Estimation for Search Systems: A Competitive Isolation PSM-DID Framework", "comment": null, "summary": "Evaluating platform-level interventions in search-based two-sided\nmarketplaces is fundamentally challenged by systemic effects such as spillovers\nand network interference. While widely used for causal inference, the PSM\n(Propensity Score Matching) - DID (Difference-in-Differences) framework remains\nsusceptible to selection bias and cross-unit interference from unaccounted\nspillovers. In this paper, we introduced Competitive Isolation PSM-DID, a novel\ncausal framework that integrates propensity score matching with competitive\nisolation to enable platform-level effect measurement (e.g., order volume, GMV)\ninstead of item-level metrics in search systems.\n  Our approach provides theoretically guaranteed unbiased estimation under\nmutual exclusion conditions, with an open dataset released to support\nreproducible research on marketplace interference (github.com/xxxx). Extensive\nexperiments demonstrate significant reductions in interference effects and\nestimation variance compared to baseline methods. Successful deployment in a\nlarge-scale marketplace confirms the framework's practical utility for\nplatform-level causal inference."}
{"id": "2511.01066", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01066", "abs": "https://arxiv.org/abs/2511.01066", "authors": ["Stephan Oepen", "Nikolay Arefev", "Mikko Aulamo", "Marta Ban", "Maja Buljan", "Laurie Burchell", "Lucas Charpentier", "Pinzhen Chen", "Mariya Fedorova", "Ona de Gibert", "Barry Haddow", "Jan Haji", "Jindri Helcl", "Andrey Kutuzov", "Zihao Li", "Risto Luukkonen", "Bhavitvya Malik", "Vladislav Mikhailov", "Amanda Myntti", "Dayyn O'Brien", "Lucie Polkov", "Sampo Pyysalo", "Gema Ramrez Snchez", "Janine Siewert", "Pavel Stepachev", "Jrg Tiedemann", "Teemu Vahtola", "Fedor Vitiugin", "Tea Vojtchov", "Jaume Zaragoza"], "title": "HPLT~3.0: Very Large-Scale Multilingual Resources for LLM and MT. Mono- and Bi-lingual Data, Multilingual Evaluation, and Pre-Trained Models", "comment": null, "summary": "We present an ongoing initiative to provide open, very large, high-quality,\nand richly annotated textual datasets for almost 200 languages. At 30 trillion\ntokens, this is likely the largest generally available multilingual collection\nof LLM pre-training data. At 30 trillion tokens, this is likely the largest\ngenerally available multilingual collection of LLM pre-training data. These\ndatasets are derived from web crawls from different sources and accompanied\nwith a complete, open-source pipeline for document selection from web archives,\ntext extraction from HTML, language identification for noisy texts, exact and\nnear-deduplication, annotation with, among others, register labels, text\nquality estimates, and personally identifiable information; and final selection\nand filtering. We report on data quality probes through contrastive and\nanalytical statistics, through manual inspection of samples for 24 languages,\nand through end-to-end evaluation of various language model architectures\ntrained on this data. For multilingual LLM evaluation, we provide a\ncomprehensive collection of benchmarks for nine European languages, with\nspecial emphasis on natively created tasks, mechanisms to mitigate prompt\nsensitivity, and refined normalization and aggregation of scores. Additionally,\nwe train and evaluate a family of 57 monolingual encoder-decoder models, as\nwell as a handful of monolingual GPT-like reference models. Besides the\nmonolingual data and models, we also present a very large collection of\nparallel texts automatically mined from this data, together with a novel\nparallel corpus synthesized via machine translation."}
{"id": "2511.01363", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01363", "abs": "https://arxiv.org/abs/2511.01363", "authors": ["Giuseppe Riva", "Brenda K. Wiederhold", "Fabrizia Mantovani"], "title": "Automatic Minds: Cognitive Parallels Between Hypnotic States and Large Language Model Processing", "comment": "4 Tables", "summary": "The cognitive processes of the hypnotized mind and the computational\noperations of large language models (LLMs) share deep functional parallels.\nBoth systems generate sophisticated, contextually appropriate behavior through\nautomatic pattern-completion mechanisms operating with limited or unreliable\nexecutive oversight. This review examines this convergence across three\nprinciples: automaticity, in which responses emerge from associative rather\nthan deliberative processes; suppressed monitoring, leading to errors such as\nconfabulation in hypnosis and hallucination in LLMs; and heightened contextual\ndependency, where immediate cues (for example, the suggestion of a therapist or\nthe prompt of the user) override stable knowledge.\n  These mechanisms reveal an observer-relative meaning gap: both systems\nproduce coherent but ungrounded outputs that require an external interpreter to\nsupply meaning. Hypnosis and LLMs also exemplify functional agency - the\ncapacity for complex, goal-directed, context-sensitive behavior - without\nsubjective agency, the conscious awareness of intention and ownership that\ndefines human action. This distinction clarifies how purposive behavior can\nemerge without self-reflective consciousness, governed instead by structural\nand contextual dynamics. Finally, both domains illuminate the phenomenon of\nscheming: automatic, goal-directed pattern generation that unfolds without\nreflective awareness. Hypnosis provides an experimental model for understanding\nhow intention can become dissociated from conscious deliberation, offering\ninsights into the hidden motivational dynamics of artificial systems.\nRecognizing these parallels suggests that the future of reliable AI lies in\nhybrid architectures that integrate generative fluency with mechanisms of\nexecutive monitoring, an approach inspired by the complex, self-regulating\narchitecture of the human mind."}
{"id": "2511.01090", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01090", "abs": "https://arxiv.org/abs/2511.01090", "authors": ["Vlad Negoita", "Mihai Masala", "Traian Rebedea"], "title": "Improving Romanian LLM Pretraining Data using Diversity and Quality Filtering", "comment": null, "summary": "Large Language Models (LLMs) have recently exploded in popularity, often\nmatching or outperforming human abilities on many tasks. One of the key factors\nin training LLMs is the availability and curation of high-quality data. Data\nquality is especially crucial for under-represented languages, where\nhigh-quality corpora are scarce. In this work we study the characteristics and\ncoverage of Romanian pretraining corpora and we examine how they differ from\nEnglish data. By training a lightweight multitask model on carefully\nLLM-annotated Romanian texts, we are able to analyze and perform multi-level\nfiltering (e.g., educational value, topic, format) to generate high-quality\npretraining datasets. Our experiments show noteworthy trends in the topics\npresent in Romanian and English data, while also proving the effectiveness of\nfiltering data through improved LLM pretraining performance across multiple\nbenchmarks."}
{"id": "2511.01375", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01375", "abs": "https://arxiv.org/abs/2511.01375", "authors": ["Hamin Koo", "Minseon Kim", "Jaehyung Kim"], "title": "Align to Misalign: Automatic LLM Jailbreak with Meta-Optimized LLM Judges", "comment": "under review, 28 pages", "summary": "Identifying the vulnerabilities of large language models (LLMs) is crucial\nfor improving their safety by addressing inherent weaknesses. Jailbreaks, in\nwhich adversaries bypass safeguards with crafted input prompts, play a central\nrole in red-teaming by probing LLMs to elicit unintended or unsafe behaviors.\nRecent optimization-based jailbreak approaches iteratively refine attack\nprompts by leveraging LLMs. However, they often rely heavily on either binary\nattack success rate (ASR) signals, which are sparse, or manually crafted\nscoring templates, which introduce human bias and uncertainty in the scoring\noutcomes. To address these limitations, we introduce AMIS (Align to MISalign),\na meta-optimization framework that jointly evolves jailbreak prompts and\nscoring templates through a bi-level structure. In the inner loop, prompts are\nrefined using fine-grained and dense feedback using a fixed scoring template.\nIn the outer loop, the template is optimized using an ASR alignment score,\ngradually evolving to better reflect true attack outcomes across queries. This\nco-optimization process yields progressively stronger jailbreak prompts and\nmore calibrated scoring signals. Evaluations on AdvBench and JBB-Behaviors\ndemonstrate that AMIS achieves state-of-the-art performance, including 88.0%\nASR on Claude-3.5-Haiku and 100.0% ASR on Claude-4-Sonnet, outperforming\nexisting baselines by substantial margins."}
{"id": "2511.01101", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01101", "abs": "https://arxiv.org/abs/2511.01101", "authors": ["Marek Strong", "Andreas Vlachos"], "title": "TSVer: A Benchmark for Fact Verification Against Time-Series Evidence", "comment": "Accepted to EMNLP 2025", "summary": "Reasoning over temporal and numerical data, such as time series, is a crucial\naspect of fact-checking. While many systems have recently been developed to\nhandle this form of evidence, their evaluation remains limited by existing\ndatasets, which often lack structured evidence, provide insufficient\njustifications for verdicts, or rely on synthetic claims. In this paper, we\nintroduce TSVer, a new benchmark dataset for fact verification focusing on\ntemporal and numerical reasoning with time-series evidence. TSVer contains 287\nreal-world claims sourced from 38 fact-checking organizations and a curated\ndatabase of 400 time series covering diverse domains. Each claim is annotated\nwith time frames across all pertinent time series, along with a verdict and\njustifications reflecting how the evidence is used to reach the verdict. Using\nan LLM-assisted multi-step annotation process, we improve the quality of our\nannotations and achieve an inter-annotator agreement of kappa=0.745 on\nverdicts. We also develop a baseline for verifying claims against time-series\nevidence and show that even the state-of-the-art reasoning models like\nGemini-2.5-Pro are challenged by time series, achieving a 63.37 accuracy score\non verdicts and an Ev2R score of 48.63 on verdict justifications."}
{"id": "2511.01396", "categories": ["cs.AI", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.01396", "abs": "https://arxiv.org/abs/2511.01396", "authors": ["Clment Yvernes", "Emilie Devijver", "Adle H. Ribeiro", "Marianne Clausel--Lesourd", "ric Gaussier"], "title": "Relaxing partition admissibility in Cluster-DAGs: a causal calculus with arbitrary variable clustering", "comment": "Accepted at The Thirty-ninth Annual Conference on Neural Information\n  Processing Systems (NeurIPS2025)", "summary": "Cluster DAGs (C-DAGs) provide an abstraction of causal graphs in which nodes\nrepresent clusters of variables, and edges encode both cluster-level causal\nrelationships and dependencies arisen from unobserved confounding. C-DAGs\ndefine an equivalence class of acyclic causal graphs that agree on\ncluster-level relationships, enabling causal reasoning at a higher level of\nabstraction. However, when the chosen clustering induces cycles in the\nresulting C-DAG, the partition is deemed inadmissible under conventional C-DAG\nsemantics. In this work, we extend the C-DAG framework to support arbitrary\nvariable clusterings by relaxing the partition admissibility constraint,\nthereby allowing cyclic C-DAG representations. We extend the notions of\nd-separation and causal calculus to this setting, significantly broadening the\nscope of causal reasoning across clusters and enabling the application of\nC-DAGs in previously intractable scenarios. Our calculus is both sound and\natomically complete with respect to the do-calculus: all valid interventional\nqueries at the cluster level can be derived using our rules, each corresponding\nto a primitive do-calculus step."}
{"id": "2511.01166", "categories": ["cs.CL", "cs.SE", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2511.01166", "abs": "https://arxiv.org/abs/2511.01166", "authors": ["Lingzhe Zhang", "Yunpeng Zhai", "Tong Jia", "Chiming Duan", "Minghua He", "Leyi Pan", "Zhaoyang Liu", "Bolin Ding", "Ying Li"], "title": "MicroRemed: Benchmarking LLMs in Microservices Remediation", "comment": "24 pages, 13 figures, 5 tables", "summary": "Large Language Models (LLMs) integrated with agent-based reasoning frameworks\nhave recently shown strong potential for autonomous decision-making and\nsystem-level operations. One promising yet underexplored direction is\nmicroservice remediation, where the goal is to automatically recover faulty\nmicroservice systems. Existing approaches, however, still rely on human-crafted\nprompts from Site Reliability Engineers (SREs), with LLMs merely converting\ntextual instructions into executable code. To advance research in this area, we\nintroduce MicroRemed, the first benchmark for evaluating LLMs in end-to-end\nmicroservice remediation, where models must directly generate executable\nAnsible playbooks from diagnosis reports to restore system functionality. We\nfurther propose ThinkRemed, a multi-agent framework that emulates the\nreflective and perceptive reasoning of SREs. Experimental results show that\nMicroRemed presents substantial challenges to current LLMs, while ThinkRemed\nimproves end-to-end remediation performance through iterative reasoning and\nsystem reflection. The benchmark is available at\nhttps://github.com/LLM4AIOps/MicroRemed."}
{"id": "2511.01415", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01415", "abs": "https://arxiv.org/abs/2511.01415", "authors": ["Amrapali Pednekar", "lvaro Garrido-Prez", "Yara Khaluf", "Pieter Simoens"], "title": "Modulation of temporal decision-making in a deep reinforcement learning agent under the dual-task paradigm", "comment": "Accepted at CogInterp workshop @ NeurIPS 2025", "summary": "This study explores the interference in temporal processing within a\ndual-task paradigm from an artificial intelligence (AI) perspective. In this\ncontext, the dual-task setup is implemented as a simplified version of the\nOvercooked environment with two variations, single task (T) and dual task\n(T+N). Both variations involve an embedded time production task, but the dual\ntask (T+N) additionally involves a concurrent number comparison task. Two deep\nreinforcement learning (DRL) agents were separately trained for each of these\ntasks. These agents exhibited emergent behavior consistent with human timing\nresearch. Specifically, the dual task (T+N) agent exhibited significant\noverproduction of time relative to its single task (T) counterpart. This result\nwas consistent across four target durations. Preliminary analysis of neural\ndynamics in the agents' LSTM layers did not reveal any clear evidence of a\ndedicated or intrinsic timer. Hence, further investigation is needed to better\nunderstand the underlying time-keeping mechanisms of the agents and to provide\ninsights into the observed behavioral patterns. This study is a small step\ntowards exploring parallels between emergent DRL behavior and behavior observed\nin biological systems in order to facilitate a better understanding of both."}
{"id": "2511.01181", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01181", "abs": "https://arxiv.org/abs/2511.01181", "authors": ["Emaad Manzoor", "Eva Ascarza", "Oded Netzer"], "title": "Learning When to Quit in Sales Conversations", "comment": null, "summary": "Salespeople frequently face the dynamic screening decision of whether to\npersist in a conversation or abandon it to pursue the next lead. Yet, little is\nknown about how these decisions are made, whether they are efficient, or how to\nimprove them. We study these decisions in the context of high-volume outbound\nsales where leads are ample, but time is scarce and failure is common. We\nformalize the dynamic screening decision as an optimal stopping problem and\ndevelop a generative language model-based sequential decision agent - a\nstopping agent - that learns whether and when to quit conversations by\nimitating a retrospectively-inferred optimal stopping policy. Our approach\nhandles high-dimensional textual states, scales to large language models, and\nworks with both open-source and proprietary language models. When applied to\ncalls from a large European telecommunications firm, our stopping agent reduces\nthe time spent on failed calls by 54% while preserving nearly all sales;\nreallocating the time saved increases expected sales by up to 37%. Upon\nexamining the linguistic cues that drive salespeople's quitting decisions, we\nfind that they tend to overweight a few salient expressions of consumer\ndisinterest and mispredict call failure risk, suggesting cognitive bounds on\ntheir ability to make real-time conversational decisions. Our findings\nhighlight the potential of artificial intelligence algorithms to correct\ncognitively-bounded human decisions and improve salesforce efficiency."}
{"id": "2511.01425", "categories": ["cs.AI", "cs.CV", "I.2.6; I.2.10"], "pdf": "https://arxiv.org/pdf/2511.01425", "abs": "https://arxiv.org/abs/2511.01425", "authors": ["Yuhang Huang", "Zekai Lin", "Fan Zhong", "Lei Liu"], "title": "Learning to Seek Evidence: A Verifiable Reasoning Agent with Causal Faithfulness Analysis", "comment": "12 pages, 3 figures. Under review at the Conference on Computer\n  Vision and Pattern Recognition (CVPR) 2026", "summary": "Explanations for AI models in high-stakes domains like medicine often lack\nverifiability, which can hinder trust. To address this, we propose an\ninteractive agent that produces explanations through an auditable sequence of\nactions. The agent learns a policy to strategically seek external visual\nevidence to support its diagnostic reasoning. This policy is optimized using\nreinforcement learning, resulting in a model that is both efficient and\ngeneralizable. Our experiments show that this action-based reasoning process\nsignificantly improves calibrated accuracy, reducing the Brier score by 18\\%\ncompared to a non-interactive baseline. To validate the faithfulness of the\nagent's explanations, we introduce a causal intervention method. By masking the\nvisual evidence the agent chooses to use, we observe a measurable degradation\nin its performance ($\\Delta$Brier=+0.029), confirming that the evidence is\nintegral to its decision-making process. Our work provides a practical\nframework for building AI systems with verifiable and faithful reasoning\ncapabilities."}
{"id": "2511.01187", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.01187", "abs": "https://arxiv.org/abs/2511.01187", "authors": ["Muhammed Saeed", "Muhammad Abdul-mageed", "Shady Shehata"], "title": "Surfacing Subtle Stereotypes: A Multilingual, Debate-Oriented Evaluation of Modern LLMs", "comment": null, "summary": "Large language models (LLMs) are widely deployed for open-ended\ncommunication, yet most bias evaluations still rely on English,\nclassification-style tasks. We introduce DebateBias-8K, a new multilingual,\ndebate-style benchmark designed to reveal how narrative bias appears in\nrealistic generative settings. Our dataset includes 8,400 structured debate\nprompts spanning four sensitive domains: women's rights, socioeconomic\ndevelopment, terrorism, and religion, across seven languages ranging from\nhigh-resource (English, Chinese) to low-resource (Swahili, Nigerian Pidgin).\nUsing four flagship models (GPT-4o, Claude 3, DeepSeek, and LLaMA 3), we\ngenerate and automatically classify over 100,000 responses. Results show that\nall models reproduce entrenched stereotypes despite safety alignment: Arabs are\noverwhelmingly linked to terrorism and religion (>=95%), Africans to\nsocioeconomic \"backwardness\" (up to <=77%), and Western groups are consistently\nframed as modern or progressive. Biases grow sharply in lower-resource\nlanguages, revealing that alignment trained primarily in English does not\ngeneralize globally. Our findings highlight a persistent divide in multilingual\nfairness: current alignment methods reduce explicit toxicity but fail to\nprevent biased outputs in open-ended contexts. We release our DebateBias-8K\nbenchmark and analysis framework to support the next generation of multilingual\nbias evaluation and safer, culturally inclusive model alignment."}
{"id": "2511.01444", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01444", "abs": "https://arxiv.org/abs/2511.01444", "authors": ["Huiting Huang", "Tieliang Gong", "Kai He", "Jialun Wu", "Erik Cambria", "Mengling Feng"], "title": "Robust Multimodal Sentiment Analysis via Double Information Bottleneck", "comment": null, "summary": "Multimodal sentiment analysis has received significant attention across\ndiverse research domains. Despite advancements in algorithm design, existing\napproaches suffer from two critical limitations: insufficient learning of\nnoise-contaminated unimodal data, leading to corrupted cross-modal\ninteractions, and inadequate fusion of multimodal representations, resulting in\ndiscarding discriminative unimodal information while retaining multimodal\nredundant information. To address these challenges, this paper proposes a\nDouble Information Bottleneck (DIB) strategy to obtain a powerful, unified\ncompact multimodal representation. Implemented within the framework of low-rank\nRenyi's entropy functional, DIB offers enhanced robustness against diverse\nnoise sources and computational tractability for high-dimensional data, as\ncompared to the conventional Shannon entropy-based methods. The DIB comprises\ntwo key modules: 1) learning a sufficient and compressed representation of\nindividual unimodal data by maximizing the task-relevant information and\ndiscarding the superfluous information, and 2) ensuring the discriminative\nability of multimodal representation through a novel attention bottleneck\nfusion mechanism. Consequently, DIB yields a multimodal representation that\neffectively filters out noisy information from unimodal data while capturing\ninter-modal complementarity. Extensive experiments on CMU-MOSI, CMU-MOSEI,\nCH-SIMS, and MVSA-Single validate the effectiveness of our method. The model\nachieves 47.4% accuracy under the Acc-7 metric on CMU-MOSI and 81.63% F1-score\non CH-SIMS, outperforming the second-best baseline by 1.19%. Under noise, it\nshows only 0.36% and 0.29% performance degradation on CMU-MOSI and CMU-MOSEI\nrespectively."}
{"id": "2511.01188", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01188", "abs": "https://arxiv.org/abs/2511.01188", "authors": ["Lvhua Wu", "Xuefeng Jiang", "Sheng Sun", "Tian Wen", "Yuwei Wang", "Min Liu"], "title": "ZoFia: Zero-Shot Fake News Detection with Entity-Guided Retrieval and Multi-LLM Interaction", "comment": null, "summary": "The rapid spread of fake news threatens social stability and public trust,\nrendering its detection an imperative research priority. Although large\nlanguage models (LLMs) excel at numerous natural language processing tasks with\ntheir remarkable contextual understanding and extensive prior knowledge, the\ntime-bounded knowledge coverage and tendency for generating hallucination\ncontent reduce their reliability when handling fast-evolving news streams.\nFurthermore, models trained on existing static datasets also often lack the\ngeneralization needed for emerging news topics. To address these challenges, we\npropose ZoFia, a novel two-stage zero-shot fake news detection framework.\nFirst, we introduce Hierarchical Salience to quantify the importance of\nentities in the news content, and propose the SC-MMR algorithm to effectively\nselect an informative and diverse set of keywords that serve as queries for\nretrieving up-to-date external evidence. Subsequently, a multi LLM interactive\nsystem, in which each agent assumes a distinct role, performs multi-view\ncollaborative analysis and adversarial debate over the news text and its\nrelated information, and finally produces an interpretable and robust judgment.\nComprehensive experiments on two public datasets demonstrate that ZoFia\nobviously outperforms existing zero-shot baselines and most of few-shot\nmethods. Our codes will be open-sourced to facilitate related communities."}
{"id": "2511.01445", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01445", "abs": "https://arxiv.org/abs/2511.01445", "authors": ["ChengZhang Yu", "YingRu He", "Hongyan Cheng", "nuo Cheng", "Zhixing Liu", "Dongxu Mu", "Zhangrui Shen", "Zhanpeng Jin"], "title": "From Passive to Proactive: A Multi-Agent System with Dynamic Task Orchestration for Intelligent Medical Pre-Consultation", "comment": "14pages, 7 figures, 7 tables", "summary": "Global healthcare systems face critical challenges from increasing patient\nvolumes and limited consultation times, with primary care visits averaging\nunder 5 minutes in many countries. While pre-consultation processes\nencompassing triage and structured history-taking offer potential solutions,\nthey remain limited by passive interaction paradigms and context management\nchallenges in existing AI systems. This study introduces a hierarchical\nmulti-agent framework that transforms passive medical AI systems into proactive\ninquiry agents through autonomous task orchestration. We developed an\neight-agent architecture with centralized control mechanisms that decomposes\npre-consultation into four primary tasks: Triage ($T_1$), History of Present\nIllness collection ($T_2$), Past History collection ($T_3$), and Chief\nComplaint generation ($T_4$), with $T_1$--$T_3$ further divided into 13\ndomain-specific subtasks. Evaluated on 1,372 validated electronic health\nrecords from a Chinese medical platform across multiple foundation models\n(GPT-OSS 20B, Qwen3-8B, Phi4-14B), the framework achieved 87.0% accuracy for\nprimary department triage and 80.5% for secondary department classification,\nwith task completion rates reaching 98.2% using agent-driven scheduling versus\n93.1% with sequential processing. Clinical quality scores from 18 physicians\naveraged 4.56 for Chief Complaints, 4.48 for History of Present Illness, and\n4.69 for Past History on a 5-point scale, with consultations completed within\n12.7 rounds for $T_2$ and 16.9 rounds for $T_3$. The model-agnostic\narchitecture maintained high performance across different foundation models\nwhile preserving data privacy through local deployment, demonstrating the\npotential for autonomous AI systems to enhance pre-consultation efficiency and\nquality in clinical settings."}
{"id": "2511.01191", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01191", "abs": "https://arxiv.org/abs/2511.01191", "authors": ["Ru Wang", "Wei Huang", "Qi Cao", "Yusuke Iwasawa", "Yutaka Matsuo", "Jiaxian Guo"], "title": "Self-Harmony: Learning to Harmonize Self-Supervision and Self-Play in Test-Time Reinforcement Learning", "comment": null, "summary": "Test-time reinforcement learning (TTRL) offers a label-free paradigm for\nadapting models using only synthetic signals at inference, but its success\nhinges on constructing reliable learning signals. Standard approaches such as\nmajority voting often collapse to spurious yet popular answers. We introduce\nSelf-Harmony, a framework built on a simple intuition: the correct answer\nshould remain stable across both an original question and its paraphrase.\nSelf-Harmony operationalizes this by employing a single model in two\ncomplementary roles: a Solver to produce answers and a Reframer to rephrase the\ninput. Based on this, we further propose a pseudo-label method: instead of\nmajority voting, it aggregates answer frequencies across these original and\nreframed views using the harmonic mean. This is a process that naturally\nselects for solutions stable under reframing, thereby avoiding the common trap\nof favoring view-dependent, spurious answers. Crucially, this requires no human\nsupervision or auxiliary models. Across diverse reasoning benchmarks,\nSelf-Harmony achieves state-of-the-art results at the label-free test-time\nsetting, ranking first in 28 of 30 settings across multiple methods. Beyond\naccuracy, it demonstrates unprecedented robustness, with zero training failures\nin all experiments, underscoring its stability and reliability."}
{"id": "2511.01527", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01527", "abs": "https://arxiv.org/abs/2511.01527", "authors": ["Hanwen Xu", "Xuyao Huang", "Yuzhe Liu", "Kai Yu", "Zhijie Deng"], "title": "TPS-Bench: Evaluating AI Agents' Tool Planning \\& Scheduling Abilities in Compounding Tasks", "comment": null, "summary": "Large language model (LLM) agents have exhibited strong problem-solving\ncompetence across domains like research and coding. Yet, it remains\nunderexplored whether LLM agents can tackle compounding real-world problems\nthat require a diverse set of tools to complete. Given a broad, heterogeneous\ntool repository, LLM agents must not only select appropriate tools based on\ntask planning analysis but also strategically schedule the execution order to\nensure efficiency. This paper introduces TPS-Bench to benchmark the ability of\nLLM agents in solving such problems that demand Tool Planning and Scheduling.\nTPS-Bench collects 200 compounding tasks of two difficulty levels, based on a\ntool repository containing hundreds of model context protocol (MCP) tools. In\nparticular, each task is composed of multiple subtasks, such as web search, map\nnavigation, calendar checking, etc., and each subtask can be completed by a\nbasic tool. Our evaluation emphasizes both task completion rate and efficiency.\nThe empirical studies on popular closed-source and open-source LLMs indicate\nthat most models can perform reasonable tool planning, but differ in\nscheduling. For example, GLM-4.5 achieves an outperforming task completion rate\nof 64.72% with extensive sequential tool calls, hence suffering from\nsignificantly long execution time. By contrast, GPT-4o prioritizes parallel\ntool calls but achieves only a 45.08% completion rate. Considering\nreinforcement learning (RL) can be a viable way to improve the scheduling\nefficiency without compromising performance, we perform an initial study on\nQwen3-1.7B and witness a 14% reduction in execution time alongside a 6% gain in\ntask completion rate based on rarely 100 RL training samples. Our code is\navailable https://github.com/hanwenxu1/mcp-agent."}
{"id": "2511.01192", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01192", "abs": "https://arxiv.org/abs/2511.01192", "authors": ["Guoxin Ma", "Xiaoming Liu", "Zhanhan Zhang", "Chengzhengxu Li", "Shengchao Liu", "Yu Lan"], "title": "DEER: Disentangled Mixture of Experts with Instance-Adaptive Routing for Generalizable Machine-Generated Text Detection", "comment": "Under Review", "summary": "Detecting machine-generated text (MGT) has emerged as a critical challenge,\ndriven by the rapid advancement of large language models (LLMs) capable of\nproducing highly realistic, human-like content. However, the performance of\ncurrent approaches often degrades significantly under domain shift. To address\nthis challenge, we propose a novel framework designed to capture both\ndomain-specific and domain-general MGT patterns through a two-stage\nDisentangled mixturE-of-ExpeRts (DEER) architecture. First, we introduce a\ndisentangled mixture-of-experts module, in which domain-specific experts learn\nfine-grained, domain-local distinctions between human and machine-generated\ntext, while shared experts extract transferable, cross-domain features. Second,\nto mitigate the practical limitation of unavailable domain labels during\ninference, we design a reinforcement learning-based routing mechanism that\ndynamically selects the appropriate experts for each input instance,\neffectively bridging the train-inference gap caused by domain uncertainty.\nExtensive experiments on five in-domain and five out-of-domain benchmark\ndatasets demonstrate that DEER consistently outperforms state-of-the-art\nmethods, achieving average F1-score improvements of 1.39% and 5.32% on\nin-domain and out-of-domain datasets respectively, along with accuracy gains of\n1.35% and 3.61% respectively. Ablation studies confirm the critical\ncontributions of both disentangled expert specialization and adaptive routing\nto model performance."}
{"id": "2511.01550", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01550", "abs": "https://arxiv.org/abs/2511.01550", "authors": ["Ujjwal Sharma", "Stevan Rudinac", "Ana Mikovi", "Willemijn van Dolen", "Marcel Worring"], "title": "Analyzing Sustainability Messaging in Large-Scale Corporate Social Media", "comment": null, "summary": "In this work, we introduce a multimodal analysis pipeline that leverages\nlarge foundation models in vision and language to analyze corporate social\nmedia content, with a focus on sustainability-related communication. Addressing\nthe challenges of evolving, multimodal, and often ambiguous corporate messaging\non platforms such as X (formerly Twitter), we employ an ensemble of large\nlanguage models (LLMs) to annotate a large corpus of corporate tweets on their\ntopical alignment with the 17 Sustainable Development Goals (SDGs). This\napproach avoids the need for costly, task-specific annotations and explores the\npotential of such models as ad-hoc annotators for social media data that can\nefficiently capture both explicit and implicit references to sustainability\nthemes in a scalable manner. Complementing this textual analysis, we utilize\nvision-language models (VLMs), within a visual understanding framework that\nuses semantic clusters to uncover patterns in visual sustainability\ncommunication. This integrated approach reveals sectoral differences in SDG\nengagement, temporal trends, and associations between corporate messaging,\nenvironmental, social, governance (ESG) risks, and consumer engagement. Our\nmethods-automatic label generation and semantic visual clustering-are broadly\napplicable to other domains and offer a flexible framework for large-scale\nsocial media analysis."}
{"id": "2511.01265", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01265", "abs": "https://arxiv.org/abs/2511.01265", "authors": ["Mo El-Haj", "Paul Rayson"], "title": "AraFinNews: Arabic Financial Summarisation with Domain-Adapted LLMs", "comment": "10 pages", "summary": "This paper investigates the impact of domain specificity on abstractive\nsummarisation of Arabic financial texts using large language models (LLMs). We\nintroduce AraFinNews, the largest publicly available Arabic financial news\ndataset to date, comprising 212,500 article--headline pairs spanning nearly a\ndecade of reporting from October 2015 to July 2025. Designed as the Arabic\nequivalent of major English summarisation corpora such as CNN/DailyMail,\nAraFinNews provides a robust benchmark for evaluating domain-specific language\nunderstanding and generation in financial contexts. Using this resource, we\nevaluate transformer-based models -- including mT5, AraT5, and the\ndomain-adapted FinAraT5 -- to examine how financial-domain pretraining\ninfluences factual accuracy, numerical reliability, and stylistic alignment\nwith professional reporting. Experimental results show that domain-adapted\nmodels generate more faithful and coherent summaries, particularly in handling\nquantitative and entity-centric information. The findings highlight the\nimportance of domain-specific adaptation for improving factual consistency and\nnarrative fluency in Arabic financial summarisation. The dataset is freely\navailable for non-commercial research at\nhttps://github.com/ArabicNLP-UK/AraFinNews."}
{"id": "2511.01581", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01581", "abs": "https://arxiv.org/abs/2511.01581", "authors": ["Chengzhang Yu", "Zening Lu", "Chenyang Zheng", "Chiyue Wang", "Yiming Zhang", "Zhanpeng Jin"], "title": "ExplicitLM: Decoupling Knowledge from Parameters via Explicit Memory Banks", "comment": "12pages, 4figures", "summary": "Large language models suffer from knowledge staleness and lack of\ninterpretability due to implicit knowledge storage across entangled network\nparameters, preventing targeted updates and reasoning transparency. We propose\nExplicitLM, a novel architecture featuring a million-scale external memory bank\nstoring human-readable knowledge as token sequences, enabling direct inspection\nand modification. We design a differentiable two-stage retrieval mechanism with\nefficient coarse-grained filtering via product key decomposition (reducing\ncomplexity from $\\mathcal{O}(N \\cdot |I|)$ to $\\mathcal{O}(\\sqrt{N} \\cdot\n|I|)$) and fine-grained Gumbel-Softmax matching for end-to-end training.\nInspired by dual-system cognitive theory, we partition knowledge into frozen\nexplicit facts (20%) and learnable implicit patterns (80%), maintained through\nExponential Moving Average updates for stability. ExplicitLM achieves up to\n43.67% improvement on knowledge-intensive tasks versus standard Transformers,\nwith 3.62$\\times$ gains in low-data regimes (10k samples). Analysis shows\nstrong correlations between memory retrieval and performance, with correct\npredictions achieving 49% higher hit rates. Unlike RAG systems with frozen\nretrieval, our jointly optimized architecture demonstrates that interpretable,\nupdatable models can maintain competitive performance while providing\nunprecedented knowledge transparency."}
{"id": "2511.01282", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01282", "abs": "https://arxiv.org/abs/2511.01282", "authors": ["Min Fang", "Zhihui Fu", "Qibin Zhao", "Jun Wang"], "title": "When, What, and How: Rethinking Retrieval-Enhanced Speculative Decoding", "comment": null, "summary": "Speculative decoding (SD) has emerged as an effective technique to accelerate\nlarge language model (LLM) inference without compromising output quality.\nHowever, the achievable speedup largely depends on the effectiveness of the\ndrafting model. While model-based methods like EAGLE-2 are accurate but costly,\nretrieval-enhanced methods like SAM-Decoding rely on heuristic switching\nstrategies that often trigger unnecessary retrievals. To address this, we\npropose ReSpec (\\textbf{Re}trieval-enhanced \\textbf{Spe}culative Decoding), a\nnovel framework that transforms heuristic drafter switching into adaptive\ndecision-making. ReSpec features three core innovations: 1) An\n\\textbf{entropy-guided adaptive trigger} quantifies contextual predictability\nto initiate retrieval only when uncertainty is low, avoiding costly low-quality\nspeculations. 2) A \\textbf{feedback-driven candidate selection} leverages\nhistorical feedback to organize multiple high-quality candidates for parallel\nverification, maximizing retrieval utility. 3) A source-aware \\textbf{relaxed\nverification strategy} applies strict checks to model-generated drafts while\nusing a relaxed verification for retrieved drafts, achieving a better balance\nbetween accuracy and efficiency. Extensive experiments on Spec-Bench\ndemonstrate that ReSpec achieves state-of-the-art acceleration,outperforming\nEAGLE-2 and SAM-Decoding by over $33\\%$ and $25\\%$, respectively, while\nmaintaining output quality."}
{"id": "2511.01639", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01639", "abs": "https://arxiv.org/abs/2511.01639", "authors": ["Sicheng Wang", "Shuhao Chen", "Jingran Zhou", "Chengyi Tu"], "title": "IVGAE-TAMA-BO: A novel temporal dynamic variational graph model for link prediction in global food trade networks with momentum structural memory and Bayesian optimization", "comment": "26pages,6figures", "summary": "Global food trade plays a crucial role in ensuring food security and\nmaintaining supply chain stability. However, its network structure evolves\ndynamically under the influence of geopolitical, economic, and environmental\nfactors, making it challenging to model and predict future trade links.\nEffectively capturing temporal patterns in food trade networks is therefore\nessential for improving the accuracy and robustness of link prediction. This\nstudy introduces IVGAE-TAMA-BO, a novel dynamic graph neural network designed\nto model evolving trade structures and predict future links in global food\ntrade networks. To the best of our knowledge, this is the first work to apply\ndynamic graph neural networks to this domain, significantly enhancing\npredictive performance. Building upon the original IVGAE framework, the\nproposed model incorporates a Trade-Aware Momentum Aggregator (TAMA) to capture\nthe temporal evolution of trade networks, jointly modeling short-term\nfluctuations and long-term structural dependencies. A momentum-based structural\nmemory mechanism further improves predictive stability and performance. In\naddition, Bayesian optimization is used to automatically tune key\nhyperparameters, enhancing generalization across diverse trade scenarios.\nExtensive experiments on five crop-specific datasets demonstrate that\nIVGAE-TAMA substantially outperforms the static IVGAE and other dynamic\nbaselines by effectively modeling temporal dependencies, while Bayesian\noptimization further boosts performance in IVGAE-TAMA-BO. These results\nhighlight the proposed framework as a robust and scalable solution for\nstructural prediction in global trade networks, with strong potential for\napplications in food security monitoring and policy decision support."}
{"id": "2511.01287", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.01287", "abs": "https://arxiv.org/abs/2511.01287", "authors": ["Qin Zhou", "Zhexin Zhang", "Zhi Li", "Limin Sun"], "title": "\"Give a Positive Review Only\": An Early Investigation Into In-Paper Prompt Injection Attacks and Defenses for AI Reviewers", "comment": null, "summary": "With the rapid advancement of AI models, their deployment across diverse\ntasks has become increasingly widespread. A notable emerging application is\nleveraging AI models to assist in reviewing scientific papers. However, recent\nreports have revealed that some papers contain hidden, injected prompts\ndesigned to manipulate AI reviewers into providing overly favorable\nevaluations. In this work, we present an early systematic investigation into\nthis emerging threat. We propose two classes of attacks: (1) static attack,\nwhich employs a fixed injection prompt, and (2) iterative attack, which\noptimizes the injection prompt against a simulated reviewer model to maximize\nits effectiveness. Both attacks achieve striking performance, frequently\ninducing full evaluation scores when targeting frontier AI reviewers.\nFurthermore, we show that these attacks are robust across various settings. To\ncounter this threat, we explore a simple detection-based defense. While it\nsubstantially reduces the attack success rate, we demonstrate that an adaptive\nattacker can partially circumvent this defense. Our findings underscore the\nneed for greater attention and rigorous safeguards against prompt-injection\nthreats in AI-assisted peer review."}
{"id": "2511.01668", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01668", "abs": "https://arxiv.org/abs/2511.01668", "authors": ["Yueqing Xi", "Yifan Bai", "Huasen Luo", "Weiliang Wen", "Hui Liu", "Haoliang Li"], "title": "Hybrid Retrieval-Augmented Generation Agent for Trustworthy Legal Question Answering in Judicial Forensics", "comment": null, "summary": "As artificial intelligence permeates judicial forensics, ensuring the\nveracity and traceability of legal question answering (QA) has become critical.\nConventional large language models (LLMs) are prone to hallucination, risking\nmisleading guidance in legal consultation, while static knowledge bases\nstruggle to keep pace with frequently updated statutes and case law. We present\na hybrid legal QA agent tailored for judicial settings that integrates\nretrieval-augmented generation (RAG) with multi-model ensembling to deliver\nreliable, auditable, and continuously updatable counsel. The system prioritizes\nretrieval over generation: when a trusted legal repository yields relevant\nevidence, answers are produced via RAG; otherwise, multiple LLMs generate\ncandidates that are scored by a specialized selector, with the top-ranked\nanswer returned. High-quality outputs then undergo human review before being\nwritten back to the repository, enabling dynamic knowledge evolution and\nprovenance tracking. Experiments on the Law\\_QA dataset show that our hybrid\napproach significantly outperforms both a single-model baseline and a vanilla\nRAG pipeline on F1, ROUGE-L, and an LLM-as-a-Judge metric. Ablations confirm\nthe complementary contributions of retrieval prioritization, model ensembling,\nand the human-in-the-loop update mechanism. The proposed system demonstrably\nreduces hallucination while improving answer quality and legal compliance,\nadvancing the practical landing of media forensics technologies in judicial\nscenarios."}
{"id": "2511.01289", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01289", "abs": "https://arxiv.org/abs/2511.01289", "authors": ["Saiyma Sittul Muna", "Rezwan Islam Salvi", "Mushfiqur Rahman Mushfique", "Ajwad Abrar"], "title": "FirstAidQA: A Synthetic Dataset for First Aid and Emergency Response in Low-Connectivity Settings", "comment": "Accepted at the 5th Muslims in Machine Learning (MusIML) Workshop,\n  co-located with NeurIPS 2025", "summary": "In emergency situations, every second counts. The deployment of Large\nLanguage Models (LLMs) in time-sensitive, low or zero-connectivity environments\nremains limited. Current models are computationally intensive and unsuitable\nfor low-tier devices often used by first responders or civilians. A major\nbarrier to developing lightweight, domain-specific solutions is the lack of\nhigh-quality datasets tailored to first aid and emergency response. To address\nthis gap, we introduce FirstAidQA, a synthetic dataset containing 5,500\nhigh-quality question answer pairs that encompass a wide range of first aid and\nemergency response scenarios. The dataset was generated using a Large Language\nModel, ChatGPT-4o-mini, with prompt-based in-context learning, using texts from\nthe Vital First Aid Book (2019). We applied preprocessing steps such as text\ncleaning, contextual chunking, and filtering, followed by human validation to\nensure accuracy, safety, and practical relevance of the QA pairs. FirstAidQA is\ndesigned to support instruction-tuning and fine-tuning of LLMs and Small\nLanguage Models (SLMs), enabling faster, more reliable, and offline-capable\nsystems for emergency settings. We publicly release the dataset to advance\nresearch on safety-critical and resource-constrained AI applications in first\naid and emergency response. The dataset is available on Hugging Face at\nhttps://huggingface.co/datasets/i-am-mushfiq/FirstAidQA."}
{"id": "2511.01824", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01824", "abs": "https://arxiv.org/abs/2511.01824", "authors": ["Yuetai Li", "Huseyin A Inan", "Xiang Yue", "Wei-Ning Chen", "Lukas Wutschitz", "Janardhan Kulkarni", "Radha Poovendran", "Robert Sim", "Saravan Rajmohan"], "title": "Simulating Environments with Reasoning Models for Agent Training", "comment": null, "summary": "LLM agents excel in compact environments requiring deep reasoning but remain\nbrittle when operating in broader, more complex contexts that demand robustness\nacross diverse tools and schemas. Building bespoke environments for training is\nheavy, brittle, and limits progress. In this paper, we demonstrate that LLMs\ncan simulate realistic environment feedback without access to actual testbed\ndata or APIs. Inspired by this capability, we propose two frameworks:\nSimia-SFT, a pipeline that synthesizes SFT data by amplifying small seed sets\ninto diverse trajectories in an environment-agnostic manner, and Simia-RL, a\nframework that enables RL training without real environment implementations\nthrough LLM-simulated feedback. Fine-tuning open models yields consistent\nimprovements across multiple benchmarks, surpassing GPT-4o and approaching\no4-mini on $\\tau^2$-Bench. Together, Simia-SFT and Simia-RL enable scalable\nagent training without environment engineering, replacing heavy and brittle\nimplementations with flexible LLM-based simulation."}
{"id": "2511.01305", "categories": ["cs.CL", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.01305", "abs": "https://arxiv.org/abs/2511.01305", "authors": ["Aman Ganapathy Manvattira", "Yifei Xu", "Ziyue Dang", "Songwu Lu"], "title": "DeepSpecs: Expert-Level Questions Answering in 5G", "comment": null, "summary": "5G technology enables mobile Internet access for billions of users. Answering\nexpert-level questions about 5G specifications requires navigating thousands of\npages of cross-referenced standards that evolve across releases. Existing\nretrieval-augmented generation (RAG) frameworks, including telecom-specific\napproaches, rely on semantic similarity and cannot reliably resolve\ncross-references or reason about specification evolution. We present DeepSpecs,\na RAG system enhanced by structural and temporal reasoning via three\nmetadata-rich databases: SpecDB (clause-aligned specification text), ChangeDB\n(line-level version diffs), and TDocDB (standardization meeting documents).\nDeepSpecs explicitly resolves cross-references by recursively retrieving\nreferenced clauses through metadata lookup, and traces specification evolution\nby mining changes and linking them to Change Requests that document design\nrationale. We curate two 5G QA datasets: 573 expert-annotated real-world\nquestions from practitioner forums and educational resources, and 350\nevolution-focused questions derived from approved Change Requests. Across\nmultiple LLM backends, DeepSpecs outperforms base models and state-of-the-art\ntelecom RAG systems; ablations confirm that explicit cross-reference resolution\nand evolution-aware retrieval substantially improve answer quality,\nunderscoring the value of modeling the structural and temporal properties of 5G\nstandards."}
{"id": "2511.00115", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00115", "abs": "https://arxiv.org/abs/2511.00115", "authors": ["Haoyuan Li", "Yuanbo Tong", "Yuchen Li", "Zirui Wang", "Chunhou Liu", "Jiamou Liu"], "title": "Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference", "comment": null, "summary": "Personality recognition from text is typically cast as hard-label\nclassification, which obscures the graded, prototype-like nature of human\npersonality judgments. We present ProtoMBTI, a cognitively aligned framework\nfor MBTI inference that operationalizes prototype theory within an LLM-based\npipeline. First, we construct a balanced, quality-controlled corpus via\nLLM-guided multi-dimensional augmentation (semantic, linguistic, sentiment).\nNext, we LoRA-fine-tune a lightweight (<=2B) encoder to learn discriminative\nembeddings and to standardize a bank of personality prototypes. At inference,\nwe retrieve top-k prototypes for a query post and perform a\nretrieve--reuse--revise--retain cycle: the model aggregates prototype evidence\nvia prompt-based voting, revises when inconsistencies arise, and, upon correct\nprediction, retains the sample to continually enrich the prototype library.\nAcross Kaggle and Pandora benchmarks, ProtoMBTI improves over baselines on both\nthe four MBTI dichotomies and the full 16-type task, and exhibits robust\ncross-dataset generalization. Our results indicate that aligning the inference\nprocess with psychological prototype reasoning yields gains in accuracy,\ninterpretability, and transfer for text-based personality modeling."}
{"id": "2511.01323", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01323", "abs": "https://arxiv.org/abs/2511.01323", "authors": ["Jiabao Ji", "Min Li", "Priyanshu Kumar", "Shiyu Chang", "Saloni Potdar"], "title": "DEEPAMBIGQA: Ambiguous Multi-hop Questions for Benchmarking LLM Answer Completeness", "comment": "25 pages", "summary": "Large language models (LLMs) with integrated search tools show strong promise\nin open-domain question answering (QA), yet they often struggle to produce\ncomplete answer set to complex questions such as Which actor from the film Heat\nwon at least one Academy Award?, which requires (1) distinguishing between\nmultiple films sharing the same title and (2) reasoning across a large set of\nactors to gather and integrate evidence. Existing QA benchmarks rarely evaluate\nboth challenges jointly. To address this, we introduce DeepAmbigQAGen, an\nautomatic data generation pipeline that constructs QA tasks grounded in text\ncorpora and linked knowledge graph, generating natural and verifiable questions\nthat systematically embed name ambiguity and multi-step reasoning. Based on\nthis, we build DeepAmbigQA, a dataset of 3,600 questions requiring multi-hop\nreasoning and half of them explicit name ambiguity resolving. Experiments\nreveal that, even state-of-the-art GPT-5 show incomplete answers, achieving\nonly 0.13 exact match on ambiguous questions and 0.21 on non-ambiguous\nquestions. These findings highlight the need for more robust QA systems aimed\nat information gathering and answer completeness."}
{"id": "2511.00198", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00198", "abs": "https://arxiv.org/abs/2511.00198", "authors": ["Chun-Hao Yang", "Bo-Han Feng", "Tzu-Yuan Lai", "Yan Yu Chen", "Yin-Kai Dean Huang", "Shou-De Lin"], "title": "Training LLMs Beyond Next Token Prediction - Filling the Mutual Information Gap", "comment": null, "summary": "Optimizing training performance in large language models (LLMs) remains an\nessential challenge, particularly in improving model performance while\nmaintaining computational costs. This work challenges the conventional approach\nof training LLMs using next-token prediction (NTP), arguing that by predicting\ninformation-rich tokens during training, there is a more effective way to train\nLLMs. We investigate the impact of the proposed solution in three kinds of\ntasks for LLMs: arithmetic, multi-label classification of text, and\nnatural-language generation. This work offers a principled approach to\noptimizing LLM training, advancing both model performance and theoretical\nunderstanding of the target-token selection strategies."}
{"id": "2511.01354", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01354", "abs": "https://arxiv.org/abs/2511.01354", "authors": ["Wenrui Cai", "Chengyu Wang", "Junbing Yan", "Jun Huang", "Xiangzhong Fang"], "title": "Thinking with DistilQwen: A Tale of Four Distilled Reasoning and Reward Model Series", "comment": "emnlp 2025 industry track", "summary": "Recently, the demand for small and efficient reasoning models to support\nreal-world applications has driven the development of knowledge distillation\ntechniques that balance reasoning performance and inference speed. In this\npaper, we further extend the DistilQwen model family, initialized from the Qwen\nmodels, by introducing four model series specifically designed to meet\nindustrial requirements. The distilled model collection comprises: (1)\nslow-thinking models, optimized for reasoning tasks that require high accuracy;\n(2) two series of adaptive-thinking models, which dynamically adjust reasoning\nstrategies based on input tasks to maximize efficiency across diverse\nscenarios; and (3) distilled reward models, which enable further reinforcement\nlearning of reasoning models using distilled knowledge. Comprehensive\nevaluations across multiple benchmarks demonstrate both high inference\nefficiency and strong reasoning performance for these models, as well as the\npractical utility of distilled reward models. We further show that these models\nsupport industry practitioners by providing scalable training and inference\nfunctionalities on the Alibaba Cloud PAI (Platform for Artificial Intelligence)\nplatform."}
{"id": "2511.00222", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00222", "abs": "https://arxiv.org/abs/2511.00222", "authors": ["Marwa Abdulhai", "Ryan Cheng", "Donovan Clay", "Tim Althoff", "Sergey Levine", "Natasha Jaques"], "title": "Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to simulate human users in\ninteractive settings such as therapy, education, and social role-play. While\nthese simulations enable scalable training and evaluation of AI agents,\noff-the-shelf LLMs often drift from their assigned personas, contradict earlier\nstatements, or abandon role-appropriate behavior. We introduce a unified\nframework for evaluating and improving persona consistency in LLM-generated\ndialogue. We define three automatic metrics: prompt-to-line consistency,\nline-to-line consistency, and Q&A consistency, that capture different types of\npersona drift and validate each against human annotations. Using these metrics\nas reward signals, we apply multi-turn reinforcement learning to fine-tune LLMs\nfor three user roles: a patient, a student, and a social chat partner. Our\nmethod reduces inconsistency by over 55%, resulting in more coherent and\nfaithful simulated users."}
{"id": "2511.01359", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01359", "abs": "https://arxiv.org/abs/2511.01359", "authors": ["Sapir Harary", "Eran Hirsch", "Aviv Slobodkin", "David Wan", "Mohit Bansal", "Ido Dagan"], "title": "PrefixNLI: Detecting Factual Inconsistencies as Soon as They Arise", "comment": "9 pages + appendix. Code, datasets, and models are available at\n  https://github.com/sapirharary/PrefixNLI", "summary": "Natural Language Inference (NLI) models have been used in various ways to\nimprove the factuality of LLM outputs. This is typically done by applying an\nNLI model to judge whether the model output is entailed from the supposed\nevidence, triggering some corrective actions, such as beam reranking at\ninference time or RL rewards during training. While NLI models are trained to\ndetect factual inconsistencies over complete sentences, decisions in the common\nautoregressive generation architecture are made for each evolving text prefix,\nduring decoding. Addressing this setting, we generalize the entailment\ndetection task to apply over arbitrary text prefixes, and suggest its utility\nfor improving generation faithfulness. Providing suitable evaluation and\ntraining datasets for this task, we train MiniTruePrefixes, a novel specialized\nmodel that better detects factual inconsistencies over text prefixes,\noutperforming comparable baseline NLI models by 5-14 F1 points in prefix-level\nentailment. We further demonstrate that integrating MiniTruePrefixes into a\ncontrolled decoding framework substantially improves factual consistency in\nabstractive summarization. When guided by MiniTruePrefixes,\nLLaMA-3.2-3B-Instruct matches the faithfulness and runtime of the 8B model from\nthe same model family, while using only half the memory."}
{"id": "2511.00268", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00268", "abs": "https://arxiv.org/abs/2511.00268", "authors": ["Shounak Paul", "Dhananjay Ghumare", "Pawan Goyal", "Saptarshi Ghosh", "Ashutosh Modi"], "title": "IL-PCSR: Legal Corpus for Prior Case and Statute Retrieval", "comment": "Accepted at EMNLP 2025 (Main)", "summary": "Identifying/retrieving relevant statutes and prior cases/precedents for a\ngiven legal situation are common tasks exercised by law practitioners.\nResearchers to date have addressed the two tasks independently, thus developing\ncompletely different datasets and models for each task; however, both retrieval\ntasks are inherently related, e.g., similar cases tend to cite similar statutes\n(due to similar factual situation). In this paper, we address this gap. We\npropose IL-PCR (Indian Legal corpus for Prior Case and Statute Retrieval),\nwhich is a unique corpus that provides a common testbed for developing models\nfor both the tasks (Statute Retrieval and Precedent Retrieval) that can exploit\nthe dependence between the two. We experiment extensively with several baseline\nmodels on the tasks, including lexical models, semantic models and ensemble\nbased on GNNs. Further, to exploit the dependence between the two tasks, we\ndevelop an LLM-based re-ranking approach that gives the best performance."}
{"id": "2511.01360", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01360", "abs": "https://arxiv.org/abs/2511.01360", "authors": ["Aadi Palnitkar", "Arjun Suresh", "Rishi Rajesh", "Puneet Puli"], "title": "Safer in Translation? Presupposition Robustness in Indic Languages", "comment": "This is a submission to LREC 2026 (Language Resources and Evaluation\n  Conference 2026). Corresponding author: aadipalnitkar96@gmail.com", "summary": "Increasingly, more and more people are turning to large language models\n(LLMs) for healthcare advice and consultation, making it important to gauge the\nefficacy and accuracy of the responses of LLMs to such queries. While there are\npre-existing medical benchmarks literature which seeks to accomplish this very\ntask, these benchmarks are almost universally in English, which has led to a\nnotable gap in existing literature pertaining to multilingual LLM evaluation.\nWithin this work, we seek to aid in addressing this gap with Cancer-Myth-Indic,\nan Indic language benchmark built by translating a 500-item subset of\nCancer-Myth, sampled evenly across its original categories, into five\nunder-served but widely used languages from the subcontinent (500 per language;\n2,500 translated items total). Native-speaker translators followed a style\nguide for preserving implicit presuppositions in translation; items feature\nfalse presuppositions relating to cancer. We evaluate several popular LLMs\nunder this presupposition stress."}
{"id": "2511.00270", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00270", "abs": "https://arxiv.org/abs/2511.00270", "authors": ["Abhinav Joshi", "Vaibhav Sharma", "Sanjeet Singh", "Ashutosh Modi"], "title": "POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation", "comment": "Accepted at EMNLP 2025 (Main)", "summary": "Sign language translation remains a challenging task due to the scarcity of\nlarge-scale, sentence-aligned datasets. Prior arts have focused on various\nfeature extraction and architectural changes to support neural machine\ntranslation for sign languages. We propose POSESTITCH-SLT, a novel pre-training\nscheme that is inspired by linguistic-templates-based sentence generation\ntechnique. With translation comparison on two sign language datasets, How2Sign\nand iSign, we show that a simple transformer-based encoder-decoder architecture\noutperforms the prior art when considering template-generated sentence pairs in\ntraining. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign\nand from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for\npose-based gloss-free translation. The results demonstrate the effectiveness of\ntemplate-driven synthetic supervision in low-resource sign language settings."}
{"id": "2511.01365", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01365", "abs": "https://arxiv.org/abs/2511.01365", "authors": ["brahim Ethem Deveci", "Duygu Ataman"], "title": "The Ouroboros of Benchmarking: Reasoning Evaluation in an Era of Saturation", "comment": "Accepted to NeurIPS 2025 Workshop on LLM Evaluation\n  (https://openreview.net/group?id=NeurIPS.cc/2025/Workshop/LLM_Evaluation)", "summary": "The rapid rise of Large Language Models (LLMs) and Large Reasoning Models\n(LRMs) has been accompanied by an equally rapid increase of benchmarks used to\nassess them. However, due to both improved model competence resulting from\nscaling and novel training advances as well as likely many of these datasets\nbeing included in pre or post training data, results become saturated, driving\na continuous need for new and more challenging replacements. In this paper, we\ndiscuss whether surpassing a benchmark truly demonstrates reasoning ability or\nare we simply tracking numbers divorced from the capabilities we claim to\nmeasure? We present an investigation focused on three model families, OpenAI,\nAnthropic, and Google, and how their reasoning capabilities across different\nbenchmarks evolve over the years. We also analyze performance trends over the\nyears across different reasoning tasks and discuss the current situation of\nbenchmarking and remaining challenges. By offering a comprehensive overview of\nbenchmarks and reasoning tasks, our work aims to serve as a first reference to\nground future research in reasoning evaluation and model development."}
{"id": "2511.00315", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00315", "abs": "https://arxiv.org/abs/2511.00315", "authors": ["Lee Xiong", "Maksim Tkachenko", "Johanes Effendi", "Ting Cai"], "title": "Language Modeling With Factorization Memory", "comment": null, "summary": "We propose Factorization Memory, an efficient recurrent neural network (RNN)\narchitecture that achieves performance comparable to Transformer models on\nshort-context language modeling tasks while also demonstrating superior\ngeneralization in long-context scenarios. Our model builds upon Mamba-2,\nenabling Factorization Memory to exploit parallel computations during training\nwhile preserving constant computational and memory complexity during inference.\nTo further optimize model efficiency and representational capacity, we develop\na sparse formulation of Factorization Memory that updates only a subset of\nrecurrent states at each step while preserving the strong performance of its\ndense counterpart. To our knowledge, this represents the first RNN architecture\nthat successfully combines sparse memory activation with competitive\nperformance across both short and long-context settings. This work provides a\nsystematic empirical analysis of Factorization Memory in comparison to\nTransformer and Mamba-2 architectures."}
{"id": "2511.01380", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01380", "abs": "https://arxiv.org/abs/2511.01380", "authors": ["Wessel Poelman", "Thomas Bauwens", "Miryam de Lhoneux"], "title": "Confounding Factors in Relating Model Performance to Morphology", "comment": "EMNLP 2025: Main Conference", "summary": "The extent to which individual language characteristics influence\ntokenization and language modeling is an open question. Differences in\nmorphological systems have been suggested as both unimportant and crucial to\nconsider (Cotterell et al., 2018; Gerz et al., 2018a; Park et al., 2021, inter\nalia). We argue this conflicting evidence is due to confounding factors in\nexperimental setups, making it hard to compare results and draw conclusions. We\nidentify confounding factors in analyses trying to answer the question of\nwhether, and how, morphology relates to language modeling. Next, we re-assess\nthree hypotheses by Arnett & Bergen (2025) for why modeling agglutinative\nlanguages results in higher perplexities than fusional languages: they look at\nmorphological alignment of tokenization, tokenization efficiency, and dataset\nsize. We show that each conclusion includes confounding factors. Finally, we\nintroduce token bigram metrics as an intrinsic way to predict the difficulty of\ncausal language modeling, and find that they are gradient proxies for\nmorphological complexity that do not require expert annotation. Ultimately, we\noutline necessities to reliably answer whether, and how, morphology relates to\nlanguage modeling."}
{"id": "2511.00416", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00416", "abs": "https://arxiv.org/abs/2511.00416", "authors": ["Yiwei Zha", "Rui Min", "Shanu Sushmita"], "title": "PADBen: A Comprehensive Benchmark for Evaluating AI Text Detectors Against Paraphrase Attacks", "comment": null, "summary": "While AI-generated text (AIGT) detectors achieve over 90\\% accuracy on direct\nLLM outputs, they fail catastrophically against iteratively-paraphrased\ncontent. We investigate why iteratively-paraphrased text -- itself AI-generated\n-- evades detection systems designed for AIGT identification. Through intrinsic\nmechanism analysis, we reveal that iterative paraphrasing creates an\nintermediate laundering region characterized by semantic displacement with\npreserved generation patterns, which brings up two attack categories:\nparaphrasing human-authored text (authorship obfuscation) and paraphrasing\nLLM-generated text (plagiarism evasion). To address these vulnerabilities, we\nintroduce PADBen, the first benchmark systematically evaluating detector\nrobustness against both paraphrase attack scenarios. PADBen comprises a\nfive-type text taxonomy capturing the full trajectory from original content to\ndeeply laundered text, and five progressive detection tasks across\nsentence-pair and single-sentence challenges. We evaluate 11 state-of-the-art\ndetectors, revealing critical asymmetry: detectors successfully identify the\nplagiarism evasion problem but fail for the case of authorship obfuscation. Our\nfindings demonstrate that current detection approaches cannot effectively\nhandle the intermediate laundering region, necessitating fundamental advances\nin detection architectures beyond existing semantic and stylistic\ndiscrimination methods. For detailed code implementation, please see\nhttps://github.com/JonathanZha47/PadBen-Paraphrase-Attack-Benchmark."}
{"id": "2511.01386", "categories": ["cs.CL", "cs.AI", "cs.IR", "H.3.3; I.2.7"], "pdf": "https://arxiv.org/pdf/2511.01386", "abs": "https://arxiv.org/abs/2511.01386", "authors": ["Muhammed Yusuf Kartal", "Suha Kagan Kose", "Korhan Sevin", "Burak Aktas"], "title": "RAGSmith: A Framework for Finding the Optimal Composition of Retrieval-Augmented Generation Methods Across Datasets", "comment": "45 pages", "summary": "Retrieval-Augmented Generation (RAG) quality depends on many interacting\nchoices across retrieval, ranking, augmentation, prompting, and generation, so\noptimizing modules in isolation is brittle. We introduce RAGSmith, a modular\nframework that treats RAG design as an end-to-end architecture search over nine\ntechnique families and 46{,}080 feasible pipeline configurations. A genetic\nsearch optimizes a scalar objective that jointly aggregates retrieval metrics\n(recall@k, mAP, nDCG, MRR) and generation metrics (LLM-Judge and semantic\nsimilarity). We evaluate on six Wikipedia-derived domains (Mathematics, Law,\nFinance, Medicine, Defense Industry, Computer Science), each with 100 questions\nspanning factual, interpretation, and long-answer types. RAGSmith finds\nconfigurations that consistently outperform naive RAG baseline by +3.8\\% on\naverage (range +1.2\\% to +6.9\\% across domains), with gains up to +12.5\\% in\nretrieval and +7.5\\% in generation. The search typically explores $\\approx\n0.2\\%$ of the space ($\\sim 100$ candidates) and discovers a robust backbone --\nvector retrieval plus post-generation reflection/revision -- augmented by\ndomain-dependent choices in expansion, reranking, augmentation, and prompt\nreordering; passage compression is never selected. Improvement magnitude\ncorrelates with question type, with larger gains on factual/long-answer mixes\nthan interpretation-heavy sets. These results provide practical, domain-aware\nguidance for assembling effective RAG systems and demonstrate the utility of\nevolutionary search for full-pipeline optimization."}
{"id": "2511.00421", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00421", "abs": "https://arxiv.org/abs/2511.00421", "authors": ["Naoto Iwase", "Hiroki Okuyama", "Junichiro Iwasawa"], "title": "MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts", "comment": null, "summary": "Large language models (LLMs) show increasing promise in medical applications,\nbut their ability to detect and correct errors in clinical texts -- a\nprerequisite for safe deployment -- remains under-evaluated, particularly\nbeyond English. We introduce MedRECT, a cross-lingual benchmark\n(Japanese/English) that formulates medical error handling as three subtasks:\nerror detection, error localization (sentence extraction), and error\ncorrection. MedRECT is built with a scalable, automated pipeline from the\nJapanese Medical Licensing Examinations (JMLE) and a curated English\ncounterpart, yielding MedRECT-ja (663 texts) and MedRECT-en (458 texts) with\ncomparable error/no-error balance. We evaluate 9 contemporary LLMs spanning\nproprietary, open-weight, and reasoning families. Key findings: (i) reasoning\nmodels substantially outperform standard architectures, with up to 13.5%\nrelative improvement in error detection and 51.0% in sentence extraction; (ii)\ncross-lingual evaluation reveals 5-10% performance gaps from English to\nJapanese, with smaller disparities for reasoning models; (iii) targeted LoRA\nfine-tuning yields asymmetric improvements in error correction performance\n(Japanese: +0.078, English: +0.168) while preserving reasoning capabilities;\nand (iv) our fine-tuned model exceeds human expert performance on structured\nmedical error correction tasks. To our knowledge, MedRECT is the first\ncomprehensive cross-lingual benchmark for medical error correction, providing a\nreproducible framework and resources for developing safer medical LLMs across\nlanguages."}
{"id": "2511.01409", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01409", "abs": "https://arxiv.org/abs/2511.01409", "authors": ["Heng Zhou", "Ao Yu", "Yuchen Fan", "Jianing Shi", "Li Kang", "Hejia Geng", "Yongting Zhang", "Yutao Fan", "Yuhao Wu", "Tiancheng He", "Yiran Qin", "Lei Bai", "Zhenfei Yin"], "title": "LiveSearchBench: An Automatically Constructed Benchmark for Retrieval and Reasoning over Dynamic Knowledge", "comment": null, "summary": "Evaluating large language models (LLMs) on question answering often relies on\nstatic benchmarks that reward memorization and understate the role of\nretrieval, failing to capture the dynamic nature of world knowledge. We present\nLiveSearchBench, an automated pipeline for constructing retrieval-dependent\nbenchmarks from recent knowledge updates. Our method computes deltas between\nsuccessive Wikidata snapshots, filters candidate triples for quality, and\nsynthesizes natural-language questions at three levels of reasoning difficulty,\neach guaranteed to admit a unique, verifiable answer through SPARQL validation.\nThe pipeline is fully automated, scalable across time, and minimizes human\nintervention, enabling continual regeneration of temporally grounded\nbenchmarks. Experiments show a pronounced performance drop when models confront\nfacts that post-date pretraining, with the gap most salient on multi-hop\nqueries. Retrieval augmented methods and larger, instruction-tuned models\nprovide partial gains but fail to close this recency gap. By design,\nLiveSearchBench shifts evaluation from static memorization toward tasks that\nrequire up-to-date retrieval and reasoning, offering a foundation for\nsystematic, long-term assessment of LLMs under evolving knowledge."}
{"id": "2511.00576", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00576", "abs": "https://arxiv.org/abs/2511.00576", "authors": ["Juan Gabriel Kostelec", "Qinghai Guo"], "title": "FlashEVA: Accelerating LLM inference via Efficient Attention", "comment": "Technical Report", "summary": "Transformer models have revolutionized natural language processing, achieving\nstate-of-the-art performance and demonstrating remarkable scalability. However,\ntheir memory demands, particularly due to maintaining full context in memory,\npose significant challenges for inference. In this paper, we present FlashEVA,\nan efficient implementation of EVA (Efficient Attention via Control Variates),\nand demonstrate how to finetune transformers to adapt to FlashEVA attention.\nOur method enables fine-tuning of Transformer models with as few as 1.5B tokens\nwhile preserving effectiveness across various downstream tasks. Notably,\nFlashEVA achieves up to 6.7x higher throughput and 5x lower peak GPU memory\nusage during inference compared to standard Transformer implementations.\nDespite these improvements, we observe limitations in retrieval-focused tasks.\nOur implementation offers control over the trade-off between throughput and\naccuracy through adjustable hyperparameters, providing flexibility for diverse\nuse cases. This work represents a significant step towards more efficient and\nadaptable Transformer-based models for inference."}
{"id": "2511.01454", "categories": ["cs.CL", "cs.DL"], "pdf": "https://arxiv.org/pdf/2511.01454", "abs": "https://arxiv.org/abs/2511.01454", "authors": ["Sergio Torres Aguilar"], "title": "\"Don't Teach Minerva\": Guiding LLMs Through Complex Syntax for Faithful Latin Translation with RAG", "comment": null, "summary": "Translating a morphology-rich, low-resource language like Latin poses\nsignificant challenges. This paper introduces a reproducible draft-based\nrefinement pipeline that elevates open-source Large Language Models (LLMs) to a\nperformance level statistically comparable to top-tier proprietary systems. Our\nmethod first uses a fine-tuned NLLB-1.3B model to generate a high-quality,\nstructurally faithful draft. A zero-shot LLM (Llama-3.3 or Qwen3) then polishes\nthis draft, a process that can be further enhanced by augmenting the context\nwith retrieved out-context examples (RAG). We demonstrate the robustness of\nthis approach on two distinct benchmarks: a standard in-domain test set\n(Rosenthal, 2023) and a new, challenging out-of-domain (OOD) set of\n12th-century Latin letters (2025). Our central finding is that this open-source\nRAG system achieves performance statistically comparable to the GPT-5 baseline,\nwithout any task-specific LLM fine-tuning. We release the pipeline, the\nChartres OOD set, and evaluation scripts and models to facilitate replicability\nand further research."}
{"id": "2511.00879", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00879", "abs": "https://arxiv.org/abs/2511.00879", "authors": ["Hyeon Hwang", "Yewon Cho", "Chanwoong Yoon", "Yein Park", "Minju Song", "Kyungjae Lee", "Gangwoo Kim", "Jaewoo Kang"], "title": "Assessing LLM Reasoning Steps via Principal Knowledge Grounding", "comment": "Accepted to EMNLP 2025 Findings", "summary": "Step-by-step reasoning has become a standard approach for large language\nmodels (LLMs) to tackle complex tasks. While this paradigm has proven\neffective, it raises a fundamental question: How can we verify that an LLM's\nreasoning is accurately grounded in knowledge? To address this question, we\nintroduce a novel evaluation suite that systematically assesses the knowledge\ngrounding of intermediate reasoning. Our framework comprises three key\ncomponents. (1) Principal Knowledge Collection, a large-scale repository of\natomic knowledge essential for reasoning. Based on the collection, we propose\n(2) knowledge-grounded evaluation metrics designed to measure how well models\nrecall and apply prerequisite knowledge in reasoning. These metrics are\ncomputed by our (3) evaluator LLM, a lightweight model optimized for\ncost-effective and reliable metric computation. Our evaluation suite\ndemonstrates remarkable effectiveness in identifying missing or misapplied\nknowledge elements, providing crucial insights for uncovering fundamental\nreasoning deficiencies in LLMs. Beyond evaluation, we demonstrate how these\nmetrics can be integrated into preference optimization, showcasing further\napplications of knowledge-grounded evaluation."}
{"id": "2511.01470", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01470", "abs": "https://arxiv.org/abs/2511.01470", "authors": ["Lujie Niu", "Lei Shen", "Yi Jiang", "Caixia Yuan", "Xiaojie Wang", "Wenbo Su", "Bo zheng"], "title": "BARD: budget-aware reasoning distillation", "comment": null, "summary": "While long Chain-of-Thought (CoT) distillation effectively transfers\nreasoning capability to smaller language models, the reasoning process often\nremains redundant and computational budget uncontrollable, leading to\ninefficient resource usage. To address this limitation, we propose\n\\textbf{Budget-Aware Reasoning Distillation (BARD)}, a novel framework that\nsimultaneously distills reasoning capability and enables fine-grained control\nover the reasoning length. BARD uses the thinking budget as a user-specified\ncontrol signal, allowing the model to dynamically balance reasoning performance\nand computational efficiency. To achieve this concept, BARD introduces a\ntwo-phase training regimen. The first phase, Supervised Fine-Tuning (SFT) on\nteacher-generated long CoT data compressed to various budget levels,\nbootstrapping the model's understanding of budget constraints. The second phase\nleverages Reinforcement Learning (RL) from a reward signal in consideration of\nreasoning performance and budget fidelity simultaneously. Incorporating the\ntwo-phase regimen is crucial to avoiding policy degradation and ensuring that\nboth objectives are optimized jointly. Extensive experiments demonstrate that\nour method empowers an 8B student model to achieve strong performance on\nchallenging reasoning benchmarks (\\textit{AIME24, AIME25, GPQA}) while\nproviding precise and adaptive control over its reasoning length across a wide\nrange of budgets."}
{"id": "2511.00960", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00960", "abs": "https://arxiv.org/abs/2511.00960", "authors": ["Abhinav P M", "Ojasva Saxena", "Oswald C", "Parameswari Krishnamurthy"], "title": "The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in Multilingual LLMs using Indian Riddles", "comment": null, "summary": "The extent to which large language models (LLMs) can perform culturally\ngrounded reasoning across non-English languages remains underexplored. This\npaper examines the reasoning and self-assessment abilities of LLMs across seven\nmajor Indian languages-Bengali, Gujarati, Hindi, Kannada, Malayalam, Tamil, and\nTelugu. We introduce a multilingual riddle dataset combining traditional\nriddles with context-reconstructed variants and evaluate five LLMs-Gemini 2.5\nPro, Gemini 2.5 Flash, Mistral-Saba, LLaMA 4 Scout, and LLaMA 4 Maverick-under\nseven prompting strategies. In the first stage, we assess riddle-solving\nperformance and find that while Gemini 2.5 Pro performs best overall, few-shot\nmethods yield only marginal gains, and accuracy varies notably across\nlanguages. In the second stage, we conduct a self-evaluation experiment to\nmeasure reasoning consistency. The results reveal a key finding: a model's\ninitial accuracy is inversely correlated with its ability to identify its own\nmistakes. Top-performing models such as Gemini 2.5 Pro are overconfident (4.34%\nTrue Negative Rate), whereas lower-performing models like LLaMA 4 Scout are\nsubstantially more self-aware (42.09% True Negative Rate). These results point\nto clear gaps in multilingual reasoning and highlight the need for models that\nnot only reason effectively but also recognize their own limitations."}
{"id": "2511.01482", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01482", "abs": "https://arxiv.org/abs/2511.01482", "authors": ["Neha Sharma", "Navneet Agarwal", "Kairit Sirts"], "title": "Towards Consistent Detection of Cognitive Distortions: LLM-Based Annotation and Dataset-Agnostic Evaluation", "comment": null, "summary": "Text-based automated Cognitive Distortion detection is a challenging task due\nto its subjective nature, with low agreement scores observed even among expert\nhuman annotators, leading to unreliable annotations. We explore the use of\nLarge Language Models (LLMs) as consistent and reliable annotators, and propose\nthat multiple independent LLM runs can reveal stable labeling patterns despite\nthe inherent subjectivity of the task. Furthermore, to fairly compare models\ntrained on datasets with different characteristics, we introduce a\ndataset-agnostic evaluation framework using Cohen's kappa as an effect size\nmeasure. This methodology allows for fair cross-dataset and cross-study\ncomparisons where traditional metrics like F1 score fall short. Our results\nshow that GPT-4 can produce consistent annotations (Fleiss's Kappa = 0.78),\nresulting in improved test set performance for models trained on these\nannotations compared to those trained on human-labeled data. Our findings\nsuggest that LLMs can offer a scalable and internally consistent alternative\nfor generating training data that supports strong downstream performance in\nsubjective NLP tasks."}
{"id": "2511.01019", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2511.01019", "abs": "https://arxiv.org/abs/2511.01019", "authors": ["Bowen Chen", "Jayesh Gajbhar", "Gregory Dusek", "Rob Redmon", "Patrick Hogan", "Paul Liu", "DelWayne Bohnenstiehl", "Dongkuan", "Xu", "Ruoying He"], "title": "OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights", "comment": "A related presentation will be given at the AGU(American Geophysical\n  Union) and AMS(American Meteorological Society) Annual Meetings", "summary": "Artificial intelligence is transforming the sciences, yet general\nconversational AI systems often generate unverified \"hallucinations\"\nundermining scientific rigor. We present OceanAI, a conversational platform\nthat integrates the natural-language fluency of open-source large language\nmodels (LLMs) with real-time, parameterized access to authoritative\noceanographic data streams hosted by the National Oceanic and Atmospheric\nAdministration (NOAA). Each query such as \"What was Boston Harbor's highest\nwater level in 2024?\" triggers real-time API calls that identify, parse, and\nsynthesize relevant datasets into reproducible natural-language responses and\ndata visualizations. In a blind comparison with three widely used AI\nchat-interface products, only OceanAI produced NOAA-sourced values with\noriginal data references; others either declined to answer or provided\nunsupported results. Designed for extensibility, OceanAI connects to multiple\nNOAA data products and variables, supporting applications in marine hazard\nforecasting, ecosystem assessment, and water-quality monitoring. By grounding\noutputs and verifiable observations, OceanAI advances transparency,\nreproducibility, and trust, offering a scalable framework for AI-enabled\ndecision support within the oceans. A public demonstration is available at\nhttps://oceanai.ai4ocean.xyz."}
{"id": "2511.01490", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01490", "abs": "https://arxiv.org/abs/2511.01490", "authors": ["Max Schaffelder", "Albert Gatt"], "title": "Synthetic Eggs in Many Baskets: The Impact of Synthetic Data Diversity on LLM Fine-Tuning", "comment": null, "summary": "As synthetic data becomes widely used in language model development,\nunderstanding its impact on model behavior is crucial. This paper investigates\nthe impact of the diversity of sources of synthetic data on fine-tuned large\nlanguage models. We focus on three key dimensions: distribution collapse,\nadversarial robustness, and self-preference bias. Our findings reveal that\nfine-tuning models on synthetic data from diverse sources can mitigate\ndistribution collapse, preserving the breadth of the output distribution and\nthe diversity of the output text. Furthermore, while both human and synthetic\nfine-tuning data can remove safeguards, the latter preserves higher output\nquality, thus making outputs potentially more usable and dangerous. Finally,\nfine-tuning reduces self-preference bias, with human data being the most\neffective, followed by multi-source synthetic data."}
{"id": "2511.01188", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01188", "abs": "https://arxiv.org/abs/2511.01188", "authors": ["Lvhua Wu", "Xuefeng Jiang", "Sheng Sun", "Tian Wen", "Yuwei Wang", "Min Liu"], "title": "ZoFia: Zero-Shot Fake News Detection with Entity-Guided Retrieval and Multi-LLM Interaction", "comment": null, "summary": "The rapid spread of fake news threatens social stability and public trust,\nrendering its detection an imperative research priority. Although large\nlanguage models (LLMs) excel at numerous natural language processing tasks with\ntheir remarkable contextual understanding and extensive prior knowledge, the\ntime-bounded knowledge coverage and tendency for generating hallucination\ncontent reduce their reliability when handling fast-evolving news streams.\nFurthermore, models trained on existing static datasets also often lack the\ngeneralization needed for emerging news topics. To address these challenges, we\npropose ZoFia, a novel two-stage zero-shot fake news detection framework.\nFirst, we introduce Hierarchical Salience to quantify the importance of\nentities in the news content, and propose the SC-MMR algorithm to effectively\nselect an informative and diverse set of keywords that serve as queries for\nretrieving up-to-date external evidence. Subsequently, a multi LLM interactive\nsystem, in which each agent assumes a distinct role, performs multi-view\ncollaborative analysis and adversarial debate over the news text and its\nrelated information, and finally produces an interpretable and robust judgment.\nComprehensive experiments on two public datasets demonstrate that ZoFia\nobviously outperforms existing zero-shot baselines and most of few-shot\nmethods. Our codes will be open-sourced to facilitate related communities."}
{"id": "2511.01512", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01512", "abs": "https://arxiv.org/abs/2511.01512", "authors": ["Ayesha Afroza Mohsin", "Mashrur Ahsan", "Nafisa Maliyat", "Shanta Maria", "Syed Rifat Raiyan", "Hasan Mahmud", "Md Kamrul Hasan"], "title": "BanglaNirTox: A Large-scale Parallel Corpus for Explainable AI in Bengali Text Detoxification", "comment": "Under review, 6 pages, 1 figure, 2 tables", "summary": "Toxic language in Bengali remains prevalent, especially in online\nenvironments, with few effective precautions against it. Although text\ndetoxification has seen progress in high-resource languages, Bengali remains\nunderexplored due to limited resources. In this paper, we propose a novel\npipeline for Bengali text detoxification that combines Pareto class-optimized\nlarge language models (LLMs) and Chain-of-Thought (CoT) prompting to generate\ndetoxified sentences. To support this effort, we construct BanglaNirTox, an\nartificially generated parallel corpus of 68,041 toxic Bengali sentences with\nclass-wise toxicity labels, reasonings, and detoxified paraphrases, using\nPareto-optimized LLMs evaluated on random samples. The resulting BanglaNirTox\ndataset is used to fine-tune language models to produce better detoxified\nversions of Bengali sentences. Our findings show that Pareto-optimized LLMs\nwith CoT prompting significantly enhance the quality and consistency of Bengali\ntext detoxification."}
{"id": "2511.01191", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01191", "abs": "https://arxiv.org/abs/2511.01191", "authors": ["Ru Wang", "Wei Huang", "Qi Cao", "Yusuke Iwasawa", "Yutaka Matsuo", "Jiaxian Guo"], "title": "Self-Harmony: Learning to Harmonize Self-Supervision and Self-Play in Test-Time Reinforcement Learning", "comment": null, "summary": "Test-time reinforcement learning (TTRL) offers a label-free paradigm for\nadapting models using only synthetic signals at inference, but its success\nhinges on constructing reliable learning signals. Standard approaches such as\nmajority voting often collapse to spurious yet popular answers. We introduce\nSelf-Harmony, a framework built on a simple intuition: the correct answer\nshould remain stable across both an original question and its paraphrase.\nSelf-Harmony operationalizes this by employing a single model in two\ncomplementary roles: a Solver to produce answers and a Reframer to rephrase the\ninput. Based on this, we further propose a pseudo-label method: instead of\nmajority voting, it aggregates answer frequencies across these original and\nreframed views using the harmonic mean. This is a process that naturally\nselects for solutions stable under reframing, thereby avoiding the common trap\nof favoring view-dependent, spurious answers. Crucially, this requires no human\nsupervision or auxiliary models. Across diverse reasoning benchmarks,\nSelf-Harmony achieves state-of-the-art results at the label-free test-time\nsetting, ranking first in 28 of 30 settings across multiple methods. Beyond\naccuracy, it demonstrates unprecedented robustness, with zero training failures\nin all experiments, underscoring its stability and reliability."}
{"id": "2511.01526", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01526", "abs": "https://arxiv.org/abs/2511.01526", "authors": ["Seokhoon Kang", "Yejin Jeon", "Seonjeong Hwang", "Gary Geunbae Lee"], "title": "Difficulty-Controllable Cloze Question Distractor Generation", "comment": null, "summary": "Multiple-choice cloze questions are commonly used to assess linguistic\nproficiency and comprehension. However, generating high-quality distractors\nremains challenging, as existing methods often lack adaptability and control\nover difficulty levels, and the absence of difficulty-annotated datasets\nfurther hinders progress. To address these issues, we propose a novel framework\nfor generating distractors with controllable difficulty by leveraging both data\naugmentation and a multitask learning strategy. First, to create a\nhigh-quality, difficulty-annotated dataset, we introduce a two-way distractor\ngeneration process in order to produce diverse and plausible distractors. These\ncandidates are subsequently refined through filtering and then categorized by\ndifficulty using an ensemble QA system. Second, this newly created dataset is\nleveraged to train a difficulty-controllable generation model via multitask\nlearning. The framework includes carefully designed auxiliary tasks that\nenhance the model's semantic understanding of distractors and its ability to\nestimate their difficulty. Experimental results demonstrate that our method\ngenerates high-quality distractors across difficulty levels and substantially\noutperforms GPT-4o in aligning distractor difficulty with human perception."}
{"id": "2511.01282", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01282", "abs": "https://arxiv.org/abs/2511.01282", "authors": ["Min Fang", "Zhihui Fu", "Qibin Zhao", "Jun Wang"], "title": "When, What, and How: Rethinking Retrieval-Enhanced Speculative Decoding", "comment": null, "summary": "Speculative decoding (SD) has emerged as an effective technique to accelerate\nlarge language model (LLM) inference without compromising output quality.\nHowever, the achievable speedup largely depends on the effectiveness of the\ndrafting model. While model-based methods like EAGLE-2 are accurate but costly,\nretrieval-enhanced methods like SAM-Decoding rely on heuristic switching\nstrategies that often trigger unnecessary retrievals. To address this, we\npropose ReSpec (\\textbf{Re}trieval-enhanced \\textbf{Spe}culative Decoding), a\nnovel framework that transforms heuristic drafter switching into adaptive\ndecision-making. ReSpec features three core innovations: 1) An\n\\textbf{entropy-guided adaptive trigger} quantifies contextual predictability\nto initiate retrieval only when uncertainty is low, avoiding costly low-quality\nspeculations. 2) A \\textbf{feedback-driven candidate selection} leverages\nhistorical feedback to organize multiple high-quality candidates for parallel\nverification, maximizing retrieval utility. 3) A source-aware \\textbf{relaxed\nverification strategy} applies strict checks to model-generated drafts while\nusing a relaxed verification for retrieved drafts, achieving a better balance\nbetween accuracy and efficiency. Extensive experiments on Spec-Bench\ndemonstrate that ReSpec achieves state-of-the-art acceleration,outperforming\nEAGLE-2 and SAM-Decoding by over $33\\%$ and $25\\%$, respectively, while\nmaintaining output quality."}
{"id": "2511.01558", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.01558", "abs": "https://arxiv.org/abs/2511.01558", "authors": ["Luciana Ciringione", "Emma Franchino", "Simone Reigl", "Isaia D'Onofrio", "Anna Serbati", "Oleksandra Poquet", "Florence Gabriel", "Massimo Stella"], "title": "Math anxiety and associative knowledge structure are entwined in psychology students but not in Large Language Models like GPT-3.5 and GPT-4o", "comment": null, "summary": "Math anxiety poses significant challenges for university psychology students,\naffecting their career choices and overall well-being. This study employs a\nframework based on behavioural forma mentis networks (i.e. cognitive models\nthat map how individuals structure their associative knowledge and emotional\nperceptions of concepts) to explore individual and group differences in the\nperception and association of concepts related to math and anxiety. We\nconducted 4 experiments involving psychology undergraduates from 2 samples (n1\n= 70, n2 = 57) compared against GPT-simulated students (GPT-3.5: n2 = 300;\nGPT-4o: n4 = 300). Experiments 1, 2, and 3 employ individual-level network\nfeatures to predict psychometric scores for math anxiety and its facets\n(observational, social and evaluational) from the Math Anxiety Scale.\nExperiment 4 focuses on group-level perceptions extracted from human students,\nGPT-3.5 and GPT-4o's networks. Results indicate that, in students, positive\nvalence ratings and higher network degree for \"anxiety\", together with negative\nratings for \"math\", can predict higher total and evaluative math anxiety. In\ncontrast, these models do not work on GPT-based data because of differences in\nsimulated networks and psychometric scores compared to humans. These results\nwere also reconciled with differences found in the ways that high/low subgroups\nof simulated and real students framed semantically and emotionally STEM\nconcepts. High math-anxiety students collectively framed \"anxiety\" in an\nemotionally polarising way, absent in the negative perception of low\nmath-anxiety students. \"Science\" was rated positively, but contrasted against\nthe negative perception of \"math\". These findings underscore the importance of\nunderstanding concept perception and associations in managing students' math\nanxiety."}
{"id": "2511.01305", "categories": ["cs.CL", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.01305", "abs": "https://arxiv.org/abs/2511.01305", "authors": ["Aman Ganapathy Manvattira", "Yifei Xu", "Ziyue Dang", "Songwu Lu"], "title": "DeepSpecs: Expert-Level Questions Answering in 5G", "comment": null, "summary": "5G technology enables mobile Internet access for billions of users. Answering\nexpert-level questions about 5G specifications requires navigating thousands of\npages of cross-referenced standards that evolve across releases. Existing\nretrieval-augmented generation (RAG) frameworks, including telecom-specific\napproaches, rely on semantic similarity and cannot reliably resolve\ncross-references or reason about specification evolution. We present DeepSpecs,\na RAG system enhanced by structural and temporal reasoning via three\nmetadata-rich databases: SpecDB (clause-aligned specification text), ChangeDB\n(line-level version diffs), and TDocDB (standardization meeting documents).\nDeepSpecs explicitly resolves cross-references by recursively retrieving\nreferenced clauses through metadata lookup, and traces specification evolution\nby mining changes and linking them to Change Requests that document design\nrationale. We curate two 5G QA datasets: 573 expert-annotated real-world\nquestions from practitioner forums and educational resources, and 350\nevolution-focused questions derived from approved Change Requests. Across\nmultiple LLM backends, DeepSpecs outperforms base models and state-of-the-art\ntelecom RAG systems; ablations confirm that explicit cross-reference resolution\nand evolution-aware retrieval substantially improve answer quality,\nunderscoring the value of modeling the structural and temporal properties of 5G\nstandards."}
{"id": "2511.01568", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01568", "abs": "https://arxiv.org/abs/2511.01568", "authors": ["Seungmin Shin", "Dooyoung Kim", "Youngjoong Ko"], "title": "ECO Decoding: Entropy-Based Control for Controllability and Fluency in Controllable Dialogue Generation", "comment": "Published at EMNLP 2025 main", "summary": "Controllable Dialogue Generation (CDG) enables chatbots to generate responses\nwith desired attributes, and weighted decoding methods have achieved\nsignificant success in the CDG task. However, using a fixed constant value to\nmanage the bias of attribute probabilities makes it challenging to find an\nideal control strength that satisfies both controllability and fluency. To\naddress this issue, we propose ECO decoding (Entropy-based COntrol), which\ndynamically adjusts the control strength at each generation step according to\nthe model's entropy in both the language model and attribute classifier\nprobability distributions. Experiments on the DailyDialog and MultiWOZ datasets\ndemonstrate that ECO decoding consistently improves controllability while\nmaintaining fluency and grammaticality, outperforming prior decoding methods\nacross various models and settings. Furthermore, ECO decoding alleviates\nprobability interpolation issues in multi-attribute generation and consequently\ndemonstrates strong performance in both single and multi-attribute scenarios."}
{"id": "2511.01323", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01323", "abs": "https://arxiv.org/abs/2511.01323", "authors": ["Jiabao Ji", "Min Li", "Priyanshu Kumar", "Shiyu Chang", "Saloni Potdar"], "title": "DEEPAMBIGQA: Ambiguous Multi-hop Questions for Benchmarking LLM Answer Completeness", "comment": "25 pages", "summary": "Large language models (LLMs) with integrated search tools show strong promise\nin open-domain question answering (QA), yet they often struggle to produce\ncomplete answer set to complex questions such as Which actor from the film Heat\nwon at least one Academy Award?, which requires (1) distinguishing between\nmultiple films sharing the same title and (2) reasoning across a large set of\nactors to gather and integrate evidence. Existing QA benchmarks rarely evaluate\nboth challenges jointly. To address this, we introduce DeepAmbigQAGen, an\nautomatic data generation pipeline that constructs QA tasks grounded in text\ncorpora and linked knowledge graph, generating natural and verifiable questions\nthat systematically embed name ambiguity and multi-step reasoning. Based on\nthis, we build DeepAmbigQA, a dataset of 3,600 questions requiring multi-hop\nreasoning and half of them explicit name ambiguity resolving. Experiments\nreveal that, even state-of-the-art GPT-5 show incomplete answers, achieving\nonly 0.13 exact match on ambiguous questions and 0.21 on non-ambiguous\nquestions. These findings highlight the need for more robust QA systems aimed\nat information gathering and answer completeness."}
{"id": "2511.01589", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2511.01589", "abs": "https://arxiv.org/abs/2511.01589", "authors": ["Wenjie Hua", "Hoang H. Nguyen", "Gangyan Ge"], "title": "BIRD: Bronze Inscription Restoration and Dating", "comment": "Accepted at EMNLP 2025 (Main Conference)", "summary": "Bronze inscriptions from early China are fragmentary and difficult to date.\nWe introduce BIRD(Bronze Inscription Restoration and Dating), a fully encoded\ndataset grounded in standard scholarly transcriptions and chronological labels.\nWe further propose an allograph-aware masked language modeling framework that\nintegrates domain- and task-adaptive pretraining with a Glyph Net (GN), which\nlinks graphemes and allographs. Experiments show that GN improves restoration,\nwhile glyph-biased sampling yields gains in dating."}
{"id": "2511.01354", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01354", "abs": "https://arxiv.org/abs/2511.01354", "authors": ["Wenrui Cai", "Chengyu Wang", "Junbing Yan", "Jun Huang", "Xiangzhong Fang"], "title": "Thinking with DistilQwen: A Tale of Four Distilled Reasoning and Reward Model Series", "comment": "emnlp 2025 industry track", "summary": "Recently, the demand for small and efficient reasoning models to support\nreal-world applications has driven the development of knowledge distillation\ntechniques that balance reasoning performance and inference speed. In this\npaper, we further extend the DistilQwen model family, initialized from the Qwen\nmodels, by introducing four model series specifically designed to meet\nindustrial requirements. The distilled model collection comprises: (1)\nslow-thinking models, optimized for reasoning tasks that require high accuracy;\n(2) two series of adaptive-thinking models, which dynamically adjust reasoning\nstrategies based on input tasks to maximize efficiency across diverse\nscenarios; and (3) distilled reward models, which enable further reinforcement\nlearning of reasoning models using distilled knowledge. Comprehensive\nevaluations across multiple benchmarks demonstrate both high inference\nefficiency and strong reasoning performance for these models, as well as the\npractical utility of distilled reward models. We further show that these models\nsupport industry practitioners by providing scalable training and inference\nfunctionalities on the Alibaba Cloud PAI (Platform for Artificial Intelligence)\nplatform."}
{"id": "2511.01615", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01615", "abs": "https://arxiv.org/abs/2511.01615", "authors": ["Francisco Portillo Lpez"], "title": "Imperfect Language, Artificial Intelligence, and the Human Mind: An Interdisciplinary Approach to Linguistic Errors in Native Spanish Speakers", "comment": "12 pages, 3 figures", "summary": "Linguistic errors are not merely deviations from normative grammar; they\noffer a unique window into the cognitive architecture of language and expose\nthe current limitations of artificial systems that seek to replicate them. This\nproject proposes an interdisciplinary study of linguistic errors produced by\nnative Spanish speakers, with the aim of analyzing how current large language\nmodels (LLM) interpret, reproduce, or correct them. The research integrates\nthree core perspectives: theoretical linguistics, to classify and understand\nthe nature of the errors; neurolinguistics, to contextualize them within\nreal-time language processing in the brain; and natural language processing\n(NLP), to evaluate their interpretation against linguistic errors. A\npurpose-built corpus of authentic errors of native Spanish (+500) will serve as\nthe foundation for empirical analysis. These errors will be tested against AI\nmodels such as GPT or Gemini to assess their interpretative accuracy and their\nability to generalize patterns of human linguistic behavior. The project\ncontributes not only to the understanding of Spanish as a native language but\nalso to the development of NLP systems that are more cognitively informed and\ncapable of engaging with the imperfect, variable, and often ambiguous nature of\nreal human language."}
{"id": "2511.01359", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01359", "abs": "https://arxiv.org/abs/2511.01359", "authors": ["Sapir Harary", "Eran Hirsch", "Aviv Slobodkin", "David Wan", "Mohit Bansal", "Ido Dagan"], "title": "PrefixNLI: Detecting Factual Inconsistencies as Soon as They Arise", "comment": "9 pages + appendix. Code, datasets, and models are available at\n  https://github.com/sapirharary/PrefixNLI", "summary": "Natural Language Inference (NLI) models have been used in various ways to\nimprove the factuality of LLM outputs. This is typically done by applying an\nNLI model to judge whether the model output is entailed from the supposed\nevidence, triggering some corrective actions, such as beam reranking at\ninference time or RL rewards during training. While NLI models are trained to\ndetect factual inconsistencies over complete sentences, decisions in the common\nautoregressive generation architecture are made for each evolving text prefix,\nduring decoding. Addressing this setting, we generalize the entailment\ndetection task to apply over arbitrary text prefixes, and suggest its utility\nfor improving generation faithfulness. Providing suitable evaluation and\ntraining datasets for this task, we train MiniTruePrefixes, a novel specialized\nmodel that better detects factual inconsistencies over text prefixes,\noutperforming comparable baseline NLI models by 5-14 F1 points in prefix-level\nentailment. We further demonstrate that integrating MiniTruePrefixes into a\ncontrolled decoding framework substantially improves factual consistency in\nabstractive summarization. When guided by MiniTruePrefixes,\nLLaMA-3.2-3B-Instruct matches the faithfulness and runtime of the 8B model from\nthe same model family, while using only half the memory."}
{"id": "2511.01619", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01619", "abs": "https://arxiv.org/abs/2511.01619", "authors": ["Nikola Ljubei", "Peter Rupnik", "Ivan Porupski", "Taja Kuzman Pungerek"], "title": "ParlaSpeech 3.0: Richly Annotated Spoken Parliamentary Corpora of Croatian, Czech, Polish, and Serbian", "comment": "Submitted to the LREC 2026 conference; 11 pages, 2 figures, 3 tables", "summary": "ParlaSpeech is a collection of spoken parliamentary corpora currently\nspanning four Slavic languages - Croatian, Czech, Polish and Serbian - all\ntogether 6 thousand hours in size. The corpora were built in an automatic\nfashion from the ParlaMint transcripts and their corresponding metadata, which\nwere aligned to the speech recordings of each corresponding parliament. In this\nrelease of the dataset, each of the corpora is significantly enriched with\nvarious automatic annotation layers. The textual modality of all four corpora\nhas been enriched with linguistic annotations and sentiment predictions.\nSimilar to that, their spoken modality has been automatically enriched with\noccurrences of filled pauses, the most frequent disfluency in typical speech.\nTwo out of the four languages have been additionally enriched with detailed\nword- and grapheme-level alignments, and the automatic annotation of the\nposition of primary stress in multisyllabic words. With these enrichments, the\nusefulness of the underlying corpora has been drastically increased for\ndownstream research across multiple disciplines, which we showcase through an\nanalysis of acoustic correlates of sentiment. All the corpora are made\navailable for download in JSONL and TextGrid formats, as well as for search\nthrough a concordancer."}
{"id": "2511.01386", "categories": ["cs.CL", "cs.AI", "cs.IR", "H.3.3; I.2.7"], "pdf": "https://arxiv.org/pdf/2511.01386", "abs": "https://arxiv.org/abs/2511.01386", "authors": ["Muhammed Yusuf Kartal", "Suha Kagan Kose", "Korhan Sevin", "Burak Aktas"], "title": "RAGSmith: A Framework for Finding the Optimal Composition of Retrieval-Augmented Generation Methods Across Datasets", "comment": "45 pages", "summary": "Retrieval-Augmented Generation (RAG) quality depends on many interacting\nchoices across retrieval, ranking, augmentation, prompting, and generation, so\noptimizing modules in isolation is brittle. We introduce RAGSmith, a modular\nframework that treats RAG design as an end-to-end architecture search over nine\ntechnique families and 46{,}080 feasible pipeline configurations. A genetic\nsearch optimizes a scalar objective that jointly aggregates retrieval metrics\n(recall@k, mAP, nDCG, MRR) and generation metrics (LLM-Judge and semantic\nsimilarity). We evaluate on six Wikipedia-derived domains (Mathematics, Law,\nFinance, Medicine, Defense Industry, Computer Science), each with 100 questions\nspanning factual, interpretation, and long-answer types. RAGSmith finds\nconfigurations that consistently outperform naive RAG baseline by +3.8\\% on\naverage (range +1.2\\% to +6.9\\% across domains), with gains up to +12.5\\% in\nretrieval and +7.5\\% in generation. The search typically explores $\\approx\n0.2\\%$ of the space ($\\sim 100$ candidates) and discovers a robust backbone --\nvector retrieval plus post-generation reflection/revision -- augmented by\ndomain-dependent choices in expansion, reranking, augmentation, and prompt\nreordering; passage compression is never selected. Improvement magnitude\ncorrelates with question type, with larger gains on factual/long-answer mixes\nthan interpretation-heavy sets. These results provide practical, domain-aware\nguidance for assembling effective RAG systems and demonstrate the utility of\nevolutionary search for full-pipeline optimization."}
{"id": "2511.01643", "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2.7; I.2.4; I.2.1; I.2.6"], "pdf": "https://arxiv.org/pdf/2511.01643", "abs": "https://arxiv.org/abs/2511.01643", "authors": ["Riccardo Campi", "Nicol Oreste Pinciroli Vago", "Mathyas Giudici", "Pablo Barrachina Rodriguez-Guisado", "Marco Brambilla", "Piero Fraternali"], "title": "A Graph-based RAG for Energy Efficiency Question Answering", "comment": null, "summary": "In this work, we investigate the use of Large Language Models (LLMs) within a\ngraph-based Retrieval Augmented Generation (RAG) architecture for Energy\nEfficiency (EE) Question Answering. First, the system automatically extracts a\nKnowledge Graph (KG) from guidance and regulatory documents in the energy\nfield. Then, the generated graph is navigated and reasoned upon to provide\nusers with accurate answers in multiple languages. We implement a human-based\nvalidation using the RAGAs framework properties, a validation dataset\ncomprising 101 question-answer pairs, and domain experts. Results confirm the\npotential of this architecture and identify its strengths and weaknesses.\nValidation results show how the system correctly answers in about three out of\nfour of the cases (75.2 +- 2.7%), with higher results on questions related to\nmore general EE answers (up to 81.0 +- 4.1%), and featuring promising\nmultilingual abilities (4.4% accuracy loss due to translation)."}
{"id": "2511.01512", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01512", "abs": "https://arxiv.org/abs/2511.01512", "authors": ["Ayesha Afroza Mohsin", "Mashrur Ahsan", "Nafisa Maliyat", "Shanta Maria", "Syed Rifat Raiyan", "Hasan Mahmud", "Md Kamrul Hasan"], "title": "BanglaNirTox: A Large-scale Parallel Corpus for Explainable AI in Bengali Text Detoxification", "comment": "Under review, 6 pages, 1 figure, 2 tables", "summary": "Toxic language in Bengali remains prevalent, especially in online\nenvironments, with few effective precautions against it. Although text\ndetoxification has seen progress in high-resource languages, Bengali remains\nunderexplored due to limited resources. In this paper, we propose a novel\npipeline for Bengali text detoxification that combines Pareto class-optimized\nlarge language models (LLMs) and Chain-of-Thought (CoT) prompting to generate\ndetoxified sentences. To support this effort, we construct BanglaNirTox, an\nartificially generated parallel corpus of 68,041 toxic Bengali sentences with\nclass-wise toxicity labels, reasonings, and detoxified paraphrases, using\nPareto-optimized LLMs evaluated on random samples. The resulting BanglaNirTox\ndataset is used to fine-tune language models to produce better detoxified\nversions of Bengali sentences. Our findings show that Pareto-optimized LLMs\nwith CoT prompting significantly enhance the quality and consistency of Bengali\ntext detoxification."}
{"id": "2511.01649", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01649", "abs": "https://arxiv.org/abs/2511.01649", "authors": ["Hung-Shin Lee", "Chen-Chi Chang", "Ching-Yuan Chen", "Yun-Hsiang Hsu"], "title": "Evaluating Cultural Knowledge Processing in Large Language Models: A Cognitive Benchmarking Framework Integrating Retrieval-Augmented Generation", "comment": "This paper has been accepted by The Electronic Library, and the full\n  article is now available on Emerald Insight", "summary": "This study proposes a cognitive benchmarking framework to evaluate how large\nlanguage models (LLMs) process and apply culturally specific knowledge. The\nframework integrates Bloom's Taxonomy with Retrieval-Augmented Generation (RAG)\nto assess model performance across six hierarchical cognitive domains:\nRemembering, Understanding, Applying, Analyzing, Evaluating, and Creating.\nUsing a curated Taiwanese Hakka digital cultural archive as the primary\ntestbed, the evaluation measures LLM-generated responses' semantic accuracy and\ncultural relevance."}
{"id": "2511.01615", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01615", "abs": "https://arxiv.org/abs/2511.01615", "authors": ["Francisco Portillo Lpez"], "title": "Imperfect Language, Artificial Intelligence, and the Human Mind: An Interdisciplinary Approach to Linguistic Errors in Native Spanish Speakers", "comment": "12 pages, 3 figures", "summary": "Linguistic errors are not merely deviations from normative grammar; they\noffer a unique window into the cognitive architecture of language and expose\nthe current limitations of artificial systems that seek to replicate them. This\nproject proposes an interdisciplinary study of linguistic errors produced by\nnative Spanish speakers, with the aim of analyzing how current large language\nmodels (LLM) interpret, reproduce, or correct them. The research integrates\nthree core perspectives: theoretical linguistics, to classify and understand\nthe nature of the errors; neurolinguistics, to contextualize them within\nreal-time language processing in the brain; and natural language processing\n(NLP), to evaluate their interpretation against linguistic errors. A\npurpose-built corpus of authentic errors of native Spanish (+500) will serve as\nthe foundation for empirical analysis. These errors will be tested against AI\nmodels such as GPT or Gemini to assess their interpretative accuracy and their\nability to generalize patterns of human linguistic behavior. The project\ncontributes not only to the understanding of Spanish as a native language but\nalso to the development of NLP systems that are more cognitively informed and\ncapable of engaging with the imperfect, variable, and often ambiguous nature of\nreal human language."}
{"id": "2511.01650", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01650", "abs": "https://arxiv.org/abs/2511.01650", "authors": ["Ayesha Gull", "Muhammad Usman Safder", "Rania Elbadry", "Preslav Nakov", "Zhuohan Xie"], "title": "EngChain: A Symbolic Benchmark for Verifiable Multi-Step Reasoning in Engineering", "comment": "24 pages, includes figures and tables; introduces the EngChain\n  benchmark", "summary": "Large Language Models (LLMs) are increasingly being applied to specialized,\nhigh-stakes domains like engineering, which demands rigorous evaluation of\ntheir complex reasoning capabilities. While current benchmarks assess language\nunderstanding, factual recall, mathematics or code generation, none capture the\nintegrative reasoning central to engineering where scientific principles,\nquantitative modeling and practical constraints must converge. To address this\ngap, we introduce EngChain, a benchmark for verifiable multi-step engineering\nproblem-solving. EngChain contains 90 problems spanning three engineering\nbranches, organized into 9 domains and 20 distinct areas. The problems are\ngenerated from symbolic templates with a high degree of randomization to ensure\ndiversity and eliminate the risk of contamination. With this benchmark, we move\nbeyond final answer accuracy with a two-stage evaluation: we first\nquantitatively verify the numerical and semantic validity of each reasoning\nstep and then introduce LLM-As-A-Judge, an automated system to qualitatively\ncategorize the identified reasoning errors."}
{"id": "2511.01643", "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2.7; I.2.4; I.2.1; I.2.6"], "pdf": "https://arxiv.org/pdf/2511.01643", "abs": "https://arxiv.org/abs/2511.01643", "authors": ["Riccardo Campi", "Nicol Oreste Pinciroli Vago", "Mathyas Giudici", "Pablo Barrachina Rodriguez-Guisado", "Marco Brambilla", "Piero Fraternali"], "title": "A Graph-based RAG for Energy Efficiency Question Answering", "comment": null, "summary": "In this work, we investigate the use of Large Language Models (LLMs) within a\ngraph-based Retrieval Augmented Generation (RAG) architecture for Energy\nEfficiency (EE) Question Answering. First, the system automatically extracts a\nKnowledge Graph (KG) from guidance and regulatory documents in the energy\nfield. Then, the generated graph is navigated and reasoned upon to provide\nusers with accurate answers in multiple languages. We implement a human-based\nvalidation using the RAGAs framework properties, a validation dataset\ncomprising 101 question-answer pairs, and domain experts. Results confirm the\npotential of this architecture and identify its strengths and weaknesses.\nValidation results show how the system correctly answers in about three out of\nfour of the cases (75.2 +- 2.7%), with higher results on questions related to\nmore general EE answers (up to 81.0 +- 4.1%), and featuring promising\nmultilingual abilities (4.4% accuracy loss due to translation)."}
{"id": "2511.01670", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01670", "abs": "https://arxiv.org/abs/2511.01670", "authors": ["Chaoqun Liu", "Mahani Aljunied", "Guizhen Chen", "Hou Pong Chan", "Weiwen Xu", "Yu Rong", "Wenxuan Zhang"], "title": "SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia", "comment": "10 pages", "summary": "We introduce SeaLLMs-Audio, the first large audio-language model (LALM)\ntailored for multiple Southeast Asian (SEA) languages-Indonesian (id), Thai\n(th), and Vietnamese (vi)-alongside English (en) and Chinese (zh). Trained on a\nlarge-scale audio corpus, SeaLLMs-Audio exhibits strong performance across\ndiverse audio-centric tasks, spanning fine-grained audio understanding and\nvoice-based interaction. Its key features include: 1) Multilingual: the model\nprimarily supports 5 languages, namely Indonesian, Thai, Vietnamese, English,\nand Chinese; 2) Multimodal: the model accepts flexible input modalities,\nincluding audio only, text only, as well as audio with text; 3) Multi-task: the\nmodel supports a wide range of tasks, including audio analysis tasks such as\nAudio Captioning, Automatic Speech Recognition, Speech-to-Text Translation,\nSpeech Emotion Recognition, Speech Question Answering, and Speech\nSummarization. It also enables voice-based dialogue, including answering\nfactual, mathematical, and general knowledge queries. As a significant step\ntowards advancing audio LLMs in Southeast Asia, we expect SeaLLMs-Audio to\nbenefit both the regional research community and industry. To automate LALM\nevaluation for Southeast Asia, we introduce SeaBench-Audio, a benchmark\nspanning multiple tasks. Experiments show that SeaLLMs-Audio achieves\ncompetitive performance compared with other LALMs on SEA languages."}
{"id": "2511.01650", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01650", "abs": "https://arxiv.org/abs/2511.01650", "authors": ["Ayesha Gull", "Muhammad Usman Safder", "Rania Elbadry", "Preslav Nakov", "Zhuohan Xie"], "title": "EngChain: A Symbolic Benchmark for Verifiable Multi-Step Reasoning in Engineering", "comment": "24 pages, includes figures and tables; introduces the EngChain\n  benchmark", "summary": "Large Language Models (LLMs) are increasingly being applied to specialized,\nhigh-stakes domains like engineering, which demands rigorous evaluation of\ntheir complex reasoning capabilities. While current benchmarks assess language\nunderstanding, factual recall, mathematics or code generation, none capture the\nintegrative reasoning central to engineering where scientific principles,\nquantitative modeling and practical constraints must converge. To address this\ngap, we introduce EngChain, a benchmark for verifiable multi-step engineering\nproblem-solving. EngChain contains 90 problems spanning three engineering\nbranches, organized into 9 domains and 20 distinct areas. The problems are\ngenerated from symbolic templates with a high degree of randomization to ensure\ndiversity and eliminate the risk of contamination. With this benchmark, we move\nbeyond final answer accuracy with a two-stage evaluation: we first\nquantitatively verify the numerical and semantic validity of each reasoning\nstep and then introduce LLM-As-A-Judge, an automated system to qualitatively\ncategorize the identified reasoning errors."}
{"id": "2511.01689", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01689", "abs": "https://arxiv.org/abs/2511.01689", "authors": ["Sharan Maiya", "Henning Bartsch", "Nathan Lambert", "Evan Hubinger"], "title": "Open Character Training: Shaping the Persona of AI Assistants through Constitutional AI", "comment": "12 pages, 6 figures, 4 tables", "summary": "The character of the \"AI assistant\" persona generated by modern chatbot large\nlanguage models influences both surface-level behavior and apparent values,\nbeliefs, and ethics. These all affect interaction quality, perceived\nintelligence, and alignment with both developer and user intentions. The\nshaping of this persona, known as character training, is a critical component\nof industry post-training, yet remains effectively unstudied in the academic\nliterature. We introduce the first open implementation of character training,\nleveraging Constitutional AI and a new data pipeline using synthetic\nintrospective data to shape the assistant persona in a more effective and\ncontrolled manner than alternatives such as constraining system prompts or\nactivation steering. Specifically, we fine-tune three popular open-weights\nmodels using 11 example personas, such as humorous, deeply caring, or even\nmalevolent. To track the effects of our approach, we introduce a method which\nanalyzes revealed preferences, uncovering clear and holistic changes in\ncharacter. We find these changes are more robust to adversarial prompting than\nthe above two alternatives, while also leading to more coherent and realistic\ngenerations. Finally, we demonstrate this fine-tuning has little to no effect\non general capabilities as measured by common benchmarks. We describe and\nopen-source our full post-training method, the implementation of which can be\nfound at https://github.com/maiush/OpenCharacterTraining."}
{"id": "2511.01670", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01670", "abs": "https://arxiv.org/abs/2511.01670", "authors": ["Chaoqun Liu", "Mahani Aljunied", "Guizhen Chen", "Hou Pong Chan", "Weiwen Xu", "Yu Rong", "Wenxuan Zhang"], "title": "SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia", "comment": "10 pages", "summary": "We introduce SeaLLMs-Audio, the first large audio-language model (LALM)\ntailored for multiple Southeast Asian (SEA) languages-Indonesian (id), Thai\n(th), and Vietnamese (vi)-alongside English (en) and Chinese (zh). Trained on a\nlarge-scale audio corpus, SeaLLMs-Audio exhibits strong performance across\ndiverse audio-centric tasks, spanning fine-grained audio understanding and\nvoice-based interaction. Its key features include: 1) Multilingual: the model\nprimarily supports 5 languages, namely Indonesian, Thai, Vietnamese, English,\nand Chinese; 2) Multimodal: the model accepts flexible input modalities,\nincluding audio only, text only, as well as audio with text; 3) Multi-task: the\nmodel supports a wide range of tasks, including audio analysis tasks such as\nAudio Captioning, Automatic Speech Recognition, Speech-to-Text Translation,\nSpeech Emotion Recognition, Speech Question Answering, and Speech\nSummarization. It also enables voice-based dialogue, including answering\nfactual, mathematical, and general knowledge queries. As a significant step\ntowards advancing audio LLMs in Southeast Asia, we expect SeaLLMs-Audio to\nbenefit both the regional research community and industry. To automate LALM\nevaluation for Southeast Asia, we introduce SeaBench-Audio, a benchmark\nspanning multiple tasks. Experiments show that SeaLLMs-Audio achieves\ncompetitive performance compared with other LALMs on SEA languages."}
{"id": "2511.01706", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01706", "abs": "https://arxiv.org/abs/2511.01706", "authors": ["Sekh Mainul Islam", "Pepa Atanasova", "Isabelle Augenstein"], "title": "Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement", "comment": "Under review", "summary": "Natural Language Explanations (NLEs) describe how Large Language Models\n(LLMs) make decisions, drawing on both external Context Knowledge (CK) and\nParametric Knowledge (PK) stored in model weights. Understanding their\ninteraction is key to assessing the grounding of NLEs, yet it remains\nunderexplored. Prior work has largely examined only single-step generation,\ntypically the final answer, and has modelled PK and CK interaction only as a\nbinary choice in a rank-1 subspace. This overlooks richer forms of interaction,\nsuch as complementary or supportive knowledge. We propose a novel rank-2\nprojection subspace that disentangles PK and CK contributions more accurately\nand use it for the first multi-step analysis of knowledge interactions across\nlonger NLE sequences. Experiments on four QA datasets and three open-weight\ninstruction-tuned LLMs show that diverse knowledge interactions are poorly\nrepresented in a rank-1 subspace but are effectively captured in our rank-2\nformulation. Our multi-step analysis reveals that hallucinated NLEs align\nstrongly with the PK direction, context-faithful ones balance PK and CK, and\nChain-of-Thought prompting for NLEs shifts generated NLEs toward CK by reducing\nPK reliance. This work provides the first framework for systematic studies of\nmulti-step knowledge interactions in LLMs through a richer rank-2 subspace\ndisentanglement. Code and data:\nhttps://github.com/copenlu/pk-ck-knowledge-disentanglement."}
{"id": "2511.01689", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01689", "abs": "https://arxiv.org/abs/2511.01689", "authors": ["Sharan Maiya", "Henning Bartsch", "Nathan Lambert", "Evan Hubinger"], "title": "Open Character Training: Shaping the Persona of AI Assistants through Constitutional AI", "comment": "12 pages, 6 figures, 4 tables", "summary": "The character of the \"AI assistant\" persona generated by modern chatbot large\nlanguage models influences both surface-level behavior and apparent values,\nbeliefs, and ethics. These all affect interaction quality, perceived\nintelligence, and alignment with both developer and user intentions. The\nshaping of this persona, known as character training, is a critical component\nof industry post-training, yet remains effectively unstudied in the academic\nliterature. We introduce the first open implementation of character training,\nleveraging Constitutional AI and a new data pipeline using synthetic\nintrospective data to shape the assistant persona in a more effective and\ncontrolled manner than alternatives such as constraining system prompts or\nactivation steering. Specifically, we fine-tune three popular open-weights\nmodels using 11 example personas, such as humorous, deeply caring, or even\nmalevolent. To track the effects of our approach, we introduce a method which\nanalyzes revealed preferences, uncovering clear and holistic changes in\ncharacter. We find these changes are more robust to adversarial prompting than\nthe above two alternatives, while also leading to more coherent and realistic\ngenerations. Finally, we demonstrate this fine-tuning has little to no effect\non general capabilities as measured by common benchmarks. We describe and\nopen-source our full post-training method, the implementation of which can be\nfound at https://github.com/maiush/OpenCharacterTraining."}
{"id": "2511.01720", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01720", "abs": "https://arxiv.org/abs/2511.01720", "authors": ["Mahammad Nuriyev"], "title": "Efficient Tool-Calling Multi-Expert NPC Agent for Commonsense Persona-Grounded Dialogue", "comment": "10 pages, 1 figure, 2 tables. Technical report for the Commonsense\n  Persona-Grounded Dialogue Challenge (CPDC) 2025, part of the Wordplay 2025\n  Workshop @ EMNLP 2025", "summary": "We present a multi-expert system for creating Non-Player Characters (NPCs)\ncapable of both natural dialogue and contextual action execution in interactive\nenvironments. Using Qwen3 as the base model and Low-Rank Adaptation (LoRA)\nadapters, we instantiate three specialists: tool calling, tool-response\ninterpretation, and direct dialogue. Our system comfortably meets the\ncomputational efficiency requirements, delivering fast responses and\nmaintaining modest resource usage on L40S GPUs. In the Commonsense\nPersona-Grounded Dialogue Challenge 2025, our method ranked second overall.\n  Code available at:\nhttps://github.com/MahammadNuriyev62/CPDC-challenge-2025-solution/"}
{"id": "2511.01706", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01706", "abs": "https://arxiv.org/abs/2511.01706", "authors": ["Sekh Mainul Islam", "Pepa Atanasova", "Isabelle Augenstein"], "title": "Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement", "comment": "Under review", "summary": "Natural Language Explanations (NLEs) describe how Large Language Models\n(LLMs) make decisions, drawing on both external Context Knowledge (CK) and\nParametric Knowledge (PK) stored in model weights. Understanding their\ninteraction is key to assessing the grounding of NLEs, yet it remains\nunderexplored. Prior work has largely examined only single-step generation,\ntypically the final answer, and has modelled PK and CK interaction only as a\nbinary choice in a rank-1 subspace. This overlooks richer forms of interaction,\nsuch as complementary or supportive knowledge. We propose a novel rank-2\nprojection subspace that disentangles PK and CK contributions more accurately\nand use it for the first multi-step analysis of knowledge interactions across\nlonger NLE sequences. Experiments on four QA datasets and three open-weight\ninstruction-tuned LLMs show that diverse knowledge interactions are poorly\nrepresented in a rank-1 subspace but are effectively captured in our rank-2\nformulation. Our multi-step analysis reveals that hallucinated NLEs align\nstrongly with the PK direction, context-faithful ones balance PK and CK, and\nChain-of-Thought prompting for NLEs shifts generated NLEs toward CK by reducing\nPK reliance. This work provides the first framework for systematic studies of\nmulti-step knowledge interactions in LLMs through a richer rank-2 subspace\ndisentanglement. Code and data:\nhttps://github.com/copenlu/pk-ck-knowledge-disentanglement."}
{"id": "2511.01805", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01805", "abs": "https://arxiv.org/abs/2511.01805", "authors": ["Jiayi Geng", "Howard Chen", "Ryan Liu", "Manoel Horta Ribeiro", "Robb Willer", "Graham Neubig", "Thomas L. Griffiths"], "title": "Accumulating Context Changes the Beliefs of Language Models", "comment": null, "summary": "Language model (LM) assistants are increasingly used in applications such as\nbrainstorming and research. Improvements in memory and context size have\nallowed these models to become more autonomous, which has also resulted in more\ntext accumulation in their context windows without explicit user intervention.\nThis comes with a latent risk: the belief profiles of models -- their\nunderstanding of the world as manifested in their responses or actions -- may\nsilently change as context accumulates. This can lead to subtly inconsistent\nuser experiences, or shifts in behavior that deviate from the original\nalignment of the models. In this paper, we explore how accumulating context by\nengaging in interactions and processing text -- talking and reading -- can\nchange the beliefs of language models, as manifested in their responses and\nbehaviors.Our results reveal that models' belief profiles are highly malleable:\nGPT-5 exhibits a 54.7% shift in its stated beliefs after 10 rounds of\ndiscussion about moral dilemmas and queries about safety, while Grok 4 shows a\n27.2% shift on political issues after reading texts from the opposing position.\nWe also examine models' behavioral changes by designing tasks that require tool\nuse, where each tool selection corresponds to an implicit belief. We find that\nthese changes align with stated belief shifts, suggesting that belief shifts\nwill be reflected in actual behavior in agentic systems. Our analysis exposes\nthe hidden risk of belief shift as models undergo extended sessions of talking\nor reading, rendering their opinions and actions unreliable."}
{"id": "2511.01805", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01805", "abs": "https://arxiv.org/abs/2511.01805", "authors": ["Jiayi Geng", "Howard Chen", "Ryan Liu", "Manoel Horta Ribeiro", "Robb Willer", "Graham Neubig", "Thomas L. Griffiths"], "title": "Accumulating Context Changes the Beliefs of Language Models", "comment": null, "summary": "Language model (LM) assistants are increasingly used in applications such as\nbrainstorming and research. Improvements in memory and context size have\nallowed these models to become more autonomous, which has also resulted in more\ntext accumulation in their context windows without explicit user intervention.\nThis comes with a latent risk: the belief profiles of models -- their\nunderstanding of the world as manifested in their responses or actions -- may\nsilently change as context accumulates. This can lead to subtly inconsistent\nuser experiences, or shifts in behavior that deviate from the original\nalignment of the models. In this paper, we explore how accumulating context by\nengaging in interactions and processing text -- talking and reading -- can\nchange the beliefs of language models, as manifested in their responses and\nbehaviors.Our results reveal that models' belief profiles are highly malleable:\nGPT-5 exhibits a 54.7% shift in its stated beliefs after 10 rounds of\ndiscussion about moral dilemmas and queries about safety, while Grok 4 shows a\n27.2% shift on political issues after reading texts from the opposing position.\nWe also examine models' behavioral changes by designing tasks that require tool\nuse, where each tool selection corresponds to an implicit belief. We find that\nthese changes align with stated belief shifts, suggesting that belief shifts\nwill be reflected in actual behavior in agentic systems. Our analysis exposes\nthe hidden risk of belief shift as models undergo extended sessions of talking\nor reading, rendering their opinions and actions unreliable."}
{"id": "2511.01807", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01807", "abs": "https://arxiv.org/abs/2511.01807", "authors": ["Adewale Akinfaderin", "Shreyas Subramanian", "Akarsha Sehwag"], "title": "Plan-and-Write: Structure-Guided Length Control for LLMs without Model Retraining", "comment": "Presented at Workshop on Prompt Optimization, KDD 2025, Toronto,\n  Canada", "summary": "Length control in Large Language Models (LLMs) is a crucial but\nunder-addressed challenge, with applications ranging from voice interfaces\nrequiring concise responses to research summaries needing comprehensive\noutputs. Current approaches to length control, including Regularized DPO,\nLength-Instruction Fine Tuning, and tool-augmented methods, typically require\nexpensive model retraining or complex inference-time tooling. This paper\npresents a prompt engineering methodology that enables precise length control\nwithout model retraining. Our structure-guided approach implements deliberate\nplanning and word counting mechanisms within the prompt, encouraging the model\nto carefully track and adhere to specified length constraints. Comprehensive\nevaluations across six state-of-the-art LLMs demonstrate that our method\nsignificantly improves length fidelity for several models compared to standard\nprompting when applied to document summarization tasks, particularly for\nshorter-to-medium length constraints. The proposed technique shows varying\nbenefits across different model architectures, with some models demonstrating\nup to 37.6% improvement in length adherence. Quality evaluations further reveal\nthat our approach maintains or enhances overall output quality compared to\nstandard prompting techniques. Our approach provides an immediately deployable\nsolution for applications requiring precise length control, particularly\nvaluable for production environments where model retraining is impractical or\ncost-prohibitive."}
{"id": "2511.01807", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01807", "abs": "https://arxiv.org/abs/2511.01807", "authors": ["Adewale Akinfaderin", "Shreyas Subramanian", "Akarsha Sehwag"], "title": "Plan-and-Write: Structure-Guided Length Control for LLMs without Model Retraining", "comment": "Presented at Workshop on Prompt Optimization, KDD 2025, Toronto,\n  Canada", "summary": "Length control in Large Language Models (LLMs) is a crucial but\nunder-addressed challenge, with applications ranging from voice interfaces\nrequiring concise responses to research summaries needing comprehensive\noutputs. Current approaches to length control, including Regularized DPO,\nLength-Instruction Fine Tuning, and tool-augmented methods, typically require\nexpensive model retraining or complex inference-time tooling. This paper\npresents a prompt engineering methodology that enables precise length control\nwithout model retraining. Our structure-guided approach implements deliberate\nplanning and word counting mechanisms within the prompt, encouraging the model\nto carefully track and adhere to specified length constraints. Comprehensive\nevaluations across six state-of-the-art LLMs demonstrate that our method\nsignificantly improves length fidelity for several models compared to standard\nprompting when applied to document summarization tasks, particularly for\nshorter-to-medium length constraints. The proposed technique shows varying\nbenefits across different model architectures, with some models demonstrating\nup to 37.6% improvement in length adherence. Quality evaluations further reveal\nthat our approach maintains or enhances overall output quality compared to\nstandard prompting techniques. Our approach provides an immediately deployable\nsolution for applications requiring precise length control, particularly\nvaluable for production environments where model retraining is impractical or\ncost-prohibitive."}
{"id": "2511.01815", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01815", "abs": "https://arxiv.org/abs/2511.01815", "authors": ["Konrad Staniszewski", "Adrian acucki"], "title": "KV Cache Transform Coding for Compact Storage in LLM Inference", "comment": null, "summary": "Serving large language models (LLMs) at scale necessitates efficient\nkey-value (KV) cache management. KV caches can be reused across conversation\nturns via shared-prefix prompts that are common in iterative code editing and\nchat. However, stale caches consume scarce GPU memory, require offloading, or\nforce recomputation. We present KVTC, a lightweight transform coder that\ncompresses KV caches for compact on-GPU and off-GPU storage. Drawing on\nclassical media compression, KVTC combines PCA-based feature decorrelation,\nadaptive quantization, and entropy coding. It requires only a brief initial\ncalibration and leaves model parameters unchanged. By exploiting redundancies\nin KV caches, KVTC achieves up to 20$\\times$ compression while maintaining\nreasoning and long-context accuracy, and 40$\\times$ or higher for specific use\ncases. We test KVTC with Llama 3, Mistral NeMo, and R1-Qwen 2.5 models across\nbenchmarks including AIME25, LiveCodeBench, GSM8K, MMLU, Qasper, RULER, and\nMATH-500. It consistently outperforms inference-time baselines such as token\neviction, quantization, and SVD-based methods, while achieving higher\ncompression ratios. These results support KVTC as a practical building block\nfor memory-efficient LLM serving with reusable KV caches."}
{"id": "2511.01815", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01815", "abs": "https://arxiv.org/abs/2511.01815", "authors": ["Konrad Staniszewski", "Adrian acucki"], "title": "KV Cache Transform Coding for Compact Storage in LLM Inference", "comment": null, "summary": "Serving large language models (LLMs) at scale necessitates efficient\nkey-value (KV) cache management. KV caches can be reused across conversation\nturns via shared-prefix prompts that are common in iterative code editing and\nchat. However, stale caches consume scarce GPU memory, require offloading, or\nforce recomputation. We present KVTC, a lightweight transform coder that\ncompresses KV caches for compact on-GPU and off-GPU storage. Drawing on\nclassical media compression, KVTC combines PCA-based feature decorrelation,\nadaptive quantization, and entropy coding. It requires only a brief initial\ncalibration and leaves model parameters unchanged. By exploiting redundancies\nin KV caches, KVTC achieves up to 20$\\times$ compression while maintaining\nreasoning and long-context accuracy, and 40$\\times$ or higher for specific use\ncases. We test KVTC with Llama 3, Mistral NeMo, and R1-Qwen 2.5 models across\nbenchmarks including AIME25, LiveCodeBench, GSM8K, MMLU, Qasper, RULER, and\nMATH-500. It consistently outperforms inference-time baselines such as token\neviction, quantization, and SVD-based methods, while achieving higher\ncompression ratios. These results support KVTC as a practical building block\nfor memory-efficient LLM serving with reusable KV caches."}
{"id": "2511.01846", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01846", "abs": "https://arxiv.org/abs/2511.01846", "authors": ["Thang Luong", "Dawsen Hwang", "Hoang H. Nguyen", "Golnaz Ghiasi", "Yuri Chervonyi", "Insuk Seo", "Junsu Kim", "Garrett Bingham", "Jonathan Lee", "Swaroop Mishra", "Alex Zhai", "Clara Huiyi Hu", "Henryk Michalewski", "Jimin Kim", "Jeonghyun Ahn", "Junhwi Bae", "Xingyou Song", "Trieu H. Trinh", "Quoc V. Le", "Junehyuk Jung"], "title": "Towards Robust Mathematical Reasoning", "comment": "EMNLP 2025 (main conference),\n  https://aclanthology.org/2025.emnlp-main.1794/", "summary": "Finding the right north-star metrics is highly critical for advancing the\nmathematical reasoning capabilities of foundation models, especially given that\nexisting evaluations are either too easy or only focus on getting correct short\nanswers. To address these issues, we present IMO-Bench, a suite of advanced\nreasoning benchmarks, vetted by a panel of top specialists and that\nspecifically targets the level of the International Mathematical Olympiad\n(IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench\nfirst tests models on 400 diverse Olympiad problems with verifiable short\nanswers. IMO-Proof Bench is the next-level evaluation for proof-writing\ncapabilities, which includes both basic and advanced IMO level problems as well\nas detailed grading guidelines to facilitate automatic grading. These\nbenchmarks played a crucial role in our historic achievement of the gold-level\nperformance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our\nmodel achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof\nBench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4%\nrespectively. We also showed that autograders built with Gemini reasoning\ncorrelate well with human evaluations and construct IMO-GradingBench, with 1000\nhuman gradings on proofs, to enable further progress in automatic evaluation of\nlong-form answers. We hope that IMO-Bench will help the community towards\nadvancing robust mathematical reasoning and release it at\nhttps://imobench.github.io/."}
{"id": "2511.01846", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01846", "abs": "https://arxiv.org/abs/2511.01846", "authors": ["Thang Luong", "Dawsen Hwang", "Hoang H. Nguyen", "Golnaz Ghiasi", "Yuri Chervonyi", "Insuk Seo", "Junsu Kim", "Garrett Bingham", "Jonathan Lee", "Swaroop Mishra", "Alex Zhai", "Clara Huiyi Hu", "Henryk Michalewski", "Jimin Kim", "Jeonghyun Ahn", "Junhwi Bae", "Xingyou Song", "Trieu H. Trinh", "Quoc V. Le", "Junehyuk Jung"], "title": "Towards Robust Mathematical Reasoning", "comment": "EMNLP 2025 (main conference),\n  https://aclanthology.org/2025.emnlp-main.1794/", "summary": "Finding the right north-star metrics is highly critical for advancing the\nmathematical reasoning capabilities of foundation models, especially given that\nexisting evaluations are either too easy or only focus on getting correct short\nanswers. To address these issues, we present IMO-Bench, a suite of advanced\nreasoning benchmarks, vetted by a panel of top specialists and that\nspecifically targets the level of the International Mathematical Olympiad\n(IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench\nfirst tests models on 400 diverse Olympiad problems with verifiable short\nanswers. IMO-Proof Bench is the next-level evaluation for proof-writing\ncapabilities, which includes both basic and advanced IMO level problems as well\nas detailed grading guidelines to facilitate automatic grading. These\nbenchmarks played a crucial role in our historic achievement of the gold-level\nperformance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our\nmodel achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof\nBench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4%\nrespectively. We also showed that autograders built with Gemini reasoning\ncorrelate well with human evaluations and construct IMO-GradingBench, with 1000\nhuman gradings on proofs, to enable further progress in automatic evaluation of\nlong-form answers. We hope that IMO-Bench will help the community towards\nadvancing robust mathematical reasoning and release it at\nhttps://imobench.github.io/."}
{"id": "2511.01854", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01854", "abs": "https://arxiv.org/abs/2511.01854", "authors": ["Elias Lumer", "Faheem Nizar", "Anmol Gulati", "Pradeep Honaganahalli Basavaraju", "Vamse Kumar Subbiah"], "title": "Tool-to-Agent Retrieval: Bridging Tools and Agents for Scalable LLM Multi-Agent Systems", "comment": null, "summary": "Recent advances in LLM Multi-Agent Systems enable scalable orchestration of\nsub-agents, each coordinating hundreds or thousands of tools or Model Context\nProtocol (MCP) servers. However, existing retrieval methods typically match\nqueries against coarse agent-level descriptions before routing, which obscures\nfine-grained tool functionality and often results in suboptimal agent\nselection. We introduce Tool-to-Agent Retrieval, a unified framework that\nembeds both tools and their parent agents in a shared vector space and connects\nthem through metadata relationships. By explicitly representing tool\ncapabilities and traversing metadata to the agent level, Tool-to-Agent\nRetrieval enables granular tool-level or agent-level retrieval, ensuring that\nagents and their underlying tools or MCP servers are equally represented\nwithout the context dilution that arises from chunking many tools together.\nEvaluating Tool-to-Agent Retrieval across eight embedding models, our approach\nachieves consistent improvements of 19.4% in Recall@5 and 17.7% in nDCG@5 over\nprevious state-of-the-art agent retrievers on the LiveMCPBench benchmark."}
{"id": "2511.00020", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00020", "abs": "https://arxiv.org/abs/2511.00020", "authors": ["Suhasnadh Reddy Veluru", "Sai Teja Erukude", "Viswa Chaitanya Marella"], "title": "Multimodal Detection of Fake Reviews using BERT and ResNet-50", "comment": "Published in IEEE", "summary": "In the current digital commerce landscape, user-generated reviews play a\ncritical role in shaping consumer behavior, product reputation, and platform\ncredibility. However, the proliferation of fake or misleading reviews often\ngenerated by bots, paid agents, or AI models poses a significant threat to\ntrust and transparency within review ecosystems. Existing detection models\nprimarily rely on unimodal, typically textual, data and therefore fail to\ncapture semantic inconsistencies across different modalities. To address this\ngap, a robust multimodal fake review detection framework is proposed,\nintegrating textual features encoded with BERT and visual features extracted\nusing ResNet-50. These representations are fused through a classification head\nto jointly predict review authenticity. To support this approach, a curated\ndataset comprising 21,142 user-uploaded images across food delivery,\nhospitality, and e-commerce domains was utilized. Experimental results indicate\nthat the multimodal model outperforms unimodal baselines, achieving an F1-score\nof 0.934 on the test set. Additionally, the confusion matrix and qualitative\nanalysis highlight the model's ability to detect subtle inconsistencies, such\nas exaggerated textual praise paired with unrelated or low-quality images,\ncommonly found in deceptive content. This study demonstrates the critical role\nof multimodal learning in safeguarding digital trust and offers a scalable\nsolution for content moderation across various online platforms."}
{"id": "2511.00092", "categories": ["cs.AI", "cs.CL", "cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2511.00092", "abs": "https://arxiv.org/abs/2511.00092", "authors": ["Shunya Minami", "Tatsuya Ishigaki", "Ikko Hamamura", "Taku Mikuriya", "Youmi Ma", "Naoaki Okazaki", "Hiroya Takamura", "Yohichi Suzuki", "Tadashi Kadowaki"], "title": "QuantumBench: A Benchmark for Quantum Problem Solving", "comment": "11 pages, 8 figures", "summary": "Large language models are now integrated into many scientific workflows,\naccelerating data analysis, hypothesis generation, and design space\nexploration. In parallel with this growth, there is a growing need to carefully\nevaluate whether models accurately capture domain-specific knowledge and\nnotation, since general-purpose benchmarks rarely reflect these requirements.\nThis gap is especially clear in quantum science, which features non-intuitive\nphenomena and requires advanced mathematics. In this study, we introduce\nQuantumBench, a benchmark for the quantum domain that systematically examine\nhow well LLMs understand and can be applied to this non-intuitive field. Using\npublicly available materials, we compiled approximately 800 questions with\ntheir answers spanning nine areas related to quantum science and organized them\ninto an eight-option multiple-choice dataset. With this benchmark, we evaluate\nseveral existing LLMs and analyze their performance in the quantum domain,\nincluding sensitivity to changes in question format. QuantumBench is the first\nLLM evaluation dataset built for the quantum domain, and it is intended to\nguide the effective use of LLMs in quantum research."}
{"id": "2511.00206", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00206", "abs": "https://arxiv.org/abs/2511.00206", "authors": ["Dirk U. Wulff", "Rui Mata"], "title": "Advancing Cognitive Science with LLMs", "comment": null, "summary": "Cognitive science faces ongoing challenges in knowledge synthesis and\nconceptual clarity, in part due to its multifaceted and interdisciplinary\nnature. Recent advances in artificial intelligence, particularly the\ndevelopment of large language models (LLMs), offer tools that may help to\naddress these issues. This review examines how LLMs can support areas where the\nfield has historically struggled, including establishing cross-disciplinary\nconnections, formalizing theories, developing clear measurement taxonomies,\nachieving generalizability through integrated modeling frameworks, and\ncapturing contextual and individual variation. We outline the current\ncapabilities and limitations of LLMs in these domains, including potential\npitfalls. Taken together, we conclude that LLMs can serve as tools for a more\nintegrative and cumulative cognitive science when used judiciously to\ncomplement, rather than replace, human expertise."}
{"id": "2511.00379", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00379", "abs": "https://arxiv.org/abs/2511.00379", "authors": ["Jiahao Wang", "Songkai Xue", "Jinghui Li", "Xiaozhen Wang"], "title": "Diverse Human Value Alignment for Large Language Models via Ethical Reasoning", "comment": "Accepted by AIES 2025, camera-ready version", "summary": "Ensuring that Large Language Models (LLMs) align with the diverse and\nevolving human values across different regions and cultures remains a critical\nchallenge in AI ethics. Current alignment approaches often yield superficial\nconformity rather than genuine ethical understanding, failing to address the\ncomplex, context-dependent nature of human values. In this paper, we propose a\nnovel ethical reasoning paradigm for LLMs inspired by well-established ethical\ndecision-making models, aiming at enhancing diverse human value alignment\nthrough deliberative ethical reasoning. Our framework consists of a structured\nfive-step process, including contextual fact gathering, hierarchical social\nnorm identification, option generation, multiple-lens ethical impact analysis,\nand reflection. This theory-grounded approach guides LLMs through an\ninterpretable reasoning process that enhances their ability to understand\nregional specificities and perform nuanced ethical analysis, which can be\nimplemented with either prompt engineering or supervised fine-tuning methods.\nWe perform evaluations on the SafeWorld benchmark that specially designed for\nregional value alignment. Experimental results demonstrate our framework\nsignificantly improves LLM alignment with diverse human values compared to\nbaseline methods, enabling more accurate social norm identification and more\nculturally appropriate reasoning. Our work provides a concrete pathway toward\ndeveloping LLMs that align more effectively with the multifaceted values of\nglobal societies through interdisciplinary research."}
{"id": "2511.00640", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00640", "abs": "https://arxiv.org/abs/2511.00640", "authors": ["Zicheng Xu", "Guanchu Wang", "Yu-Neng Chuang", "Guangyao Zheng", "Alexander S. Szalay", "Zirui Liu", "Vladimir Braverman"], "title": "DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching", "comment": null, "summary": "Large Reasoning Models (LRMs) demonstrate strong performance on complex\nreasoning tasks, yet they often suffer from overthinking, producing excessively\nlong chain-of-thought (CoT) traces that increase inference cost and may degrade\naccuracy. Our analysis reveals a clear anti-correlation between reasoning\nlength and accuracy, where across multiple stochastic decodes, the short\nreasoning paths consistently achieve the highest correctness, while longer ones\naccumulate errors and repetitions. These short optimal reasoning paths can be\nfound ideally through full enumeration of the reasoning space. However, the\ntree-structured reasoning space grows exponentially with sequence length,\nrendering exhaustive exploration infeasible. To address this, we propose DTS, a\nmodel-agnostic decoding framework that sketches the reasoning space by\nselectively branching at high-entropy tokens and applies early stopping to\nselect the shortest completed reasoning path. This approach approximates the\noptimal solution that enhances both efficiency and accuracy, without requiring\nadditional training or supervision. Experiments on AIME2024 and AIME2025\ndatasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves\naccuracy by up to 8%, reduces average reasoning length by 23%, and decreases\nrepetition frequency by 12%, demonstrating DTS's ability for scalable and\nefficient LRM reasoning."}
{"id": "2511.00651", "categories": ["cs.AI", "cs.CL", "cs.IT", "cs.MA", "cs.NI", "math.IT"], "pdf": "https://arxiv.org/pdf/2511.00651", "abs": "https://arxiv.org/abs/2511.00651", "authors": ["Chenhua Shi", "Bhavika Jalli", "Gregor Macdonald", "John Zou", "Wanlu Lei", "Mridul Jain", "Joji Philip"], "title": "Leveraging Multi-Agent System (MAS) and Fine-Tuned Small Language Models (SLMs) for Automated Telecom Network Troubleshooting", "comment": "6 pages, 7 figures, 1 table", "summary": "Telecom networks are rapidly growing in scale and complexity, making\neffective management, operation, and optimization increasingly challenging.\nAlthough Artificial Intelligence (AI) has been applied to many telecom tasks,\nexisting models are often narrow in scope, require large amounts of labeled\ndata, and struggle to generalize across heterogeneous deployments.\nConsequently, network troubleshooting continues to rely heavily on Subject\nMatter Experts (SMEs) to manually correlate various data sources to identify\nroot causes and corrective actions. To address these limitations, we propose a\nMulti-Agent System (MAS) that employs an agentic workflow, with Large Language\nModels (LLMs) coordinating multiple specialized tools for fully automated\nnetwork troubleshooting. Once faults are detected by AI/ML-based monitors, the\nframework dynamically activates agents such as an orchestrator, solution\nplanner, executor, data retriever, and root-cause analyzer to diagnose issues\nand recommend remediation strategies within a short time frame. A key component\nof this system is the solution planner, which generates appropriate remediation\nplans based on internal documentation. To enable this, we fine-tuned a Small\nLanguage Model (SLM) on proprietary troubleshooting documents to produce\ndomain-grounded solution plans. Experimental results demonstrate that the\nproposed framework significantly accelerates troubleshooting automation across\nboth Radio Access Network (RAN) and Core network domains."}
{"id": "2511.00751", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00751", "abs": "https://arxiv.org/abs/2511.00751", "authors": ["Chiyan Loo"], "title": "Reevaluating Self-Consistency Scaling in Multi-Agent Systems", "comment": "7 pages, 3 figures", "summary": "This study examines the trade-offs of increasing sampled reasoning paths in\nself-consistency for modern large language models (LLMs). Earlier research with\nolder models showed that combining multiple reasoning chains improves results\nbefore reaching a plateau. Using Gemini 2.5 models on HotpotQA and Math-500, we\nrevisit those claims under current model conditions. Each configuration pooled\noutputs from varying sampled reasoning paths and compared them to a single\nchain-of-thought (CoT) baseline. Larger models exhibited a more stable and\nconsistent improvement curve. The results confirm that performance gains taper\noff after moderate sampling, aligning with past findings. This plateau suggests\ndiminishing returns driven by overlap among reasoning paths. Self-consistency\nremains useful, but high-sample configurations offer little benefit relative to\ntheir computational cost."}
{"id": "2511.00926", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00926", "abs": "https://arxiv.org/abs/2511.00926", "authors": ["Kyung-Hoon Kim"], "title": "LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory", "comment": "19 pages, 6 figures, 28 models tested across 4,200 trials", "summary": "As Large Language Models (LLMs) grow in capability, do they develop\nself-awareness as an emergent behavior? And if so, can we measure it? We\nintroduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for\nmeasuring self-awareness through strategic differentiation. Using the \"Guess\n2/3 of Average\" game, we test 28 models (OpenAI, Anthropic, Google) across\n4,200 trials with three opponent framings: (A) against humans, (B) against\nother AI models, and (C) against AI models like you. We operationalize\nself-awareness as the capacity to differentiate strategic reasoning based on\nopponent type. Finding 1: Self-awareness emerges with model advancement. The\nmajority of advanced models (21/28, 75%) demonstrate clear self-awareness,\nwhile older/smaller models show no differentiation. Finding 2: Self-aware\nmodels rank themselves as most rational. Among the 21 models with\nself-awareness, a consistent rationality hierarchy emerges: Self > Other AIs >\nHumans, with large AI attribution effects and moderate self-preferencing. These\nfindings reveal that self-awareness is an emergent capability of advanced LLMs,\nand that self-aware models systematically perceive themselves as more rational\nthan humans. This has implications for AI alignment, human-AI collaboration,\nand understanding AI beliefs about human capabilities."}
{"id": "2511.01033", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01033", "abs": "https://arxiv.org/abs/2511.01033", "authors": ["Tiberiu Musat", "Tiago Pimentel", "Lorenzo Noci", "Alessandro Stolfo", "Mrinmaya Sachan", "Thomas Hofmann"], "title": "On the Emergence of Induction Heads for In-Context Learning", "comment": null, "summary": "Transformers have become the dominant architecture for natural language\nprocessing. Part of their success is owed to a remarkable capability known as\nin-context learning (ICL): they can acquire and apply novel associations solely\nfrom their input context, without any updates to their weights. In this work,\nwe study the emergence of induction heads, a previously identified mechanism in\ntwo-layer transformers that is particularly important for in-context learning.\nWe uncover a relatively simple and interpretable structure of the weight\nmatrices implementing the induction head. We theoretically explain the origin\nof this structure using a minimal ICL task formulation and a modified\ntransformer architecture. We give a formal proof that the training dynamics\nremain constrained to a 19-dimensional subspace of the parameter space.\nEmpirically, we validate this constraint while observing that only 3 dimensions\naccount for the emergence of an induction head. By further studying the\ntraining dynamics inside this 3-dimensional subspace, we find that the time\nuntil the emergence of an induction head follows a tight asymptotic bound that\nis quadratic in the input context length."}
