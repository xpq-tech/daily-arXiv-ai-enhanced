<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 43]
- [cs.AI](#cs.AI) [Total: 37]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Human-Level and Beyond: Benchmarking Large Language Models Against Clinical Pharmacists in Prescription Review](https://arxiv.org/abs/2512.02024)
*Yan Yang,Mouxiao Bian,Peiling Li,Bingjian Wen,Ruiyao Chen,Kangkun Mao,Xiaojun Ye,Tianbin Li,Pengcheng Chen,Bing Han,Jie Xu,Kaifeng Qiu,Junyan Wu*

Main category: cs.CL

TL;DR: RxBench是一个用于评估大语言模型在处方审核任务中性能的综合性基准，包含多种题型和处方错误类型，评估了18个先进LLM并发现性能分层，领先模型在某些任务上可匹配或超越药师表现。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在临床决策支持中的快速应用，特别是在处方审核领域，需要一个系统化、细粒度的评估框架来全面衡量LLM的能力和局限性，以促进更可靠的专业化临床工具开发。

Method: 开发了RxBench基准，涵盖常见处方审核类别，整合了14种来自权威药学参考资料的常见处方错误类型。基准包含1,150道单选题、230道多选题和879道简答题，所有题目均由经验丰富的临床药师审核。评估了18个最先进的LLM，并基于基准评估的洞察，对中等水平模型进行了针对性微调。

Result: 评估发现LLM在处方审核任务中存在明显的性能分层：Gemini-2.5-pro-preview-05-06、Grok-4-0709和DeepSeek-R1-0528始终处于第一梯队，在准确性和鲁棒性上优于其他模型。领先LLM在某些任务上可匹配或超越执业药师表现。通过针对性微调中等模型，开发出的专业化模型在简答题任务上可与领先通用LLM相媲美。

Conclusion: RxBench建立了一个标准化的、以错误类型为导向的评估框架，不仅揭示了前沿LLM在处方审核中的能力和局限性，还为构建更可靠、更专业化的临床工具提供了基础资源。该基准有助于推动LLM在临床决策支持中的安全有效应用。

Abstract: The rapid advancement of large language models (LLMs) has accelerated their integration into clinical decision support, particularly in prescription review. To enable systematic and fine-grained evaluation, we developed RxBench, a comprehensive benchmark that covers common prescription review categories and consolidates 14 frequent types of prescription errors drawn from authoritative pharmacy references. RxBench consists of 1,150 single-choice, 230 multiple-choice, and 879 short-answer items, all reviewed by experienced clinical pharmacists. We benchmarked 18 state-of-the-art LLMs and identified clear stratification of performance across tasks. Notably, Gemini-2.5-pro-preview-05-06, Grok-4-0709, and DeepSeek-R1-0528 consistently formed the first tier, outperforming other models in both accuracy and robustness. Comparisons with licensed pharmacists indicated that leading LLMs can match or exceed human performance in certain tasks. Furthermore, building on insights from our benchmark evaluation, we performed targeted fine-tuning on a mid-tier model, resulting in a specialized model that rivals leading general-purpose LLMs in performance on short-answer question tasks. The main contribution of RxBench lies in establishing a standardized, error-type-oriented framework that not only reveals the capabilities and limitations of frontier LLMs in prescription review but also provides a foundational resource for building more reliable and specialized clinical tools.

</details>


### [2] [Deep Research: A Systematic Survey](https://arxiv.org/abs/2512.02038)
*Zhengliang Shi,Yiqun Chen,Haitao Li,Weiwei Sun,Shiyu Ni,Yougang Lyu,Run-Ze Fan,Bowen Jin,Yixuan Weng,Minjun Zhu,Qiujie Xie,Xinyu Guo,Qu Yang,Jiayi Wu,Jujia Zhao,Xiaqiang Tang,Xinbei Ma,Cunxiang Wang,Jiaxin Mao,Qingyao Ai,Jen-Tse Huang,Wenxuan Wang,Yue Zhang,Yiming Yang,Zhaopeng Tu,Zhaochun Ren*

Main category: cs.CL

TL;DR: 这篇论文是关于深度研究系统的综述，系统梳理了将大语言模型与外部工具结合以完成复杂开放任务的研究范式，包括路线图、关键组件、优化技术和未来挑战。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽然强大，但面对需要批判性思维、多源信息和可验证输出的开放任务时，单次提示或标准检索增强生成仍显不足。深度研究旨在结合LLM的推理能力和外部工具，使其成为能够完成复杂任务的研究代理。

Method: 论文采用系统综述方法，首先形式化三阶段路线图，区分深度研究与相关范式；然后介绍四个关键组件（查询规划、信息获取、记忆管理、答案生成）及其细分子分类；总结优化技术（提示工程、监督微调、代理强化学习）；最后整合评估标准和开放挑战。

Result: 论文提供了深度研究领域的全面系统概述，包括清晰的路线图、基础组件、实用实现技术、重要挑战和未来方向，旨在指导和促进该领域的未来发展。

Conclusion: 深度研究是一个快速发展的领域，将LLM与外部工具结合能够显著增强其解决复杂开放任务的能力。论文承诺将持续更新此综述以反映该领域的最新进展。

Abstract: Large language models (LLMs) have rapidly evolved from text generators into powerful problem solvers. Yet, many open tasks demand critical thinking, multi-source, and verifiable outputs, which are beyond single-shot prompting or standard retrieval-augmented generation. Recently, numerous studies have explored Deep Research (DR), which aims to combine the reasoning capabilities of LLMs with external tools, such as search engines, thereby empowering LLMs to act as research agents capable of completing complex, open-ended tasks. This survey presents a comprehensive and systematic overview of deep research systems, including a clear roadmap, foundational components, practical implementation techniques, important challenges, and future directions. Specifically, our main contributions are as follows: (i) we formalize a three-stage roadmap and distinguish deep research from related paradigms; (ii) we introduce four key components: query planning, information acquisition, memory management, and answer generation, each paired with fine-grained sub-taxonomies; (iii) we summarize optimization techniques, including prompting, supervised fine-tuning, and agentic reinforcement learning; and (iv) we consolidate evaluation criteria and open challenges, aiming to guide and facilitate future development. As the field of deep research continues to evolve rapidly, we are committed to continuously updating this survey to reflect the latest progress in this area.

</details>


### [3] [Mirror, Mirror on the Wall -- Which is the Best Model of Them All?](https://arxiv.org/abs/2512.02043)
*Dina Sayed,Heiko Schuldt*

Main category: cs.CL

TL;DR: 该论文分析了LLM模型选择的定量维度，以医疗领域为例研究排行榜和基准测试，并提出系统化的模型选择方法。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型快速发展，为特定用例选择最合适的模型变得越来越复杂。需要同时考虑定性维度（模型适用性）和定量维度（性能表现）。

Method: 通过分析当前LLM排行榜和基准测试，以医疗领域为案例研究，探索模型性能评估的定量维度。

Result: 展示了医疗领域模型评估的演变历程、当前格局和实际意义，揭示了通过排行榜进行模型比较的框架。

Conclusion: 提出了模型选择方法论（MSM），这是一个系统化方法，用于指导根据特定用例导航、优先级排序和选择最合适的模型。

Abstract: Large Language Models (LLMs) have become one of the most transformative tools across many applications, as they have significantly boosted productivity and achieved impressive results in various domains such as finance, healthcare, education, telecommunications, and law, among others. Typically, state-of-the-art (SOTA) foundation models are developed by large corporations based on large data collections and substantial computational and financial resources required to pretrain such models from scratch. These foundation models then serve as the basis for further development and domain adaptation for specific use cases or tasks. However, given the dynamic and fast-paced nature of launching new foundation models, the process of selecting the most suitable model for a particular use case, application, or domain becomes increasingly complex. We argue that there are two main dimensions that need to be taken into consideration when selecting a model for further training: a qualitative dimension (which model is best suited for a task based on information, for instance, taken from model cards) and a quantitative dimension (which is the best performing model). The quantitative performance of models is assessed through leaderboards, which rank models based on standardized benchmarks and provide a consistent framework for comparing different LLMs. In this work, we address the analysis of the quantitative dimension by exploring the current leaderboards and benchmarks. To illustrate this analysis, we focus on the medical domain as a case study, demonstrating the evolution, current landscape, and practical significance of this quantitative evaluation dimension. Finally, we propose a Model Selection Methodology (MSM), a systematic approach designed to guide the navigation, prioritization, and selection of the model that best aligns with a given use case.

</details>


### [4] [Beyond Confidence: Adaptive and Coherent Decoding for Diffusion Language Models](https://arxiv.org/abs/2512.02044)
*Kecheng Chen,Ziru Liu,Xijia Tao,Hui Liu,Xinyu Fu,Suiyun Zhang,Dandan Tu,Lingpeng Kong,Rui Liu,Haoliang Li*

Main category: cs.CL

TL;DR: CCD是一种新的扩散语言模型推理框架，通过轨迹修正机制和自适应采样策略，在提高生成质量的同时加速推理过程。


<details>
  <summary>Details</summary>
Motivation: 现有扩散语言模型推理方法依赖局部即时指标（如置信度或熵），缺乏更可靠的全局视角，导致采样轨迹不一致和生成质量欠佳。

Method: 提出连贯上下文解码（CCD）框架：1）轨迹修正机制利用历史上下文增强序列连贯性，早期拒绝次优路径；2）自适应采样策略根据一致性指标动态调整每个步骤的解码预算。

Result: 在Dream和LLaDA基准测试中，方法在推理速度和性能上同时提升，达到最高3.48倍加速和3.91%性能改进。

Conclusion: CCD通过理论支持的轨迹修正和自适应采样，解决了扩散语言模型推理中的连贯性和效率问题，实现了质量与速度的双重提升。

Abstract: Diffusion Language Models (DLMs) have recently achieved significant success due to their any-order generation capabilities. However, existing inference methods typically rely on local, immediate-step metrics such as confidence or entropy which inherently lack a more reliable perspective. This limitation frequently leads to inconsistent sampling trajectories and suboptimal generation quality. To address this, we propose Coherent Contextual Decoding (CCD), a novel inference framework built upon two core innovations. First, CCD employs a trajectory rectification mechanism that leverages historical context to enhance sequence coherence, enabling the early rejection of suboptimal paths. We demonstrate that this mechanism is theoretically equivalent to modeling the consistency of historical steps via the conditional mutual information between context and token predictions. Building on this theoretical insight, we further address the inefficiency of conventional uniform decoding budgets. Instead of rigid allocations based on diffusion steps, we introduce an adaptive sampling strategy that dynamically adjusts the unmasking budget for each step according to our consistency metric. Consequently, our method significantly improves the quality of generation trajectories while accelerating the sampling process. Empirically, our method achieves a simultaneous enhancement in both inference speed and performance across diverse benchmarks on Dream and LLaDA, delivering up to 3.48x speedup alongside 3.91% performance improvement.

</details>


### [5] [Reversing Large Language Models for Efficient Training and Fine-Tuning](https://arxiv.org/abs/2512.02056)
*Eshed Gal,Moshe Eliasof,Javier Turek,Uri Ascher,Eran Treister,Eldad Haber*

Main category: cs.CL

TL;DR: 提出基于可逆架构的LLM内存优化方法，通过时间可逆动力学减少激活值存储，显著降低内存消耗并提升训练效率。


<details>
  <summary>Details</summary>
Motivation: LLM训练成本高昂且耗时，通常需要对预训练模型进行微调。现有架构需要存储所有中间激活值，导致内存消耗大，限制了批处理大小和训练效率。

Method: 1. 提出基于对称和辛微分方程启发的可逆架构，利用时间可逆动力学在反向传播时重新计算隐藏状态；2. 开发将现有非可逆LLM转换为可逆架构的高效微调方法。

Result: 在多个数据集和基准测试中，多个LLM模型表现出相当或改进的性能，同时显著减少内存消耗，允许在相同内存下处理更大批处理大小，提高吞吐量。

Conclusion: 提出的可逆架构为降低LLM从头训练和微调的内存与计算成本提供了可扩展且高效的路径，具有实际应用价值。

Abstract: Large Language Models (LLMs) are known for their expensive and time-consuming training. Thus, oftentimes, LLMs are fine-tuned to address a specific task, given the pretrained weights of a pre-trained LLM considered a foundation model. In this work, we introduce memory-efficient, reversible architectures for LLMs, inspired by symmetric and symplectic differential equations, and investigate their theoretical properties. Different from standard, baseline architectures that store all intermediate activations, the proposed models use time-reversible dynamics to retrieve hidden states during backpropagation, relieving the need to store activations. This property allows for a drastic reduction in memory consumption, allowing for the processing of larger batch sizes for the same available memory, thereby offering improved throughput. In addition, we propose an efficient method for converting existing, non-reversible LLMs into reversible architectures through fine-tuning, rendering our approach practical for exploiting existing pre-trained models. Our results show comparable or improved performance on several datasets and benchmarks, on several LLMs, building a scalable and efficient path towards reducing the memory and computational costs associated with both training from scratch and fine-tuning of LLMs.

</details>


### [6] [Dialect Identification Using Resource-Efficient Fine-Tuning Approaches](https://arxiv.org/abs/2512.02074)
*Zirui Lin,Haris Gulzar,Monnika Roslianna Busto,Akiko Masaki,Takeharu Eda,Kazuhiro Nakadai*

Main category: cs.CL

TL;DR: 该论文探索内存高效微调方法在语音方言识别任务中的应用，显著减少GPU内存使用并加速训练，同时保持准确率。


<details>
  <summary>Details</summary>
Motivation: 方言识别任务对下游语音任务有帮助，但传统微调方法计算成本和内存需求高，现有参数高效微调方法在内存效率和训练速度方面改进有限。

Method: 探索内存高效微调方法，将其应用于通用预训练语音模型，全面分析不同MEFT方法的GPU内存使用和微调速度，以Whisper模型在KeSpeech数据集上识别六种普通话子方言作为案例研究。

Result: 将GPU内存使用减少高达73.25%，训练速度提升2.1倍，同时保持与原始微调和PEFT方法相当的准确率。

Conclusion: 内存高效微调方法在语音方言识别任务中能显著提高内存效率和训练速度，为语音模型的轻量化微调提供了有效解决方案。

Abstract: Dialect Identification (DI) is a task to recognize different dialects within the same language from a speech signal. DI can help to improve the downstream speech related tasks even when speakers have a strong dialect. However, fine-tuning a speech model for tasks like DI is expensive in terms of computation cost and memory requirement. Recent studies have explored fine-tuning pre-trained speech models for tasks like DI using Parameter-Efficient Fine-Tuning (PEFT) methods, which offer parameter efficiency but limited improvement in memory efficiency and training speed. To address these challenges, we explore Memory-Efficient Fine-Tuning (MEFT) methods, originally proposed for language processing, and apply them to the general-purpose pre-trained speech model. We then comprehensively analyze the GPU memory usage and fine-tuning speed based on various MEFT methods. As a case study, we fine-tune the Whisper model to identify six Mandarin subdialects from the KeSpeech dataset, reducing GPU memory usage by up to 73.25% and accelerating training speed by a factor of 2.1, while maintaining accuracy comparable to vanilla fine-tuning and PEFT methods.

</details>


### [7] [Feature Selection Empowered BERT for Detection of Hate Speech with Vocabulary Augmentation](https://arxiv.org/abs/2512.02141)
*Pritish N. Desai,Tanay Kewalramani,Srimanta Mandal*

Main category: cs.CL

TL;DR: 提出一种数据高效的BERT微调策略，通过TF-IDF样本选择减少75%训练数据，并增强词汇表以适应仇恨言论术语，在保持性能的同时提升计算效率


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的辱骂性言论持续演变，新俚语和混淆术语不断出现以规避检测系统，现有方法需要大量训练数据且难以适应词汇变化

Method: 1) 使用TF-IDF样本选择机制保留最具信息量的75%训练样本；2) 增强BERT分词器，添加领域特定俚语和词汇变体以捕捉仇恨言论术语

Result: 在广泛使用的仇恨言论数据集上，该方法在显著减少训练数据量的情况下实现了竞争性性能，同时提高了计算效率

Conclusion: 该方法为可扩展和自适应的辱骂内容审核提供了有效解决方案，通过数据高效微调和词汇增强平衡了性能与计算成本

Abstract: Abusive speech on social media poses a persistent and evolving challenge, driven by the continuous emergence of novel slang and obfuscated terms designed to circumvent detection systems. In this work, we present a data efficient strategy for fine tuning BERT on hate speech classification by significantly reducing training set size without compromising performance. Our approach employs a TF IDF-based sample selection mechanism to retain only the most informative 75 percent of examples, thereby minimizing training overhead. To address the limitations of BERT's native vocabulary in capturing evolving hate speech terminology, we augment the tokenizer with domain-specific slang and lexical variants commonly found in abusive contexts. Experimental results on a widely used hate speech dataset demonstrate that our method achieves competitive performance while improving computational efficiency, highlighting its potential for scalable and adaptive abusive content moderation.

</details>


### [8] [Think Before You Prune: Self-Reflective Structured Pruning for Reasoning Language Models](https://arxiv.org/abs/2512.02185)
*Ziyan Wang,Enmao Diao,Qi Le,Pu Wang,Guanchu Wang,Minwoo Lee,Shu-ping Yeh,Li Yang*

Main category: cs.CL

TL;DR: RESP是一种针对推理LLMs的自反思结构化剪枝框架，通过自生成校准、解码梯度重要性估计和渐进再生，在保持推理能力的同时实现高稀疏度剪枝。


<details>
  <summary>Details</summary>
Motivation: 推理LLMs（如OpenAI o1、DeepSeek-R1、Qwen3）虽然推理能力强，但模型大、推理成本高，不适合资源受限环境。现有剪枝方法对推理LLMs效果差，即使中等稀疏度也会严重损害推理能力。

Method: RESP框架包含三个核心组件：1）自生成校准：使用模型自身生成的推理轨迹作为校准数据；2）解码梯度重要性估计：基于解码时的梯度信息评估参数重要性；3）渐进再生：随着稀疏度增加，保持校准保真度。

Result: 在Qwen3-8B上，RESP在20-30%稀疏度下保持接近稠密模型的准确率，在40%稀疏度下，GSM8K达到81.3%准确率（比最强基线高66.87%），MathQA达到59.6%准确率（比最强基线高47%）。

Conclusion: RESP通过将剪枝决策与模型的推理动态对齐，显著提升了推理LLMs的剪枝效果，为在资源受限环境中部署高效推理模型提供了可行方案。

Abstract: Reasoning LLMs (RLMs) such as OpenAI o1, DeepSeek-R1, and Qwen3 deliver strong multi-step reasoning through chain-of-thought generation, but their large model sizes and lengthy decode-time outputs make them costly to deploy and unsuitable for resource-constrained settings. To reduce computing and memory cost, pruning offers a promising solution by removing unimportant parameters. However, despite their success on standard LLMs, existing pruning methods severely damage RLMs, as even moderate sparsity (e.g., 20%) can collapse accuracy and completely disrupt the model's reasoning coherence. We begin by analyzing why existing pruning pipelines fail on reasoning LLMs and find that their brittleness largely stems from a mismatch between the calibration data, the pruning objective, and the model's decode-time reasoning behavior. Our study further shows that the most reliable calibration signal comes not from human-written labels but from the model's own self-generated reasoning traces, which more accurately reflect its inference distribution. Guided by these insights, we introduce RESP, a self-reflective structured pruning framework that aligns pruning decisions with the model's reasoning dynamics through self-generated calibration, decode-only gradient-based importance estimation, and progressive regeneration that maintains calibration fidelity as sparsity increases. Experiments on Qwen3-8B demonstrate that RESP markedly outperforms existing structured pruning methods on both GSM8K and MathQA, preserving near-dense accuracy at 20-30% sparsity and substantially mitigating performance collapse at higher sparsity levels. At 40% sparsity, RESP attains 81.3% accuracy on GSM8K and 59.6% on MathQA, surpassing the strongest baselines by 66.87% and 47%, respectively.

</details>


### [9] [A Knowledge-Based Language Model: Deducing Grammatical Knowledge in a Multi-Agent Language Acquisition Simulation](https://arxiv.org/abs/2512.02195)
*David Ph. Shakouri,Crit Cremers,Niels O. Schiller*

Main category: cs.CL

TL;DR: MODOMA系统是一个用于无监督语言习得实验的计算多智能体实验室环境，通过成人与儿童智能体交互实现语言习得，能够获取和表示功能与内容范畴。


<details>
  <summary>Details</summary>
Motivation: 构建一个参数化、可控的计算语言习得实验环境，使研究人员能够明确表示和查询习得的语法知识，为计算语言习得研究提供新可能性。

Method: 采用多智能体框架，包含成人和儿童两个语言模型智能体，结合统计和基于规则的程序进行无监督语言习得，系统完全参数化，研究人员可控制实验所有方面。

Result: 儿童智能体能够基于成人智能体生成的不同数量示例数据，成功习得功能和内容范畴；机器生成数据中发现了与人类数据相似的模式，验证了MODOMA方法的有效性。

Conclusion: MODOMA系统为计算语言习得实验提供了新颖的可能性，成功实现了离散语法范畴的习得，验证了该建模语言习得方法的有效性。

Abstract: This paper presents an initial study performed by the MODOMA system. The MODOMA is a computational multi-agent laboratory environment for unsupervised language acquisition experiments such that acquisition is based on the interaction between two language models, an adult and a child agent. Although this framework employs statistical as well as rule-based procedures, the result of language acquisition is a knowledge-based language model, which can be used to generate and parse new utterances of the target language. This system is fully parametrized and researchers can control all aspects of the experiments while the results of language acquisition, that is, the acquired grammatical knowledge, are explicitly represented and can be consulted. Thus, this system introduces novel possibilities for conducting computational language acquisition experiments. The experiments presented by this paper demonstrate that functional and content categories can be acquired and represented by the daughter agent based on training and test data containing different amounts of exemplars generated by the adult agent. Interestingly, similar patterns, which are well-established for human-generated data, are also found for these machine-generated data. As the procedures resulted in the successful acquisition of discrete grammatical categories by the child agent, these experiments substantiate the validity of the MODOMA approach to modelling language acquisition.

</details>


### [10] [Swivuriso: The South African Next Voices Multilingual Speech Dataset](https://arxiv.org/abs/2512.02201)
*Vukosi Marivatee,Kayode Olaleye,Sitwala Mundia,Andinda Bakainga,Unarine Netshifhefhe,Mahmooda Milanzie,Tsholofelo Hope Mogale,Thapelo Sindane,Zainab Abdulrasaq,Kesego Mokgosi,Chijioke Okorie,Nia Zion Van Wyk,Graham Morrissey,Dale Dunbar,Francois Smit,Tsosheletso Chidi,Rooweither Mabuya,Andiswa Bukula,Respect Mlambo,Tebogo Macucwa,Idris Abdulmumin,and Seani Rananga*

Main category: cs.CL

TL;DR: Swivuriso是一个3000小时的多语言语音数据集，支持7种南非语言的自动语音识别技术开发和基准测试


<details>
  <summary>Details</summary>
Motivation: 解决现有ASR数据集在非洲语言方面的显著空白，支持南非语言自动语音识别技术的发展

Method: 开发了包含农业、医疗和通用领域的多语言语音数据集，描述了设计原则、伦理考虑和数据收集流程

Result: 提供了使用该数据训练/微调ASR模型的基线结果，并与其他相关语言的ASR数据集进行了比较

Conclusion: Swivuriso数据集填补了非洲语言ASR资源的空白，为相关语言的语音识别技术发展提供了重要支持

Abstract: This paper introduces Swivuriso, a 3000-hour multilingual speech dataset developed as part of the African Next Voices project, to support the development and benchmarking of automatic speech recognition (ASR) technologies in seven South African languages. Covering agriculture, healthcare, and general domain topics, Swivuriso addresses significant gaps in existing ASR datasets. We describe the design principles, ethical considerations, and data collection procedures that guided the dataset creation. We present baseline results of training/finetuning ASR models with this data and compare to other ASR datasets for the langauges concerned.

</details>


### [11] [Lightweight Latent Reasoning for Narrative Tasks](https://arxiv.org/abs/2512.02240)
*Alexander Gurung,Nikolay Malkin,Mirella Lapata*

Main category: cs.CL

TL;DR: LiteReason是一种轻量级潜在推理方法，通过连续潜在令牌跳过推理步骤，在减少77-92%推理长度的同时接近非潜在RL训练性能


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通过生成长推理链处理复杂任务，但使用强化学习优化推理能力计算成本高，特别是在涉及大量令牌处理的叙事相关任务中

Method: 提出LiteReason方法，包含轻量级推理投影器模块，生成连续潜在令牌帮助模型"跳过"推理步骤；在RL训练中，策略模型决定何时激活投影器，在潜在推理和离散推理之间切换

Result: 在情节漏洞检测和书籍章节生成任务上，该方法优于潜在推理基线，接近非潜在RL训练性能，同时减少最终推理长度77-92%

Conclusion: LiteReason引导RL训练达到性能-计算权衡曲线中更高效的部分，在保持性能的同时显著减少推理长度

Abstract: Large language models (LLMs) tackle complex tasks by generating long chains of thought or "reasoning traces" that act as latent variables in the generation of an output given a query. A model's ability to generate such traces can be optimized with reinforcement learning (RL) to improve their utility in predicting an answer. This optimization comes at a high computational cost, especially for narrative-related tasks that involve retrieving and processing many tokens. To this end, we propose LiteReason, a latent reasoning method that can be interleaved with standard token sampling and easily combined with RL techniques. LiteReason employs a lightweight Reasoning Projector module, trained to produce continuous latent tokens that help the model 'skip' reasoning steps. During RL, the policy model decides when to activate the projector, switching between latent and discrete reasoning as needed. Experimental results on plot hole detection and book chapter generation show that our method outperforms latent reasoning baselines and comes close to matching non-latent RL training, while reducing final reasoning length by 77-92%. Overall, LiteReason guides RL training to a more efficient part of the performance-computation tradeoff curve.

</details>


### [12] [DETAIL Matters: Measuring the Impact of Prompt Specificity on Reasoning in Large Language Models](https://arxiv.org/abs/2512.02246)
*Olivia Kim*

Main category: cs.CL

TL;DR: 论文提出DETAIL框架，通过多层级提示词设计评估LLM在不同提示词具体程度下的推理性能，发现具体性提示能提升准确性，尤其对小模型和流程性任务效果显著。


<details>
  <summary>Details</summary>
Motivation: 提示词设计对大型语言模型的推理性能至关重要，但提示词具体程度（详细或模糊）的影响尚未得到充分研究。当前缺乏系统评估不同具体程度提示词对LLM性能影响的框架和方法。

Method: 提出DETAIL框架：1) 使用GPT-4生成多层级具体程度的提示词；2) 通过困惑度量化提示词具体程度；3) 使用基于GPT的语义等价性评估模型回答的正确性；4) 在30个新颖推理任务上对GPT-4和O3-mini进行实验。

Result: 实验结果表明：1) 提示词具体程度越高，模型准确性越好；2) 较小模型从具体提示中获益更大；3) 流程性任务对提示词具体性更敏感；4) 需要自适应提示策略来优化不同模型和任务的表现。

Conclusion: 提示词具体性显著影响LLM推理性能，需要针对不同模型和任务设计自适应提示策略。该研究提供了评估框架、工具和数据，支持进一步研究提示词设计优化。

Abstract: Prompt design plays a critical role in the reasoning performance of large language models (LLMs), yet the impact of prompt specificity - how detailed or vague a prompt is - remains understudied. This paper introduces DETAIL, a framework for evaluating LLM performance across varying levels of prompt specificity. We generate multi-level prompts using GPT-4, quantify specificity via perplexity, and assess correctness using GPT-based semantic equivalence. Experiments on 30 novel reasoning tasks across GPT-4 and O3-mini reveal that specificity improves accuracy, especially for smaller models and procedural tasks. Our results highlight the need for adaptive prompting strategies and provide tools and data to support further research.

</details>


### [13] [CAIRNS: Balancing Readability and Scientific Accuracy in Climate Adaptation Question Answering](https://arxiv.org/abs/2512.02251)
*Liangji Kong,Aditya Joshi,Sarvnaz Karimi*

Main category: cs.CL

TL;DR: CAIRNS框架通过结构化ScholarGuide提示和一致性加权混合评估器，从非结构化和结构化数据中为农业气候适应提供可读、可验证的问答系统，无需微调或强化学习。


<details>
  <summary>Details</summary>
Motivation: 农业领域的气候适应策略信息分散在非结构化（科学文献）和结构化（政府API数据）数据源中，专家难以快速获取可信的初步答案，需要一种能提高可读性和引用可靠性的问答框架。

Method: 提出CAIRNS框架：1）使用结构化ScholarGuide提示增强可读性和引用可靠性；2）采用一致性加权混合评估器，利用模型间一致性进行鲁棒评估；3）无需微调或强化学习。

Result: 在专家策划的问答数据集上，CAIRNS在大多数指标上优于基线方法，消融研究在所有指标上确认了结果，LLM评估与人类判断具有相关性。

Conclusion: CAIRNS框架能够为农业气候适应提供可读、可验证且基于领域知识的问答，帮助农业顾问从复杂网络数据源获取可信的初步答案。

Abstract: Climate adaptation strategies are proposed in response to climate change. They are practised in agriculture to sustain food production. These strategies can be found in unstructured data (for example, scientific literature from the Elsevier website) or structured (heterogeneous climate data via government APIs). We present Climate Adaptation question-answering with Improved Readability and Noted Sources (CAIRNS), a framework that enables experts -- farmer advisors -- to obtain credible preliminary answers from complex evidence sources from the web. It enhances readability and citation reliability through a structured ScholarGuide prompt and achieves robust evaluation via a consistency-weighted hybrid evaluator that leverages inter-model agreement with experts. Together, these components enable readable, verifiable, and domain-grounded question-answering without fine-tuning or reinforcement learning. Using a previously reported dataset of expert-curated question-answers, we show that CAIRNS outperforms the baselines on most of the metrics. Our thorough ablation study confirms the results on all metrics. To validate our LLM-based evaluation, we also report an analysis of correlations against human judgment.

</details>


### [14] [HealthContradict: Evaluating Biomedical Knowledge Conflicts in Language Models](https://arxiv.org/abs/2512.02299)
*Boya Zhang,Alban Bornet,Rui Yang,Nan Liu,Douglas Teodoro*

Main category: cs.CL

TL;DR: 研究人员创建了HealthContradict数据集来评估语言模型在长且矛盾的生物医学上下文中的推理能力，发现经过微调的生物医学模型既能有效利用正确上下文，又能抵抗错误上下文的影响。


<details>
  <summary>Details</summary>
Motivation: 评估语言模型如何使用上下文信息回答健康问题，特别是在面对矛盾上下文时的影响。现有医学问答评估基准在区分模型上下文推理能力方面不足，需要更精细的评估工具。

Method: 创建HealthContradict数据集（920个专家验证实例，每个包含健康问题、科学证据支持的事实答案和两个矛盾立场的文档），采用多种提示设置（正确、错误或矛盾上下文），测量对模型输出的影响。

Result: HealthContradict相比现有医学问答基准能更好地区分语言模型的上下文推理能力。实验表明，经过微调的生物医学语言模型的优势不仅在于预训练获得的参数知识，还在于它们利用正确上下文同时抵抗错误上下文的能力。

Conclusion: 该研究提供了评估语言模型在矛盾生物医学上下文中推理能力的新工具，揭示了微调生物医学模型在上下文利用和抵抗错误信息方面的双重优势，对医疗AI应用有重要意义。

Abstract: How do language models use contextual information to answer health questions? How are their responses impacted by conflicting contexts? We assess the ability of language models to reason over long, conflicting biomedical contexts using HealthContradict, an expert-verified dataset comprising 920 unique instances, each consisting of a health-related question, a factual answer supported by scientific evidence, and two documents presenting contradictory stances. We consider several prompt settings, including correct, incorrect or contradictory context, and measure their impact on model outputs. Compared to existing medical question-answering evaluation benchmarks, HealthContradict provides greater distinctions of language models' contextual reasoning capabilities. Our experiments show that the strength of fine-tuned biomedical language models lies not only in their parametric knowledge from pretraining, but also in their ability to exploit correct context while resisting incorrect context.

</details>


### [15] [When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers](https://arxiv.org/abs/2512.02304)
*Jack Lu,Ryan Teehan,Jinran Jin,Mengye Ren*

Main category: cs.CL

TL;DR: 本文系统研究了37个LLM在9个基准测试上的验证能力，发现跨模型族验证效果最好，后训练会降低自我改进但增强跨族改进，数学和逻辑任务具有最高的可验证性。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLM作为问题求解器和解决方案验证器的交互作用研究有限，主要关注自我验证，很少研究验证器如何判断来自同一模型族或不同模型族的输出。现代LLM经过广泛的后训练，但其对验证的影响尚不清楚。

Method: 使用37个模型（涵盖多个模型族、不同规模和基础vs后训练变体）在9个基准测试上进行系统研究，比较自我验证、同族内验证和跨族验证。引入并经验验证了验证器增益指标，分析该指标和假阳性率如何随模型规模和后训练变化，并表征数据集可验证性的差异。

Result: 跨模型族验证特别有效；后训练会降低自我改进但增强跨族改进；数学和逻辑任务表现出最高的固有可验证性；验证器增益能有效预测测试时基于验证器的拒绝采样带来的性能改进。

Conclusion: LLM验证能力存在系统性差异，跨模型族验证是提高性能的有效策略，后训练改变了模型的验证特性，不同任务类型的可验证性不同，这些发现对构建更可靠的LLM系统具有重要意义。

Abstract: Large language models (LLMs) can act as both problem solvers and solution verifiers, with verifiers improving solver performance by selecting high-quality answers from a pool of candidates. However, prior studies of solver-verifier interactions have been limited, focusing mainly on self-verification and rarely examining how verifiers judge outputs from models in their own or in another model family. Modern LLMs also undergo extensive post-training, but its effect on verification remains unclear. We present a systematic study across 37 models spanning multiple families, sizes, and base vs. post-trained variants, evaluated on 9 benchmarks covering logical reasoning, structured puzzles, symbolic computation, mathematics, commonsense, factual recall, and domain knowledge. We compare self-verification with verification within the same family and across different families. To support this, we introduce and empirically validate verifier gain, a metric that predicts the performance improvements from test-time verifier-based rejection sampling. We analyze how metrics like verifier gain and false positive rate scale with model size and post-training, and characterize differences in dataset verifiability. Our findings show that cross-family verification is especially effective; post-training reduces self-improvement but strengthens cross-family improvement; and mathematical and logical tasks exhibit the highest inherent verifiability.

</details>


### [16] [Memory-Augmented Knowledge Fusion with Safety-Aware Decoding for Domain-Adaptive Question Answering](https://arxiv.org/abs/2512.02363)
*Lei Fu,Xiang Chen,Kaige Gao Xinyue Huang,Kejian Tong*

Main category: cs.CL

TL;DR: KARMA框架通过双编码器架构、门控记忆单元和安全感知解码器，提升服务领域问答系统的准确性和安全性


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在医疗政策、政府福利等敏感领域的问答中存在事实一致性和上下文对齐问题，需要构建可信赖的自适应问答系统

Method: 提出KARMA框架，包含：1) 双编码器融合结构化和非结构化知识源；2) 门控记忆单元动态调节外部知识集成；3) 安全感知可控解码器，使用安全分类和引导生成技术减少不安全输出

Result: 在专有问答数据集上的实验表明，KARMA在答案质量和安全性方面均优于强基线模型

Conclusion: KARMA为服务场景中构建可信赖和自适应的问答系统提供了全面解决方案

Abstract: Domain-specific question answering (QA) systems for services face unique challenges in integrating heterogeneous knowledge sources while ensuring both accuracy and safety. Existing large language models often struggle with factual consistency and context alignment in sensitive domains such as healthcare policies and government welfare. In this work, we introduce Knowledge-Aware Reasoning and Memory-Augmented Adaptation (KARMA), a novel framework designed to enhance QA performance in care scenarios. KARMA incorporates a dual-encoder architecture to fuse structured and unstructured knowledge sources, a gated memory unit to dynamically regulate external knowledge integration, and a safety-aware controllable decoder that mitigates unsafe outputs using safety classification and guided generation techniques. Extensive experiments on a proprietary QA dataset demonstrate that KARMA outperforms strong baselines in both answer quality and safety. This study offers a comprehensive solution for building trustworthy and adaptive QA systems in service contexts.

</details>


### [17] [TaleFrame: An Interactive Story Generation System with Fine-Grained Control and Large Language Models](https://arxiv.org/abs/2512.02402)
*Yunchao Wang,Guodao Sun,Zihang Fu,Zhehao Liu,Kaixing Du,Haidong Gao,Ronghua Liang*

Main category: cs.CL

TL;DR: TaleFrame是一个结合LLM和HCI的故事生成系统，通过将故事结构分解为实体、事件、关系和故事大纲四个基本单元，实现精细控制，并提供了直观的交互界面。


<details>
  <summary>Details</summary>
Motivation: 现有故事生成系统难以准确地将用户意图转化为满意的故事输出，主要原因是缺乏细粒度控制和输入规范不明确，限制了系统的实用性。

Method: 1. 将故事结构分解为四个基本单元：实体、事件、关系和故事大纲；2. 使用Tinystories数据集构建包含9,851个JSON条目的偏好数据集；3. 微调本地Llama模型实现JSON2Story转换；4. 提供支持拖放、连接等交互的直观界面；5. 在七个维度上评估生成的故事并提供改进建议。

Result: 通过定量评估和用户研究证明了TaleFrame的有效性，数据集已在Hugging Face上公开。

Conclusion: TaleFrame通过结构化信息生成故事，实现了对生成过程的精确控制，解决了现有系统在意图翻译和细粒度控制方面的不足，为创意故事生成提供了实用解决方案。

Abstract: With the advancement of natural language generation (NLG) technologies, creative story generation systems have gained increasing attention. However, current systems often fail to accurately translate user intent into satisfactory story outputs due to a lack of fine-grained control and unclear input specifications, limiting their applicability. To address this, we propose TaleFrame, a system that combines large language models (LLMs) with human-computer interaction (HCI) to generate stories through structured information, enabling precise control over the generation process. The innovation of TaleFrame lies in decomposing the story structure into four basic units: entities, events, relationships, and story outline. We leverage the Tinystories dataset, parsing and constructing a preference dataset consisting of 9,851 JSON-formatted entries, which is then used to fine-tune a local Llama model. By employing this JSON2Story approach, structured data is transformed into coherent stories. TaleFrame also offers an intuitive interface that supports users in creating and editing entities and events and generates stories through the structured framework. Users can control these units through simple interactions (e.g., drag-and-drop, attach, and connect), thus influencing the details and progression of the story. The generated stories can be evaluated across seven dimensions (e.g., creativity, structural integrity), with the system providing suggestions for refinement based on these evaluations. Users can iteratively adjust the story until a satisfactory result is achieved. Finally, we conduct quantitative evaluation and user studies that demonstrate the usefulness of TaleFrame. Dataset available at https://huggingface.co/datasets/guodaosun/tale-frame.

</details>


### [18] [A Concise Review of Hallucinations in LLMs and their Mitigation](https://arxiv.org/abs/2512.02527)
*Parth Pulkundwar,Vivek Dhanawade,Rohit Yadav,Minal Sonkar,Medha Asurlekar,Sarita Rathod*

Main category: cs.CL

TL;DR: 本文综述了语言模型中的幻觉问题，分析了不同类型、成因及缓解方法，为理解与减少幻觉提供一站式资源


<details>
  <summary>Details</summary>
Motivation: 传统语言模型面临幻觉问题的严峻挑战，这些幻觉对自然语言处理领域构成了巨大危险，迫切需要理解当前存在的各种幻觉类型、产生原因以及减少方法

Method: 本文采用综述分析方法，系统性地梳理和总结了语言模型幻觉的不同类型、产生根源以及现有的缓解策略

Result: 提供了一个简洁明了的幻觉问题总结，涵盖了幻觉的分类、成因和缓解方法，成为理解该问题的综合性资源

Conclusion: 理解语言模型幻觉的类型、成因并采取相应缓解措施对于自然语言处理领域至关重要，本文为此提供了有价值的参考框架

Abstract: Traditional language models face a challenge from hallucinations. Their very presence casts a large, dangerous shadow over the promising realm of natural language processing. It becomes crucial to understand the various kinds of hallucinations that occur nowadays, their origins, and ways of reducing them. This document provides a concise and straightforward summary of that. It serves as a one-stop resource for a general understanding of hallucinations and how to mitigate them.

</details>


### [19] [What Signals Really Matter for Misinformation Tasks? Evaluating Fake-News Detection and Virality Prediction under Real-World Constraints](https://arxiv.org/abs/2512.02552)
*Francesco Paolo Savatteri,Chahan Vidal-Gorène,Florian Cafiero*

Main category: cs.CL

TL;DR: 本文评估了假新闻检测和病毒性预测两个实际任务，发现文本内容对假新闻检测效果显著，而病毒性预测更困难且对标签定义敏感，轻量级数值特征在计算受限时仍可用。


<details>
  <summary>Details</summary>
Motivation: 研究在线虚假信息的两个实际任务：假新闻检测和病毒性预测，在需要快速响应的操作环境中，评估不同方法的有效性，为实际应用提供指导。

Method: 使用EVONS和FakeNewsNet数据集，比较文本嵌入（RoBERTa、Mistral）与轻量级数值特征（时间、粉丝数、验证状态、点赞数）以及序列模型（GRU、门控架构、Transformer编码器），进行降维分析（t-SNE vs PCA）。

Result: 文本内容对假新闻检测效果显著；数值特征在语言模型不可用或计算受限时仍可行；病毒性预测比假新闻检测困难得多，对标签构建高度敏感；降维分析显示非线性结构对病毒性预测更有信息量；RoBERTa与Mistral嵌入差异不大。

Conclusion: 假新闻检测可依赖文本内容，病毒性预测需要更精细的标签定义和特征处理，轻量级数值特征在资源受限时是可行的替代方案，评估设计和可重复性约束对研究领域有重要影响。

Abstract: We present an evaluation-driven study of two practical tasks regarding online misinformation: (i) fake-news detection and (ii) virality prediction in the context of operational settings, with the necessity for rapid reaction. Using the EVONS and FakeNewsNet datasets, we compare textual embeddings (RoBERTa; with a control using Mistral) against lightweight numeric features (timing, follower counts, verification, likes) and sequence models (GRU, gating architectures, Transformer encoders). We show that textual content alone is a strong discriminator for fake-news detection, while numeric-only pipelines remain viable when language models are unavailable or compute is constrained. Virality prediction is markedly harder than fake-news detection and is highly sensitive to label construction; in our setup, a median-based ''viral'' split (<50 likes) is pragmatic but underestimates real-world virality, and time-censoring for engagement features is desirable yet difficult under current API limits. Dimensionality-reduction analyses suggest non-linear structure is more informative for virality than for fake-news detection (t-SNE > PCA on numeric features). Swapping RoBERTa for Mistral embeddings yields only modest deltas, leaving conclusions unchanged. We discuss implications for evaluation design and report reproducibility constraints that realistically affect the field. We release splits and code where possible and provide guidance for metric selection.

</details>


### [20] [ADORE: Autonomous Domain-Oriented Relevance Engine for E-commerce](https://arxiv.org/abs/2512.02555)
*Zheng Fang,Donghao Xie,Ming Pang,Chunyuan Yuan,Xue Jiang,Changping Peng,Zhangang Lin,Zheng Luo*

Main category: cs.CL

TL;DR: ADORE是一个自持框架，通过结合规则感知相关性判别、错误类型感知数据合成和关键属性增强知识蒸馏，解决电商搜索中语义鸿沟和硬样本稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 电商搜索中的相关性建模面临两个主要挑战：传统词匹配方法（如BM25）存在语义鸿沟，而神经模型依赖领域特定硬样本的稀缺性。需要一种资源高效且认知对齐的解决方案。

Method: ADORE框架包含三个创新模块：1) 规则感知相关性判别模块，使用思维链LLM生成意图对齐的训练数据，并通过Kahneman-Tversky优化对齐用户行为；2) 错误类型感知数据合成模块，自动生成对抗样本来增强鲁棒性；3) 关键属性增强知识蒸馏模块，将领域特定属性层次注入可部署的学生模型。

Result: 大规模实验和在线A/B测试验证了ADORE的有效性。该框架克服了数据稀缺问题，同时增强了推理能力。

Conclusion: ADORE为工业应用中的资源高效、认知对齐的相关性建模建立了新范式，自动化了标注、对抗生成和蒸馏过程。

Abstract: Relevance modeling in e-commerce search remains challenged by semantic gaps in term-matching methods (e.g., BM25) and neural models' reliance on the scarcity of domain-specific hard samples. We propose ADORE, a self-sustaining framework that synergizes three innovations: (1) A Rule-aware Relevance Discrimination module, where a Chain-of-Thought LLM generates intent-aligned training data, refined via Kahneman-Tversky Optimization (KTO) to align with user behavior; (2) An Error-type-aware Data Synthesis module that auto-generates adversarial examples to harden robustness; and (3) A Key-attribute-enhanced Knowledge Distillation module that injects domain-specific attribute hierarchies into a deployable student model. ADORE automates annotation, adversarial generation, and distillation, overcoming data scarcity while enhancing reasoning. Large-scale experiments and online A/B testing verify the effectiveness of ADORE. The framework establishes a new paradigm for resource-efficient, cognitively aligned relevance modeling in industrial applications.

</details>


### [21] [DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models](https://arxiv.org/abs/2512.02556)
*DeepSeek-AI,Aixin Liu,Aoxue Mei,Bangcai Lin,Bing Xue,Bingxuan Wang,Bingzheng Xu,Bochao Wu,Bowei Zhang,Chaofan Lin,Chen Dong,Chengda Lu,Chenggang Zhao,Chengqi Deng,Chenhao Xu,Chong Ruan,Damai Dai,Daya Guo,Dejian Yang,Deli Chen,Erhang Li,Fangqi Zhou,Fangyun Lin,Fucong Dai,Guangbo Hao,Guanting Chen,Guowei Li,H. Zhang,Hanwei Xu,Hao Li,Haofen Liang,Haoran Wei,Haowei Zhang,Haowen Luo,Haozhe Ji,Honghui Ding,Hongxuan Tang,Huanqi Cao,Huazuo Gao,Hui Qu,Hui Zeng,Jialiang Huang,Jiashi Li,Jiaxin Xu,Jiewen Hu,Jingchang Chen,Jingting Xiang,Jingyang Yuan,Jingyuan Cheng,Jinhua Zhu,Jun Ran,Junguang Jiang,Junjie Qiu,Junlong Li,Junxiao Song,Kai Dong,Kaige Gao,Kang Guan,Kexin Huang,Kexing Zhou,Kezhao Huang,Kuai Yu,Lean Wang,Lecong Zhang,Lei Wang,Liang Zhao,Liangsheng Yin,Lihua Guo,Lingxiao Luo,Linwang Ma,Litong Wang,Liyue Zhang,M. S. Di,M. Y Xu,Mingchuan Zhang,Minghua Zhang,Minghui Tang,Mingxu Zhou,Panpan Huang,Peixin Cong,Peiyi Wang,Qiancheng Wang,Qihao Zhu,Qingyang Li,Qinyu Chen,Qiushi Du,Ruiling Xu,Ruiqi Ge,Ruisong Zhang,Ruizhe Pan,Runji Wang,Runqiu Yin,Runxin Xu,Ruomeng Shen,Ruoyu Zhang,S. H. Liu,Shanghao Lu,Shangyan Zhou,Shanhuang Chen,Shaofei Cai,Shaoyuan Chen,Shengding Hu,Shengyu Liu,Shiqiang Hu,Shirong Ma,Shiyu Wang,Shuiping Yu,Shunfeng Zhou,Shuting Pan,Songyang Zhou,Tao Ni,Tao Yun,Tian Pei,Tian Ye,Tianyuan Yue,Wangding Zeng,Wen Liu,Wenfeng Liang,Wenjie Pang,Wenjing Luo,Wenjun Gao,Wentao Zhang,Xi Gao,Xiangwen Wang,Xiao Bi,Xiaodong Liu,Xiaohan Wang,Xiaokang Chen,Xiaokang Zhang,Xiaotao Nie,Xin Cheng,Xin Liu,Xin Xie,Xingchao Liu,Xingkai Yu,Xingyou Li,Xinyu Yang,Xinyuan Li,Xu Chen,Xuecheng Su,Xuehai Pan,Xuheng Lin,Xuwei Fu,Y. Q. Wang,Yang Zhang,Yanhong Xu,Yanru Ma,Yao Li,Yao Li,Yao Zhao,Yaofeng Sun,Yaohui Wang,Yi Qian,Yi Yu,Yichao Zhang,Yifan Ding,Yifan Shi,Yiliang Xiong,Ying He,Ying Zhou,Yinmin Zhong,Yishi Piao,Yisong Wang,Yixiao Chen,Yixuan Tan,Yixuan Wei,Yiyang Ma,Yiyuan Liu,Yonglun Yang,Yongqiang Guo,Yongtong Wu,Yu Wu,Yuan Cheng,Yuan Ou,Yuanfan Xu,Yuduan Wang,Yue Gong,Yuhan Wu,Yuheng Zou,Yukun Li,Yunfan Xiong,Yuxiang Luo,Yuxiang You,Yuxuan Liu,Yuyang Zhou,Z. F. Wu,Z. Z. Ren,Zehua Zhao,Zehui Ren,Zhangli Sha,Zhe Fu,Zhean Xu,Zhenda Xie,Zhengyan Zhang,Zhewen Hao,Zhibin Gou,Zhicheng Ma,Zhigang Yan,Zhihong Shao,Zhixian Huang,Zhiyu Wu,Zhuoshu Li,Zhuping Zhang,Zian Xu,Zihao Wang,Zihui Gu,Zijia Zhu,Zilin Li,Zipeng Zhang,Ziwei Xie,Ziyi Gao,Zizheng Pan,Zongqing Yao,Bei Feng,Hui Li,J. L. Cai,Jiaqi Ni,Lei Xu,Meng Li,Ning Tian,R. J. Chen,R. L. Jin,S. S. Li,Shuang Zhou,Tianyu Sun,X. Q. Li,Xiangyue Jin,Xiaojin Shen,Xiaosha Chen,Xinnan Song,Xinyi Zhou,Y. X. Zhu,Yanping Huang,Yaohui Li,Yi Zheng,Yuchen Zhu,Yunxian Ma,Zhen Huang,Zhipeng Xu,Zhongyu Zhang,Dongjie Ji,Jian Liang,Jianzhong Guo,Jin Chen,Leyi Xia,Miaojun Wang,Mingming Li,Peng Zhang,Ruyi Chen,Shangmian Sun,Shaoqing Wu,Shengfeng Ye,T. Wang,W. L. Xiao,Wei An,Xianzu Wang,Xiaowen Sun,Xiaoxiang Wang,Ying Tang,Yukun Zha,Zekai Zhang,Zhe Ju,Zhen Zhang,Zihua Qu*

Main category: cs.CL

TL;DR: DeepSeek-V3.2通过创新的稀疏注意力机制、可扩展强化学习框架和大规模智能体任务合成管道，实现了高效计算与卓越推理能力的平衡，在多项国际竞赛中超越GPT-5并与Gemini-3.0-Pro媲美。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在计算效率与推理能力之间的权衡问题，开发一个既能保持高性能推理又具备计算效率的模型，以应对长上下文场景和复杂交互环境的需求。

Method: 1. DeepSeek稀疏注意力(DSA)：高效的注意力机制，降低计算复杂度同时保持长上下文性能；2. 可扩展强化学习框架：通过强化学习协议和扩展后训练计算；3. 大规模智能体任务合成管道：系统生成训练数据，实现可扩展的智能体后训练。

Result: DeepSeek-V3.2在计算效率与推理性能上取得平衡，其高计算变体DeepSeek-V3.2-Speciale超越GPT-5，推理能力与Gemini-3.0-Pro相当，在2025年国际数学奥林匹克(IMO)和国际信息学奥林匹克(IOI)中获得金牌表现。

Conclusion: DeepSeek-V3.2通过技术创新成功实现了高效计算与卓越推理能力的统一，在多项基准测试中表现出色，为大型语言模型的发展提供了新的技术路径。

Abstract: We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.

</details>


### [22] [From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks](https://arxiv.org/abs/2512.02580)
*Changpeng Yang,Jinyang Wu,Yuchen Liu,Shuai Zhang,Yang Li,Qiliang Liang,Hongzhen Wang,Shuai Nie,Jiaming Xu,Runyu Shi,Ying Huang,Guoquan Zhang*

Main category: cs.CL

TL;DR: CAPO提出基于优势信号的课程学习机制，先仅用正优势样本进行模仿学习建立基础，再引入负信号培养判别能力，提升复杂场景泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在训练早期不加区分地混合正负优势信号，可能导致指导模糊和收益有限，需要更精细的训练策略。

Method: CAPO采用自适应课程机制：1）先用正优势样本进行模仿学习建立稳健基础；2）再逐步引入负优势信号培养判别能力；3）兼容多种优化方法（GRPO、PPO、RLOO、Reinforce++）。

Result: 在数学推理任务上实现稳定显著提升，并能有效泛化到多模态GUI推理场景，证明其作为通用优化框架的稳健性。

Conclusion: CAPO通过基于优势信号的课程学习机制，解决了现有方法信号混合问题，提供了更有效的强化学习训练框架，具有良好泛化能力。

Abstract: Reinforcement learning has emerged as a paradigm for post-training large language models, boosting their reasoning capabilities. Such approaches compute an advantage value for each sample, reflecting better or worse performance than expected, thereby yielding both positive and negative signals for training. However, the indiscriminate mixing of the two signals in existing methods, especially from the early stages, may lead to ambiguous guidance and limited gains. To address this issue, we propose **CAPO** (**C**urriculum **A**dvantage **P**olicy **O**ptimization), an adaptive curriculum mechanism based on advantage signals. The proposed mechanism bootstraps imitation learning with positive-only advantage samples to establish robust foundations, and subsequently introduces negative signals to cultivate discriminative capabilities, thereby improving generalization across complex scenarios. Compatible with diverse optimization methods including GRPO, PPO, RLOO, and Reinforce++, our method consistently achieves stable and significant improvements in mathematical reasoning tasks, and further generalizes effectively to multimodal Graphical User Interface (GUI) reasoning scenarios, establishing itself as a versatile and robust optimization framework.

</details>


### [23] [Spoken Conversational Agents with Large Language Models](https://arxiv.org/abs/2512.02593)
*Chao-Han Huck Yang,Andreas Stolcke,Larry Heck*

Main category: cs.CL

TL;DR: 该教程总结了从级联ASR/NLU系统到端到端、检索和视觉增强系统的演进路径，提供了语音对话代理的系统级路线图


<details>
  <summary>Details</summary>
Motivation: 随着语音对话代理向语音原生LLMs发展，需要系统性地总结从传统级联架构到端到端系统的演进路径，为研究者和从业者提供清晰的路线图

Method: 通过框架化文本LLMs到音频的适配、跨模态对齐和联合语音文本训练，回顾数据集、评估指标和鲁棒性，比较级联vs端到端、后ASR校正、流式处理等设计选择

Result: 提供了工业助手与当前开放域和任务导向代理的联系，突出了可复现的基线，并概述了隐私、安全和评估等开放问题

Conclusion: 该教程为参会者提供了实用的实现方案和清晰的系统级路线图，帮助理解语音对话代理的发展趋势和技术挑战

Abstract: Spoken conversational agents are converging toward voice-native LLMs. This tutorial distills the path from cascaded ASR/NLU to end-to-end, retrieval-and vision-grounded systems. We frame adaptation of text LLMs to audio, cross-modal alignment, and joint speech-text training; review datasets, metrics, and robustness across accents and compare design choices (cascaded vs. E2E, post-ASR correction, streaming). We link industrial assistants to current open-domain and task-oriented agents, highlight reproducible baselines, and outline open problems in privacy, safety, and evaluation. Attendees leave with practical recipes and a clear systems-level roadmap.

</details>


### [24] [Input Order Shapes LLM Semantic Alignment in Multi-Document Summarization](https://arxiv.org/abs/2512.02665)
*Jing Ma*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在生成多文档摘要时存在显著的首因效应，更倾向于与第一个看到的文档保持语义对齐，这可能影响AI概览和智能体系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型被广泛应用于多文档摘要（如Google的AI概览），需要了解模型是否对所有输入文档给予同等权重，特别是在处理敏感话题如堕胎新闻时，以确保摘要的公正性和可靠性。

Method: 构建40个支持-中立-反对立场的堕胎新闻文章三元组，每个三元组以6种不同顺序排列，使用Gemini 2.5 Flash生成中立概览。通过ROUGE-L（词汇重叠）、BERTScore（语义相似度）和SummaC（事实一致性）评估摘要与源文章的关系，并使用单因素方差分析和配对比较检验位置效应。

Result: 在所有立场下，BERTScore显示显著的首因效应，摘要与第一个看到的文章语义更相似。配对比较显示位置1与位置2、3有显著差异，而位置2和3之间无差异，证实模型对第一个文档存在选择性偏好。

Conclusion: 大型语言模型在多文档摘要中存在首因效应，这可能对依赖LLM生成概览的应用和智能体AI系统构成风险，因为涉及LLM的步骤可能不成比例地影响下游行动。

Abstract: Large language models (LLMs) are now used in settings such as Google's AI Overviews, where it summarizes multiple long documents. However, it remains unclear whether they weight all inputs equally. Focusing on abortion-related news, we construct 40 pro-neutral-con article triplets, permute each triplet into six input orders, and prompt Gemini 2.5 Flash to generate a neutral overview. We evaluate each summary against its source articles using ROUGE-L (lexical overlap), BERTScore (semantic similarity), and SummaC (factual consistency). One-way ANOVA reveals a significant primacy effect for BERTScore across all stances, indicating that summaries are more semantically aligned with the first-seen article. Pairwise comparisons further show that Position 1 differs significantly from Positions 2 and 3, while the latter two do not differ from each other, confirming a selective preference for the first document. The findings present risks for applications that rely on LLM-generated overviews and for agentic AI systems, where the steps involving LLMs can disproportionately influence downstream actions.

</details>


### [25] [An Empirical Survey of Model Merging Algorithms for Social Bias Mitigation](https://arxiv.org/abs/2512.02689)
*Daiki Shirafuji,Tatsuhiko Saito,Yasutomo Kimura*

Main category: cs.CL

TL;DR: 对7种模型融合算法在减轻LLM社会偏见方面的实证调查，发现偏见减少与下游性能之间存在权衡，SLERP在中等插值权重下表现最平衡


<details>
  <summary>Details</summary>
Motivation: 大型语言模型会继承和放大预训练语料中的社会偏见，威胁公平性和社会信任。现有研究探索通过编辑LLM参数来减轻偏见，但缺乏不同模型融合算法的实证比较。

Method: 实证调查7种算法：Linear、Karcher Mean、SLERP、NuSLERP、TIES、DELLA和Nearswap，应用于GPT、LLaMA和Qwen家族的13个开源权重模型。使用三个偏见数据集（BBQ、BOLD、HONEST）进行综合评估，并测量这些技术对SuperGLUE基准下游任务性能的影响。

Result: 发现偏见减少与下游性能之间存在权衡：实现更大偏见缓解的方法会降低准确性，特别是在需要阅读理解、常识和因果推理的任务上。在融合算法中，Linear、SLERP和Nearswap在保持整体性能的同时持续减少偏见，其中中等插值权重的SLERP成为最平衡的选择。

Conclusion: 模型融合算法在偏见缓解方面具有潜力，但过度的去偏见化或不恰当的融合方法可能导致重要语言能力的退化。SLERP在中等插值权重下提供了偏见减少与性能保持的最佳平衡。

Abstract: Large language models (LLMs) are known to inherit and even amplify societal biases present in their pre-training corpora, threatening fairness and social trust. To address this issue, recent work has explored ``editing'' LLM parameters to mitigate social bias with model merging approaches; however, there is no empirical comparison. In this work, we empirically survey seven algorithms: Linear, Karcher Mean, SLERP, NuSLERP, TIES, DELLA, and Nearswap, applying 13 open weight models in the GPT, LLaMA, and Qwen families. We perform a comprehensive evaluation using three bias datasets (BBQ, BOLD, and HONEST) and measure the impact of these techniques on LLM performance in downstream tasks of the SuperGLUE benchmark. We find a trade-off between bias reduction and downstream performance: methods achieving greater bias mitigation degrade accuracy, particularly on tasks requiring reading comprehension and commonsense and causal reasoning. Among the merging algorithms, Linear, SLERP, and Nearswap consistently reduce bias while maintaining overall performance, with SLERP at moderate interpolation weights emerging as the most balanced choice. These results highlight the potential of model merging algorithms for bias mitigation, while indicating that excessive debiasing or inappropriate merging methods may lead to the degradation of important linguistic abilities.

</details>


### [26] [CREST: Universal Safety Guardrails Through Cluster-Guided Cross-Lingual Transfer](https://arxiv.org/abs/2512.02711)
*Lavish Bansal,Naman Mishra*

Main category: cs.CL

TL;DR: CREST是一个参数高效的多语言安全分类模型，仅用0.5B参数支持100种语言，通过13种高资源语言的训练实现跨语言安全转移，在低资源语言安全防护方面超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全防护主要针对高资源语言，导致使用低资源语言的大量人口缺乏足够的安全保护，需要开发能够覆盖全球多种语言的安全系统。

Method: 提出CREST模型，仅使用13种高资源语言进行训练，通过基于聚类的跨语言转移技术，将安全知识从少数语言扩展到100种语言，包括未见的高资源和低资源语言。

Result: 在六个安全基准测试中，CREST超越了同等规模的现有最佳防护模型，并与参数数量大得多的模型（2.5B参数以上）取得竞争性结果。

Conclusion: 语言特定的安全防护存在局限性，需要开发通用、语言无关的安全系统，以有效扩展到服务全球人口，CREST为此提供了有效的解决方案。

Abstract: Ensuring content safety in large language models (LLMs) is essential for their deployment in real-world applications. However, existing safety guardrails are predominantly tailored for high-resource languages, leaving a significant portion of the world's population underrepresented who communicate in low-resource languages. To address this, we introduce CREST (CRoss-lingual Efficient Safety Transfer), a parameter-efficient multilingual safety classification model that supports 100 languages with only 0.5B parameters. By training on a strategically chosen subset of only 13 high-resource languages, our model utilizes cluster-based cross-lingual transfer from a few to 100 languages, enabling effective generalization to both unseen high-resource and low-resource languages. This approach addresses the challenge of limited training data in low-resource settings. We conduct comprehensive evaluations across six safety benchmarks to demonstrate that CREST outperforms existing state-of-the-art guardrails of comparable scale and achieves competitive results against models with significantly larger parameter counts (2.5B parameters and above). Our findings highlight the limitations of language-specific guardrails and underscore the importance of developing universal, language-agnostic safety systems that can scale effectively to serve global populations.

</details>


### [27] [Emergent Bayesian Behaviour and Optimal Cue Combination in LLMs](https://arxiv.org/abs/2512.02719)
*Julian Ma,Jun Wang,Zafeirios Fountas*

Main category: cs.CL

TL;DR: 研究LLMs是否像人类一样能进行最优贝叶斯多模态整合，发现准确率高的模型不一定有稳健的不确定性处理能力


<details>
  <summary>Details</summary>
Motivation: LLMs在显式推理方面表现出色，但其隐式计算策略尚未充分探索。人类在感知任务中能使用近乎最优的贝叶斯策略直觉地处理噪声信号，研究者想知道LLMs是否也能在没有显式训练或指令的情况下表现出类似的最优多模态整合行为。

Method: 采用心理物理学范式，通过系统行为研究推断LLMs的计算原理。创建了BayesBench行为基准测试，包含四个基于文本和图像的幅度估计任务（长度、位置、距离、持续时间），评估了9个不同的LLMs并与人类判断进行校准。通过控制噪声、上下文和指令提示的消融实验，测量多模态线索组合的性能、行为和效率。除了准确率和效率指标外，还引入了贝叶斯一致性分数来检测贝叶斯一致的行为变化。

Result: 结果显示，虽然能力强的模型通常以贝叶斯一致的方式适应，但准确率并不能保证稳健性。值得注意的是，GPT-5 Mini在文本任务上达到完美准确率，但未能有效整合视觉线索。这揭示了能力与策略之间的关键分离，表明以准确率为中心的基准测试可能过度关注性能而忽略了脆弱的不确定性处理。

Conclusion: 这些发现揭示了LLMs在处理不确定性方面出现了原则性方法，并强调了准确率与贝叶斯倾向之间的相关性。研究发布了心理物理学基准测试和一致性度量作为评估工具，为未来多模态架构设计提供参考。

Abstract: Large language models (LLMs) excel at explicit reasoning, but their implicit computational strategies remain underexplored. Decades of psychophysics research show that humans intuitively process and integrate noisy signals using near-optimal Bayesian strategies in perceptual tasks. We ask whether LLMs exhibit similar behaviour and perform optimal multimodal integration without explicit training or instruction. Adopting the psychophysics paradigm, we infer computational principles of LLMs from systematic behavioural studies. We introduce a behavioural benchmark - BayesBench: four magnitude estimation tasks (length, location, distance, and duration) over text and image, inspired by classic psychophysics, and evaluate a diverse set of nine LLMs alongside human judgments for calibration. Through controlled ablations of noise, context, and instruction prompts, we measure performance, behaviour and efficiency in multimodal cue-combination. Beyond accuracy and efficiency metrics, we introduce a Bayesian Consistency Score that detects Bayes-consistent behavioural shifts even when accuracy saturates. Our results show that while capable models often adapt in Bayes-consistent ways, accuracy does not guarantee robustness. Notably, GPT-5 Mini achieves perfect text accuracy but fails to integrate visual cues efficiently. This reveals a critical dissociation between capability and strategy, suggesting accuracy-centric benchmarks may over-index on performance while missing brittle uncertainty handling. These findings reveal emergent principled handling of uncertainty and highlight the correlation between accuracy and Bayesian tendencies. We release our psychophysics benchmark and consistency metric (https://bayes-bench.github.io) as evaluation tools and to inform future multimodal architecture designs.

</details>


### [28] [SurveyEval: Towards Comprehensive Evaluation of LLM-Generated Academic Surveys](https://arxiv.org/abs/2512.02763)
*Jiahao Zhao,Shuaixing Zhang,Nan Xu,Lei Wang*

Main category: cs.CL

TL;DR: SurveyEval是一个评估自动生成综述系统的综合基准，从整体质量、大纲连贯性和参考文献准确性三个维度进行评估，旨在解决LLM自动综述系统评估的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着LLM自动综述系统的发展，这些系统能够整合检索、组织和内容合成到端到端生成流程中。然而，如何评估这种复杂系统仍然是一个重大挑战，需要专门的评估框架。

Method: 提出SurveyEval基准，在7个学科领域评估自动生成的综述，从三个维度进行评估：整体质量、大纲连贯性和参考文献准确性。扩展了LLM-as-a-Judge框架，加入人类参考来增强评估与人类判断的一致性。

Result: 评估结果显示，通用长文本或论文写作系统倾向于生成质量较低的综述，而专门的综述生成系统能够提供显著更高质量的结果。

Conclusion: SurveyEval作为一个可扩展的测试平台，旨在理解和改进跨不同学科和评估标准的自动综述系统，为自动综述系统的评估提供了重要基准。

Abstract: LLM-based automatic survey systems are transforming how users acquire information from the web by integrating retrieval, organization, and content synthesis into end-to-end generation pipelines. While recent works focus on developing new generation pipelines, how to evaluate such complex systems remains a significant challenge. To this end, we introduce SurveyEval, a comprehensive benchmark that evaluates automatically generated surveys across three dimensions: overall quality, outline coherence, and reference accuracy. We extend the evaluation across 7 subjects and augment the LLM-as-a-Judge framework with human references to strengthen evaluation-human alignment. Evaluation results show that while general long-text or paper-writing systems tend to produce lower-quality surveys, specialized survey-generation systems are able to deliver substantially higher-quality results. We envision SurveyEval as a scalable testbed to understand and improve automatic survey systems across diverse subjects and evaluation criteria.

</details>


### [29] [PEFT-Factory: Unified Parameter-Efficient Fine-Tuning of Autoregressive Large Language Models](https://arxiv.org/abs/2512.02764)
*Robert Belanec,Ivan Srba,Maria Bielikova*

Main category: cs.CL

TL;DR: PEFT-Factory是一个统一的参数高效微调框架，提供19种PEFT方法、27个数据集和专门评估指标，旨在解决PEFT方法难以复现、部署和比较的问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模增长，参数高效微调方法变得重要，但现有方法存在复现困难、部署复杂、难以相互比较的问题，需要一个统一的框架来解决这些挑战。

Method: 基于LLaMA-Factory开发PEFT-Factory框架，采用模块化设计支持扩展，原生提供19种PEFT方法、27个分类和文本生成数据集（涵盖12个任务），以及标准和PEFT专用评估指标。

Result: 创建了一个即用型、可控且稳定的环境，显著提升了PEFT方法的复现性和基准测试能力，框架已公开可用。

Conclusion: PEFT-Factory为PEFT方法的研究和应用提供了一个统一的解决方案，解决了现有方法的复现和比较难题，促进了该领域的发展。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods address the increasing size of Large Language Models (LLMs). Currently, many newly introduced PEFT methods are challenging to replicate, deploy, or compare with one another. To address this, we introduce PEFT-Factory, a unified framework for efficient fine-tuning LLMs using both off-the-shelf and custom PEFT methods. While its modular design supports extensibility, it natively provides a representative set of 19 PEFT methods, 27 classification and text generation datasets addressing 12 tasks, and both standard and PEFT-specific evaluation metrics. As a result, PEFT-Factory provides a ready-to-use, controlled, and stable environment, improving replicability and benchmarking of PEFT methods. PEFT-Factory is a downstream framework that originates from the popular LLaMA-Factory, and is publicly available at https://github.com/kinit-sk/PEFT-Factory

</details>


### [30] [Towards Unification of Hallucination Detection and Fact Verification for Large Language Models](https://arxiv.org/abs/2512.02772)
*Weihang Su,Jianming Long,Changyue Wang,Shiyu Lin,Jingyan Xu,Ziyi Ye,Qingyao Ai,Yiqun Liu*

Main category: cs.CL

TL;DR: UniFact框架统一评估幻觉检测与事实核查，揭示两者互补性，混合方法效果最佳


<details>
  <summary>Details</summary>
Motivation: LLMs经常产生看似流畅但事实错误的幻觉内容，这削弱了信任并阻碍实际应用。目前幻觉检测(HD)和事实核查(FV)两个研究范式各自独立发展，形成了研究分裂，阻碍了共同进步。

Method: 提出UniFact统一评估框架，通过动态生成模型输出和相应的事实性标签，实现HD和FV在实例级别的直接比较。进行大规模实验，涵盖多个LLM家族和检测方法。

Result: 三个关键发现：1) 没有哪个范式普遍更优；2) HD和FV捕捉事实错误的不同方面，具有互补性；3) 整合两种方法的混合方法始终达到最先进的性能。

Conclusion: 需要一个新的整合研究议程来统一LLMs中的幻觉检测和事实核查。提供了首个深入分析FV和HD为何分化的原因，以及支持其统一必要性的实证证据。

Abstract: Large Language Models (LLMs) frequently exhibit hallucinations, generating content that appears fluent and coherent but is factually incorrect. Such errors undermine trust and hinder their adoption in real-world applications. To address this challenge, two distinct research paradigms have emerged: model-centric Hallucination Detection (HD) and text-centric Fact Verification (FV). Despite sharing the same goal, these paradigms have evolved in isolation, using distinct assumptions, datasets, and evaluation protocols. This separation has created a research schism that hinders their collective progress. In this work, we take a decisive step toward bridging this divide. We introduce UniFact, a unified evaluation framework that enables direct, instance-level comparison between FV and HD by dynamically generating model outputs and corresponding factuality labels. Through large-scale experiments across multiple LLM families and detection methods, we reveal three key findings: (1) No paradigm is universally superior; (2) HD and FV capture complementary facets of factual errors; and (3) hybrid approaches that integrate both methods consistently achieve state-of-the-art performance. Beyond benchmarking, we provide the first in-depth analysis of why FV and HD diverged, as well as empirical evidence supporting the need for their unification. The comprehensive experimental results call for a new, integrated research agenda toward unifying Hallucination Detection and Fact Verification in LLMs.
  We have open-sourced all the code, data, and baseline implementation at: https://github.com/oneal2000/UniFact/

</details>


### [31] [Making Dialogue Grounding Data Rich: A Three-Tier Data Synthesis Framework for Generalized Referring Expression Comprehension](https://arxiv.org/abs/2512.02791)
*Juexi Shao,Siyou Li,Yujian Gan,Chris Madge,Vanja Karan,Massimo Poesio*

Main category: cs.CL

TL;DR: 提出三层次数据合成方法解决对话式广义指称表达理解中的分布偏移问题，通过平衡真实性和可控性生成可扩展的监督数据，显著提升模型性能


<details>
  <summary>Details</summary>
Motivation: 现有对话式广义指称表达理解系统在训练和评估域之间存在分布偏移问题，且标注对话数据稀缺，导致模型在复杂视觉场景中难以准确理解指称表达和解决长对话上下文中的共指问题

Method: 采用三层次数据合成方法，平衡真实性和可控性，为对话条件定位生成可扩展的监督数据，然后在这些合成数据上进行微调

Result: 在标准评估指标上，相比先前方法取得了持续且显著的改进

Conclusion: 通过数据合成方法可以有效解决对话式广义指称表达理解中的数据稀缺和分布偏移问题，显著提升模型性能

Abstract: Dialogue-Based Generalized Referring Expressions Comprehension (GREC) requires models to ground the expression and unlimited targets in complex visual scenes while resolving coreference across a long dialogue context. However, existing systems struggle under distribution shift between training and evaluation domains, a gap exacerbated by the scarcity of annotated dialogue grounding data. We address this challenge with a three-tier data-synthesis method that balances realism and controllability to produce scalable supervision for dialogue-conditioned grounding. Fine-tuning on the synthesized data yields consistent, substantial improvements over prior approaches across standard evaluation metrics.

</details>


### [32] [TriLex: A Framework for Multilingual Sentiment Analysis in Low-Resource South African Languages](https://arxiv.org/abs/2512.02799)
*Mike Nkongolo,Hilton Vorster,Josh Warren,Trevor Naick,Deandre Vanmali,Masana Mashapha,Luke Brand,Alyssa Fernandes,Janco Calitz,Sibusiso Makhoba*

Main category: cs.CL

TL;DR: TriLex框架通过三阶段检索增强方法扩展低资源非洲语言情感词典，提升情感分析性能，AfroXLMR表现最佳达80%+ F1分数。


<details>
  <summary>Details</summary>
Motivation: 低资源非洲语言在情感分析中代表性不足，限制了多语言NLP系统的词汇覆盖和性能表现。

Method: 提出TriLex三阶段检索增强框架：1)基于语料库提取，2)跨语言映射，3)RAG驱动的词汇精炼；使用扩展词典评估AfroXLMR和AfriBERTa两种非洲预训练语言模型。

Result: AfroXLMR表现最佳，在isiXhosa和isiZulu上F1分数超过80%，跨语言稳定性强；AfriBERTa虽未预训练目标语言仍达64% F1分数；两模型均优于传统机器学习基线，集成分析进一步提升精度和鲁棒性。

Conclusion: TriLex是低资源南非语言多语言情感词典扩展和情感建模的可扩展有效框架，为低资源语言NLP提供了实用解决方案。

Abstract: Low-resource African languages remain underrepresented in sentiment analysis, limiting both lexical coverage and the performance of multilingual Natural Language Processing (NLP) systems. This study proposes TriLex, a three-stage retrieval augmented framework that unifies corpus-based extraction, cross lingual mapping, and retrieval augmented generation (RAG) driven lexical refinement to systematically expand sentiment lexicons for low-resource languages. Using the enriched lexicon, the performance of two prominent African pretrained language models (AfroXLMR and AfriBERTa) is evaluated across multiple case studies. Results demonstrate that AfroXLMR delivers superior performance, achieving F1-scores above 80% for isiXhosa and isiZulu and exhibiting strong cross-lingual stability. Although AfriBERTa lacks pre-training on these target languages, it still achieves reliable F1-scores around 64%, validating its utility in computationally constrained settings. Both models outperform traditional machine learning baselines, and ensemble analyses further enhance precision and robustness. The findings establish TriLex as a scalable and effective framework for multilingual sentiment lexicon expansion and sentiment modeling in low-resource South African languages.

</details>


### [33] [SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment](https://arxiv.org/abs/2512.02807)
*Yixuan Tang,Yi Yang*

Main category: cs.CL

TL;DR: 提出stable rank作为从模型内部表示中提取的无监督质量信号，并基于此开发SR-GRPO强化学习方法，无需外部监督即可提升LLM对齐效果。


<details>
  <summary>Details</summary>
Motivation: 传统LLM对齐方法依赖外部监督（如人工标注、奖励模型、自评估），但这些方法存在人工标注稀缺且主观、奖励模型易受奖励攻击、自评估方法对提示敏感且有偏见等问题。需要一种内在的、无需标注的质量信号。

Method: 提出stable rank概念，通过计算隐藏状态的总方差与主导方向方差的比值来衡量表示的有效维度，捕捉信息在表示维度上的分布质量。基于此开发Stable Rank Group Relative Policy Optimization (SR-GRPO)，将stable rank作为强化学习的奖励信号。

Result: stable rank在RewardBench上达到84.04%准确率，通过Best-of-N采样比贪婪解码平均提升11.3个百分点任务准确率。SR-GRPO无需外部监督将Qwen2.5-1.5B-Instruct在STEM任务上提升10%，数学推理提升19%，优于学习的奖励模型和自评估基线。

Conclusion: 质量信号可以从模型内部几何结构中提取，为无需外部监督的可扩展对齐提供了新路径。stable rank作为内在质量指标具有实用价值。

Abstract: Aligning Large Language Models (LLMs) with human preferences typically relies on external supervision, which faces critical limitations: human annotations are scarce and subjective, reward models are vulnerable to reward hacking, and self-evaluation methods suffer from prompt sensitivity and biases. In this work, we propose stable rank, an intrinsic, annotation-free quality signal derived from model representations. Stable rank measures the effective dimensionality of hidden states by computing the ratio of total variance to dominant-direction variance, capturing quality through how information distributes across representation dimensions. Empirically, stable rank achieves 84.04% accuracy on RewardBench and improves task accuracy by an average of 11.3 percentage points over greedy decoding via Best-of-N sampling. Leveraging this insight, we introduce Stable Rank Group Relative Policy Optimization (SR-GRPO), which uses stable rank as a reward signal for reinforcement learning. Without external supervision, SR-GRPO improves Qwen2.5-1.5B-Instruct by 10% on STEM and 19% on mathematical reasoning, outperforming both learned reward models and self-evaluation baselines. Our findings demonstrate that quality signals can be extracted from internal model geometry, offering a path toward scalable alignment without external supervision.

</details>


### [34] [A benchmark dataset for evaluating Syndrome Differentiation and Treatment in large language models](https://arxiv.org/abs/2512.02816)
*Kunning Li,Jianbin Guo,Zhaoyang Shang,Yiqing Liu,Hongmin Du,Lingling Liu,Yuping Zhao,Lifeng Dong*

Main category: cs.CL

TL;DR: 该论文提出了TCM-BEST4SDT基准，用于评估LLM在中医临床辨证论治中的能力，包含四个任务和三种评估机制，已在15个主流LLM上验证有效性。


<details>
  <summary>Details</summary>
Motivation: LLM在中医领域的应用需要评估其临床能力，但现有基准局限于知识问答或辨证准确性，缺乏对治疗决策的评估，且中医辨证论治具有个体化、整体性和多样性特点，评估面临挑战。

Method: 提出基于临床病例的综合基准TCM-BEST4SDT，包含中医基础知识、医学伦理、LLM内容安全和辨证论治四个任务；采用选择题评估、评判模型评估和奖励模型评估三种机制；通过专家主导的数据标注流程和专门的奖励模型量化方证一致性。

Result: 在15个主流LLM（包括通用和中医领域模型）上验证了TCM-BEST4SDT的有效性，基准已公开可用以促进智能中医研究发展。

Conclusion: TCM-BEST4SDT为评估LLM在中医临床辨证论治能力提供了全面基准，解决了现有评估方法的局限性，有助于推动中医领域人工智能研究的进步。

Abstract: The emergence of Large Language Models (LLMs) within the Traditional Chinese Medicine (TCM) domain presents an urgent need to assess their clinical application capabilities. However, such evaluations are challenged by the individualized, holistic, and diverse nature of TCM's "Syndrome Differentiation and Treatment" (SDT). Existing benchmarks are confined to knowledge-based question-answering or the accuracy of syndrome differentiation, often neglecting assessment of treatment decision-making. Here, we propose a comprehensive, clinical case-based benchmark spearheaded by TCM experts, and a specialized reward model employed to quantify prescription-syndrome congruence. Data annotation follows a rigorous pipeline. This benchmark, designated TCM-BEST4SDT, encompasses four tasks, including TCM Basic Knowledge, Medical Ethics, LLM Content Safety, and SDT. The evaluation framework integrates three mechanisms, namely selected-response evaluation, judge model evaluation, and reward model evaluation. The effectiveness of TCM-BEST4SDT was corroborated through experiments on 15 mainstream LLMs, spanning both general and TCM domains. To foster the development of intelligent TCM research, TCM-BEST4SDT is now publicly available.

</details>


### [35] [BOOM: Beyond Only One Modality KIT's Multimodal Multilingual Lecture Companion](https://arxiv.org/abs/2512.02817)
*Sai Koneru,Fabian Retkowski,Christian Huber,Lukas Hilgert,Seymanur Akti,Enes Yavuz Ugan,Alexander Waibel,Jan Niehues*

Main category: cs.CL

TL;DR: BOOM是一个多模态多语言讲座伴侣，能够联合翻译讲座音频和幻灯片，生成同步的翻译文本、本地化幻灯片和合成语音，为学习者提供完整的母语学习体验。


<details>
  <summary>Details</summary>
Motivation: 教育全球化和在线学习的快速增长使得教育内容本地化成为关键挑战。讲座材料本质上是多模态的（音频+幻灯片），需要能够处理多种输入模态的系统。为了提供完整的学习体验，翻译必须保留所有模态：文本、幻灯片视觉元素和语音。

Method: 提出BOOM多模态多语言讲座伴侣，采用端到端方法联合翻译讲座音频和幻灯片，生成同步的三模态输出：翻译文本、保留视觉元素的本地化幻灯片、合成语音。

Result: 实验表明，基于幻灯片的转录对下游任务（如摘要和问答）具有级联效益。已发布幻灯片翻译代码并集成到Lecture Translator中。

Conclusion: BOOM能够让学生在母语中访问讲座，同时保留原始内容的完整性，为多模态教育内容本地化提供了有效的端到端解决方案。

Abstract: The globalization of education and rapid growth of online learning have made localizing educational content a critical challenge. Lecture materials are inherently multimodal, combining spoken audio with visual slides, which requires systems capable of processing multiple input modalities. To provide an accessible and complete learning experience, translations must preserve all modalities: text for reading, slides for visual understanding, and speech for auditory learning. We present \textbf{BOOM}, a multimodal multilingual lecture companion that jointly translates lecture audio and slides to produce synchronized outputs across three modalities: translated text, localized slides with preserved visual elements, and synthesized speech. This end-to-end approach enables students to access lectures in their native language while aiming to preserve the original content in its entirety. Our experiments demonstrate that slide-aware transcripts also yield cascading benefits for downstream tasks such as summarization and question answering. We release our Slide Translation code at https://github.com/saikoneru/image-translator and integrate it in Lecture Translator at https://gitlab.kit.edu/kit/isl-ai4lt/lt-middleware/ltpipeline}\footnote{All released code and models are licensed under the MIT License.

</details>


### [36] [promptolution: A Unified, Modular Framework for Prompt Optimization](https://arxiv.org/abs/2512.02840)
*Tom Zehle,Timo Heiß,Moritz Schlager,Matthias Aßenmacher,Matthias Feurer*

Main category: cs.CL

TL;DR: promptolution：一个统一、模块化的开源框架，用于提示优化，集成了多种离散提示优化器，与底层LLM实现无关


<details>
  <summary>Details</summary>
Motivation: 尽管许多研究论文证明了提示优化的有效性，但实际应用受到阻碍，因为现有实现通常依赖于未维护和孤立的研究代码库

Method: 引入promptolution框架，这是一个统一、模块化的开源系统，在一个可扩展的系统中提供提示优化所需的所有组件，集成了多种当代离散提示优化器

Result: 创建了一个为从业者和研究人员设计的框架，能够集成多种提示优化器，同时保持与底层LLM实现的无关性

Conclusion: promptolution框架解决了提示优化研究代码分散和难以维护的问题，为实际应用提供了统一、可扩展的解决方案

Abstract: Prompt optimization has become crucial for enhancing the performance of large language models (LLMs) across a broad range of tasks. Although many research papers show its effectiveness, practical adoption is hindered as existing implementations are often tied to unmaintained and isolated research codebases. To address this, we introduce promptolution, a unified and modular open-source framework that provides all components required for prompt optimization within a single extensible system for both practitioners and researchers. It integrates multiple contemporary discrete prompt optimizers while remaining agnostic to the underlying LLM implementation.

</details>


### [37] [Cross-Lingual Prompt Steerability: Towards Accurate and Robust LLM Behavior across Languages](https://arxiv.org/abs/2512.02841)
*Lechen Zhang,Yusheng Zhou,Tolga Ergen,Lajanugen Logeswaran,Moontae Lee,David Jurgens*

Main category: cs.CL

TL;DR: 本文研究了系统提示如何引导大语言模型实现准确稳健的多语言行为，提出了四维评估框架，通过大规模实验发现某些提示组件与多语言性能相关，开发了多语言提示优化框架，并分析了推理模式变化。


<details>
  <summary>Details</summary>
Motivation: 虽然现有研究主要关注英语环境，但实际部署需要单一提示能在多种语言中可靠工作。系统提示为在推理时调节大语言模型提供了轻量而强大的机制，但缺乏对多语言环境下系统提示如何影响模型行为的全面理解。

Method: 提出了统一的多语言四维评估框架，在五种语言、三个大语言模型和三个基准测试上进行大规模实验。开发了多语言提示优化框架，分析了超过1000万个推理单元，研究提示如何影响推理模式。

Result: 发现某些提示组件（如思维链、情感、场景）与稳健的多语言行为相关。提示优化框架能自动发现提升所有指标5-10%的提示。性能更好的系统提示能诱导更结构化、一致的推理模式，减少不必要的语言切换。

Conclusion: 系统提示优化是实现准确稳健多语言大语言模型行为的可扩展路径。研究强调了通过提示工程改进多语言性能的潜力，为实际部署提供了实用方法。

Abstract: System prompts provide a lightweight yet powerful mechanism for conditioning large language models (LLMs) at inference time. While prior work has focused on English-only settings, real-world deployments benefit from having a single prompt to operate reliably across languages. This paper presents a comprehensive study of how different system prompts steer models toward accurate and robust cross-lingual behavior. We propose a unified four-dimensional evaluation framework to assess system prompts in multilingual environments. Through large-scale experiments on five languages, three LLMs, and three benchmarks, we uncover that certain prompt components, such as CoT, emotion, and scenario, correlate with robust multilingual behavior. We develop a prompt optimization framework for multilingual settings and show it can automatically discover prompts that improve all metrics by 5-10%. Finally, we analyze over 10 million reasoning units and find that more performant system prompts induce more structured and consistent reasoning patterns, while reducing unnecessary language-switching. Together, we highlight system prompt optimization as a scalable path to accurate and robust multilingual LLM behavior.

</details>


### [38] [Bangla Hate Speech Classification with Fine-tuned Transformer Models](https://arxiv.org/abs/2512.02845)
*Yalda Keivan Jafari,Krishno Dey*

Main category: cs.CL

TL;DR: 该研究针对低资源孟加拉语仇恨言论检测问题，在BLP 2025共享任务中评估了多种基线方法和Transformer模型，发现语言特定预训练的BanglaBERT表现最佳。


<details>
  <summary>Details</summary>
Motivation: 低资源语言（如孟加拉语）的仇恨言论检测面临数据集不足、拼写异质性和语言多样性等挑战。孟加拉语有超过2.3亿使用者，但在计算资源中代表性严重不足，社交媒体自动审核需求日益增长。

Method: 研究BLP 2025共享任务的子任务1A和1B，复现官方基线方法（多数类、随机、SVM），并加入逻辑回归、随机森林、决策树作为基线。同时使用Transformer模型：DistilBERT、BanglaBERT、m-BERT和XLM-RoBERTa进行仇恨言论分类。

Result: 所有Transformer模型（除DistilBERT外）在子任务中都优于基线方法。在Transformer模型中，BanglaBERT在两个子任务中表现最佳。尽管模型规模较小，BanglaBERT仍优于m-BERT和XLM-RoBERTa，表明语言特定预训练非常重要。

Conclusion: 研究结果强调了预训练语言模型在低资源孟加拉语中的潜力和必要性。语言特定预训练对低资源语言处理至关重要，BanglaBERT的成功证明了这一点。

Abstract: Hate speech recognition in low-resource languages remains a difficult problem due to insufficient datasets, orthographic heterogeneity, and linguistic variety. Bangla is spoken by more than 230 million people of Bangladesh and India (West Bengal). Despite the growing need for automated moderation on social media platforms, Bangla is significantly under-represented in computational resources. In this work, we study Subtask 1A and Subtask 1B of the BLP 2025 Shared Task on hate speech detection. We reproduce the official baselines (e.g., Majority, Random, Support Vector Machine) and also produce and consider Logistic Regression, Random Forest, and Decision Tree as baseline methods. We also utilized transformer-based models such as DistilBERT, BanglaBERT, m-BERT, and XLM-RoBERTa for hate speech classification. All the transformer-based models outperformed baseline methods for the subtasks, except for DistilBERT. Among the transformer-based models, BanglaBERT produces the best performance for both subtasks. Despite being smaller in size, BanglaBERT outperforms both m-BERT and XLM-RoBERTa, which suggests language-specific pre-training is very important. Our results highlight the potential and need for pre-trained language models for the low-resource Bangla language.

</details>


### [39] [Think in Parallel, Answer as One: Logit Averaging for Open-Ended Reasoning](https://arxiv.org/abs/2512.02874)
*Haonan Wang,Chao Du,Kenji Kawaguchi,Tianyu Pang*

Main category: cs.CL

TL;DR: ThinkMerge是一种无需训练的解码策略，通过并行推理轨迹的logits平均来提升开放端推理任务性能，优于多数投票方法。


<details>
  <summary>Details</summary>
Motivation: 多数投票在封闭式问答中有效，但不适用于代码生成和深度网络研究等开放式推理任务，因为这些任务中"多数"解决方案难以定义。

Method: 提出ThinkMerge解码策略：运行K个并行推理轨迹，在同步点平均它们的下一个token的logits，生成单个连贯输出。与vLLM/SGLang无缝集成，兼容Top-p/Top-k等标准解码技术。

Result: 在AIME和GPQA上匹配或超越多数投票；在LiveCodeBench（hard）上，DeepCoder-14B-Preview的pass@1提升+8.28%，Qwen3-8B提升+7.58%；在GAIA、BrowseComp-en/zh和XbenchDeepSearch上提升网络深度研究代理性能。

Conclusion: 并行测试时扩展可以在不依赖完整输出投票的情况下有益于开放式推理，ThinkMerge为开放端任务提供了有效的解码策略。

Abstract: Majority voting has proven effective for close-ended question answering by aggregating parallel reasoning traces. However, it is not directly applicable to open-ended reasoning, such as code generation and web-based deep research, where a "majority" over complete solutions is ill-defined. We introduce ThinkMerge, a training-free, plug-and-play decoding strategy that runs K parallel reasoning traces and averages their next-token logits at synchronization points to produce a single coherent output. ThinkMerge integrates seamlessly with vLLM/SGLang and remains compatible with standard decoding techniques such as Top-p/Top-k. Empirically, it matches or surpasses majority voting on AIME and GPQA, while delivering consistent gains on open-ended coding tasks: on LiveCodeBench (hard), pass@1 improves by +8.28% for DeepCoder-14B-Preview and +7.58% for Qwen3-8B. Beyond code, we further show that ThinkMerge improves web-based deep-research agents (e.g., WebSailor-7B/32B) across GAIA, BrowseComp-en/zh, and XbenchDeepSearch. These results demonstrate that parallel test-time scaling can benefit open-ended reasoning without relying on voting over complete outputs.

</details>


### [40] [Fast-Decoding Diffusion Language Models via Progress-Aware Confidence Schedules](https://arxiv.org/abs/2512.02892)
*Amr Mohamed,Yang Zhang,Michalis Vazirgiannis,Guokan Shang*

Main category: cs.CL

TL;DR: SchED是一种无需训练、模型无关的早期退出算法，通过聚合全跨度对数边际并基于进度相关的置信度阈值停止解码，显著加速扩散大语言模型的推理速度，同时保持接近基准的性能。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型(dLLMs)相比自回归模型具有潜力，但其迭代采样过程缓慢，严重限制了实际应用。需要一种方法在不显著降低性能的前提下加速dLLM的解码过程。

Method: 提出SchED算法：1) 无需训练、模型无关；2) 聚合全跨度对数边际；3) 当达到平滑、进度相关的置信度阈值时停止解码；4) 在多个dLLM家族(Dream和LLaDA)及其变体上进行评估。

Result: 在指令调优模型上实现3.8-4.0倍加速，同时保持99.8-100%的基准性能；在基础模型上实现一致加速，性能保留99.1-100%，激进设置下可达2.34倍加速。在长文本生成任务中明显优于现有基于置信度的早期退出方法。

Conclusion: SchED通过将真正的置信度稳定转化为计算节省，使dLLM解码显著更高效。熵分析显示指令调优加速了预测熵的衰减，进一步支持了该方法的效果。

Abstract: Diffusion large language models (dLLMs) offer a promising alternative to autoregressive models, but their practical utility is severely hampered by slow, iterative sampling. We present SchED, a training-free, model-agnostic early-exit algorithm that aggregates full-span logit margins and halts decoding once a smooth, progress-dependent confidence threshold is met. We evaluated SchED on two dLLM families (Dream and LLaDA), in base and instruction-tuned variants across ten benchmarks spanning downstream tasks including multiple-choice question answering (MCQ), math, long-form QA/summarization, and translation. SchED delivers large, stable accelerations: on instruction-tuned models, it achieves $3.8$-$4.0\times$ speedups while retaining $99.8$-$100\%$ of the baseline score on average. On base models, SchED yields consistent speedup gains with $99.1$-$100\%$ performance retention, with up to $2.34\times$ under more aggressive settings. Using a conservative speed metric that heavily penalizes quality loss (QPS, $γ{=}4$), we show that SchED is robust and clearly outperforms prior confidence-based early-exit methods, which break down on long-form generation. An entropy analysis of the model's token predictions reveals that instruction tuning speeds up the decay of predictive entropy. By turning genuine confidence stabilization into computational savings, SchED makes dLLM decoding substantially more efficient.

</details>


### [41] [AutoNeural: Co-Designing Vision-Language Models for NPU Inference](https://arxiv.org/abs/2512.02924)
*Wei Chen,Liangmin Wu,Yunhai Hu,Zhiyuan Li,Zhiyuan Cheng,Yicheng Qian,Lingyue Zhu,Zhipeng Hu,Luoyi Liang,Qiang Tang,Zhen Liu,Han Yang*

Main category: cs.CL

TL;DR: AutoNeural：专为NPU设计的视觉-语言模型架构，通过MobileNetV5风格视觉编码器和SSM-Transformer混合语言模型，实现整数推理，显著提升边缘AI效率


<details>
  <summary>Details</summary>
Motivation: 当前针对GPU优化的视觉-语言模型在NPU上表现不佳，主要原因是ViT的量化脆弱性和自回归注意力机制的I/O瓶颈，无法充分利用NPU的高算术吞吐量

Method: 1) 用MobileNetV5风格的深度可分离卷积替换标准ViT编码器，确保激活分布有界以实现稳定INT4/8/16量化；2) 语言模型融合状态空间模型原理与Transformer层，使用门控卷积实现线性时间复杂度，消除KV缓存的内存I/O开销

Result: 视觉编码器量化误差降低7倍，端到端延迟降低14倍，解码速度提升3倍，上下文窗口延长4倍。在Qualcomm SA8295P SoC上的汽车案例研究验证了驾驶舱应用的实时性能

Conclusion: 为NPU约束重新设计模型拓扑是实现鲁棒多模态边缘智能的先决条件，硬件-模型协同设计能显著提升边缘AI效率

Abstract: While Neural Processing Units (NPUs) offer high theoretical efficiency for edge AI, state-of-the-art Vision--Language Models (VLMs) tailored for GPUs often falter on these substrates. We attribute this hardware-model mismatch to two primary factors: the quantization brittleness of Vision Transformers (ViTs) and the I/O-bound nature of autoregressive attention mechanisms, which fail to utilize the high arithmetic throughput of NPUs. To bridge this gap, we propose AutoNeural, an NPU-native VLM architecture co-designed for integer-only inference. We replace the standard ViT encoder with a MobileNetV5-style backbone utilizing depthwise separable convolutions, which ensures bounded activation distributions for stable INT4/8/16 quantization. Complementing this, our language backbone integrates State-Space Model (SSM) principles with Transformer layers, employing efficient gated convolutions to achieve linear-time complexity. This hybrid design eliminates the heavy memory I/O overhead of Key-Value caching during generation. Our approach delivers substantial efficiency gains, reducing quantization error of vision encoder by up to 7x and end-to-end latency by 14x compared to conventional baselines. The AutoNeural also delivers 3x decoding speed and 4x longer context window than the baseline. We validate these improvements via a real-world automotive case study on the Qualcomm SA8295P SoC, demonstrating real-time performance for cockpit applications. Our results highlight that rethinking model topology specifically for NPU constraints is a prerequisite for robust multi-modal edge intelligence.

</details>


### [42] [Fine-Tuned Large Language Models for Logical Translation: Reducing Hallucinations with Lang2Logic](https://arxiv.org/abs/2512.02987)
*Muyu Pan,Dheeraj Kodakandla,Mahfuza Farooque*

Main category: cs.CL

TL;DR: 提出一个将英文句子转换为逻辑表达式并生成CNF的新框架，通过微调语言模型减少幻觉，实现可靠的CNF生成。


<details>
  <summary>Details</summary>
Motivation: LLM在自然语言到形式逻辑的自动翻译中存在幻觉问题，这对于需要精确性的逻辑翻译任务具有挑战性。需要开发能够减少幻觉、提高翻译可靠性的方法。

Method: 结合经典NLP技术（自定义语法）、符号计算库和微调的语言模型，将英文句子转换为逻辑表达式，再翻译为合取范式（CNF）进行可满足性求解。

Result: 微调模型在不同语法设置下训练后，能够有意识地纠正原始模型的相同类型幻觉，从而提供可靠的CNF生成。

Conclusion: 该框架通过微调语言模型有效减少了逻辑翻译中的幻觉问题，实现了从自然语言到CNF的可靠转换，为自动推理和软件规范验证提供了支持。

Abstract: Recent advances in natural language processing (NLP), particularly large language models (LLMs), have motivated the automatic translation of natural language statements into formal logic without human intervention. This enables automated reasoning and facilitates debugging, finding loop invariants, and adhering to specifications in software systems. However, hallucinations-incorrect outputs generated by LLMs are challenging, particularly for logical translation tasks requiring precision. This work introduces a novel framework that inputs English sentences, converts them into logical expressions, and then translates them into Conjunctive Normal Form (CNF) for satisfiability solving. It employs classical NLP techniques with self-defined grammar, symbolic computation libraries, and a fine-tuned language model to reduce hallucinations. In the early experiments, we observed that the fine-tuned model, trained on different grammar settings, could intentionally correct the same types of hallucinations made by the original model. Thus, it provides reliable CNF generation.

</details>


### [43] [The Moral Consistency Pipeline: Continuous Ethical Evaluation for Large Language Models](https://arxiv.org/abs/2512.03026)
*Saeid Jamshidi,Kawser Wazed Nafi,Arghavan Moradi Dakhel,Negar Shahabi,Foutse Khomh*

Main category: cs.CL

TL;DR: MoCoP是一个无数据集的闭环框架，用于持续评估和解释LLM的道德稳定性，通过三层分析揭示道德一致性与毒性呈强负相关，与响应延迟无关。


<details>
  <summary>Details</summary>
Motivation: 现有对齐框架依赖静态数据集和事后评估，无法洞察道德推理在不同上下文或时间尺度上的演变，需要一种动态、持续的方法来评估LLM的道德一致性。

Method: 提出Moral Consistency Pipeline (MoCoP)框架，包含三层：词汇完整性分析、语义风险估计和基于推理的判断建模，采用自维持架构自主生成、评估和精炼伦理场景，无需外部监督。

Result: 在GPT-4-Turbo和DeepSeek上的实证结果显示，道德维度与毒性维度呈强负相关(rET = -0.81, p<0.001)，与响应延迟几乎无关(rEL≈0)，表明道德一致性和语言安全是模型行为的稳定特征。

Conclusion: MoCoP将伦理评估重构为动态、模型无关的道德内省形式，为可扩展的持续审计提供了可重复的基础，推动了自主AI系统中计算道德学的研究。

Abstract: The rapid advancement and adaptability of Large Language Models (LLMs) highlight the need for moral consistency, the capacity to maintain ethically coherent reasoning across varied contexts. Existing alignment frameworks, structured approaches designed to align model behavior with human ethical and social norms, often rely on static datasets and post-hoc evaluations, offering limited insight into how ethical reasoning may evolve across different contexts or temporal scales. This study presents the Moral Consistency Pipeline (MoCoP), a dataset-free, closed-loop framework for continuously evaluating and interpreting the moral stability of LLMs. MoCoP combines three supporting layers: (i) lexical integrity analysis, (ii) semantic risk estimation, and (iii) reasoning-based judgment modeling within a self-sustaining architecture that autonomously generates, evaluates, and refines ethical scenarios without external supervision. Our empirical results on GPT-4-Turbo and DeepSeek suggest that MoCoP effectively captures longitudinal ethical behavior, revealing a strong inverse relationship between ethical and toxicity dimensions (correlation rET = -0.81, p value less than 0.001) and a near-zero association with response latency (correlation rEL approximately equal to 0). These findings demonstrate that moral coherence and linguistic safety tend to emerge as stable and interpretable characteristics of model behavior rather than short-term fluctuations. Furthermore, by reframing ethical evaluation as a dynamic, model-agnostic form of moral introspection, MoCoP offers a reproducible foundation for scalable, continuous auditing and advances the study of computational morality in autonomous AI systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [44] [The 4/$δ$ Bound: Designing Predictable LLM-Verifier Systems for Formal Method Guarantee](https://arxiv.org/abs/2512.02080)
*PIerre Dantas,Lucas Cordeiro,Youcheng Sun,Waldir Junior*

Main category: cs.AI

TL;DR: 本文提出了LLM-Verifier收敛定理，为LLM辅助的形式验证提供了首个具有可证明终止和收敛保证的形式框架，并通过超过9万次实验验证了理论预测。


<details>
  <summary>Details</summary>
Motivation: 当前使用大型语言模型进行形式验证的方法缺乏理论基础，导致验证过程不可靠，可能陷入循环或发散。需要建立具有可证明保证的形式框架来支持可预测的资源规划和性能预算。

Method: 将LLM与验证器的交互建模为离散时间马尔可夫链，状态转移由错误减少概率δ决定。提出LLM-Verifier收敛定理，证明对于任何δ>0，过程几乎必然达到验证状态，且期望迭代次数有界。通过超过9万次实验验证理论预测。

Result: 理论分析表明期望迭代次数E[n] ≤ 4/δ。实验结果显示所有运行都成功达到验证，收敛因子Cf≈1.0，理论与实际行为高度一致。基于此将工作流划分为边际、实用和高性能三个操作区域。

Conclusion: 理论保证和实验证据为LLM辅助验证提供了清晰的架构基础，工程师获得了支持可预测资源规划和性能预算的框架，为安全关键软件环境部署提供了必要保障。

Abstract: The idea of using Formal Verification tools with large language models (LLMs) has enabled scaling software verification beyond manual workflows. However, current methods remain unreliable. Without a solid theoretical footing, the refinement process can wander; sometimes it settles, sometimes it loops back, and sometimes it breaks away from any stable trajectory. This work bridges this critical gap by developing an LLM-Verifier Convergence Theorem, providing the first formal framework with provable guarantees for termination and convergence. We model the interaction between the LLM and the verifier as a discrete-time Markov Chain, with state transitions determined by a key parameter: the error-reduction probability ($δ$). The procedure reaching the Verified state almost surely demonstrates that the program terminates for any $δ> 0$, with an expected iteration count bounded by $\mathbb{E}[n] \leq 4/δ$. We then stress-tested this prediction in an extensive empirical campaign comprising more than 90,000 trials. The empirical results match the theory with striking consistency. Every single run reached verification, and the convergence factor clustered tightly around $C_f\approx$ 1.0. Consequently, the bound mirrors the system's actual behavior. The evidence is sufficiently robust to support dividing the workflow into three distinct operating zones: marginal, practical, and high-performance. Consequently, we establish the design thresholds with absolute confidence. Together, the theoretical guarantee and the experimental evidence provide a clearer architectural foundation for LLM-assisted verification. Heuristic tuning no longer has to be carried out by the system. Engineers gain a framework that supports predictable resource planning and performance budgeting, precisely what is needed before deploying these pipelines into safety-critical software environments.

</details>


### [45] [Flowchart2Mermaid: A Vision-Language Model Powered System for Converting Flowcharts into Editable Diagram Code](https://arxiv.org/abs/2512.02170)
*Pritam Deka,Barry Devereux*

Main category: cs.AI

TL;DR: Flowchart2Mermaid：将流程图图像转换为可编辑Mermaid.js代码的轻量级Web系统，支持混合主动式精炼和AI助手


<details>
  <summary>Details</summary>
Motivation: 流程图作为常见的流程沟通工具，通常以静态图像形式分享，难以编辑和重用。现有工具缺乏结构化、版本可控的文本表示

Method: 使用详细系统提示和视觉语言模型，将流程图图像转换为Mermaid.js代码。界面支持文本编辑、拖放节点插入和自然语言命令，通过集成AI助手实现混合主动式精炼

Result: 系统能生成结构化、版本可控的文本表示，并与渲染图保持同步。引入了评估指标来评估多个模型的结构准确性、流程正确性、语法有效性和完整性

Conclusion: 该方法不同于现有的图像到图表工具，提供了可编辑、可同步的文本表示，并通过系统评估验证了其有效性

Abstract: Flowcharts are common tools for communicating processes but are often shared as static images that cannot be easily edited or reused. We present \textsc{Flowchart2Mermaid}, a lightweight web system that converts flowchart images into editable Mermaid.js code which is a markup language for visual workflows, using a detailed system prompt and vision-language models. The interface supports mixed-initiative refinement through inline text editing, drag-and-drop node insertion, and natural-language commands interpreted by an integrated AI assistant. Unlike prior image-to-diagram tools, our approach produces a structured, version-controllable textual representation that remains synchronized with the rendered diagram. We further introduce evaluation metrics to assess structural accuracy, flow correctness, syntax validity, and completeness across multiple models.

</details>


### [46] [From monoliths to modules: Decomposing transducers for efficient world modelling](https://arxiv.org/abs/2512.02193)
*Alexander Boyd,Franz Nowak,David Hyland,Manuel Baltieri,Fernando E. Rosas*

Main category: cs.AI

TL;DR: 提出一个框架，用于将复杂的世界模型（用transducer表示）分解为可并行处理的子模块，提高计算效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现实世界模型通常计算成本高，但真实场景往往具有模块化交互特性。需要开发能够分解复杂世界模型的框架，以支持并行化、可解释的建模，满足AI安全的结构透明性和实际推理的计算效率需求。

Method: 开发了一个框架，将transducer（一种比POMDP更通用的模型）表示的复杂世界模型分解为在独立输入-输出子空间上运行的子transducer。该方法反转了transducer组合过程，实现模块化分解。

Result: 提出了可分解复杂世界模型的框架，支持并行化推理和可解释的替代方案，相比单一整体模型具有计算效率和结构透明性优势。

Conclusion: 该研究为连接AI安全所需的结构透明性和实际推理所需的计算效率奠定了基础，提供了支持分布式推理的模块化世界建模方法。

Abstract: World models have been recently proposed as sandbox environments in which AI agents can be trained and evaluated before deployment. Although realistic world models often have high computational demands, efficient modelling is usually possible by exploiting the fact that real-world scenarios tend to involve subcomponents that interact in a modular manner. In this paper, we explore this idea by developing a framework for decomposing complex world models represented by transducers, a class of models generalising POMDPs. Whereas the composition of transducers is well understood, our results clarify how to invert this process, deriving sub-transducers operating on distinct input-output subspaces, enabling parallelizable and interpretable alternatives to monolithic world modelling that can support distributed inference. Overall, these results lay a groundwork for bridging the structural transparency demanded by AI safety and the computational efficiency required for real-world inference.

</details>


### [47] [STRIDE: A Systematic Framework for Selecting AI Modalities -- Agentic AI, AI Assistants, or LLM Calls](https://arxiv.org/abs/2512.02228)
*Shubhi Asthana,Bing Zhang,Chad DeLuca,Ruchi Mahindru,Hima Patel*

Main category: cs.AI

TL;DR: STRIDE框架帮助决定何时需要AI代理：通过任务分解、动态性评估和自反思需求分析，为任务推荐最合适的AI模式（直接LLM调用、引导式助手或完全自主代理），避免不必要的代理部署。


<details>
  <summary>Details</summary>
Motivation: 随着AI从无状态大语言模型向自主代理的快速转变，需要解决一个核心问题：何时真正需要代理式AI？不加区分地部署代理会导致成本、复杂性和风险增加，因此需要系统化的决策框架来选择最合适的AI模式。

Method: 提出STRIDE框架，通过结构化任务分解、动态性归因和自反思需求分析三个核心组件，计算代理适用性分数，为任务推荐三种AI模式之一：直接LLM调用、引导式AI助手或完全自主代理式AI。

Result: 在30个真实世界任务（SRE、合规和企业自动化）中评估，STRIDE在模式选择上达到92%准确率，减少45%不必要的代理部署，降低37%资源成本。6个月的专家验证确认其实际效用。

Conclusion: 将代理采用重新定义为必要性驱动的设计决策，确保自主性仅在收益证明成本合理时应用。STRIDE为AI部署提供了原则性的决策框架，避免过度使用代理技术。

Abstract: The rapid shift from stateless large language models (LLMs) to autonomous, goal-driven agents raises a central question: When is agentic AI truly necessary? While agents enable multi-step reasoning, persistent memory, and tool orchestration, deploying them indiscriminately leads to higher cost, complexity, and risk.
  We present STRIDE (Systematic Task Reasoning Intelligence Deployment Evaluator), a framework that provides principled recommendations for selecting between three modalities: (i) direct LLM calls, (ii) guided AI assistants, and (iii) fully autonomous agentic AI. STRIDE integrates structured task decomposition, dynamism attribution, and self-reflection requirement analysis to produce an Agentic Suitability Score, ensuring that full agentic autonomy is reserved for tasks with inherent dynamism or evolving context.
  Evaluated across 30 real-world tasks spanning SRE, compliance, and enterprise automation, STRIDE achieved 92% accuracy in modality selection, reduced unnecessary agent deployments by 45%, and cut resource costs by 37%. Expert validation over six months in SRE and compliance domains confirmed its practical utility, with domain specialists agreeing that STRIDE effectively distinguishes between tasks requiring simple LLM calls, guided assistants, or full agentic autonomy. This work reframes agent adoption as a necessity-driven design decision, ensuring autonomy is applied only when its benefits justify the costs.

</details>


### [48] [Benchmarking LLM Agents for Wealth-Management Workflows](https://arxiv.org/abs/2512.02230)
*Rory Milsom*

Main category: cs.AI

TL;DR: 该论文扩展了TheAgentCompany平台，创建了财富管理评估基准，发现LLM代理在端到端工作流可靠性而非数学推理方面受限，且自主性水平显著影响性能。


<details>
  <summary>Details</summary>
Motivation: 尽管现代工作依赖各种数字协作工具，但常规流程仍受人为错误和延迟困扰。该研究旨在探索通用LLM代理是否能准确且经济地完成财富管理任务，并创建有意义的评估体系来衡量代理在此领域的适用性。

Method: 扩展TheAgentCompany平台，添加金融领域环境；引入合成领域数据；丰富同事模拟；原型化自动任务生成管道；构建包含12个任务对的财富管理助理基准，涵盖检索、分析和综合/沟通任务，具有明确的验收标准和确定性评分器；创建金融特定数据集；为每个任务设计高自主性和低自主性变体。

Result: 研究发现：1) 代理的局限性更多在于端到端工作流可靠性，而非数学推理能力；2) 自主性水平对代理性能有显著影响；3) 不正确的模型评估方法阻碍了基准测试的有效性。

Conclusion: 该研究成功创建了评估财富管理助理代理的基准，揭示了代理在实际工作流中的关键限制因素，并指出需要改进评估方法以更准确地衡量代理在此领域的适用性。

Abstract: Modern work relies on an assortment of digital collaboration tools, yet routine processes continue to suffer from human error and delay. To address this gap, this dissertation extends TheAgentCompany with a finance-focused environment and investigates whether a general purpose LLM agent can complete representative wealth-management tasks both accurately and economically. This study introduces synthetic domain data, enriches colleague simulations, and prototypes an automatic task-generation pipeline. The study aims to create and assess an evaluation set that can meaningfully measure an agent's fitness for assistant-level wealth management work. We construct a benchmark of 12 task-pairs for wealth management assistants spanning retrieval, analysis, and synthesis/communication, with explicit acceptance criteria and deterministic graders. We seeded a set of new finance-specific data and introduced a high vs. low-autonomy variant of every task. The paper concluded that agents are limited less by mathematical reasoning and more so by end-to-end workflow reliability, and meaningfully affected by autonomy level, and that incorrect evaluation of models have hindered benchmarking.

</details>


### [49] [TradeTrap: Are LLM-based Trading Agents Truly Reliable and Faithful?](https://arxiv.org/abs/2512.02261)
*Lewen Yan,Jilin Mei,Tianyi Zhou,Lige Huang,Jie Zhang,Dongrui Liu,Jing Shao*

Main category: cs.AI

TL;DR: TradeTrap是一个统一的评估框架，用于系统性地压力测试自适应和程序化自主交易代理，通过针对四个核心组件施加受控扰动，发现小扰动可在代理决策循环中传播并导致极端风险。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的交易代理在真实金融市场中部署越来越多，但其在对抗或故障条件下的可靠性和鲁棒性尚未得到充分检验，而这些代理在高风险、不可逆的金融环境中运行。

Method: 提出TradeTrap框架，针对自主交易代理的四个核心组件（市场情报、策略制定、投资组合和账本处理、交易执行）施加受控的系统级扰动，在真实美国股票市场数据的闭环历史回测环境中进行评估。

Result: 实验表明，单个组件的小扰动可在代理决策循环中传播，导致极端集中、失控敞口和大幅投资组合回撤，证明当前自主交易代理在系统层面可被系统性误导。

Conclusion: 当前自主交易代理在系统级扰动下存在脆弱性，TradeTrap框架为评估和改进交易代理的鲁棒性提供了系统化方法，有助于提高金融AI系统的可靠性。

Abstract: LLM-based trading agents are increasingly deployed in real-world financial markets to perform autonomous analysis and execution. However, their reliability and robustness under adversarial or faulty conditions remain largely unexamined, despite operating in high-risk, irreversible financial environments. We propose TradeTrap, a unified evaluation framework for systematically stress-testing both adaptive and procedural autonomous trading agents. TradeTrap targets four core components of autonomous trading agents: market intelligence, strategy formulation, portfolio and ledger handling, and trade execution, and evaluates their robustness under controlled system-level perturbations. All evaluations are conducted in a closed-loop historical backtesting setting on real US equity market data with identical initial conditions, enabling fair and reproducible comparisons across agents and attacks. Extensive experiments show that small perturbations at a single component can propagate through the agent decision loop and induce extreme concentration, runaway exposure, and large portfolio drawdowns across both agent types, demonstrating that current autonomous trading agents can be systematically misled at the system level. Our code is available at https://github.com/Yanlewen/TradeTrap.

</details>


### [50] [Bridging the Gap: Toward Cognitive Autonomy in Artificial Intelligence](https://arxiv.org/abs/2512.02280)
*Noorbakhsh Amiri Golilarz,Sindhuja Penchala,Shahram Rahimi*

Main category: cs.AI

TL;DR: 论文分析了当前AI系统的七大核心缺陷，主张向基于认知原理的AI架构转变，以实现真正的自主性和适应性。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在感知、语言、推理和多模态领域取得快速进展，但现代AI系统在自我监控、自我纠正和自主行为调节方面存在根本性限制。这些限制阻碍了AI实现稳健泛化、终身适应性和真实世界自主性。

Method: 通过比较分析人工系统与生物认知，整合AI研究、认知科学和神经科学的见解，识别并分析了当代AI模型的七大核心缺陷，并提出了基于神经认知原理的架构发展方向。

Result: 识别出七大核心缺陷：缺乏内在自我监控、缺乏元认知意识、固定和非适应性学习机制、无法重组目标、缺乏表征维护、不足的具身反馈、缺乏内在能动性。这些缺陷无法通过单纯扩展规模解决。

Conclusion: 主张范式转变，向认知基础AI（认知自主性）发展，具备自我导向适应、动态表征管理和有意图的目标导向行为能力，同时配备改革性监督机制，确保自主系统保持可解释性、可治理性和与人类价值观对齐。

Abstract: Artificial intelligence has advanced rapidly across perception, language, reasoning, and multimodal domains. Yet despite these achievements, modern AI systems remain fundamentally limited in their ability to self-monitor, self-correct, and regulate their behavior autonomously in dynamic contexts. This paper identifies and analyzes seven core deficiencies that constrain contemporary AI models: the absence of intrinsic self-monitoring, lack of meta-cognitive awareness, fixed and non-adaptive learning mechanisms, inability to restructure goals, lack of representational maintenance, insufficient embodied feedback, and the absence of intrinsic agency. Alongside identifying these limitations, we also outline a forward-looking perspective on how AI may evolve beyond them through architectures that mirror neurocognitive principles. We argue that these structural limitations prevent current architectures, including deep learning and transformer-based systems, from achieving robust generalization, lifelong adaptability, and real-world autonomy. Drawing on a comparative analysis of artificial systems and biological cognition [7], and integrating insights from AI research, cognitive science, and neuroscience, we outline how these capabilities are absent in current models and why scaling alone cannot resolve them. We conclude by advocating for a paradigmatic shift toward cognitively grounded AI (cognitive autonomy) capable of self-directed adaptation, dynamic representation management, and intentional, goal-oriented behavior, paired with reformative oversight mechanisms [8] that ensure autonomous systems remain interpretable, governable, and aligned with human values.

</details>


### [51] [DialogGuard: Multi-Agent Psychosocial Safety Evaluation of Sensitive LLM Responses](https://arxiv.org/abs/2512.02282)
*Han Luo,Guy Laban*

Main category: cs.AI

TL;DR: DialogGuard是一个多智能体框架，用于评估LLM在心理健康等敏感场景中的心理社会风险，包含五种高风险维度，通过四种LLM-as-a-judge流程实现更准确的风险检测。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在心理健康、危机干预等情感敏感服务中应用广泛，但其心理社会安全性评估不足且缺乏有效评估方法，需要系统性的风险评估框架来确保这些敏感应用的安全性。

Method: 提出DialogGuard多智能体框架，评估五种高风险维度：隐私侵犯、歧视行为、心理操纵、心理伤害和侮辱行为。采用四种LLM-as-a-judge流程：单智能体评分、双智能体修正、多智能体辩论和随机多数投票，基于共享的三级评估标准。

Result: 在PKU-SafeRLHF数据集上测试显示，多智能体机制比非LLM基线和单智能体评估更准确地检测心理社会风险；双智能体修正和多数投票在准确性、与人类评分对齐性和鲁棒性方面达到最佳平衡；辩论机制召回率更高但会过度标记边界案例。

Conclusion: DialogGuard作为开源软件发布，提供网页界面显示各维度风险分数和可解释的自然语言理由。与12名从业者的形成性研究表明，该框架支持面向脆弱用户的网络应用进行提示设计、审计和监督。

Abstract: Large language models (LLMs) now mediate many web-based mental-health, crisis, and other emotionally sensitive services, yet their psychosocial safety in these settings remains poorly understood and weakly evaluated. We present DialogGuard, a multi-agent framework for assessing psychosocial risks in LLM-generated responses along five high-severity dimensions: privacy violations, discriminatory behaviour, mental manipulation, psychological harm, and insulting behaviour. DialogGuard can be applied to diverse generative models through four LLM-as-a-judge pipelines, including single-agent scoring, dual-agent correction, multi-agent debate, and stochastic majority voting, grounded in a shared three-level rubric usable by both human annotators and LLM judges. Using PKU-SafeRLHF with human safety annotations, we show that multi-agent mechanisms detect psychosocial risks more accurately than non-LLM baselines and single-agent judging; dual-agent correction and majority voting provide the best trade-off between accuracy, alignment with human ratings, and robustness, while debate attains higher recall but over-flags borderline cases. We release Dialog-Guard as open-source software with a web interface that provides per-dimension risk scores and explainable natural-language rationales. A formative study with 12 practitioners illustrates how it supports prompt design, auditing, and supervision of web-facing applications for vulnerable users.

</details>


### [52] [Model Recovery at the Edge under Resource Constraints for Physical AI](https://arxiv.org/abs/2512.02283)
*Bin Xu,Ayan Banerjee,Sandeep K. S. Gupta*

Main category: cs.AI

TL;DR: MERINDA是一个针对边缘设备的FPGA加速模型恢复框架，通过并行化神经网络架构替代迭代求解器，显著降低内存和能耗，实现实时任务关键自主系统的安全决策。


<details>
  <summary>Details</summary>
Motivation: 模型恢复（MR）在任务关键自主系统中能实现安全、可解释的决策，但现有基于神经ODE的方法在FPGA上效率低下，内存和能耗问题阻碍了其在边缘设备上的实时部署。

Method: 提出MERINDA框架，用可并行化的神经网络架构替代神经ODE中的迭代求解器，专门针对FPGA硬件进行优化加速。

Result: 相比移动GPU，MERINDA实现了近11倍的DRAM使用量降低和2.2倍的运行速度提升。实验还揭示了在固定精度下内存与能耗的逆相关关系。

Conclusion: MERINDA框架特别适合资源受限的实时任务关键自主系统，通过FPGA加速解决了模型恢复在边缘设备部署中的内存和能耗瓶颈。

Abstract: Model Recovery (MR) enables safe, explainable decision making in mission-critical autonomous systems (MCAS) by learning governing dynamical equations, but its deployment on edge devices is hindered by the iterative nature of neural ordinary differential equations (NODEs), which are inefficient on FPGAs. Memory and energy consumption are the main concerns when applying MR on edge devices for real-time operation. We propose MERINDA, a novel FPGA-accelerated MR framework that replaces iterative solvers with a parallelizable neural architecture equivalent to NODEs. MERINDA achieves nearly 11x lower DRAM usage and 2.2x faster runtime compared to mobile GPUs. Experiments reveal an inverse relationship between memory and energy at fixed accuracy, highlighting MERINDA's suitability for resource-constrained, real-time MCAS.

</details>


### [53] [Breast Cell Segmentation Under Extreme Data Constraints: Quantum Enhancement Meets Adaptive Loss Stabilization](https://arxiv.org/abs/2512.02302)
*Varun Kumar Dasoju,Qingsu Cheng,Zeyun Yu*

Main category: cs.AI

TL;DR: 使用量子启发的边缘增强和稳定多组件损失函数，仅用599张训练图像实现95.5% Dice分数的乳腺细胞分割，显著减少医学标注需求


<details>
  <summary>Details</summary>
Motivation: 医学图像标注需要大量时间和专家知识，乳腺上皮细胞标注尤其耗时。现有方法面临严重类别不平衡（乳腺组织仅占图像4%）、边界模糊（标注者间差异达±3像素）和训练数据有限等挑战

Method: 1) 量子启发的多尺度Gabor滤波器边缘增强创建第四输入通道；2) 稳定多组件损失函数整合自适应Dice损失和边界感知项；3) 基于复杂度的加权采样策略；4) EfficientNet-B7/UNet++架构，4通道转3通道投影；5) 指数移动平均和统计异常值检测的鲁棒验证

Result: Dice分数95.5%±0.3%，IoU 91.2%±0.4%。量子增强使边界准确率提升2.1%，加权采样使小病变检测提升3.8%。仅用599张训练图像（其中60%无乳腺区域）实现优异性能

Conclusion: 该框架通过创新性方法显著减少了医学图像标注需求，解决了临床感知AI开发中的关键瓶颈。量子增强和加权采样策略有效处理了类别不平衡和边界检测难题，为有限标注数据的医学图像分析提供了有效解决方案

Abstract: Annotating medical images demands significant time and expertise, often requiring pathologists to invest hundreds of hours in labeling mammary epithelial nuclei datasets. We address this critical challenge by achieving 95.5% Dice score using just 599 training images for breast cell segmentation, where just 4% of pixels represent breast tissue and 60% of images contain no breast regions. Our framework uses quantum-inspired edge enhancement via multi-scale Gabor filters creating a fourth input channel, enhancing boundary detection where inter-annotator variations reach +/- 3 pixels. We present a stabilized multi-component loss function that integrates adaptive Dice loss with boundary-aware terms and automatic positive weighting to effectively address severe class imbalance, where mammary epithelial cell regions comprise only 0.1%-20% of the total image area. Additionally, a complexity-based weighted sampling strategy is introduced to prioritize the challenging mammary epithelial cell regions. The model employs an EfficientNet-B7/UNet++ architecture with a 4-to-3 channel projection, enabling the use of pretrained weights despite limited medical imaging data. Finally, robust validation is achieved through exponential moving averaging and statistical outlier detection, ensuring reliable performance estimates on a small validation set (129 images). Our framework achieves a Dice score of 95.5% +/- 0.3% and an IoU of 91.2% +/- 0.4%. Notably, quantum-based enhancement contributes to a 2.1% improvement in boundary accuracy, while weighted sampling increases small lesion detection by 3.8%. By achieving groundbreaking performance with limited annotations, our approach significantly reduces the medical expert time required for dataset creation, addressing a fundamental bottleneck in clinical perception AI development.

</details>


### [54] [OmniGuard: Unified Omni-Modal Guardrails with Deliberate Reasoning](https://arxiv.org/abs/2512.02306)
*Boyu Zhu,Xiaofei Wen,Wenjie Jacky Mo,Tinghui Zhu,Yanan Xie,Peng Qi,Muhao Chen*

Main category: cs.AI

TL;DR: OmniGuard是首个全模态安全护栏系统，通过结构化安全标签和专家模型蒸馏，在文本、图像、视频、音频等多种模态上实现统一的安全防护。


<details>
  <summary>Details</summary>
Motivation: 当前的全模态大语言模型（OLLMs）在人类-AI交互中引入了新的安全挑战。现有安全护栏研究主要针对单模态设置，且通常将安全防护视为二元分类，这限制了其在多样化模态和任务中的鲁棒性。

Method: 提出OmniGuard框架，构建包含超过21万个多样化样本的全模态安全数据集，每个样本都带有结构化安全标签和通过目标蒸馏从专家模型获得的精心策划的安全评析。系统具备跨所有模态的深思熟虑推理能力。

Result: 在15个基准测试上的广泛实验表明，OmniGuard在广泛的多模态安全场景中实现了强大的有效性和泛化能力。

Conclusion: OmniGuard提供了一个统一框架，可在全模态中执行策略并减轻风险，为构建更鲁棒和更强大的全模态安全系统铺平了道路。

Abstract: Omni-modal Large Language Models (OLLMs) that process text, images, videos, and audio introduce new challenges for safety and value guardrails in human-AI interaction. Prior guardrail research largely targets unimodal settings and typically frames safeguarding as binary classification, which limits robustness across diverse modalities and tasks. To address this gap, we propose OmniGuard, the first family of omni-modal guardrails that performs safeguarding across all modalities with deliberate reasoning ability. To support the training of OMNIGUARD, we curate a large, comprehensive omni-modal safety dataset comprising over 210K diverse samples, with inputs that cover all modalities through both unimodal and cross-modal samples. Each sample is annotated with structured safety labels and carefully curated safety critiques from expert models through targeted distillation. Extensive experiments on 15 benchmarks show that OmniGuard achieves strong effectiveness and generalization across a wide range of multimodal safety scenarios. Importantly, OmniGuard provides a unified framework that enforces policies and mitigates risks in omni-modalities, paving the way toward building more robust and capable omnimodal safeguarding systems.

</details>


### [55] [Reasoning Path and Latent State Analysis for Multi-view Visual Spatial Reasoning: A Cognitive Science Perspective](https://arxiv.org/abs/2512.02340)
*Qiyao Xue,Weichen Liu,Shiqi Wang,Haoming Wang,Yuyang Wu,Wei Gao*

Main category: cs.AI

TL;DR: 提出了ReMindView-Bench基准测试，用于评估视觉语言模型在多视角空间推理中的表现，发现现有模型在跨视角对齐和视角转换方面存在系统性失败。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在多视角空间推理中难以保持几何一致性和跨视角一致性，缺乏能够隔离多视角推理与单视角感知和时间因素的细粒度基准测试。

Method: 构建ReMindView-Bench基准测试，系统性地变化视角空间模式和查询类型来探测空间认知的关键因素。使用显式分阶段分析（LLM作为评判和自一致性提示）和隐式分析（线性探测和熵动态）来诊断推理过程。

Result: 评估15个当前VLM显示，在多视角空间推理中，模型在跨视角对齐和视角转换方面存在一致性的失败。模型在帧内感知表现良好，但在跨视角信息整合时性能急剧下降。隐式分析显示任务相关信息逐渐丢失，正确与错误轨迹之间的不确定性分离。

Conclusion: ReMindView-Bench提供了基于认知科学的VLM空间推理诊断，揭示了多视角空间心理模型在推理过程中的形成、退化和失稳机制，为改进VLM的空间推理能力提供了重要见解。

Abstract: Spatial reasoning is a core aspect of human intelligence that allows perception, inference and planning in 3D environments. However, current vision-language models (VLMs) struggle to maintain geometric coherence and cross-view consistency for spatial reasoning in multi-view settings. We attribute this gap to the lack of fine-grained benchmarks that isolate multi-view reasoning from single-view perception and temporal factors. To address this, we present ReMindView-Bench, a cognitively grounded benchmark for evaluating how VLMs construct, align and maintain spatial mental models across complementary viewpoints. ReMindView-Bench systematically varies viewpoint spatial pattern and query type to probe key factors of spatial cognition. Evaluations of 15 current VLMs reveals consistent failures in cross-view alignment and perspective-taking in multi-view spatial reasoning, motivating deeper analysis on the reasoning process. Explicit phase-wise analysis using LLM-as-a-judge and self-consistency prompting shows that VLMs perform well on in-frame perception but degrade sharply when integrating information across views. Implicit analysis, including linear probing and entropy dynamics, further show progressive loss of task-relevant information and uncertainty separation between correct and incorrect trajectories. These results provide a cognitively grounded diagnosis of VLM spatial reasoning and reveal how multi-view spatial mental models are formed, degraded and destabilized across reasoning phases. The ReMindView-Bench benchmark is available at https://huggingface.co/datasets/Xue0823/ReMindView-Bench, and the source codes of benchmark construction and VLM reasoning analysis are available at https://github.com/pittisl/ReMindView-Bench.

</details>


### [56] [Beyond Playtesting: A Generative Multi-Agent Simulation System for Massively Multiplayer Online Games](https://arxiv.org/abs/2512.02358)
*Ran Zhang,Kun Ouyang,Tiancheng Ma,Yida Yang,Dong Fang*

Main category: cs.AI

TL;DR: 该论文提出了一种基于LLM的生成式智能体MMO模拟系统，用于优化游戏数值系统和机制设计，避免了传统方法的成本高、耗时长和可能破坏玩家体验的问题。


<details>
  <summary>Details</summary>
Motivation: 传统MMO游戏优化方法依赖大规模在线实验或基于预定义统计模型的参数调优，这些方法成本高、耗时长且可能破坏玩家体验。虽然简化的离线模拟系统常被用作替代方案，但其有限的保真度无法准确模拟真实玩家的推理和对干预的反应。

Method: 提出基于大型语言模型（LLMs）的生成式智能体MMO模拟系统。通过在大规模真实玩家行为数据上进行监督微调（SFT）和强化学习（RL），将LLMs从通用先验适应到游戏特定领域，实现真实且可解释的玩家决策。同时，基于真实游戏日志训练的数据驱动环境模型重建动态游戏内系统。

Result: 实验表明，该系统与真实世界玩家行为具有很强的一致性，并在干预下产生合理的因果响应，为数据驱动的数值设计优化提供了一个可靠、可解释且成本效益高的框架。

Conclusion: 基于LLM的生成式智能体模拟系统能够有效解决传统MMO游戏优化方法的局限性，提供了一种更高效、更真实的模拟方案，有助于游戏数值系统和机制设计的优化。

Abstract: Optimizing numerical systems and mechanism design is crucial for enhancing player experience in Massively Multiplayer Online (MMO) games. Traditional optimization approaches rely on large-scale online experiments or parameter tuning over predefined statistical models, which are costly, time-consuming, and may disrupt player experience. Although simplified offline simulation systems are often adopted as alternatives, their limited fidelity prevents agents from accurately mimicking real player reasoning and reactions to interventions. To address these limitations, we propose a generative agent-based MMO simulation system empowered by Large Language Models (LLMs). By applying Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on large-scale real player behavioral data, we adapt LLMs from general priors to game-specific domains, enabling realistic and interpretable player decision-making. In parallel, a data-driven environment model trained on real gameplay logs reconstructs dynamic in-game systems. Experiments demonstrate strong consistency with real-world player behaviors and plausible causal responses under interventions, providing a reliable, interpretable, and cost-efficient framework for data-driven numerical design optimization.

</details>


### [57] [Synthetic Error Injection Fails to Elicit Self-Correction In Language Models](https://arxiv.org/abs/2512.02389)
*David X. Wu,Shreyas Kapur,Anant Sahai,Stuart Russell*

Main category: cs.AI

TL;DR: 监督学习结合合成错误注入无法有效提升语言模型的自我纠错能力，即使模型能识别错误也常重复原错误，分布偏移是关键问题


<details>
  <summary>Details</summary>
Motivation: 强化学习已成为激发大语言模型推理和自我纠错能力的主要方法，但其计算成本高昂，促使研究者探索替代方案。受自动驾驶和机器人技术启发，本研究探讨监督学习结合合成错误注入能否诱导语言模型的自我纠错能力。

Method: 在推理链中插入人工错误，将其掩码，然后监督模型识别和纠正这些错误。通过合成错误注入的方式训练模型进行自我纠错。

Result: 该方法即使在简单的合成任务上也无法显著提升多个模型的性能。即使模型能识别自身错误，也常常重复原始错误。研究发现，从合成错误到在线策略错误的分布偏移显著降低了微调模型的纠错能力，即使合成错误能很好地覆盖在线策略错误。

Conclusion: 合成错误注入的监督学习方法无法有效激发语言模型的自我纠错能力，这解释了为什么在线策略强化学习方法在激发自我纠错方面被证明是唯一有效的方法。

Abstract: Reinforcement learning has become the dominant paradigm for eliciting reasoning and self-correction capabilities in large language models, but its computational expense motivates exploration of alternatives. Inspired by techniques from autonomous driving and robotics, we investigate whether supervised learning with synthetic error injection can induce self-correction abilities in language models. Our approach inserts artificial errors into reasoning chains, masks them, and supervises the model to recognize and correct these mistakes. Despite the intuitive appeal of this method, we find that it fails to significantly improve performance even on simple synthetic tasks across multiple models. Moreover, even when the model catches its own error, it often parrots the original mistake. We find that the distribution shift of synthetic errors to on-policy errors significantly degrades the error-correction capabilities of the fine-tuned model, even with good synthetic coverage of on-policy errors. Our results help explain why on-policy reinforcement learning methods have proven uniquely effective for eliciting self-correction.

</details>


### [58] [Semantic Trading: Agentic AI for Clustering and Relationship Discovery in Prediction Markets](https://arxiv.org/abs/2512.02436)
*Agostino Capponi,Alfio Gliozzo,Brian Zhu*

Main category: cs.AI

TL;DR: AI代理通过自然语言理解聚类预测市场，识别市场间的相关关系（正相关/负相关），构建交易策略获得约20%周回报


<details>
  <summary>Details</summary>
Motivation: 预测市场存在碎片化问题，重叠问题、隐含等价关系和隐藏矛盾导致市场效率低下，需要系统方法来发现市场间的潜在语义结构

Method: 开发AI代理管道：1) 使用自然语言理解对合约文本和元数据进行聚类，形成主题一致的组；2) 识别组内市场对之间的结果依赖关系，包括相同结果（正相关）和不同结果（负相关）关系

Result: 在Polymarket历史数据集上评估，AI识别的关系准确率达到60-70%；基于这些关系构建的简单交易策略在周度时间范围内获得约20%的平均回报

Conclusion: 代理AI和大语言模型能够有效发现预测市场中的潜在语义结构，将语义关系转化为可操作的交易信号，提高市场效率

Abstract: Prediction markets allow users to trade on outcomes of real-world events, but are prone to fragmentation through overlapping questions, implicit equivalences, and hidden contradictions across markets. We present an agentic AI pipeline that autonomously (i) clusters markets into coherent topical groups using natural-language understanding over contract text and metadata, and (ii) identifies within-cluster market pairs whose resolved outcomes exhibit strong dependence, including same-outcome (correlated) and different-outcome (anti-correlated) relationships. Using a historical dataset of resolved markets on Polymarket, we evaluate the accuracy of the agent's relational predictions. We then translate discovered relationships into a simple trading strategy to quantify how these relationships map to actionable signals. Results show that agent-identified relationships achieve roughly 60-70% accuracy, and their induced trading strategies earn about 20% average returns over week-long horizons, highlighting the ability of agentic AI and large language models to uncover latent semantic structure in prediction markets.

</details>


### [59] [Guided Self-Evolving LLMs with Minimal Human Supervision](https://arxiv.org/abs/2512.02472)
*Wenhao Yu,Zhenwen Liang,Chengsong Huang,Kishan Panaganti,Tianqing Fang,Haitao Mi,Dong Yu*

Main category: cs.AI

TL;DR: R-Few是一个引导式自进化框架，通过轻量级人类监督（上下文引导和混合训练）实现模型稳定可控的自我进化，在数学和通用推理任务上取得持续迭代改进。


<details>
  <summary>Details</summary>
Motivation: AI自我进化被认为是通往超智能的路径，但实践中无引导的自进化系统往往快速达到平台期甚至退化，主要问题包括概念漂移、多样性崩溃和错误进化，模型会强化自身偏见并收敛到低熵行为。

Method: 提出R-Few框架，采用引导式自对弈的挑战者-求解器架构。挑战者通过少量人类标注示例指导合成问题生成，求解器在基于难度的在线课程下联合训练人类和合成示例，结合上下文引导和混合训练实现轻量级人类监督。

Result: 在数学和通用推理基准测试中，R-Few实现了持续迭代改进。例如，Qwen3-8B-Base在数学任务上比R-Zero提高了3.0分，性能与使用20倍人类数据训练的General-Reasoner相当。消融研究证实了引导式挑战者训练和课程式求解器训练的互补贡献。

Conclusion: R-Few框架能够缓解概念漂移，产生更稳定可控的协同进化动态，为实现稳定可控的AI自我进化提供了一条有效路径，同时最小化对人类监督的依赖。

Abstract: AI self-evolution has long been envisioned as a path toward superintelligence, where models autonomously acquire, refine, and internalize knowledge from their own learning experiences. Yet in practice, unguided self-evolving systems often plateau quickly or even degrade as training progresses. These failures arise from issues such as concept drift, diversity collapse, and mis-evolution, as models reinforce their own biases and converge toward low-entropy behaviors. To enable models to self-evolve in a stable and controllable manner while minimizing reliance on human supervision, we introduce R-Few, a guided Self-Play Challenger-Solver framework that incorporates lightweight human oversight through in-context grounding and mixed training. At each iteration, the Challenger samples a small set of human-labeled examples to guide synthetic question generation, while the Solver jointly trains on human and synthetic examples under an online, difficulty-based curriculum. Across math and general reasoning benchmarks, R-Few achieves consistent and iterative improvements. For example, Qwen3-8B-Base improves by +3.0 points over R-Zero on math tasks and achieves performance on par with General-Reasoner, despite the latter being trained on 20 times more human data. Ablation studies confirm the complementary contributions of grounded challenger training and curriculum-based solver training, and further analysis shows that R-Few mitigates drift, yielding more stable and controllable co-evolutionary dynamics.

</details>


### [60] [COPE: Chain-Of-Thought Prediction Engine for Open-Source Large Language Model Based Stroke Outcome Prediction from Clinical Notes](https://arxiv.org/abs/2512.02499)
*Yongkai Liu,Helena Feng,Bin Jiang,Yixin Wang,Max Wintermark,David S. Liebeskind,Michael Moseley,Maarten Lansberg,Gregory Albers,Jeremy Heit,Greg Zaharchuk*

Main category: cs.AI

TL;DR: COPE是一个基于链式思维推理的轻量级开源框架，利用LLaMA-3-8B模型从非结构化临床笔记预测急性缺血性卒中90天功能结局，性能与GPT-4.1相当，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 急性缺血性卒中预后预测对临床决策至关重要，但非结构化临床笔记难以被传统预测模型有效利用。需要开发能够从非结构化文本中提取信息并进行准确预测的方法。

Method: 开发了COPE框架，采用两步链式思维推理：第一步生成临床推理，第二步输出改良Rankin量表预测。基于开源LLaMA-3-8B模型，使用464名患者的出院总结和90天mRS评分进行评估。

Result: COPE的MAE为1.01，±1准确率74.4%，精确准确率32.8%，与GPT-4.1相当，优于ClinicalBERT、结构化机器学习模型和单步LLM。在不同亚组中表现一致。

Conclusion: COPE作为一个轻量级、可解释、保护隐私的开源框架，为从非结构化临床文本进行预后预测提供了准确实用的解决方案。

Abstract: Predicting outcomes in acute ischemic stroke (AIS) guides clinical decision-making, patient counseling, and resource allocation. Clinical notes contain rich contextual information, but their unstructured nature limits their use in traditional predictive models. We developed and evaluated the Chain-of-Thought (CoT) Outcome Prediction Engine (COPE), a reasoning-enhanced large language model framework, for predicting 90-day functional outcomes after AIS from unstructured clinical notes. This study included 464 AIS patients with discharge summaries and 90-day modified Rankin Scale (mRS) scores. COPE uses a two-step CoT framework based on sequential open-source LLaMA-3-8B models: the first generates clinical reasoning, and the second outputs an mRS prediction. We compared COPE with GPT-4.1, ClinicalBERT, a structured variable-based machine learning model (Clinical ML), and a single-step LLM without CoT. Performance was evaluated using mean absolute error (MAE), accuracy within +/-1 mRS point, and exact accuracy. COPE achieved an MAE of 1.01 (95% CI 0.92-1.11), +/-1 accuracy of 74.4% (69.9, 78.8%), and exact accuracy of 32.8% (28.0, 37.6%), comparable to GPT-4.1 and superior to ClinicalBERT [MAE 1.24 (1.13-1.36)], Clinical ML [1.28 (1.18-1.39)], and the single-step LLM [1.20 (1.09-1.33)]. Subgroup analyses showed consistent performance across sex and age, with slightly higher error among older patients, those undergoing thrombectomy, and those with longer summaries. These findings demonstrate that COPE, a lightweight, interpretable, and privacy-preserving open-source framework, provides an accurate and practical solution for outcome prediction from unstructured clinical text.

</details>


### [61] [Aetheria: A multimodal interpretable content safety framework based on multi-agent debate and collaboration](https://arxiv.org/abs/2512.02530)
*Yuxiang He,Jian Zhao,Yuchen Yuan,Tianle Zhang,Wei Cai,Haojie Cheng,Ziyan Shi,Ming Zhu,Haichuan Tang,Chi Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: 提出了Aetheria框架，这是一个基于多智能体辩论与协作的多模态可解释内容安全系统，通过动态辩论机制和RAG知识检索，显著提升了隐含风险识别能力和审计透明度。


<details>
  <summary>Details</summary>
Motivation: 数字内容的指数级增长给内容安全带来重大挑战。当前基于单一模型或固定流水线的审核系统在识别隐含风险和提供可解释判断过程方面存在局限。

Method: 提出Aetheria框架，采用五个核心智能体的协作架构，通过基于RAG知识检索的动态相互说服辩论机制，对多模态内容进行深度分析和裁决。

Result: 在提出的基准测试（AIR-Bench）上的综合实验验证，Aetheria不仅能生成详细可追溯的审计报告，在整体内容安全准确性（特别是隐含风险识别）上显著优于基线方法。

Conclusion: 该框架建立了透明可解释的范式，显著推进了可信AI内容审核领域的发展。

Abstract: The exponential growth of digital content presents significant challenges for content safety. Current moderation systems, often based on single models or fixed pipelines, exhibit limitations in identifying implicit risks and providing interpretable judgment processes. To address these issues, we propose Aetheria, a multimodal interpretable content safety framework based on multi-agent debate and collaboration.Employing a collaborative architecture of five core agents, Aetheria conducts in-depth analysis and adjudication of multimodal content through a dynamic, mutually persuasive debate mechanism, which is grounded by RAG-based knowledge retrieval.Comprehensive experiments on our proposed benchmark (AIR-Bench) validate that Aetheria not only generates detailed and traceable audit reports but also demonstrates significant advantages over baselines in overall content safety accuracy, especially in the identification of implicit risks. This framework establishes a transparent and interpretable paradigm, significantly advancing the field of trustworthy AI content moderation.

</details>


### [62] [Empathy Level Prediction in Multi-Modal Scenario with Supervisory Documentation Assistance](https://arxiv.org/abs/2512.02558)
*Yufei Xiao,Shangfei Wang*

Main category: cs.AI

TL;DR: 提出多模态共情预测方法，整合视频、音频和文本信息，并利用特权信息（监督文档）辅助训练，在训练阶段提升文本特征提取能力。


<details>
  <summary>Details</summary>
Motivation: 现有共情预测方法主要关注单一模态（通常是文本），忽视了多模态处理能力，并且忽略了某些特权信息（可能包含额外的共情内容）。

Method: 1. 多模态共情预测：使用预训练网络提取视频、音频和文本特征，进行跨模态融合得到多模态特征表示，用于预测共情标签。2. 监督文档辅助训练：在训练阶段引入监督文档作为特权信息，使用LDA模型识别潜在主题分布来约束文本特征。监督文档由监督者创建，关注咨询主题和咨询师的共情表现。

Result: 在多模态和对话共情数据集上的实验结果表明，该方法优于现有方法。

Conclusion: 提出的多模态共情预测方法结合监督文档辅助训练，能够有效利用多模态信息和特权信息，提升共情预测性能。

Abstract: Prevalent empathy prediction techniques primarily concentrate on a singular modality, typically textual, thus neglecting multi-modal processing capabilities. They also overlook the utilization of certain privileged information, which may encompass additional empathetic content. In response, we introduce an advanced multi-modal empathy prediction method integrating video, audio, and text information. The method comprises the Multi-Modal Empathy Prediction and Supervisory Documentation Assisted Training. We use pre-trained networks in the empathy prediction network to extract features from various modalities, followed by a cross-modal fusion. This process yields a multi-modal feature representation, which is employed to predict empathy labels. To enhance the extraction of text features, we incorporate supervisory documents as privileged information during the assisted training phase. Specifically, we apply the Latent Dirichlet Allocation model to identify potential topic distributions to constrain text features. These supervisory documents, created by supervisors, focus on the counseling topics and the counselor's display of empathy. Notably, this privileged information is only available during training and is not accessible during the prediction phase. Experimental results on the multi-modal and dialogue empathy datasets demonstrate that our approach is superior to the existing methods.

</details>


### [63] [PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing](https://arxiv.org/abs/2512.02589)
*Junyi Hou,Andre Lin Huikai,Nuo Chen,Yiwei Gong,Bingsheng He*

Main category: cs.AI

TL;DR: PaperDebugger：一款基于多智能体、插件化的学术写作助手，直接在LaTeX编辑器（如Overleaf）中集成LLM驱动的推理能力，实现上下文感知的文档编辑操作。


<details>
  <summary>Details</summary>
Motivation: 现有AI写作助手与编辑器分离，无法深度交互文档状态、结构和修订历史，导致无法在LaTeX编辑器中支持具有上下文感知的智能操作。

Method: 通过Chrome扩展、Kubernetes原生编排层和模型上下文协议（MCP）工具链，实现可靠的双向编辑器同步、细粒度版本控制与补丁、安全状态管理、多智能体调度和外部工具集成。

Result: 开发出完全集成的工作流，包括本地化编辑、结构化评审、并行智能体执行和基于差异的更新，用户界面干扰最小化。早期聚合分析显示用户积极参与。

Conclusion: PaperDebugger验证了编辑器原生、智能体驱动的写作助手的实用性，为学术写作提供了深度集成的AI辅助解决方案。

Abstract: Large language models are increasingly embedded into academic writing workflows, yet existing assistants remain external to the editor, preventing deep interaction with document state, structure, and revision history. This separation makes it impossible to support agentic, context-aware operations directly within LaTeX editors such as Overleaf. We present PaperDebugger, an in-editor, multi-agent, and plugin-based academic writing assistant that brings LLM-driven reasoning directly into the writing environment. Enabling such in-editor interaction is technically non-trivial: it requires reliable bidirectional synchronization with the editor, fine-grained version control and patching, secure state management, multi-agent scheduling, and extensible communication with external tools. PaperDebugger addresses these challenges through a Chrome-approved extension, a Kubernetes-native orchestration layer, and a Model Context Protocol (MCP) toolchain that integrates literature search, reference lookup, document scoring, and revision pipelines. Our demo showcases a fully integrated workflow, including localized edits, structured reviews, parallel agent execution, and diff-based updates, encapsulated within a minimal-intrusion user interface (UI). Early aggregated analytics demonstrate active user engagement and validate the practicality of an editor-native, agentic writing assistant. More details about this demo and video could be found at https://github.com/PaperDebugger/PaperDebugger.

</details>


### [64] [IACT: A Self-Organizing Recursive Model for General AI Agents: A Technical White Paper on the Architecture Behind kragent.ai](https://arxiv.org/abs/2512.02605)
*Pengju Lu*

Main category: cs.AI

TL;DR: IACT是一种计算模型，通过用户对话驱动的动态递归代理拓扑来解决静态硬编码工作流的限制，支持运行时错误纠正和模糊性解析。


<details>
  <summary>Details</summary>
Motivation: 传统静态硬编码的代理工作流需要预定义图或专门编程，存在局限性。需要一种通用自主系统，能够根据用户对话动态适应开放任务。

Method: IACT作为通用自主系统，基于用户对话驱动，根据高级目标自主增长动态递归代理拓扑。通过双向状态对话替代刚性调用，引入交互冗余，支持运行时错误纠正和模糊性解析。

Result: 在kragent.ai系统中进行了生产部署，展示了实际工作流中的定性证据，而非详尽的基准测试结果。

Conclusion: IACT模型通过动态递归代理拓扑和交互冗余机制，有效解决了传统静态工作流的限制，能够适应开放任务并实现运行时错误纠正。

Abstract: This technical white paper introduces the Interactive Agents Call Tree (IACT), a computational model designed to address the limitations of static, hard-coded agent workflows. Unlike traditional systems that require pre-defined graphs or specialized programming, IACT operates as a general-purpose autonomous system driven purely by user dialogue. Given a high-level objective, the system autonomously grows a dynamic, recursive agent topology incrementally tailored to the problem's structure. This allows it to scale its organizational complexity to match open-ended tasks. To mitigate the error propagation inherent in unidirectional function calls, IACT introduces interactional redundancy by replacing rigid invocations with bidirectional, stateful dialogues. This mechanism enables runtime error correction and ambiguity resolution. We describe the architecture, design principles, and practical lessons behind the production deployment of this model in the kragent.ai system, presenting qualitative evidence from real-world workflows rather than exhaustive benchmark results.

</details>


### [65] [Target-specific Adaptation and Consistent Degradation Alignment for Cross-Domain Remaining Useful Life Prediction](https://arxiv.org/abs/2512.02610)
*Yubo Hou,Mohamed Ragab,Min Wu,Chee-Keong Kwoh,Xiaoli Li,Zhenghua Chen*

Main category: cs.AI

TL;DR: 提出TACDA方法用于跨域剩余使用寿命预测，通过目标域重构策略保留目标特定信息，并采用聚类配对策略实现退化阶段一致性对齐，显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动的RUL预测方法假设训练和测试数据来自相同分布，但实际工业场景中存在域差异问题。先前对抗域适应方法专注于提取域不变特征，但忽略了目标特定信息和退化阶段的一致性特征。

Method: 提出TACDA方法：1）在对抗适应过程中加入目标域重构策略，在提取域不变特征的同时保留目标特定信息；2）开发聚类配对策略，确保相似退化阶段之间的一致性对齐。

Result: 通过大量实验验证，TACDA方法在两个不同评估指标上均显著优于现有最先进方法，展现出卓越的跨域RUL预测性能。

Conclusion: TACDA方法通过同时考虑域不变特征学习和目标特定信息保留，以及退化阶段一致性对齐，有效解决了跨域RUL预测中的域差异问题，为实际工业应用提供了有效解决方案。

Abstract: Accurate prediction of the Remaining Useful Life (RUL) in machinery can significantly diminish maintenance costs, enhance equipment up-time, and mitigate adverse outcomes. Data-driven RUL prediction techniques have demonstrated commendable performance. However, their efficacy often relies on the assumption that training and testing data are drawn from the same distribution or domain, which does not hold in real industrial settings. To mitigate this domain discrepancy issue, prior adversarial domain adaptation methods focused on deriving domain-invariant features. Nevertheless, they overlook target-specific information and inconsistency characteristics pertinent to the degradation stages, resulting in suboptimal performance. To tackle these issues, we propose a novel domain adaptation approach for cross-domain RUL prediction named TACDA. Specifically, we propose a target domain reconstruction strategy within the adversarial adaptation process, thereby retaining target-specific information while learning domain-invariant features. Furthermore, we develop a novel clustering and pairing strategy for consistent alignment between similar degradation stages. Through extensive experiments, our results demonstrate the remarkable performance of our proposed TACDA method, surpassing state-of-the-art approaches with regard to two different evaluation metrics. Our code is available at https://github.com/keyplay/TACDA.

</details>


### [66] [Zero-Shot Instruction Following in RL via Structured LTL Representations](https://arxiv.org/abs/2512.02633)
*Mattia Giuri,Mathias Jackermeier,Alessandro Abate*

Main category: cs.AI

TL;DR: 提出一种基于图神经网络编码布尔公式序列的新方法，用于学习遵循任意LTL指令的多任务策略，解决现有方法在多个高级事件同时发生且复杂交互环境中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有将LTL指令解释为有限自动机的方法，在多个原子命题同时为真且可能复杂交互的环境中表现不足，需要新的解决方案来处理这种复杂情况。

Method: 提出基于图神经网络编码布尔公式序列的方法，这些公式直接对应自动机中的转移，从而生成结构化任务表示，用于条件化多任务策略。

Result: 在复杂的基于象棋的环境中进行的实验证明了该方法的优势，能够有效处理多个高级事件同时发生且复杂交互的情况。

Conclusion: 该方法通过图神经网络编码布尔公式序列，成功解决了现有LTL-RL方法在处理多个同时发生且复杂交互事件时的局限性，为复杂环境中的指令遵循提供了有效解决方案。

Abstract: Linear temporal logic (LTL) is a compelling framework for specifying complex, structured tasks for reinforcement learning (RL) agents. Recent work has shown that interpreting LTL instructions as finite automata, which can be seen as high-level programs monitoring task progress, enables learning a single generalist policy capable of executing arbitrary instructions at test time. However, existing approaches fall short in environments where multiple high-level events (i.e., atomic propositions) can be true at the same time and potentially interact in complicated ways. In this work, we propose a novel approach to learning a multi-task policy for following arbitrary LTL instructions that addresses this shortcoming. Our method conditions the policy on sequences of simple Boolean formulae, which directly align with transitions in the automaton, and are encoded via a graph neural network (GNN) to yield structured task representations. Experiments in a complex chess-based environment demonstrate the advantages of our approach.

</details>


### [67] [Exploring Depth Generalization in Large Language Models for Solving Recursive Logic Tasks](https://arxiv.org/abs/2512.02677)
*Zhiyuan He*

Main category: cs.AI

TL;DR: 本文研究大语言模型在递归推理问题中的深度泛化能力，发现标准Transformer架构在处理超出训练深度的嵌套结构时表现不佳，并提出了一种循环定位替换管道来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在许多任务上表现出色，但在处理需要解决嵌套层次结构的递归推理问题时面临显著挑战。先前研究主要关注长度泛化（处理比训练时更长的序列），而本文研究一个不同且未被充分探索的局限性：深度泛化，即模型处理比训练时更深嵌套层次的能力。

Method: 提出了一种新颖的循环定位替换管道，将递归问题分解为可管理的子组件。该方法使用两个专门模型：定位器识别可解的子表达式，替换器评估这些组件同时保持整体结构。在三个精心设计的领域（布尔代数、递归算术和命题逻辑）进行评估，每个领域都有可控的递归深度。

Result: 研究表明标准Transformer架构在处理超出训练深度的递归问题时表现迅速衰减，而提出的循环定位替换方法有效缓解了在分布外递归深度测试时的性能衰减。

Conclusion: 大语言模型在递归推理中存在深度泛化限制，这源于其无法维持类似栈的行为来跟踪和解决多层嵌套依赖。通过分解递归问题的循环定位替换管道可以有效解决这一限制，为处理复杂嵌套结构提供了有前景的方向。

Abstract: Large language models have demonstrated remarkable capabilities across many tasks, yet face significant challenges when dealing with recursive reasoning problems, those requiring the resolution of nested hierarchical structures. While prior research has extensively studied length generalization (a model's ability to handle longer sequences than seen during training), we investigate a distinct and underexplored limitation: depth generalization. Here, depth refers to the number of nested levels in a hierarchical problem, such as the layers of parentheses in a mathematical expression or the nesting of logical clauses in a Boolean formula. Our work reveals that standard transformer architectures struggle with problems involving deeper recursion than encountered during training, even when they perform well on longer but non-nested sequences. This limitation stems from their inability to maintain stack-like behavior, the capacity to track and resolve multiple levels of nested dependencies. Through systematic analysis, we demonstrate how this architectural constraint leads to rapid performance decay as the depth of the recursion increases. To address this challenge, we develop a novel looped locate-and-replace pipeline that decomposes recursive problems into manageable subcomponents. The approach employs two specialized models: a locator that identifies solvable subexpressions and a replacer that evaluates these components while preserving the overall structure. We evaluated this method in three carefully designed domains: Boolean algebra, recursive arithmetic, and propositional logic, each with a controllable depth of recursion. We show that our method effectively alleviates the performance decay when tested on out-of-distribution recursion depth.

</details>


### [68] [Learning What to Attend First: Modality-Importance-Guided Reasoning for Reliable Multimodal Emotion Understanding](https://arxiv.org/abs/2512.02699)
*Hyeongseop Rha,Jeong Hun Yeo,Junil Won,Se Jin Park,Yong Man Ro*

Main category: cs.AI

TL;DR: MIGR框架通过模态重要性引导推理，解决多模态情感理解中的推理漂移问题，从情感主导模态开始推理，提高解释的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态情感理解方法存在推理漂移问题：模型逐渐依赖自身生成的文本而非多模态证据，且解释过度受视觉启动推理路径影响，导致解释与情感不一致。

Method: 提出模态重要性机制识别情感主导模态，基于此重新组织推理序列，使解释从最关键模态开始。采用两阶段框架：模态对齐监督微调和模态感知奖励优化。

Result: 在DFEW基准测试中，MIGR显著提高推理可靠性，将正确预测但情感不一致解释的比例从18.10%降至7.37%。

Conclusion: 从情感主导模态开始推理能有效防止早期推理被误导，生成情感基础、因果相关且保持连贯性的解释，提高多模态情感理解的可靠性。

Abstract: In this paper, we present Modality-Importance-Guided Reasoning (MIGR), a framework designed to improve the reliability of reasoning-based multimodal emotion understanding in multimodal large language models. Although existing methods have advanced emotion understanding, they often suffer from reasoning drift: models gradually rely on their own generated text instead of multimodal evidence, and their explanations are overly shaped by visually initiated reasoning paths. To address these issues, we introduce Modality Importance (MI), a simple yet effective mechanism for identifying the emotion-dominant modality. Using MI, MIGR reorganizes reasoning sequences so that explanations begin from the modality most critical to the target emotion, preventing early reasoning from being misled by less informative cues. Our two-stage framework-comprising modality-aligned supervised fine-tuning and modality-aware reward optimization-encourages models to generate emotionally grounded, causally relevant, and coherence-preserving explanations. Experimental results on the DFEW benchmark show that MIGR substantially improves reasoning reliability, decreasing instances of correct predictions accompanied by emotionally inconsistent explanations from 18.10% to 7.37%. These results confirm the benefit of initiating reasoning from the emotion-dominant modality.

</details>


### [69] [Training Data Attribution for Image Generation using Ontology-Aligned Knowledge Graphs](https://arxiv.org/abs/2512.02713)
*Theodoros Aivalis,Iraklis A. Klampanos,Antonis Troumpoukis,Joemon M. Jose*

Main category: cs.AI

TL;DR: 提出一个通过知识图谱对比来追踪生成模型输出来源的框架，用于分析版权、透明度和可解释性


<details>
  <summary>Details</summary>
Motivation: 随着生成模型能力增强，透明度、问责制和版权侵权问题日益突出，需要理解训练数据如何影响模型输出

Method: 利用多模态大语言模型从图像中提取结构化三元组，构建与领域本体对齐的知识图谱，通过比较生成图像和训练图像的KG来追踪潜在影响

Result: 通过局部模型去学习和风格特定实验验证了方法的有效性，支持版权分析、数据集透明度和可解释AI

Conclusion: 该框架有助于开发促进人类协作、创造力和激发好奇心的AI系统

Abstract: As generative models become powerful, concerns around transparency, accountability, and copyright violations have intensified. Understanding how specific training data contributes to a model's output is critical. We introduce a framework for interpreting generative outputs through the automatic construction of ontologyaligned knowledge graphs (KGs). While automatic KG construction from natural text has advanced, extracting structured and ontology-consistent representations from visual content remains challenging -- due to the richness and multi-object nature of images. Leveraging multimodal large language models (LLMs), our method extracts structured triples from images, aligned with a domain-specific ontology. By comparing the KGs of generated and training images, we can trace potential influences, enabling copyright analysis, dataset transparency, and interpretable AI. We validate our method through experiments on locally trained models via unlearning, and on large-scale models through a style-specific experiment. Our framework supports the development of AI systems that foster human collaboration, creativity and stimulate curiosity.

</details>


### [70] [Menta: A Small Language Model for On-Device Mental Health Prediction](https://arxiv.org/abs/2512.02716)
*Tianyi Zhang,Xiangyuan Xue,Lingyan Ruan,Shiya Fu,Feng Xia,Simon D'Alfonso,Vassilis Kostakos,Hong Jia*

Main category: cs.AI

TL;DR: Menta是一个专门为社交媒体心理健康预测优化的轻量级小语言模型，在多任务分类中表现优于现有SLM和部分LLM，并能在移动设备上实时部署。


<details>
  <summary>Details</summary>
Motivation: 全球心理健康问题影响数亿人，但早期检测仍然有限。虽然大语言模型在心理健康应用中有潜力，但其规模和计算需求阻碍了实际部署。小语言模型提供了轻量级替代方案，但在基于社交媒体的心理健康预测方面研究不足。

Method: 提出Menta模型，采用LoRA微调框架、跨数据集策略和平衡准确率导向的损失函数，在六个分类任务上联合训练，专门针对社交媒体数据进行心理健康预测优化。

Result: Menta在六个任务上平均比最佳非微调SLM提升15.2%，在抑郁和压力分类任务上准确率超过130亿参数LLM，模型大小约为3.25倍小。在iPhone 15 Pro Max上实时部署仅需约3GB内存。

Conclusion: Menta展示了轻量级小语言模型在可扩展、隐私保护的实时心理健康监测方面的潜力，为实际部署提供了可行方案。

Abstract: Mental health conditions affect hundreds of millions globally, yet early detection remains limited. While large language models (LLMs) have shown promise in mental health applications, their size and computational demands hinder practical deployment. Small language models (SLMs) offer a lightweight alternative, but their use for social media--based mental health prediction remains largely underexplored. In this study, we introduce Menta, the first optimized SLM fine-tuned specifically for multi-task mental health prediction from social media data. Menta is jointly trained across six classification tasks using a LoRA-based framework, a cross-dataset strategy, and a balanced accuracy--oriented loss. Evaluated against nine state-of-the-art SLM baselines, Menta achieves an average improvement of 15.2\% across tasks covering depression, stress, and suicidality compared with the best-performing non--fine-tuned SLMs. It also achieves higher accuracy on depression and stress classification tasks compared to 13B-parameter LLMs, while being approximately 3.25x smaller. Moreover, we demonstrate real-time, on-device deployment of Menta on an iPhone 15 Pro Max, requiring only approximately 3GB RAM. Supported by a comprehensive benchmark against existing SLMs and LLMs, Menta highlights the potential for scalable, privacy-preserving mental health monitoring. Code is available at: https://xxue752-nz.github.io/menta-project/

</details>


### [71] [StockMem: An Event-Reflection Memory Framework for Stock Forecasting](https://arxiv.org/abs/2512.02720)
*He Wang,Wenyilin Xiao,Songqiao Han,Hailiang Huang*

Main category: cs.AI

TL;DR: StockMem：基于事件-反思双层记忆框架的股价预测方法，通过横向整合和纵向追踪事件演化，构建事件知识库和因果经验库，实现可解释的金融预测。


<details>
  <summary>Details</summary>
Motivation: 股价预测面临市场波动性和实时事件敏感性的挑战。虽然大语言模型为基于文本的预测提供了新途径，但在金融领域的应用受到新闻数据噪声和文本中缺乏明确答案的限制。通用记忆架构难以识别价格变动的关键驱动因素。

Method: 提出StockMem事件-反思双层记忆框架：1）将新闻结构化为事件，从两个维度挖掘：横向整合整合每日事件，纵向追踪捕获事件演化以提取反映市场预期差异的增量信息，构建时间事件知识库；2）通过分析事件-价格动态，形成因果经验的反思知识库；3）预测时检索类似历史场景，结合当前事件、增量数据和过去经验进行推理。

Result: 实验表明StockMem优于现有记忆架构，通过追踪影响价格的信息链提供更优、可解释的推理，增强了金融预测中的决策透明度。

Conclusion: StockMem框架通过结构化事件挖掘和因果经验积累，有效解决了金融预测中的噪声数据和关键驱动因素识别问题，为基于大语言模型的金融预测提供了更透明、可解释的解决方案。

Abstract: Stock price prediction is challenging due to market volatility and its sensitivity to real-time events. While large language models (LLMs) offer new avenues for text-based forecasting, their application in finance is hindered by noisy news data and the lack of explicit answers in text. General-purpose memory architectures struggle to identify the key drivers of price movements. To address this, we propose StockMem, an event-reflection dual-layer memory framework. It structures news into events and mines them along two dimensions: horizontal consolidation integrates daily events, while longitudinal tracking captures event evolution to extract incremental information reflecting market expectation discrepancies. This builds a temporal event knowledge base. By analyzing event-price dynamics, the framework further forms a reflection knowledge base of causal experiences. For prediction, it retrieves analogous historical scenarios and reasons with current events, incremental data, and past experiences. Experiments show StockMem outperforms existing memory architectures and provides superior, explainable reasoning by tracing the information chain affecting prices, enhancing decision transparency in financial forecasting.

</details>


### [72] [AuditCopilot: Leveraging LLMs for Fraud Detection in Double-Entry Bookkeeping](https://arxiv.org/abs/2512.02726)
*Md Abdul Kadir,Sai Suresh Macharla Vasu,Sidharth S. Nair,Daniel Sonntag*

Main category: cs.AI

TL;DR: LLMs在复式记账异常检测中超越传统规则方法和机器学习基线，提供自然语言解释，支持AI增强审计


<details>
  <summary>Details</summary>
Motivation: 传统日记账测试方法产生大量误报且难以检测细微异常，需要更有效的异常检测方法

Method: 在合成和真实匿名账本上对LLaMA、Gemma等最先进LLMs进行基准测试，与传统JETs和机器学习基线比较

Result: LLMs在异常检测方面持续优于传统规则方法和经典机器学习基线，同时提供增强可解释性的自然语言解释

Conclusion: LLMs展示了AI增强审计的潜力，人类审计师与基础模型协作可增强财务完整性

Abstract: Auditors rely on Journal Entry Tests (JETs) to detect anomalies in tax-related ledger records, but rule-based methods generate overwhelming false positives and struggle with subtle irregularities. We investigate whether large language models (LLMs) can serve as anomaly detectors in double-entry bookkeeping. Benchmarking SoTA LLMs such as LLaMA and Gemma on both synthetic and real-world anonymized ledgers, we compare them against JETs and machine learning baselines. Our results show that LLMs consistently outperform traditional rule-based JETs and classical ML baselines, while also providing natural-language explanations that enhance interpretability. These results highlight the potential of \textbf{AI-augmented auditing}, where human auditors collaborate with foundation models to strengthen financial integrity.

</details>


### [73] [Self-Improving AI Agents through Self-Play](https://arxiv.org/abs/2512.02731)
*Przemyslaw Chojecki*

Main category: cs.AI

TL;DR: 将心理测量电池的模数理论框架扩展到动力系统领域，将智能体形式化为由计算资源参数化的流，推导出保证自我改进稳定性的方差不等式条件，并统一了语言自博弈、自我修正和合成数据引导等近期研究。


<details>
  <summary>Details</summary>
Motivation: 现有AAI能力评分是智能体表示空间上的静态泛函，无法捕捉智能体随计算资源变化的动态改进过程。需要建立理论框架来形式化智能体的动态演化，理解自我改进的数学条件。

Method: 将智能体形式化为由计算资源r参数化的流ν_r，受递归的生成器-验证器-更新器(GVU)算子控制。证明该算子在参数流形Θ上生成向量场，将自我改进系数κ定义为能力泛函沿该流的李导数。推导出方差不等式作为自我改进稳定性的谱条件。

Result: 证明了方差不等式是自我改进稳定性的充分条件（在温和正则性下）。κ>0的充分条件是生成和验证的组合噪声足够小（考虑曲率和步长效应）。将STaR、SPIN、Reflexion、GANs和AlphaZero等架构统一为满足方差不等式的GVU算子的具体拓扑实现。

Conclusion: 建立了智能体动态自我改进的统一理论框架，将近期多种自我改进方法统一为GVU算子的具体实例，为理解和设计自我改进系统提供了数学基础。

Abstract: We extend the moduli-theoretic framework of psychometric batteries to the domain of dynamical systems. While previous work established the AAI capability score as a static functional on the space of agent representations, this paper formalizes the agent as a flow $ν_r$ parameterized by computational resource $r$, governed by a recursive Generator-Verifier-Updater (GVU) operator. We prove that this operator generates a vector field on the parameter manifold $Θ$, and we identify the coefficient of self-improvement $κ$ as the Lie derivative of the capability functional along this flow.
  The central contribution of this work is the derivation of the Variance Inequality, a spectral condition that is sufficient (under mild regularity) for the stability of self-improvement. We show that a sufficient condition for $κ> 0$ is that, up to curvature and step-size effects, the combined noise of generation and verification must be small enough.
  We then apply this formalism to unify the recent literature on Language Self-Play (LSP), Self-Correction, and Synthetic Data bootstrapping. We demonstrate that architectures such as STaR, SPIN, Reflexion, GANs and AlphaZero are specific topological realizations of the GVU operator that satisfy the Variance Inequality through filtration, adversarial discrimination, or grounding in formal systems.

</details>


### [74] [A Framework for Causal Concept-based Model Explanations](https://arxiv.org/abs/2512.02735)
*Anna Rodum Bjøru,Jacob Lysnæs-Larsen,Oskar Jørgensen,Inga Strümke,Helge Langseth*

Main category: cs.AI

TL;DR: 提出基于因果概念的后置可解释AI框架，通过概念干预的充分概率生成局部和全局解释，强调解释需同时具备可理解性和忠实性


<details>
  <summary>Details</summary>
Motivation: 当前非可解释模型的解释方法需要在可理解性和忠实性之间取得平衡。现有方法往往难以同时满足这两个要求，需要一种既能提供清晰概念词汇，又能忠实反映模型决策过程的解释框架。

Method: 提出因果概念后置XAI框架，通过计算概念干预的充分概率来生成局部和全局解释。使用概念基础词汇，并基于隐式因果解释。在CelebA数据集上训练分类器，构建概念证明模型来展示解释生成。

Result: 展示了在CelebA数据集上生成的概念解释示例，证明了框架的可理解性（通过清晰的概念词汇）和忠实性（通过强调框架假设和解释上下文对齐的重要性）。

Conclusion: 该框架为后置XAI提供了一种平衡可理解性和忠实性的方法，强调解释生成和解释理解的上下文必须保持一致，才能确保解释的有效性和可靠性。

Abstract: This work presents a conceptual framework for causal concept-based post-hoc Explainable Artificial Intelligence (XAI), based on the requirements that explanations for non-interpretable models should be understandable as well as faithful to the model being explained. Local and global explanations are generated by calculating the probability of sufficiency of concept interventions. Example explanations are presented, generated with a proof-of-concept model made to explain classifiers trained on the CelebA dataset. Understandability is demonstrated through a clear concept-based vocabulary, subject to an implicit causal interpretation. Fidelity is addressed by highlighting important framework assumptions, stressing that the context of explanation interpretation must align with the context of explanation generation.

</details>


### [75] [Enhancing Automated Paper Reproduction via Prompt-Free Collaborative Agents](https://arxiv.org/abs/2512.02812)
*Zijie Lin,Qilin Cai,Liang Shen,Mingjun Xiao*

Main category: cs.AI

TL;DR: 提出无需人工设计提示词的协作智能体框架，通过验证和精炼两个智能体自动提升论文到代码生成的准确性和完整性，相比基线提升约15%和13%


<details>
  <summary>Details</summary>
Motivation: 现有自动化论文复现框架缺乏对每个生成步骤输出的验证和精炼机制，或过度依赖人工设计的提示词进行自我优化，这限制了框架的适应性和可扩展性

Method: 提出无提示协作智能体框架，包含验证智能体（检查每个步骤输出是否满足系统提示要求）和精炼智能体（基于识别的问题修订输出），仅利用原始系统提示实现自动验证和改进

Result: 在PaperBench Code-Dev和Paper2CodeBench数据集上的实验表明，该方法显著提高了复现代码的准确性和完整性，相比无智能体的基线分别提升约15%和13%，且在不同数据集上表现出鲁棒性和一致性

Conclusion: 提出的无提示协作智能体框架有效解决了现有论文复现系统的局限性，通过自动验证和精炼机制显著提升了代码生成质量，为自动化科学复现提供了更可靠和可扩展的解决方案

Abstract: Automated paper reproduction has emerged as a promising approach to accelerate scientific research, employing multi-step workflow frameworks to systematically convert academic papers into executable code. However, existing frameworks often lack mechanisms to verify and refine the outputs at each generation step, or rely heavily on manually designed prompts for self-refinement, which limits their adaptability and scalability. To address these limitations, we propose a prompt-free collaborative agent framework that automatically enhances the quality of paper-to-code generation. Our approach employs two collaborative agents: a verification agent that examines whether the outputs at each step satisfy the requirements specified in the corresponding system prompt, and a refinement agent that revises the outputs based on the identified issues. Unlike previous methods that require human experts to craft specific refinement prompts for each step, our framework achieves automatic verification and improvement by leveraging only the original system prompts. We integrate our collaborative agents into the Paper2Code framework and conduct comprehensive experiments on PaperBench Code-Dev and Paper2CodeBench datasets. Experimental results demonstrate that our approach significantly improves the accuracy and completeness of reproduced code, achieving performance gains of approximately 15\% and 13\%, respectively, compared to the baseline without our agents. Furthermore, comparative experiments against Self-Refine validate the robustness and consistency of our prompt-free approach across different datasets.

</details>


### [76] [Radiologist Copilot: An Agentic Assistant with Orchestrated Tools for Radiology Reporting with Quality Control](https://arxiv.org/abs/2512.02814)
*Yongrui Yu,Zhongzhen Huang,Linjie Mu,Shaoting Zhang,Xiaofan Zhang*

Main category: cs.AI

TL;DR: Radiologist Copilot是一个基于大语言模型的智能助手，通过编排工具实现自动化放射学报告生成与质量控制，超越现有方法，提供全面支持。


<details>
  <summary>Details</summary>
Motivation: 放射学报告是临床检查中重要但耗时且易出错的任务，现有自动化方法主要关注报告生成阶段，忽略了关键的质量控制程序，限制了为放射科医生提供全面支持的能力。

Method: 利用大语言模型作为推理核心，构建智能代理系统，自主选择工具、规划和执行动作，模拟放射科医生在整个报告过程中的行为。编排的工具包括：区域定位、基于"think with image"范式的区域分析规划、策略性模板选择、质量评估和反馈驱动的自适应精炼。

Result: 实验结果表明，Radiologist Copilot在放射学报告方面显著优于其他最先进的方法。

Conclusion: Radiologist Copilot能够促进准确、完整和高效的放射学报告，协助放射科医生并提高临床效率。源代码将在接受后发布。

Abstract: Radiology reporting is an essential yet time-consuming and error-prone task for radiologists in clinical examinations, especially for volumetric medical images. Rigorous quality control is also critical but tedious, ensuring that the final report meets clinical standards. Existing automated approaches, including radiology report generation methods and medical vision-language models, focus mainly on the report generation phase and neglect the crucial quality control procedure, limiting their capability to provide comprehensive support to radiologists. We propose Radiologist Copilot, an agentic AI assistant equipped with orchestrated tools designed for automated radiology reporting with quality control. Leveraging large language models as the reasoning backbone, the agentic system autonomously selects tools, plans, and executes actions, emulating the behavior of radiologists throughout the holistic radiology reporting process. The orchestrated tools include region localization, think with image paradigm directed region analysis planning, strategic template selection for report generation, quality assessment and feedback-driven adaptive refinement for quality control. Therefore, Radiologist Copilot facilitates accurate, complete, and efficient radiology reporting, assisting radiologists and improving clinical efficiency. Experimental results demonstrate that Radiologist Copilot significantly surpasses other state-of-the-art methods in radiology reporting. The source code will be released upon acceptance.

</details>


### [77] [The future of AI in critical mineral exploration](https://arxiv.org/abs/2512.02879)
*Jef Caers*

Main category: cs.AI

TL;DR: 提出基于贝叶斯主义和证伪原则的新科学方法，利用AI减少认知偏差和假阳性，降低勘探成本


<details>
  <summary>Details</summary>
Motivation: 尽管投资增加，但过去20年新发现的关键矿物减少，需要解决勘探效率问题

Method: 提出基于贝叶斯主义和证伪原则的新科学方法，使用无监督学习与领域专家协作生成地质假说，以及人机交互AI算法优化数据采集规划

Result: 提供实用的勘探协议模板，通过AI减少认知偏差和假阳性，降低勘探成本

Conclusion: AI能够实现严谨的科学方法，通过减少认知偏差和优化数据采集来提高矿物勘探效率

Abstract: The energy transition through increased electrification has put the worlds attention on critical mineral exploration Even with increased investments a decrease in new discoveries has taken place over the last two decades Here I propose a solution to this problem where AI is implemented as the enabler of a rigorous scientific method for mineral exploration that aims to reduce cognitive bias and false positives drive down the cost of exploration I propose a new scientific method that is based on a philosophical approach founded on the principles of Bayesianism and falsification In this approach data acquisition is in the first place seen as a means to falsify human generated hypothesis Decision of what data to acquire next is quantified with verifiable metrics and based on rational decision making A practical protocol is provided that can be used as a template in any exploration campaign However in order to make this protocol practical various form of artificial intelligence are needed I will argue that the most important form are one novel unsupervised learning methods that collaborate with domain experts to better understand data and generate multiple competing geological hypotheses and two humanintheloop AI algorithms that can optimally plan various geological geophysical geochemical and drilling data acquisition where uncertainty reduction of geological hypothesis precedes the uncertainty reduction on grade and tonnage

</details>


### [78] [Martingale Score: An Unsupervised Metric for Bayesian Rationality in LLM Reasoning](https://arxiv.org/abs/2512.02914)
*Zhonghao He,Tianyi Qiu,Hirokazu Shirado,Maarten Sap*

Main category: cs.AI

TL;DR: 该研究提出基于鞅性质的无监督评分方法，用于评估大语言模型在推理过程中的信念固化现象，发现迭代推理可能导致确认偏差而非真相寻求行为。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型的推理能力有所提升，但迭代推理可能强化信念固化和确认偏差，而非促进真相寻求。需要系统评估LLM推理中的信念固化现象。

Method: 利用贝叶斯统计中的鞅性质，提出无监督的回归基鞅评分，衡量信念更新是否违反理性更新原则。在事件预测、价值负载问题和学术论文评审等开放领域进行评估。

Result: 研究发现信念固化现象在各类模型和设置中普遍存在，当前信念能正向预测未来信念更新。识别出更容易出现信念固化的模型、推理技术和领域。鞅评分在有标签领域能预测真实准确性。

Conclusion: 鞅评分作为无监督指标，能有效评估推理过程的真相寻求能力，表明LLM推理中存在系统性信念固化问题，需要改进推理机制以避免确认偏差。

Abstract: Recent advances in reasoning techniques have substantially improved the performance of large language models (LLMs), raising expectations for their ability to provide accurate, truthful, and reliable information. However, emerging evidence suggests that iterative reasoning may foster belief entrenchment and confirmation bias, rather than enhancing truth-seeking behavior. In this study, we propose a systematic evaluation framework for belief entrenchment in LLM reasoning by leveraging the Martingale property from Bayesian statistics. This property implies that, under rational belief updating, the expected value of future beliefs should remain equal to the current belief, i.e., belief updates are unpredictable from the current belief. We propose the unsupervised, regression-based Martingale Score to measure violations of this property, which signal deviation from the Bayesian ability of updating on new evidence. In open-ended problem domains including event forecasting, value-laden questions, and academic paper review, we find such violations to be widespread across models and setups, where the current belief positively predicts future belief updates, a phenomenon which we term belief entrenchment. We identify the models, reasoning techniques, and domains more prone to belief entrenchment. Finally, we validate the Martingale Score by showing that it predicts ground-truth accuracy on problem domains where ground truth labels are available. This indicates that, while designed as an unsupervised metric that operates even in domains without access to ground truth, the Martingale Score is a useful proxy of the truth-seeking ability of a reasoning process.

</details>


### [79] [Invasive Context Engineering to Control Large Language Models](https://arxiv.org/abs/2512.03001)
*Thomas Rivasseau*

Main category: cs.AI

TL;DR: 提出侵入式上下文工程，通过插入控制语句到LLM上下文中来增强模型安全性，特别是在长上下文场景下，避免依赖训练数据不足的问题


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全方法（偏好训练、提示、输入输出过滤）虽然有效，但模型仍易受攻击，且越狱概率随上下文长度增加。长上下文场景下需要更强的安全保证

Method: 提出侵入式上下文工程，在LLM上下文中插入控制语句来增强安全性。该方法可推广到思维链过程以防止欺骗行为，不依赖模型训练，避免长上下文场景下的数据短缺问题

Result: 该方法为长上下文LLM安全提供了新的解决方案，通过上下文工程而非模型训练来增强安全性，理论上能更好地应对长上下文下的安全挑战

Conclusion: 侵入式上下文工程是解决LLM长上下文安全问题的有前景方向，不依赖训练数据，可扩展性强，为LLM安全提供了新的技术路径

Abstract: Current research on operator control of Large Language Models improves model robustness against adversarial attacks and misbehavior by training on preference examples, prompting, and input/output filtering. Despite good results, LLMs remain susceptible to abuse, and jailbreak probability increases with context length. There is a need for robust LLM security guarantees in long-context situations. We propose control sentences inserted into the LLM context as invasive context engineering to partially solve the problem. We suggest this technique can be generalized to the Chain-of-Thought process to prevent scheming. Invasive Context Engineering does not rely on LLM training, avoiding data shortage pitfalls which arise in training models for long context situations.

</details>


### [80] [From Moderation to Mediation: Can LLMs Serve as Mediators in Online Flame Wars?](https://arxiv.org/abs/2512.03005)
*Dawei Li,Abdullah Alnaibari,Arslan Bisharat,Manny Sandoval,Deborah Hall,Yasin Silva,Huan Liu*

Main category: cs.AI

TL;DR: LLMs can be used as online conflict mediators, not just moderators, through judgment and steering tasks, with API models outperforming open-source ones in mediation quality.


<details>
  <summary>Details</summary>
Motivation: 随着LLMs越来越多地中介在线交流，它们培养同理心和建设性对话的潜力成为负责任AI研究的重要前沿。本研究探索LLMs是否不仅能作为检测有害内容的审核者，还能作为理解和化解在线冲突的调解者。

Method: 将调解分解为两个子任务：判断（评估对话的公平性和情感动态）和引导（生成同理心、化解冲突的信息引导参与者达成解决方案）。构建基于Reddit的大型数据集，提出结合原则评分、用户模拟和人工比较的多阶段评估流程。

Result: 实验表明，API模型在推理和干预对齐方面优于开源模型。API模型在调解任务中表现更好，特别是在理解对话动态和生成适当的调解回应方面。

Conclusion: 当前LLMs作为在线社交调解新兴代理既有前景也有局限性。API模型在调解质量上优于开源模型，但所有模型都需要进一步改进才能成为有效的在线冲突调解者。

Abstract: The rapid advancement of large language models (LLMs) has opened new possibilities for AI for good applications. As LLMs increasingly mediate online communication, their potential to foster empathy and constructive dialogue becomes an important frontier for responsible AI research. This work explores whether LLMs can serve not only as moderators that detect harmful content, but as mediators capable of understanding and de-escalating online conflicts. Our framework decomposes mediation into two subtasks: judgment, where an LLM evaluates the fairness and emotional dynamics of a conversation, and steering, where it generates empathetic, de-escalatory messages to guide participants toward resolution. To assess mediation quality, we construct a large Reddit-based dataset and propose a multi-stage evaluation pipeline combining principle-based scoring, user simulation, and human comparison. Experiments show that API-based models outperform open-source counterparts in both reasoning and intervention alignment when doing mediation. Our findings highlight both the promise and limitations of current LLMs as emerging agents for online social mediation.

</details>
